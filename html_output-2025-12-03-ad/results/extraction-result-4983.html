<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4983 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4983</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4983</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-99829812c2e5b2ee2669c839020addd2921dc673</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/99829812c2e5b2ee2669c839020addd2921dc673" target="_blank">Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection</a></p>
                <p><strong>Paper TL;DR:</strong> This paper introduces Natural Language to First-Order Logic (NL2FOL), a framework to autoformalize natural language to FOL step by step using Large Language Models (LLMs), and uses Satisfiability Modulo Theory solvers to reason about the logical validity of natural language statements.</p>
                <p><strong>Paper Abstract:</strong> Translating natural language into formal language such as First-Order Logic (FOL) is a foundational challenge in NLP with wide-ranging applications in automated reasoning, misinformation tracking, and knowledge validation. In this paper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework to autoformalize natural language to FOL step by step using Large Language Models (LLMs). Our approach addresses key challenges in this translation process, including the integration of implicit background knowledge. By leveraging structured representations generated by NL2FOL, we use Satisfiability Modulo Theory (SMT) solvers to reason about the logical validity of natural language statements. We present logical fallacy detection as a case study to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach also provides interpretable insights into the reasoning process and demonstrates robustness without requiring model fine-tuning or labeled training data. Our framework achieves strong performance on multiple datasets. On the LOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing effectively to the LOGICCLIMATE dataset with an F1-score of 80%.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4983.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4983.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NL2FOL (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NL2FOL multi-step pipeline using GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-step neurosymbolic pipeline introduced in this paper that decomposes natural language into FOL via multiple LLM prompts (claim/implication parsing, entity/property extraction, background-knowledge NLI), compiles the negated formula to SMT, and uses an SMT solver to classify validity and generate counterexamples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4o (chat-capable large transformer family used via few-shot prompting in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>NL2FOL multi-step pipeline (LLM decomposition + SMT solver)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>A structured, multi-module approach: (1) decompose input into claim(s) and implication(s); (2) extract referring expressions and properties; (3) run NLI-based relation checks and retrieve background knowledge; (4) synthesize a FOL formula via LLM; (5) negate and compile to SMT and run solver (CVC4). Diversity arises from multiple distinct reasoning modules, use of background-knowledge retrieval, NLI checks, and an external symbolic solver.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification: detect logical fallacy (invalid implication) vs valid statement by translating natural language to FOL and checking satisfiability with an SMT solver; datasets are LOGIC (fallacies), LOGICClimate (out-of-domain fallacies), and SNLI entailments as valid examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LOGIC F1 = 0.78; LOGICClimate F1 = 0.80 (reported using GPT-4o as the LLM in the NL2FOL pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>GPT-4o end-to-end few-shot classification: LOGIC F1 = 0.96; LOGICClimate F1 = 0.58</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The diverse, multi-step NL2FOL pipeline achieves strong, interpretable performance and generalizes well to out-of-domain examples (LOGICClimate), improving recall and out-of-domain robustness compared to LLM-only end-to-end methods; however, the strongest LLM end-to-end classifier (GPT-4o) achieved higher in-domain F1 on LOGIC than NL2FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>When using GPT-4o, end-to-end few-shot classification substantially outperformed NL2FOL on the in-domain LOGIC benchmark (F1 0.96 vs 0.78), indicating that a structured/diverse pipeline does not always beat a high-capability single-step LLM for in-domain data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4983.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4983.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>End-to-end LLM (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end few-shot classification using GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-call few-shot in-context prompting approach where the LLM is asked directly to label inputs as 'Logical Fallacy' or 'Valid' without intermediate FOL conversion or SMT solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4o (large chat-capable transformer), used in few-shot setup for direct classification.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>End-to-end few-shot classification (single-call LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single prompt with in-context examples; model produces a direct binary label. Similarity arises because the reasoning follows a single chain produced by one LLM call without ensembling or multiple diverse reasoning passes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of statements into logical fallacy vs valid using few-shot prompts to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LOGIC F1 = 0.96; LOGICClimate F1 = 0.58</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>NL2FOL (multi-step) with GPT-4o: LOGIC F1 = 0.78; LOGICClimate F1 = 0.80</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>End-to-end (single-call) GPT-4o attains very high in-domain performance on LOGIC but shows a pronounced drop on out-of-domain LOGICClimate; NL2FOL improves out-of-domain generalization but has lower in-domain F1 for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>End-to-end GPT-4o outperformed the diverse NL2FOL pipeline on the in-domain LOGIC benchmark, showing that in-domain, highly capable LLMs running a single reasoning pass can be superior to a structured multi-module approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4983.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4983.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NL2FOL (Llama-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NL2FOL multi-step pipeline using Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same NL2FOL pipeline but using the open Llama2-7B model for the LLM prompting stages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama2 variant with ~7B parameters, used here in few-shot prompting for the NL2FOL pipeline and also evaluated end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>NL2FOL multi-step pipeline (LLM decomposition + SMT solver)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Multi-module decomposition with background-knowledge retrieval and subsequent SMT-based validation, implemented with Llama2-7B as the prompting LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of fallacy vs valid via NL2FOL pipeline with Llama2-7B prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LOGIC F1 = 0.71; LOGICClimate F1 = 0.73</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Llama2-7B end-to-end few-shot classification: LOGIC F1 = 0.58; LOGICClimate F1 = 0.47</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>For a smaller open model (Llama2-7B), the diverse NL2FOL pipeline substantially improves F1 over end-to-end classification, particularly by boosting recall and out-of-domain performance, indicating that the multi-step, grounded approach helps weaker LLMs reason more reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>None reported where end-to-end Llama2-7B outperformed the NL2FOL pipeline; NL2FOL consistently improved performance for this model in both datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4983.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4983.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>End-to-end LLM (Llama2-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end few-shot classification using Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct few-shot prompting with Llama2-7B to classify inputs as logical fallacy or valid without intermediate formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source ~7B parameter transformer model used in few-shot in-context classification.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>End-to-end few-shot classification (single-call LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single LLM prompt with in-context examples for direct binary labeling; no explicit intermediate decomposition or solver.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification task of fallacy vs valid.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LOGIC F1 = 0.58; LOGICClimate F1 = 0.47</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>NL2FOL (multi-step) with Llama2-7B: LOGIC F1 = 0.71; LOGICClimate F1 = 0.73</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>End-to-end Llama2-7B underperforms compared to the diverse NL2FOL pipeline, suggesting the pipeline compensates for the model's limited reasoning by structuring the task and integrating symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No negative result in which the end-to-end Llama2-7B beat the NL2FOL pipeline was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4983.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4983.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NL2FOL (GPT-4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NL2FOL multi-step pipeline using GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>NL2FOL pipeline implemented with GPT-4o-mini for the LLM prompting stages; used to evaluate grounding strategies and pipeline robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's cost-efficient variant GPT-4o-mini used for few-shot prompting in NL2FOL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>NL2FOL multi-step pipeline (LLM decomposition + SMT solver)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Multi-stage decomposition plus grounding via NLI and background-knowledge retrieval; diversity stems from modular steps and addition of external context for each subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification using the NL2FOL pipeline with GPT-4o-mini prompting; also used to compare different grounding methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LOGIC F1 = 0.75; LOGICClimate F1 = 0.77</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>GPT-4o-mini end-to-end few-shot classification: LOGIC F1 = 0.91; LOGICClimate F1 = 0.60</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NL2FOL with GPT-4o-mini improves out-of-domain recall and robustness compared to end-to-end for climate domain, though end-to-end GPT-4o-mini had higher in-domain F1 on LOGIC; NL2FOL yields more interpretable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>End-to-end GPT-4o-mini outperformed NL2FOL on in-domain LOGIC (F1 0.91 vs 0.75), showing the structured pipeline isn't always superior for every model/dataset combination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4983.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4983.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>End-to-end LLM (GPT-4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end few-shot classification using GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Single-call few-shot prompting of GPT-4o-mini to directly label inputs as logical fallacy or valid.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4o-mini, a smaller/faster variant of GPT-4o employed for few-shot classification.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>End-to-end few-shot classification (single-call LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Direct few-shot prompt classification without intermediate formalization or symbolic solving.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of fallacy vs valid.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LOGIC F1 = 0.91; LOGICClimate F1 = 0.60</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>NL2FOL (multi-step) with GPT-4o-mini: LOGIC F1 = 0.75; LOGICClimate F1 = 0.77</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>End-to-end GPT-4o-mini achieves very high in-domain performance but degrades more on out-of-domain data compared to NL2FOL; NL2FOL trades some in-domain accuracy for better generalization and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>For in-domain LOGIC, end-to-end GPT-4o-mini is superior to NL2FOL, indicating that the more 'similar' single-pass reasoning can be more effective when the model has seen similar data or is sufficiently capable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4983.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4983.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMT vs LLM classifier (NL2FOL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMT-based classification (CVC4) compared to LLM classifier (GPT-4o) within NL2FOL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison within the NL2FOL pipeline of using an SMT solver (CVC4) to decide validity versus using an LLM (GPT-4o) to classify the generated logical form.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CVC4 (SMT) / GPT-4o (classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CVC4 is an SMT solver used to check satisfiability of negated FOL formulas; GPT-4o is an LLM baseline classifier applied to the same logical forms.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>SMT-based formal verification vs LLM-based classification</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>SMT-based method uses formal satisfiability checking of the negated FOL formula to determine validity (symbolic, deterministic); the LLM-based method asks GPT-4o to classify the generated logical form (neural, heuristic).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity) within NL2FOL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide validity of NL2FOL-generated FOL formulas either by running an SMT solver or by asking an LLM to classify the logical form.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SMT (CVC4) with NL2FOL: LOGIC F1 = 0.78; LOGICClimate F1 = 0.80</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>LLM classifier (GPT-4o) on same logical forms: LOGIC F1 = 0.66; LOGICClimate F1 = 0.73</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an SMT solver to evaluate the logical forms significantly outperforms using an LLM to classify the same formulas, demonstrating the advantage of symbolic solvers for precise logical inference in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No case reported where the LLM classifier outperformed the SMT-based classifier on both datasets; SMT outperformed LLM across reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4983.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4983.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grounding methods (NL2FOL, GPT-4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of background-knowledge grounding methods within NL2FOL using GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled comparison of different grounding strategies for the Background Relation Extractor: (a) no grounding, (b) LLM (properties only), (c) LLM with sentence context, (d) BART-MNLI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini / BART-MNLI</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Grounding implementations: GPT-4o-mini used as the LLM for NLI/contextual grounding; BART-MNLI is a smaller model fine-tuned for NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Background-knowledge grounding strategies (no grounding vs LLM vs LLM w/ context vs BART-MNLI)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Different grounding approaches change how background relations between properties are detected: 'no grounding' omits extra context, 'LLM' checks properties in isolation, 'LLM w/ context' includes the whole sentence when checking entailment, and 'BART-MNLI' uses a fine-tuned NLI model. Diversity here refers to varying the source and context used for the NLI check.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity) for NL2FOL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate how different background-knowledge extraction/grounding strategies affect NL2FOL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Best: LLM w/ context (c) — LOGIC F1 = 0.78; LOGICClimate F1 = 0.80. No grounding (a) baseline — LOGIC F1 = 0.66; LOGICClimate F1 = 0.69.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>No grounding (a) — LOGIC F1 = 0.66; LOGICClimate F1 = 0.69; BART-MNLI (d) — LOGIC F1 = 0.70; LOGICClimate F1 = 0.77; LLM (b) — LOGIC F1 = 0.75; LOGICClimate F1 = 0.79</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Including sentence context when using an LLM for NLI grounding yields the best performance, indicating that richer contextual grounding substantially improves both precision and recall; omitting grounding reduces recall but sometimes yields higher precision.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No-grounding baseline had very high recall but lower precision; BART-MNLI (smaller NLI model) performed worse than LLM with context, indicating smaller fine-tuned NLI models can underperform context-aware LLM grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logical fallacy detection <em>(Rating: 2)</em></li>
                <li>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 2)</em></li>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 1)</em></li>
                <li>Formal specifications from natural language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4983",
    "paper_id": "paper-99829812c2e5b2ee2669c839020addd2921dc673",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "NL2FOL (GPT-4o)",
            "name_full": "NL2FOL multi-step pipeline using GPT-4o",
            "brief_description": "A multi-step neurosymbolic pipeline introduced in this paper that decomposes natural language into FOL via multiple LLM prompts (claim/implication parsing, entity/property extraction, background-knowledge NLI), compiles the negated formula to SMT, and uses an SMT solver to classify validity and generate counterexamples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI GPT-4o (chat-capable large transformer family used via few-shot prompting in experiments).",
            "reasoning_method_name": "NL2FOL multi-step pipeline (LLM decomposition + SMT solver)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "A structured, multi-module approach: (1) decompose input into claim(s) and implication(s); (2) extract referring expressions and properties; (3) run NLI-based relation checks and retrieve background knowledge; (4) synthesize a FOL formula via LLM; (5) negate and compile to SMT and run solver (CVC4). Diversity arises from multiple distinct reasoning modules, use of background-knowledge retrieval, NLI checks, and an external symbolic solver.",
            "task_name": "LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)",
            "task_description": "Binary classification: detect logical fallacy (invalid implication) vs valid statement by translating natural language to FOL and checking satisfiability with an SMT solver; datasets are LOGIC (fallacies), LOGICClimate (out-of-domain fallacies), and SNLI entailments as valid examples.",
            "performance": "LOGIC F1 = 0.78; LOGICClimate F1 = 0.80 (reported using GPT-4o as the LLM in the NL2FOL pipeline)",
            "comparison_with_other_method": true,
            "performance_other_method": "GPT-4o end-to-end few-shot classification: LOGIC F1 = 0.96; LOGICClimate F1 = 0.58",
            "key_findings": "The diverse, multi-step NL2FOL pipeline achieves strong, interpretable performance and generalizes well to out-of-domain examples (LOGICClimate), improving recall and out-of-domain robustness compared to LLM-only end-to-end methods; however, the strongest LLM end-to-end classifier (GPT-4o) achieved higher in-domain F1 on LOGIC than NL2FOL.",
            "counter_examples_or_negative_results": "When using GPT-4o, end-to-end few-shot classification substantially outperformed NL2FOL on the in-domain LOGIC benchmark (F1 0.96 vs 0.78), indicating that a structured/diverse pipeline does not always beat a high-capability single-step LLM for in-domain data.",
            "uuid": "e4983.0",
            "source_info": {
                "paper_title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "End-to-end LLM (GPT-4o)",
            "name_full": "End-to-end few-shot classification using GPT-4o",
            "brief_description": "A single-call few-shot in-context prompting approach where the LLM is asked directly to label inputs as 'Logical Fallacy' or 'Valid' without intermediate FOL conversion or SMT solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "OpenAI GPT-4o (large chat-capable transformer), used in few-shot setup for direct classification.",
            "reasoning_method_name": "End-to-end few-shot classification (single-call LLM)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single prompt with in-context examples; model produces a direct binary label. Similarity arises because the reasoning follows a single chain produced by one LLM call without ensembling or multiple diverse reasoning passes.",
            "task_name": "LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)",
            "task_description": "Binary classification of statements into logical fallacy vs valid using few-shot prompts to the LLM.",
            "performance": "LOGIC F1 = 0.96; LOGICClimate F1 = 0.58",
            "comparison_with_other_method": true,
            "performance_other_method": "NL2FOL (multi-step) with GPT-4o: LOGIC F1 = 0.78; LOGICClimate F1 = 0.80",
            "key_findings": "End-to-end (single-call) GPT-4o attains very high in-domain performance on LOGIC but shows a pronounced drop on out-of-domain LOGICClimate; NL2FOL improves out-of-domain generalization but has lower in-domain F1 for this model.",
            "counter_examples_or_negative_results": "End-to-end GPT-4o outperformed the diverse NL2FOL pipeline on the in-domain LOGIC benchmark, showing that in-domain, highly capable LLMs running a single reasoning pass can be superior to a structured multi-module approach.",
            "uuid": "e4983.1",
            "source_info": {
                "paper_title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "NL2FOL (Llama-7B)",
            "name_full": "NL2FOL multi-step pipeline using Llama2-7B",
            "brief_description": "Same NL2FOL pipeline but using the open Llama2-7B model for the LLM prompting stages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B",
            "model_description": "Meta's Llama2 variant with ~7B parameters, used here in few-shot prompting for the NL2FOL pipeline and also evaluated end-to-end.",
            "reasoning_method_name": "NL2FOL multi-step pipeline (LLM decomposition + SMT solver)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Multi-module decomposition with background-knowledge retrieval and subsequent SMT-based validation, implemented with Llama2-7B as the prompting LLM.",
            "task_name": "LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)",
            "task_description": "Binary classification of fallacy vs valid via NL2FOL pipeline with Llama2-7B prompting.",
            "performance": "LOGIC F1 = 0.71; LOGICClimate F1 = 0.73",
            "comparison_with_other_method": true,
            "performance_other_method": "Llama2-7B end-to-end few-shot classification: LOGIC F1 = 0.58; LOGICClimate F1 = 0.47",
            "key_findings": "For a smaller open model (Llama2-7B), the diverse NL2FOL pipeline substantially improves F1 over end-to-end classification, particularly by boosting recall and out-of-domain performance, indicating that the multi-step, grounded approach helps weaker LLMs reason more reliably.",
            "counter_examples_or_negative_results": "None reported where end-to-end Llama2-7B outperformed the NL2FOL pipeline; NL2FOL consistently improved performance for this model in both datasets.",
            "uuid": "e4983.2",
            "source_info": {
                "paper_title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "End-to-end LLM (Llama2-7B)",
            "name_full": "End-to-end few-shot classification using Llama2-7B",
            "brief_description": "Direct few-shot prompting with Llama2-7B to classify inputs as logical fallacy or valid without intermediate formalization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama2-7B",
            "model_description": "Open-source ~7B parameter transformer model used in few-shot in-context classification.",
            "reasoning_method_name": "End-to-end few-shot classification (single-call LLM)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single LLM prompt with in-context examples for direct binary labeling; no explicit intermediate decomposition or solver.",
            "task_name": "LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)",
            "task_description": "Binary classification task of fallacy vs valid.",
            "performance": "LOGIC F1 = 0.58; LOGICClimate F1 = 0.47",
            "comparison_with_other_method": true,
            "performance_other_method": "NL2FOL (multi-step) with Llama2-7B: LOGIC F1 = 0.71; LOGICClimate F1 = 0.73",
            "key_findings": "End-to-end Llama2-7B underperforms compared to the diverse NL2FOL pipeline, suggesting the pipeline compensates for the model's limited reasoning by structuring the task and integrating symbolic reasoning.",
            "counter_examples_or_negative_results": "No negative result in which the end-to-end Llama2-7B beat the NL2FOL pipeline was reported.",
            "uuid": "e4983.3",
            "source_info": {
                "paper_title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "NL2FOL (GPT-4o-mini)",
            "name_full": "NL2FOL multi-step pipeline using GPT-4o-mini",
            "brief_description": "NL2FOL pipeline implemented with GPT-4o-mini for the LLM prompting stages; used to evaluate grounding strategies and pipeline robustness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini",
            "model_description": "OpenAI's cost-efficient variant GPT-4o-mini used for few-shot prompting in NL2FOL experiments.",
            "reasoning_method_name": "NL2FOL multi-step pipeline (LLM decomposition + SMT solver)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Multi-stage decomposition plus grounding via NLI and background-knowledge retrieval; diversity stems from modular steps and addition of external context for each subtask.",
            "task_name": "LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)",
            "task_description": "Binary classification using the NL2FOL pipeline with GPT-4o-mini prompting; also used to compare different grounding methods.",
            "performance": "LOGIC F1 = 0.75; LOGICClimate F1 = 0.77",
            "comparison_with_other_method": true,
            "performance_other_method": "GPT-4o-mini end-to-end few-shot classification: LOGIC F1 = 0.91; LOGICClimate F1 = 0.60",
            "key_findings": "NL2FOL with GPT-4o-mini improves out-of-domain recall and robustness compared to end-to-end for climate domain, though end-to-end GPT-4o-mini had higher in-domain F1 on LOGIC; NL2FOL yields more interpretable outputs.",
            "counter_examples_or_negative_results": "End-to-end GPT-4o-mini outperformed NL2FOL on in-domain LOGIC (F1 0.91 vs 0.75), showing the structured pipeline isn't always superior for every model/dataset combination.",
            "uuid": "e4983.4",
            "source_info": {
                "paper_title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "End-to-end LLM (GPT-4o-mini)",
            "name_full": "End-to-end few-shot classification using GPT-4o-mini",
            "brief_description": "Single-call few-shot prompting of GPT-4o-mini to directly label inputs as logical fallacy or valid.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini",
            "model_description": "OpenAI GPT-4o-mini, a smaller/faster variant of GPT-4o employed for few-shot classification.",
            "reasoning_method_name": "End-to-end few-shot classification (single-call LLM)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Direct few-shot prompt classification without intermediate formalization or symbolic solving.",
            "task_name": "LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity)",
            "task_description": "Binary classification of fallacy vs valid.",
            "performance": "LOGIC F1 = 0.91; LOGICClimate F1 = 0.60",
            "comparison_with_other_method": true,
            "performance_other_method": "NL2FOL (multi-step) with GPT-4o-mini: LOGIC F1 = 0.75; LOGICClimate F1 = 0.77",
            "key_findings": "End-to-end GPT-4o-mini achieves very high in-domain performance but degrades more on out-of-domain data compared to NL2FOL; NL2FOL trades some in-domain accuracy for better generalization and interpretability.",
            "counter_examples_or_negative_results": "For in-domain LOGIC, end-to-end GPT-4o-mini is superior to NL2FOL, indicating that the more 'similar' single-pass reasoning can be more effective when the model has seen similar data or is sufficiently capable.",
            "uuid": "e4983.5",
            "source_info": {
                "paper_title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "SMT vs LLM classifier (NL2FOL)",
            "name_full": "SMT-based classification (CVC4) compared to LLM classifier (GPT-4o) within NL2FOL",
            "brief_description": "Comparison within the NL2FOL pipeline of using an SMT solver (CVC4) to decide validity versus using an LLM (GPT-4o) to classify the generated logical form.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CVC4 (SMT) / GPT-4o (classifier)",
            "model_description": "CVC4 is an SMT solver used to check satisfiability of negated FOL formulas; GPT-4o is an LLM baseline classifier applied to the same logical forms.",
            "reasoning_method_name": "SMT-based formal verification vs LLM-based classification",
            "reasoning_method_type": "other",
            "reasoning_method_description": "SMT-based method uses formal satisfiability checking of the negated FOL formula to determine validity (symbolic, deterministic); the LLM-based method asks GPT-4o to classify the generated logical form (neural, heuristic).",
            "task_name": "LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity) within NL2FOL",
            "task_description": "Decide validity of NL2FOL-generated FOL formulas either by running an SMT solver or by asking an LLM to classify the logical form.",
            "performance": "SMT (CVC4) with NL2FOL: LOGIC F1 = 0.78; LOGICClimate F1 = 0.80",
            "comparison_with_other_method": true,
            "performance_other_method": "LLM classifier (GPT-4o) on same logical forms: LOGIC F1 = 0.66; LOGICClimate F1 = 0.73",
            "key_findings": "Using an SMT solver to evaluate the logical forms significantly outperforms using an LLM to classify the same formulas, demonstrating the advantage of symbolic solvers for precise logical inference in this task.",
            "counter_examples_or_negative_results": "No case reported where the LLM classifier outperformed the SMT-based classifier on both datasets; SMT outperformed LLM across reported metrics.",
            "uuid": "e4983.6",
            "source_info": {
                "paper_title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Grounding methods (NL2FOL, GPT-4o-mini)",
            "name_full": "Comparison of background-knowledge grounding methods within NL2FOL using GPT-4o-mini",
            "brief_description": "A controlled comparison of different grounding strategies for the Background Relation Extractor: (a) no grounding, (b) LLM (properties only), (c) LLM with sentence context, (d) BART-MNLI.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini / BART-MNLI",
            "model_description": "Grounding implementations: GPT-4o-mini used as the LLM for NLI/contextual grounding; BART-MNLI is a smaller model fine-tuned for NLI.",
            "reasoning_method_name": "Background-knowledge grounding strategies (no grounding vs LLM vs LLM w/ context vs BART-MNLI)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Different grounding approaches change how background relations between properties are detected: 'no grounding' omits extra context, 'LLM' checks properties in isolation, 'LLM w/ context' includes the whole sentence when checking entailment, and 'BART-MNLI' uses a fine-tuned NLI model. Diversity here refers to varying the source and context used for the NLI check.",
            "task_name": "LOGIC+SNLI and LOGICClimate+SNLI (logical fallacy detection / validity) for NL2FOL",
            "task_description": "Evaluate how different background-knowledge extraction/grounding strategies affect NL2FOL performance.",
            "performance": "Best: LLM w/ context (c) — LOGIC F1 = 0.78; LOGICClimate F1 = 0.80. No grounding (a) baseline — LOGIC F1 = 0.66; LOGICClimate F1 = 0.69.",
            "comparison_with_other_method": true,
            "performance_other_method": "No grounding (a) — LOGIC F1 = 0.66; LOGICClimate F1 = 0.69; BART-MNLI (d) — LOGIC F1 = 0.70; LOGICClimate F1 = 0.77; LLM (b) — LOGIC F1 = 0.75; LOGICClimate F1 = 0.79",
            "key_findings": "Including sentence context when using an LLM for NLI grounding yields the best performance, indicating that richer contextual grounding substantially improves both precision and recall; omitting grounding reduces recall but sometimes yields higher precision.",
            "counter_examples_or_negative_results": "No-grounding baseline had very high recall but lower precision; BART-MNLI (smaller NLI model) performed worse than LLM with context, indicating smaller fine-tuned NLI models can underperform context-aware LLM grounding.",
            "uuid": "e4983.7",
            "source_info": {
                "paper_title": "Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logical fallacy detection",
            "rating": 2
        },
        {
            "paper_title": "LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 2
        },
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 1
        },
        {
            "paper_title": "Formal specifications from natural language",
            "rating": 1
        }
    ],
    "cost": 0.019119499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Autoformalizing Natural Language to First-Order Logic: A Case Study in Logical Fallacy Detection</h1>
<p>Abhinav Lalwani ${ }^{1 <em>}$ Tasha Kim ${ }^{1 </em>}$ Lovish Chopra ${ }^{1 *}$ Christopher Hahn ${ }^{2}$<br>Zhijing Jin ${ }^{3,4,5, \dagger}$ Mrinmaya Sachan ${ }^{4, \dagger}$<br>${ }^{1}$ Stanford University ${ }^{2} \mathrm{X}$, the moonshot factory<br>${ }^{3}$ Max Planck Institute for Intelligent Systems ${ }^{4}$ ETH Zürich ${ }^{5}$ University of Toronto<br>{lalwani, tashakim, lovish}@stanford.edu {jinzhi, msachan}@ethz.ch</p>
<h4>Abstract</h4>
<p>Translating natural language into formal language such as First-Order Logic (FOL) is a foundational challenge in NLP with wideranging applications in automated reasoning, misinformation tracking, and knowledge validation. In this paper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework to autoformalize natural language to FOL step-by-step using Large Language Models (LLMs). Our approach addresses key challenges in this translation process, including the integration of implicit background knowledge. By leveraging structured representations generated by NL2FOL, we use Satisfiability Modulo Theory (SMT) solvers to reason about the logical validity of natural language statements. We present logical fallacy detection as a case study to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach also provides interpretable insights into the reasoning process and demonstrates robustness without requiring model fine-tuning or labeled training data. Our framework achieves strong performance on multiple datasets - on the LOGIC dataset, NL2FOL achieves an F1-score of $78 \%$, while generalizing effectively to the LOGICClimate dataset with an F1-score of $80 \%{ }^{1}$</p>
<h2>1 Introduction</h2>
<p>In recent years, Large Language Models (LLMs) have shown impressive advancements in understanding and generating natural language (Brown et al., 2020). Despite this progress, their ability to tackle complex reasoning tasks remains limited (Bubeck et al., 2023; Wei et al., 2022). These challenges are especially prevalent in multistep logical deductions, abstract reasoning, and knowledge integration in various domains (Dalvi et al., 2021; Chen et al., 2024). Addressing these limitations</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and improving the reasoning capabilities of LLMs has become a critical focus in AI research (Haluptzok et al., 2022; Gendron et al., 2024).</p>
<p>In contrast, formal reasoning tools such as Satisfiability Modulo Theory (SMT) solvers excel in reasoning, providing rigorous, provable guarantees by leveraging symbolic representations and logical calculus (Barrett et al., 2009; De Moura and Bjørner, 2008). However, a key limitation of formal solvers is their reliance on structured logical input, such as First Order Logic (FOL), which must accurately capture the semantics and context of natural language statements (Beltagy et al., 2016). This presents the challenge of translating unstructured natural language into a structured form required for formal reasoning while preserving essential context and meaning.</p>
<p>This also brings a unique opportunity: if we can reliably translate natural language into structured logical forms, we can harness the power of formal solvers to reason systematically over natural language statements. However, achieving this translation is nontrivial, as it involves accurately capturing natural language semantics (Beltagy et al., 2016). Moreover, translating to a formal logical form may cause implicit and external context to be lost, which must be reintroduced to ensure logical accuracy.</p>
<p>To address these challenges, we present NL2FOL, a novel framework that bridges the gap between natural language and formal reasoning systems. NL2FOL employs a structured, step-by-step pipeline to translate natural language inputs into first-order logic (FOL) representations, leveraging large language models (LLMs) at each step for enhanced precision and adaptability. A distinguishing feature of NL2FOL is its seamless integration of background knowledge into the generated logical forms, overcoming a major limitation of traditional formal logic frameworks - the inability to capture</p>
<table>
<thead>
<tr>
<th>Fallacy Name</th>
<th>Example</th>
<th>Logical Form</th>
</tr>
</thead>
<tbody>
<tr>
<td>Faulty Generalization</td>
<td>Sometimes flu vaccines don’t work; therefore vaccines are useless.</td>
<td>$(\exists x \in \text { FluVaccines }(\text { DoesntWork }(x)) \wedge$ <br> (FluVaccines $\subseteq$ Vaccines $) \Rightarrow$ <br> ( $\forall y \in$ Vaccines (DoesntWork $(y)))$</td>
</tr>
<tr>
<td>False Causality</td>
<td>Every time I wash my car, it rains. Me washing my car has a definite effect on the weather.</td>
<td>occurredAfter(washingCar, rain) caused(washingCar, rain)</td>
</tr>
<tr>
<td>Ad Populum</td>
<td>Everyone should like coffee: 95\% of teachers do!</td>
<td>(like(coffee, 95\%Teachers)) (like(coffee, everyone))</td>
</tr>
<tr>
<td>False Dilemma</td>
<td>I don’t want to give up my car, so I don’t think I can support fighting climate change.</td>
<td>$\forall(a)(\operatorname{giveUpCar}(a)$ <br> dontSupportFightingClimateChange $(a))$</td>
</tr>
</tbody>
</table>
<p>Table 1: Sample logical fallacies from Jin et al. (2022) along with examples and their logical forms. For each type of fallacy, we show one possible logical form.
implicit information embedded in natural language.
In this paper, we demonstrate the effectiveness of NL2FOL through a case study on logical fallacy detection, showcasing its ability to identify and explain faulty reasoning in natural language arguments. Detecting logical fallacies is particularly challenging as they often rely on reasoning patterns that appear plausible yet are fundamentally flawed (Jin et al., 2022). To address this, NL2FOL translates logical fallacies from natural language into FOL representations, enabling formal solvers to verify logical validity. These solvers generate counterexamples and explanations, which are interpreted back into natural language to enhance human comprehensibility. By incorporating intermediate natural language outputs, our pipeline improves interpretability, transparency, and debuggability (Bai et al., 2020).</p>
<p>We show that our framework achieves strong performance on the logical fallacy detection benchmarks LOGIC and LOGICClimate (Jin et al., 2022), with F1 scores of $78 \%$ and $80 \%$, respectively - outperforming existing models by $22 \%$ on the challenge set, LOGICClimate. These results highlight NL2FOL as a generalizable and interpretable tool for reasoning tasks that demand the precision of formal reasoning systems. By analyzing the strengths and weaknesses of LLMs at each step of the NL2FOL pipeline, we further identify opportunities for improving logical reasoning capabilities. Even though LLMs prove to be effective in parsing and generating logical representations for structured inputs, they often struggle with ambiguities in natural language and incorporating nuanced contextual knowledge. The ability to integrate symbolic solvers with language models positions NL2FOL as a powerful neurosymbolic approach, bridging the gap between formal reasoning and natural lan-
guage understanding.</p>
<h2>2 Related Work</h2>
<p>Logical fallacy detection. Existing work on classifying logical fallacies includes argument sufficiency classification (Stab and Gurevych, 2017), ad hominem fallacies from Reddit posts (Habernal et al., 2018b) and dialogues (Habernal et al., 2018a), rule parsers (Nakpih and Santini, 2020), structure-aware Transformers (Jin et al., 2022), multitask instruction based prompting (Alhindi et al., 2022), and instance-based reasoning (Sourati et al., 2022). To our knowledge, our work is the first on few-shot classification of logical fallacies in a step-by-step, explainable manner. By ensuring that the reasoning process is transparent, we allow users to understand and verify the system decision.</p>
<p>Natural language to formal logic. While early work on mapping text to formal logic relied heavily on grammar-based approaches (Purdy, 1991; Angeli and Manning, 2014; MacCartney and Manning, 2014), recent advances in deep learning and foundation models have enabled new data-driven techniques for translating natural language to linear temporal logic (Cosler et al., 2023; Fuggitti and Chakraborti, 2023; Liu et al., 2022) and first-order logic (Singh et al., 2020; Yang et al., 2024; Hahn et al., 2022). Neural models for parsing natural language to first-order logic (Singh et al., 2020; Yang et al., 2024) and neuro-symbolic approach combining language models with first-order logic provers (Olausson et al., 2023) have since been explored. However, these approaches still face challenges in accurately capturing implicit information or transforming complex ambiguous sentences into logical form, mainly attributed to linguistic ambiguity.</p>
<p>Aly et al. (2023) integrated LLMs with logical inference for fact verification, and while our method</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the proposed framework used for logical fallacy detection. <em>Module A</em> converts natural language input to a first-order logic formula merged with contextual relationships, <em>Module B</em> compiles the negation of a given logical formula to an SMT file with well-defined sorts for variables and predicates, and <em>Module C</em> runs CVC on the SMT file and if the negation is satisfiable, interprets the counter-model in natural language.</p>
<p>Shares the fundamental idea of employing LLMs to construct proofs and analyze relationships between textual spans, our task adds a layer of contextual reasoning by requiring the incorporation of background knowledge and maintaining interdependency between proof steps, which is not present in approaches where each proof step is treated as an independent, isolated process.</p>
<p><strong>Theory solvers.</strong> Recent work by Hahn et al. (2022) demonstrated the potential of integrating symbolic solvers with large language models (LLMs), such as tool-augmented LLMs, to combine neural and symbolic reasoning. While such approaches are promising, they often struggle to translate natural language into symbolic representations and effectively capture background knowledge. Other recent approaches (Olausson et al., 2023; Pan et al., 2023) have used theory solvers to logically reason with natural language, which we build on with several key advancements. First, we introduce a framework that handles naturalistic, real-world data and tasks with ambiguous premises and conclusions. Then, we present a method to incorporate background knowledge into logical formulas. Finally, we show that our approach introduces interpretability by allowing human verification and modification throughout the intermediate reasoning steps.</p>
<h1>3 Methodology</h1>
<p>Although powerful, LLMs struggle to detect logical fallacies in language, as it requires proper logical analysis (Jin et al., 2022). On the other hand, SMT solvers can reason over logical formulas with theoretical guarantees but require the input to be in a structured, logical form. This approach combines the strengths of both to classify logical fallacies.</p>
<p><strong>Task formulation.</strong> The task input is an argument in natural language comprising one or more sentences, which is converted into formal logical form using a chain of LLMs. Following this, an SMT solver processes the logical form and returns whether it is valid. If invalid, the SMT solver provides a counterexample explaining why it is a logical fallacy, which is then interpreted with an LLM.</p>
<p><strong>First-order logic.</strong> In FOL, propositions are represented using predicates that express properties or relations over objects in a domain. These predicates can be combined with constants, representing specific objects and variables that represent unspecified elements in the domain. An Interpretation assigns meaning to these symbols within a given context, while a Sort categorizes objects into different types, facilitating precise reasoning about their properties. Logical connectives of FOL, such as implication (⇒), universal quantifiers (∀), existential quantifiers (⇒), and operators for conjunction/and (∧), disjunction/or (∨), and negation/not (¬), allow for the construction of intricate statements.</p>
<p><strong>Module A: Natural language to first-order logic.</strong> Our approach for converting given natural language sentences into a logical form comprises multiple steps involving few-shot prompting of LLMs: (i) decomposing a sentence into multiple smaller parts that can be represented in first-order logic, (ii) identifying relationships between different subcomponents to merge them and obtain a resultant logical formula, and (iii) identifying real-world relationships between these sub-components (background knowledge) and augmenting them to ob-</p>
<p>tain a FOL formula by incorporating background knowledge in the statement. We demonstrate with a Logical Fallacy (LF) and a Valid (V) example.</p>
<h2>1. LF Example: A Logical Fallacy Input</h2>
<p>I met a tall man who loved to eat cheese, now I believe all tall people like cheese.</p>
<h2>2. V Example: A Valid Input</h2>
<p>A boy is jumping on a skateboard in the middle of a red bridge. Thus the boy does a skateboarding trick.</p>
<p>Our pipeline begins with a semantic decomposition module which decomposes natural language arguments into respective claims and implications. Generally, a sentence can be split into some claims and implications based on those claims (see Prompt 2).</p>
<h2>1. LF Example: Claim and Implication Parser</h2>
<p>Claim: A tall man loved to eat cheese.
Implication: All tall people like cheese.</p>
<h2>2. V Example: Claim and Implication Parser</h2>
<p>Claim: A boy is jumping on a skateboard in the middle of a red bridge.
Implication: The boy does a skateboarding trick.
The claims and implications are split into further sub-components and used to build up the logical form of the sentence. The next step is to identify entities in the sentence. In our work, we treat noun phrases or surrogates for noun phrases as entities (see Prompt 3). Then, we find the relationship between the different entities using Zero-Shot classification via Natural Language Inference (NLI). These relationships (e.g., subset, equality, not related) are generally helpful in deciding appropriate quantifiers in the logical form. For example, if the entities are man and people, then it can be inferred that man is a subset of people and that the man would be bound by an existential quantifier in the sentence $x$ (see Prompt 4).</p>
<div class="codehilite"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="n">LF</span><span class="w"> </span><span class="n">Example</span><span class="p">:</span><span class="w"> </span><span class="n">Entity</span><span class="w"> </span><span class="n">Extractor</span>
<span class="n">Referring</span><span class="w"> </span><span class="nb">exp</span><span class="n">ressions</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">man</span><span class="p">:</span><span class="w"> </span><span class="n">x</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">cheese</span><span class="p">:</span><span class="w"> </span><span class="n">c</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">people</span><span class="p">:</span><span class="w"> </span><span class="n">y</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="err">\</span><span class="n">subseteq</span><span class="w"> </span><span class="n">y</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<h2>2. V Example: Entity Extractor</h2>
<p>Referring expressions:</p>
<ul>
<li>boy: b</li>
<li>skateboard: s</li>
<li>bridge</li>
<li>skateboardingTrick: y</li>
</ul>
<p>The other set of sub-components are properties, which describe a trait of a referring expression or relationship between multiple referring expressions. These properties are predicates in first-order logic. We use a single module to extract the properties and
the relation between properties and entities. (see Prompt 5). We also find the relationships between various properties (see Prompt 6). For instance, in the LF Example, it can be inferred that Like and Love are contextually similar. Similarly, in our valid example, jumping over skateboard implies doing a skateboard trick. These relationships provide an additional context that is not directly present in the statement.</p>
<p>To identify these contextual relationships, we run NLI between each pair of properties, i.e., by setting one property as the hypothesis and the other as the premise as the input to the NLI model. If we find that any one property entails the other, we add the relationship property $1 \Rightarrow$ property 2 to our context. Before running the NLI model between a pair of properties, we replace the variables in each property with the referring expressions that they represent. This adds additional context that helps the NLI model identify relations. For instance, in the V Example, the NLI model is unable to find the relation between $\operatorname{JumpsOn}(x, s)$ and $\operatorname{Does}(x, y)$, but it can identify the relationship between $\operatorname{JumpsOn}($ boy, skateboard) and Does(boy, skateboardingTrick).</p>
<h2>1. LF Example: Property Extractor + Background Knowledge Retriever</h2>
<p>Properties: Tall, Love, Like
Property entity relations: $\operatorname{Tall}(x), \operatorname{Love}(x, c)$
Background knowledge:</p>
<ol>
<li>$\forall x(\operatorname{Like}(x, c) \Rightarrow \operatorname{Love}(x, c))$</li>
<li>$\forall x(\operatorname{Love}(x, c) \Rightarrow \operatorname{Like}(x, c))$</li>
<li>$x \subseteq y$</li>
<li>V Example: Property Extractor + Background Knowledge Retriever
Properties: JumpsOn, inMiddleOf, Red, Does
Property entity relations: $\operatorname{JumpsOn}(b, s)$,
$\operatorname{Red}($ bridge), inMiddleOf $(b$, bridge), Does $(b, y)$
Background knowledge:</li>
<li>$\forall x(\operatorname{JumpsOn}(b, s) \Rightarrow \operatorname{Does}(b, y))$</li>
</ol>
<p>Finally, we combine all of this information using the relationships between properties and entities to obtain the FOL form of the sentence with the help of an LLM (see Prompt 7). For a logical fallacy, the negation of the formula is expected to be satisfiable. On the contrary, for a valid statement, the negation of the formula should be unsatisfiable.</p>
<h2>1. LF Example: NL2FOL Output</h2>
<p>First-order logic: $((\forall x(\operatorname{Like}(x, c) \Rightarrow \operatorname{Love}(x, c))) \wedge$ $(\forall x(\operatorname{Love}(x, c) \Rightarrow \operatorname{Like}(x, c))) \wedge(\exists x(\operatorname{Tall}(x) \wedge$ $\operatorname{Love}(x, c)))) \Rightarrow(\forall y(\operatorname{Tall}(y) \Rightarrow \operatorname{Like}(y, c)))$</p>
<h2>2. V Example: NL2FOL Output</h2>
<p>First-order logic: $\quad(\forall x(\operatorname{JumpsOn}(x, s) \quad \Rightarrow$ $\operatorname{Does}(x, y)) \wedge \operatorname{Red}($ bridge $) \wedge$ inMiddleOf $(b$, bridge $) \wedge$ $\operatorname{JumpsOn}(b, s)) \Rightarrow \operatorname{Does}(b, y)$</p>
<p>Module B: First-order logic to SMT. The next step involves automatically creating an SMT file for the negation of the first-order logical formula generated. While one can easily write an SMT file for a logical formula manually, generating one automatically for an arbitrary formula has not been done before. Thus, we develop a compiler that parses a given logical formula and converts it into an SMT file that can be given to CVC as input, as described in Algorithm 1 (See Appendix).</p>
<p>Module C: Interpreting SMT results. To verify the validity of the logical formulas, we utilize an SMT solver, CVC4 <em>Barrett et al. (2011)</em>. The solver determines whether the formula is valid or invalid, hence a logical fallacy. In the case of invalidity, the model provides a counterexample to the original logical formula, which shows that the given claim or implication is a logical fallacy.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Example</span><span class="w"> </span><span class="p">(</span><span class="nx">Module</span><span class="w"> </span><span class="nx">B</span><span class="w"> </span><span class="nx">Output</span><span class="p">):</span>
<span class="nx">I</span><span class="w"> </span><span class="nx">met</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">tall</span><span class="w"> </span><span class="nx">man</span><span class="w"> </span><span class="nx">who</span><span class="w"> </span><span class="nx">loved</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">eat</span><span class="w"> </span><span class="nx">cheese</span><span class="p">,</span><span class="w"> </span><span class="nx">now</span><span class="w"> </span><span class="nx">I</span>
<span class="nx">believe</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">tall</span><span class="w"> </span><span class="nx">people</span><span class="w"> </span><span class="k">like</span><span class="w"> </span><span class="nx">cheese</span><span class="p">.</span>
<span class="w">    </span><span class="err">\</span><span class="nx">downarrow</span>
<span class="nx">First</span><span class="o">-</span><span class="nx">order</span><span class="w"> </span><span class="nx">logic</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">quad</span><span class="p">((</span><span class="err">\</span><span class="k">forall</span><span class="w"> </span><span class="nx">x</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Like</span><span class="p">}(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">Rightarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Love</span><span class="p">}(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="p">)))</span><span class="w"> </span><span class="err">\</span><span class="nx">wedge</span><span class="err">\</span><span class="p">)</span>
<span class="err">\</span><span class="p">((</span><span class="err">\</span><span class="k">forall</span><span class="w"> </span><span class="nx">x</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Love</span><span class="p">}(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">Rightarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Like</span><span class="p">}(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="p">)))</span><span class="w"> </span><span class="err">\</span><span class="nx">wedge</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="p">(</span><span class="err">\</span><span class="nx">exists</span><span class="w"> </span><span class="nx">x</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Tall</span><span class="p">}(</span><span class="nx">x</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">wedge</span><span class="err">\</span><span class="p">)</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Love</span><span class="p">}(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="p">))))</span><span class="w"> </span><span class="err">\</span><span class="nx">Rightarrow</span><span class="p">(</span><span class="err">\</span><span class="k">forall</span><span class="w"> </span><span class="nx">y</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Tall</span><span class="p">}(</span><span class="nx">y</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">Rightarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Like</span><span class="p">}(</span><span class="nx">y</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="p">)))</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="nx">downarrow</span>
<span class="nx">SMT</span><span class="w"> </span><span class="nx">classification</span><span class="p">:</span><span class="w"> </span><span class="nx">Logical</span><span class="w"> </span><span class="nx">fallacy</span>
<span class="nx">Explanation</span><span class="p">:</span><span class="w"> </span><span class="nx">Counterexample</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">downarrow</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="nx">John</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">tall</span><span class="w"> </span><span class="p">(</span><span class="nx">Tall</span><span class="p">(</span><span class="nx">John</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">True</span><span class="p">).</span><span class="w"> </span><span class="nx">John</span><span class="w"> </span><span class="nx">likes</span>
<span class="w">    </span><span class="nx">cheese</span><span class="w"> </span><span class="p">(</span><span class="nx">Likes</span><span class="p">(</span><span class="nx">John</span><span class="p">,</span><span class="w"> </span><span class="nx">Cheese</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">True</span><span class="p">).</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="nx">Jane</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">tall</span><span class="w"> </span><span class="p">(</span><span class="nx">Tall</span><span class="p">(</span><span class="nx">Jane</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">True</span><span class="p">).</span><span class="w"> </span><span class="nx">No</span><span class="w"> </span><span class="kd">constraint</span>
<span class="w">        </span><span class="nx">Jane</span><span class="w"> </span><span class="nx">likes</span><span class="w"> </span><span class="nx">cheese</span><span class="p">.</span>
</code></pre></div>

<p>Therefore, there exists a tall person (John) who likes cheese, but it does not follow that all tall people like cheese, since Jane serves as a counterexample.</p>
<p>Figure 2: Example of logical fallacy detection using NL2FOL. The resulting classification is explained using a counterexample generated by the SMT solver.</p>
<p>The result of the SMT solver is hard to interpret, as it uses technical terminology generally only well understood by those who are familiar with CVC4 and SMT. To obtain an explanation in natural language, we prompt an LLM with the claim, implication, referring expressions, properties, FOL formula, and the counterexample generated by CVC4. The model then interprets the counterexample with natural language, as depicted in Figure 2.</p>
<h2>4 Experiments</h2>
<p>We evaluate our approach on both logical fallacies (positive class) and valid statements (negative class). For logical fallacies, we use the LOGIC and</p>
<p>LogicClimate <em>Jin et al. (2022)</em> datasets, originally designed for training models to identify and classify different fallacies. These datasets contain examples of logical fallacies, each labeled with multiple categories from 13 different categories, including faulty generalization, circular claim, and ad hominem. The LOGIC dataset contains 2,449 examples of common logical fallacies collected mostly from quiz websites. The LOGICClimate dataset comprises 1,079 examples of logical fallacies drawn from climate change news articles on the Climate Feedback platform. It is intended to test the model's ability to generalize out-of-domain.</p>
<p>To test our approach with valid statements, we use the Stanford Natural Language Inference (SNLI) corpus <em>Bowman et al. (2015)</em>, which supports the development of natural language inference systems. This dataset features over 570,000 humanannotated sentence pairs, where each pair consists of a premise and a hypothesis labeled as entailment, contradiction, or neutral. We focus on the entailment class in this study, extracting over 170,000 sentence pairs where the premise entails the hypothesis. We construct valid sentences by combining the premise and hypothesis into a single sentence.</p>
<p>The task is set up as a simple binary classification task, where the input consists of sentences drawn from the LOGIC or LOGICClimate datasets labeled as logical fallacies or from the SNLI dataset labeled as valid sentences. Here, we treat logical fallacies as the positive class. To ensure a balanced evaluation, we select an equal number of fallacies and valid statements, allowing for a fair comparison across both classes. Finally, our model is evaluated on standard binary classification metrics such as precision, recall, f1 score, and accuracy.</p>
<p>Models. We compare our method to pretrained language models, including Llama2-7B <em>Touvron et al. (2023)</em>, GPT4o-mini <em>OpenAI (2024)</em>, GPT4o <em>OpenAI et al. (2024a)</em> and OpenAI o1-preview <em>OpenAI et al. (2024b)</em> with few-shot in-context examples (see Prompt 1). We also run NL2FOL with each of the above models used for the LLM prompting stages. Llama2-7B was chosen for our experiments as it had the best performance during testing over an initial subset of the data, outperforming Llama3.1-8B <em>Grattafiori et al. (2024)</em>, Llama3.2-11B <em>AI (2024a)</em>, and Ministral-8B <em>AI (2024b)</em>. We evaluate BART (140M parameters) <em>Lewis et al. (2020)</em> finetuned on MNLI *Williams</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Method</th>
<th>LOGIC</th>
<th></th>
<th></th>
<th></th>
<th>LOGICCLIMATE</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Acc.</td>
<td>P.</td>
<td>R.</td>
<td>F1</td>
<td>Acc.</td>
<td>P.</td>
<td>R.</td>
<td>F1</td>
</tr>
<tr>
<td>Llama-7B</td>
<td>End-to-end</td>
<td>0.41</td>
<td>0.45</td>
<td>0.82</td>
<td>0.58</td>
<td>0.31</td>
<td>0.38</td>
<td>0.62</td>
<td>0.47</td>
</tr>
<tr>
<td></td>
<td>NL2FOL (Ours)</td>
<td>0.63</td>
<td>0.58</td>
<td>0.92</td>
<td>0.71</td>
<td>0.66</td>
<td>0.60</td>
<td>0.94</td>
<td>0.73</td>
</tr>
<tr>
<td>GPT-4o-mini</td>
<td>End-to-end</td>
<td>0.91</td>
<td>0.94</td>
<td>0.88</td>
<td>0.91</td>
<td>0.64</td>
<td>0.67</td>
<td>0.55</td>
<td>0.60</td>
</tr>
<tr>
<td></td>
<td>NL2FOL (Ours)</td>
<td>0.70</td>
<td>0.64</td>
<td>0.91</td>
<td>0.75</td>
<td>0.73</td>
<td>0.66</td>
<td>0.93</td>
<td>0.77</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>End-to-end</td>
<td>0.96</td>
<td>0.96</td>
<td>0.96</td>
<td>0.96</td>
<td>0.70</td>
<td>0.95</td>
<td>0.42</td>
<td>0.58</td>
</tr>
<tr>
<td></td>
<td>NL2FOL (Ours)</td>
<td>0.78</td>
<td>0.76</td>
<td>0.82</td>
<td>0.78</td>
<td>0.80</td>
<td>0.80</td>
<td>0.80</td>
<td>0.80</td>
</tr>
<tr>
<td>OpenAI o1-preview</td>
<td>End-to-end</td>
<td>0.93</td>
<td>0.89</td>
<td>0.98</td>
<td>0.93</td>
<td>0.73</td>
<td>0.84</td>
<td>0.56</td>
<td>0.67</td>
</tr>
<tr>
<td></td>
<td>NL2FOL (Ours)</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of few-shot model performance metrics (abbreviations: Acc. $=$ accuracy, $\mathrm{P} .=$ precision, $\mathrm{R} .=$ recall, F1 = F1 score) on the LOGIC+SNLI and LOGICCLIMATE+SNLI datasets using End-to-end vs. NL2FOL (Ours). Results on NL2FOL with o1-preview are omitted as o1-preview failed to complete the pipeline in most cases, likely due to its poor instruction following capabilities.
et al., 2018) to analyze the relationships between properties and referring expressions. We ran the experiments on a V100 GPU, with one run costing around 2 GPU hours.</p>
<p>Prompt tuning. For prompt tuning, 20 samples from the LOGIC dataset were selected and manually annotated with intermediate and final results. They were then split into 10 train and 10 validation examples. For each prompt, we start with a simple description of the task. 4-6 examples were randomly selected from the train set as in-context examples, with the relevant intermediate outputs depending on the stage. Results were tested on the validation examples, and the prompt was updated to address common mistakes. To ensure fairness, a fixed number of 5 improvement iterations was used for each prompt, and the one showing best performance over the validation examples was chosen.</p>
<h2>5 Results and Discussion</h2>
<p>As shown in Table 2, our method achieves an F1 score of $78 \%$ when used with GPT-4o on the LOGIC dataset. When run end-to-end, the Llama7B model reached an F1 score of only $58 \%$, but when used with the NL2FOL pipeline, reached a score of $71 \%$. Although end-to-end classification has shown better performance in other models, comparisons can be skewed because they may have been exposed to the LOGIC dataset and its labels during training because this dataset was compiled from publicly accessible web sources. On average, NL2FOL demonstrated high recall, whereas end-to-end classification demonstrated high precision.</p>
<p>Our challenge set LOGICCLIMATE+SNLI contains real-world logical fallacies from climate change news. Since this dataset was used to test gener-
alization, the in-context examples we provide to all models are from the LOGIC dataset. NL2FOL yields results that are highly similar to the results from LOGIC, whereas end-to-end classification saw a drop in performance. This demonstrates that our system is also robust and adapts well to realworld texts, including texts with significant domainspecific context. This makes it effective in detecting and mitigating misinformation. Specifically, on this dataset, we find that NL2FOL outperforms direct translation with all LLMs that we tested.</p>
<h3>5.1 Quantitative Analysis</h3>
<p>Error analysis and interpretability. The proposed method is interpretable due to the use of natural language inputs and outputs at each step of the pipeline. This structure allows for precise identification of the specific module responsible for a failure by examining intermediate results. To evaluate this aspect, we performed an in-depth error analysis by annotating the module responsible for failure in 100 incorrect predictions made by the model. The results are summarized in Table 4.</p>
<p>Our analysis reveals that the majority of errors occur in the 'Background Knowledge Retriever', involving missed or incorrectly added contextual information in the logical form. Other errors typically pertain to incorrect identification of claims, implications, or properties. In contrast, inaccuracies in the generation of logical forms are relatively infrequent, suggesting that the model performs well in constructing accurate logical representations when provided with reliable information about the constituent entities and properties within a sentence. This finding underscores the importance of improving the background knowledge retriever module to improve overall model performance.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Sentence</th>
<th style="text-align: center;">Logical Form</th>
<th style="text-align: center;">Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">X has been around for years now. Y is new. Therefore, Y is better than X .</td>
<td style="text-align: center;">$(\operatorname{IsNew}(\mathrm{a}) \wedge \sim \operatorname{IsNew}(\mathrm{b})) \Rightarrow(\operatorname{IsBetterThan}(\mathrm{a}, \mathrm{b}))$</td>
<td style="text-align: center;">LF: Correct prediction</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">Everyone is doing the Low-Carb Diet.</td>
<td style="text-align: center;">$(\exists \mathrm{b}(\exists \mathrm{a}(\operatorname{IsDoing}(\mathrm{b}, \mathrm{a})))) \Rightarrow(\exists \mathrm{c}(\exists \mathrm{a}(\operatorname{IsDo}-$ ing(c,a)))).</td>
<td style="text-align: center;">V: Incorrect prediction - Wrong translation given when no claim given</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">V</td>
<td style="text-align: center;">Two dogs are fighting in a field. Consequently, the two dogs are outside.</td>
<td style="text-align: center;">$(\exists \mathrm{b}(\exists \mathrm{a}(\operatorname{IsFighting}(\mathrm{a}, \mathrm{b}) \wedge \operatorname{IsInField}(\mathrm{b}) \wedge \operatorname{IsIn}-$ Field(b)))) $\Rightarrow(\exists \mathrm{a}(\operatorname{IsOutside}(\mathrm{a})))$</td>
<td style="text-align: center;">LF: Incorrect prediction - Missing semantic ground truth claim: $\forall \mathrm{a}$ (IsInField(a) $\Rightarrow$ IsOutside(a))</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">V</td>
<td style="text-align: center;">A baseball player gets ready to catch a fly ball near the outfield fence. Therefore, a person is playing baseball outdoors.</td>
<td style="text-align: center;">$(\exists \mathrm{a}(\operatorname{IsGettingReady}(\mathrm{a}) \wedge(\operatorname{IsABaseballPlayer}(\mathrm{a})$ $\wedge$ IsCatchingFlyBall(a) $\wedge$ IsNearOutfieldFence(a))) $\wedge(\forall \mathrm{e}($ IsABaseballPlayer(e) $\Rightarrow$ IsPlayingBaseball(e))) $\wedge(\forall \mathrm{f}($ IsPlayingBaseball(f) $\Rightarrow$ IsABaseballPlayer(f))) $\wedge(\forall \mathrm{g}($ IsNearOutfieldFence(g) $\Rightarrow$ IsOutdoors(g)))) $\Rightarrow(\exists \mathrm{c}(\exists \mathrm{a}($ IsPlayingBaseball(a) $\wedge$ IsOutdoors(c))))</td>
<td style="text-align: center;">V: Correct Prediction - The method identifies additional context by establishing relationships such as Is BaseballPlayer implying IsPlaying Baseball, and IsNearOutfieldFence implying IsOutdoors.</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">V</td>
<td style="text-align: center;">A woman sits alone on a park bench in the sun. Hence, a woman is in a park.</td>
<td style="text-align: center;">$(\operatorname{IsSittingOn}(\mathrm{a}, \mathrm{b}) \wedge \operatorname{isParkBench}(\mathrm{b}) \wedge \operatorname{IsIn}-$ Sun(a)) $\Rightarrow(\operatorname{IsInPark}(\mathrm{a}))$.</td>
<td style="text-align: center;">LF: Incorrect prediction - Missing semantic ground truth claim: $\forall i / \forall b$ (IsSittingOn(a, b) $\wedge$ isParkBench(b) $\Rightarrow$ IsInPark(a))</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">V</td>
<td style="text-align: center;">A woman is standing at a podium. Thus, a person is at a podium.</td>
<td style="text-align: center;">$(\exists a \exists b(\operatorname{IsStandingAt}(\mathrm{b}, \mathrm{a})) \wedge \forall f \forall e \forall d(\operatorname{IsStandin}-$ $\operatorname{gAt}(\mathrm{d}, \mathrm{e}) \Rightarrow \operatorname{IsAt}(\mathrm{f}, \mathrm{e})) \Rightarrow \exists e \exists a(\operatorname{IsAt}(\mathrm{c}, \mathrm{a}))$</td>
<td style="text-align: center;">V: Correct prediction - The method identifies additional context by establishing the relationship Is StandingAt implying IsAt.</td>
</tr>
</tbody>
</table>
<p>Table 3: Some example outputs of our model (abbreviations: LF = Logical Fallacy, V = Valid statement)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Sub-Module with Error</th>
<th style="text-align: left;">Error Proportion</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Claim and Implication Parser</td>
<td style="text-align: left;">0.19</td>
</tr>
<tr>
<td style="text-align: left;">Incorrect Label</td>
<td style="text-align: left;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">Property Extractor</td>
<td style="text-align: left;">0.13</td>
</tr>
<tr>
<td style="text-align: left;">Background Knowledge Retriever</td>
<td style="text-align: left;">0.54</td>
</tr>
<tr>
<td style="text-align: left;">FOL Formulation Engine</td>
<td style="text-align: left;">0.13</td>
</tr>
</tbody>
</table>
<p>Table 4: Categorization of model errors by type on NL2FOL (GPT-4o), based on a review by domain experts in the logic of 100 randomly sampled examples</p>
<p>Impact of adding background knowledge to NL2FOL. Based on the error analysis, missing or incorrect background knowledge was a significant contributor to incorrect predictions of our method. To quantitatively assess the impact of grounding on model performance, we evaluated several approaches for NLI in the Background Relation Extractor. These included: (a) a pipeline without any background knowledge as a baseline, (b) a model without context where the LLM (GPT4o) only processes the input properties, (c) an LLM that incorporates both the input sentence and properties and (d) a smaller model specifically fine-tuned for NLI (BART-MNLI). Results are presented in Table 5.</p>
<p>We see that precision and recall both improve significantly with better grounding techniques. The</p>
<p>LLM model with sentence context achieves the highest overall performance. This is likely due to the sentence context providing information about clauses that are omitted due to the choice of representation in FOL. This indicates that integrating robust grounding mechanisms is critical to enhancing the accuracy and reliability of the method.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">LOGIC+SNLI</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">LOGICCLIMATE+SNLI</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: left;">Acc.</td>
<td style="text-align: left;">P.</td>
<td style="text-align: left;">R.</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;">Acc.</td>
<td style="text-align: left;">P.</td>
<td style="text-align: left;">R.</td>
<td style="text-align: left;">F1</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">(a) No Grounding</td>
<td style="text-align: left;">0.54</td>
<td style="text-align: left;">0.52</td>
<td style="text-align: left;">$\mathbf{0 . 8 8}$</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">0.57</td>
<td style="text-align: left;">0.54</td>
<td style="text-align: left;">$\mathbf{0 . 9 4}$</td>
<td style="text-align: left;">0.69</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">(b) LLM</td>
<td style="text-align: left;">0.76</td>
<td style="text-align: left;">$\mathbf{0 . 7 8}$</td>
<td style="text-align: left;">0.74</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.79</td>
<td style="text-align: left;">0.80</td>
<td style="text-align: left;">0.78</td>
<td style="text-align: left;">0.79</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">(c) LLM w/ context</td>
<td style="text-align: left;">$\mathbf{0 . 7 8}$</td>
<td style="text-align: left;">0.76</td>
<td style="text-align: left;">0.82</td>
<td style="text-align: left;">$\mathbf{0 . 7 8}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 0}$</td>
<td style="text-align: left;">0.80</td>
<td style="text-align: left;">0.80</td>
<td style="text-align: left;">$\mathbf{0 . 8 0}$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">(d) BART-MNLI</td>
<td style="text-align: left;">0.71</td>
<td style="text-align: left;">0.71</td>
<td style="text-align: left;">0.70</td>
<td style="text-align: left;">0.70</td>
<td style="text-align: left;">0.77</td>
<td style="text-align: left;">$\mathbf{0 . 8 1}$</td>
<td style="text-align: left;">0.71</td>
<td style="text-align: left;">0.77</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of different grounding methods on NL2FOL (GPT4o-mini) across the LOGIC+SNLI and LogicClimate+SNLI datasets</p>
<p>Impact of using an SMT solver. To assess the impact of using an SMT solver in our pipeline, we compared its performance against an LLM as a baseline for classifying the logical forms as valid or fallacies. The results, summarized in Table 6, demonstrate a significant improvement in performance metrics with the integration of the SMT solver. Results reveal the SMT-based approach significantly outperforms the LLM-based approach in all metrics across both the LOGIC and LOGIC-</p>
<p>Climate datasets. This underscores the advantage of formal reasoning systems like SMT solvers for tasks requiring precise logical inference and structured reasoning compared to LLMs, which may lack systematic consistency in such contexts.</p>
<h3>5.2 Qualitative Analysis</h3>
<h3>5.2.1 Success Modes of NL2FOL</h3>
<p>S1: Captures implicit information not mentioned in premises. Previous works that directly translate natural language to logical forms suffer from an inability to capture implicit information not mentioned in the premises (Olausson et al., 2023). Our "Background Knowledge Retriever" step allows us to capture this information in the final logical form. An illustration of this can be found in Example 4 of Table 3.</p>
<p>S2: Captures explicit information that is missed in the representation. Our pipeline is also able to capture information that is explicitly mentioned in the premises but missed due to the choice of representation in logical form. In Example 6, in Table 3, the fact that the woman is both standing and is at the podium is lost due to the choice representation IsStandingAt. However, the fact that the woman is at the podium is recovered in the final logical form due to the identified background knowledge IsStandingAt implies IsAt.</p>
<p>S3: Comparison to direct translation. To evaluate the efficacy of the multi-step LLM pipeline, we compared it against a direct translation approach, where natural language inputs were converted into logical forms with a single LLM call using a fewshot prompt. However, this task proved to be excessively complex for LLMs. Llama failed to generate any output, citing an inability to comprehend the prompt. Larger LLMs exhibited significant limitations, with over $95 \%$ of their outputs containing syntax errors. These findings highlight the inadequacy of direct translation for complex logical reasoning tasks and underscore the necessity of a structured, multi-step approach to ensure the accuracy and syntactic correctness of the logical form.</p>
<h3>5.2.2 Failure Modes of NL2FOL</h3>
<p>F1: Misses some background knowledge. As can be observed in Table 4, incorrect identification of background knowledge is the most common cause for incorrect classifications. This is because any gaps in background knowledge can cause a valid statement to be identified as a logical fallacy, and</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LOGIC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LOGICClimate</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Classifier</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">P.</td>
<td style="text-align: center;">R.</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">P.</td>
<td style="text-align: center;">R.</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: left;">SMT</td>
<td style="text-align: center;">$\mathbf{0 . 7 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 0}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.73</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of classification methods used with NL2FOL (GPT4o) on LOGIC and LOGICClimate
an incorrectly added clause can cause a fallacy to be identified as valid. One such case is present in example 3 of the Table 3. In this case, the model is not able to identify the extra context statement because the NLI model does not identify a required ground-truth relation. If this context were to be added to the claim of the logical formula, then the statement would have been predicted to be valid.</p>
<p>F2: Limitations of NLI. Our current approach is limited to discerning relationships between two properties at a time rather than handling multiple relationships concurrently. For reference, consider Example 5 in Table 3. Here, the semantic claim involves the conjunction of two properties entailing the third, while the 'Background Knowledge Retriever' only checks whether one property entails the other. Finding such complex extra context requires more advanced techniques or additional human intervention. Including them could further improve the precision of the model overall.</p>
<p>F3: Imprecision of LLMs. Among the logical fallacies that our model incorrectly predicted to be a valid statement, most of these predictions failed due to the imprecision of the LLM, leading to false translations and incorrect results. Example 2 demonstrates a case where the input does not have any claim but instead jumps straight to an implication. However, the model is not able to identify that the example has no claim. As a result, we obtain an incorrect translation with our technique.</p>
<h2>6 Conclusion</h2>
<p>We present an effective and automatic solution to detect fallacies and tackle misinformation. We developed a strategy to distinguish logical fallacies from valid statements, involving a chaining approach to convert a sentence to first-order logic using LLMs, followed by using SMT solvers to identify whether the first-order logical statement is valid or not. If not, we interpret the counter-model generated by the SMT solver in natural language. Our proposed technique shows promising results in identifying logical fallacies and valid statements,</p>
<p>as well as good generalizability across domains.</p>
<h2>Ethics Statement</h2>
<p>While the intended outcome of this research is to help fight misinformation and promote rational discourse, there are several ethical challenges that we must consider. First, dependence on AI to identify logical fallacies could influence how individuals engage in debates and discussions. There is a risk that people may over-rely on AI judgments, potentially stifling complex statements or dissenting opinions that are essential for a healthy democratic process. Moreover, the use of AI in moderating discussions, especially in identifying logical fallacies, raises ethical questions about the automation of content moderation. While it can enhance the quality of public discourse by filtering out fallacious statements, it also risks automating censorship and impacting the dynamics of online communities. In the wrong hands, logical fallacy detection tools could be exploited to silence speech or suppress viewpoints under the pretext of promoting rational discourse. This potentially allows governments or organizations to stifle opposition or critique.</p>
<p>To address these issues, we advocate for the development of ethical guidelines for AI use that emphasize transparency, accountability, and active user engagement. These measures are crucial in encouraging public literacy in AI and logical fallacies, ultimately empowering individuals to critically assess both AI output and arguments they may encounter.</p>
<h2>Limitations</h2>
<p>Scope of logical reasoning tasks. Correct identification of background knowledge is crucial for our method. While we have shown its potential in detecting logical fallacies for short and structured premises, it is important to note that this approach may miss complex relational constructs (for example, $(a \wedge b) \Rightarrow(c \vee d))$ ), in which richer logical patterns may often be required in real-world reasoning tasks such as those present in multi-paragraph contexts or Question-Answering (QA) datasets.</p>
<p>Generalizability to other tasks and domains. We have demonstrated promising results of our approach to logical fallacy detection, but whether the findings generalize to other logical tasks and domains remains unexplored. The performance of our approach in other languages is untested and may introduce unforeseen challenges.</p>
<p>Going beyond first-order logic. It is unknown whether our approach would be sufficiently expressive for reasoning tasks requiring higher-order or non-classical logic, as we limit our exploration to first-order logic. Conceptually, extending our method to the aforementioned domains is feasible but would require modification to the SMT integration and LLM-driven logic translation processes. Thus, further testing may include translating to logic beyond FOL, such as temporal and higherorder logic.</p>
<p>Computational cost. Using LLMs and SMT solvers can incur high computational costs, such as high-performance GPUs for LLM inference, CPUs optimized for SMT solvers, and high API usage, particularly for models like GPT-o1 and Llama-7B.</p>
<h2>References</h2>
<p>Meta AI. 2024a. Llama 3.2-11b model card. Accessed: 2025-02-15.</p>
<p>Mistral AI. 2024b. Ministral-8b-instruct-2410 model card. Accessed: 2025-02-15.</p>
<p>Tariq Alhindi, Tuhin Chakrabarty, Elena Musi, and Smaranda Muresan. 2022. Multitask instruction-based prompting for fallacy recognition. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8172-8187, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Rami Aly, Marek Strong, and Andreas Vlachos. 2023. QA-natver: Question answering for natural logic-based fact verification. In The 2023 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Gabor Angeli and Christopher D Manning. 2014. Naturalli: Natural logic inference for common sense reasoning. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 534-545.</p>
<p>Bing Bai, Jian Liang, Guanhua Zhang, Hao Li, Kun Bai, and Fei Wang. 2020. Why attentions may not be interpretable? Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining.</p>
<p>Clark Barrett, Christopher L. Conway, Morgan Deters, Liana Hadarean, Dejan Jovanović, Tim King, Andrew Reynolds, and Cesare Tinelli. 2011. Cvc4. In Computer Aided Verification, pages 171-177, Berlin, Heidelberg. Springer Berlin Heidelberg.</p>
<p>Clark Barrett, Aaron Stump, and Cesare Tinelli. 2009. Satisfiability modulo theories. Communications of the ACM, 52(9):69-77.</p>
<p>Iz Beltagy, Stephen Roller, Pengxiang Cheng, Katrin Erk, and Raymond J. Mooney. 2016. Representing</p>
<p>meaning with a combination of logical and distributional models. Computational Linguistics, 42(4):763-808.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4.</p>
<p>Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. 2024. Self-play fine-tuning converts weak language models to strong language models.</p>
<p>Matthias Cosler, Christopher Hahn, Daniel Mendoza, Frederik Schmitt, and Caroline Trippel. 2023. nl2spec: Interactively translating unstructured natural language to temporal logics with large language models. In Computer Aided Verification. CAV 2023. Lecture Notes in Computer Science, volume 13965, Cham. Springer.</p>
<p>Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining answers with entailment trees. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7358-7370, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Leonardo De Moura and Nikolaj Bjørner. 2008. Z3: an efficient smt solver. In Proceedings of the Theory and Practice of Software, 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS'08/ETAPS'08, page 337-340, Berlin, Heidelberg. Springer-Verlag.</p>
<p>Francesco Fuggitti and Tathagata Chakraborti. 2023. Nl2ltl - a python package for converting natural language (nl) instructions to linear temporal logic (ltl) formulas. In AAAI Conference on Artificial Intelligence.</p>
<p>Gaël Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. 2024. Large language models are not strong abstract reasoners.</p>
<p>Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,</p>
<p>Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Cauchereux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jernya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwoi Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers,</p>
<p>Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajmfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta,</p>
<p>Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd of models.</p>
<p>Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, and Benno Kiesel. 2018a. "dummy, grandpa, do you know anything?": Identifying and characterizing ad hominem fallacies in the wild. In Proceedings of the 12th International AAAI Conference on Web and Social Media (ICWSM), pages 206-215.</p>
<p>Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, and Benno Stein. 2018b. Before name-calling: Dynamics and triggers of ad hominem fallacies in web argumentation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 386-396, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Christopher Hahn, Frederik Schmitt, Julia J. Tillman, Niklas Metzger, Julian Siber, and Bernd Finkbeiner. 2022. Formal specifications from natural language.</p>
<p>Patrick M. Haluptzok, Matthew Bowers, and Adam Tauman Kalai. 2022. Language modexrls can teach themselves to program better. ArXiv, abs/2207.14502.</p>
<p>Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schoelkopf. 2022. Logical fallacy detection. In Findings of the Association for</p>
<p>Computational Linguistics: EMNLP 2022, pages 71807198, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Jason Xinyu Liu, Ziyi Yang, Benjamin Schornstein, Sam Liang, Ifrah Idrees, Stefanie Tellex, and Ankit Shah. 2022. Lang2LTL: Translating natural language commands to temporal specification with large language models. In CoRL Workshop on Language and Robot Learning.
B. MacCartney and C. D. Manning. 2014. Natural logic and natural language inference. In H. Bunt, J. Bos, and S. Pulman, editors, Computing Meaning: Volume 4, pages 129-147. Springer Netherlands, Dordrecht.</p>
<p>Callistus Ireneous Nakpih and Simone Santini. 2020. Automated discovery of logical fallacies in legal argumentation. International Journal of Artificial Intelligence \&amp; Applications.</p>
<p>Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, and Roger Levy. 2023. LINC: A neurosymbolic approach for logical reasoning by combining language models with firstorder logic provers. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5153-5176, Singapore. Association for Computational Linguistics.</p>
<p>OpenAI. 2024. Gpt-4o mini: Advancing cost-efficient intelligence. Accessed: 2025-02-15.</p>
<p>OpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Mądry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette,</p>
<p>Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian O’Connell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg</p>
<p>Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Taran Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunningham, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. 2024a. Gpt-4o system card.</p>
<p>OpenAI, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pa-
chocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Kimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. 2024b. Openai ol system card.</p>
<p>Liangming Pan, Alon Albalak, Xinyi Wang, and William Wang. 2023. Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3806-3824, Singapore. Association for Computational Linguistics.</p>
<p>William C Purdy. 1991. A logic for natural language. Notre Dame Journal of Formal Logic, 32(3):409-425.</p>
<p>Hrituraj Singh, Milan Aggarwal, and Balaji Krishnamurthy. 2020. Exploring neural models for parsing natural language into first-order logic. ArXiv, abs/2002.06544.</p>
<p>Zhivar Sourati, Vishnu Priya Prasanna Venkatesh, Darshan Deshpande, Himanshu Rawlani, Filip Ilievski, Hồng-Ân Sandlin, and Alain Mermoud. 2022. Robust and explainable identification of logical fallacies in nat-</p>
<p>ural language arguments. Knowledge Based Systems, 266:110418.</p>
<p>Christian Stab and Iryna Gurevych. 2017. Recognizing insufficiently supported arguments in argumentative essays. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 980-990, Valencia, Spain. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoging Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, et al. 2022. Chain of thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Yuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi, and Faramarz Fekri. 2024. Harnessing the power of large language models for natural language to firstorder logic translation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6942-6959, Bangkok, Thailand. Association for Computational Linguistics.</p>
<h2>Appendix</h2>
<h2>A Algorithms</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="n">Compiling</span><span class="w"> </span><span class="n">Logical</span><span class="w"> </span><span class="n">Formula</span><span class="w"> </span><span class="n">to</span>
<span class="n">SMT</span>
<span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="n">Logical</span><span class="w"> </span><span class="n">formula</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">L</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">natural</span><span class="w"> </span><span class="n">language</span><span class="w"> </span><span class="ow">or</span>
<span class="w">    </span><span class="n">First</span><span class="o">-</span><span class="n">Order</span><span class="w"> </span><span class="n">Logic</span><span class="w"> </span><span class="p">(</span><span class="n">FOL</span><span class="p">)</span>
<span class="n">Output</span><span class="p">:</span><span class="w"> </span><span class="n">SMT</span><span class="w"> </span><span class="n">file</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">formatted</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">formal</span><span class="w"> </span><span class="n">solvers</span>
<span class="mi">1</span><span class="w"> </span><span class="n">Step</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="n">Tokenize</span><span class="w"> </span><span class="n">Formula</span>
<span class="mi">2</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">T</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Tokenize</span><span class="w"> </span>\<span class="p">((</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">L</span><span class="p">})</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span>\<span class="p">)</span><span class="w"> </span><span class="n">Split</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">L</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">tokens</span><span class="w"> </span><span class="n">based</span>
<span class="w">    </span><span class="n">on</span><span class="w"> </span><span class="n">operators</span><span class="p">,</span><span class="w"> </span><span class="n">parentheses</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">commas</span>
<span class="mi">3</span><span class="w"> </span><span class="n">Step</span><span class="w"> </span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="n">Process</span><span class="w"> </span><span class="n">Tokens</span>
<span class="mi">4</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">emptyset</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span>\<span class="p">)</span><span class="w"> </span><span class="n">Initialize</span><span class="w"> </span><span class="n">processed</span><span class="w"> </span><span class="n">tokens</span><span class="w"> </span><span class="n">set</span>
<span class="mi">5</span><span class="w"> </span><span class="n">foreach</span><span class="w"> </span><span class="n">token</span><span class="w"> </span>\<span class="p">(</span><span class="n">t</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">T</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="mi">6</span><span class="w"> </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">t</span>\<span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">predicate</span><span class="w"> </span><span class="n">then</span>
<span class="mi">7</span>
<span class="w">    </span><span class="n">Identify</span><span class="w"> </span><span class="n">arguments</span><span class="w"> </span><span class="n">of</span><span class="w"> </span>\<span class="p">(</span><span class="n">t</span>\<span class="p">)</span>
<span class="w">    </span><span class="mi">8</span>
<span class="w">    </span><span class="n">Recursively</span><span class="w"> </span><span class="n">ProcessTokens</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">arguments</span>
<span class="w">    </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span><span class="n">t</span>\<span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">operator</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">variable</span><span class="w"> </span><span class="n">then</span>
<span class="w">    </span><span class="mi">10</span>
<span class="w">        </span><span class="n">Add</span><span class="w"> </span>\<span class="p">(</span><span class="n">t</span>\<span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="p">}</span>\<span class="p">)</span>
<span class="mi">11</span><span class="w"> </span><span class="n">Step</span><span class="w"> </span><span class="mi">3</span><span class="p">:</span><span class="w"> </span><span class="n">Convert</span><span class="w"> </span><span class="n">Formula</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">Prefix</span><span class="w"> </span><span class="n">Notation</span>
<span class="mi">12</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">F</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">prefix</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">InfixToPrefix</span><span class="p">(</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="p">})</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span>\<span class="p">)</span><span class="w"> </span><span class="nb nb-Type">Transform</span><span class="w"> </span><span class="n">logical</span>
<span class="w">    </span><span class="n">formula</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">infix</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">prefix</span><span class="w"> </span><span class="n">notation</span>
<span class="mi">13</span><span class="w"> </span><span class="n">Recursively</span><span class="w"> </span><span class="n">apply</span><span class="w"> </span><span class="n">InfixToPrefix</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">predicate</span>
<span class="w">    </span><span class="n">arguments</span>
<span class="mi">14</span><span class="w"> </span><span class="n">Step</span><span class="w"> </span><span class="mi">4</span><span class="p">:</span><span class="w"> </span><span class="n">Determine</span><span class="w"> </span><span class="n">Sorts</span>
<span class="mi">15</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">sorti</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">UnifySort</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">F</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">prefix</span><span class="w"> </span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span>\<span class="p">)</span><span class="w"> </span><span class="n">Assign</span><span class="w"> </span><span class="n">sorts</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="n">variables</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">predicates</span>
<span class="mi">16</span><span class="w"> </span><span class="n">Step</span><span class="w"> </span><span class="mi">5</span><span class="p">:</span><span class="w"> </span><span class="n">Format</span><span class="w"> </span><span class="n">Formula</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">SMT</span>
<span class="mi">17</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">F</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">SMT</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Parenthesize</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">F</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">prefix</span><span class="w"> </span><span class="p">}}</span>\<span class="p">)</span><span class="w"> </span><span class="n">according</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">SMT</span><span class="o">-</span><span class="n">LIB</span>
<span class="w">    </span><span class="n">syntax</span>
<span class="mi">18</span><span class="w"> </span><span class="n">Step</span><span class="w"> </span><span class="mi">6</span><span class="p">:</span><span class="w"> </span><span class="n">Generate</span><span class="w"> </span><span class="n">SMT</span><span class="w"> </span><span class="n">File</span>
<span class="mi">19</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">GenerateSMT</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">sorti</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">F</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">SMT</span><span class="w"> </span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="mi">20</span><span class="w"> </span><span class="n">Include</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="n">declare</span><span class="o">-</span><span class="n">sort</span><span class="p">)</span><span class="w"> </span><span class="n">statements</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">sorts</span><span class="o">.</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="n">declare</span><span class="o">-</span><span class="n">fun</span><span class="p">)</span><span class="w"> </span><span class="n">statements</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">variables</span><span class="w"> </span><span class="ow">and</span>
<span class="w">        </span><span class="n">predicates</span><span class="o">.</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">Negation</span><span class="w"> </span><span class="n">of</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">F</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">SMT</span><span class="w"> </span><span class="p">}}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="n">check</span><span class="o">-</span><span class="n">sat</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="p">(</span><span class="n">get</span><span class="o">-</span><span class="n">model</span><span class="p">)</span><span class="w"> </span><span class="n">commands</span><span class="o">.</span>
<span class="k">return</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span>\<span class="p">)</span><span class="w"> </span><span class="n">Return</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">SMT</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="ow">in</span>
<span class="w">    </span><span class="n">formal</span><span class="w"> </span><span class="n">solvers</span>
</code></pre></div>

<h2>B Prompt Examples</h2>
<p>Note: Additional in-context examples were removed for brevity and denoted ' $[\ldots]$ ' in the following prompts.</p>
<h2>B. 1 End-to-end LLM Prompts</h2>
<h2>Prompt 1: Classifying with in-context ex-</h2>
<p>amples (Few-shot)</p>
<p>Logical fallacies are common errors in reasoning that undermine the logic of an argument.</p>
<p>A sentence is logically valid if and only if it is not possible for it to be false.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Algorithm</span><span class="w"> </span><span class="mi">2</span>:<span class="w"> </span><span class="nv">UnifySort</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">Predicate</span><span class="w"> </span>\<span class="ss">(</span><span class="nv">A</span><span class="ss">(</span><span class="nv">x</span>,<span class="w"> </span><span class="nv">y</span><span class="ss">)</span>\<span class="ss">)</span>
</code></pre></div>

<p>Input: Predicate $A(x, y)$ with arguments and potential instances
Output: Unified sort for predicate $A$ or an error if sorts are incompatible
1 Step 1: Declare the Current Sort Initialize the current sort of $A$ : (NULL, NULL, Bool)
2 Step 2: Process Each Instance of Predicate $A$ foreach instance of predicate $A$ do</p>
<p>Step 2.1: Determine Instance Sorts foreach argument $x_{i}$ in the instance do
if $x_{i}$ is a formula then
Set $\operatorname{sort}\left(x_{i}\right)=$ Bool
else if $x_{i}$ is a variable then
Set $\operatorname{sort}\left(x_{i}\right)=$ sort(variable) // May be NULL</p>
<p>Step 2.2: Unify Current Sort with Instance Sort
foreach statement sort in current and instance sorts do
if sorts are not NULL and different then
Raise an error: Incompatible sorts
else if current sort is NULL and instance sort is not NULL then
Update current sort:
current_sort $\leftarrow$ instance_sort
else if instance sort is NULL and current sort is not NULL then
Update variable sort to match current sort</p>
<p>Here are some examples of classifying sentences as logical fallacies or valid sentences:</p>
<p>Example 1:
Input: "I met a tall man who loved to eat cheese, now I believe all tall people like cheese"
Answer: Logical Fallacy
$[\ldots]$</p>
<p>Now, classify the following sentence. Answer with either "Logical Fallacy" or "Valid" at the start of your answer.</p>
<p>Input:</p>
<h2>B. 2 Intermediate NL2FOL Prompts</h2>
<h2>Prompt 2. Extracting claim and implication</h2>
<p>Here are some examples of extracting claims and implications from an input paragraph. There can be multiple claims but only one implication.</p>
<p>Input: "I met a tall man who loved to eat cheese, now I believe all tall people like cheese."
Output:
Claim: "A tall man loves cheese."
Implication: "All tall people like cheese."
[...]
Do not use any subordinating conjunctions in the implication. Replace pronouns with the appropriate nouns so that there are no pronouns. Now extract the claim and implication for the following input.</p>
<p>Input:</p>
<h2>Prompt 3. Getting referring expressions</h2>
<p>You are given a sentence. Referring expressions are noun phrases, pronouns, and proper names that refer to some individual objects that have some properties associated with them. Here are some examples of finding referring expressions in a sentence:</p>
<p>Input: "A tall man loved cheese"
Referring expressions: A tall man
[...]
Now, find the referring expressions for the following input:</p>
<h2>Prompt 4. Getting entity relations</h2>
<p>Please determine the relationship between the two entities provided below. Choose the number corresponding to the statement that best describes their relationship:</p>
<ol>
<li>"[Entity A]" is equal to "[Entity B]".</li>
<li>"[Entity A]" is a subset of "[Entity B]".</li>
<li>"[Entity B]" is a subset of "[Entity A]".</li>
<li>"[Entity A]" is not related to "[Entity B]".</li>
</ol>
<p>Instructions:</p>
<ul>
<li>Equality check: If the two entities are equal (case-insensitive after stripping whitespace), select statement 1.</li>
<li>Subset determination: If they are not equal, assess whether one entity is a subset of the other based on general knowledge and logical reasoning.</li>
<li>If "[Entity A]" is a subset of "[Entity B]", select statement 2.</li>
<li>If "[Entity B]" is a subset of "[Entity A]", select statement 3.</li>
<li>Unrelated entities: If none of the above statements accurately describes the relationship.</li>
</ul>
<p>Here are some examples:</p>
<p>Example 1:
Entity A: "dogs"
Entity B: "animals"
Analysis: All dogs are animals, so "dogs" is a subset of "animals".
Answer: 2
$[\ldots]$</p>
<p>Entities:</p>
<ul>
<li>Entity A:</li>
<li>Entity B:</li>
</ul>
<p>Your Task:</p>
<ul>
<li>Analyze the relationship between "Entity A" and "Entity B" based on the instructions. - Provide only the number (1, 2, 3, or 4) that corresponds to the statement you have selected.</li>
</ul>
<h2>Prompt 5. Getting properties (claim)</h2>
<p>Given a sentence, and the referring expressions of that sentence. Properties are anything that describes a relationship between two referring expressions, or they may describe a trait of a referring</p>
<p>expression. These properties are essentially predicates in first-order logic.</p>
<p>Here are some examples of finding properties in a sentence:</p>
<p>Example 1:</p>
<p>Input sentence: A tall man loves cheese
Referring expressions: tall man: a, cheese: b
Properties: IsTall(x), LovesCheese(x)
[...]</p>
<p>Now extract the properties for the following input:</p>
<h2>Prompt 6: Getting property relations</h2>
<p>You are given two logical clauses. Your task is to identify whether or not the first clause entails the second clause, taking into account external knowledge or 'common sense'. Also, take into account the context from the input sentence.</p>
<p>Here are some examples:</p>
<p>Example 1:
Input sentence: A boy is jumping on skateboard in the middle of a red bridge. Thus, the boy does a skateboarding trick.
Clause 1: JumpsOn(boy,skateboard)
Clause 2: Does(boy, skateboarding_trick)
Answer: ENTAILMENT
[...]
Now given the following clauses. identify whether the first clause entails the second clause.</p>
<h2>Prompt 7: Retrieving FOL expression</h2>
<p>Given a sentence, the referring expressions of that sentence, and properties which are associated with the referring expressions. Use the given properties to convert the
sentence into a first-order logical form. Use -&gt; to represent implies, \&amp; to represent and, I to represent or and to represent negations.</p>
<p>Example 1:
Input Sentence: A tall man loves cheese
Referring Expressions: A tall man: x
Properties: IsTall(x), LovesCheese(x)
Logical Form: IsTall(x) \&amp; LovesCheese(x)
[...]
The complete set of prompt examples is available in our public code repository at https://github. com/lovishchopra/NL2FOL. We encourage readers to visit the repository for details and latest updates.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ Equal contribution
${ }^{\ddagger}$ Work done while at Stanford University
${ }^{\ddagger}$ Co-supervision
${ }^{\dagger}$ Code available at: github.com/lovishchopra/NL2FOL&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
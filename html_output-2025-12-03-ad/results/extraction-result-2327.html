<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2327 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2327</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2327</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-219558521</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2006.05422v1.pdf" target="_blank">Report from the A.I. For Nuclear Physics Workshop</a></p>
                <p><strong>Paper Abstract:</strong> This report is an outcome of the workshop"AI for Nuclear Physics"held at Thomas Jefferson National Accelerator Facility on March 4-6, 2020. The workshop brought together 184 scientists to explore opportunities for Nuclear Physics in the area of Artificial Intelligence. The workshop consisted of plenary talks, as well as six working groups. The report includes the workshop deliberations and additional contributions to describe prospects for using AI across Nuclear Physics research.</p>
                <p><strong>Cost:</strong> 0.035</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2327.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2327.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LQCD Sign-Problem ML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learning approaches to ameliorate the fermion sign problem in lattice QCD</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of neural networks and path-optimization / manifold-deformation methods (including supervised and unsupervised learning) and flow-based generative models to change integration contours or learn sampling manifolds to reduce or eliminate the fermionic sign problem in finite-density and real-time lattice field theory computations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Lattice quantum chromodynamics / finite-density quantum field theory</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Monte Carlo evaluation of the path integral is obstructed by an oscillatory sign (complex weight) leading to exponentially large variance (the sign problem), preventing practical calculation of finite-density, real-time, or light-cone observables.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>limited for full-physics large lattices (exploratory work uses toy models and small lattices); training data often produced by existing Monte Carlo/HMC methods; labeledness varies (often unsupervised).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>high-dimensional structured arrays (lattice gauge configurations), complex-valued fields on grids; highly correlated, high-dimensional.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>extremely high: high dimensionality, complex non-local correlations, exponential variance with system size; sampling/integration in complex manifold space.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established theoretical domain (lattice QCD) with mature numerical methods (HMC), but the specific finite-density problem is an active, unsolved challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high - results must be physically interpretable and preserve gauge symmetries and continuum limits; methods must be validated thoroughly.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Path-optimization neural networks / flow-based generative models / manifold-learning</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Neural networks parameterize deformations of integration contours into complexified field space (path optimization) or flow-based generative models learn bijective maps to generate gauge configurations with correct distribution; training uses samples from existing algorithms (supervised/fine-tuning) or unsupervised objectives minimizing a complex-phase variance metric; architectures may enforce symmetries (equivariance) where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>unsupervised / generative modeling / physics-informed ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and promising for this class of problem; requires domain-aware architectures (gauge equivariance) and careful validation; still exploratory for production-scale calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Early studies on toy models and small lattices show potential; current methods are not yet competitive with standard large-scale numerical approaches but could deliver transformative improvements if scaled and validated.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Very high — could enable first-principles calculations at finite density and in real time, unlocking many previously inaccessible physics regimes (nuclear matter, transport coefficients, PDFs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to Hybrid Monte Carlo (HMC); ML approaches aim to produce more decorrelated and acceptable proposals or remove oscillatory phase factors, whereas HMC suffers in the continuum/large-volume limits.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Use of toy models and small lattices for rapid iteration; incorporation of symmetry/equivariance into architectures; collaboration between lattice theorists and ML researchers; unsupervised training objectives tailored to sign cancellation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Domain-aware generative and manifold-learning ML methods can learn sampling transformations that mitigate the sign problem, but require physics constraints and careful validation to be competitive with established Monte Carlo techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2327.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LQCD Flow-based MCMC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flow-based generative models for configuration generation in lattice gauge theory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of invertible flow (normalizing flow) models and equivariant flows to sample gauge-field configurations more efficiently than Markov-chain proposals, producing decorrelated proposals for Monte Carlo.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Lattice gauge theory configuration generation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generating independent gauge-field configurations for expectation-value estimation is computationally expensive as continuum limits and large volumes increase autocorrelation times for Markov chain Monte Carlo.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Training data can be generated by existing HMC algorithms (so moderately available for small/medium lattices); full-scale labeled large-volume datasets are expensive to produce.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>high-dimensional lattice gauge fields (structured grids with local group-valued variables), highly correlated spatially.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional, non-linear manifold (Lie-group) constraints, need to respect gauge invariance; large combinatorial proposal space.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature domain for traditional MCMC; ML for configuration generation is emergent and rapidly developing.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - proposals must preserve detailed balance or be corrected and validated to ensure correct ensemble sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Equivariant normalizing flows / generative models for MCMC</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Invertible flow architectures map simple latent distributions to gauge configurations while including group equivariance to respect symmetry; models are trained to approximate target Boltzmann distribution and used to generate proposals or as importance samplers; techniques may be combined with Metropolis corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>generative modeling / unsupervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate and promising; requires architecture design enforcing gauge symmetries and mechanisms to ensure exact sampling (e.g., Metropolis correction).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Promising in prototype studies; potential to dramatically reduce autocorrelation and wall-clock cost, but production-level efficacy still under evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — if successful, would accelerate lattice computations across many observables and parameter regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers potential advantages over HMC by proposing more decorrelated samples; must be compared on acceptance rates, autocorrelation times, and exactness of sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Incorporation of physical symmetries (equivariance), hybrid schemes combining ML proposals with Metropolis acceptance, and availability of prototype datasets for training and benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Flow-based generative models with physics-aware design can produce high-quality proposals that reduce correlation times in lattice sampling, promising large computational savings if validated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2327.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Propagator Inversion ML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learning-assisted propagator inversion in lattice QCD</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using ML methods to reconstruct high-precision quark propagators from computationally-cheaper low-precision inversions, reducing cost of large matrix solves in lattice QCD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Lattice QCD linear solves (propagator computation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Computing quark propagators requires inverting very large sparse matrices for many sources and gauge configurations, which is often the dominant computational expense.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Abundant numerical data from existing inversions across many gauge configurations; labeled pairs of (low-precision, high-precision) solutions can be generated for supervised learning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Large sparse matrices and vector solutions; structured by lattice geometry and boundary conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high computational complexity due to matrix sizes; inversion cost scales poorly with lattice volume and condition number.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established numerical linear-algebra problem in a mature domain; ML techniques for acceleration are emergent.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — reconstructed propagators must respect required numerical accuracy and symmetries; uncertainties must be quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised ML denoising/regression on low-precision solves</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train regression/denoising networks (or other supervised models) to map low-precision inversion results to full-precision propagators, leveraging many example pairs; can be used as an inexpensive correction or preconditioner.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / surrogate modeling</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable where many low/high precision pairs are available; especially suited to regimes with repeatable structure across configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported as producing 'enormous savings' in compute resources in prototype reports; promising but requires careful error control.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — could reduce the dominant cost in many lattice QCD calculations and enable larger-scale studies with existing resources.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Alternative is to perform all inversions at full precision (standard), or use multi-shift/multigrid linear algebra techniques; ML complements these methods and may act as faster approximations or post-processing corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of many paired training solves, physics-aware network design, and robust uncertainty quantification to ensure scientific reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Supervised ML can learn to correct low-precision linear solves into full-precision propagators, offering large computational savings contingent on rigorous verification and uncertainty control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2327.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAN-based Fast Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GAN/ VAE generative models for fast detector and physics simulation (e.g., FAT-GAN / ETHER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of generative adversarial networks and variational autoencoders to emulate detector responses and event-level particle production, accelerating Monte Carlo simulations while attempting to preserve physics distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Event generation and detector response simulation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>High-fidelity particle and detector simulations (e.g., calorimeter showers) are computationally expensive and limit the scale of analyses; fast surrogates that preserve physics distributions are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Abundant simulated training data from physics generators (Pythia) and GEANT-style detector simulation; labeled (inputs/outputs) available for supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Event-level records (particle four-momenta), detector response images or multi-dimensional histograms; multimodal (tabular + images).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High dimensionality and multimodality; need to capture complex correlations and rare tails critical for physics analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>ML-based fast simulation is an active area with demonstrated prototypes in HEP and growing adaptation in NP.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high — surrogates must reproduce distributions and uncertainties reliably; interpretability desirable for trust.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Generative Adversarial Networks (GANs) / Feature-Augmented and Transformed GAN (FAT-GAN) / VAEs</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>GAN architectures (including FAT-GAN variants) are trained to produce event or detector-response samples conditioned on physics inputs; FAT-GAN augments/generated features and transformations to improve discriminator sensitivity; training uses adversarial loss and feature augmentation strategies to match distributions from Pythia/GEANT.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>generative modeling / supervised generative</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for simulation acceleration; needs careful training and validation to ensure fidelity across kinematic ranges and rare-event tails.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Prototypes (e.g., ETHER / FAT-GAN) can faithfully reproduce key distributions from Pythia in tests; calorimeter emulators have shown usable performance in related fields, making this approach promising for NP use cases.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — could reduce simulation wall time drastically, enabling larger studies, faster design iterations, and HPC/Exascale utilization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to parameterized fast simulations (hand-tuned), GANs can better capture complex correlations; must be benchmarked against full GEANT and analytic parametric fast-sims.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large high-quality simulated training datasets, feature engineering (FAT), robust validation metrics, and inclusion of uncertainties in generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Generative ML models can recreate detailed detector and event distributions much faster than full simulation, but require rigorous validation and uncertainty quantification before replacing physics-grade simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2327.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accelerator RL/BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement learning and Bayesian optimization for accelerator design and control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of reinforcement learning (RL), Bayesian Gaussian Processes (GP) optimization and Bayesian optimization to explore large accelerator design/control parameter spaces for tuning lattices, injector parameters, separators, and real-time control policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Particle accelerator design, tuning, and real-time control</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>High-dimensional, often noisy and non-differentiable parameter spaces for accelerator components and control systems require efficient global optimization and adaptive control to maximize performance and uptime.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Datasets from simulations and operational logs are available but can be heterogeneous; for RL, simulation environments or live-interaction data required.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional parameter vectors, time-series telemetry, and performance metrics; noisy and sometimes sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Large search spaces with non-linear, multi-objective, and possibly non-differentiable responses; real-time latency constraints for control.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Accelerator physics is mature; ML/RL applications are actively deployed and researched at several facilities but require adaptation to domain constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for safety-critical deployments — interpretability and uncertainty quantification required before autonomous control.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Reinforcement learning (policy networks), Bayesian optimization / Gaussian processes</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>RL trains policy networks (possibly with exploration-exploitation safeguards) in simulation or controlled live settings; Bayesian optimization (GP-based) is used for global black-box parameter tuning including noisy, expensive evaluations; both require careful hyperparameter tuning, domain-aware kernels, and safety constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>reinforcement learning / Bayesian optimization (supervised/black-box optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and already in use for tuning and optimization; policies must be constrained for safe live operation and tuned for problem-specific characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated to speed up optimization tasks and enable exploration of large parameter spaces; effectiveness sensitive to kernel/architecture choices and hyperparameter tuning (incorrect choices can yield poor results).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can improve accelerator performance, reduce tuning time, optimize designs, and enable near-real-time automated adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms manual tuning and naive grid searches for complex, multi-dimensional problems; needs comparison against established physics-based optimization and engineering heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-fidelity simulators for offline training, use of frameworks (CANDLE, ExaRL) for hyperparameter scans, inclusion of safety constraints and domain knowledge, and integration with facility control systems.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>RL and Bayesian optimization are well-suited to explore complex accelerator parameter spaces and can dramatically accelerate tuning and design when paired with domain-aware models and safety-conscious deployment strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2327.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accelerator Anomaly Detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learning anomaly detection and prognostics for accelerator subsystems (CEBAF and others)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of unsupervised and supervised ML (including PCA, correlation analysis, and anomaly detection algorithms) to monitor trends, detect precursors to faults, and prognosticate failures in accelerator subsystems such as SRF cavities, BPMs, cryogenics, and LLRF.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Accelerator operations and maintenance</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Reduce unscheduled beam downtime and predict subsystem faults by detecting anomalous behavior in multi-modal operational telemetry before failures occur.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large volumes of multi-sensor time-series telemetry exist (multi-modal, multi-frequency) but are noisy and heterogeneous; data often stored in facility logs and increasingly captured for ML pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time-series telemetry, multivariate sensor streams, and correlated channel measurements; high-dimensional and noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High due to multi-causality, non-stationarity, and the need to detect subtle precursors amid noise; real-time processing constraints for operational use.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Operational facilities have mature monitoring systems, but ML-based anomaly detection is an emerging operational capability with initial deployments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — operators require interpretable alerts and quantified uncertainties to act on predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Unsupervised anomaly detection, PCA, supervised binary classifiers</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Use PCA and correlation analysis for sensor-placement and dimensionality reduction; unsupervised models (clustering, autoencoders, anomaly scoring) detect outliers and precursor patterns; supervised binary classifiers trained on labeled fault/non-fault events used for prognostics where labeled failures exist.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>unsupervised learning / supervised classification (for prognostics)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and already used (e.g., CEBAF); demands integration with validation, reproducibility, and uncertainty estimation pipelines before wide operational adoption.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Deployed tools have detected precursor trends and faulty devices; however, current pipelines often lack full uncertainty quantification, V&V, and integration for operational decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Medium-to-high — can reduce downtime and maintenance costs and improve beam availability if integrated with operator workflows and validated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Offers earlier detection than threshold-based alarms or manual monitoring; must be benchmarked against rule-based systems and human-in-the-loop performance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Comprehensive data capture/streaming, labeled historical fault data, domain-aware features, uncertainty quantification, and operator-in-the-loop validation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Anomaly-detection ML applied to rich telemetry can reveal precursors to rare failures, but operational impact requires uncertainty-aware, validated models and close integration with facility workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2327.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPC/Neutrino DNNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep convolutional neural networks for tracking, event classification, and semantic segmentation in time-projection chambers and neutrino detectors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of convolutional neural networks (including U-Net architectures and sparse DNNs) for pixel-level particle identification, track segmentation, event classification, and fast reconstruction in time projection chambers and liquid-argon neutrino detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Experimental particle/nuclear detectors (TPCs, liquid-argon detectors)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Reconstruct particle tracks and classify signal/background in detectors with high channel counts and complex event topologies; reduce analysis time and improve selection efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large amounts of simulated labeled data and experimental calibration datasets exist; real experimental labels can be sparse and require careful detector-response modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-resolution 2D/3D images or sparse voxel arrays representing detector charge/time readouts; multimodal metadata (timing, energy).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional image-like data with varied topologies, overlapping tracks, and noise; real-time or high-throughput constraints for large detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Maturing rapidly — image-based ML methods are already deployed in many experiments (MicroBooNE, EXO-200, AT-TPC analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium - scientific analyses require uncertainties and bias checks; interpretability useful for trust and systematic studies.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Convolutional Neural Networks (CNNs), U-Net architectures, sparse DNNs</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>CNNs/U-Nets perform pixel-level segmentation and event-level classification; sparse DNN implementations exploit data sparsity for computational efficiency; models trained on labeled simulated and hand-classified data, with augmentation and domain-specific pre-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable; has been shown to match or exceed traditional reconstruction methods and to substantially speed up processing.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Example: Hall B drift chamber track reconstruction using a deep, fully-connected neural network achieved an equivalent accuracy to traditional methods but with ~6x speedup (reported in the workshop).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>DNNs provide comparable or superior selection/classification performance and large speedups in analysis throughput; challenges include domain shift between simulation and real data and uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — improves analysis sensitivity, reduces turnaround time, and enables more sophisticated online/trigger processing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to traditional cut-based selection and fitting algorithms, DNNs achieved similar or improved accuracy with significant speed gains; must be benchmarked for bias and systematic uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of large simulated labeled datasets, realistic detector response modeling, sparse architectures for computational efficiency, and community best practices for validation and data formats.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep image-based models are well-suited to TPC and neutrino detector data, delivering substantial speed and sensitivity improvements when paired with realistic training data and validation protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2327.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Holistic Experiment Triggers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge-deployed neural networks and intelligent trigger systems for real-time experimental data reduction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Incorporation of neural-network-based classifiers and compact ML models into FPGA/edge hardware for low-latency triggers, data compactification, and anomaly flagging in high-rate experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Real-time experimental data acquisition and trigger systems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>High-channel-count detectors produce data at rates that challenge storage and offline analysis; intelligent low-latency triggers can reduce storage needs while preserving physics-rich events.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Streaming high-rate detector data; labeled triggers may be limited for rare signals; simulated datasets used for training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-rate streaming time-series and image-like detector readouts; multimodal and sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Stringent low-latency and resource constraints on inference; must operate with partial/noisy data and maintain high classification fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging — there are proposals and prototype efforts to deploy neural networks on FPGAs and edge devices for triggers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - trigger decisions affect data acquisition and must be reliable and interpretable to some degree.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Compact CNNs / quantized neural networks deployed on FPGAs (edge ML)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Design and quantize neural network models to meet FPGA/edge latency and resource constraints; train with representative simulated and calibration data; integrate as early trigger decision layers for classification/anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / edge ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for experiments with high data rates where early event selection is possible; hardware and model robustness are critical.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Promising pathway to reduce data volumes and capture rare events otherwise discarded by conventional triggers; requires robust low-latency designs and detector-aware training.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables higher effective luminosity and more efficient data collection, especially for rare-process searches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Superior to simple hardware-threshold triggers in capturing complex signatures; must be validated against software high-level triggers and human-defined selection criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Hardware-aware model design, realistic training data including detector effects, low-bit quantization strategies, and thorough latency testing.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Moving ML inference to the detector edge (FPGAs) can enable intelligent, low-latency triggers and data reduction, but success requires co-design of models and hardware with strong validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2327.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian QCF Inference ML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learning inverse mapping for Bayesian inference of quantum correlation functions (QCFs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of mixture density networks and parameter-supervised autoencoders to perform inverse mappings from observables to high-dimensional partonic correlation functions (PDFs, GPDs, TMDs), providing fast approximate posterior inference and uncertainty-aware reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Global QCD analysis and extraction of parton distribution/quantum correlation functions</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Recover multidimensional QCFs (PDFs, GPDs, TMDs) from large, high-precision experimental datasets via an ill-posed inverse problem with potentially multiple solutions and correlated uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Substantial experimental data exists (DIS, collider) but sparsity in some kinematic regions; combination with lattice QCD pseudo-data is growing.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Multidimensional observables, tabular datasets with kinematic variables and uncertainties; high dimensional parameter/function spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Ill-posed inverse problems with multiple solutions, high dimensionality of function spaces, and entangled systematic uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Evolving — traditional global fits (NNPDF, JAM) use Monte Carlo sampling; ML-based inverse mappers are recent prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - physics interpretability, uncertainty quantification, and consistency with factorization/field-theory constraints are required.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Mixture Density Networks (MDN) and parameter-supervised autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>MDNs map observables to multi-modal probability densities over parameter/function space, allowing multiple inverse solutions; parameter-supervised autoencoders learn compact latent representations conditioned on physics parameters enabling rapid posterior sampling; prototypes trained and validated on DIS toy models and real global DIS analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised / generative inverse modeling / probabilistic ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate and demonstrated in prototypes; can accelerate mapping between data and QCFs and provide uncertainty-aware predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Prototypes were reported to map PDFs to within 1-sigma confidence levels consistent with recent global Monte Carlo fits.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Prototypes show promise for matching traditional Monte Carlo fits while being faster; extension to full 3D QCFs remains a future challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — could enable interactive global analysis, rapid assessment of experimental impact, and combined fits including lattice data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to Monte Carlo global fits (e.g., NNPDF, JAM), ML prototypes can be much faster while reproducing uncertainty bands in tests; full domain validation required.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Incorporation of physics constraints, ability to represent multimodal posterior structure (MDNs), and validated benchmarking against established global fits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Probabilistic ML inverse mappers (MDN/autoencoders) can reproduce global-fit uncertainty bands efficiently, offering a path to faster, interactive QCF inference when combined with domain constraints and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2327.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heavy-ion PCA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Principal Component Analysis (PCA) and dimensionality reduction for relativistic heavy-ion collision data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of PCA and related unsupervised methods to compress high-dimensional heavy-ion event observables, extract dominant fluctuation modes, and reveal effective degrees of freedom relevant to final-state flow harmonics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Relativistic heavy-ion collisions / quark-gluon plasma studies</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Characterize complex event-by-event fluctuations and map initial-state fluctuations to final-state collective observables (flow harmonics) in high-dimensional data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Simulated and experimental event ensembles are available; high-statistics datasets exist though detector acceptance must be factored in for real data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional event-by-event observables (flows, particle spectra), often organized as vectors or images; correlated variables across events.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional correlated variability across events; need to isolate physically meaningful modes amid fluctuations and noise.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>PCA and statistical dimensionality reduction are established tools in heavy-ion physics and are commonly used.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium - PCA components require physical interpretation to map to initial-state modes and hydrodynamic response.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Principal Component Analysis (PCA)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>PCA is applied to event-by-event observables to extract dominant orthogonal modes of fluctuation; top components are correlated with physical quantities (e.g., flow harmonics) and used to reduce dimensionality for further analysis or Bayesian inference.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>unsupervised learning / dimensionality reduction</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited and already applied successfully to compress and interpret heavy-ion event structure.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>PCA rediscovers known flow harmonics and identifies the most important fluctuation modes, aiding model calibration and parameter extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Medium — improves interpretability and reduces dimensionality for downstream Bayesian inference and model comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PCA provides a linear orthogonal basis compared to non-linear embeddings; for many flow analyses PCA suffices and is interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-statistics ensembles, careful pre-processing, and mapping of components to physical response functions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Linear dimensionality reduction via PCA effectively isolates dominant fluctuation modes that connect initial-state variability to measured flow observables, aiding model calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2327.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heavy-ion Bayesian</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian parameter estimation for extraction of QGP properties from heavy-ion collision simulations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of Bayesian inference and global parameter estimation (with emulator surrogates) to infer QGP transport coefficients and equation-of-state parameters by comparing simulation outputs to experimental observables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Relativistic heavy-ion collisions / extraction of QGP transport properties</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Constrain model parameters (e.g., shear/bulk viscosity, EoS parameters) of multi-stage collision simulations by fitting to large sets of experimental observables with quantified uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Simulation outputs can be produced in large ensembles; experimental observables with uncertainties are available from RHIC/LHC datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Tabular simulation outputs and experimental observables aggregated across multiple collision systems and centralities; uncertainty covariance matrices are involved.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High-dimensional model space with computationally expensive forward models (hydrodynamics + cascades); correlated parameters and non-linear responses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Relatively mature approach in heavy-ion community with several high-profile Bayesian studies published.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - inferred parameters must be physically interpretable and consistent with theory (lattice QCD constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Bayesian inference with emulators / Gaussian process emulators</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Build surrogate emulators (e.g., Gaussian processes) of expensive simulation outputs to enable efficient likelihood evaluations within Bayesian parameter estimation; use MCMC or nested sampling to obtain posteriors over transport coefficients and EoS parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>probabilistic modeling / Bayesian inference</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited and already widely applied; requires careful emulator validation and inclusion of systematic uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Studies report posterior constraints indicating non-zero shear and bulk viscosity consistent with lattice QCD EoS priors (qualitative in report).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Bayesian frameworks have successfully constrained QGP properties and quantified uncertainties, enabling robust model-to-data comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — provides rigorous quantitative extraction of medium properties and informs theoretical modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Superior to single-fit optimization by providing full posterior distributions and uncertainty propagation; depends on emulator fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-fidelity simulations, validated emulators, comprehensive experimental datasets, and careful uncertainty accounting (statistical + systematic).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Bayesian emulation enables tractable, uncertainty-quantified inference of QGP transport properties from expensive multi-stage simulations, converting complex data into physical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2327.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heavy-ion CNN Phase</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep convolutional neural networks to classify nuclear phase transition types in heavy-ion collisions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of deep CNNs (image-based inputs or point-cloud networks) to classify whether heavy-ion collision simulations correspond to different equation-of-state phase transition types (first-order vs crossover, spinodal vs Maxwell), achieving high classification accuracy in simulation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>QCD phase diagram / heavy-ion collisions</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Determine from complex final-state particle distributions whether the underlying EoS encoded a first-order phase transition versus a crossover, in the presence of hydrodynamic evolution and entropy production.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Simulated event ensembles produced under different EoS assumptions; experimental application requires accounting for detector acceptance and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Event-level particle distributions that can be rendered as images or point-clouds (four-momenta, PID, charge).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — information about the phase transition is partially lost during evolution; signal subtle and entangled with fluctuations and viscous effects.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging research direction with promising simulation-level results.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - interpretability and identification of discriminating features needed to translate classifier outputs to physical conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep convolutional neural networks (CNNs) and point-cloud networks</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train CNNs on image-like representations or point-cloud networks on lists of particle features to classify event ensembles by underlying EoS; apply interpretation techniques (prediction-difference analysis, attention masks) to localize important input regions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised deep learning / classification</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable in simulation studies; transfer to experimental data requires detector-aware training and systematic controls.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported classification accuracy of approximately 93% for distinguishing two classes (in one cited hydrodynamic simulation study).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Networks succeed in decoding subtle signatures encoded in final-state distributions; interpretation algorithms help identify most informative phase-space regions.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Medium-to-high — could provide new handles on locating critical behavior in the QCD phase diagram if robust to detector/systematic effects.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms simple summary-statistic based classification in simulations; must be compared with traditional statistical observables and Bayesian model-selection approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large simulated training sets under differing physics assumptions, interpretation tools to link ML features to physics, and inclusion of detector effects for real-data application.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep CNNs can detect subtle imprints of different phase-transition physics in heavy-ion final states in simulations, offering a path toward data-driven searches for critical behavior when systematically validated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2327.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hydro Emulation UNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stacked-UNet deep-learning surrogate for relativistic hydrodynamic simulations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of deep stacked U-Net convolutional architectures to predict time evolution of hydrodynamic fields (energy density, fluid velocity) orders of magnitude faster than direct PDE solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Relativistic hydrodynamics in heavy-ion collision modeling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Numerical integration of relativistic hydrodynamic PDEs for event-by-event simulations is computationally expensive, limiting ensemble sizes and speed of model-to-data inference.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Training data generated from viscous hydrodynamic solvers (2+1D) across initial conditions; simulation data abundant but costly to produce.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatio-temporal grid fields (energy density, velocities) — image-like tensors evolving in time.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: non-linear PDEs with shock-like features, dissipation, and sensitivity to initial conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emulator approaches are emergent; PDE-solving remains the standard but ML surrogates are being developed.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - surrogate must reproduce physically-constrained evolution and conserve relevant quantities or quantify deviations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Stacked U-Net deep convolutional architectures</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train U-Net style conv-nets to take current field states and predict subsequent time-evolution steps; stacking and training strategies enable stable rollouts; trained networks serve as emulators replacing expensive numerics.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised deep learning / physics surrogate modeling</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for rapid approximate evolution and large-ensemble studies; must be validated for stability and physical conservation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported speedup: trained network can solve hydrodynamic evolution ~600x faster than direct numerical PDE solver; GPU parallelization gave ~60-100x speedup compared to CPU numerics (comparison in report).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Emulator reproduces key features of hydrodynamic evolution in tests; major speed gains facilitate statistical studies though long-term stability and fidelity require careful assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables many more simulation realizations for Bayesian inference and uncertainty quantification in heavy-ion physics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Faster than numerical PDE solvers by orders of magnitude; trades off exactness for speed, necessitating careful validation and possibly hybrid ML-PDE approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality training trajectories, architecture choices that respect spatio-temporal correlations, and assessment of conservation properties and stability during rollout.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Deep convolutional surrogates (stacked U-Nets) can emulate complex hydrodynamic PDE evolution with massive speedups, enabling larger-scale inference but requiring rigorous validation to ensure physical fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e2327.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Project8 ML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learning classification and CNN-based track reconstruction in Project 8 cyclotron radiation data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of SVMs to classify track types and U-Net CNNs for track identification and reconstruction in noisy cyclotron radiation emission spectroscopy (CRES) spectrogram data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Neutrino-mass measurement via cyclotron radiation emission spectroscopy (Project 8)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Identify and group electron cyclotron-radiation tracks in noisy spectrograms to reconstruct electron events for precise endpoint spectrum measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Experimental spectrogram data with labeled track examples from collaboration studies; simulated data also available for training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time-frequency spectrogram images with multiple track topologies and noise; features include slopes and power densities.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate-high: overlapping tracks, missing carriers, and variety of track topologies in noisy backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Active experimental program; ML methods are in active development and integration into reconstruction pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — classifiers aid reconstruction but downstream physics analyses require quantified efficiencies and biases.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Support Vector Machines (SVM) for track-type classification; U-Net convolutional neural networks for track identification</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Feature extraction (slope, power density) followed by SVM classification of track types; CNN U-Net performs pixel-level segmentation of spectrograms to identify tracks and produce inputs for event reconstruction; training uses labeled examples and augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / classification and segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Directly applicable and under deployment/optimization in the Project 8 reconstruction chain.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Early studies show ML methods improve identification and aid reconstruction in multi-track events; development focuses on handling diverse topologies and quantifying detection efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Medium — improved track reconstruction will enhance event selection and reduce systematic uncertainties in neutrino-mass measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Improves over manual/heuristic track-finding methods in noisy spectrograms with multi-track events.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of labeled examples, robust augmentation, and careful evaluation of detection efficiency and biases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Combining classical classifiers for track-type labeling with CNN-based segmentation can robustly identify complex spectrogram tracks, aiding high-precision experimental reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e2327.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NEXT DNNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep neural networks for background rejection and semantic segmentation in NEXT high-pressure gaseous xenon TPCs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of deep convolutional and sparse neural networks for signal/background classification, event reconstruction, and semantic segmentation in NEXT neutrinoless double-beta decay detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Neutrinoless double-beta decay search (NEXT experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Distinguish rare signal topologies from background events and reconstruct topological features (Bragg peaks) with high efficiency and resolution in ton-scale TPC designs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Simulated ton-scale datasets and small-detector experimental data (NEXT-NEW) available; labeled simulations used for network training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional 3D sparse voxelized detector charge images with localized energy deposit topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High due to rare signal rates, complex topologies, and need for extreme background suppression with sub-percent-level systematics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>DNN-based methods are actively used and published on for NEXT; scaling to ton-scale designs is in progress using HPC resources (Summit).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - analyses require well-understood efficiencies, background rejection rates, and systematic uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep convolutional neural networks (including sparse DNNs) and semantic segmentation models</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Sparse DNNs and convolutional architectures trained on simulated detector responses to perform event classification and pixel/voxel-level segmentation of tracks; models optimized for parallel HPC execution for large-scale sensitivity studies.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised deep learning / segmentation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable and already providing benefits in prototype detectors; scaling and uncertainty quantification remain active tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Published results indicate improved background rejection and reconstruction capability; ongoing Summit-scale studies aim to optimize sensitivity for ton-scale detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — better classification and reconstruction directly improve experimental sensitivity to neutrinoless double-beta decay.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms classical cut-based topology selections by leveraging full pixel-level information and learned features.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Realistic simulation including detector effects, sparse-network efficiency for large volumes, and HPC resources for training at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Sparse and semantic-segmentation DNNs exploit the rich topological information in gaseous TPCs to enhance background rejection and reconstruction fidelity, crucial for next-generation rare-event searches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2327.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e2327.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WANDA ML for Nuclear Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI/ML-enabled pipeline improvements for nuclear data compilation, evaluation, and surrogate modeling (WANDA recommendations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Community-identified applications of AI/ML to accelerate and improve the nuclear data pipeline: trend-finding, surrogate physics emulators, Bayesian evaluations, uncertainty quantification, NLP for literature ingestion, and experimental design optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Nuclear data evaluation, compilation, and applications (reactor, security, astrophysics)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>The nuclear data pipeline requires rapid, accurate, reproducible evaluation and incorporation of experimental and model data, with comprehensive uncertainty quantification and metadata for applied use.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large but heterogeneous nuclear data from experiments, evaluations, and simulations; needs standardized, QA-vetted databases with metadata for ML use.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Tabular nuclear datasets, experimental reports (text), evaluated files, and covariance/uncertainty matrices; multimodal including textual literature.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Complex relationships between microscopic nuclear data and integral experiment responses; large experimental literature to ingest and connect.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established nuclear-data infrastructure exists, but ML integration is nascent and recommendations are to accelerate adoption.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - safety-critical applications demand quantified uncertainties, traceability, and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>NLP for literature ingestion; surrogate ML emulators; Bayesian ML for evaluation and UQ; active learning for experimental design</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Apply NLP to extract experimental results and metadata from publications; use ML emulators to reproduce expensive multi-physics transport code outputs; employ Bayesian ML to quantify intrinsic uncertainties and guide experimental design and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid: NLP + supervised/unsupervised + Bayesian ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Broadly applicable across pipeline stages; requires curated datasets and community-standard metadata to be fully effective.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Community consensus that AI/ML can accelerate evaluations, uncover missed trends, and produce fast surrogates, but practical deployment requires investment in standardized data and QA practices.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — improved evaluation speed/quality and automated literature ingestion would benefit reactor design, isotope production, national security, and astrophysics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>AI/ML would complement, not replace, expert evaluators; NLP can scale literature curation beyond manual efforts.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Development of standardized FAIR datasets with metadata, community curation, QA processes, and sharing of trained models and applicability notes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>AI/ML can transform the nuclear-data pipeline (from literature to surrogate models) but requires community-wide data standards, QA, and uncertainty-aware model integration to be trusted for safety-critical applications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Report from the A.I. For Nuclear Physics Workshop', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Flow-based generative models for Markov chain Monte Carlo in lattice field theory <em>(Rating: 2)</em></li>
                <li>Equivariant flow-based sampling for lattice gauge theory <em>(Rating: 2)</em></li>
                <li>Simulation of electron-proton scattering events by a Feature-Augmented and Transformed Generative Adversarial Network (FAT-GAN) <em>(Rating: 2)</em></li>
                <li>Machine Learning to Enable Orders of Magnitude Speedup in Multi-Objective Optimization of Particle Accelerator Systems <em>(Rating: 2)</em></li>
                <li>A convolutional neural network neutrino event classifier <em>(Rating: 2)</em></li>
                <li>An equation-of-state-meter of quantum chromodynamics transition from deep learning <em>(Rating: 2)</em></li>
                <li>Cyclotron Radiation Emission Spectroscopy Signal Classification with Machine Learning in Project 8 <em>(Rating: 2)</em></li>
                <li>Stacked-UNet applications to relativistic hydrodynamics <em>(Rating: 1)</em></li>
                <li>Bayesian extrapolation and model averaging (quantified limits of the nuclear landscape) <em>(Rating: 1)</em></li>
                <li>WANDA: AI/ML for nuclear data. summary of the session on AI/ML at the workshop on applied nuclear data activities 2020 <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2327",
    "paper_id": "paper-219558521",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "LQCD Sign-Problem ML",
            "name_full": "Machine-learning approaches to ameliorate the fermion sign problem in lattice QCD",
            "brief_description": "Application of neural networks and path-optimization / manifold-deformation methods (including supervised and unsupervised learning) and flow-based generative models to change integration contours or learn sampling manifolds to reduce or eliminate the fermionic sign problem in finite-density and real-time lattice field theory computations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Lattice quantum chromodynamics / finite-density quantum field theory",
            "problem_description": "Monte Carlo evaluation of the path integral is obstructed by an oscillatory sign (complex weight) leading to exponentially large variance (the sign problem), preventing practical calculation of finite-density, real-time, or light-cone observables.",
            "data_availability": "limited for full-physics large lattices (exploratory work uses toy models and small lattices); training data often produced by existing Monte Carlo/HMC methods; labeledness varies (often unsupervised).",
            "data_structure": "high-dimensional structured arrays (lattice gauge configurations), complex-valued fields on grids; highly correlated, high-dimensional.",
            "problem_complexity": "extremely high: high dimensionality, complex non-local correlations, exponential variance with system size; sampling/integration in complex manifold space.",
            "domain_maturity": "well-established theoretical domain (lattice QCD) with mature numerical methods (HMC), but the specific finite-density problem is an active, unsolved challenge.",
            "mechanistic_understanding_requirements": "high - results must be physically interpretable and preserve gauge symmetries and continuum limits; methods must be validated thoroughly.",
            "ai_methodology_name": "Path-optimization neural networks / flow-based generative models / manifold-learning",
            "ai_methodology_description": "Neural networks parameterize deformations of integration contours into complexified field space (path optimization) or flow-based generative models learn bijective maps to generate gauge configurations with correct distribution; training uses samples from existing algorithms (supervised/fine-tuning) or unsupervised objectives minimizing a complex-phase variance metric; architectures may enforce symmetries (equivariance) where possible.",
            "ai_methodology_category": "unsupervised / generative modeling / physics-informed ML",
            "applicability": "Applicable and promising for this class of problem; requires domain-aware architectures (gauge equivariance) and careful validation; still exploratory for production-scale calculations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Early studies on toy models and small lattices show potential; current methods are not yet competitive with standard large-scale numerical approaches but could deliver transformative improvements if scaled and validated.",
            "impact_potential": "Very high — could enable first-principles calculations at finite density and in real time, unlocking many previously inaccessible physics regimes (nuclear matter, transport coefficients, PDFs).",
            "comparison_to_alternatives": "Compared conceptually to Hybrid Monte Carlo (HMC); ML approaches aim to produce more decorrelated and acceptable proposals or remove oscillatory phase factors, whereas HMC suffers in the continuum/large-volume limits.",
            "success_factors": "Use of toy models and small lattices for rapid iteration; incorporation of symmetry/equivariance into architectures; collaboration between lattice theorists and ML researchers; unsupervised training objectives tailored to sign cancellation metrics.",
            "key_insight": "Domain-aware generative and manifold-learning ML methods can learn sampling transformations that mitigate the sign problem, but require physics constraints and careful validation to be competitive with established Monte Carlo techniques.",
            "uuid": "e2327.0",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "LQCD Flow-based MCMC",
            "name_full": "Flow-based generative models for configuration generation in lattice gauge theory",
            "brief_description": "Use of invertible flow (normalizing flow) models and equivariant flows to sample gauge-field configurations more efficiently than Markov-chain proposals, producing decorrelated proposals for Monte Carlo.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Lattice gauge theory configuration generation",
            "problem_description": "Generating independent gauge-field configurations for expectation-value estimation is computationally expensive as continuum limits and large volumes increase autocorrelation times for Markov chain Monte Carlo.",
            "data_availability": "Training data can be generated by existing HMC algorithms (so moderately available for small/medium lattices); full-scale labeled large-volume datasets are expensive to produce.",
            "data_structure": "high-dimensional lattice gauge fields (structured grids with local group-valued variables), highly correlated spatially.",
            "problem_complexity": "High-dimensional, non-linear manifold (Lie-group) constraints, need to respect gauge invariance; large combinatorial proposal space.",
            "domain_maturity": "Mature domain for traditional MCMC; ML for configuration generation is emergent and rapidly developing.",
            "mechanistic_understanding_requirements": "High - proposals must preserve detailed balance or be corrected and validated to ensure correct ensemble sampling.",
            "ai_methodology_name": "Equivariant normalizing flows / generative models for MCMC",
            "ai_methodology_description": "Invertible flow architectures map simple latent distributions to gauge configurations while including group equivariance to respect symmetry; models are trained to approximate target Boltzmann distribution and used to generate proposals or as importance samplers; techniques may be combined with Metropolis corrections.",
            "ai_methodology_category": "generative modeling / unsupervised learning",
            "applicability": "Appropriate and promising; requires architecture design enforcing gauge symmetries and mechanisms to ensure exact sampling (e.g., Metropolis correction).",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Promising in prototype studies; potential to dramatically reduce autocorrelation and wall-clock cost, but production-level efficacy still under evaluation.",
            "impact_potential": "High — if successful, would accelerate lattice computations across many observables and parameter regimes.",
            "comparison_to_alternatives": "Offers potential advantages over HMC by proposing more decorrelated samples; must be compared on acceptance rates, autocorrelation times, and exactness of sampling.",
            "success_factors": "Incorporation of physical symmetries (equivariance), hybrid schemes combining ML proposals with Metropolis acceptance, and availability of prototype datasets for training and benchmarking.",
            "key_insight": "Flow-based generative models with physics-aware design can produce high-quality proposals that reduce correlation times in lattice sampling, promising large computational savings if validated.",
            "uuid": "e2327.1",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Propagator Inversion ML",
            "name_full": "Machine-learning-assisted propagator inversion in lattice QCD",
            "brief_description": "Using ML methods to reconstruct high-precision quark propagators from computationally-cheaper low-precision inversions, reducing cost of large matrix solves in lattice QCD.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Lattice QCD linear solves (propagator computation)",
            "problem_description": "Computing quark propagators requires inverting very large sparse matrices for many sources and gauge configurations, which is often the dominant computational expense.",
            "data_availability": "Abundant numerical data from existing inversions across many gauge configurations; labeled pairs of (low-precision, high-precision) solutions can be generated for supervised learning.",
            "data_structure": "Large sparse matrices and vector solutions; structured by lattice geometry and boundary conditions.",
            "problem_complexity": "Very high computational complexity due to matrix sizes; inversion cost scales poorly with lattice volume and condition number.",
            "domain_maturity": "Established numerical linear-algebra problem in a mature domain; ML techniques for acceleration are emergent.",
            "mechanistic_understanding_requirements": "High — reconstructed propagators must respect required numerical accuracy and symmetries; uncertainties must be quantified.",
            "ai_methodology_name": "Supervised ML denoising/regression on low-precision solves",
            "ai_methodology_description": "Train regression/denoising networks (or other supervised models) to map low-precision inversion results to full-precision propagators, leveraging many example pairs; can be used as an inexpensive correction or preconditioner.",
            "ai_methodology_category": "supervised learning / surrogate modeling",
            "applicability": "Applicable where many low/high precision pairs are available; especially suited to regimes with repeatable structure across configurations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported as producing 'enormous savings' in compute resources in prototype reports; promising but requires careful error control.",
            "impact_potential": "High — could reduce the dominant cost in many lattice QCD calculations and enable larger-scale studies with existing resources.",
            "comparison_to_alternatives": "Alternative is to perform all inversions at full precision (standard), or use multi-shift/multigrid linear algebra techniques; ML complements these methods and may act as faster approximations or post-processing corrections.",
            "success_factors": "Availability of many paired training solves, physics-aware network design, and robust uncertainty quantification to ensure scientific reliability.",
            "key_insight": "Supervised ML can learn to correct low-precision linear solves into full-precision propagators, offering large computational savings contingent on rigorous verification and uncertainty control.",
            "uuid": "e2327.2",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "GAN-based Fast Simulation",
            "name_full": "GAN/ VAE generative models for fast detector and physics simulation (e.g., FAT-GAN / ETHER)",
            "brief_description": "Use of generative adversarial networks and variational autoencoders to emulate detector responses and event-level particle production, accelerating Monte Carlo simulations while attempting to preserve physics distributions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Event generation and detector response simulation",
            "problem_description": "High-fidelity particle and detector simulations (e.g., calorimeter showers) are computationally expensive and limit the scale of analyses; fast surrogates that preserve physics distributions are needed.",
            "data_availability": "Abundant simulated training data from physics generators (Pythia) and GEANT-style detector simulation; labeled (inputs/outputs) available for supervised training.",
            "data_structure": "Event-level records (particle four-momenta), detector response images or multi-dimensional histograms; multimodal (tabular + images).",
            "problem_complexity": "High dimensionality and multimodality; need to capture complex correlations and rare tails critical for physics analyses.",
            "domain_maturity": "ML-based fast simulation is an active area with demonstrated prototypes in HEP and growing adaptation in NP.",
            "mechanistic_understanding_requirements": "Medium to high — surrogates must reproduce distributions and uncertainties reliably; interpretability desirable for trust.",
            "ai_methodology_name": "Generative Adversarial Networks (GANs) / Feature-Augmented and Transformed GAN (FAT-GAN) / VAEs",
            "ai_methodology_description": "GAN architectures (including FAT-GAN variants) are trained to produce event or detector-response samples conditioned on physics inputs; FAT-GAN augments/generated features and transformations to improve discriminator sensitivity; training uses adversarial loss and feature augmentation strategies to match distributions from Pythia/GEANT.",
            "ai_methodology_category": "generative modeling / supervised generative",
            "applicability": "Highly applicable for simulation acceleration; needs careful training and validation to ensure fidelity across kinematic ranges and rare-event tails.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Prototypes (e.g., ETHER / FAT-GAN) can faithfully reproduce key distributions from Pythia in tests; calorimeter emulators have shown usable performance in related fields, making this approach promising for NP use cases.",
            "impact_potential": "High — could reduce simulation wall time drastically, enabling larger studies, faster design iterations, and HPC/Exascale utilization.",
            "comparison_to_alternatives": "Compared to parameterized fast simulations (hand-tuned), GANs can better capture complex correlations; must be benchmarked against full GEANT and analytic parametric fast-sims.",
            "success_factors": "Large high-quality simulated training datasets, feature engineering (FAT), robust validation metrics, and inclusion of uncertainties in generated outputs.",
            "key_insight": "Generative ML models can recreate detailed detector and event distributions much faster than full simulation, but require rigorous validation and uncertainty quantification before replacing physics-grade simulators.",
            "uuid": "e2327.3",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Accelerator RL/BO",
            "name_full": "Reinforcement learning and Bayesian optimization for accelerator design and control",
            "brief_description": "Application of reinforcement learning (RL), Bayesian Gaussian Processes (GP) optimization and Bayesian optimization to explore large accelerator design/control parameter spaces for tuning lattices, injector parameters, separators, and real-time control policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Particle accelerator design, tuning, and real-time control",
            "problem_description": "High-dimensional, often noisy and non-differentiable parameter spaces for accelerator components and control systems require efficient global optimization and adaptive control to maximize performance and uptime.",
            "data_availability": "Datasets from simulations and operational logs are available but can be heterogeneous; for RL, simulation environments or live-interaction data required.",
            "data_structure": "High-dimensional parameter vectors, time-series telemetry, and performance metrics; noisy and sometimes sparse.",
            "problem_complexity": "Large search spaces with non-linear, multi-objective, and possibly non-differentiable responses; real-time latency constraints for control.",
            "domain_maturity": "Accelerator physics is mature; ML/RL applications are actively deployed and researched at several facilities but require adaptation to domain constraints.",
            "mechanistic_understanding_requirements": "High for safety-critical deployments — interpretability and uncertainty quantification required before autonomous control.",
            "ai_methodology_name": "Reinforcement learning (policy networks), Bayesian optimization / Gaussian processes",
            "ai_methodology_description": "RL trains policy networks (possibly with exploration-exploitation safeguards) in simulation or controlled live settings; Bayesian optimization (GP-based) is used for global black-box parameter tuning including noisy, expensive evaluations; both require careful hyperparameter tuning, domain-aware kernels, and safety constraints.",
            "ai_methodology_category": "reinforcement learning / Bayesian optimization (supervised/black-box optimization)",
            "applicability": "Applicable and already in use for tuning and optimization; policies must be constrained for safe live operation and tuned for problem-specific characteristics.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated to speed up optimization tasks and enable exploration of large parameter spaces; effectiveness sensitive to kernel/architecture choices and hyperparameter tuning (incorrect choices can yield poor results).",
            "impact_potential": "High — can improve accelerator performance, reduce tuning time, optimize designs, and enable near-real-time automated adjustments.",
            "comparison_to_alternatives": "Outperforms manual tuning and naive grid searches for complex, multi-dimensional problems; needs comparison against established physics-based optimization and engineering heuristics.",
            "success_factors": "High-fidelity simulators for offline training, use of frameworks (CANDLE, ExaRL) for hyperparameter scans, inclusion of safety constraints and domain knowledge, and integration with facility control systems.",
            "key_insight": "RL and Bayesian optimization are well-suited to explore complex accelerator parameter spaces and can dramatically accelerate tuning and design when paired with domain-aware models and safety-conscious deployment strategies.",
            "uuid": "e2327.4",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Accelerator Anomaly Detection",
            "name_full": "Machine-learning anomaly detection and prognostics for accelerator subsystems (CEBAF and others)",
            "brief_description": "Application of unsupervised and supervised ML (including PCA, correlation analysis, and anomaly detection algorithms) to monitor trends, detect precursors to faults, and prognosticate failures in accelerator subsystems such as SRF cavities, BPMs, cryogenics, and LLRF.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Accelerator operations and maintenance",
            "problem_description": "Reduce unscheduled beam downtime and predict subsystem faults by detecting anomalous behavior in multi-modal operational telemetry before failures occur.",
            "data_availability": "Large volumes of multi-sensor time-series telemetry exist (multi-modal, multi-frequency) but are noisy and heterogeneous; data often stored in facility logs and increasingly captured for ML pipelines.",
            "data_structure": "Time-series telemetry, multivariate sensor streams, and correlated channel measurements; high-dimensional and noisy.",
            "problem_complexity": "High due to multi-causality, non-stationarity, and the need to detect subtle precursors amid noise; real-time processing constraints for operational use.",
            "domain_maturity": "Operational facilities have mature monitoring systems, but ML-based anomaly detection is an emerging operational capability with initial deployments.",
            "mechanistic_understanding_requirements": "High — operators require interpretable alerts and quantified uncertainties to act on predictions.",
            "ai_methodology_name": "Unsupervised anomaly detection, PCA, supervised binary classifiers",
            "ai_methodology_description": "Use PCA and correlation analysis for sensor-placement and dimensionality reduction; unsupervised models (clustering, autoencoders, anomaly scoring) detect outliers and precursor patterns; supervised binary classifiers trained on labeled fault/non-fault events used for prognostics where labeled failures exist.",
            "ai_methodology_category": "unsupervised learning / supervised classification (for prognostics)",
            "applicability": "Applicable and already used (e.g., CEBAF); demands integration with validation, reproducibility, and uncertainty estimation pipelines before wide operational adoption.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Deployed tools have detected precursor trends and faulty devices; however, current pipelines often lack full uncertainty quantification, V&V, and integration for operational decision-making.",
            "impact_potential": "Medium-to-high — can reduce downtime and maintenance costs and improve beam availability if integrated with operator workflows and validated.",
            "comparison_to_alternatives": "Offers earlier detection than threshold-based alarms or manual monitoring; must be benchmarked against rule-based systems and human-in-the-loop performance.",
            "success_factors": "Comprehensive data capture/streaming, labeled historical fault data, domain-aware features, uncertainty quantification, and operator-in-the-loop validation.",
            "key_insight": "Anomaly-detection ML applied to rich telemetry can reveal precursors to rare failures, but operational impact requires uncertainty-aware, validated models and close integration with facility workflows.",
            "uuid": "e2327.5",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "TPC/Neutrino DNNs",
            "name_full": "Deep convolutional neural networks for tracking, event classification, and semantic segmentation in time-projection chambers and neutrino detectors",
            "brief_description": "Use of convolutional neural networks (including U-Net architectures and sparse DNNs) for pixel-level particle identification, track segmentation, event classification, and fast reconstruction in time projection chambers and liquid-argon neutrino detectors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Experimental particle/nuclear detectors (TPCs, liquid-argon detectors)",
            "problem_description": "Reconstruct particle tracks and classify signal/background in detectors with high channel counts and complex event topologies; reduce analysis time and improve selection efficiency.",
            "data_availability": "Large amounts of simulated labeled data and experimental calibration datasets exist; real experimental labels can be sparse and require careful detector-response modeling.",
            "data_structure": "High-resolution 2D/3D images or sparse voxel arrays representing detector charge/time readouts; multimodal metadata (timing, energy).",
            "problem_complexity": "High-dimensional image-like data with varied topologies, overlapping tracks, and noise; real-time or high-throughput constraints for large detectors.",
            "domain_maturity": "Maturing rapidly — image-based ML methods are already deployed in many experiments (MicroBooNE, EXO-200, AT-TPC analyses).",
            "mechanistic_understanding_requirements": "Medium - scientific analyses require uncertainties and bias checks; interpretability useful for trust and systematic studies.",
            "ai_methodology_name": "Convolutional Neural Networks (CNNs), U-Net architectures, sparse DNNs",
            "ai_methodology_description": "CNNs/U-Nets perform pixel-level segmentation and event-level classification; sparse DNN implementations exploit data sparsity for computational efficiency; models trained on labeled simulated and hand-classified data, with augmentation and domain-specific pre-processing.",
            "ai_methodology_category": "supervised learning / deep learning",
            "applicability": "Highly applicable; has been shown to match or exceed traditional reconstruction methods and to substantially speed up processing.",
            "effectiveness_quantitative": "Example: Hall B drift chamber track reconstruction using a deep, fully-connected neural network achieved an equivalent accuracy to traditional methods but with ~6x speedup (reported in the workshop).",
            "effectiveness_qualitative": "DNNs provide comparable or superior selection/classification performance and large speedups in analysis throughput; challenges include domain shift between simulation and real data and uncertainty quantification.",
            "impact_potential": "High — improves analysis sensitivity, reduces turnaround time, and enables more sophisticated online/trigger processing.",
            "comparison_to_alternatives": "Compared to traditional cut-based selection and fitting algorithms, DNNs achieved similar or improved accuracy with significant speed gains; must be benchmarked for bias and systematic uncertainties.",
            "success_factors": "Availability of large simulated labeled datasets, realistic detector response modeling, sparse architectures for computational efficiency, and community best practices for validation and data formats.",
            "key_insight": "Deep image-based models are well-suited to TPC and neutrino detector data, delivering substantial speed and sensitivity improvements when paired with realistic training data and validation protocols.",
            "uuid": "e2327.6",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Holistic Experiment Triggers",
            "name_full": "Edge-deployed neural networks and intelligent trigger systems for real-time experimental data reduction",
            "brief_description": "Incorporation of neural-network-based classifiers and compact ML models into FPGA/edge hardware for low-latency triggers, data compactification, and anomaly flagging in high-rate experiments.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Real-time experimental data acquisition and trigger systems",
            "problem_description": "High-channel-count detectors produce data at rates that challenge storage and offline analysis; intelligent low-latency triggers can reduce storage needs while preserving physics-rich events.",
            "data_availability": "Streaming high-rate detector data; labeled triggers may be limited for rare signals; simulated datasets used for training.",
            "data_structure": "High-rate streaming time-series and image-like detector readouts; multimodal and sparse.",
            "problem_complexity": "Stringent low-latency and resource constraints on inference; must operate with partial/noisy data and maintain high classification fidelity.",
            "domain_maturity": "Emerging — there are proposals and prototype efforts to deploy neural networks on FPGAs and edge devices for triggers.",
            "mechanistic_understanding_requirements": "High - trigger decisions affect data acquisition and must be reliable and interpretable to some degree.",
            "ai_methodology_name": "Compact CNNs / quantized neural networks deployed on FPGAs (edge ML)",
            "ai_methodology_description": "Design and quantize neural network models to meet FPGA/edge latency and resource constraints; train with representative simulated and calibration data; integrate as early trigger decision layers for classification/anomaly detection.",
            "ai_methodology_category": "supervised learning / edge ML",
            "applicability": "Highly applicable for experiments with high data rates where early event selection is possible; hardware and model robustness are critical.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Promising pathway to reduce data volumes and capture rare events otherwise discarded by conventional triggers; requires robust low-latency designs and detector-aware training.",
            "impact_potential": "High — enables higher effective luminosity and more efficient data collection, especially for rare-process searches.",
            "comparison_to_alternatives": "Superior to simple hardware-threshold triggers in capturing complex signatures; must be validated against software high-level triggers and human-defined selection criteria.",
            "success_factors": "Hardware-aware model design, realistic training data including detector effects, low-bit quantization strategies, and thorough latency testing.",
            "key_insight": "Moving ML inference to the detector edge (FPGAs) can enable intelligent, low-latency triggers and data reduction, but success requires co-design of models and hardware with strong validation.",
            "uuid": "e2327.7",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Bayesian QCF Inference ML",
            "name_full": "Machine-learning inverse mapping for Bayesian inference of quantum correlation functions (QCFs)",
            "brief_description": "Use of mixture density networks and parameter-supervised autoencoders to perform inverse mappings from observables to high-dimensional partonic correlation functions (PDFs, GPDs, TMDs), providing fast approximate posterior inference and uncertainty-aware reconstructions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Global QCD analysis and extraction of parton distribution/quantum correlation functions",
            "problem_description": "Recover multidimensional QCFs (PDFs, GPDs, TMDs) from large, high-precision experimental datasets via an ill-posed inverse problem with potentially multiple solutions and correlated uncertainties.",
            "data_availability": "Substantial experimental data exists (DIS, collider) but sparsity in some kinematic regions; combination with lattice QCD pseudo-data is growing.",
            "data_structure": "Multidimensional observables, tabular datasets with kinematic variables and uncertainties; high dimensional parameter/function spaces.",
            "problem_complexity": "Ill-posed inverse problems with multiple solutions, high dimensionality of function spaces, and entangled systematic uncertainties.",
            "domain_maturity": "Evolving — traditional global fits (NNPDF, JAM) use Monte Carlo sampling; ML-based inverse mappers are recent prototypes.",
            "mechanistic_understanding_requirements": "High - physics interpretability, uncertainty quantification, and consistency with factorization/field-theory constraints are required.",
            "ai_methodology_name": "Mixture Density Networks (MDN) and parameter-supervised autoencoders",
            "ai_methodology_description": "MDNs map observables to multi-modal probability densities over parameter/function space, allowing multiple inverse solutions; parameter-supervised autoencoders learn compact latent representations conditioned on physics parameters enabling rapid posterior sampling; prototypes trained and validated on DIS toy models and real global DIS analyses.",
            "ai_methodology_category": "supervised / generative inverse modeling / probabilistic ML",
            "applicability": "Appropriate and demonstrated in prototypes; can accelerate mapping between data and QCFs and provide uncertainty-aware predictions.",
            "effectiveness_quantitative": "Prototypes were reported to map PDFs to within 1-sigma confidence levels consistent with recent global Monte Carlo fits.",
            "effectiveness_qualitative": "Prototypes show promise for matching traditional Monte Carlo fits while being faster; extension to full 3D QCFs remains a future challenge.",
            "impact_potential": "High — could enable interactive global analysis, rapid assessment of experimental impact, and combined fits including lattice data.",
            "comparison_to_alternatives": "Compared to Monte Carlo global fits (e.g., NNPDF, JAM), ML prototypes can be much faster while reproducing uncertainty bands in tests; full domain validation required.",
            "success_factors": "Incorporation of physics constraints, ability to represent multimodal posterior structure (MDNs), and validated benchmarking against established global fits.",
            "key_insight": "Probabilistic ML inverse mappers (MDN/autoencoders) can reproduce global-fit uncertainty bands efficiently, offering a path to faster, interactive QCF inference when combined with domain constraints and validation.",
            "uuid": "e2327.8",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Heavy-ion PCA",
            "name_full": "Principal Component Analysis (PCA) and dimensionality reduction for relativistic heavy-ion collision data",
            "brief_description": "Use of PCA and related unsupervised methods to compress high-dimensional heavy-ion event observables, extract dominant fluctuation modes, and reveal effective degrees of freedom relevant to final-state flow harmonics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Relativistic heavy-ion collisions / quark-gluon plasma studies",
            "problem_description": "Characterize complex event-by-event fluctuations and map initial-state fluctuations to final-state collective observables (flow harmonics) in high-dimensional data.",
            "data_availability": "Simulated and experimental event ensembles are available; high-statistics datasets exist though detector acceptance must be factored in for real data.",
            "data_structure": "High-dimensional event-by-event observables (flows, particle spectra), often organized as vectors or images; correlated variables across events.",
            "problem_complexity": "High-dimensional correlated variability across events; need to isolate physically meaningful modes amid fluctuations and noise.",
            "domain_maturity": "PCA and statistical dimensionality reduction are established tools in heavy-ion physics and are commonly used.",
            "mechanistic_understanding_requirements": "Medium - PCA components require physical interpretation to map to initial-state modes and hydrodynamic response.",
            "ai_methodology_name": "Principal Component Analysis (PCA)",
            "ai_methodology_description": "PCA is applied to event-by-event observables to extract dominant orthogonal modes of fluctuation; top components are correlated with physical quantities (e.g., flow harmonics) and used to reduce dimensionality for further analysis or Bayesian inference.",
            "ai_methodology_category": "unsupervised learning / dimensionality reduction",
            "applicability": "Well-suited and already applied successfully to compress and interpret heavy-ion event structure.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "PCA rediscovers known flow harmonics and identifies the most important fluctuation modes, aiding model calibration and parameter extraction.",
            "impact_potential": "Medium — improves interpretability and reduces dimensionality for downstream Bayesian inference and model comparison.",
            "comparison_to_alternatives": "PCA provides a linear orthogonal basis compared to non-linear embeddings; for many flow analyses PCA suffices and is interpretable.",
            "success_factors": "High-statistics ensembles, careful pre-processing, and mapping of components to physical response functions.",
            "key_insight": "Linear dimensionality reduction via PCA effectively isolates dominant fluctuation modes that connect initial-state variability to measured flow observables, aiding model calibration.",
            "uuid": "e2327.9",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Heavy-ion Bayesian",
            "name_full": "Bayesian parameter estimation for extraction of QGP properties from heavy-ion collision simulations",
            "brief_description": "Use of Bayesian inference and global parameter estimation (with emulator surrogates) to infer QGP transport coefficients and equation-of-state parameters by comparing simulation outputs to experimental observables.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Relativistic heavy-ion collisions / extraction of QGP transport properties",
            "problem_description": "Constrain model parameters (e.g., shear/bulk viscosity, EoS parameters) of multi-stage collision simulations by fitting to large sets of experimental observables with quantified uncertainties.",
            "data_availability": "Simulation outputs can be produced in large ensembles; experimental observables with uncertainties are available from RHIC/LHC datasets.",
            "data_structure": "Tabular simulation outputs and experimental observables aggregated across multiple collision systems and centralities; uncertainty covariance matrices are involved.",
            "problem_complexity": "High-dimensional model space with computationally expensive forward models (hydrodynamics + cascades); correlated parameters and non-linear responses.",
            "domain_maturity": "Relatively mature approach in heavy-ion community with several high-profile Bayesian studies published.",
            "mechanistic_understanding_requirements": "High - inferred parameters must be physically interpretable and consistent with theory (lattice QCD constraints).",
            "ai_methodology_name": "Bayesian inference with emulators / Gaussian process emulators",
            "ai_methodology_description": "Build surrogate emulators (e.g., Gaussian processes) of expensive simulation outputs to enable efficient likelihood evaluations within Bayesian parameter estimation; use MCMC or nested sampling to obtain posteriors over transport coefficients and EoS parameters.",
            "ai_methodology_category": "probabilistic modeling / Bayesian inference",
            "applicability": "Well-suited and already widely applied; requires careful emulator validation and inclusion of systematic uncertainties.",
            "effectiveness_quantitative": "Studies report posterior constraints indicating non-zero shear and bulk viscosity consistent with lattice QCD EoS priors (qualitative in report).",
            "effectiveness_qualitative": "Bayesian frameworks have successfully constrained QGP properties and quantified uncertainties, enabling robust model-to-data comparisons.",
            "impact_potential": "High — provides rigorous quantitative extraction of medium properties and informs theoretical modeling.",
            "comparison_to_alternatives": "Superior to single-fit optimization by providing full posterior distributions and uncertainty propagation; depends on emulator fidelity.",
            "success_factors": "High-fidelity simulations, validated emulators, comprehensive experimental datasets, and careful uncertainty accounting (statistical + systematic).",
            "key_insight": "Bayesian emulation enables tractable, uncertainty-quantified inference of QGP transport properties from expensive multi-stage simulations, converting complex data into physical constraints.",
            "uuid": "e2327.10",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Heavy-ion CNN Phase",
            "name_full": "Deep convolutional neural networks to classify nuclear phase transition types in heavy-ion collisions",
            "brief_description": "Application of deep CNNs (image-based inputs or point-cloud networks) to classify whether heavy-ion collision simulations correspond to different equation-of-state phase transition types (first-order vs crossover, spinodal vs Maxwell), achieving high classification accuracy in simulation studies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "QCD phase diagram / heavy-ion collisions",
            "problem_description": "Determine from complex final-state particle distributions whether the underlying EoS encoded a first-order phase transition versus a crossover, in the presence of hydrodynamic evolution and entropy production.",
            "data_availability": "Simulated event ensembles produced under different EoS assumptions; experimental application requires accounting for detector acceptance and efficiency.",
            "data_structure": "Event-level particle distributions that can be rendered as images or point-clouds (four-momenta, PID, charge).",
            "problem_complexity": "High — information about the phase transition is partially lost during evolution; signal subtle and entangled with fluctuations and viscous effects.",
            "domain_maturity": "Emerging research direction with promising simulation-level results.",
            "mechanistic_understanding_requirements": "High - interpretability and identification of discriminating features needed to translate classifier outputs to physical conclusions.",
            "ai_methodology_name": "Deep convolutional neural networks (CNNs) and point-cloud networks",
            "ai_methodology_description": "Train CNNs on image-like representations or point-cloud networks on lists of particle features to classify event ensembles by underlying EoS; apply interpretation techniques (prediction-difference analysis, attention masks) to localize important input regions.",
            "ai_methodology_category": "supervised deep learning / classification",
            "applicability": "Applicable in simulation studies; transfer to experimental data requires detector-aware training and systematic controls.",
            "effectiveness_quantitative": "Reported classification accuracy of approximately 93% for distinguishing two classes (in one cited hydrodynamic simulation study).",
            "effectiveness_qualitative": "Networks succeed in decoding subtle signatures encoded in final-state distributions; interpretation algorithms help identify most informative phase-space regions.",
            "impact_potential": "Medium-to-high — could provide new handles on locating critical behavior in the QCD phase diagram if robust to detector/systematic effects.",
            "comparison_to_alternatives": "Outperforms simple summary-statistic based classification in simulations; must be compared with traditional statistical observables and Bayesian model-selection approaches.",
            "success_factors": "Large simulated training sets under differing physics assumptions, interpretation tools to link ML features to physics, and inclusion of detector effects for real-data application.",
            "key_insight": "Deep CNNs can detect subtle imprints of different phase-transition physics in heavy-ion final states in simulations, offering a path toward data-driven searches for critical behavior when systematically validated.",
            "uuid": "e2327.11",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Hydro Emulation UNet",
            "name_full": "Stacked-UNet deep-learning surrogate for relativistic hydrodynamic simulations",
            "brief_description": "Use of deep stacked U-Net convolutional architectures to predict time evolution of hydrodynamic fields (energy density, fluid velocity) orders of magnitude faster than direct PDE solvers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Relativistic hydrodynamics in heavy-ion collision modeling",
            "problem_description": "Numerical integration of relativistic hydrodynamic PDEs for event-by-event simulations is computationally expensive, limiting ensemble sizes and speed of model-to-data inference.",
            "data_availability": "Training data generated from viscous hydrodynamic solvers (2+1D) across initial conditions; simulation data abundant but costly to produce.",
            "data_structure": "Spatio-temporal grid fields (energy density, velocities) — image-like tensors evolving in time.",
            "problem_complexity": "High: non-linear PDEs with shock-like features, dissipation, and sensitivity to initial conditions.",
            "domain_maturity": "Emulator approaches are emergent; PDE-solving remains the standard but ML surrogates are being developed.",
            "mechanistic_understanding_requirements": "High - surrogate must reproduce physically-constrained evolution and conserve relevant quantities or quantify deviations.",
            "ai_methodology_name": "Stacked U-Net deep convolutional architectures",
            "ai_methodology_description": "Train U-Net style conv-nets to take current field states and predict subsequent time-evolution steps; stacking and training strategies enable stable rollouts; trained networks serve as emulators replacing expensive numerics.",
            "ai_methodology_category": "supervised deep learning / physics surrogate modeling",
            "applicability": "Applicable for rapid approximate evolution and large-ensemble studies; must be validated for stability and physical conservation.",
            "effectiveness_quantitative": "Reported speedup: trained network can solve hydrodynamic evolution ~600x faster than direct numerical PDE solver; GPU parallelization gave ~60-100x speedup compared to CPU numerics (comparison in report).",
            "effectiveness_qualitative": "Emulator reproduces key features of hydrodynamic evolution in tests; major speed gains facilitate statistical studies though long-term stability and fidelity require careful assessment.",
            "impact_potential": "High — enables many more simulation realizations for Bayesian inference and uncertainty quantification in heavy-ion physics.",
            "comparison_to_alternatives": "Faster than numerical PDE solvers by orders of magnitude; trades off exactness for speed, necessitating careful validation and possibly hybrid ML-PDE approaches.",
            "success_factors": "High-quality training trajectories, architecture choices that respect spatio-temporal correlations, and assessment of conservation properties and stability during rollout.",
            "key_insight": "Deep convolutional surrogates (stacked U-Nets) can emulate complex hydrodynamic PDE evolution with massive speedups, enabling larger-scale inference but requiring rigorous validation to ensure physical fidelity.",
            "uuid": "e2327.12",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Project8 ML",
            "name_full": "Machine-learning classification and CNN-based track reconstruction in Project 8 cyclotron radiation data",
            "brief_description": "Use of SVMs to classify track types and U-Net CNNs for track identification and reconstruction in noisy cyclotron radiation emission spectroscopy (CRES) spectrogram data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Neutrino-mass measurement via cyclotron radiation emission spectroscopy (Project 8)",
            "problem_description": "Identify and group electron cyclotron-radiation tracks in noisy spectrograms to reconstruct electron events for precise endpoint spectrum measurement.",
            "data_availability": "Experimental spectrogram data with labeled track examples from collaboration studies; simulated data also available for training.",
            "data_structure": "Time-frequency spectrogram images with multiple track topologies and noise; features include slopes and power densities.",
            "problem_complexity": "Moderate-high: overlapping tracks, missing carriers, and variety of track topologies in noisy backgrounds.",
            "domain_maturity": "Active experimental program; ML methods are in active development and integration into reconstruction pipelines.",
            "mechanistic_understanding_requirements": "Medium — classifiers aid reconstruction but downstream physics analyses require quantified efficiencies and biases.",
            "ai_methodology_name": "Support Vector Machines (SVM) for track-type classification; U-Net convolutional neural networks for track identification",
            "ai_methodology_description": "Feature extraction (slope, power density) followed by SVM classification of track types; CNN U-Net performs pixel-level segmentation of spectrograms to identify tracks and produce inputs for event reconstruction; training uses labeled examples and augmentation.",
            "ai_methodology_category": "supervised learning / classification and segmentation",
            "applicability": "Directly applicable and under deployment/optimization in the Project 8 reconstruction chain.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Early studies show ML methods improve identification and aid reconstruction in multi-track events; development focuses on handling diverse topologies and quantifying detection efficiency.",
            "impact_potential": "Medium — improved track reconstruction will enhance event selection and reduce systematic uncertainties in neutrino-mass measurement.",
            "comparison_to_alternatives": "Improves over manual/heuristic track-finding methods in noisy spectrograms with multi-track events.",
            "success_factors": "Availability of labeled examples, robust augmentation, and careful evaluation of detection efficiency and biases.",
            "key_insight": "Combining classical classifiers for track-type labeling with CNN-based segmentation can robustly identify complex spectrogram tracks, aiding high-precision experimental reconstructions.",
            "uuid": "e2327.13",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "NEXT DNNs",
            "name_full": "Deep neural networks for background rejection and semantic segmentation in NEXT high-pressure gaseous xenon TPCs",
            "brief_description": "Application of deep convolutional and sparse neural networks for signal/background classification, event reconstruction, and semantic segmentation in NEXT neutrinoless double-beta decay detectors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Neutrinoless double-beta decay search (NEXT experiment)",
            "problem_description": "Distinguish rare signal topologies from background events and reconstruct topological features (Bragg peaks) with high efficiency and resolution in ton-scale TPC designs.",
            "data_availability": "Simulated ton-scale datasets and small-detector experimental data (NEXT-NEW) available; labeled simulations used for network training.",
            "data_structure": "High-dimensional 3D sparse voxelized detector charge images with localized energy deposit topologies.",
            "problem_complexity": "High due to rare signal rates, complex topologies, and need for extreme background suppression with sub-percent-level systematics.",
            "domain_maturity": "DNN-based methods are actively used and published on for NEXT; scaling to ton-scale designs is in progress using HPC resources (Summit).",
            "mechanistic_understanding_requirements": "High - analyses require well-understood efficiencies, background rejection rates, and systematic uncertainties.",
            "ai_methodology_name": "Deep convolutional neural networks (including sparse DNNs) and semantic segmentation models",
            "ai_methodology_description": "Sparse DNNs and convolutional architectures trained on simulated detector responses to perform event classification and pixel/voxel-level segmentation of tracks; models optimized for parallel HPC execution for large-scale sensitivity studies.",
            "ai_methodology_category": "supervised deep learning / segmentation",
            "applicability": "Highly applicable and already providing benefits in prototype detectors; scaling and uncertainty quantification remain active tasks.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Published results indicate improved background rejection and reconstruction capability; ongoing Summit-scale studies aim to optimize sensitivity for ton-scale detectors.",
            "impact_potential": "High — better classification and reconstruction directly improve experimental sensitivity to neutrinoless double-beta decay.",
            "comparison_to_alternatives": "Outperforms classical cut-based topology selections by leveraging full pixel-level information and learned features.",
            "success_factors": "Realistic simulation including detector effects, sparse-network efficiency for large volumes, and HPC resources for training at scale.",
            "key_insight": "Sparse and semantic-segmentation DNNs exploit the rich topological information in gaseous TPCs to enhance background rejection and reconstruction fidelity, crucial for next-generation rare-event searches.",
            "uuid": "e2327.14",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "WANDA ML for Nuclear Data",
            "name_full": "AI/ML-enabled pipeline improvements for nuclear data compilation, evaluation, and surrogate modeling (WANDA recommendations)",
            "brief_description": "Community-identified applications of AI/ML to accelerate and improve the nuclear data pipeline: trend-finding, surrogate physics emulators, Bayesian evaluations, uncertainty quantification, NLP for literature ingestion, and experimental design optimization.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Nuclear data evaluation, compilation, and applications (reactor, security, astrophysics)",
            "problem_description": "The nuclear data pipeline requires rapid, accurate, reproducible evaluation and incorporation of experimental and model data, with comprehensive uncertainty quantification and metadata for applied use.",
            "data_availability": "Large but heterogeneous nuclear data from experiments, evaluations, and simulations; needs standardized, QA-vetted databases with metadata for ML use.",
            "data_structure": "Tabular nuclear datasets, experimental reports (text), evaluated files, and covariance/uncertainty matrices; multimodal including textual literature.",
            "problem_complexity": "Complex relationships between microscopic nuclear data and integral experiment responses; large experimental literature to ingest and connect.",
            "domain_maturity": "Established nuclear-data infrastructure exists, but ML integration is nascent and recommendations are to accelerate adoption.",
            "mechanistic_understanding_requirements": "High - safety-critical applications demand quantified uncertainties, traceability, and interpretability.",
            "ai_methodology_name": "NLP for literature ingestion; surrogate ML emulators; Bayesian ML for evaluation and UQ; active learning for experimental design",
            "ai_methodology_description": "Apply NLP to extract experimental results and metadata from publications; use ML emulators to reproduce expensive multi-physics transport code outputs; employ Bayesian ML to quantify intrinsic uncertainties and guide experimental design and validation.",
            "ai_methodology_category": "hybrid: NLP + supervised/unsupervised + Bayesian ML",
            "applicability": "Broadly applicable across pipeline stages; requires curated datasets and community-standard metadata to be fully effective.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Community consensus that AI/ML can accelerate evaluations, uncover missed trends, and produce fast surrogates, but practical deployment requires investment in standardized data and QA practices.",
            "impact_potential": "High — improved evaluation speed/quality and automated literature ingestion would benefit reactor design, isotope production, national security, and astrophysics.",
            "comparison_to_alternatives": "AI/ML would complement, not replace, expert evaluators; NLP can scale literature curation beyond manual efforts.",
            "success_factors": "Development of standardized FAIR datasets with metadata, community curation, QA processes, and sharing of trained models and applicability notes.",
            "key_insight": "AI/ML can transform the nuclear-data pipeline (from literature to surrogate models) but requires community-wide data standards, QA, and uncertainty-aware model integration to be trusted for safety-critical applications.",
            "uuid": "e2327.15",
            "source_info": {
                "paper_title": "Report from the A.I. For Nuclear Physics Workshop",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Flow-based generative models for Markov chain Monte Carlo in lattice field theory",
            "rating": 2,
            "sanitized_title": "flowbased_generative_models_for_markov_chain_monte_carlo_in_lattice_field_theory"
        },
        {
            "paper_title": "Equivariant flow-based sampling for lattice gauge theory",
            "rating": 2,
            "sanitized_title": "equivariant_flowbased_sampling_for_lattice_gauge_theory"
        },
        {
            "paper_title": "Simulation of electron-proton scattering events by a Feature-Augmented and Transformed Generative Adversarial Network (FAT-GAN)",
            "rating": 2,
            "sanitized_title": "simulation_of_electronproton_scattering_events_by_a_featureaugmented_and_transformed_generative_adversarial_network_fatgan"
        },
        {
            "paper_title": "Machine Learning to Enable Orders of Magnitude Speedup in Multi-Objective Optimization of Particle Accelerator Systems",
            "rating": 2,
            "sanitized_title": "machine_learning_to_enable_orders_of_magnitude_speedup_in_multiobjective_optimization_of_particle_accelerator_systems"
        },
        {
            "paper_title": "A convolutional neural network neutrino event classifier",
            "rating": 2,
            "sanitized_title": "a_convolutional_neural_network_neutrino_event_classifier"
        },
        {
            "paper_title": "An equation-of-state-meter of quantum chromodynamics transition from deep learning",
            "rating": 2,
            "sanitized_title": "an_equationofstatemeter_of_quantum_chromodynamics_transition_from_deep_learning"
        },
        {
            "paper_title": "Cyclotron Radiation Emission Spectroscopy Signal Classification with Machine Learning in Project 8",
            "rating": 2,
            "sanitized_title": "cyclotron_radiation_emission_spectroscopy_signal_classification_with_machine_learning_in_project_8"
        },
        {
            "paper_title": "Stacked-UNet applications to relativistic hydrodynamics",
            "rating": 1,
            "sanitized_title": "stackedunet_applications_to_relativistic_hydrodynamics"
        },
        {
            "paper_title": "Bayesian extrapolation and model averaging (quantified limits of the nuclear landscape)",
            "rating": 1,
            "sanitized_title": "bayesian_extrapolation_and_model_averaging_quantified_limits_of_the_nuclear_landscape"
        },
        {
            "paper_title": "WANDA: AI/ML for nuclear data. summary of the session on AI/ML at the workshop on applied nuclear data activities 2020",
            "rating": 2,
            "sanitized_title": "wanda_aiml_for_nuclear_data_summary_of_the_session_on_aiml_at_the_workshop_on_applied_nuclear_data_activities_2020"
        }
    ],
    "cost": 0.034949,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A.I. for Nuclear Physics Editors
March 4-6, 2020</p>
<p>Paulo Bedaque 
University of Maryland</p>
<p>Amber Boehnlein 
Thomas Jefferson National Accelerator Facility</p>
<p>Mario Cromaz 
Lawrence Berkeley National Laboratory</p>
<p>Markus Diefenthaler 
Thomas Jefferson National Accelerator Facility</p>
<p>Lat-Ifa Elouadrhiri 
Thomas Jefferson National Accelerator Facility</p>
<p>Tanja Horn 
Catholic University</p>
<p>Michelle Kuchera 
Davidson College</p>
<p>David Lawrence 
Thomas Jefferson National Accelerator Facility</p>
<p>Dean Lee 
Michigan State University</p>
<p>Steven Lidia 
Michigan State University</p>
<p>Robert Mckeown 
Thomas Jefferson National Accelerator Facility</p>
<p>Wally Melnitchouk 
Thomas Jefferson National Accelerator Facility</p>
<p>Witold Nazarewicz 
Michigan State University</p>
<p>Kostas Orginos 
Thomas Jefferson National Accelerator Facility</p>
<p>Yves Roblin 
Thomas Jefferson National Accelerator Facility</p>
<p>Michael Scott Smith 
Malachi Schram 
Xin-Nian Wang 
Lawrence Berkeley National Laboratory</p>
<p>A.I. for Nuclear Physics Editors</p>
<p>ii Group photo from the workshop AI for Nuclear Physics held at Thomas Jefferson National Accel-erator Facility on
March 4-6, 2020This report is an outcome of the workshop AI for Nuclear Physics held at Thomas Jefferson National Accelerator Facility on https://www.jlab.org/conference/AI2020 Disclaimer This report was prepared as an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any agency thereof, nor any of their em-ployees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favor-ing by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof. iii (Lead Editor), 7 College of William &amp; Mary 8 Oak Ridge National Laboratory 9 Pacific Northwest National Laboratory
5.2.1. NP Communities of Practice 30 5.2.2. Engagement with Data Science community 30 6. Engagement with ASCR 31 7. The importance of Data Management 32 8. Workforce development 32 8.1. Education 32 9. The Level of AI Literacy 33 References 34 A. Committees 41 B. Working Groups and Conveners 42Executive SummaryNuclear science is concerned with the understanding of the nature of matter, its basic constituents and their interaction to form the elements and the properties we observe. This includes the forms of matter we see around us and also exotic forms such as those that existed in the first moments after the Big Bang and that exist today inside neutron stars. The techniques, tools, and expertise needed for nuclear physics (NP) research are therefore diverse in nature. State-of-the art accelerators are being developed to illuminate the dynamical basis of the core of the atom in terms of the fundamental constituents called quarks and gluons and to increase the number of isotopes with known properties. This scientific infrastructure is reaching scales and complexities that require computational methods for tasks such as anomaly detection in operational data. New methodologies are needed to detect anomalies and to optimize operating parameters, predict failures as well as to discover new optimization algorithms. Artificial Intelligence (AI) is a rapidly developing field focused on computational technologies that can be trained, with data, to augment or automate human skill. Over the last few decades AI has become increasingly prominent in all sectors of everyday life, largely due to the adoption of statistical and probabilistic methods, the availability of large amounts of data, and increased computer processing power.The US government is initiating a broad-based, multidisciplinary, multi-agency program to build a sustained national AI ecosystem. Based upon two decades of research, development, and planning, the US government recognizes the importance of AI to advances in technology, national security and national infrastructure [1]. The national AI Initiative [2] provides a framework to establish a national strategy for US leadership in AI. Key areas of emphasis include: investments in AI research and development, unleashing AI data and resources, setting Government standards, and building the AI workforce. Several workshops and committees have identified the scientific opportunities for AI, as well as challenges from the intersection of AI with data-intensive science such as NP and high-performance computing. The present report is based on the "AI for Nuclear Physics Workshop" held in March of 2020 and outlines ongoing AI activities, possible contributions the NP community could make to identify and fill possible gaps in current AI technologies</p>
<p>This report is an outcome of the workshop AI for Nuclear Physics held at Thomas Jefferson National Accelerator Facility on March 4-6, 2020. The workshop brought together 184 scientists to explore opportunities for Nuclear Physics in the area of Artificial Intelligence. The workshop consisted of plenary talks, as well as six working groups.</p>
<p>This material is based upon work supported by the U.S. Department of Energy, Office of Science, Office of Nuclear Physics under contract DE-AC05-06OR23177. Participation of students and early career professionals was supported by NSF, Division  and needs to benefit NP research programs. The Workshop brought together the communities directly using AI technologies and provided a venue for identifying the needs and commonalities. For the purpose of this report we define Artificial Intelligence (AI) to broadly represent the next generation of methods to build models from data and to use these models alone or in conjunction with simulation and scalable computing to advance scientific research. These methods include (but are not limited to) machine learning (ML) 1 , deep learning (DL) 2 , statistical methods, data analytics, and automated control.</p>
<p>AI has tremendous potential within NP Research. It can provide new insights and discoveries from both experimental and computational data produced at user facilities. All top priorities of the 2015 Long-Range Plan on Research Opportunities and Directions [3] can benefit from AI. A common theme is to investigate and apply AI methods with well-understood uncertainty quantification, both systematic and statistical, to accelerator science, NP experimentation, and NP theory. At the same time, a number of activities and technologies in the diverse NP research portfolio has the potential to contribute to the emerging AI programs. For example, NP presents data on short time scales and with many different configurations that expose the limitations of current methods and could contribute to making AI more interpretable for the long term.</p>
<p>A general characteristic of the application of AI in NP is the identification of small changes in patterns (statistical variations) in multi-dimensional and highly-correlated data (parameters, channels). This process includes the evaluation of models where one can use AI methods to identify the most promising computational pathways where AI determined parameterizations can be used to avoid performance-limiting sections. Traditional AI tools have been applied successfully to some of these problems, in particular image classification. However, NP data are very diverse and to address the most interesting challenges more science insight has to be built into current AI technologies and AI tools have to be tuned to optimize performance in each application domain. Furthermore, NP data volume and complexity is increasing at a rapid pace. To take full advantage of AI in NP will thus require investments and changes in methodology for the provisioning of computing and handling of data. This in turn will require adequate computing resources, e.g., access to GPU computing and disk storage at appropriate scales.</p>
<p>AI has the potential to transform NP. However, to fully realize AI contributions to NP, and vice versa, close collaboration among universities, technology companies, national laboratories, and other government agencies will be essential. Such collaboration will be required to bring, for example, state-of-the art AI techniques to the NP community. Workforce development is key to increase the level of AI-literacy in NP. The challenges are similar to those outlined in the NSAC Report on 'Nuclear Physics and Quantum Information Science' [4] and include educational activities, creation of a community of AI knowledgeable researchers, and collaboration between NP and AI experts. Cross-disciplinary partnerships can help facilitate these connections. The list of Community identified Needs and Commonalities for AI Research essential for NP Applications as identified at this Workshop are presented below and also appear in more detail in Sec. 2 of the report:</p>
<p>(i) Need for workforce development: There is a need to develop and sustain an AI capable workforce within NP.</p>
<p>• Need for educational activities in AI: The goal is to retain talented students in AIrelated fields and to help them to secure employment in a wide range of careers, thus ensuring that the new techniques and concepts developed in NP laboratories are widely disseminated.</p>
<p>• Need for broader community: It is essential to have a community of researchers knowledgeable in AI technologies.</p>
<p>• Need for collaborations: Long term commitment to partnerships between NP researchers and experts in AI/ML/Data Science is crucial as it takes time-for all parties involved-to learn the language and methods.</p>
<p>(ii) Need for uncertainty quantification: The evaluation and comparison of uncertainty predictions using different modalities is required for widespread use of AI in NP.</p>
<p>(iii) Need for appropriate use of industry standard tools: significant effort is required in the careful tuning of ML tools (hyperparameter determination) to optimize performance in each application domain.</p>
<p>(iv) Need for problem-specific tools: the most interesting challenges that can be approached in NP with AI will require approaches that go beyond industry standard tools.</p>
<p>(v) Need for comprehensive data management: To maximize the usefulness of the data, it will be important to have standards on the processing of data, the application of theoretical assumptions, and the treatment of systematic uncertainties that will be used as training samples or as part of combined analysis. This meta-data will be encoded in the datasets.</p>
<p>(vi) Need for adequate computing resources: AI techniques are computationally intensive and success in using these techniques will require access to GPU computing and disk storage at appropriate scales.</p>
<p>Priority Research Directions</p>
<p>One aspect of the workshop was to explore areas where the application of AI could have a profound impact on Nuclear Physics Research. This section summarizes those research directions. Additional detail can be found in the Summary of Workshop Sessions.</p>
<p>Future prospects</p>
<p>Accelerator design and operations: Many areas of accelerator design and operations will benefit from investments in AI and ML technologies.</p>
<p>• Optimized design of accelerator systems. Development and validation of virtual diagnostics (e.g. longitudinal phase space monitors or predictors); Design and simulation of novel accelerators, and advanced engineered materials; Optimized diagnostic design and deployment; Improvement to beam sources and injector performance.</p>
<p>• Improving facility performance and user experience. Data-driven beam generation, transport, delivery optimization; Automated learning for operator support; Hardware acceleration of ML in distributed control systems; Anomaly detection and mitigation (eg. LLRF, beam diagnostics); System health monitoring (e.g., targets, cryoplant); Data driven system maintenance.</p>
<p>Holistic approach to experimentation: As a long-term vision, disparate data sources (such as accelerator parameters, experimental controls, and the detector data) would be intelligently combined and interpreted to improve experiments. Real time analysis and feedback will enable the quick diagnostics and optimization of experimental setups. ML expert systems can increase the scientific output from the beam time allocated to each experiment.</p>
<p>Experiment design not limited by computation: Future experimental advances in acceleratorbased NP research hinges on increased luminosity, which provides the statistics necessary to observe rare processes. ML methods will reduce computational barriers to reach this goal. Intelligent decisions about data reduction and storage are required to ensure the relevant physics is captured. Depending on the experiment, AI can improve the physics content of the data taken through data compactification, sophisticated triggers (both software and hardware-based), and fast-online analysis.</p>
<p>Improving simulation and analysis: Improving simulation and data analysis using ML techniques is proceeding with two general aims: (i) to use these new techniques to improve the sensitivity of current instruments and accuracy of the data, and (ii) to decrease the time simulations and analyses takes allowing for faster turnaround time to produce scientific results. Improving sensitivity allows more information to be extracted from datasets, which decreases uncertainty in results and increases discovery potential. Decreasing simulation and analysis time, saves costs and ultimately allows for a higher volume of scientific output by accelerating the feedback loop between experiment, analysis, and theory.</p>
<p>Game changer in nuclear theory: A number of case studies have been identified. They are listed in the following.</p>
<p>Sign problem in LQCD:</p>
<p>The application of Monte Carlo techniques to systems at finite density (as in nuclear matter), real-time evolution (transport coefficients) and light-cone evolution (parton distribution functions) are hindered by the fermionic sign-problem. AI methods have begun to be applied, both in supervised and unsupervised learning modes. Potentially radical advances can be expected along this direction once the full power of AI is unleashed in this problem.</p>
<p>Extraction of physical observables:</p>
<p>To extract quantities of interest from correlation functions computed in LQCD in some cases requires the solution of an ill-defined inverse problem. AI methods now being applied to tackling the relevant inverse problems are showing great promise for achieving important milestones in our understanding of hadron structure from first principles.</p>
<p>Propagator inversion in LQCD:</p>
<p>The computation of observables in LQCD requires the calculation of quark propagators in the background of a large number of gauge configurations. Mathematically this requires the inversion of a large matrix. ML methods are beginning to be used to take cheaper inversions, done with low precision, and recovering the full precision propagator, with enormous savings in computer resources.</p>
<p>Bayesian inference and global QCD analysis:</p>
<p>Recent progress in ML with deep learning is enabling the development of new tools to advance the science of femtography, which shows great promise for high-precision determination of hadronic structure combining all available experimental data. Such approaches will be necessary for determining 3D nucleon tomography.</p>
<p>Identifying rare events:</p>
<p>In the current approach to data taking and analysis, rare events, which can often represent major discoveries, can be easily overlooked when analysing data with preset ideas about what one is looking for. AI/ML can be used to generate events, with known theoretical parameters and models, and then compare the experimental stream readout with the pre-prepared theory expectations, to identify unusual or unexpected events that can be set aside for more focused study later.</p>
<p>Microscopic description of nuclear fission:</p>
<p>Various ML tools will help by dramatically speeding-up manybody simulations of nuclear fission by means of fast emulators for constrained density functional theory calculations in many-dimensional collective spaces; action minimization in the classically forbidden regions; new tools for dissipative dynamics; and computing of missing fission data.</p>
<p>Origin of elements: A quantitative understanding of astrophysical processes responsible for the existence of elements requires knowledge of nuclear properties and reaction rates of thousands of rare isotopes, many of which cannot be reached experimentally. The missing nuclear data for astrophysical network simulations must be provided by massive extrapolations based on nuclear models. For some quantities such as nuclear masses, Bayesian ML has shown promise when aiming at informed predictions including both a reduction of extrapolation errors and quantified bounds.</p>
<p>Quantified computations of heavy nuclei using realistic inter-nucleon forces:</p>
<p>Predictions for heavy and very heavy nuclei such as Pb-208 using A-body approaches based on realistic two-and threenucleon interactions with full uncertainty quantification will be enabled by Bayesian calibration using pseudo-data from microscopic calculations with supervised ML.</p>
<p>Discovering correlations and emergent phenomena:</p>
<p>Unsupervised learning can be used to discover correlations in nuclear wave functions based on microscopic Hamiltonians. There are terabytes of data from calculations with nucleonic degrees of freedom that can be data mined to discover emergent phenomena such as clustering, superfluidity, and nuclear collective modes such as rotations and vibrations.</p>
<p>Development of a spectroscopic-quality nuclear energy density functional:</p>
<p>Predictive and quantified nuclear energy density functional rooted in many-nucleon theory is needed. This development constitutes a massive inverse problem involving a variety of AI tools. The resulting spectroscopic-quality functional-crucial for understanding of rare isotopes-will properly extrapolate in mass, isospin, and angular momentum to provide predictions in the regions where data are not available.</p>
<p>Neutron star and dense matter equation of state:</p>
<p>Data from intermediate-energy heavy-ion collisions and neutronstar merger events can be explored using AI tools to deduce the nuclear matter equation of state. ML classification tools can also be used in conjunction with calculations of infinite nucleonic matter to map out the phase diagram and associated order parameters.</p>
<p>Community identified Needs and Commonalities</p>
<p>AI has tremendous potential within the context of NP Research. However, the current AI tools and methodologies have limitations that have to be addressed for the long term.</p>
<p>Need for Workforce Development: There is a need to develop and sustain an AI capable workforce within NP. This challenge is similar to the workforce development challenge for Quantum Information Sciences, outlined in 'NP and Quantum Information Science' Report.</p>
<p>Educational activities in AI: To this end, there is an urgent need to develop a range of outreach, recruitment, and educational activities. These activities will serve to raise interest in AI-related fields. The goal is to retain talented students in AI-related fields and to help them to secure employment in a wide range of careers, thus ensuring that the new techniques and concepts developed in NP laboratories are widely disseminated.</p>
<p>• University-wide AI courses: There is a need for inter-disciplinary AI courses involving Applied Mathematics, Statistics, and Computer Science experts, as well as domain scientists. • Graduate Fellowships are proven tools that enable the development of a well-educated workforce and could be used to good effect in the area of AI.</p>
<p>Need for broader community: To achieve the goals outlined by the community, it is essential to have a community of researchers knowledgeable in AI technologies.</p>
<p>• A centralized community based forum could provide a common foundation to build our technologies, allow for quick dissemination of new techniques, and provide a bridge from available AI resources to NP related applications. • Successful inter-disciplinary research require mechanisms such as the ability to create joint faculty/staff appointments. Given the wide range of use cases, such appointments would be beneficial at many institutions engaged in the NP Research Portfolio.</p>
<p>Need for collaborations: Collaboration with ML/AI/Data Science experts over a long-term is essential to successfully bring state of the art AI techniques to the NP community. Long term commitment to partnerships between NP researchers and experts in AI/ML/Data Science is crucial as it takes time -for all parties involved -to learn the language and methods.</p>
<p>Need for problem-specific tools: The current surge in AI has provided great advances in software tools and hardware that can provide the basis of ML systems used in data processing. Readily available off the shelf solutions are well suited for several types of problems, particularly image classification. However, NP applications are unique in that they are often aimed at accelerating calculation, whether in the evaluation of models where one can use AI techniques to identify the most promising calculative pathways to simulation where AI-determined parametrizations can be used to circumvent performance-limiting elements. While traditional ML tools may be applied to these problems, significant effort is required in the careful tuning of ML tools (hyperparameter determination) to optimize performance in each application domain.</p>
<p>Enabling Infrastructure for AI in NP: Taking full advantage of AI for NP will require investments and changes in methodology for the provisioning of computing and handling of data. Two particular areas concern data management and provisioning for resources. Need for standardized frameworks: The development of standardized frameworks such as ExaLearn and CANDLE have been extremely beneficial in other domains, and could provide a model for NP. It may be possible to adapt existing frameworks. Need for comprehensive data management: AI techniques are reliant on large volumes of data for training and the subsequent evaluation of models. For this reason, applications of AI are dependent on effective data management. Such data could be sourced from theoretical calculation, simulation, or experiment. Providing accessibility of the data to the wider NP community and increasing uniformity in data representation would create a connectivity across experiments that could increase collaboration and accelerate the development of AI techniques and tools. Such AI techniques could also facilitate near real-time calibration and analysis. To maximize the usefulness of the data, it will be important to have standards on the processing of data, the application of theoretical assumptions, and the treatment of systematic uncertainties that will be used as training samples or as part of combined analysis. This meta-data will be encoded in the datasets. Need for adequate computing resources AI techniques are computationally intensive and success in using these techniques will require access to GPU computing and disk storage at appropriate scales.</p>
<p>Need for uncertainty quantification: A common theme is to investigate and apply AI methods with well-understood uncertainty quantification, both systematic and statistical, to accelerator science, NP experimentation, and NP theory. The commonly used ML algorithms do not provide error estimations with model predictions, which are essential to understand outcomes. In addition, an evaluation of metrics for the evaluation and comparison of uncertainty predictions using different modalities is required for widespread use of AI in NP.</p>
<p>Workshop Overview</p>
<p>The AI for Nuclear Physics Workshop was held at Thomas Jefferson National Accelerator Facility March 4-6, 2020. The intent of the workshop was to make a broad survey of current AI projects in NP and to gather community driven input towards establishing priority research directions, areas of commonality across the NP community (and beyond), and general needs, including workforce development. The agenda focused on plenary sessions in the morning with topical working sessions in the afternoon, with most of the presentations available from the agenda. 184 people attended the workshop. The AI for Nuclear Physics Workshop Agenda focused on summaries of status of the usage of AI in Nuclear Theory, Nuclear Experiment and Accelerator Science and operations. The connection between the scientific goals outlined in the Nuclear Science Advisory Committee long range plan [3] and AI was presented by Tim Hallman, Department of Energy Associate Director for the Office of Nuclear Physics. A second focus was the connection to broader efforts within DOE, including overview talks from the DOE Artificial Intelligence Technology Office, a summary of the AI for Science Townhall process, and a summary of the NeuroData without Borders Project [5] and the Exascale Computing Project applications ExaLearn and CANDLE [6] projects.</p>
<p>An adjunct hackathon event was held on March 3, 2020. 8 teams each with four members participated. The challenge problem was drawn from a common task in NP, measuring the properties of charged particles traversing a detector. The challenge was structured as progressive, with five sub-challenges. To enable evaluation of the success of the teams, an automated scoring system and leader board was developed, with the top two scoring teams being awarded prizes. The computational approaches and tools used by the teams had significant variation, demonstrating that creativity in problem solving remains a feature of research undertaken with AI Events such as this can be useful for furthering skills in AI for participants who already have basic knowledge.</p>
<p>Summary of Workshop Sessions</p>
<p>To serve as a record of the discussions, the conveners of the working group sessions have prepared summaries based on the workshop discussions and presentations. The discussions reflect independent deliberations, and consequently some differences of opinion. A list of the working groups and conveners are listed in the Appendix B. As a note, due to conflicting workshops, some NP communities were not properly represented at this workshop. Where possible, contributions from those communities were solicited and appear at the end of this section.</p>
<p>Lattice QCD and Other Quantum Field Theories</p>
<p>Lattice field theory is a cornerstone of all subfields of NP, from nuclear structure to hadronic physics, heavy-ion collisions, and neutron stars. It is based on the Monte Carlo evaluation, in one guise or another, of the quantum path integral. Despite enormous successes achieved in the last few years, computing power currently prevent us from addressing many of the central questions of NP.</p>
<p>Lattice calculations are divided into the generation of gauge configurations, calculation of the observables of interest and data analysis. Artificial intelligence techniques have begun to be applied to all these stages as well as extending the applicability regime of lattice techniques.</p>
<p>Case Studies and Future Prospects</p>
<p>Sign problem: The application of Monte Carlo techniques to systems at finite density (as in nuclear/neutron matter), real-time evolution (transport coefficients) and light-cone evolution (parton distribution functions) are hindered by the famous sign-problem. It has been realized recently that the sign problem can be solved or ameliorated by evaluating the path integral not over real fields but over a manifold deformed into complex space instead. Up to now, the choice of manifolds has been guided by either impossibly expensive calculations or (human) insight into particular models. AI methods have begun to be applied, both in supervised and unsupervised learning modes [7][8][9][10][11]. Potentially radical advances can be expected along this direction once the full power of AI is unleashed in this problem.</p>
<p>Configuration generation: The usually most expensive part of a Monte Carlo calculation is the generation of configurations through the use of a Markov chain where, at each step, a new configuration is proposed and accepted or rejected with a probability depending on the new and old configurations. The practical feasibility of the method relies on being able to propose configurations that are significantly different from the old one while at the same time are likely to be accepted. The method used almost universally in QCD is the hybrid Monte Carlo algorithm (invented by the lattice QCD community and now widely used in all branches of science) becomes extremely expensive as the continuum limit is approached. A significant effort is being put into using different AI techniques to create algorithms to make better proposals, more decorrelated and more "acceptable", in order to speed up the process [12,13]. The training of the algorithms is accomplished either by the use of configurations generated by standard algorithms or, more ambitiously, through fully unsupervised learning. The basic ideas of such algorithms are already developed by the AI community and used for various applications in the engineering and software industry.</p>
<p>Propagator inversion: The computation of observables in lattice QCD requires the calculation of quark propagators in the background of a large number of gauge configurations. Mathematically this requires the inversion of a large matrix and, in some applications, like the extraction of nuclear forces, it can be the most expensive part of the calculation. Machine learning methods are beginning to be used to take cheaper inversions, done with low precision, and recovering the full precision propagator, with enormous savings in computer resources.</p>
<p>Observables: The extraction of physical observables from correlation functions computed in lattice QCD in some cases requires the solution of an ill-defined inverse problem. Such problems include the computation of parton distribution functions, generalized parton distribution functions, and transverse momentum dependent distribution functions, as well as the extraction of spectral densities and scattering phase shifts. These observables are the prime objective of the JLab 12GeV program where the 3D structure and spectrum of hadrons are studied, as well as the heavy-ion physics community. AI methods are now being applied to tackling the relevant inverse problems to address these physics goals showing great promise for achieving important milestones in our understanding of hadron structure from first principles.</p>
<p>Enabling Discoveries/What is Needed</p>
<p>All the work summarized in this section is exploratory. The potential is enormous although at this time, the AI techniques are not yet competitive with the standard in the field numerical investigations of quantum field theories. That said, success in a single one of the approaches has revolutionary potential in the field. The approach for AI studies is based on toy models and small lattices where novel ideas can go through the cycle of implementation/testing/improvement very quickly. This requires a model of support that favors small, flexible groups, fosters informal communication between researchers both within NP and the AI community while keeping the field attractive to young people who may have options to pursue a career in the private sector.</p>
<p>Low-Energy Nuclear Theory</p>
<p>Current Status</p>
<p>ML applications of layered feed-forward networks to modeling nuclear masses and other observables were carried out in the early 1990s [14,15]. But it is only fairly recently that the AI tools have been more broadly adopted by nuclear theorists and applied to various problems in nuclear structure and reactions. The main areas of modern AI applications are the following: fast emulation for big simulations; revealing the information content of measured observables with respect to current theory; identifying crucial experimental data for better constraining theory; revealing the structure of theoretical models by means of advanced parameter estimation and model reduction; uncertainty quantification of theoretical results; and improving the predictive capability by assessing extrapolations, as theoretical models are often applied to entirely new nuclear systems and conditions that are not accessible to experiment. Effective field theory analysis of neutron-proton scattering cross section using Bayesian Gaussian processes [16]. The use of Gaussian processes permits efficient and accurate assessment of credible intervals.</p>
<p>A variety of AI/ML tools have been used: various flavors of neural networks, Bayesian calibration, Bayesian model averaging, radial basis function, and support for vector machines. The application areas include interpolation and extrapolation of nuclear masses [17][18][19][20][21][22][23][24][25][26], charge radii [27,28], excited states [29][30][31], beta decay [32,33], alpha decay [34,35], fission yields [36,37], FRIB reach observed Figure 2. Bayesian extrapolation and model averaging. The quantified separation energy landscape in the neutron drip-line region obtained with the Bayesian model averaging [17]. The color marks the probability p ex that a given isotope is bound with respect to neutron decay. For each proton number, p ex is shown along the isotopic chain versus the neutron number relative to that of the heaviest isotope for which a neutron separation energy has been measured. The domain of nuclei that have been experimentally observed is marked by stars. To provide a realistic estimate of the discovery potential with modern radioactive ionbeam facilities, the isotopes within FRIB's experimental reach are delimited by the shadowed solid line.</p>
<p>nucleon-nucleon phase shifts [38], neutron-alpha scattering and three-body parameters [39], nuclear reactions cross sections [40][41][42][43][44], estimates of truncation errors [16,[45][46][47], model calibration and reduction [48][49][50][51][52][53][54], and variational calculations [55].</p>
<p>The low-energy nuclear theory community has been involved in educational efforts in the area of AI. Examples are summer schools, courses, and conferences, including a series of annual meetings on enhancing the interaction between nuclear experiment and theory through information and statistics (ISNET).</p>
<p>Case Studies and Future Prospects</p>
<p>The following case studies are examples of high-impact science that can be enabled by AI. Microscopic description of fission. Modern many-body approaches to fission [56], aided by AI, will provide a predictive description of fission that will produce data for heavy-element research, nuclear astrophysics, and stockpile stewardship. Here, AI-tools will help on several levels, including: development of emulators for constrained density functional theory calculations in manydimensional collective spaces [48,57], action minimization in the classically forbidden regions, description of dissipative dynamics, and the use of neural networks to compute incomplete fission data [36,37]. Origin of heavy elements. The astrophysical rapid neutron capture r-process responsible for the existence of many heavy elements is predicted to involve many elements that are close to the neutron drip line; the structure of these very exotic nuclei thus directly impacts how elements are produced in stellar nucleosynthesis [58]. A quantitative understanding of the r-process requires knowledge of nuclear properties and reaction rates of ∼3,000 very neutron-rich isotopes, many of which cannot be reached experimentally. The missing nuclear data for astrophysical simulations must be provided by massive extrapolations based on nuclear models augmented by the most recent experimental data. Here, ML, with its unified statistical treatment of all uncertainties, can make informed predictions for some of the relevant quantities that reduce extrapolation errors and quantified bounds [19,21,22,25]. Quantified computations of heavy nuclei using realistic inter-nucleon forces. Predictions for heavy and very heavy nuclei such as 208 Pb using A-body approaches based on realistic two-and three-nucleon interactions with full uncertainty quantification will be enabled by Bayesian calibration using pseudo-data from microscopic calculations with supervised ML [51,52]. Development of a spectroscopic-quality nuclear energy density functional: Predictive and quantified nuclear energy density functional rooted in many-nucleon theory [59] will be developed. This task constitutes a massive inverse problem [60] involving a variety of AI tools. The resulting spectroscopic-quality functional-crucial for understanding of rare isotopes-will properly extrapolate in mass, isospin, and angular momentum to provide predictions in the regions where data are not available. Discovering nucleonic correlations and emergent phenomena. Unsupervised learning can be used to discover correlations in calculations of nuclear wave functions that use a microscopic Hamiltonian. There are terabytes of data from calculations with nucleonic degrees of freedom that can be data mined to discover emergent phenomena such as clustering [61][62][63][64], superfluidity [65], and nuclear rotation [66]. Neutron star and dense matter equation of state Data from intermediate-energy heavy-ion collisions and neutron-star merger events can be explored using AI tools to deduce the nuclear matter equation of state [67][68][69]. ML classification tools can also be used in conjunction with calculations of infinite nucleonic matter to map out the phase diagram and associated order parameters.</p>
<p>Enabling Discoveries/What is Needed</p>
<p>The low-energy nuclear theory community is eager to embrace the diverse toolbox offered by AI. Progress in the field could be accelerated by deploying additional resources to meet the most important needs. Need for collaborations. Many barriers can be overcome by establishing collaborations that have long-term perspective. Considering the low level of AI literacy in the community, access to ML/AI/Data science experts is essential. (Semi-)Permanent access to experts in AI/ML/Data Science is crucial as it takes time-for all parties involved-to learn the language and methods. The best solution is to hire a AI/ML/Data Science expert as a joint faculty (or postdoc). Funding mechanisms should be defined to support local and national collaborations in NP and ML/AI/Data science. Need for inter-disciplinary research. Inter-disciplinary research is popular but making it succeed is difficult. Disciplinary boundaries mitigate against hiring ML/AI/Data Science experts involved in NP research. The silo mentality, especially in academia, is a serious problem and is hurting innovation. Formal mechanisms must address the issues of how scholarship is assessed and how teaching is assigned and evaluated, particularly before tenure. Programs should be established to fund AI/NP bridge positions at universities; this would help to create joint faculty appointments at many institutions. Need for a comprehensive approach to AI education. There is, at present, only a patchwork of AI educational efforts in the low-energy nuclear-physics community. A coherent approach to AI education, involving multiple university departments, such as Physics, Statistics, and Computer Science, is needed. While online courses can be effective, they cannot replace regular in-person lectures. Establishing graduate fellowships in the area of ML/AI/Data science applied to NP problems would enable the development of a well-educated workforce in this area. Some universities have "dual Ph.D." programs that allow individual students to work within two different graduate programs. Certificates in AI/ML are a less intensive but still beneficial approach to this problem.</p>
<p>Accelerator Science and Operations</p>
<p>We identify three distinct areas where AI/ML could improve the reliability and performance of the NP accelerator facilities while reducing the operational cost. These areas are:</p>
<p>• Accelerator and material design optimization</p>
<p>• Provenance and prognostication for accelerator sub-systems</p>
<p>• Dynamic optimization of real time operation controls</p>
<p>Although these areas can be investigated independently, providing a "optimal automated accelerator" would require all areas.</p>
<p>Accelerator and material design optimization</p>
<p>Computational techniques lay at the center of accelerator design. Modern simulation codes are capable of self-consistent tracking 10 9 charged particles through complex, nonlinear external field environments, and in modeling interactions with materials. Highly developed and benchmarked engineering codes are employed to design and optimize acceleration structures, high power beam targets, vacuum systems, plasma and solid-state devices for instrumentation.</p>
<p>ML/AI techniques are coming into common use during the design stage to facilitate studies of complex beam dynamics in search of optimum lattices and working point tunes, to study novel schemes for cooling hadron beams, to improve diagnostic schemes for beam measurements, to create performance gains in high intensity and high brightness beam sources, to name but a few [70].</p>
<p>Reinforcement learning and Bayesian optimization are techniques that can be used to explore large design parameter spaces. However, in order for these techniques to provide reliable and optimal solutions they need to be configured and tuned for the specific application. An incorrect kernel selection used in a Gaussian Process technique can lead to disastrous results. Similarly, using a sub-optimal search strategy and/or policy model architecture in reinforcement learning will converge to sub-optimal result. Therefore, it's critical to build or leverage a framework, such as CANDLE and ExaRL, to improve the chances of an optimal solution. </p>
<p>Provenance and prognostication for accelerator sub-systems</p>
<p>Scientific productivity at accelerator-based NP facilities is directly impacted by unscheduled losses of beam time. The trip rate (see Figure 3) is attributable to multiple causation factors that vary in frequency and severity. Some of the main causes are due to excessive beam losses detected by the Machine Protection System (MPS) and to loss of RF cavity control (RF). Machine learning tools for anomaly detection have been deployed at CEBAF [71], and other laboratories [72] to monitor trends in system behaviors precursor to faults.</p>
<p>Design of beam loss monitor networks using Correlation and Principal Component Analysis (PCA) [73] is used to determine optimum locations to place beam loss diagnostics to monitor for all known loss mechanisms in specific beamlines. Unsupervised learning techniques are used to detect faulty beam position monitors that determine beam trajectories [74].</p>
<p>Beyond effects that directly influence beam delivery to experiments, ML techniques are being considered to assist in other critical operational aspects. Predictive schemes for equipment maintenance can be used to proactively identify components requiring attention prior to critical need. Cryogenic production and distribution will benefit from online monitoring and predictive capabilities provided by supervised and unsupervised learning by quickly detecting unplanned helium losses and alerting operators.</p>
<p>The current efforts leverage existing ML frameworks and tools. However, a detailed integration for verification, validation, and reproducibility have not been developed. Additionally, there are no current efforts to integrate uncertainty quantification into the machine learning pipeline. Finally, implementing domain aware ML, when appropriate, could provide better forward prediction models for failure and anomaly detection. These components will be critical to provide a a full featured and reliable monitoring and prognostication system.</p>
<p>As more sub-systems are integrated into a comprehensive monitoring/logging framework, managing the data-load will become increasingly important. These large-scale online data sets faces a range of challenges, including multi-modal and multi-frequency high-dimensional, noisy, and uncertain input data.</p>
<p>Dynamic optimization of real time operation controls</p>
<p>Frontier accelerator facilities such as FRIB and EIC will require years of operational experience to fully develop functional capabilities at their design level. AI/ML techniques are in use to improve the control over particle beams, incorporating Reinforcement Learning (RL) techniques within the accelerator control system [Schram FNAL]. Particle Swarm techniques have been tested to optimize the tuning of aperiodic ion transport lines, and are in development for advanced particle separators [75]. Bayesian Gaussian Processes (GP) and Neural Network (NN) methods are in use to train laser-driven photoinjector facilities in one or more degrees of freedom ( Figure 4). AI/ML activities are being pursued at many NP accelerator facilities and at associated universities. These activities are mainly oriented towards addressing local issues, and are performed by individual scientists or small teams with or without direct support from data science experts. Strategic development and deployment of AI/ML techniques across the DOE complex has high leverage of performance for investment.</p>
<p>Similar to the design optimization effort, techniques such as reinforcement learning can be use to explore the large control parameter space to dynamically optimize for real time system. Leveraging existing frameworks, such as CANDLE and ExaRL, to optimize the learning will be important, however, additional safeguards will be required to ensure that the policy network model doesn't diverge while in a real time system. The ability to process the data in a timely manner will be critical to the applicability of these techniques. Leverage and making advancement in cutting edge technology will provide the ability to deploy better models in real time systems.</p>
<p>Summary and Final Thoughts</p>
<p>We identify specific areas of accelerator design and operations that would benefit from investments in AI and ML technologies.</p>
<p>• Data capture and streaming Developing a comprehensive data capturing and streaming framework will be critical to maximize the utility of the AI/ML tools. Having enough time series data from relevant sensors will be be required to build causal models that properly account for system lags, etc. As we gain confidence and understanding in these AI/ML models, moving them closer to the sensor will allow facilities to automate parts of the operations yielding reduced downtime and operational cost. Development in AI/ML at the edge (FPGAs, etc) and model robustness will be vital. As the NP community expands its use in AI/ML will require access to greater resources to train AI/ML models. Data aggregation and distribution to these compute resources will be an important factor.</p>
<p>• Uncertainty quantification and robustness The need to associate uncertainties with the AI/ML predictions is critical for all efforts. However, it's particularly important when applied to Scientific User Facilities. AI/ML applications for anomaly detection and fault prediction require a quantifiable estimation of uncertainty to determine the proper coarse of action and trade-off (false positives, etc.).</p>
<p>• Optimized design of accelerator systems Development and validation of virtual diagnostics (eg. longitudinal phase space monitors or predictors). Design and simulation of novel accelerators, and advanced engineered materials. Optimized diagnostic design and deployment and improvement to beam sources and injector performance.</p>
<p>• Improving facility performance and user experience Data-driven beam generation, transport, delivery optimization. Automated learning for operator support. Hardware acceleration of ML in distributed control systems. Anomaly detection and classification and mitigation (eg. LLRF, beam diagnostics); System health monitoring (eg. targets, cryoplant); Data driven system maintenance.</p>
<p>• Benchmark techniques on standard models; dedicated accelerator studies Dedicated studies on machines and diagnostic support. Identify specific beamlines, injectors and accelerator facilities to facilitate design and implementation of technologies, algorithms, data pipeline structures.</p>
<p>• Develop capability in AI/ML for computing at the edge (FPGA, etc.) Moving AI/ML workflows closer to the sensor will allow for computing resources to be leveraged and distributed where necessary, allowing for high density data transfers to be conducted locally with reduced load on facility networks.</p>
<p>• AI cookbook of techniques and Data Science workshops/training Development of a community standardized toolkit for training AI/ML scientists and provide answers to commonly encountered issues.</p>
<p>Experimental Methods</p>
<p>Current Status</p>
<p>AI applications to experimental applications are being developed across the subfields of NP. In some experiments which like those depending on image analysis, AI techniques have been successfully applied. This includes the time projection chamber experiments and neutrino experiments [76][77][78][79]. Work has also been done to analyze jet substructure [80], and in detector rejection methods [81]. Current efforts expand upon this work, building on existing AI technologies.</p>
<p>Significant AI endeavors in experimental NP have been in tracking in various detector setups, as highlighted above. Two examples are track classification in the Active-Target Time Projection Chamber at the FRIB and track selection in the CLAS12 drift chambers at Jefferson Lab. Figures 5 and 6 demonstrate two benefits that AI leverages over traditional methods. In the first, classification machine learning methods were used to improve data selection over traditional cut methods. In the second, equivalent accuracy was achieved with AI methods, but with significant (6x) speedup over traditional fitting methods.  </p>
<p>Case Studies and Future Prospects</p>
<p>Holistic approach to experimentation As a long-term, "moonshot" vision, disparate data sources would be intelligently combined and interpreted to improve experiments. Data sources include accelerator parameters, experimental controls, and the detector data itself. Real time analysis and feedback enables the quick diagnostics and optimization of experimental setups. Acceleratorbased, quick-turnaround experiments are a unique challenge in NP. ML expert systems can increase the scientific output from the beamtime allocated to each experiment. Ideally, this holistic approach can be applied to the design of the experiment itself by optimizing machine and detector properties as a single system. Experiment design not limited by computation Future experimental advances in acceleratorbased NP research hinges on increased luminosity, which provides the statistics necessary to observe rare processes. ML methods will reduce computational barriers to this goal. Intelligent decisions about data storage is required to ensure the relevant physics is captured. Depending on the experiment, AI can improve the data taken through data compactification, sophisticated triggers (both software and hardware-based), and fast-online analysis.</p>
<p>An example would be the incorporation of neural networks in the FPGAs which comprise the front-end triggers of complex experiments. The very large channel counts afforded by modern semiconductor detectors combined with high beam luminosity yield data rates that can be prohibitively demanding. Incorporating intelligent triggers with very low latency early in the signal processing chain makes this data challenge more manageable. Furthermore such triggers could act as classifiers allowing for anomaly detection on the data stream prior to the trigger decision flagging interesting events that would normally be silently discarded.</p>
<p>Improving analysis As seen in Sec. 4.4.1, improving data analysis using ML techniques is currently proceeding with two general aims:</p>
<p>• to use these new techniques to improve the sensitivity of current instruments and accuracy of the data, and</p>
<p>• to decrease the time such analysis takes allowing for faster turn around time to produce scientific results.</p>
<p>Improving sensitivity creates more accurate datasets, which decreases uncertainty in results and increases the potential for discovery. Decreasing analysis time saves costs and allows for a higher volume of scientific output. Uncertainty quantification A near term goal is to apply AI methods with well-understood uncertainty quantification, both systematic and statistical, to experimental methods. The dominant ML algorithms used in experimental HEP and NP do not provide error estimations with model predictions, which are essential to understand experimental results. In addition, an evaluation of metrics for the evaluation and comparison of uncertainty predictions from different models is required for widespread use of AI in experimental NP.</p>
<p>Enabling Discoveries/What is Needed</p>
<p>Educate and build a broader community To achieve the experimental goals outlined by the community, we must build a community of researchers knowledgeable in AI technologies. This would be greatly facilitated by centrally located, NP-supported and maintained educational resources and tutorials. Centralized resources allows for: a common foundation to build our technologies, quick dissemination of new techniques, and a bridge from available AI resources to NP related applications.</p>
<p>Build an infrastructure for AI / ML scientists in the NP community. This includes laboratory positions, the establishment of university collaborations, and joint positions.</p>
<p>Standardized data formats In order to collaborate and use AI tools effectively it is important to standardize the way we present data to these systems. Most AI tools in current use are created by industry or large open source projects with established communities. Taking on common data formats and workflows allows us to move with these communities (and each other) more quickly and effectively.</p>
<p>Event Generation and Simulation</p>
<p>Current Status</p>
<p>Simulations of physics processes and detector response are required in NP to design experiments, develop and verify analyses, and compare to theory. They are also used in theory and phenomenology to simulate data and investigate theory advances. High-precision measurements at CEBAF, RHIC, the upcoming EIC and other NP facilities require simulations with high-precision and high accuracy. Achieving the statistical accuracy needed is often computationally intensive with the simulation of the shower evolution in calorimeters being a prime example. As alternative, fast simulations with parameterizations of detector response or other computationally efficient approximations are pursued. However, they still lack the accuracy required for high-precision measurements. Here, AI provides a promising alternative. Fast generative models, e.g., GANs or VAEs, are being utilized to model physics processes and detector responses accurately and accelerate simulations. Beyond that, Bayesian optimization is applied for tuning simulations and detector design, with AI-optimized detector design being emerging for the EIC.</p>
<p>Case Studies and Future Prospects</p>
<p>Accelerate simulations High-Energy Physics has used AI, in particular GAN-based architectures, to successfully accelerate detector simulations. In some cases, in particular in case of calorimeters, the models can be directly applied to fast simulations in NP. In many cases, e.g., for particle identification detectors, new approaches to fast particle identification can be developed as, e.g., shown for Cherenkov detectors [82]. The resulting fast turnaround time for simulations with high-precision and high-accuracy will allow for rapid improvements of the physics reach and detection capabilities of NP experiments.</p>
<p>HPC utilization NP experiments have few payloads appropriate to the Leadership Computing Facilities, in particular for the upcoming exascale supercomputers where accelerator technologies are being applied extensively. AI is the best near-term prospect for using accelerated hardware efficiently. Physics and detector simulations based on AI would be an ideal payload for the Exascale Computing Project.</p>
<p>AI-driven detector design Advanced detector design requires performing computationally intensive simulations as part of the detector-design optimization process. Nowadays there are various AI-based global optimization procedures, e.g., reinforcement learning or evolutionary algorithm. Among these, Bayesian Optimization has gained popularity for its ability of performing global optimization of black-box functions which additionally can be noisy and non-differentiable. For example, an automated, highly-parallelized, and self-consistent framework based on Bayesian Optimization has been recently developed [83], where a PID detector for the future EIC has been considered as a case study. These studies showed an improvement in performance and provided useful hints on the relevance of different features of the detector. The same procedure can be applied to any other detector, or even combination of detectors. Also, costs can be added as parameter in the detector-design optimization process.</p>
<p>AI for event generators Monte Carlo event generators describe collision processes through a combination of theory and phenomenological models. AI approaches can be applied to experimental data and map out the underlying probability distributions governing the spectrum of final-state particles in a given process. This information can be used to construct event generators in a model-independent way, providing unique ways to quantitatively test the validity of theoretical assumptions or models. Such an event generator would store the same information as that contained in the experimental data and can be viewed as compact data storage utility. A prototype event generator is currently being developed with the ETHER (Empricailly Trained Hadronic Event Regenerator) project, as illustrated in Fig. 7 for a comparison of Pythia generated electronproton scattering events with those produced by a Feature-Augmented and Transformed (FAT) GAN [84].</p>
<p>Enabling Discoveries/What is Needed</p>
<p>AI research is multidisciplinary. An interplay of applied mathematics, computer science, and NP will facilitate the development of AI approaches to the unique questions of NP. This will allow, e.g., to design activation functions particular to NP applications or to build efficient neural networks no more complex than necessary. The multidisciplinary approach will also be helpful to understand the requirements for explainable AI and uncertainty quantification for NP simulations. To cultivate multidisciplinary AI development, access to reference data sets, as well as supplementary information for non-experts on what the NP data entails is essential.</p>
<p>Bayesian Inference for Quantum Correlation Functions</p>
<p>Determining the 3-dimensional "tomographic" structure of the proton and nuclei in terms of the elementary quark and gluon (or parton) degrees of freedom of QCD remains one of the central challenges in modern NP. A fundamental complication in this endeavor is the fact that quarks and gluons always remain confined inside hadrons and never observed directly in experiments. This constitutes a classic "inverse problem": how to reliably infer the quantum correlation functions (QCFs) that characterize hadron structure and the emergence of hadrons in terms of partons from the experimental data - Fig. 8.</p>
<p>Existing approaches to extract QCFs, such parton distribution functions (PDFs), fragmentation functions (FFs), transverse momentum dependent distributions (TMDs) or generalized parton distributions (GPDs), from data rely on Bayesian likelihood inference, coupled with suitable parametrizations of the distribution functions on the internal parton momenta. The complexity of mapping between the large quantities of high-precision data expected from JLab 12 GeV (as well as from the future EIC) and the multidimensional QCFs, many of which have never been been explored, will require the creation of a new paradigm in order to assess the impact of the data. An important opportunity therefore exists for utilizing AI/ML techniques to develop the next generation of QCD analysis tools that can more efficiently map between observables and QCFs and maximum the science output from future facilities. </p>
<p>Current Status</p>
<p>Historically the extraction of 1-dimensional QCFs, such as PDFs or FFs, has relied on the maximum likelihood method, which is adequate for cases involving a small number of distributions, but can introduce significant bias and error when applied to more complicated problems involving multidimensional functions. Current state-of-the-art analyses seek to overcome these problems by employing Monte Carlo sampling (NNPDF [85] and JAM [86] Collaborations) to take into account the multiple solutions, and simultaneously determining various types of QCFs which appear in different observables to account for feedback effects [87].</p>
<p>Other examples of state-of-the-art techniques currently employed for 1-D QCF studies include the use of neural net methodology for proton PDFs [85], and the application of generative adversarial networks (GANs) for mapping PDFs [88]. In the transverse momentum sector, the first global TMD analysis was performed recently [89] using the JAM MC methodology extended to the 3-D sector. Exploratory studies of fitting GPDs with neural networks were made for a limited set of deeply-virtual Compton scattering data [90], and recently the more general approach of parametrizing Compton form factors (integrals of GPDs) with neutral nets has been explored [91]. Finally, as lattice QCD simulations at physical quark parameters are becoming more feasible, synergies between global QCD analysis of experimental data and lattice results are being actively explored [92], including the first attempts to perform simultaneous fits to measured cross sections and lattice matrix elements of nonlocal operators, whose Fourier transforms are related to PDFs.</p>
<p>Case Studies and Future Prospects</p>
<p>The history of applying ML tools to study the hadron substructure is rather brief. A recent example used neural nets to construct a universal Monte Carlo event generator (UMCEG) for electron-proton scattering, that is free of theoretical assumptions about underlying particle dynamics [84]. This project, funded by the Jefferson Lab LDRD program, applied generative adversarial network (GAN) technology to simulate particle production at the event level. A new featureaugmented and transformed GAN (FAT-GAN) was developed to select a set of transformed features from particle momenta (generated directly by the generator), and use these to produce a set of augmented features that improve the sensitivity of the discriminator. The new FAT-GAN was tested on pseudodata generated by the Pythia event generator [93], and was able to faithfully reproduce the distribution of final state electron momenta in inclusive electron scattering. The FAT-GAN strategy can be generalized to GANs for simulating other reactions under different conditions, as well as learning exclusive events, and alternative strategies, for example using convolutional neural networks (CNNs), can also be explored. Another important recent application of AI has been to the development of inverse mapping methodology using machine learning for Bayesian inference of QCFs -see Fig. 9. Two machine learning prototypes have been explored, based on a mixture density network and a parametersupervised autoencoder, which have been tested and validated first on a toy model for inclusive DIS, and subsequently on a real global analysis of DIS data. The prototypes were found to be capable of mapping PDFs to within 1-σ CL, consistent with those found in recent global Monte Carlo fits [86]. Extension of the methodology to the 3-D sector remains an important future challenge.</p>
<p>FC NN FC NN FC NN
σ −1 P MDN σ 1 σ 2 σ N α 1 α 2 α M</p>
<p>Enabling Discoveries/What is Needed</p>
<p>To maximize the potential benefit from AI for QCF inference studies, collaboration between QCD physicists and machine learning experts is needed in order to translate the domain knowledge of QCD into generic problem definitions that can be addressed with cutting-edge AI technology. To this end, the creation of joint positions between NP and AI will promote cross-disciplinary fluency in both fields.</p>
<p>The development of an interactive web-based global analysis platform to perform global QCD analysis "on the fly" will allow users to study how different setups (choice of specific data sets or kinematic regions, or improvements on data uncertainties from future facilities, such as the EIC) can affect the inferred QCFs. The vision is to move from the limited paradigm where QCFs are numerically tabulated at interpolation grids, with rigid connections between the data and QCFs, to a more flexible paradigm where QCFs can be generated dynamically from user input.</p>
<p>The creation of such web-based analysis infrastructure would be a valuable tool for the NP community, but will require identifying the most efficient computing platform to host such a service and computing resources for its realization. There is also a critical need for production-level hardware resources to enable the analysis of the large quantities of high-precision data expected from new experimental facilities, in order to understand the deep connections between the data and the QCFs.</p>
<p>Additional Contributions Received</p>
<p>Relativistic Heavy Ions</p>
<p>At extremely high temperature or density, quarks and gluons become deconfined and form a new state of matter -Quark Gluon Plasma (QGP). One can study this matter through high energy nuclear collisions at Relativistic heavy ion collider (RHIC), Large hadron collider (LHC) and other facilities, as well as computer simulations by analyzing the four-momenta and species of final state particles produced in each single collision. The dynamical evolution of the collision systems can be described by hybrid models with relativistic hydrodynamics and hadronic cascades at different stages of the collision. One can infer the initial state of the collision and the intermediate evolution from comparisons the data on final state particles from experiments and simulations.</p>
<p>AI plays an important role in compressing the high dimensional heavy-ion collision data to low dimensions, extracting the model parameters and their uncertainties with Bayesian analysis, classifying the equation state, regressing the initial nuclear structure or in solving partial differential equations of relativistic hydrodynamics using deep neural networks. These AI applications are described in the following.</p>
<p>Compressing data to low dimensions: Many experimental observables are designed to compress complex high energy nuclear data to low dimensions using simple projection, statistical mean, variance and correlations along a few directions. Unsupervised learning algorithms such as PCA is widely used in the field of high energy nuclear physics, to automatically extract the most informative features in data. PCA can be used to determine the magnitude of different longitudinal fluctuation modes [94], which helps to constrain the initial state entropy deposition along the beam direction in heavy-ion collisions. Since the initial state fluctuations of entropy density in the transverse plane is converted to final state correlations of particles in momentum space, the collectivity and anisotropy of final state particles along the azimuthal angle direction are quantified by the flow harmonics v n . The v 3 factorization breaking is well described using 2 initial state fluctuation modes given by PCA and a linear hydrodynamic response [95]. PCA also rediscovers flow harmonics [96] which are originally computed from Fourier decomposition.</p>
<p>Bayesian analysis to extract QGP properties: Bayesian analysis uses the likelihood between low dimensional experimental data and model output to constrain model parameters, such as the QCD equation of state [97]. The prior QCD EoS used in hydrodynamics is parameterized to cover the physical equation of state functional space.The posterior distribution of the EoS agree with lattice QCD calculations. To take into account the effect of other entangled parameters, Trento + iEBE-VISHNU + UrQMD model is used to do a global fitting using Bayesian analysis [98][99][100]. The clear peak structure in the posterior distributions of model parameters indicates non-zero shear and bulk viscosity of the QGP. When high energy partons traverse through QGP, they loss energy by elastic scattering and gluon radiations. The Bayesian analysis is also used to constrain the heavy quark diffusion coefficients [101], the light quarkq [102] and the jet energy loss distribution [103].</p>
<p>Jet classification in heavy ion collisions: The applications of neural network was used in 1996 to determine the impact parameter of heavy-ion collisions [104], with a one-hidden layer neural network. Various architectures of deep neural network are used in jet flavor classification for proton-proton collisions. However, the applications to heavy-ion jet classification is rare. The classification performance worsens due to soft gluon radiations affecting soft jet substructure [105]. Recently a point-cloud-like network called particle/energy flow network is employed in jet flavor classification [106] and is used to design new physical observables for heavy-ion jets [80].</p>
<p>Classification for nuclear phase transition: Beam energy scan (BES) project aims to locate the QCD critical point that separates the first order phase transition and smooth crossover in the QCD phase diagram by colliding heavy ions at various energies. Deep convolution neural network is used to classify these two different nuclear phase transition regions [107] using relativistic hydrodynamic simulations of heavy ion collisions. The phase transition type used in the equation of state is encoded in the evolution and deep neural network helps to decode this information from the complex final state output of heavy-ion collisions. Although there is entropy production and information loss, the network succeeds in classifying nuclear phase transition types with approximately 93% accuracy. Deep convolution neural network uses images as input, a more natural representation of the heavy-ion data of a list of particles with their four momenta, pid and charge information. Point cloud network is a perfect architecture for this data structure. A recent study uses point cloud network to classify Spinodal and Maxwell constructions of the first order phase transition [108].</p>
<p>Regression for nuclear-shape deformation: Most heavy ions used at RHIC and LHC are deformed. The collisions of deformed nuclei produce complex correlations between charged multiplicity and anisotropic flow. Using Monte Carlo simulation data, a 34-layer residual network is used to predict the values of nuclear shape deformations [109]. The network succeeds in predicting the magnitude of nuclear shape deformations but not their signs, which indicates that there is a degeneracy between high-energy collisions of prolate-prolate and oblate-oblate nuclei.</p>
<p>Interpretation and explanation: Interpretation is important in understanding what has been learned by the black box deep neural network. In the classification task for nuclear phase transition, a prediction difference analysis algorithm is used to locate the most important phase space regions in the input for classification. In the regression task for nuclear shape deformation, a regression attention mask algorithm is developed to highlight the regions that are important for the decision making.</p>
<p>Accelerate relativistic hydrodynamic simulations: Accumulating data in heavy-ion collisions is slow. Stacked-UNet is used to solve relativistic hydrodynamic equations [110]. The time evolution of the energy density and fluid velocity from neural network method agree with 2+1D viscous hydrodynamics. The trained network can solve hydrodynamic equations 600 times faster than numerically solving partial differential equations. As a comparison, the GPU parallelization brings 60 to 100 times speed up.</p>
<p>Current study of heavy-ion collisions with machine learning have used data set generated with model simulations. To apply these techniques to real experimental data, one has to taken into account the acceptance and efficiencies of the detectors. This can be accomplished through incorporation of the characteristics of the detector in the model simulations which are used to train the network for final application to real experimental data. In the meantime, advance in the accelerated model simulations with more realistic physics scenarios are needed for more robust AI studies.</p>
<p>Project 8</p>
<p>The Project 8 collaboration is developing an experiment to measure the absolute neutrino mass with cyclotron radiation emission spectroscopy (CRES). The event reconstruction process for Project 8 can be framed as a challenge of feature recognition in noisy data, where the features to find are the electron tracks and how they are grouped together. The Project 8 collaboration has studied two uses of machine learning to improve track and event reconstruction. The first application was to differentiate different types of tracks by their characteristics [111]. Figure 10 shows an electron event with five visible tracks. The four sideband tracks and one visible main-carrier track are labeled. We first analyzed individual tracks and extract parameters like slope and power density, and then applied a Support Vector Machine to distinguish three track populations: main carrier tracks with high pitch angles (the angle of the electron's momentum relative to the magnetic field in the experiment), main carrier tracks with low pitch angles, and sidebands. Having this information can help in reconstructing events, avoiding problems that might occur when particular tracks are not observed, like the missing main carrier in Figure 10.</p>
<p>Machine learning can be applied to Project 8 data to identify tracks, as well. We are developing a method for identifying tracks using a Convolutional Neural Network (CNN). This particular task is a straightforward application of a CNN with a U-Net architecture. Such a tool, once optimized, will be used to do the initial optimization of the tracks in events such as Figure 10. While the initial application of the CNN to Project 8 data is straightforward, there are a variety of details to establish, such as accounting for all of the necessary track topologies, and understanding the efficiency of detection. Figure 10. A multi-track CRES event featuring five visible tracks. In the first set of tracks, on the left, the main carrier track is not visible; it is important to know that the visible tracks are sidebands so that the initial track start can be determined accurately. Figure is from [111].</p>
<p>NEXT</p>
<p>The NEXT neutrinoless double beta decay program has as its primary physics goal discovering or severely limiting parameter space for the Majorana nature of neutrinos in 136Xe decays. NEXT will undertake this search in a staged program of high pressure gas xenon Time Projection chambers (HPgXeTPCs), culminating in a multi-ton detector that will be effectively background-free.</p>
<p>HPgXeTPCs, because of the benefits of gaseous xenon (perhaps with a He additive) including its small Fano factor, allow to see the topology of the double beta decay while achieving sub 1% energy resolution. The technology and the path to the necessary low background model has been demonstrated in a small detector NEXT-NEW. A future design of the 1-ton-scale High Definition (HD) design is shown below, along with a typical double beta event in simulation with its Bragg peaks at the end of each track. The exquisite topological information in these detectors calls out for Deep Convolutional Neural Nets (DNNs) to perform tasks such as signal and background classification and, in fact, full semantic segmentation-based event reconstruction.</p>
<p>The collaboration has already published [112] work on DNNs applied to NEXT-NEW data. A team is now at work on its Summit allocation to extract optimal sensitivity from simulated ton-scale designs. The team has already shown its effective use of sparse DNNs in similar highlyparallel applications on Summit, and early work is already bringing benefits to the extremely promising (multi) ton NEXT program.</p>
<p>WANDA</p>
<p>The sequence of steps whereby nuclear data is compiled, evaluated, processed, and incorporated into applications is referred to as the "Nuclear Data Pipeline"". The pipeline provides the critical connection between laboratory measurements and their eventual use in models of reactors, isotope production, detectors for non-proliferation, supernova explosions, radchem networks, and many other systems. To improve and supercharge this pipeline, the nuclear data community has extensive needs, including: more rapid, accurate, and robust evaluations; quicker compilation of data and accompanying contextual information from published experimental work; robust methods to optimize experimental design for verification, validation, and benchmarking; wider use of realistic physics models in transport simulations; and reproduction of the results of complex multi-physics codes via fast-execution surrogate models. AI/ML tools have tremendous potential to address all of these critical needs. During the recent Workshop for Applied Nuclear Data Activities (WANDA) [113], the nuclear data community has recently identified a number of key areas in which AI/ML advances have already made significant impacts and show substantial promise both in the short term and long into the future. Targeted investments are needed now to fully realize the potential of AI/ML in nuclear data, preferably by leveraging AI/ML advances in other areas for use in nuclear physics and simultaneously driving AI/ML innovations. Some of the many areas to emphasize include:</p>
<p>• Using AI/ML tools to identify systematic trends in nuclear data that were missed by human evaluators, and developing AI/ML emulators to incorporate complex physics models into evaluations, so that evaluations can be more robust, new physics can be uncovered, and predictive power can be improved.</p>
<p>• Exploiting AI/ML tools to process complex relationships between nuclear data and integral experiments to develop rigorous validation approaches, so that that AI/ML tools can be confidently deployed in nuclear energy, nuclear security, and other applications where safety is paramount.</p>
<p>• Quantifying the intrinsic uncertainties of AI/ML tools, so that their results can be fully integrated into the nuclear data UQ process that is critical to the validation, verification, benchmarking, and normalization activities widely utilized across nuclear data activities.</p>
<p>• Developing a new, standardized, QA-vetted, well-characterized database of nuclear information including UQ that can be easily input into AI/ML codes, so that AI/ML advances can be more quickly used across the nuclear data pipeline.</p>
<p>• Collecting and sharing fitted models, training data, and notes on their applicability and limitations, so that the reproducibility of AI/ML results can be enhanced and advances from across disciplines can be best leveraged for the widest utilization.</p>
<p>• Using AI/ML tools to both develop surrogate physics models and use them to sequentially search and optimize over a wide space of experimental design, so that the most impactful data are targeted and collected more efficiently, and so that specific deficiencies in data needed for robust evaluations are avoided.</p>
<p>• Developing natural language processing (NLP) tools to automatically compile new results, so that errors in data entry can be reduced, consistency checks can be facilitated, expert validation and verifications can be quickly done, and database insertion can be seamlessly performed.</p>
<p>• Fostering collaborations between nuclear researchers and AI/ML experts, so that appropriate algorithms can be efficiently determined for a given problem and subsequently trained, tuned, and deployed for maximum scientific impact while minimizing biases or unphysical results.</p>
<p>Cross Cutting Topics</p>
<p>The breakout sessions of the workshop focused on topics in NP with the knowledge that there are commonalities between these topics, and indeed, across many scientific domains. These 'cross cutting' areas span the spectrum from the development of methodologies and mathematics for AI approaches, the need for sophisticated data management and curation, and means to establish an AI cognizant workforce. The following section outlines a number of these cross cutting topics.</p>
<p>Statistical methods and tools</p>
<p>Statistics and statistical methods are based on probability spaces, defined in terms of sets and probability measures. They aim to provide a better understanding and quantified characterization of a given set of data. Data mining uses statistics as well as other methods to find patterns in order to explain phenomena. Machine learning uses data mining and other learning algorithms in order to predict future outcomes, and AI uses models based by machine learning to make intelligent decisions. As the application of AI to NP is in the early stages, the integration of statistical methods and uncertainty quantification into more advanced AI applications is still under development.</p>
<p>Overview of approaches in NP</p>
<p>In lattice QCD and other lattice field theories, AI has been used for configuration generation, propagator inversion, observables, and overcoming the sign problem. Among the various statistical methods utilized, Jensen-Shannon divergences have been used to distinguish gauge field ensembles using deep neural networks [114]. Bayesian neural networks have been used for spectral reconstruction [115] and reconstructing parton distribution functions [116]. Machine learning regression errors for parton distribution functions has also been quantified using bias correction and bootstrap resampling [117]. Bayesian inference and other statistical methods applied to model parameterizations of partonic structure are key to extracting parton distribution functions, fragmentation functions, transverse momentum dependent distributions, and generalized parton distributions.</p>
<p>In low-energy nuclear theory, Bayesian methods have been used across a variety of different problems for uncertainty quantification. This includes Bayesian calibration for nucleon-nucleon phase shifts [38] and direct nuclear reactions [40][41][42]; Bayesian Gaussian processes for truncation errors in effective field theory [16,45] and uncertainties in neutron-alpha scattering and threebody parameters [39]; Bayesian calibration for A-body calculations [53] and mass models [48,54]; Bayesian extrapolations [21]; Bayesian model averaging [17,22,23]; and Bayesian neural networks for r-process beta decays [32], alpha decays [34,35], and spallation cross sections [43]. Bayesian regularization as well as other approaches have been used for uncertainty quantification in applying neural networks to applications such as the extrapolation of truncation errors in nuclear structure calculations [46,47] and variational methods [55].</p>
<p>AI applications to experimental NP are being developed across the subfields of NP. Experiments that map well to existing AI technologies, such as image analysis problems, have demonstrated success in NP. Examples include time projection chamber experiments and neutrino experiments [76][77][78][79]. Work has also been done to analyze jet substructure [80], and in detector rejection methods [81]. Current efforts are expanding upon this work, building on existing AI technologies. In the future one would like to apply AI methods to experimental methods with systematic and statistical uncertainty quantification. Similarly, Bayesian optimization will also be extremely useful for tuning event simulations and detector design.</p>
<p>AI techniques in accelerator science and operations have been adopted and utilized for some time. Early usage of simulated annealing and genetic algorithm techniques [118,119] were applied to optimize the distribution of pure permanent magnets in extended undulator assemblies for SASE FELs. SVD techniques have been used to optimize steering control in storage rings [120]. Genetic algorithms and optimization techniques have been used to brightness of electron photoinjectors [121] and electron synchrotrons and storage rings [122]. Recent reviews [70,123] have identified uses of artificial neural networks, convolutional neural networks, Bayesian optimization, reinforcement learning, random forest, and other methods in accelerator controls [124], longitudinal phase space prediction [125], anomaly detection in SRF cavities and beam diagnostics [71,74], FEL performance enhancement [126], etc. Current efforts are expanding in all areas of accelerator control, optimization and design, diagnostics and prognostics.</p>
<p>Use of Current Tools</p>
<p>The current surge in AI has provided great advances in software tools and hardware that can provide the basis of machine learning systems used in data processing. Readily available off the shelf solutions are well suited for basic classification problems, particularly for images. Analysis of experimental data however, requires regression networks that often need careful tuning to specific problems and data sets. In addition, scientific results require well understood systematic uncertainties in values obtained from any analysis. For example, charged particle tracking requires not only a 5 parameter state vector, but also a 15 parameter diagonal covariance matrix to represent its uncertainties. These are needed as inputs to kinematic fitting routines which combine constraints imposed by physics with the experimentally measured values in order to achieve optimal resolutions. Scientific results also require study to ensure no bias is introduced by the analysis technique. More so than is needed by industrial applications.</p>
<p>Collaborations and collaborative activities</p>
<p>The importance of community and collaboration were a cross cutting theme in the workshop. There are distinct types of collaboration, each of which is beneficial.</p>
<p>NP Communities of Practice</p>
<p>Relative to other communities, the NP scientists have relatively few communities of practice that enable knowledge exchange on technical topics or the ability to articulate the requirements for community developed and supported tools. This is in contrast to, for example, the HEP community, which has several sanctioned or funded activities that focus on computing, including the HEP Software Foundation (HSF) (sponsored by CERN), the IRIS-HEP collaboration (funded by NSF) and the Forum for Computational Excellence (funded by OHEP). HSF in particular was instrumental in developing computing focused white papers for the the European Strategy for Particle Physics.</p>
<p>The scientific expertise to approach the challenges in NP lies within the NP community. For that reason, communities of practice within NP to share knowledge computing knowledge could be invaluable tools towards addressing common challenges. These are happening on a small scale, such as the Jefferson Lab AI lunch series and the Monthly Computing Round Table hosted jointly by BNL and Jefferson Lab. Community based groups could serve as a clearing house for training opportunity announcements and similarly to the HSF, as a tool for organizing community white papers. One of the outcomes of the AI for NP Workshop is the establishment of a proto-community that came together to produce this report.</p>
<p>Several concrete actions that could be undertaken by a community of practice is developing a portal for community based A.I. training resources and the development of AI recipe books. Another topic could be a discussion around data management standards. Extending these activities to include AI experts would be beneficial to creating a much needed community to leverage the rapid advancement of methods and tools in the AI/ML communities.</p>
<p>Engagement with Data Science community</p>
<p>The NP community recognizes the importance of engaging the data science community to develop technologies that enable innovation in NP. Research groups have begun collaboration with computer scientists with demonstrated success [84]. However, a broader effort to formally collaborate with the AI scientists can advance AI technologies in NP while taking advantage of unique aspects of data in NP to inform innovation in AI. Fostering such collaboration is essential for long term success in developing AI techniques that realize the potential for impacting NP challenges.</p>
<p>In order to best interact with the AI community, both parties must identify and engage in mutually beneficial research topics. This requires education and interaction of the two fields. To maximize collaboration, laboratories and institutions can create an infrastructure in which AI scientists are an integral part of the field. This can be accomplished through joint projects that includes welldefined metrics of success for an AI scientist working in a physics field. The collaboration will be mutually beneficial, with the AI work not considered a service, but as a true collaboration. This can be evidenced by nuclear physicists and AI scientists publishing together, whether in physics or AI journals. Building a merged community of physicists and data scientists brings challenges in nuclear physics data analysis to the consideration of AI researchers as they develop new methods. This will allow AI technology to advance in line with our community's needs.</p>
<p>Engagement with ASCR</p>
<p>For the past few years, the U.S. Department of Energy, Office of Science program in Advanced Scientific Computing Research (ASCR) has been conducted several workshops directly and indirectly focused on AL/ML which resulted in several reports.</p>
<p>In January 2018, the ASCR Basic Research Needs workshop on Scientific Machine Learning [127] identified six priority research directions (PRDs). The first three focused on the foundation research themes: 1) Domain-awareness, 2) Interpretable, and 3) Robust. Within the NP community, the use of domain aware ML to leveraging scientific domain knowledge by enforcing physical conservation law and governing equations was identified. Additionally, providing robust ML solutions is important for scientific research and critical when deployed at scientific user facilities (SUFs). The last three focused on capability research themes: 4) Data-Intensive, 5) Enhanced Modeling and Simulation, 6) Intelligent Automation and Decision Support. All three PRDs of these items have clear applications within the NP community. For example the semi-automation of emerging SUFs could significantly reduce operational cost and downtime.</p>
<p>Although not explicitly focused on ML, ASCR convened a workshop on in situ data management (ISDM) on January 28-29, 2019 [128]. The goal of the ISDM workshop was to consider in situ data management to support traditional and future scientific computing needs. Six PRDs were identified: 1) Pervasive, 2) Co-designed, 3) In Situ Algorithms, 4) Controllable, 5) Composable, and 6) Transparent. These priorities are of particular interest to DOE NP since they could directly feed into the existing and future facilities, such as FRIB and the EIC. In particular, the need for provenance and reproducibility was explicitly linked to the development and use of ML. Additionally, in situ algorithm and controllable ISDM would enable semi-automated SUFs.</p>
<p>On June 5, 2019, the DOE Office of Science (SC) organized a one day workshop centered on the topic of Data and Models: A Framework for advancing AI in science report [129]. Three priority opportunities were identified: 1) democratize access to benchmark science data, 2) make AI operational in science with composable services and 3) address open questions in AI with frameworks. Providing a Findable, Accessible, Interoperable, and Reusable (FAIR) dataset and composable tools would accelerate the ability for members of the NP community to develop new algorithms and efficiently train them using these services.</p>
<p>Between July and October 2019, four town hall meetings dubbed "AI for Science" were conducted to discuss and identify the scientific needs and opportunities across a diverse collection of domains (biology, physics, mathematics, accelerators, computing, etc.). Some of the most notable grand challenges for NP included: Automate and/or optimize the operation of accelerators and detector systems; Improve experimental design and real time tuning.</p>
<p>Finally, it was identified that the NP community could benefit from using existing AI/ML solutions by leveraging existing ASCR investments. For example, the Exascale Computing Project (ECP) has created tools that can accelerate computationally expensive tasks. For example, using CANDLE to perform large scalable hyper-parameter optimizing scans could potentially significantly improve on existing results. Similarly, the ECP ExaLearn project is now developing scalable tools to address common AI/ML challenges such as developing surrogate models, inverse problems, and automated design and controls challenges. These tools could save a significant amount of development time and allow the NP community to focus on solving domain specific challenges.</p>
<p>As the NP community expands its use in AI/ML it will require access to greater computing resources to train AI/ML models. The NP community should leverage the existing ASCR computing facilities and develop a data aggregation and distribution community plan.</p>
<p>The importance of Data Management</p>
<p>AI techniques are reliant on the quantity and quality of the data and for this reason, applications of AI are likely to result in a paradigm shift in data management. Accessibility of the data to the wider NP community would create a connectivity across experiments that could increase collaboration. Viewing data as a valuable commodity impacts decisions on how data from experiments and simulations is collected, cataloged and accessed. AI techniques could also facilitate near real-time calibration and analysis.</p>
<p>As mentioned in several of the summaries, current analysis techniques often 'flatten' the experimental data. To maximize the usefulness of the data, it will be important to have agreements and documentation on 'processing' of experimental data, the application of theoretical assumptions and the treatment of systematic uncertainties that will be used as training samples or as part of combined analysis. All relevant information about the data will have to be stored with the data. This should trend towards the development of appropriate standards consistent with FAIR data principles and frameworks that capture data and metadata.</p>
<p>Workforce development</p>
<p>Education</p>
<p>There are only 26,000 AI researchers currently in the US. This is estimated to represent only a fifth of the current demand. There is an urgent need for training in AI, at a variety of educational levels and for diverse audiences. To this end,there is an urgent need to develop a range of outreach, recruitment, and educational activities. NP research will serve to raise interest in AI-related fields. The goal is to retain talented students in AI-related fields and to help them to secure employment in a wide range of careers, thus ensuring that the new techniques and concepts developed in NP laboratories are widely disseminated. Unfortunately, the current educational efforts in AI in NP-while extremely valuable-are patchwork. They include summer schools, topical programs, workshops, and conferences.</p>
<p>A coherent inter-disciplinary approach is needed. Several mechanisms were discussed at the Workshop aiming at improving the situation.</p>
<p>University-wide AI courses: There is a need for inter-disciplinary AI courses involving Applied Mathematics, Statistics, and Computer Science experts, as well as domain scientists. Online courses play important role, but the in-person approaches are superior. Dual Ph.D. Programs: Some universities allow "dual Ph.D." programs that allow individual students to work within two different graduate programs. Students start graduate school in their primary department, and then enter such a program by arranging a secondary affiliation upon choice of a research project and advisor. Certificates in AI/ML are a less intensive but still beneficial approach to this problem.</p>
<p>Educational Outreach Opportunities: AI practice is inherently interdisciplinary and an effort should be made to introduce the AI field to young physicists, computer and data scientists, mathematicians, and others in related fields as they choose their career paths. Conferences and Workshops play an important role in this cross-pollination. For example, workshop organizers received a grant from the National Science Foundation that funded travel for 18 graduate-and undergraduate-students and early career professionals, most of whom indicated they would not have been able to attend the workshop without this support. The pre-workshop hackathon provided students with an opportunity to creatively collaborate on a problem solving competition related to AI.</p>
<p>The Level of AI Literacy</p>
<p>The interest in the workshop was very good: as many as 184 scientists came to the meeting and many attended remotely. According to the data gathered by the Workshop's questionnaire, around 40% participants are new to AI, 70% would like to apply techniques from this workshop, and 40% actively working on project using AI. These numbers well reflect the current situation: many nuclear physicists understand the potential benefits of AI, but there is a steep learning curve. Considering the current efforts, more sophistication in using AI tools is needed. Indeed, majority of NP users apply off-the-shelf tools; fewer understand the AI glossary and make informed choices about the modern AI tools that suit their problem best. Even fewer practitioners are advanced users or innovators who consider uncertainty quantification to be an essential part of the answer and/or consider the full feedback between AI and physics problem (AI application is modified depending on the physics outcome).</p>
<p>In short, at this point, NP community at large does not fully grasp the depth of the AI universe with the majority of work being carried out by users often helped by enthusiastic undergraduate and graduate students. But the foundations are there: nuclear physicists have good technical background and they are used to problem-driven approaches to tool selection. This helps in choosing the best/right tools for the problems. One has to remember, however, that the newest AI tools are almost always largely untested. It takes some experience to know which tools to use. Simply understanding that this is true will help nuclear physicists avoid dangerous pitfalls.</p>
<p>How can the level of AI literacy be improved? As discussed in Sec. 5.2 the fastest route to an AI-educated community involves easy access to ML/AI/Data science experts. In the long-term, education of younger generation is essential. Several mechanism to improve the situation in this area are proposed in Sec. 8.</p>
<p>Figure 1 .
1Bayesian calibration.</p>
<p>,Figure 3 .
3CEBAF beam trip event rates (Courtesy R. Michaud, Jefferson Lab).</p>
<p>,Figure 4 .
4Neural network model used to train tunable laser profiles at Argonne National Laboratory (courtesy R. Roussel, ANL).</p>
<p>Figure 5 .
5Histogram visualizing the classification of events from the 46Ar(p,p ) experiments in the Active-Target Time Projection Chamber at the Facility for Rare Isotope Beams. Based on the "goodness of fit" (χ 2 ) distribution of the entire dataset, a cut (dashed line) was chosen at 40 (in arbitrary units). The events that were handclassified as protons from this run are hatched[77].</p>
<p>Figure 6 .
6Ratio of result from the traditional analysis method to results from a deep, fully-connected neural network for tracks in the Hall B drift chamber at Jefferson Lab. The neural network algorithm performs comparably to traditional methods, but with 6x speedup, allowing for faster analysis.</p>
<p>Figure 7 .
7In the pioneering ETHER project to construct Monte Carlo event generators agnostic of theoretical assumptions and phenomenological models, GANs are being developed as a repository for the behavior of the theory expressed in Pythia. As shown in the comparison of inclusive DIS kinematics from Pythia (left panel) and ETHER (right panel), the AI approach is being able to learn the Pythia data.</p>
<p>Figure 8 .
8Illustration of factorization in electron-proton scattering. The black box is interpreted on the right in terms of short-distance reactions of quarks and gluons, with the details of proton structure and hadronization parametrized in terms of QCFs.</p>
<p>Figure 9 .
9AI architecture for inverse mapping connecting an N -dimensional space of observables σ i into an M -dimensional space of parameters α j . Aside from the fully connected neural networks (FC NN), the inverse mapper is equipped with a mixture density network (MDN) layer to allow for possible multiple solutions for the inverse problem. The subscript "P" differentiates the parametrized inverse function σ −1 from the true inverse function.</p>
<p>of Physics, under the grant 'Artificial Intelligence (AI) Workshop in Nuclear Physics,' Award Number 2017170. Support for the Hackathon was provided by the University of Virginia School of Data Sciences and by AmazonWeb Services 
4.2.1. Current Status 
10 
4.2.2. Case Studies and Future Prospects 
11 
4.2.3. Enabling Discoveries/What is Needed 
12 
4.3. Accelerator Science and Operations 
13 
4.3.1. Accelerator and material design optimization 
13 
4.3.2. Provenance and prognostication for accelerator sub-systems 
14 
4.3.3. Dynamic optimization of real time operation controls 
15 
4.3.4. Summary and Final Thoughts 
15 
4.4. Experimental Methods 
16 
4.4.1. Current Status 
16 
4.4.2. Case Studies and Future Prospects 
17 
4.4.3. Enabling Discoveries/What is Needed 
18 
4.5. Event Generation and Simulation 
19 
4.5.1. Current Status 
19 
4.5.2. Case Studies and Future Prospects 
19 
4.5.3. Enabling Discoveries/What is Needed 
20 
4.6. Bayesian Inference for Quantum Correlation Functions 
20 
4.6.1. Current Status 
21 
4.6.2. Case Studies and Future Prospects 
22 
4.6.3. Enabling Discoveries/What is Needed 
23 
4.7. Additional Contributions Received 
23 
4.7.1. Relativistic Heavy Ions 
23 
4.7.2. Project 8 
25 
4.7.3. NEXT 
26 
4.7.4. WANDA 
27 </p>
<ol>
<li>Cross Cutting Topics 
28 
5.1. Statistical methods and tools 
28 
5.1.1. Overview of approaches in NP 
28 
5.1.2. Use of Current Tools 
29 
5.2. Collaborations and collaborative activities 
29 </li>
</ol>
<p>Graduate Fellowships: Establishing graduate NP/AI fellowships, similar to, e.g., DOE Compu-tational Science Graduate Fellowship or DOE NNSA Stewardship Science Graduate Fellowship, would enable the development of a well-educated workforce in this area.
Machine learning enables computers to learn from experience or examples.2 Deep learning is a class of ML algorithm that are composed of multiple hidden layers.</p>
<p>The National Artificial Intelligence Research and Development Strategic Plan, National Science and Technology Council, Networking and Information Technology R&amp;D Subcommittee. "The National Artificial Intelligence Research and Development Strategic Plan, National Science and Technology Council, Networking and Information Technology R&amp;D Subcommittee," (2016).</p>
<p>Artificial intelligence for the american people. "Artificial intelligence for the american people," (2019).</p>
<p>Reaching for the horizon: The 2015 long range plan for nuclear science. Ani Aprahamian, Ani Aprahamian et al., "Reaching for the horizon: The 2015 long range plan for nuclear science," (2015).</p>
<p>Nuclear physics and quantum information science, a report by the nsac quantum information science subcommittee. "Nuclear physics and quantum information science, a report by the nsac quantum information science subcommittee," (2019).</p>
<p>. Neurodata. "Neurodata," (2020).</p>
<p>Candle project. "Candle project," (2020).</p>
<p>Fermions at Finite Density in 2+1 Dimensions with Sign-Optimized Manifolds. Andrei Alexandru, Paulo F Bedaque, Henry Lamm, Scott Lawrence, Neill C Warrington, 10.1103/PhysRevLett.121.191602arXiv:1808.09799Phys. Rev. Lett. 121191602hep-latAndrei Alexandru, Paulo F. Bedaque, Henry Lamm, Scott Lawrence, and Neill C. Warrington, "Fermions at Finite Density in 2+1 Dimensions with Sign-Optimized Manifolds," Phys. Rev. Lett. 121, 191602 (2018), arXiv:1808.09799 [hep-lat].</p>
<p>Toward solving the sign problem with path optimization method. Yuto Mori, Kouji Kashiwa, Akira Ohnishi, 10.1103/physrevd.96.111501Physical Review D. 96Yuto Mori, Kouji Kashiwa, and Akira Ohnishi, "Toward solving the sign problem with path opti- mization method," Physical Review D 96 (2017), 10.1103/physrevd.96.111501.</p>
<p>Finite-Density Monte Carlo Calculations on Sign-Optimized Manifolds. Andrei Alexandru, Paulo F Bedaque, Henry Lamm, Scott Lawrence, 10.1103/PhysRevD.97.094510arXiv:1804.00697Phys. Rev. D. 9794510hep-latAndrei Alexandru, Paulo F. Bedaque, Henry Lamm, and Scott Lawrence, "Finite-Density Monte Carlo Calculations on Sign-Optimized Manifolds," Phys. Rev. D 97, 094510 (2018), arXiv:1804.00697 [hep-lat].</p>
<p>A simple approach towards the sign problem using path optimisation. Francis Bursa, Michael Kroyter, 10.1007/JHEP12(2018)054arXiv:1805.04941JHEP. 1254hep-latFrancis Bursa and Michael Kroyter, "A simple approach towards the sign problem using path opti- misation," JHEP 12, 054 (2018), arXiv:1805.04941 [hep-lat].</p>
<p>Path Optimization for the Sign Problem in Field Theories Using Neural Network. Akira Ohnishi, Yuto Mori, Kouji Kashiwa, 10.7566/JPSCP.26.024011JPS Conf. Proc. 2624011Akira Ohnishi, Yuto Mori, and Kouji Kashiwa, "Path Optimization for the Sign Problem in Field Theories Using Neural Network," JPS Conf. Proc. 26, 024011 (2019).</p>
<p>Flow-based generative models for Markov chain Monte Carlo in lattice field theory. M S Albergo, G Kanwar, P E Shanahan, 10.1103/PhysRevD.100.034515arXiv:1904.12072Phys. Rev. D. 10034515hep-latM.S. Albergo, G. Kanwar, and P.E. Shanahan, "Flow-based generative models for Markov chain Monte Carlo in lattice field theory," Phys. Rev. D 100, 034515 (2019), arXiv:1904.12072 [hep-lat].</p>
<p>Equivariant flow-based sampling for lattice gauge theory. Gurtej Kanwar, Michael S Albergo, Denis Boyda, Kyle Cranmer, Daniel C Hackett, Sébastien Racanière, Danilo Jimenez Rezende, Phiala E Shanahan, arXiv:2003.06413hep-latGurtej Kanwar, Michael S. Albergo, Denis Boyda, Kyle Cranmer, Daniel C. Hackett, Sébastien Racanière, Danilo Jimenez Rezende, and Phiala E. Shanahan, "Equivariant flow-based sampling for lattice gauge theory," (2020), arXiv:2003.06413 [hep-lat].</p>
<p>Learning and prediction of nuclear stability by neural networks. S Gazula, J W Clark, H Bohr, 10.1016/0375-9474(92)90191-LNucl. Phys. A. 540S. Gazula, J.W. Clark, and H. Bohr, "Learning and prediction of nuclear stability by neural net- works," Nucl. Phys. A 540, 1 -26 (1992).</p>
<p>Neural network models of nuclear systematics. K A Gernoth, J W Clark, J S Prater, H Bohr, 10.1016/0370-2693(93)90738-4Phys. Lett. B. 300K.A. Gernoth, J.W. Clark, J.S. Prater, and H. Bohr, "Neural network models of nuclear systematics," Phys. Lett. B 300, 1 -7 (1993).</p>
<p>Quantifying Correlated Truncation Errors in Effective Field Theory. J A Melendez, R J Furnstahl, D R Phillips, M T Pratola, S Wesolowski, 10.1103/PhysRevC.100.044001arXiv:1904.10581Phys. Rev. 10044001nucl-thJ. A. Melendez, R. J. Furnstahl, D. R. Phillips, M. T. Pratola, and S. Wesolowski, "Quanti- fying Correlated Truncation Errors in Effective Field Theory," Phys. Rev. C100, 044001 (2019), arXiv:1904.10581 [nucl-th].</p>
<p>Quantified limits of the nuclear landscape. Léo Neufcourt, Yuchen Cao, Samuel A Giuliani, Witold Nazarewicz, Erik Olsen, Oleg B Tarasov, 10.1103/PhysRevC.101.044307Phys. Rev. C. 10144307Léo Neufcourt, Yuchen Cao, Samuel A. Giuliani, Witold Nazarewicz, Erik Olsen, and Oleg B. Tarasov, "Quantified limits of the nuclear landscape," Phys. Rev. C 101, 044307 (2020).</p>
<p>Nuclear mass predictions for the crustal composition of neutron stars: A Bayesian neural network approach. R Utama, J Piekarewicz, H B Prosper, 10.1103/PhysRevC.93.014311Phys. Rev. C. 9314311R. Utama, J. Piekarewicz, and H. B. Prosper, "Nuclear mass predictions for the crustal composition of neutron stars: A Bayesian neural network approach," Phys. Rev. C 93, 014311 (2016).</p>
<p>Refining mass formulas for astrophysical applications: A Bayesian neural network approach. R Utama, J Piekarewicz, 10.1103/PhysRevC.96.044308Phys. Rev. C. 9644308R. Utama and J. Piekarewicz, "Refining mass formulas for astrophysical applications: A Bayesian neural network approach," Phys. Rev. C 96, 044308 (2017).</p>
<p>Nuclear mass predictions based on Bayesian neural network approach with pairing and shell effects. Z M Niu, H Z Liang, 10.1016/j.physletb.2018.01.002Phys. Lett. B. 778Z.M. Niu and H.Z. Liang, "Nuclear mass predictions based on Bayesian neural network approach with pairing and shell effects," Phys. Lett. B 778, 48 -53 (2018).</p>
<p>Bayesian approach to model-based extrapolation of nuclear observables. L Neufcourt, Y Cao, W Nazarewicz, F Viens, 10.1103/PhysRevC.98.034318Phys. Rev. C. 9834318L. Neufcourt, Y. Cao, W. Nazarewicz, and F. Viens, "Bayesian approach to model-based extrapola- tion of nuclear observables," Phys. Rev. C 98, 034318 (2018).</p>
<p>Neutron drip line in the Ca region from Bayesian model averaging. Léo Neufcourt, Yuchen Cao, Witold Nazarewicz, Erik Olsen, Frederi Viens, 10.1103/PhysRevLett.122.062502Phys. Rev. Lett. 12262502Léo Neufcourt, Yuchen Cao, Witold Nazarewicz, Erik Olsen, and Frederi Viens, "Neutron drip line in the Ca region from Bayesian model averaging," Phys. Rev. Lett. 122, 062502 (2019).</p>
<p>Beyond the proton drip line: Bayesian analysis of proton-emitting nuclei. L Neufcourt, Y Cao, S A Giuliani, W Nazarewicz, E Olsen, O B Tarasov, Phys. Rev. C. pressL. Neufcourt, Y. Cao, S. A. Giuliani, W. Nazarewicz, E. Olsen, and O. B. Tarasov, "Beyond the proton drip line: Bayesian analysis of proton-emitting nuclei," Phys. Rev. C, in press (2020).</p>
<p>Comparative study of radial basis function and bayesian neural network approaches in nuclear mass predictions. Z M Niu, J Y Fang, Y F Niu, 10.1103/PhysRevC.100.054311Phys. Rev. C. 10054311Z. M. Niu, J. Y. Fang, and Y. F. Niu, "Comparative study of radial basis function and bayesian neural network approaches in nuclear mass predictions," Phys. Rev. C 100, 054311 (2019).</p>
<p>Propagation of statistical uncertainties of Skyrme mass models to simulations of r-process nucleosynthesis. T M Sprouse, R Perez, R Surman, M R Mumpower, G C Mclaughlin, N Schunck, Phys. Rev. C. T. M. Sprouse, R. Navarro Perez, R. Surman, M. R. Mumpower, G. C. McLaughlin, and N. Schunck, "Propagation of statistical uncertainties of Skyrme mass models to simulations of r-process nucle- osynthesis," Phys. Rev. C (2020).</p>
<p>Impact of statistical uncertainties on the composition of the outer crust of a neutron star. A Pastore, D Neill, H Powell, K Medler, C Barton, arXiv:1912.11365A. Pastore, D. Neill, H. Powell, K. Medler, and C. Barton, "Impact of statistical uncertainties on the composition of the outer crust of a neutron star," arXiv:1912.11365 (2019).</p>
<p>Nuclear charge radii: density functional theory meets Bayesian neural networks. R Utama, W C Chen, J Piekarewicz, 10.1088/0954-3899/43/11/114002J. Phys. G. 43114002R. Utama, W. C. Chen, and J. Piekarewicz, "Nuclear charge radii: density functional theory meets Bayesian neural networks," J. Phys. G 43, 114002 (2016).</p>
<p>Predictions of nuclear charge radii and physical interpretations based on the naive Bayesian probability classifier. Yunfei Ma, Chen Su, Jian Liu, Zhongzhou Ren, Chang Xu, Yonghao Gao, 10.1103/PhysRevC.101.014304Phys. Rev. C. 10114304Yunfei Ma, Chen Su, Jian Liu, Zhongzhou Ren, Chang Xu, and Yonghao Gao, "Predictions of nuclear charge radii and physical interpretations based on the naive Bayesian probability classifier," Phys. Rev. C 101, 014304 (2020).</p>
<p>Improvement Studies of an Effective Interaction for N=Z sd-shell Nuclei by Neural Networks. Nadjet Serkan Akkoyun, Fatima Laouet, Benrachi, arXiv:2001.08561nucl-thSerkan Akkoyun, Nadjet Laouet, and Fatima Benrachi, "Improvement Studies of an Effective Inter- action for N=Z sd-shell Nuclei by Neural Networks," (2020), arXiv:2001.08561 [nucl-th].</p>
<p>First Excited 2+ Energy State Estimations of Even-even Nuclei by Using Artificial Neural Networks. S Akkoyun, H Kaya, arXiv:2002.08218nucl-thS. Akkoyun and H. Kaya, "First Excited 2+ Energy State Estimations of Even-even Nuclei by Using Artificial Neural Networks," (2020), arXiv:2002.08218 [nucl-th].</p>
<p>Taming nuclear complexity with a committee of multilayer neural networks. R D Lasseri, D Regnier, J P Ebran, A Penon, arXiv:1910.04132Phys. Rev. Lett. nucl-thR. D. Lasseri, D. Regnier, J. P. Ebran, and A. Penon, "Taming nuclear complexity with a committee of multilayer neural networks," Phys. Rev. Lett.. (2020), arXiv:1910.04132 [nucl-th].</p>
<p>Predictions of nuclear β-decay half-lives with machine learning and their impact on r-process nucleosynthesis. Z M Niu, H Z Liang, B H Sun, W H Long, Y F Niu, 10.1103/PhysRevC.99.064307Phys. Rev. C. 9964307Z. M. Niu, H. Z. Liang, B. H. Sun, W. H. Long, and Y. F. Niu, "Predictions of nuclear β-decay half-lives with machine learning and their impact on r-process nucleosynthesis," Phys. Rev. C 99, 064307 (2019).</p>
<p>Statistical Global Modeling of Beta--Decay Halflives Systematics Using Multilayer Feedforward Neural Networks and Support Vector Machines. N J Costiris, E Mavrommatis, K A Gernoth, J W Clark, H Li, arXiv:0809.038317th Symposium of the HELLENIC NUCLEAR PHYSICS SOCIETY ON NUCLEAR Physics Ioannina. Greecenucl-thN. J. Costiris, E. Mavrommatis, K. A. Gernoth, J. W. Clark, and H. Li, "Statistical Global Modeling of Beta--Decay Halflives Systematics Using Multilayer Feedforward Neural Networks and Sup- port Vector Machines," in 17th Symposium of the HELLENIC NUCLEAR PHYSICS SOCIETY ON NUCLEAR Physics Ioannina, Greece, May 30-31, 2008 (2008) arXiv:0809.0383 [nucl-th].</p>
<p>Alpha half-lives calculation of superheavy nuclei with Qα-value predictions based on the Bayesian neural network approach. Cristofher Zuñiga Ubaldo Baños Rodríguez, Marcello Vargas, Sergio Gonçalves, Fernando Barbosa Duarte, Guzmán, 10.1088/1361-6471/ab2c86J. Phys. 46115109Ubaldo Baños Rodríguez, Cristofher Zuñiga Vargas, Marcello Gonçalves, Sergio Barbosa Duarte, and Fernando Guzmán, "Alpha half-lives calculation of superheavy nuclei with Qα-value predictions based on the Bayesian neural network approach," J. Phys. G46, 115109 (2019).</p>
<p>Bayesian Neural Network improvements to nuclear mass formulae and predictions in the SuperHeavy Elements region. Cristofher Zuñiga Ubaldo Baños Rodríguez, Marcello Vargas, Sergio Gonçalves, Fernando Barbosa Duarte, Guzmán, 10.1209/0295-5075/127/42001EPL. 12742001Ubaldo Baños Rodríguez, Cristofher Zuñiga Vargas, Marcello Gonçalves, Sergio Barbosa Duarte, and Fernando Guzmán, "Bayesian Neural Network improvements to nuclear mass formulae and predictions in the SuperHeavy Elements region," EPL 127, 42001 (2019).</p>
<p>Bayesian evaluation of incomplete fission yields. Zi-Ao Wang, Junchen Pei, Yue Liu, Yu Qiang, 10.1103/PhysRevLett.123.122501Phys. Rev. Lett. 123122501Zi-Ao Wang, Junchen Pei, Yue Liu, and Yu Qiang, "Bayesian evaluation of incomplete fission yields," Phys. Rev. Lett. 123, 122501 (2019).</p>
<p>Constraining fission yields using machine learning. A Lovell, A Mohan, P Talou, M Chertkov, 10.1051/epjconf/201921104006EPJ Web Conf. 2114006A. Lovell, A. Mohan, P. Talou, and M. Chertkov, "Constraining fission yields using machine learn- ing," EPJ Web Conf. 211, 04006 (2019).</p>
<p>Exploring Bayesian parameter estimation for chiral effective field theory using nucleon-nucleon phase shifts. S Wesolowski, R J Furnstahl, J A Melendez, D R Phillips, 10.1088/1361-6471/aaf5fcarXiv:1808.08211J. Phys. 4645102nucl-thS. Wesolowski, R. J. Furnstahl, J. A. Melendez, and D. R. Phillips, "Exploring Bayesian parameter estimation for chiral effective field theory using nucleon-nucleon phase shifts," J. Phys. G46, 045102 (2019), arXiv:1808.08211 [nucl-th].</p>
<p>Quantifying uncertainties in neutron-alpha scattering with chiral nucleon-nucleon and three-nucleon forces. Konstantinos Kravvaris, Kevin R Quinlan, Sofia Quaglioni, Kyle A Wendt, Petr Navratil, arXiv:2004.08474nucl-thKonstantinos Kravvaris, Kevin R. Quinlan, Sofia Quaglioni, Kyle A. Wendt, and Petr Navratil, "Quantifying uncertainties in neutron-alpha scattering with chiral nucleon-nucleon and three-nucleon forces," (2020), arXiv:2004.08474 [nucl-th].</p>
<p>Direct comparison between Bayesian and frequentist uncertainty quantification for nuclear reactions. G B King, A E Lovell, L Neufcourt, F M Nunes, 10.1103/PhysRevLett.122.232502arXiv:1905.05072Phys. Rev. Lett. 122232502nucl-thG. B. King, A. E. Lovell, L. Neufcourt, and F. M. Nunes, "Direct comparison between Bayesian and frequentist uncertainty quantification for nuclear reactions," Phys. Rev. Lett. 122, 232502 (2019), arXiv:1905.05072 [nucl-th].</p>
<p>Constraining Transfer Cross Sections Using Bayes' Theorem. A E Lovell, F M Nunes, 10.1103/PhysRevC.97.064612arXiv:1801.06096Phys. Rev. 9764612nucl-thA. E. Lovell and F. M. Nunes, "Constraining Transfer Cross Sections Using Bayes' Theorem," Phys. Rev. C97, 064612 (2018), arXiv:1801.06096 [nucl-th].</p>
<p>Exploring experimental conditions to reduce uncertainties in the optical potential. M Catacora-Rios, G B King, A E Lovell, F M Nunes, 10.1103/PhysRevC.100.064615Phys. Rev. 10064615M. Catacora-Rios, G. B. King, A. E. Lovell, and F. M. Nunes, "Exploring experimental conditions to reduce uncertainties in the optical potential," Phys. Rev. C100, 064615 (2019).</p>
<p>Isotopic cross-sections in proton induced spallation reactions based on the Bayesian neural network method. Chun-Wang Ma, Dan Peng, Hui-Ling Wei, Zhong-Ming Niu, Yu-Ting Wang, R Wada, 10.1088/1674-1137/44/1/014104Chinese Phys. C. 4414104Chun-Wang Ma, Dan Peng, Hui-Ling Wei, Zhong-Ming Niu, Yu-Ting Wang, and R. Wada, "Isotopic cross-sections in proton induced spallation reactions based on the Bayesian neural network method," Chinese Phys. C 44, 014104 (2020).</p>
<p>Estimation of fusion reaction cross-sections by artificial neural networks. S Akkoyun, 10.1016/j.nimb.2019.11.014NIM B. 462S. Akkoyun, "Estimation of fusion reaction cross-sections by artificial neural networks," NIM B 462, 51 -54 (2020).</p>
<p>Bayesian truncation errors in chiral effective field theory: nucleon-nucleon observables. J A Melendez, S Wesolowski, R J Furnstahl, 10.1103/PhysRevC.96.024003arXiv:1704.03308Phys. Rev. 9624003nucl-thJ. A. Melendez, S. Wesolowski, and R. J. Furnstahl, "Bayesian truncation errors in chiral effec- tive field theory: nucleon-nucleon observables," Phys. Rev. C96, 024003 (2017), arXiv:1704.03308 [nucl-th].</p>
<p>Extrapolation of nuclear structure observables with artificial neural networks. W G Jiang, G Hagen, T Papenbrock, 10.1103/PhysRevC.100.054326Phys. Rev. C. 10054326W. G. Jiang, G. Hagen, and T. Papenbrock, "Extrapolation of nuclear structure observables with artificial neural networks," Phys. Rev. C 100, 054326 (2019).</p>
<p>Deep learning: Extrapolation tool for ab initio nuclear theory. Gianina Alina Negoita, James P Vary, Glenn R Luecke, Pieter Maris, Andrey M Shirokov, Jae Ik, Youngman Shin, Esmond G Kim, Chao Ng, Matthew Yang, Gurpur M Lockner, Prabhu, 10.1103/PhysRevC.99.054308Phys. Rev. C. 9954308Gianina Alina Negoita, James P. Vary, Glenn R. Luecke, Pieter Maris, Andrey M. Shirokov, Ik Jae Shin, Youngman Kim, Esmond G. Ng, Chao Yang, Matthew Lockner, and Gurpur M. Prabhu, "Deep learning: Extrapolation tool for ab initio nuclear theory," Phys. Rev. C 99, 054308 (2019).</p>
<p>Uncertainty quantification for nuclear density functional theory and information content of new measurements. J D Mcdonnell, N Schunck, D Higdon, J Sarich, S M Wild, W Nazarewicz, 10.1103/PhysRevLett.114.122501Phys. Rev. Lett. 114122501J. D. McDonnell, N. Schunck, D. Higdon, J. Sarich, S. M. Wild, and W. Nazarewicz, "Uncertainty quantification for nuclear density functional theory and information content of new measurements," Phys. Rev. Lett. 114, 122501 (2015).</p>
<p>Uncertainty quantification in the nuclear shell model. Sota Yoshida, Noritaka Shimizu, Tomoaki Togashi, Takaharu Otsuka, 10.1103/PhysRevC.98.061301Phys. Rev. C. 9861301Sota Yoshida, Noritaka Shimizu, Tomoaki Togashi, and Takaharu Otsuka, "Uncertainty quantifica- tion in the nuclear shell model," Phys. Rev. C 98, 061301 (2018).</p>
<p>Non-parametric Bayesian approach to extrapolation problems in configuration interaction methods. Sota Yoshida, arXiv:1907.04974nucl-thSota Yoshida, "Non-parametric Bayesian approach to extrapolation problems in configuration inter- action methods," (2019), arXiv:1907.04974 [nucl-th].</p>
<p>Bayesian optimization in ab initio nuclear physics. A Ekström, C Forssén, Dimitrakakis, H Dubhashi, A S T Johansson, H Muhammad, A Salomonsson, Schliep, 10.1088/1361-6471/ab2b14J. Phys. G. 4695101A Ekström, C Forssén, C Dimitrakakis, D Dubhashi, H T Johansson, A S Muhammad, H Salomon- sson, and A Schliep, "Bayesian optimization in ab initio nuclear physics," J. Phys. G 46, 095101 (2019).</p>
<p>Global sensitivity analysis of bulk properties of an atomic nucleus. Andreas Ekström, Gaute Hagen, 10.1103/PhysRevLett.123.252501Phys. Rev. Lett. 123252501Andreas Ekström and Gaute Hagen, "Global sensitivity analysis of bulk properties of an atomic nucleus," Phys. Rev. Lett. 123, 252501 (2019).</p>
<p>Ab initio models of atomic nuclei: challenges and new ideas. A Ekström, arXiv:1912.02227nucl-thA. Ekström, "Ab initio models of atomic nuclei: challenges and new ideas," (2019), 1912.02227, arXiv:1912.02227 [nucl-th].</p>
<p>Statistical aspects of nuclear mass models. Vojtech Kejzlar, Léo Neufcourt, Witold Nazarewicz, Paul-Gerhard Reinhard, arXiv:2002.04151nucl-thVojtech Kejzlar, Léo Neufcourt, Witold Nazarewicz, and Paul-Gerhard Reinhard, "Statistical aspects of nuclear mass models," (2020), 2002.04151, arXiv:2002.04151 [nucl-th].</p>
<p>Machine learning the deuteron. J W T Keeble, A Rios, arXiv:1911.13092nucl-thJ. W. T. Keeble and A. Rios, "Machine learning the deuteron," (2019), arXiv:1911.13092 [nucl-th].</p>
<p>Microscopic theory of nuclear fission: a review. N Schunck, L M Robledo, 10.1088/0034-4885/79/11/116301Rep. Prog. Phys. 79116301N. Schunck and L. M. Robledo, "Microscopic theory of nuclear fission: a review," Rep. Prog. Phys. 79, 116301 (2016).</p>
<p>Consistent empirical physical formulas for potential energy curves of 38-66ti isotopes by using neural networks. S Akkoyun, T Bayram, S O Kara, N Yildiz, 10.1134/S1547477113060022Phys. Part. Nucl. Lett. 10S. Akkoyun, T. Bayram, S. O. Kara, and N. Yildiz, "Consistent empirical physical formulas for potential energy curves of 38-66ti isotopes by using neural networks," Phys. Part. Nucl. Lett. 10, 528-534 (2013).</p>
<p>r-process nucleosynthesis: connecting rare-isotope beam facilities with the cosmos. C J Horowitz, 10.1088/1361-6471/ab0849J. Phys. G. 4683001C. J. Horowitz et al., "r-process nucleosynthesis: connecting rare-isotope beam facilities with the cosmos," J. Phys. G 46, 083001 (2019).</p>
<p>Microscopically based energy density functionals for nuclei using the density matrix expansion. ii. full optimization and validation. R Navarro Pérez, N Schunck, A Dyhdalo, R J Furnstahl, S K Bogner, 10.1103/PhysRevC.97.054304Phys. Rev. C. 9754304R. Navarro Pérez, N. Schunck, A. Dyhdalo, R. J. Furnstahl, and S. K. Bogner, "Microscopically based energy density functionals for nuclei using the density matrix expansion. ii. full optimization and validation," Phys. Rev. C 97, 054304 (2018).</p>
<p>Nuclear energy density optimization: Shell structure. M Kortelainen, J Mcdonnell, W Nazarewicz, E Olsen, P.-G Reinhard, J Sarich, N Schunck, S M Wild, D Davesne, J Erler, A Pastore, 10.1103/PhysRevC.89.054314Phys. Rev. C. 8954314M. Kortelainen, J. McDonnell, W. Nazarewicz, E. Olsen, P.-G. Reinhard, J. Sarich, N. Schunck, S. M. Wild, D. Davesne, J. Erler, and A. Pastore, "Nuclear energy density optimization: Shell structure," Phys. Rev. C 89, 054314 (2014).</p>
<p>Nuclear binding near a quantum phase transition. Serdar Elhatisari, Ning Li, Alexander Rokash, Jose Manuel Alarcón, Dechuan Du, Nico Klein, Bingnan Lu, - G Ulf, Evgeny Meißner, Hermann Epelbaum, Timo A Krebs, Dean Lähde, Gautam Lee, Rupak, 10.1103/PhysRevLett.117.132501arXiv:1602.04539Phys. Rev. Lett. 117132501nucl-thSerdar Elhatisari, Ning Li, Alexander Rokash, Jose Manuel Alarcón, Dechuan Du, Nico Klein, Bing- nan Lu, Ulf-G. Meißner, Evgeny Epelbaum, Hermann Krebs, Timo A. Lähde, Dean Lee, and Gautam Rupak, "Nuclear binding near a quantum phase transition," Phys. Rev. Lett. 117, 132501 (2016), arXiv:1602.04539 [nucl-th].</p>
<p>Ab initio Calculations of the Isotopic Dependence of Nuclear Clustering. Serdar Elhatisari, Evgeny Epelbaum, Hermann Krebs, Timo A Lähde, Dean Lee, Ning Li, Bing-Nan Lu, - G Ulf, Gautam Meißner, Rupak, 10.1103/PhysRevLett.119.222505arXiv:1702.05177Phys. Rev. Lett. 119222505nucl-thSerdar Elhatisari, Evgeny Epelbaum, Hermann Krebs, Timo A. Lähde, Dean Lee, Ning Li, Bing-nan Lu, Ulf-G. Meißner, and Gautam Rupak, "Ab initio Calculations of the Isotopic Dependence of Nuclear Clustering," Phys. Rev. Lett. 119, 222505 (2017), arXiv:1702.05177 [nucl-th].</p>
<p>Microscopic Clustering in Light Nuclei. Martin Freer, Hisashi Horiuchi, Yoshiko Kanada-En&apos;yo, Dean Lee, Ulf-G Meißner, 10.1103/RevModPhys.90.035004arXiv:1705.06192Rev. Mod. Phys. 9035004nucl-thMartin Freer, Hisashi Horiuchi, Yoshiko Kanada-En'yo, Dean Lee, and Ulf-G. Meißner, "Micro- scopic Clustering in Light Nuclei," Rev. Mod. Phys. 90, 035004 (2018), arXiv:1705.06192 [nucl-th].</p>
<p>Clustering of Four-Component Unitary Fermions. William G Dawkins, J Carlson, U Van Kolck, Alexandros Gezerlis, arXiv:1908.04288cond-mat.quant-gasWilliam G. Dawkins, J. Carlson, U. van Kolck, and Alexandros Gezerlis, "Clustering of Four- Component Unitary Fermions," (2019), arXiv:1908.04288 [cond-mat.quant-gas].</p>
<p>Pairing correlations and eigenvalues of two-body density matrix in atomic nuclei. Michelangelo Sambataro, Nicolae Sandulescu, 10.1016/j.aop.2019.168061arXiv:1911.04408Annals Phys. 413168061nucl-thMichelangelo Sambataro and Nicolae Sandulescu, "Pairing correlations and eigenvalues of two-body density matrix in atomic nuclei," Annals Phys. 413, 168061 (2020), arXiv:1911.04408 [nucl-th].</p>
<p>Probing ab initio emergence of nuclear rotation. M A Caprio, P J Fasano, P Maris, A E Mccoy, J P Vary, arXiv:1912.00083nucl-thM. A. Caprio, P. J. Fasano, P. Maris, A. E. McCoy, and J. P. Vary, "Probing ab initio emergence of nuclear rotation," (2019), arXiv:1912.00083 [nucl-th].</p>
<p>Constraining the symmetry energy with heavy-ion collisions and bayesian analyses. P Morfouace, C Y Tsang, Y Zhang, W G Lynch, M B Tsang, D D S Coupland, M Youngs, Z Chajecki, M A Famiano, T K Ghosh, G Jhang, Jenny Lee, H Liu, A Sanetullaev, R Showalter, J Winkelbauer, 10.1016/j.physletb.2019.135045Phys. Lett. B. 799135045P. Morfouace, C.Y. Tsang, Y. Zhang, W.G. Lynch, M.B. Tsang, D.D.S. Coupland, M. Youngs, Z. Cha- jecki, M.A. Famiano, T.K. Ghosh, G. Jhang, Jenny Lee, H. Liu, A. Sanetullaev, R. Showalter, and J. Winkelbauer, "Constraining the symmetry energy with heavy-ion collisions and bayesian analy- ses," Phys. Lett. B 799, 135045 (2019).</p>
<p>Insights on skyrme parameters from gw170817. C Y Tsang, M B Tsang, F J Danielewicz, W G Fattoyev, Lynch, 10.1016/j.physletb.2019.05.055Phys. Lett. B. 796C.Y. Tsang, M.B. Tsang, Pawel Danielewicz, F.J. Fattoyev, and W.G. Lynch, "Insights on skyrme parameters from gw170817," Phys. Lett. B 796, 1 -5 (2019).</p>
<p>Bayesian modeling of the nuclear equation of state for neutron star tidal deformabilities and GW170817. Y Lim, J W Holt, 10.1140/epja/i2019-12917-9EPJA. 55209Y. Lim and J. W. Holt, "Bayesian modeling of the nuclear equation of state for neutron star tidal deformabilities and GW170817," EPJA 55, 209 (2019).</p>
<p>Machine Learning to Enable Orders of Magnitude Speedup in Multi-Objective Optimization of Particle Accelerator Systems. A Edelen, N Neveu, Y Huber, M Frey, A Mayes, Adelmann, arXiv:1903.07759arXiv:1903.07759A. Edelen, N. Neveu, Y. Huber, M. Frey, C Mayes, and A. Adelmann, "Machine Learning to Enable Orders of Magnitude Speedup in Multi-Objective Optimization of Particle Accelerator Systems," arXiv:1903.07759 (2019), arXiv:1903.07759.</p>
<p>Srf cavity fault classification using machine learning at cebaf. A Solopova, A Carpenter, T Powers, Y Roblin, C Tennant, K Iftekharuddin, L Vidyaratne, Proceedings of the 2019 International Particle Accelerator Conference. the 2019 International Particle Accelerator ConferenceMelbourne2019A. Solopova, A. Carpenter, T. Powers, Y. Roblin, C. Tennant, K. Iftekharuddin, and L. Vidyaratne, "Srf cavity fault classification using machine learning at cebaf," Proceedings of the 2019 International Particle Accelerator Conference, Melbourne, 2019 (2019).</p>
<p>Predicting particle accelerator failures using binary classifiers. M Rescic, R Seviour, W Blokland, Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment. 955163240M. Rescic, R. Seviour, and W. Blokland, "Predicting particle accelerator failures using binary clas- sifiers," Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrome- ters, Detectors and Associated Equipment 955, 163240 (2020).</p>
<p>Optimization of beam loss monitor network for fault modes. Z Liu, Z He, S M Lidia, D Liu, Q Zhao, Proceedings of the 6th International Particle Accelerator Conference. the 6th International Particle Accelerator ConferenceRichmond, VAZ. Liu, Z. He, S.M. Lidia, D. Liu, and Q. Zhao, "Optimization of beam loss monitor network for fault modes," Proceedings of the 6th International Particle Accelerator Conference, Richmond, VA, May 2015 (2015).</p>
<p>Unsupervised machine learning for detection of faulty bpms. E Fol, J Coello De Portugal, R Tomas, Proceedings of the 2019 International Particle Accelerator Conference. the 2019 International Particle Accelerator ConferenceMelbourne2019E. Fol, J.M Coello de Portugal, and R. Tomas, "Unsupervised machine learning for detection of faulty bpms," Proceedings of the 2019 International Particle Accelerator Conference, Melbourne, 2019 (2019).</p>
<p>Experimental test of an online ion-optics optimizer. A Amthor, Z M Schillaci, D Morrissey, M Portillo, S Schwarz, M Steiner, C S Sumithrarachchi, 10.1016/j.nima.2018.04.001Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment. 895A. Amthor, Z.M. Schillaci, D. Morrissey, M. Portillo, S. Schwarz, M. Steiner, and C.S. Sum- ithrarachchi, "Experimental test of an online ion-optics optimizer," Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment 895 (2018), 10.1016/j.nima.2018.04.001.</p>
<p>A convolutional neural network neutrino event classifier. A Aurisano, A Radovic, D Rocco, A Himmel, M D Messier, E Niner, G Pawloski, F Psihas, A Sousa, P Vahle, Journal of Instrumentation. 119001A. Aurisano, A. Radovic, D. Rocco, A. Himmel, M.D. Messier, E. Niner, G. Pawloski, F. Psihas, A. Sousa, and P. Vahle, "A convolutional neural network neutrino event classifier," Journal of Instru- mentation 11, P09001 (2016).</p>
<p>Machine learning methods for track classification in the at-tpc. Raghuram Michelle P Kuchera, Ramanujan, Z Jack, Ryan R Taylor, Daniel Strauss, Joshua Bazin, Ruiming Bradt, Chen, Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment. Michelle P Kuchera, Raghuram Ramanujan, Jack Z Taylor, Ryan R Strauss, Daniel Bazin, Joshua Bradt, and Ruiming Chen, "Machine learning methods for track classification in the at-tpc," Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment (2019).</p>
<p>Deep neural networks for energy and position reconstruction in EXO-200. S Delaquis, 10.1088/1748-0221/13/08/p08023Journal of Instrumentation. 13S. Delaquis et al., "Deep neural networks for energy and position reconstruction in EXO-200," Jour- nal of Instrumentation 13, P08023-P08023 (2018).</p>
<p>A Deep Neural Network for Pixel-Level Electromagnetic Particle Identification in the MicroBooNE Liquid Argon Time Projection Chamber. C Adams, MicroBooNEarXiv:1808.07269physics.ins-detC. Adams et al. (MicroBooNE), "A Deep Neural Network for Pixel-Level Electromagnetic Particle Identification in the MicroBooNE Liquid Argon Time Projection Chamber," (2018), arXiv:1808.07269 [physics.ins-det].</p>
<p>Automated Discovery of Jet Substructure Analyses. Yue Shi, Lai , arXiv:1810.00835nuclthYue Shi Lai, "Automated Discovery of Jet Substructure Analyses," (2018), arXiv:1810.00835 [nucl- th].</p>
<p>A new Transition Radiation detector based on GEM technology. F Barbosa, 10.1016/j.nima.2019.162356Nucl. Instrum. Meth. 942162356F. Barbosa et al., "A new Transition Radiation detector based on GEM technology," Nucl. Instrum. Meth. A942, 162356 (2019).</p>
<p>DeepRICH: Learning Deeply Cherenkov Detectors. Cristiano Fanelli, Jary Pomponi, arXiv:1911.11717physics.data-anCristiano Fanelli and Jary Pomponi, "DeepRICH: Learning Deeply Cherenkov Detectors," (2019), arXiv:1911.11717 [physics.data-an].</p>
<p>AI-optimized detector design for the future Electron-Ion Collider: the dual-radiator RICH case. E Cisbani, arXiv:1911.05797physics.ins-detE. Cisbani et al., "AI-optimized detector design for the future Electron-Ion Collider: the dual-radiator RICH case," (2019), arXiv:1911.05797 [physics.ins-det].</p>
<p>Simulation of electron-proton scattering events by a Feature-Augmented and Transformed Generative Adversarial Network (FAT-GAN). N Yasir Alanazi, Tianbo Sato, W Liu, Michelle P Melnitchouk, Evan Kuchera, Michael Pritchard, Ryan Robertson, Luisa Strauss, Yaohang Velasco, Li, arXiv:2001.11103arXiv:2001.11103hep-phYasir Alanazi, N. Sato, Tianbo Liu, W. Melnitchouk, Michelle P. Kuchera, Evan Pritchard, Michael Robertson, Ryan Strauss, Luisa Velasco, and Yaohang Li, "Simulation of electron-proton scatter- ing events by a Feature-Augmented and Transformed Generative Adversarial Network (FAT-GAN)," arXiv:2001.11103 (2020), arXiv:2001.11103 [hep-ph].</p>
<p>Parton Distributions with Theory Uncertainties: General Formalism and First Phenomenological Studies. Rabah Abdul Khalek, NNPDF10.1140/epjc/s10052-019-7401-4arXiv:1906.10698Eur. Phys. J. C. 79931hep-phRabah Abdul Khalek et al. (NNPDF), "Parton Distributions with Theory Uncertainties: General Formalism and First Phenomenological Studies," Eur. Phys. J. C 79, 931 (2019), arXiv:1906.10698 [hep-ph].</p>
<p>Strange quark suppression from a simultaneous Monte Carlo analysis of parton distributions and fragmentation functions. N Sato, JAMC Andres, JAMJ J Ethier, JAMW Melnitchouk, JAMarXiv:1905.03788hep-phN. Sato, C. Andres, J. J. Ethier, and W. Melnitchouk (JAM), "Strange quark suppression from a simultaneous Monte Carlo analysis of parton distributions and fragmentation functions," (2019), arXiv:1905.03788 [hep-ph].</p>
<p>First simultaneous extraction of spin-dependent parton distributions and fragmentation functions from a global QCD analysis. J J Ethier, N Sato, W Melnitchouk, 10.1103/PhysRevLett.119.132001arXiv:1705.05889Phys. Rev. Lett. 119132001hep-phJ. J. Ethier, N. Sato, and W. Melnitchouk, "First simultaneous extraction of spin-dependent parton distributions and fragmentation functions from a global QCD analysis," Phys. Rev. Lett. 119, 132001 (2017), arXiv:1705.05889 [hep-ph].</p>
<p>Machine Learning tools for global PDF fits. Juan Rojo, arXiv:1809.0439213th Conference on Quark Confinement and the Hadron Spectrum (Confinement XIII). Maynooth, Irelandhep-phJuan Rojo, "Machine Learning tools for global PDF fits," in 13th Conference on Quark Confinement and the Hadron Spectrum (Confinement XIII) Maynooth, Ireland, July 31-August 6, 2018 (2018) arXiv:1809.04392 [hep-ph].</p>
<p>The origin of single transverse-spin asymmetries in high-energy collisions. Justin Cammarota, Leonard Gamberg, Zhong-Bo Kang, Joshua A Miller, Daniel Pitonyak, Alexei Prokudin, Ted C Rogers, Nobuo Sato, arXiv:2002.08384hep-phJustin Cammarota, Leonard Gamberg, Zhong-Bo Kang, Joshua A. Miller, Daniel Pitonyak, Alexei Prokudin, Ted C. Rogers, and Nobuo Sato, "The origin of single transverse-spin asymmetries in high-energy collisions," (2020), arXiv:2002.08384 [hep-ph].</p>
<p>Neural network generated parametrizations of deeply virtual Compton form factors. Kresimir Kumericki, Dieter Mueller, Andreas Schafer, 10.1007/JHEP07(2011)073arXiv:1106.2808JHEP. 0773hep-phKresimir Kumericki, Dieter Mueller, and Andreas Schafer, "Neural network generated parametriza- tions of deeply virtual Compton form factors," JHEP 07, 073 (2011), arXiv:1106.2808 [hep-ph].</p>
<p>GPD phenomenology and DVCS fitting. Kresimir Kumericki, Simonetta Liuti, Herve Moutarde, 10.1140/epja/i2016-16157-3arXiv:1602.02763Eur. Phys. J. A52. 157hep-phKresimir Kumericki, Simonetta Liuti, and Herve Moutarde, "GPD phenomenology and DVCS fit- ting," Eur. Phys. J. A52, 157 (2016), arXiv:1602.02763 [hep-ph].</p>
<p>Parton distributions and lattice QCD calculations: a community white paper. Huey-Wen Lin, 10.1016/j.ppnp.2018.01.007arXiv:1711.07916Prog. Part. Nucl. Phys. 100hep-phHuey-Wen Lin et al., "Parton distributions and lattice QCD calculations: a community white paper," Prog. Part. Nucl. Phys. 100, 107-160 (2018), arXiv:1711.07916 [hep-ph].</p>
<p>A Brief Introduction to PYTHIA 8.1. Torbjorn Sjostrand, Stephen Mrenna, Peter Z Skands, 10.1016/j.cpc.2008.01.036arXiv:0710.3820Comput. Phys. Commun. 178hep-phTorbjorn Sjostrand, Stephen Mrenna, and Peter Z. Skands, "A Brief Introduction to PYTHIA 8.1," Comput. Phys. Commun. 178, 852-867 (2008), arXiv:0710.3820 [hep-ph].</p>
<p>Principal component analysis of event-by-event fluctuations. Rajeev S Bhalerao, Jean-Yves Ollitrault, Subrata Pal, Derek Teaney, 10.1103/PhysRevLett.114.152301Phys. Rev. Lett. 114152301Rajeev S. Bhalerao, Jean-Yves Ollitrault, Subrata Pal, and Derek Teaney, "Principal component analysis of event-by-event fluctuations," Phys. Rev. Lett. 114, 152301 (2015).</p>
<p>Subleading harmonic flows in hydrodynamic simulations of heavy ion collisions. Aleksas Mazeliauskas, Derek Teaney, 10.1103/PhysRevC.91.044902Phys. Rev. C. 9144902Aleksas Mazeliauskas and Derek Teaney, "Subleading harmonic flows in hydrodynamic simulations of heavy ion collisions," Phys. Rev. C 91, 044902 (2015).</p>
<p>Principal Component Analysis of collective flow in Relativistic Heavy-Ion Collisions. Ziming Liu, Wenbin Zhao, Huichao Song, 10.1140/epjc/s10052-019-7379-yEur. Phys. J. C. 79870Ziming Liu, Wenbin Zhao, and Huichao Song, "Principal Component Analysis of collective flow in Relativistic Heavy-Ion Collisions," Eur. Phys. J. C 79, 870 (2019).</p>
<p>Constraining the Eq. of State of Super-Hadronic Matter from Heavy-Ion Collisions. Scott Pratt, Evan Sangaline, Paul Sorensen, Hui Wang, 10.1103/PhysRevLett.114.202301Phys. Rev. Lett. 114202301Scott Pratt, Evan Sangaline, Paul Sorensen, and Hui Wang, "Constraining the Eq. of State of Super- Hadronic Matter from Heavy-Ion Collisions," Phys. Rev. Lett. 114, 202301 (2015).</p>
<p>Applying Bayesian parameter estimation to relativistic heavy-ion collisions: simultaneous characterization of the initial state and quark-gluon plasma medium. Jonah E Bernhard, J Scott Moreland, Steffen A Bass, Jia Liu, Ulrich Heinz, 10.1103/PhysRevC.94.024907Phys. Rev. C. 9424907Jonah E. Bernhard, J. Scott Moreland, Steffen A. Bass, Jia Liu, and Ulrich Heinz, "Applying Bayesian parameter estimation to relativistic heavy-ion collisions: simultaneous characterization of the initial state and quark-gluon plasma medium," Phys. Rev. C 94, 024907 (2016).</p>
<p>Bayesian estimation of the specific shear and bulk viscosity of quark-gluon plasma. Jonah E Bernhard, J Scott Moreland, Steffen A Bass, 10.1038/s41567-019-0611-8Nature Phys. 15Jonah E. Bernhard, J. Scott Moreland, and Steffen A. Bass, "Bayesian estimation of the specific shear and bulk viscosity of quark-gluon plasma," Nature Phys. 15, 1113-1117 (2019).</p>
<p>. J F Paquet, JETSCAPERevisiting Bayesian constraints on the transport coefficients of QCD," (2020J. F. Paquet et al. (JETSCAPE), "Revisiting Bayesian constraints on the transport coefficients of QCD," (2020).</p>
<p>Data-driven analysis for the temperature and momentum dependence of the heavy-quark diffusion coefficient in relativistic heavy-ion collisions. Yingru Xu, Jonah E Bernhard, Steffen A Bass, Marlene Nahrgang, Shanshan Cao, 10.1103/PhysRevC.97.014907Phys. Rev. C. 9714907Yingru Xu, Jonah E. Bernhard, Steffen A. Bass, Marlene Nahrgang, and Shanshan Cao, "Data-driven analysis for the temperature and momentum dependence of the heavy-quark diffusion coefficient in relativistic heavy-ion collisions," Phys. Rev. C 97, 014907 (2018).</p>
<p>Bayesian extraction ofq with multi-stage jet evolution approach. Ron Soltz, Jetscape10.22323/1.345.00489th International Conference on Hard and Electromagnetic Probes of High-Energy Nuclear Collisions: Hard Probes 2018 (HP2018). Aix-Les-Bains, France48ProceedingsRon Soltz (Jetscape), "Bayesian extraction ofq with multi-stage jet evolution approach," Proceed- ings, 9th International Conference on Hard and Electromagnetic Probes of High-Energy Nuclear Collisions: Hard Probes 2018 (HP2018): Aix-Les-Bains, France, October 1-5, 2018, PoS Hard- Probes2018, 048 (2019).</p>
<p>Bayesian extraction of jet energy loss distributions in heavy-ion collisions. Yayun He, Long-Gang, Xin-Nian Pang, Wang, 10.1103/PhysRevLett.122.2523021808.05310Phys. Rev. Lett. 122252302Yayun He, Long-Gang Pang, and Xin-Nian Wang, "Bayesian extraction of jet energy loss distribu- tions in heavy-ion collisions," Phys. Rev. Lett. 122, 252302 (2019), 1808.05310.</p>
<p>Neural networks for impact parameter determination. S A Bass, A Bischoff, J A Maruhn, Horst Stoecker, W Greiner, 10.1103/PhysRevC.53.2358nucl-th/9601024Phys. Rev. C. 53S. A. Bass, A. Bischoff, J. A. Maruhn, Horst Stoecker, and W. Greiner, "Neural networks for impact parameter determination," Phys. Rev. C 53, 2358-2363 (1996), nucl-th/9601024.</p>
<p>Probing heavy ion collisions using quark and gluon jet substructure with machine learning. Yang-Ting Chien, 10.1016/j.nuclphysa.2018.11.009the 27th International Conference on Ultrarelativistic Nucleus-Nucleus Collisions: Quark Matter. 982Yang-Ting Chien, "Probing heavy ion collisions using quark and gluon jet substructure with machine learning," Nucl. Phys. A 982, 619 -622 (2019), the 27th International Conference on Ultrarelativistic Nucleus-Nucleus Collisions: Quark Matter 2018.</p>
<p>Energy flow polynomials: a complete linear basis for jet substructure. Patrick T Komiske, Eric M Metodiev, Jesse Thaler, 10.1007/JHEP04(2018)013JHEP. 201813Patrick T. Komiske, Eric M. Metodiev, and Jesse Thaler, "Energy flow polynomials: a complete linear basis for jet substructure," JHEP 2018, 13 (2018).</p>
<p>An equation-of-state-meter of quantum chromodynamics transition from deep learning. Long-Gang Pang, Kai Zhou, Nan Su, Hannah Petersen, Horst Stöcker, Xin-Nian Wang, 10.1038/s41467-017-02726-3Nature Commun. 9210Long-Gang Pang, Kai Zhou, Nan Su, Hannah Petersen, Horst Stöcker, and Xin-Nian Wang, "An equation-of-state-meter of quantum chromodynamics transition from deep learning," Nature Com- mun. 9, 210 (2018).</p>
<p>A machine learning study to identify spinodal clumping in high energy nuclear collisions. Jan Steinheimer, Longgang Pang, Kai Zhou, Volker Koch, Jørgen Randrup, Horst Stoecker, 10.1007/JHEP12(2019)122JHEP. 12122Jan Steinheimer, Longgang Pang, Kai Zhou, Volker Koch, Jørgen Randrup, and Horst Stoecker, "A machine learning study to identify spinodal clumping in high energy nuclear collisions," JHEP 12, 122 (2019).</p>
<p>Interpretable deep learning for nuclear deformation in heavy ion collisions. Long-Gang Pang, Kai Zhou, Xin-Nian Wang, arXiv:1906.06429nucl-thLong-Gang Pang, Kai Zhou, and Xin-Nian Wang, "Interpretable deep learning for nuclear deforma- tion in heavy ion collisions," (2019), arXiv:1906.06429 [nucl-th].</p>
<p>Applications of deep learning to relativistic hydrodynamics. Hengfeng Huang, Bowen Xiao, Huixin Xiong, Zeming Wu, Yadong Mu, Huichao Song, arXiv:1801.03334nucl-thHengfeng Huang, Bowen Xiao, Huixin Xiong, Zeming Wu, Yadong Mu, and Huichao Song, "Ap- plications of deep learning to relativistic hydrodynamics," (2018), arXiv:1801.03334 [nucl-th].</p>
<p>Cyclotron Radiation Emission Spectroscopy Signal Classification with Machine Learning in Project 8. A Esfahani, 10.1088/1367-2630/ab71bdarXiv:1909.08115New J. Phys. 2233004nucl-exA. Ashtari Esfahani et al., "Cyclotron Radiation Emission Spectroscopy Signal Classification with Machine Learning in Project 8," New J. Phys. 22, 033004 (2020), arXiv:1909.08115 [nucl-ex].</p>
<p>Background rejection in NEXT using deep neural networks. J Renner, 10.1088/1748-0221/12/01/t01004J. Instrum. 12J. Renner et al., "Background rejection in NEXT using deep neural networks," J. Instrum. 12, T01004-T01004 (2017).</p>
<p>WANDA: AI/ML for nuclear data. summary of the session on AI/ML at the workshop on applied nuclear data activities 2020. V Sobes, M Grosskopf, K Wendt, D Brown, M S Smith, P Talou, ORNL/TM-2020/1535V. Sobes, M. Grosskopf, K. Wendt, D. Brown, M. S. Smith, and P. Talou, "WANDA: AI/ML for nuclear data. summary of the session on AI/ML at the workshop on applied nuclear data activities 2020, March 3-5, 2020, ORNL/TM-2020/1535," .</p>
<p>Machine learning action parameters in lattice quantum chromodynamics. E Phiala, Daniel Shanahan, William Trewartha, Detmold, 10.1103/PhysRevD.97.094506arXiv:1801.05784Phys. Rev. 9794506hep-latPhiala E. Shanahan, Daniel Trewartha, and William Detmold, "Machine learning action parameters in lattice quantum chromodynamics," Phys. Rev. D97, 094506 (2018), arXiv:1801.05784 [hep-lat].</p>
<p>Lukas Kades, Jan M Pawlowski, Alexander Rothkopf, Manuel Scherzer, Julian M Urban, Sebastian J Wetzel, Nicolas Wink, Felix Ziegler, arXiv:1905.04305Spectral Reconstruction with Deep Neural Networks. physics.comp-phLukas Kades, Jan M. Pawlowski, Alexander Rothkopf, Manuel Scherzer, Julian M. Urban, Sebas- tian J. Wetzel, Nicolas Wink, and Felix Ziegler, "Spectral Reconstruction with Deep Neural Net- works," (2019), arXiv:1905.04305 [physics.comp-ph].</p>
<p>Reconstructing parton distribution functions from Ioffe time data: from Bayesian methods to Neural Networks. Joseph Karpie, Kostas Orginos, Alexander Rothkopf, Savvas Zafeiropoulos, 10.1007/JHEP04(2019)057arXiv:1901.05408JHEP. 0457hep-latJoseph Karpie, Kostas Orginos, Alexander Rothkopf, and Savvas Zafeiropoulos, "Reconstructing parton distribution functions from Ioffe time data: from Bayesian methods to Neural Networks," JHEP 04, 057 (2019), arXiv:1901.05408 [hep-lat].</p>
<p>Machine-learning prediction for quasiparton distribution function matrix elements. Rui Zhang, Zhouyou Fan, Ruizi Li, Huey-Wen Lin, Boram Yoon, 10.1103/PhysRevD.101.0345161909.10990Phys. Rev. 10134516hep-latRui Zhang, Zhouyou Fan, Ruizi Li, Huey-Wen Lin, and Boram Yoon, "Machine-learning prediction for quasiparton distribution function matrix elements," Phys. Rev. D101, 034516 (2020), 1909.10990 [hep-lat].</p>
<p>Faster magnet sorting with a threshold acceptance algorithm. S Lidia, R Carr, Review of Scientific Instrumentation. 661865S. Lidia and R. Carr, "Faster magnet sorting with a threshold acceptance algorithm," Review of Scientific Instrumentation 66, 1865 (1995).</p>
<p>Application of genetic algorithms to sorting, swapping and shimming of the soleil undulator magnets. O Chubar, O Rudenko, C Benabderrahmane, O Marcouille, J M Filhol, M E Couprie, http:/arxiv.org/abs/https:/aip.scitation.org/doi/pdf/10.1063/1.2436074AIP Conference Proceedings. 879O. Chubar, O. Rudenko, C. Benabderrahmane, O. Marcouille, J. M. Filhol, and M. E. Couprie, "Application of genetic algorithms to sorting, swapping and shimming of the soleil undulator magnets," AIP Conference Proceedings 879, 359-362 (2007), https://aip.scitation.org/doi/pdf/10.1063/1.2436074.</p>
<p>Optimum steering of photon beam lines in spear. J Corbett, F Fong, M Lee, V Ziemann, Proceedings of the 1993 IEEE Particle Accelerator Conference. the 1993 IEEE Particle Accelerator ConferenceWashingtonJ. Corbett, F. Fong, M. Lee, and V. Ziemann, "Optimum steering of photon beam lines in spear," Proceedings of the 1993 IEEE Particle Accelerator Conference, Washington , 1483-1485 (1993).</p>
<p>Multivariant optimization of a high brightness dc gun photoinjector. I V Bazarov, C K Sinclair, Physical Review Special Topics Accelerator and Beams. 834202I.V. Bazarov and C.K. Sinclair, "Multivariant optimization of a high brightness dc gun photoinjector," Physical Review Special Topics Accelerator and Beams 8, 034202 (2005).</p>
<p>Simultaneous optimization of beam emittance and dynamic aperture for electron storage ring using genetic algorithm. W Gao, L Wang, W Li, Physical Review Special Topics Accelerator and Beams. 1494001W. Gao, L. Wang, and W. Li, "Simultaneous optimization of beam emittance and dynamic aperture for electron storage ring using genetic algorithm," Physical Review Special Topics Accelerator and Beams 14, 094001 (2011).</p>
<p>Innovative applications of genetic algorithms to problems in accelerator physics. A Hofler, B Terzic, M Kramer, A Zvezdin, V Morozov, Y Roblin, F Lin, C Jarvis, Physical Review Special Topics Accelerator and Beams. 1610101A. Hofler, B. Terzic, M. Kramer, A. Zvezdin, V. Morozov, Y. Roblin, F. Lin, and C. Jarvis, "Innova- tive applications of genetic algorithms to problems in accelerator physics," Physical Review Special Topics Accelerator and Beams 16, 010101 (2013).</p>
<p>Neural networks for modeling and control of particle accelerators. A L Edelen, S Biedron, B E Chase, D EdstromJr, S V Milton, P Stabile, IEEE Transactions on Nuclear Science. 63A.L. Edelen, S. Biedron, B.E. Chase, D. Edstrom Jr., S.V. Milton, and P. Stabile, "Neural networks for modeling and control of particle accelerators," IEEE Transactions on Nuclear Science 63, 878- 897 (2016).</p>
<p>Machine learningbased longitudinal phase space prediction of particle accelerators. C Emma, A Edelen, M J Hogan, B Shea, G White, V Yakimenko, Physical Review Accelerators and Beams. 21112802C. Emma, A. Edelen, M.J. Hogan, B. O'Shea, G. White, and V. Yakimenko, "Machine learning- based longitudinal phase space prediction of particle accelerators," Physical Review Accelerators and Beams 21, 112802 (2018).</p>
<p>Model-independent tuning for maximizing free electron laser pulse energy. A Scheinker, D Bohler, S Tomin, R Kammering, I Zagorodnov, H Schlarb, M Scholz, B Beutner, W Decking, Physical Review Accelerators and Beams. 2282802A. Scheinker, D. Bohler, S. Tomin, R. Kammering, I. Zagorodnov, H. Schlarb, M. Scholz, B. Beut- ner, and W. Decking, "Model-independent tuning for maximizing free electron laser pulse energy," Physical Review Accelerators and Beams 22, 082802 (2019).</p>
<p>Workshop Report on Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence. "Workshop Report on Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence," (2019).</p>
<p>. ASCR Workshop on In Situ Data Management: Enabling Scientific Discovery from Diverse Data Sources. "ASCR Workshop on In Situ Data Management: Enabling Scientific Discovery from Diverse Data Sources," (2019).</p>
<p>Data and Models: A Framework for Advancing AI in Science. "Data and Models: A Framework for Advancing AI in Science," (2019).</p>
<p>Committees Local Organizing Committee • Amber Boehnlein. A , Jefferson LabA. Committees Local Organizing Committee • Amber Boehnlein (Jefferson Lab)</p>
<p>. Elouadrhiri • Latifa, Jefferson Lab• Latifa Elouadrhiri (Jefferson Lab)</p>
<p>. • Robert Mckeown, Jefferson Lab• Robert McKeown (Jefferson Lab)</p>
<p>. • Yves Roblin, Jefferson Lab• Yves Roblin (Jefferson Lab)</p>
<p>Advisory Committee, • Robert Mckeown, Working Groups and Conveners WG1. Lattice QCD and Other Quantum Field Theories: Paulo Bedaque. Jefferson Lab • Peter Alonzi, University of Virginia • Oliver Baker, Yale University • Jason Detwiler, University of Washington • Carl Gagliardi, Texas A&amp;M University • Sean Liddick, Michigan State University • Amy Lovell, Los Alamos National Lab • Bronson Messer, Oak Ridge National Lab • Curtis Meyer, Carnegie Mellon University • Zein-Eddine Meziani, Argonne National Lab • Jeff Nichols, Oak Ridge National Lab • Alan Poon, Lawrence Berkeley Lab • Alessandro Roggero, University of Washington • Phiala Shanahan, Massachusetts Institute of Technology • Torre Wenaus, Brookhaven National Lab • Ying Wu, Duke University B. ; University of Maryland) and Kostas Orginos (William&amp;Mary/Jefferson LabAdvisory Committee • Robert McKeown, Jefferson Lab • Peter Alonzi, University of Virginia • Oliver Baker, Yale University • Jason Detwiler, University of Washington • Carl Gagliardi, Texas A&amp;M University • Sean Liddick, Michigan State University • Amy Lovell, Los Alamos National Lab • Bronson Messer, Oak Ridge National Lab • Curtis Meyer, Carnegie Mellon University • Zein-Eddine Meziani, Argonne National Lab • Jeff Nichols, Oak Ridge National Lab • Alan Poon, Lawrence Berkeley Lab • Alessandro Roggero, University of Washington • Phiala Shanahan, Massachusetts Institute of Technology • Torre Wenaus, Brookhaven National Lab • Ying Wu, Duke University B. Working Groups and Conveners WG1. Lattice QCD and Other Quantum Field Theories: Paulo Bedaque (University of Mary- land) and Kostas Orginos (William&amp;Mary/Jefferson Lab)</p>
<p>WG2. Nuclear Structure Theory: Witold Nazarewicz. Michigan State University) and Dean Lee (Michigan State UniversityWG2. Nuclear Structure Theory: Witold Nazarewicz (Michigan State University) and Dean Lee (Michigan State University)</p>
<p>Methods: Michelle Kuchera (Davidson College) and Mario Cromaz (LBNL). Wg4, WG4. Experimental Methods: Michelle Kuchera (Davidson College) and Mario Cromaz (LBNL)</p>
<p>WG5. Event Generation and Simulation: Markus Diefenthaler. Jefferson LabWG5. Event Generation and Simulation: Markus Diefenthaler (Jefferson Lab)</p>
<p>Wg6, Bayesian, Inference for Quantum Correlation Functions: Wally Melnitchouk. Jefferson LabWG6. Bayesian Inference for Quantum Correlation Functions: Wally Melnitchouk (Jeffer- son Lab)</p>            </div>
        </div>

    </div>
</body>
</html>
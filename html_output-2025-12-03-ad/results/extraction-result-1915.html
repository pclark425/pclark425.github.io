<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1915 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1915</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1915</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-280000708</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.19816v1.pdf" target="_blank">CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong generalization across manipulation tasks. However, they remain constrained by a single-frame observation paradigm and cannot fully benefit from the motion information offered by aggregated multi-frame historical observations, as the large vision-language backbone introduces substantial computational cost and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm through an efficient post-training stage. CronusVLA comprises three key components: (1) single-frame pretraining on large-scale embodied datasets with autoregressive action tokens prediction, which establishes an embodied vision-language foundation; (2) multi-frame encoding, adapting the prediction of vision-language backbones from discrete action tokens to motion features during post-training, and aggregating motion features from historical frames into a feature chunking; (3) cross-frame decoding, which maps the feature chunking to accurate actions via a shared decoder with cross-attention. By reducing redundant token computation and caching past motion features, CronusVLA achieves efficient inference. As an application of motion features, we further propose an action adaptation mechanism based on feature-action retrieval to improve model performance during finetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with 70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world Franka experiments also show the strong performance and robustness.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1915.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1915.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CronusVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CronusVLA (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-frame extension of single-frame vision-language-action models that transfers discrete action-token pretraining into continuous per-frame motion features, aggregates historical motion features into a feature chunk, and decodes actions with a cross-frame diffusion decoder; designed for efficient inference and long-horizon robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CronusVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified VLA framework built on a pretrained vision-language backbone (Prismatic / LLaMA2 or Qwen2.5 + DINOv2/SigLIP). Single-frame pretraining predicts discrete action tokens; post-training replaces token outputs with learnable continuous motion features f_t, aggregates M past-frame features into a feature chunk F^M_t, and uses a Transformer-based cross-frame decoder with cross-attention and a modulator to predict action chunks via a diffusion objective. Supports cached per-frame features for fast inference and an action-adaptation retrieval module during finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language single-frame pretraining (discrete action-token autoregressive on embodied datasets) followed by multi-frame post-training (feature-level) and diffusion-based decoder training</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on large-scale embodied image+language+action datasets (OXE / Open X-Embodiment: 27 datasets) with discrete action-token supervision; post-trained (cross-embodiment) on Fractal (~87k episodes, 3.8M images) and Bridge-v2 (~60k trajectories, 2.1M images) multi-frame clips (about 148k episodes and 5M multi-frame clips). Datasets include language instructions, object interactions, spatial relationships, and continuous end-effector actions discretized into 256 bins.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (multi-step / long-horizon manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on simulated benchmarks (SimplerEnv with Google Robot and WidowX Robot; LIBERO suites: Spatial, Object, Goal, Long) and real-world Franka platform tasks. Action space: continuous end-effector delta actions (detokenized from 256-bin discretization in pretraining) / delta EE actions for Franka. Tasks include pick-and-place, drawer open/close, place-in-drawer (long-horizon), button-press sequences, stacking, and robustness tests (occlusion, distractors). Both simulation and real-world experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Yes — explicit: pretraining datasets (OXE / Fractal / Bridge-v2) are embodied manipulation datasets containing object interactions, spatial relationships and action sequences; authors state single-frame pretraining transfers visual perception to embodied scenes and post-training on similar manipulation datasets improves multi-frame motion representations. Degree of overlap: high for manipulation primitives and common objects; domain shift handled via cross-embodiment post-training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>SimplerEnv average success (CronusVLA 7B after post-training): 70.9% overall success; Google Robot VM: 78.6% VM, 73.8% VA (varies by task); Put-in-Drawer: VM 64.8%, VA 65.1%. LIBERO (finetuned): CronusVLA (7B) average 86.2% (Object 94.7%, Goal 91.3%, Long 68.7%); CronusVLA (0.5B) average 80.7%. Real-world Franka: improved robustness and higher success than DP3 and OpenVLA across tasks (detailed per-task results in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Training multi-frame VLA from scratch (same multi-frame training budget of 50k steps) yields much lower SimplerEnv scores: reported scratch variants scored 10.4% and 18.1% versus CronusVLA 70.9% after 50k steps. Basic single-frame post-trained (discrete) baseline (Basic-Post) achieves 31.0% average success (in SimplerEnv).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Yes — single-frame VLA pretraining + multi-frame post-training is far more sample-efficient: after 50k multi-frame training steps CronusVLA reaches 70.9% SimplerEnv vs 10.4%/18.1% for models trained from scratch on same budgets. Finetuning: CronusVLA-7B finetuned 10k steps then action-adapter 5k–12k steps to reach LIBERO scores reported. In real-world Franka, authors used 30–50 human teleoperation demonstration episodes per pick/long task during finetuning and report strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Partial/architectural only — the paper argues that cross-attention in the decoder enables adaptive selection between current and past motion features and attributes robustness (to occlusion, distractors, corrupted frames) to this mechanism; no detailed attention-map visualizations are provided, but ablations (w/o cross-attention) show worse performance and worse scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Yes — action-adaptation retrieval analysis: modulated motion features (Z_f) produce a more concentrated similarity distribution (0.9–1.0) for top-6 retrieved candidates compared to raw multi-frame visual features, indicating stronger alignment between feature embeddings and corresponding actions. Authors flatten/normalize modulated features to build retrieval matrix; retrieval helps generate action priors.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Moderate evidence — empirical correlation between multi-frame modulated motion features and future action sequences: retrieval of top-k similar motion-feature chunks yields reference actions that improve finetuning (action adaptation yields +~4 percentage points on LIBERO average: 82.2% → 86.2% when adapter is used). Ablations show modulated features are more predictive of actions than raw visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Limited — ablations on motion feature types compare 'final' (last-layer feature), 'visual + final' and 'multi-layer final' (features from multiple LLM layers). Multi-layer final improved performance but increased complexity; authors choose final-layer feature for efficiency, implying higher-level features (final LLM layer) are most useful for action prediction after VLM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Positive transfer when: (1) single-frame VLA pretraining on large embodied datasets (OXE) is used; (2) post-training uses cross-embodiment datasets (Fractal + Bridge-v2) for robustness; (3) decoder models multi-frame relations while keeping backbone perception aligned (multi-frame regularization). Negative or poor transfer when naively feeding multiple frames into the VLM (quadratic self-attention cost) — +Multi-frame (naive) gives marginal gain (+1.4%) but large inference slowdown. Training from scratch without pretrained embodied knowledge fails to converge under comparable budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Partial — experiments include generalization/robustness tests on unseen objects and backgrounds in real-world Franka tasks and SimplerEnv VA setting; paper reports CronusVLA performs better than baselines on zero-shot variants (Variant Aggregation) but does not provide detailed per-object split numbers comparing seen vs unseen objects beyond benchmark scores.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot / finetune regime: CronusVLA demonstrates strong few-shot finetuning (10k steps finetune + adapter training on limited expert demonstrations: e.g., 30–50 teleoperated episodes per task in Franka experiments) and robust zero-shot generalization to simulation VA settings; explicit pure zero-shot (no finetuning) results vs baselines not emphasized beyond cross-embodiment post-training gains.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Yes — ablations include extracting motion features from different LLM layers (multi-layer final uses layers 24,16,8 vs final uses last layer only), showing multi-layer features can help but add cost; multi-frame regularization also examined by freezing gradients on past-frame features to preserve single-frame backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Indirect evidence: naive multi-frame input to VLMs (+Multi-frame) produced only small performance gain (+1.4%) while severely reducing inference speed (from 5.18Hz to 3.09Hz), and training from scratch (no embodied pretraining) resulted in very poor performance (10.4%/18.1%) under same budget — showing that improper integration of temporal data or lack of embodied pretraining can harm practical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared to vision-only pretraining (e.g., ImageNet-only). Comparison is mainly between (a) vision-language pretrained single-frame VLA plus multi-frame post-training (CronusVLA) and (b) models trained from scratch or single-frame baselines. Paper shows vision-language pretraining + discrete action-token pretraining yields much faster convergence and better final performance than training multi-frame models from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Yes — authors analyze effect of number of frames: CronusVLA-7B peaks at ~7 frames; 0.5B variant peaks at ~4 frames; more frames can degrade performance and increase latency. They also show convergence dynamics: single-frame pretraining accelerates multi-frame training convergence strongly; training-from-scratch struggles to converge within same training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit PCA/intrinsic dimensionality measurements are reported; embedding analyses are similarity-density distributions for retrieval but no formal dimensionality metrics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1915.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1915.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source single-frame vision-language-action model that tokenizes actions and trains on the OXE embodied dataset; used both as a baseline and a starting point for CronusVLA's single-frame pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenVLA: An Open-Source Vision-Language-Action Model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-frame VLA that discretizes continuous robot actions into tokens (256 bins) and performs autoregressive token prediction using a VLM backbone (Prismatic-like architectures). Processes a single third-person image and language instruction to predict next action tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining applied to embodied single-frame data (autoregressive token prediction over discretized actions)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on OXE / Open X-Embodiment datasets (27 datasets) containing third-person images, language instructions and end-effector action trajectories; includes object descriptions, spatial relations and manipulation demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (single-frame conditioned)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Single-step action prediction from a single third-person view and instruction; evaluated on SimplerEnv and LIBERO benchmarks as a baseline, tasks include pick/place, drawer manipulation, etc., in sim and real robot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Yes — explicitly trained on embodied image+instruction+action datasets; aligns language and visual affordances to action tokens, but limited to single-frame context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>LIBERO (OpenVLA 7B) average reported ~76.5% (per Table 2); SimplerEnv results: varied across tasks (OpenVLA often below CronusVLA in multi-frame tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported in this paper for OpenVLA specifically; OpenVLA itself is a pretrained VLA so baseline comparisons are via other models.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Indirect: OpenVLA requires many epochs for pretraining (authors note 28–40 epochs on OXE for single-frame models), and CronusVLA uses OpenVLA-style pretraining as an efficient foundation; no explicit numeric sample-efficiency curve provided here for OpenVLA alone.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No detailed attention visualization reported here for OpenVLA in this paper (used as a baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not analyzed in this paper for OpenVLA beyond using it as a pretrained checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Implicit in OpenVLA design (discrete action tokenization) but this paper does not present additional grounding analyses for OpenVLA beyond baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>OpenVLA transfers visual perception from VLM pretraining to embodied tasks but is limited by single-frame context; CronusVLA shows improvements by post-training multi-frame modeling on top of OpenVLA-like pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly analyzed for OpenVLA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>OpenVLA is used as a pretrained starting point and baseline; few-shot/finetune behavior not deeply analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported specifically for OpenVLA here beyond single-frame limitations in long-horizon/ambiguous states.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared in this paper; OpenVLA is a vision-language pretrained model and compared in benchmarks to CronusVLA and other VLAs.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>OpenVLA is single-frame; temporal dynamics not modeled natively, which the paper highlights as a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality measurements provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1915.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1915.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TraceVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TraceVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA method that integrates a visual trace prompting (additional visual trace image) to add temporal information to single-frame VLA inputs, mentioned as related work and a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TraceVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-frame VLA augmented with a visual-trace prompt (an extra image encoding past trajectory) appended to the current observation to incorporate some temporal context without redesigning backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (single-frame VLM) with added visual prompting</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Uses datasets similar to OpenVLA / OXE with additional annotations for trace prompting; contains action demonstrations but not full multi-frame feature aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (bringing limited temporal cue via trace prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on SimplerEnv and other embodied benchmarks as a baseline; aims to improve spatial-temporal awareness via extra visual prompting rather than full multi-frame modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Partial — uses visual trace annotations to align past motion into current observation; authors argue it may not fully exploit pretrained VLM capabilities because VLMs were rarely trained with such traces.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in paper as a baseline (TraceVLA 7B results in SimplerEnv and LIBERO are provided): example LIBERO averages in Table 2 show TraceVLA (7B) ~74.8% average (lower than CronusVLA 86.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not quantified in this paper beyond being a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in this paper for TraceVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>No direct grounding analysis here; TraceVLA uses visual prompting to add temporal cues but lacks feature-level motion aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works as a lightweight temporal cue addition but authors argue limitations vs full multi-frame feature transfer approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not directly, but authors suggest trace prompting may underutilize pretrained VLM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Adds a single extra trace image as temporal cue; not a full multi-frame model.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>None provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1915.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1915.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboVLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboVLMs (memory-based VLA models referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Memory-based extensions of single-frame VLMs that add memory/memory-heads to model temporal relations, cited as related work and a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboVLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-frame VLA approach that attaches memory/memory-heads to pretrained VLMs to capture temporal relations across frames for manipulation; trained to predict actions from sequences rather than a single frame.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretrained VLM backbone with additional memory heads trained for embodied tasks (varies by implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Post-training on embodied datasets (Fractal/Bridge-like) in some checkpoints; contains manipulation demonstrations and action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (multi-frame via memory heads)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on SimplerEnv and other manipulation benchmarks as a baseline; reported checkpoints trained on Fractal/Bridge and evaluated on Google/WidowX robots.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not deeply analyzed in this paper; presented as a competing strategy to incorporate multi-frame info by training memory heads.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in SimplerEnv comparisons: RoboVLMs (2B) reported as baseline; CronusVLA outperforms RoboVLMs in many SimplerEnv metrics (CronusVLA 7B gives large relative improvements reported). Exact RoboVLM numbers vary by checkpoint and dataset in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not explicitly quantified in this paper for RoboVLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not analyzed in this paper; RoboVLMs are described as training embodiment capabilities from scratch on top of VLMs which can miss benefits of discrete-to-feature transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Authors argue memory-head approaches forcibly extend single-frame VLMs to multi-frame prediction by training embodiment from scratch and may overlook efficient transfer strategies that retain single-frame perception.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-specific analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not directly, but authors claim these methods 'overlook benefits' of transferring single-frame pretrained VLA models to multi-frame, implying suboptimal transfer under some budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1915.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1915.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action model that focuses on spatial representations and unifies action spaces via adaptive action grids; used as a competitive baseline in benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VLA that emphasizes spatial reasoning and unified action grids to map visual inputs and language instructions to actions; is a pretrained model finetuned/evaluated on manipulation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining on embodied datasets and additional post-training; models contain large LLM backbones (~3B) and spatial-specific modules</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on subsets of OXE-like embodied datasets, aligning language and spatial visual features and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation with emphasis on spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on LIBERO and SimplerEnv tasks; focuses on spatial/generalization aspects of manipulation (stacking, placing, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed for spatial alignment between vision and action; in this paper SpatialVLA is a strong baseline but CronusVLA reports higher average performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>LIBERO average reported ~78.1% (Table 2); CronusVLA (7B) outperforms it (86.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not explicitly quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>SpatialVLA design targets spatial grounding, but this paper does not analyze its grounding mechanisms beyond benchmark comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Per authors, SpatialVLA is a competitive pretrained VLA; CronusVLA achieves +8.1 percentage points over SpatialVLA on LIBERO average, suggesting multi-frame modeling + action adaptation helps transfer for long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1915.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1915.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action model that discretizes continuous actions and leverages large vision-language backbones to transfer web-scale knowledge to robot control; cited as prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-2: Vision-language-action models transfer web knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Discretizes robot actions into tokens (action tokenizer) and uses a large VLM backbone (e.g., PaLI-X) for autoregressive action token prediction conditioned on an image and language instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining leveraging web-scale multi-modal data and embodied action-token supervision</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining involves large web-scale image-text data for VLMs and embodied robot demonstration datasets for tokenized action prediction; contains object/action language and some affordance cues.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation / vision-language conditioned control</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Single-frame instruction-conditioned action generation for manipulation tasks; baseline in evaluation suites.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to align web-scale VLM knowledge with robotic actions by tokenization; alignment is partial and relies on action-token retokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported as an early baseline in the paper; specific per-benchmark numbers not detailed here beyond referencing it among baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not covered in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not covered in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>RT-2's tokenization approach is an explicit mechanism for grounding continuous actions into discrete language-model-friendly tokens; no direct grounding analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>RT-2 exemplifies that large VLMs can be adapted to robot control by tokenizing actions, but the paper uses this to motivate CronusVLA's different (feature-level multi-frame) approach.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1915.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1915.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Magma</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Magma</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal foundation model / VLA referenced as prior work for unified action prediction and multimodal embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Magma: A foundation model for multimodal ai agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Magma</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A foundation multimodal model that aims to unify action prediction and multimodal embodied tasks; included as a baseline/comparative large-model approach in related work and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal pretraining (vision + language + additional modalities depending on implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale multimodal datasets; contains diverse image-text pairs and may be adapted to embodied tasks in downstream finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Multimodal embodied tasks / robotic manipulation (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Used as a comparative baseline in SimplerEnv / LIBERO tasks; specifics depend on Magma's official experiments (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>General multimodal alignment between vision and language; concrete overlap with robot manipulation datasets not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in SimplerEnv tables as a baseline with moderate performance across tasks (varies by setting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Mentioned as a generalist multimodal baseline; no direct transfer condition analysis within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1915.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1915.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prismatic (Prismatic VLM / backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prismatic (vision-language model backbone used in CronusVLA 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language backbone combining LLaMA2 with DINOv2 and SigLIP vision encoders (referred to as Prismatic/Prismatic-like backbone) used as the pretrained single-frame VLA starting point.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prismatic vlms: Investigating the design space of visually-conditioned language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prismatic (VLM backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A visually-conditioned language model that fuses an LLM (LLaMA2) with pretrained visual encoders (DINOv2, SigLIP) and is used as the core vision-language backbone for single-frame VLA pretraining (Prismatic/Prismatic-like). Processes single-view images and language instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on large image-text corpora (LAION, Conceptual Captions, VQA, etc.) and instruction-tuning; then adapted to embodied action token prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on web-scale image-text corpora (~558K image-caption pairs for initial alignment; instruction-tuning on ~665K multimodal examples) providing object descriptions, actions in captions, and general visual-language semantics; later further trained on embodied OXE datasets for action-token prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Used as backbone for VLA models applied to robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Supports single-frame action token prediction and, after CronusVLA modifications, computes per-frame motion features for multi-frame decoding in manipulation tasks (both sim and real-world).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High for natural image-language semantics; specialized embodied alignment achieved via subsequent discrete action-token pretraining on OXE embodied datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>As a backbone, it enables CronusVLA 7B to reach reported SimplerEnv and LIBERO scores (e.g., LIBERO 86.2% average after full pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable here (backbone always pretrained).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Using this pretrained backbone plus discrete embodied pretraining is argued to much improve convergence and sample efficiency versus training a VLM or VLA from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not analyzed in detail in this paper beyond noting costs of self-attention when naively adding frames.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not detailed beyond role as backbone for feature extraction; CronusVLA builds motion features on top of its hidden layers.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect: as a pretrained VLM, it provides visual-language alignment that facilitates downstream token-to-action or feature-to-action mapping after discrete embodied pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>CronusVLA ablations indicate last-layer features of the LLM encoder are effective motion features; Prismatic provides these features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Prismatic's general V-L pretraining helps when subsequent embodied discrete-action pretraining (OXE) and cross-embodiment post-training are used.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed specifically for Prismatic in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Prismatic provides general V-L priors that help few-shot finetuning in CronusVLA pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>CronusVLA experiments interrogate features from different LLM layers (multi-layer final vs final), indicating deeper LLM layers supply useful motion features.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>OpenVLA: An Open-Source Vision-Language-Action Model. <em>(Rating: 2)</em></li>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. <em>(Rating: 2)</em></li>
                <li>SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model. <em>(Rating: 2)</em></li>
                <li>Magma: A foundation model for multimodal ai agents. <em>(Rating: 1)</em></li>
                <li>Prismatic vlms: Investigating the design space of visually-conditioned language models. <em>(Rating: 2)</em></li>
                <li>RoboVLMs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1915",
    "paper_id": "paper-280000708",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "CronusVLA",
            "name_full": "CronusVLA (this paper)",
            "brief_description": "A multi-frame extension of single-frame vision-language-action models that transfers discrete action-token pretraining into continuous per-frame motion features, aggregates historical motion features into a feature chunk, and decodes actions with a cross-frame diffusion decoder; designed for efficient inference and long-horizon robotic manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CronusVLA",
            "model_description": "Unified VLA framework built on a pretrained vision-language backbone (Prismatic / LLaMA2 or Qwen2.5 + DINOv2/SigLIP). Single-frame pretraining predicts discrete action tokens; post-training replaces token outputs with learnable continuous motion features f_t, aggregates M past-frame features into a feature chunk F^M_t, and uses a Transformer-based cross-frame decoder with cross-attention and a modulator to predict action chunks via a diffusion objective. Supports cached per-frame features for fast inference and an action-adaptation retrieval module during finetuning.",
            "pretraining_type": "vision-language single-frame pretraining (discrete action-token autoregressive on embodied datasets) followed by multi-frame post-training (feature-level) and diffusion-based decoder training",
            "pretraining_data_description": "Pretrained on large-scale embodied image+language+action datasets (OXE / Open X-Embodiment: 27 datasets) with discrete action-token supervision; post-trained (cross-embodiment) on Fractal (~87k episodes, 3.8M images) and Bridge-v2 (~60k trajectories, 2.1M images) multi-frame clips (about 148k episodes and 5M multi-frame clips). Datasets include language instructions, object interactions, spatial relationships, and continuous end-effector actions discretized into 256 bins.",
            "target_task_name": "Robotic manipulation (multi-step / long-horizon manipulation)",
            "target_task_description": "Evaluated on simulated benchmarks (SimplerEnv with Google Robot and WidowX Robot; LIBERO suites: Spatial, Object, Goal, Long) and real-world Franka platform tasks. Action space: continuous end-effector delta actions (detokenized from 256-bin discretization in pretraining) / delta EE actions for Franka. Tasks include pick-and-place, drawer open/close, place-in-drawer (long-horizon), button-press sequences, stacking, and robustness tests (occlusion, distractors). Both simulation and real-world experiments reported.",
            "semantic_alignment": "Yes — explicit: pretraining datasets (OXE / Fractal / Bridge-v2) are embodied manipulation datasets containing object interactions, spatial relationships and action sequences; authors state single-frame pretraining transfers visual perception to embodied scenes and post-training on similar manipulation datasets improves multi-frame motion representations. Degree of overlap: high for manipulation primitives and common objects; domain shift handled via cross-embodiment post-training.",
            "performance_with_language_pretraining": "SimplerEnv average success (CronusVLA 7B after post-training): 70.9% overall success; Google Robot VM: 78.6% VM, 73.8% VA (varies by task); Put-in-Drawer: VM 64.8%, VA 65.1%. LIBERO (finetuned): CronusVLA (7B) average 86.2% (Object 94.7%, Goal 91.3%, Long 68.7%); CronusVLA (0.5B) average 80.7%. Real-world Franka: improved robustness and higher success than DP3 and OpenVLA across tasks (detailed per-task results in paper).",
            "performance_without_language_pretraining": "Training multi-frame VLA from scratch (same multi-frame training budget of 50k steps) yields much lower SimplerEnv scores: reported scratch variants scored 10.4% and 18.1% versus CronusVLA 70.9% after 50k steps. Basic single-frame post-trained (discrete) baseline (Basic-Post) achieves 31.0% average success (in SimplerEnv).",
            "sample_efficiency_comparison": "Yes — single-frame VLA pretraining + multi-frame post-training is far more sample-efficient: after 50k multi-frame training steps CronusVLA reaches 70.9% SimplerEnv vs 10.4%/18.1% for models trained from scratch on same budgets. Finetuning: CronusVLA-7B finetuned 10k steps then action-adapter 5k–12k steps to reach LIBERO scores reported. In real-world Franka, authors used 30–50 human teleoperation demonstration episodes per pick/long task during finetuning and report strong performance.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Partial/architectural only — the paper argues that cross-attention in the decoder enables adaptive selection between current and past motion features and attributes robustness (to occlusion, distractors, corrupted frames) to this mechanism; no detailed attention-map visualizations are provided, but ablations (w/o cross-attention) show worse performance and worse scaling.",
            "embedding_space_analysis": "Yes — action-adaptation retrieval analysis: modulated motion features (Z_f) produce a more concentrated similarity distribution (0.9–1.0) for top-6 retrieved candidates compared to raw multi-frame visual features, indicating stronger alignment between feature embeddings and corresponding actions. Authors flatten/normalize modulated features to build retrieval matrix; retrieval helps generate action priors.",
            "action_grounding_evidence": "Moderate evidence — empirical correlation between multi-frame modulated motion features and future action sequences: retrieval of top-k similar motion-feature chunks yields reference actions that improve finetuning (action adaptation yields +~4 percentage points on LIBERO average: 82.2% → 86.2% when adapter is used). Ablations show modulated features are more predictive of actions than raw visual features.",
            "hierarchical_features_evidence": "Limited — ablations on motion feature types compare 'final' (last-layer feature), 'visual + final' and 'multi-layer final' (features from multiple LLM layers). Multi-layer final improved performance but increased complexity; authors choose final-layer feature for efficiency, implying higher-level features (final LLM layer) are most useful for action prediction after VLM pretraining.",
            "transfer_conditions": "Positive transfer when: (1) single-frame VLA pretraining on large embodied datasets (OXE) is used; (2) post-training uses cross-embodiment datasets (Fractal + Bridge-v2) for robustness; (3) decoder models multi-frame relations while keeping backbone perception aligned (multi-frame regularization). Negative or poor transfer when naively feeding multiple frames into the VLM (quadratic self-attention cost) — +Multi-frame (naive) gives marginal gain (+1.4%) but large inference slowdown. Training from scratch without pretrained embodied knowledge fails to converge under comparable budgets.",
            "novel_vs_familiar_objects": "Partial — experiments include generalization/robustness tests on unseen objects and backgrounds in real-world Franka tasks and SimplerEnv VA setting; paper reports CronusVLA performs better than baselines on zero-shot variants (Variant Aggregation) but does not provide detailed per-object split numbers comparing seen vs unseen objects beyond benchmark scores.",
            "zero_shot_or_few_shot": "Few-shot / finetune regime: CronusVLA demonstrates strong few-shot finetuning (10k steps finetune + adapter training on limited expert demonstrations: e.g., 30–50 teleoperated episodes per task in Franka experiments) and robust zero-shot generalization to simulation VA settings; explicit pure zero-shot (no finetuning) results vs baselines not emphasized beyond cross-embodiment post-training gains.",
            "layer_analysis": "Yes — ablations include extracting motion features from different LLM layers (multi-layer final uses layers 24,16,8 vs final uses last layer only), showing multi-layer features can help but add cost; multi-frame regularization also examined by freezing gradients on past-frame features to preserve single-frame backbone.",
            "negative_transfer_evidence": "Indirect evidence: naive multi-frame input to VLMs (+Multi-frame) produced only small performance gain (+1.4%) while severely reducing inference speed (from 5.18Hz to 3.09Hz), and training from scratch (no embodied pretraining) resulted in very poor performance (10.4%/18.1%) under same budget — showing that improper integration of temporal data or lack of embodied pretraining can harm practical performance.",
            "comparison_to_vision_only": "Not directly compared to vision-only pretraining (e.g., ImageNet-only). Comparison is mainly between (a) vision-language pretrained single-frame VLA plus multi-frame post-training (CronusVLA) and (b) models trained from scratch or single-frame baselines. Paper shows vision-language pretraining + discrete action-token pretraining yields much faster convergence and better final performance than training multi-frame models from scratch.",
            "temporal_dynamics": "Yes — authors analyze effect of number of frames: CronusVLA-7B peaks at ~7 frames; 0.5B variant peaks at ~4 frames; more frames can degrade performance and increase latency. They also show convergence dynamics: single-frame pretraining accelerates multi-frame training convergence strongly; training-from-scratch struggles to converge within same training budget.",
            "dimensionality_analysis": "No explicit PCA/intrinsic dimensionality measurements are reported; embedding analyses are similarity-density distributions for retrieval but no formal dimensionality metrics.",
            "uuid": "e1915.0"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA",
            "brief_description": "An open-source single-frame vision-language-action model that tokenizes actions and trains on the OXE embodied dataset; used both as a baseline and a starting point for CronusVLA's single-frame pretraining.",
            "citation_title": "OpenVLA: An Open-Source Vision-Language-Action Model.",
            "mention_or_use": "use",
            "model_name": "OpenVLA",
            "model_description": "Single-frame VLA that discretizes continuous robot actions into tokens (256 bins) and performs autoregressive token prediction using a VLM backbone (Prismatic-like architectures). Processes a single third-person image and language instruction to predict next action tokens.",
            "pretraining_type": "vision-language pretraining applied to embodied single-frame data (autoregressive token prediction over discretized actions)",
            "pretraining_data_description": "Pretrained on OXE / Open X-Embodiment datasets (27 datasets) containing third-person images, language instructions and end-effector action trajectories; includes object descriptions, spatial relations and manipulation demonstrations.",
            "target_task_name": "Robotic manipulation (single-frame conditioned)",
            "target_task_description": "Single-step action prediction from a single third-person view and instruction; evaluated on SimplerEnv and LIBERO benchmarks as a baseline, tasks include pick/place, drawer manipulation, etc., in sim and real robot settings.",
            "semantic_alignment": "Yes — explicitly trained on embodied image+instruction+action datasets; aligns language and visual affordances to action tokens, but limited to single-frame context.",
            "performance_with_language_pretraining": "LIBERO (OpenVLA 7B) average reported ~76.5% (per Table 2); SimplerEnv results: varied across tasks (OpenVLA often below CronusVLA in multi-frame tasks).",
            "performance_without_language_pretraining": "Not reported in this paper for OpenVLA specifically; OpenVLA itself is a pretrained VLA so baseline comparisons are via other models.",
            "sample_efficiency_comparison": "Indirect: OpenVLA requires many epochs for pretraining (authors note 28–40 epochs on OXE for single-frame models), and CronusVLA uses OpenVLA-style pretraining as an efficient foundation; no explicit numeric sample-efficiency curve provided here for OpenVLA alone.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No detailed attention visualization reported here for OpenVLA in this paper (used as a baseline).",
            "embedding_space_analysis": "Not analyzed in this paper for OpenVLA beyond using it as a pretrained checkpoint.",
            "action_grounding_evidence": "Implicit in OpenVLA design (discrete action tokenization) but this paper does not present additional grounding analyses for OpenVLA beyond baseline performance.",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "OpenVLA transfers visual perception from VLM pretraining to embodied tasks but is limited by single-frame context; CronusVLA shows improvements by post-training multi-frame modeling on top of OpenVLA-like pretraining.",
            "novel_vs_familiar_objects": "Not explicitly analyzed for OpenVLA in this paper.",
            "zero_shot_or_few_shot": "OpenVLA is used as a pretrained starting point and baseline; few-shot/finetune behavior not deeply analyzed here.",
            "layer_analysis": "Not provided in this paper.",
            "negative_transfer_evidence": "Not reported specifically for OpenVLA here beyond single-frame limitations in long-horizon/ambiguous states.",
            "comparison_to_vision_only": "Not directly compared in this paper; OpenVLA is a vision-language pretrained model and compared in benchmarks to CronusVLA and other VLAs.",
            "temporal_dynamics": "OpenVLA is single-frame; temporal dynamics not modeled natively, which the paper highlights as a limitation.",
            "dimensionality_analysis": "No dimensionality measurements provided.",
            "uuid": "e1915.1"
        },
        {
            "name_short": "TraceVLA",
            "name_full": "TraceVLA",
            "brief_description": "A VLA method that integrates a visual trace prompting (additional visual trace image) to add temporal information to single-frame VLA inputs, mentioned as related work and a baseline.",
            "citation_title": "Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies.",
            "mention_or_use": "mention",
            "model_name": "TraceVLA",
            "model_description": "Single-frame VLA augmented with a visual-trace prompt (an extra image encoding past trajectory) appended to the current observation to incorporate some temporal context without redesigning backbone.",
            "pretraining_type": "vision-language pretraining (single-frame VLM) with added visual prompting",
            "pretraining_data_description": "Uses datasets similar to OpenVLA / OXE with additional annotations for trace prompting; contains action demonstrations but not full multi-frame feature aggregation.",
            "target_task_name": "Robotic manipulation (bringing limited temporal cue via trace prompting)",
            "target_task_description": "Evaluated on SimplerEnv and other embodied benchmarks as a baseline; aims to improve spatial-temporal awareness via extra visual prompting rather than full multi-frame modeling.",
            "semantic_alignment": "Partial — uses visual trace annotations to align past motion into current observation; authors argue it may not fully exploit pretrained VLM capabilities because VLMs were rarely trained with such traces.",
            "performance_with_language_pretraining": "Reported in paper as a baseline (TraceVLA 7B results in SimplerEnv and LIBERO are provided): example LIBERO averages in Table 2 show TraceVLA (7B) ~74.8% average (lower than CronusVLA 86.2%).",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not quantified in this paper beyond being a baseline.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in this paper for TraceVLA.",
            "embedding_space_analysis": "Not provided here.",
            "action_grounding_evidence": "No direct grounding analysis here; TraceVLA uses visual prompting to add temporal cues but lacks feature-level motion aggregation.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Works as a lightweight temporal cue addition but authors argue limitations vs full multi-frame feature transfer approaches.",
            "novel_vs_familiar_objects": "Not analyzed here.",
            "zero_shot_or_few_shot": "Not emphasized.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Not directly, but authors suggest trace prompting may underutilize pretrained VLM capabilities.",
            "comparison_to_vision_only": "Not directly compared here.",
            "temporal_dynamics": "Adds a single extra trace image as temporal cue; not a full multi-frame model.",
            "dimensionality_analysis": "None provided.",
            "uuid": "e1915.2"
        },
        {
            "name_short": "RoboVLMs",
            "name_full": "RoboVLMs (memory-based VLA models referenced)",
            "brief_description": "Memory-based extensions of single-frame VLMs that add memory/memory-heads to model temporal relations, cited as related work and a comparative baseline.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RoboVLMs",
            "model_description": "Multi-frame VLA approach that attaches memory/memory-heads to pretrained VLMs to capture temporal relations across frames for manipulation; trained to predict actions from sequences rather than a single frame.",
            "pretraining_type": "Vision-language pretrained VLM backbone with additional memory heads trained for embodied tasks (varies by implementation)",
            "pretraining_data_description": "Post-training on embodied datasets (Fractal/Bridge-like) in some checkpoints; contains manipulation demonstrations and action sequences.",
            "target_task_name": "Robotic manipulation (multi-frame via memory heads)",
            "target_task_description": "Evaluated on SimplerEnv and other manipulation benchmarks as a baseline; reported checkpoints trained on Fractal/Bridge and evaluated on Google/WidowX robots.",
            "semantic_alignment": "Not deeply analyzed in this paper; presented as a competing strategy to incorporate multi-frame info by training memory heads.",
            "performance_with_language_pretraining": "Reported in SimplerEnv comparisons: RoboVLMs (2B) reported as baseline; CronusVLA outperforms RoboVLMs in many SimplerEnv metrics (CronusVLA 7B gives large relative improvements reported). Exact RoboVLM numbers vary by checkpoint and dataset in paper tables.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not explicitly quantified in this paper for RoboVLMs.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided here.",
            "embedding_space_analysis": "Not provided here.",
            "action_grounding_evidence": "Not analyzed in this paper; RoboVLMs are described as training embodiment capabilities from scratch on top of VLMs which can miss benefits of discrete-to-feature transfer.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Authors argue memory-head approaches forcibly extend single-frame VLMs to multi-frame prediction by training embodiment from scratch and may overlook efficient transfer strategies that retain single-frame perception.",
            "novel_vs_familiar_objects": "Not provided.",
            "zero_shot_or_few_shot": "Not discussed here.",
            "layer_analysis": "No layer-specific analysis in this paper.",
            "negative_transfer_evidence": "Not directly, but authors claim these methods 'overlook benefits' of transferring single-frame pretrained VLA models to multi-frame, implying suboptimal transfer under some budgets.",
            "comparison_to_vision_only": "Not provided.",
            "uuid": "e1915.3"
        },
        {
            "name_short": "SpatialVLA",
            "name_full": "SpatialVLA",
            "brief_description": "A vision-language-action model that focuses on spatial representations and unifies action spaces via adaptive action grids; used as a competitive baseline in benchmarks.",
            "citation_title": "SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model.",
            "mention_or_use": "mention",
            "model_name": "SpatialVLA",
            "model_description": "VLA that emphasizes spatial reasoning and unified action grids to map visual inputs and language instructions to actions; is a pretrained model finetuned/evaluated on manipulation benchmarks.",
            "pretraining_type": "vision-language pretraining on embodied datasets and additional post-training; models contain large LLM backbones (~3B) and spatial-specific modules",
            "pretraining_data_description": "Pretrained on subsets of OXE-like embodied datasets, aligning language and spatial visual features and actions.",
            "target_task_name": "Robotic manipulation with emphasis on spatial reasoning",
            "target_task_description": "Evaluated on LIBERO and SimplerEnv tasks; focuses on spatial/generalization aspects of manipulation (stacking, placing, etc.).",
            "semantic_alignment": "Designed for spatial alignment between vision and action; in this paper SpatialVLA is a strong baseline but CronusVLA reports higher average performance.",
            "performance_with_language_pretraining": "LIBERO average reported ~78.1% (Table 2); CronusVLA (7B) outperforms it (86.2%).",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not explicitly quantified in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in this paper.",
            "embedding_space_analysis": "Not provided here.",
            "action_grounding_evidence": "SpatialVLA design targets spatial grounding, but this paper does not analyze its grounding mechanisms beyond benchmark comparisons.",
            "hierarchical_features_evidence": "Not presented here.",
            "transfer_conditions": "Per authors, SpatialVLA is a competitive pretrained VLA; CronusVLA achieves +8.1 percentage points over SpatialVLA on LIBERO average, suggesting multi-frame modeling + action adaptation helps transfer for long-horizon tasks.",
            "novel_vs_familiar_objects": "Not analyzed here.",
            "zero_shot_or_few_shot": "Not discussed here.",
            "layer_analysis": "Not provided.",
            "uuid": "e1915.4"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2",
            "brief_description": "A vision-language-action model that discretizes continuous actions and leverages large vision-language backbones to transfer web-scale knowledge to robot control; cited as prior work.",
            "citation_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control.",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "Discretizes robot actions into tokens (action tokenizer) and uses a large VLM backbone (e.g., PaLI-X) for autoregressive action token prediction conditioned on an image and language instruction.",
            "pretraining_type": "Vision-language pretraining leveraging web-scale multi-modal data and embodied action-token supervision",
            "pretraining_data_description": "Pretraining involves large web-scale image-text data for VLMs and embodied robot demonstration datasets for tokenized action prediction; contains object/action language and some affordance cues.",
            "target_task_name": "Robotic manipulation / vision-language conditioned control",
            "target_task_description": "Single-frame instruction-conditioned action generation for manipulation tasks; baseline in evaluation suites.",
            "semantic_alignment": "Designed to align web-scale VLM knowledge with robotic actions by tokenization; alignment is partial and relies on action-token retokenization.",
            "performance_with_language_pretraining": "Reported as an early baseline in the paper; specific per-benchmark numbers not detailed here beyond referencing it among baselines.",
            "performance_without_language_pretraining": "Not provided.",
            "sample_efficiency_comparison": "Not quantified here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not covered in this paper.",
            "embedding_space_analysis": "Not covered in this paper.",
            "action_grounding_evidence": "RT-2's tokenization approach is an explicit mechanism for grounding continuous actions into discrete language-model-friendly tokens; no direct grounding analysis in this paper.",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "RT-2 exemplifies that large VLMs can be adapted to robot control by tokenizing actions, but the paper uses this to motivate CronusVLA's different (feature-level multi-frame) approach.",
            "novel_vs_familiar_objects": "Not analyzed here.",
            "zero_shot_or_few_shot": "Not discussed here.",
            "layer_analysis": "Not provided.",
            "uuid": "e1915.5"
        },
        {
            "name_short": "Magma",
            "name_full": "Magma",
            "brief_description": "A multimodal foundation model / VLA referenced as prior work for unified action prediction and multimodal embodied tasks.",
            "citation_title": "Magma: A foundation model for multimodal ai agents.",
            "mention_or_use": "mention",
            "model_name": "Magma",
            "model_description": "A foundation multimodal model that aims to unify action prediction and multimodal embodied tasks; included as a baseline/comparative large-model approach in related work and tables.",
            "pretraining_type": "Multimodal pretraining (vision + language + additional modalities depending on implementation)",
            "pretraining_data_description": "Large-scale multimodal datasets; contains diverse image-text pairs and may be adapted to embodied tasks in downstream finetuning.",
            "target_task_name": "Multimodal embodied tasks / robotic manipulation (as referenced)",
            "target_task_description": "Used as a comparative baseline in SimplerEnv / LIBERO tasks; specifics depend on Magma's official experiments (not detailed here).",
            "semantic_alignment": "General multimodal alignment between vision and language; concrete overlap with robot manipulation datasets not detailed here.",
            "performance_with_language_pretraining": "Reported in SimplerEnv tables as a baseline with moderate performance across tasks (varies by setting).",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not quantified in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided here.",
            "embedding_space_analysis": "Not provided here.",
            "action_grounding_evidence": "Not detailed in this paper.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Mentioned as a generalist multimodal baseline; no direct transfer condition analysis within this paper.",
            "novel_vs_familiar_objects": "Not provided.",
            "zero_shot_or_few_shot": "Not provided.",
            "layer_analysis": "Not provided.",
            "uuid": "e1915.6"
        },
        {
            "name_short": "Prismatic (Prismatic VLM / backbone)",
            "name_full": "Prismatic (vision-language model backbone used in CronusVLA 7B)",
            "brief_description": "A vision-language backbone combining LLaMA2 with DINOv2 and SigLIP vision encoders (referred to as Prismatic/Prismatic-like backbone) used as the pretrained single-frame VLA starting point.",
            "citation_title": "Prismatic vlms: Investigating the design space of visually-conditioned language models.",
            "mention_or_use": "use",
            "model_name": "Prismatic (VLM backbone)",
            "model_description": "A visually-conditioned language model that fuses an LLM (LLaMA2) with pretrained visual encoders (DINOv2, SigLIP) and is used as the core vision-language backbone for single-frame VLA pretraining (Prismatic/Prismatic-like). Processes single-view images and language instruction.",
            "pretraining_type": "Vision-language pretraining on large image-text corpora (LAION, Conceptual Captions, VQA, etc.) and instruction-tuning; then adapted to embodied action token prediction.",
            "pretraining_data_description": "Pretrained on web-scale image-text corpora (~558K image-caption pairs for initial alignment; instruction-tuning on ~665K multimodal examples) providing object descriptions, actions in captions, and general visual-language semantics; later further trained on embodied OXE datasets for action-token prediction.",
            "target_task_name": "Used as backbone for VLA models applied to robotic manipulation",
            "target_task_description": "Supports single-frame action token prediction and, after CronusVLA modifications, computes per-frame motion features for multi-frame decoding in manipulation tasks (both sim and real-world).",
            "semantic_alignment": "High for natural image-language semantics; specialized embodied alignment achieved via subsequent discrete action-token pretraining on OXE embodied datasets.",
            "performance_with_language_pretraining": "As a backbone, it enables CronusVLA 7B to reach reported SimplerEnv and LIBERO scores (e.g., LIBERO 86.2% average after full pipeline).",
            "performance_without_language_pretraining": "Not applicable here (backbone always pretrained).",
            "sample_efficiency_comparison": "Using this pretrained backbone plus discrete embodied pretraining is argued to much improve convergence and sample efficiency versus training a VLM or VLA from scratch.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not analyzed in detail in this paper beyond noting costs of self-attention when naively adding frames.",
            "embedding_space_analysis": "Not detailed beyond role as backbone for feature extraction; CronusVLA builds motion features on top of its hidden layers.",
            "action_grounding_evidence": "Indirect: as a pretrained VLM, it provides visual-language alignment that facilitates downstream token-to-action or feature-to-action mapping after discrete embodied pretraining.",
            "hierarchical_features_evidence": "CronusVLA ablations indicate last-layer features of the LLM encoder are effective motion features; Prismatic provides these features.",
            "transfer_conditions": "Prismatic's general V-L pretraining helps when subsequent embodied discrete-action pretraining (OXE) and cross-embodiment post-training are used.",
            "novel_vs_familiar_objects": "Not analyzed specifically for Prismatic in this paper.",
            "zero_shot_or_few_shot": "Prismatic provides general V-L priors that help few-shot finetuning in CronusVLA pipeline.",
            "layer_analysis": "CronusVLA experiments interrogate features from different LLM layers (multi-layer final vs final), indicating deeper LLM layers supply useful motion features.",
            "uuid": "e1915.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "OpenVLA: An Open-Source Vision-Language-Action Model.",
            "rating": 2
        },
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control.",
            "rating": 2
        },
        {
            "paper_title": "Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies.",
            "rating": 2
        },
        {
            "paper_title": "SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model.",
            "rating": 2
        },
        {
            "paper_title": "Magma: A foundation model for multimodal ai agents.",
            "rating": 1
        },
        {
            "paper_title": "Prismatic vlms: Investigating the design space of visually-conditioned language models.",
            "rating": 2
        },
        {
            "paper_title": "RoboVLMs",
            "rating": 1
        }
    ],
    "cost": 0.023899749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation
24 Jun 2025</p>
<p>Hao Li 
University of Science and Technology of China</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Shuai Yang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Zhejiang University</p>
<p>Yilun Chen 
Shanghai Artificial Intelligence Laboratory</p>
<p>Yang Tian 
Shanghai Artificial Intelligence Laboratory</p>
<p>Xiaoda Yang 
Zhejiang University</p>
<p>Xinyi Chen 
Hanqing Wang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Tai Wang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Feng Zhao 
University of Science and Technology of China</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Dahua Lin 
The Chinese University of Hong Kong</p>
<p>Jiangmiao Pang 
Shanghai Artificial Intelligence Laboratory</p>
<p>Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation
24 Jun 20251E086BAA45A47B8B3554751CECF4E29FarXiv:2506.19816v1[cs.RO]
Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong generalization across manipulation tasks.However, they remain constrained by a single-frame observation paradigm and cannot fully benefit from the motion information offered by aggregated multi-frame historical observations, as the large vision-language backbone introduces substantial computational cost and inference latency.We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm through an efficient post-training stage.CronusVLA comprises three key components: (1) single-frame pretraining on large-scale embodied datasets with autoregressive action tokens prediction, which establishes an embodied vision-language foundation; (2) multi-frame encoding, adapting the prediction of vision-language backbones from discrete action tokens to motion features during post-training, and aggregating motion features from historical frames into a feature chunking; (3) cross-frame decoding, which maps the feature chunking to accurate actions via a shared decoder with cross-attention.By reducing redundant token computation and caching past motion features, CronusVLA achieves efficient inference.As an application of motion features, we further propose an action adaptation mechanism based on feature-action retrieval to improve model performance during finetuning.CronusVLA achieves state-of-the-art performance on SimplerEnv with 70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO.Real-world Franka experiments also show the strong performance and robustness.Project website.</p>
<p>Introduction</p>
<p>Current low-level policies [1,2,3,4,5,6,7] trained on expert demonstrations have achieved strong task-specific performance, yet their generalization capabilities remain limited due to constrained model capacity and the absence of broad pretraining.The rise of vision-language models (VLMs) [8,9,10,11] has paved the way for general vision-language-action (VLA) models by offering powerful backbones and pretrained vision-language representations.Recent VLA methods [12,13,14,15] primarily adapt advanced VLMs on large-scale heterogeneous manipulation datasets [16,17,18] by re-engineering the tokenizer, while others [19,20,21,22,23] draw inspiration from low-level policy designs, incorporating techniques like specialized heads and action chunks for better performance.</p>
<p>Prior low-level policies [24,25,26,27] have shown that leveraging multi-frame historical inputs across the temporal dimension considerably improves performance, as motion information from multiframe observations helps determine the current execution phase and effectively resolve ambiguities.However, most existing VLA models [13,28,12,14,15], built on the single-frame paradigm of VLMs, are typically trained with only a single current observation and instruction.Directly feeding multiple historical observations, as commonly done in low-level policies, poses two key challenges: (1) the self-attention computation of vision-language backbones (i.e. the language model and vision encoders) cost scales quadratically with the length of input tokens, hindering large-scale embodied pretraining;</p>
<p>(2) redundant visual tokens considerably degrade inference speed, limiting the practicality of such models in real-world manipulation.TraceVLA [15] integrates motion information by adding an additional visual-tracing image to OpenVLA [12], which may not fully exploit the pretrained capabilities, as OpenVLA and the original VLMs [9] are both rarely trained with such traces.</p>
<p>To facilitate efficient multi-frame modeling, we propose CronusVLA, a general framework for multiframe training and inference, which is adapted from a single-frame VLA model, as illustrated in Figure 1.The approach comprises three key components: (1) Single-frame Pretraining: We first train a basic single-frame VLA model using standard autoregressive prediction over discrete action tokens, enabling efficient utilization of large-scale heterogeneous embodied datasets and establishing an embodied vision-language foundation.(2) Multi-frame Encoding: We augment the basic singleframe VLA model with learnable motion features and post-train it on high-quality cross-embodiment datasets.This post-training effectively adapts the prediction of vision-language backbones from token-level outputs to feature-level.Aggregating motion features from multiple historical frames into a feature chunking ensures the transition of the model from single-frame awareness to multi-frame.(3) Cross-frame Decoding: the feature chunking is modulated to balance current and past information and then decoded by a cross-frame decoder.This process forms a unified mapping from multi-frame input to future actions and progressively shifts the core of action prediction from autoregressive token prediction to the integration of multiple motion features.We further employ multi-frame regularization, which enhances training convergence capability by limiting the influence of past frames and ensures that only the current frame updates the vision-language backbone.</p>
<p>The core insight of our approach: per-frame feature primarily captures spatial information from a single observation, while by aggregating motion features across multiple time steps, motion patterns naturally emerge.The cross-frame decoder exploits this aggregation to build a unified representation that encodes both spatial structure and temporal dynamics.Our approach offers two advantages in terms of efficiency: (1) Fast inference: Each motion feature inherits motion information from multiple discrete action tokens, enabling flexible action prediction in once forward process without autoregressive token generation.Additionally, past motion features can be cached to avoid redundant computation in the vision-language backbone.These designs enable substantial inference speedups over prior VLA models, even with the added complexity of multi-frame modeling (see Figure 1).</p>
<p>(2) Long-horizon compatibility: The cross-frame decoder employs a cross-attention architecture that enables comprehensive decoding while mitigating the typical computational burden of long sequences, resulting in near-constant inference speed regardless of sequence length.Given the strong correlation between motion feature chunkings and executed actions, we further introduce an action adaptation mechanism based on feature-action retrieval.This mechanism retrieves standard actions as informative priors to guide action prediction, ensuring more accurate action prediction.</p>
<p>Our contributions are summarized as follows:</p>
<p>• We propose CronusVLA, a general end-to-end framework that extends VLA models to the multiframe paradigm.Based on single-frame pretraining, CronusVLA unifies multi-frame encoding and cross-frame decoding for action prediction, enabling scalable manipulation learning.• We design a multi-frame post-training strategy that aggregates motion information across frames and decodes it using a cross-frame decoder.This approach enables efficient action prediction while also supporting fast inference and long-horizon compatibility.An action adaptation mechanism is further introduced to provide action prior, resulting in considerable performance gains.• We conduct extensive experiments across three embodiments and diverse manipulation tasks in both the simulation and real world.CronusVLA achieves state-of-the-art performance on the simulation benchmark SimplerEnv [29] with an average 70.9% success rate and achieves a 12.7% overall improvement over OpenVLA on LIBERO [30] benchmark.CronusVLA also demonstrates strong performance and robustness across real-world simple and long-horizon tasks with Franka platform.</p>
<p>Related Works</p>
<p>Vision-Language-Action models.Current VLA models usually integrate action generation based on the framework of VLMs [9,10,11,31].By employing an action tokenizer, RT-2 [13] discretizes 7D actions and employs the PaLI-X [32] for autoregressive prediction, while OpenVLA also tokenizes actions and trains the Prismatic [9] on the OXE dataset [16].LLaRVA [33] directly outputs 2D waypoints and actions following the LLaVA [10] framework.SpatialVLA [14] unifies the action space of various robots via their adaptive action grids.3D-VLA [6] and Magma [34] unify action prediction and multimodal embodied tasks within a single model.These methods aim to minimally adapt VLMs into a general-purpose manipulation policy by treating actions as discrete tokens.Instead, the remaining works [22,35,20,19] abandon the discrete formulation and do not aim to preserve the core paradigm of VLMs.They augment the original VLMs with additional action heads [36,37,38,39], meanwhile, train embodiment capabilities and continuous action prediction from scratch.Our method focuses on how to transfer a single-frame, discretely pretrained embodied VLA model to the multi-frame setting with continuous action prediction.</p>
<p>Multi-frame modeling for robotic manipulation.Most early VLAs [12,28,13,14,21] treat each action prediction as a temporally independent decision, and are also trained in a single-frame manner.In contrast, low-level policies [40,1,24,41] incorporate multi-frame modeling by processing multiple images simultaneously based on their lightweight architectures.Other policies [25,26,27] are pre-trained on large-scale video generation tasks [42,43,17] by interleaving multi-step action and multi-frame image prediction, and effectively learn multi-frame awareness from diverse training data.However, directly applying this strategy to large-scale VLAs introduces considerable computational overhead.To address these limitations, prior work has made preliminary attempts.</p>
<p>TraceVLA [15] uses visual prompting to draw the multi-frame past trace on the current observation.The most related works to ours are RoboFlamingo [35] and RoboVLMs [20], which primarily adopt memory-based heads to model temporal relations across frames.They forcibly extend single-frame pretrained VLMs to multi-frame action prediction by training embodiment capabilities from scratch, akin to standard policy learning.However, they overlook the benefits of effectively transferring a single-frame pretrained VLA model to the multi-frame paradigm, both in terms of training efficiency and performance.In contrast, our method builds on a single-frame pretrained VLA model and explicitly establishes multi-frame capabilities in an additional decoder during post-training, which retains the single-frame perception meanwhile enabling multi-frame modeling.</p>
<p>Methodology</p>
<p>We present the details of our training strategy and model design in this section.In Section 3.1, we describe the single-frame training process.In Section 3.2, we introduce the multi-frame encoding approach and motion features.Cross-frame decoding are shown in Section 3.3, focusing on the cross-frame decoder and multi-frame regularization.Finally, Section 3.4 details the action adaptation mechanism.The overview is illustrated in Figure 2. , where multi-frame modeling is achieved by aggregating motion features from several preceding frames in a cross-frame decoder.In (c), the upper section details the architecture of the decoder, while the lower section shows how the action adapter generates reference actions to serve as prior information.</p>
<p>Single-frame Pretraining</p>
<p>As illustrated in Figure 2 (a), our first step is to establish a foundational vision-language backbone.Off-the-shelf pretrained VLMs [9,44] are adapted into our basic single-frame VLA model by learning diverse manipulation demonstrations D i = (I t , a t , l) Ti t=0 [17,16], where T i is the length of episode i.Here, l is the language instruction, I t denotes the observation from a one-view camera at step t, and a t ∈ R n represents the corresponding actions.Discrete action tokens are derived from continuous robot actions a t via the extended action tokenizer, which maps them into 256 bins, and are trained using the next-token prediction objective with token-level cross-entropy loss, following [13,12].Given I t and l, the model predicts the next-step action tokens and detokenizes them, a t = VLA(I t , l).We observe that the single-frame pretraining effectively transfers the visual perception capabilities of vision encoders [45,46] to embodied scenes, which provides an effective vision-language foundation for multi-frame post-training.Meanwhile, it can better maintain the single-frame visual perception and multi-modal understanding learned during general vision-language pretraining.</p>
<p>Multi-frame Encoding</p>
<p>From discrete action tokens to motion feature chunking.Vision tokens v i , i ∈ [0, n v ] and text tokens l i , i ∈ [0, n l ] are causally computed in the vision-language backbone of our basic single-frame VLA, it autoregressively predict discrete action tokens by summarizing information from all previous tokens.As shown in Figure 2 (b), instead of generating discrete action tokens a t , we introduce learnable motion features f t ∈ R d in the backbone's hidden layers as continuous representations.This feature is designed to integrate the pretrained model's embodied visionlanguage summarization capability and is computed as f t = VL(I t , l).As our basic VLA models are in the single-frame formulation, we introduce feature chunking F M t = {f t−M +1 , . . ., f t−1 , f t } to effectively represent a multi-frame relationship.It is a chunking of historical motion features and can represent multi-frame observations of M steps at the feature-level.During training, we perform multi-frame prediction over M steps by restructuring inputs at the batch level, enabling the vision-language backbone to independently process B × M single-frame inputs per iteration, where B denotes the original batch size.This yields the motion feature chunking F M t for the crossframe decoder.During inference, we maintain the feature chunking using a first-in, first-out queue mechanism, which considerably accelerates inference by reusing prior vision-language computations.</p>
<p>Cross-frame Decoding</p>
<p>Cross-frame decoder.The cross-frame decoder performs action prediction by using the multi-frame motion information embedded within the feature chunking F M t to get action chunking a t:t+K−1 = Decoder(F M t ), as shown in Figure 2 (b~c).Following [38], we construct a Transformer-based decoder composed of self-attention network and MLP layers, and train it using a diffusion loss L diff .To balance the contributions of the current and past motion features in action prediction, we employ a modulator to dynamically modulate the motion features.Specifically, the current motion feature f t ∈ R d is divided to match the number of past motion features M − 1 through DIV function, and processed together to produce the modulated feature Z f : ft = DIV(f t ), where
f t ∈ R d , ft ∈ R (M −1)×d ,(1)Z f = Modulator(F M t ) = MLP {f t−M +1 , . . . , f t−1 }, ft , where Z f ∈ R 2•(M −1)×d ′ ,(2)
where DIV consists of a dimensionality-expanding MLP followed by a feature-splitting operation.</p>
<p>We further adopt a cross-attention mechanism to separate actions and motion features, which enables effective interaction while avoiding increased computational overhead, ensuring the decoder remains scalable to longer horizons.Specifically, Z f is fed into the cross-attention network and mapped to the keys and values, where noised actions â serve as queries.Noised actions are iteratively denoised conditioned on Z f for the final action output.</p>
<p>Post-training with multi-frame regularization.We introduce the multi-frame regularization to decouple the vision-language backbone from multi-frame modeling in the decoder, ensuring its training remains consistent with the single-frame paradigm.Specifically, the past motion features {f t−M +1 , . . ., f t−1 } within the feature chunking F M t are treated as auxiliary inputs to the decoder, with their influence limited to the decoding part.Their gradient flow is restricted, preventing any updates to the vision-language backbone, and just serve solely as a regularization term to facilitate training.The overall objective changes, where sg means the stop-gradient operation:
{f t−M +1 , . . . , f t−1 } = sg VL(I t−k , l) k = 1, . . . , M −1 ,(3)L total = L diff ât:t+K−1 , a t:t+K−1 | {f t−M +1 , . . . , f t−1 }, ft ,(4)
where VL denotes processed by the vision-language backbone.This method offers two advantages:</p>
<p>(1) Extracting past motion features without gradient computation reduces computational and memory overhead, enabling efficient training.(2) Updating on a single-frame basis preserves the pretrained single-frame perceptual capabilities of the backbone network and promotes faster convergence.During finetuning, we employ an action adaptation mechanism for action prediction as shown in Figure 2 (c).We observe that motion feature chunkings F M t effectively capture action patterns from expert demonstrations and exhibit strong correlations with future actions.This enables the use of feature chunkings to retrieve standard actions as coarse priors for guiding action prediction.In Figure 3, each multiframe clip I t−M :t of all expert demonstrations, with length = M , is processed to extract the motion feature chunkings F M t and its associated action sequences.The feature chunking F M t is then transformed into modulated features Z f via the modulator.Assuming there are N clips, we simply flatten and normalize Z f ∈ R C×d ′ into Ẑf ∈ R D , and then form the retrieval matrix X ∈ R N ×D , which supports a feature-to-action search.The adapter performs action retrieval by computing the cosine similarity of the key (i.e., the current Ẑf ) to all vectors in the retrieval matrix X, that is s = X Ẑ⊤ f .The top-k most similar entries ŝ:
ŝ, I = Topk(s), w = softmax(ŝ) ∈ R N (5)
where I ∈ R k denotes the indices of the top-k matches, w is the normalized similarity weights.The reference action is aggregated from the retrieved entries as:
ât:t+K−1 =    0, if max(ŝ) &lt; τ k i=1 w i • a Ii t:t+K−1 , otherwise,(6)
where τ is a confidence threshold to reject ambiguous retrievals.If similarity is low, a Ii t:t+K−1 will be set to zero vectors to indicate uncertainty.Otherwise, it is weighted by corresponding actions to produce the reference actions ât:t+K−1 .Inspired by fusion mechanisms in video [47] and action generation [48] that integrate both priors and noisy information to predict future states, we propose concatenating ât:t+K−1 with noised actions and projecting them into the decoder's shared feature space via a linear layer, serving as a initial state to guide final action generation.</p>
<p>Table 1: Performance comparison on Google Robot and WidowX Robot in SimplerEnv [29].The experiments are conducted across 12 tasks, including both visual matching (VM) and visual aggregation (VA) settings.The "(x B)" following the method name indicates the parameter size of the large language model.The percentage sign % of the success rate is omitted.Implementation details.Our focus is on exploring multi-frame modeling during post-training, building on standard off-the-shelf pretraining methods.In this section, we primarily investigate the performance of our post-trained model.After pretraining the basic single-frame VLA following [12,50] with the OXE dataset [16], we select two high-quality datasets, Bridge-v2 [51] and Fractal [40] datasets, to conduct further cross-embodiment post-training with multi-frame modeling, which include about 148k episodes and 5M multi-frame clips.Our CronusVLA 7B is built on 7B Llama 2 [52], and CronusVLA 0.5B is built on Qwen2.5 0.5B [53].Following [9], they both employ the Dinov2 [46] and SigLip [45] as vision encoders.For input condition, CronusVLA is built on a third-person camera and a text instruction.In addition to the current one-frame observation, CronusVLA-7B is configured with a default of 6 past frames, while CronusVLA-0.5Buses 3 past frames.All experiments are based on A100 GPUs.More training details are included in Appendix C.</p>
<p>SimplerEnv and baselines.We conduct simulation experiments within SimplerEnv [29], a benchmark designed to evaluate the models in performing various tasks with the WidowX Robot (WR) and the Google Robot (GR) environment.The GR environment includes two experimental settings, Visual Matching (VM) and Variant Aggregation (VA), one strictly follows a real-to-sim replication, and another introduces environmental variations.WidowX Robot environment only includes the VM setting.Visualizations are shown in Figure 4. We report the average success rate and adopt the same evaluation setup as in [21].For pretrained models, RT-1-X [16,40], RT-2-X [16,13], and Octo-Based [49] are early baselines, OpenVLA [12], CogACT [21], and Magma [34] are trained on subsets of the OXE dataset and have model sizes exceeding 7B parameters.For the remaining reported models, they are all post-trained on the Fractal and Bridge-v2 datasets.RoboVLMs (2B) [20] is a multi-frame VLA model; we report the results of two official checkpoints trained separately on the two datasets, evaluated independently on the Google and WidowX robots.SpatialVLA (official mixture version) [14] is post-trained from a pretrained version.TraceVLA (7B) and TraceVLA-Phi3 (4B) [15] are trained with additional visual prompting annotations.Basic-Post (7B) denotes the model post-trained in a discrete manner, which starts from our pretrained basic model [12].All models are evaluated using official checkpoints, or their official results are directly reported.Figure 4: Visualization of SimplerEnv [29] and LIBERO [30] simulation settings.robot, CronusVLA 7B shows superior performance and achieves the highest average success rate, +41.5% higher than SpatialVLA and +17.7% higher than CogACT.CronusVLA 0.5B, with a 0.5B language model, outperforms many prior models trained on larger (from 2B to 7B) language models.It achieves the best results in Pick Coke Can and Move Near tasks over all other models, suggesting that excessive parameters may not always be beneficial for simple tasks, emphasizing the value of effective modeling.Both CronusVLA 0.5B and 7B show a promising performance in SimplerEnv.</p>
<p>Results in</p>
<p>Main Results for Finetuning</p>
<p>Evaluation Setup in LIBERO.We evaluate our finetuning stage in the LIBERO [30] simulation benchmark.LIBERO comprises four task suites, including LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long, and they separately evaluate spatial reasoning, object and goal generalization, and long-horizon planning from precise placement to multi-step goal execution.Based on our multi-frame post-trained models, we conduct finetuning for each suite with the action adaptation strategy.We compare our method with the low-level policy Diffusion Policy [1], MDT [54], both of which are trained from scratch.Other models that are finetuned from pre-trained weights, including Octo [49], OpenVLA [12], TraceVLA [15], SpatialVLA [14].All above models except MDT are conditioned on one third-person observation and language instruction, without multi-view observations and state.CronusVLA 7B is finetuned 10K steps starting from our post-trained weight, and the other 5K~12K steps are employed to train the action adapter.The past frame number is 3. Evaluation setup with Franka platform.As shown in Figure 5, we evaluate our method on several real-world tasks with the Franka Research 3 Robot, and utilize the delta end-effector action to control Franka; meanwhile, we utilize a third-person camera for visual input.Three task suites are designed:</p>
<p>Results in LIBERO.</p>
<p>(1) Simple pick-and-place, involves the picking and placing objects with varying colors and shapes across different locations and orientations; (2) Long-horizon tasks, requires coordinated multi-step manipulation and includes putting multiple objects, opening the drawer then closing it, placing objects into a drawer and pressing buttons in a specific order; and (3) Generalization and robustness tasks, evaluating performance on unseen objects, novel instructions, camera occlusions, distractor objects and so on.We manually collect 30 demonstration episodes for each pick objects task, and 50 episodes for the other tasks.All expert demonstrations are used for co-training, and the number of successful rollouts of 25 trials is reported.We implement 3D Diffusion Policy (DP3) [55], and OpenVLA [12] finetuned on these demonstrations, our CronusVLA 7B is finetuned from the post-trained weight.More details of deployment and task descriptions are in Appendix E. In all designed distracted situations, our model shows more general and robust capability than DP3 and OpenVLA.DP3 is sensitive to distractions from extraneous objects and human interference.In the Camera Occlusions task, both DP3 and OpenVLA are adversely affected by frequent visual input dropouts, leading to performance degradation due to their reliance on precise observations for every step.In contrast, our multi-frame modeling effectively withstands such disturbances.We attribute this robustness to the adaptive selection capability of our cross-attention mechanism in the cross-frame decoder, which can prevent corrupted current observations from inducing out-of-distribution actions and also can mitigate the influence of noisy past observations on current execution.</p>
<p>Ablations Study</p>
<p>Ablations on post-training strategies.In Table 3, we evaluate the performance of our multi-frame post-training strategy by comparing the following settings: (1) Basic-Post: Starting from our basic single-frame VLA model and being further post-trained discretely without multi-frame modeling.</p>
<p>(2) + Multi-frame: Built on the Basic-Post by directly adding multiple frames as input to the vision-language backbone and post-trained in the discrete action space.(3) + Trans.&amp;Multi-frame:</p>
<p>Transferring the discrete action prediction to motion feature prediction, but adding multi-frame inputs to the vision-language backbone instead of the decoder.(4) Ours: Our full pipeline that models multi-frame awareness in the decoder.All settings are post-trained on the same datasets, and the past frame number of (2)~( 4) is 6.As shown in    (3.09Hz, a 40.3% decrease), suggesting that naively adding multiple frames may be difficult to obtain an obvious gain even feeding the same input.In contrast, with the transfer from the discrete action pertaining and naive multi-frame modeling, + Trans.&amp;Multi-frameboosts performance to 68.1%, but still slows the inference speed (5.03 Hz).Finally, our full method achieves the best overall performance (70.9%) and considerably improves inference frequency to 8.73 Hz (+68.5%).This shows that our method not only enhances performance but also improves inference efficiency.</p>
<p>Ablation on cross-frame decoder.In Table 4, we present more exploration of architectural variations within the cross-frame decoder.The w/o. modulator setting omits the modulator, treating current (1 frame) and past motion features (6 frames) indiscriminately, which substantially degrades performance due to the potential dominance of irrelevant historical information over critical current cues.For w/o. cross-atten, we remove the cross-attention mechanism of our original design and replace it with a direct self-attention network.It shows our design achieves better overall performance and a higher per-task success rate.Notably, self-attention leads to quadratic growth in computation with past frames increasing, whereas our cross-attention approach scales linearly, enabling more efficient support for long-horizon modeling.We also investigate a flow matching approach [39], substituting the diffusion objective with a flow matching objective, as shown in Table 4 #3.</p>
<p>The impact of frame number.In this section, we analyze how varying the number of input frames affects different multi-frame modeling strategies.Given that tasks differ in their reliance on temporal information, we conduct a representative analysis in the SimplerEnv benchmark.The study compares CronusVLA 7B, CronusVLA 0.5B, and a baseline model (Basic-Post) with a naive multi-frame extension.As shown in Figure 6 (a), results show that increasing the total frame count (the current one frame plus past frames) from 2 to 9 yields different gains in average success rate across 2.5k trials on SimplerEnv.More frames do not consistently lead to better outcomes: CronusVLA 7B performs best with 7 frames, while the 0.5B variant performs best with 4, suggesting that a moderate amount of temporal information can enhance performance, whereas excessive temporal input may lead to performance degradation.Compared with Basic-Post, CronusVLA maintains high inference speed across frame counts, avoiding excessive latency overhead.</p>
<p>Ablation on multi-frame regularization.We find that multi-frame regularization facilitates more accurate action learning and improves model convergence.We attribute this to the property that, during post-training, our multi-frame regularization strategy effectively preserves the single-frame perception capability while allowing sufficient training of the cross-frame decoder.As a result, it consistently outperforms the variant without regularization on SimplerEnv, as shown in Figure 6   Ablation on action adaptation.(1) Action similarity.We compare the action similarity between multi-frame visual features extracted by the vision encoder, as well as the action similarity between the modulated features.We retrieve the top-6 most similar candidates across different episodes and analyze the distribution of their corresponding action similarities, as shown in Figure 6 (c).It shows that our modulated features exhibit a more concentrated distribution in the 0.9 ~1.0 similarity range, indicating a higher likelihood of retrieving accurate actions.This suggests that composing motion features across multiple timesteps is more effective for action retrieval than directly using multi-frame visual features, as the latter are more susceptible to interference from irrelevant backgrounds or fine-grained visual details.</p>
<p>(2) Action adaptation mechanism.As shown in Table 5</p>
<p>Conclusion</p>
<p>We propose CronusVLA, a scalable multi-frame vision-language-action (VLA) framework extended from single-frame VLA models.By integrating single-frame pretraining, multi-frame encoding, and cross-frame decoding, CronusVLA effectively incorporates multi-frame context while mitigating computational overhead.Extensive evaluations demonstrate that CronusVLA achieves improved performance on SimplerEnv and LIBERO, and also exhibits strong robustness in real-world robotic manipulation.Limitations and Future Work are discussed in detail in Appendix H.</p>
<p>A Case Study for Long-horizon Tasks A.1 Real-world Experiments</p>
<p>Our CronusVLA adopts a multi-frame modeling strategy to improve the performance of VLA models, particularly in long-horizon tasks.We find that incorporating temporal information from multiple frames considerably enhances the model's ability to perform multi-step execution, which is crucial for accurately completing complex, sequential instructions.As illustrated in Figure 7, we present a representative case from a real-world long-horizon task: pressing red, yellow, and green buttons in sequence.This task requires both instruction following and proper task decomposition.OpenVLA, as a single-frame model, struggles with these aspects and fails to execute the correct order, as shown in Figure 7 (A, Error 1).Furthermore, due to the presence of ambiguous states-where similar observations occur before and after button presses-OpenVLA cannot reliably distinguish between them, leading to repeated presses of the same button (A, Error 2).In contrast, CronusVLA demonstrates strong task decomposition and instruction-following capabilities, and its multi-frame temporal modeling provides critical context for resolving such ambiguities, enabling more robust and accurate execution in long-horizon scenarios.</p>
<p>A.2 Simulation Experiments</p>
<p>Our model achieves a considerable performance breakthrough on the Put in Drawer task of Sim-plerEnv benchmark, reaching 64.8% and 65.1% in Visual Matching and Variant Aggregation, compared to baseline methods [12,14,20,15], which achieve only 0%-30% success rates.As shown in Figure 8, we present a case study in the SimplerEnv setting with a Google Robot.In</p>
<p>B Additional Experiments B.1 More Explorations on Multi-frame Training</p>
<p>Single-frame pretraining considerably accelerates the convergence of multi-frame training.We compare the effectiveness of applying multi-frame post-training to a pretrained single-frame VLA model with training a multi-frame VLA entirely from scratch.To explore the differences under equal GPU budgets, we adopt identical settings and train a plain VLM model (without discrete embodied pretraining) for the same number of steps (50k for multi-frame modeling), as shown in Figure 9.</p>
<p>In this comparison, Scratch (oxe) refers to training the plain VLM from scratch with multi-frame modeling, on the same OXE datasets as OpenVLA.Scratch (subset) denotes training the VLM with multi-frame modeling on the Fractal and Bridge-v2 datasets.The average SimplerEnv score of our method after training for 50k steps is 70.9, while models trained from scratch fail to converge within the limited training budget (10.4 v.s.70.9 and 18.1 v.s.70.9).We attribute this to the absence of prior embodied knowledge for training from scratch.Forcing the plain VLM to simultaneously learn embodied perception and multi-frame representations is an inherently challenging process within a limited training budget.Notably, even single-frame models, such as OpenVLA [12], typically require 28 to 40 epochs to pretrain on OXE, whereas our setting with 50k steps amounts to fewer than 5 epochs, making training from scratch particularly challenging.Our method can leverage pretrained on-the-shelf VLA checkpoints (e.g., [12,14,50]) in the community, making it a more practical and resource-efficient alternative.</p>
<p>Pretraining with multi-frame regularization.We also investigate the impact of our multi-frame regularization on training a multi-frame VLA entirely from scratch.Without this regularization, as shown by the SimplerEnv scores of Scratch (subset) w/o reg.and Scratch (oxe) w/o reg. in Figure 9, both settings exhibit lower average success rates and a tendency toward slower convergence compared to their regularized counterparts.These results further highlight the effectiveness of multi-frame regularization in facilitating stable and efficient multi-frame training.</p>
<p>sup-fig-F5</p>
<p>Training Steps</p>
<p>B.2 Ablation on Motion Feature Types</p>
<p>Building on the CronusVLA 0.5B model with a minimal frame count (1 past frame, a total of 2), we investigate the impact of different motion feature representations on performance, focusing on settings with limited temporal context.Specifically, we compare three configurations: (1) final, which uses the last-layer of a single learnable feature placed after all visual and language tokens; (2) visual + final, which uses two learnable features, and one inserted after the final visual token and the other after all tokens, all features are extracted from the last layer; and (3) multi-layer final, which extracts the same position's features from three different LLM layers (layers 24, 16, and 8 out of 24).As shown in Table 6, multi-layer final improves performance but increases model complexity, reducing overall usability.Additionally, incorporating extra visual tokens degrades performance, possibly due to misalignment with the VLM's pretrained causal structure, increasing optimization difficulty.Balancing performance and efficiency, we adopt the final strategy in our main model.</p>
<p>B.3 Ablation on Post-training Data</p>
<p>As shown in Table 7, we evaluate the performance of our model in SimplerEnv under different post-training datasets.All experiments are conducted using the CronusVLA 0.5B model with a total two-frame input.We consider three configurations: (1) training on Bridge-v2 [51] and evaluating on the WidowX robot, (2) training on Fractal [40] and evaluating on the Google robot, and (3) joint training on both datasets with evaluation on both platforms.Results indicate that joint training consistently improves performance across robots.We attribute this to the complementary nature of the datasets, which facilitates learning more robust and generalized representations, thereby mitigating overfitting associated with single-dataset training.</p>
<p>B.4 Hyperparameter Sensitivity Analysis of Action Adaptation</p>
<p>We investigate the impact of two key hyperparameters in our action adaptation module: the number of top-k nearest neighbors (K) and the similarity threshold (B), which together govern how reference actions are aggregated and filtered based on quality.In Figure 10 (1) and ( 2), we show how varying top K affects performance on LIBERO-Long and the LIBERO average across all tasks.In Figure 10 (3) and (4), we illustrate the effect of different threshold B values.The results (1) and ( 2) suggest that too few sampled actions may lack robustness, while too many can reduce informativeness.A properly chosen top K enables the model to collect meaningful guidance.Similarly, a low threshold B admits low-quality sampled actions that degrade performance, while a high threshold B may overly restrict the pool, limiting access to useful references.</p>
<p>sup-fig-F4</p>
<p>Figure 10: Hyperparameter sensitivity analysis of action adaptation.We study the impact of two key hyperparameters in the action adaptation module across LIBERO tasks, including top-k nearest neighbors and the similarity threshold.</p>
<p>B.5 Detailed Main Result in SimplerEnv</p>
<p>We report detailed evaluation results for all SimplerEnv settings, focusing on key baselines for comparison.Table 8 presents results on the Google Robot across both Visual Matching and Variant Aggregation, covering four tasks.Extended evaluations include coke can manipulation (horizontal, vertical, and upright grasping) and drawer manipulation (opening and closing).Visualization examples are provided in Figure 18 and Figure 19.Additionally, Table 9 reports results on the WidowX Robot, comparing our models against representative baselines.More visualization results are shown in Figure 20.Metrics include grasp success rate and overall task success.Averaged across all tasks, both CronusVLA 0.5B and CronusVLA 7B outperform prior baselines in most cases, demonstrating the effectiveness of our training strategy and model architecture in achieving robust performance and generalization.</p>
<p>C.1 Pretraining Details</p>
<p>For the 7B model, we follow the training setup and protocol of OpenVLA [12] by leveraging 27 datasets from Open X-Embodiment [16].The same procedure is applied to the 0.5B model based on the VLM of [50].Training is performed by minimizing the cross-entropy loss between the predicted token distribution and ground truth tokens.For more details, please refer to [12].</p>
<p>C.2 Post-training Details</p>
<p>Our models are post-trained on cross-embodiment datasets, including Fractal [13] and Bridge-v2 [51].</p>
<p>We follow the data processing pipeline of Octo [49] and OpenVLA [12], including RLDS preprocessing and augmentations such as random resized cropping, and adjustments to brightness, contrast, saturation, and hue.To enhance generalization, the data loader randomly samples across datasets, trajectories, and time steps.All components, including the vision encoder, projection layer, language model, and decoder, are trained end-to-end by minimizing the mean squared error between predicted and ground-truth diffusion noise.Training employs a 100-step diffusion noise schedule.The language model does not generate text tokens autoregressively; we omit text prediction cross-entropy loss and discard the language head.</p>
<p>CronusVLA 7B is initialized from the pretrained single-frame 7B VLA model.Post-training is conducted on 64 A100 GPUs for approximately 50k gradient steps, with a total batch size of 512 and default 6-frame past observation sequences.We use AdamW with a learning rate of 4e-5 and a linear scheduler, without weight decay or warm-up.PyTorch FSDP is applied to reduce memory usage.Due to the inherent instability of diffusion-based training, which is sensitive to randomness, we select the best-performing checkpoint between 45k and 55k steps, evaluated every 2.5k steps.CronusVLA 0.5B utilizes Qwen2.5 0.5B [56] as the LLM backbone, SigLIP [45] and DINOv2 [46] as the vision encoder.Post-training is performed on 32 A100 GPUs, about 40k gradient steps, with a batch size of 1024 and default 3-frame past observation sequences.We also use AdamW with a learning rate of 4e-5 and a linear scheduler, without weight decay or warm-up.</p>
<p>C.3 Finetuning Details</p>
<p>Building upon the post-trained models, we further finetune on expert demonstrations using an action adaptation strategy.As shown in Figure 12, the process consists of two phases.In the first phase, the post-trained model is adapted to the specific task setup (such as the task suites of LIBERO) by finetuning the whole model (except for Adapter).CronusVLA-7B is finetuned for 10K steps across all tasks.In the second phase, the adapted model is used to construct a retrieval matrix from expert demonstrations, which guides further training.During this phase, the Vision-Language Backbone and Modulator are frozen, and only the Action Adapter and Decoder are trained for an additional 5K-12K steps.Training is conducted with a batch size of 256 on 16 A100 GPUs and a total 15K-22K steps, using a learning rate of 2e-5 and a fixed sequence length of 3-frame past observations.</p>
<p>D.1 Model Architecture</p>
<p>As shown in Table 10, we detail the model architecture.The language model is either LLaMA2 or Qwen2.5, and the vision encoder combines SigLIP and DINOv2, taking 224×224 RGB images as input.A linear layer serves as the cross-modal projector (MM projector).These components comprise the core VLA module, which processes B • F individual samples.The decoder integrates multiple inputs: ground-truth actions, timestep encodings, and multi-frame motion features from the VLA.Actions and timesteps are embedded into a 768-dimensional space via the Action and Timestep Embedders, respectively.The Action Adapter is activated only during finetuning.Motion features are modulated by the Modulator, and the decoder consists of 12 layers of Self-Attention and Cross-Attention, facilitating interaction between noised actions and motion features.</p>
<p>D.2 Feature Chunking</p>
<p>The core logic of feature chunking is depicted in Figure 12 (b).At inference, feature chunking accelerates prediction by caching previously computed motion features.To maintain a consistent feature length M at each step, a first-in-first-out queue is employed.For early timesteps (t &lt; M) with insufficient historical features, we adopt a standard low-level policy strategy by padding the buffer with repeated first-frame features.As similar padding is introduced during training via stochastic sampling, this method does not degrade performance.</p>
<p>D.3 Action Adapter</p>
<p>To guide final action generation, reference actions, which are retrieved based on similarity in the modulated feature space, are incorporated into the decoder as lightweight plugins, as shown in Figure 12 (c).These reference actions act as auxiliary cues rather than strict supervision, given the challenge of obtaining precise ground-truth actions via retrieval.Empirically, we adopt a straightforward approach: the reference action is concatenated with the noised action and passed through an MLP to project it into the decoder's latent space.The resulting embedding is added to the input noised actions as an auxiliary signal, applied only during the initial processing and not propagated across subsequent decoder layers.For each finetuning demonstration dataset, specifically the four LIBERO tasks, a retrieval matrix is constructed.Following the random sampling strategy used during training, we restrict the retrieval similarity threshold to 0.6.To utilize the ground-truth actions during training, the most similar action is explicitly masked.The retrieval process is summarized in Algorithm 1.</p>
<p>• Stack the red cube on the blue cube.This task poses a greater challenge than the previous pick-and-place setup.The robot is required to accurately grasp a red cube from a designated area and stack it on a randomly positioned blue cube.The varying positions of both cubes across trials increase task complexity, especially for models relying exclusively on third-person visual observations.Illustrated in Figure 14 (c).</p>
<p>• Stack the red cup into the green one.This task evaluates the model's capabilities of instruction following and spatial perception.We introduce considerable spatial variability by randomly placing the red, green, and yellow cups within three predefined regions and frequently permuting their positions.During data collection, either the red or yellow cup is randomly placed into the green cup, with corresponding language instructions annotated.And evaluation is restricted to the instruction, "stack the red cup into the green one".Illustrated in Figure 14 (d).</p>
<p>Long-horizon Tasks.We evaluate tasks involving multi-step object manipulation with at least two sub-tasks to evaluate the model's proficiency in sequential execution and long-horizon task composition.Detailed success rates are shown in Table 12, visualizations are shown in Figure 15 and our video demo:</p>
<p>• Put the multiple objects on the plate in order.This task assesses the model's ability of multi-step execution and instructions following.It extends the single-object pick-and-place setup to a multi-object setting.During data collection, one object is randomly selected for each grasping step until all objects are relocated, with varying relative placements in the bowl.In evaluation, the instruction of grasping order is fixed.Illustrated in Figure 15 (a).</p>
<p>• Open and then close the drawer.This task evaluates the model's ability to manipulate articulated objects through sequential actions, emphasizing precise localization of interaction points.The robot must grasp a drawer handle, pull to open, release and reposition the gripper, and push to close the drawer.During both training and evaluation, the drawer is rotated differently, with its initial opening set to one of three predefined states.Success is defined by the correct execution of both opening and closing.Data collection is split into two stages, while evaluation follows a unified sequential protocol.Illustrated in Figure 15 (b).</p>
<p>• Open the drawer and place the carrot into the drawer.This task assesses the model's stability across a wide spatial range and its capability in long-horizon manipulation.The robot must sequentially pull open a drawer, grasp an object from the plate, and place it into the drawer.Variations include the carrot's position and orientation within the plate, as well as the drawer's initial opening state.Illustrated in Figure 15 (c).</p>
<p>• Press the buttons in order.This evaluation primarily measures the model's ability to disambiguate observations and accurately follow instructions.The robot sequentially presses three color-coded buttons whose positions vary within a defined range.The pressing order of testing uses a fixed sequence: red, yellow, then green.The task introduces substantial visual ambiguity, as similar observations occur at different stages, such as before and after button presses.Illustrated in Figure 15 (d).</p>
<p>Generalization and Robustness.We evaluate the generalization and robustness of different policies.</p>
<p>All policies are evaluated on the Put the carrot on the plate task and trained on the data collected in all seven tasks of Simple Pick-and-place.Detailed success rates are shown in Table 13, visualizations are shown in Figure 16 and our video demo:</p>
<p>• Material variance.This experiment aims to evaluate the model's generalization across objects.Carrots differing in appearance and material properties are used, as is shown in Figure 16 (a).We conduct the evaluation using the same spatial variance settings as in the Put the carrot on the plate task.</p>
<p>• Human interference.This experiment assesses the model's robustness under manual disturbances.In each test trial, irrelevant or similar objects are thrown to introduce interference, as shown in Figure 16 (b).</p>
<p>• Camera occlusion.This experiment evaluates the model's robustness to incomplete observations.The camera view is occluded at a fixed frequency to simulate partial or corrupted input, as illustrated in Figure 16 (c).</p>
<p>G Complexity Analysis</p>
<p>For directly processing M past-moment frames, assuming the VLM splits each image into P tokens and the instruction into I tokens, the total number of tokens becomes (M + 1) • P + I, and the self-attention complexity of the language model increases from O((P + I) 2 ) to O(((M + 1)
• P + I) 2 ) ≃ O(M 2 ),(7)
where P ≫ I, thus substantially increasing computational demands.Notably, since the action head or action detokenizer of former VLA models contains fewer parameters than the VLM backbone, the naive approach's complexity is dominated by the quadratic term O(M 2 ).In contrast, our approach processes each frame independently through the VLM and models multi-frame relationships at the feature level, only incurring a linear increase with the number of frames:
O(M • T V LM ) + O(M • T decoder ) ≃ O(M • T V LM ) ≃ O(M ),(8)
where T VLM and T decoder denote the inference time of the VLM and the cross-frame decoder, respectively.As T decoder ≪ T VLM , the overall complexity scales linearly with M , leading to limited impact on inference speed in practice.</p>
<p>H Limitation and Future Works</p>
<p>Efficient utilization of inter-frame information.Inspired by the effectiveness of multi-frame inputs in low-level action prediction, we introduce a feature-level multi-frame modeling approach that utilizes motion features extracted by the VLA module.However, in scenarios where observations and instructions remain largely unchanged across frames, large language models (LLMs) tend to process redundant tokens, leading to considerable spatial redundancy.Mitigating this redundancy by capturing inter-frame differences can enhance model efficiency and lower training costs.A promising direction involves exploring cache-like mechanisms that leverage inter-frame image differences and the compositional nature of LLMs to improve efficiency further.</p>
<p>Language-driven embodied manipulation.Most existing VLAs are fine-tuned from pretrained VLMs but often underexploit the explicit language reasoning capabilities that can guide manipulation, relying instead on implicit vision-language alignment.Similarly, CronusVLA does not explicitly leverage foundational language abilities during inference.In future work, we aim to better integrate language-driven reasoning with action generation to enhance manipulation performance.</p>
<p>Toward a more powerful and generalizable architecture.CronusVLA builds upon the original OpenVLA design, which processes only single-view images and language instructions, without incorporating proprioceptive states or multi-view observations.However, integrating richer modalities is essential for enhancing performance and generalization.In future work, we aim to extend the CronusVLA framework to support multi-view and state-conditioned temporal modeling for broader applicability across diverse embodied tasks.</p>
<p>I Broader Impact</p>
<p>Social Impact.This paper aims to enhance the temporal understanding of vision-language-action (VLA) models for long-horizon robotic manipulation tasks.Improved temporal modeling can benefit assistive robotics in daily life and industrial settings by enabling robots to interpret and execute complex, time-extended instructions more effectively.</p>
<p>Figure 1 :
1
Figure 1: CronusVLA is a multi-frame modeling framework that begins with single-frame pretraining on large-scale manipulation datasets.During post-training on high-quality cross-embodiment datasets, multi-frame encoding and cross-frame decoding are included.The action adaptation is conducted during finetuning.Evaluations in simulation and the real world demonstrate that CronusVLA substantially outperforms prior baselines, with fast inference and long-horizon compatibility.</p>
<p>Figure 2 :
2
Figure 2: Overview of CronusVLA framework.(a) illustrates the training of the basic single-frame VLA.By duplicating the model weights, we perform multi-frame post-training as shown in (b), where multi-frame modeling is achieved by aggregating motion features from several preceding frames in a cross-frame decoder.In (c), the upper section details the architecture of the decoder, while the lower section shows how the action adapter generates reference actions to serve as prior information.</p>
<p>Figure 3 :
3
fig-3</p>
<p>Figure 5 :
5
Figure 5: Real-world experiment with Franka platform.Evaluation of basic pick-and-place capabilities is in (a), long-horizon tasks in (b) demonstrate the advantages of multi-frame modeling in handling temporally dependent manipulations, and (c) generalization and robustness tests, particularly under camera occlusion and various disturbances, highlight the robustness of our model.Results with Franka platform.CronusVLA outperforms other models across almost all tasks.For simple pick-and-place tasks in Figure5(a), all three policies perform well when handling simple objects; however, for tasks requiring more precise manipulation, such as grasping the edge of a bowl or stacking one block (or cup) on another, CronusVLA shows better in-domain performance.Long-horizon tasks are shown in Figure5 (b).CronusVLA exhibits stronger long-horizon learning capabilities, achieving consistently better performance than DP3 and OpenVLA under limited expert demonstrations.Notably, in press buttons in order, OpenVLA tends to press the same button multiple times, indicating state confusion in long-horizon tasks due to the absence of multi-frame information.CronusVLA has inherent temporal awareness to effectively handle state-ambiguous scenarios for button pressing, and has better perception ability for the positioning and orientation required for opening or closing drawers.Generalization and robustness tasks are illustrated in Figure5 (c) and (d).In all designed distracted situations, our model shows more general and robust capability than DP3 and OpenVLA.DP3 is sensitive to distractions from extraneous objects and human interference.In the Camera Occlusions task, both DP3 and OpenVLA are adversely affected by frequent visual input dropouts, leading to performance degradation due to their reliance on precise observations for every step.In contrast, our multi-frame modeling effectively withstands such disturbances.We attribute this robustness to the adaptive selection capability of our cross-attention mechanism in the cross-frame decoder, which can prevent corrupted current observations from inducing out-of-distribution actions and also can mitigate the influence of noisy past observations on current execution.</p>
<p>(c)probability density of action similarity (top-6) visual features modulated features overall success rate (%) the number of total frame inference speed (Hz) (a)the performance and inference speed of multi-frame models 0.9 success rate (%) settings in SimplerEnv (b)ablation for multi-frame regularization</p>
<p>Figure 6 :
6
Figure 6: Extended ablation studies.(a) Varying the number of input frames affects overall success rate and inference speed.(b) The effect of multi-frame regularization on performance, which is across all SimplerEnv settings and based on CronusVLA 7B.(c) Probability density curves of action similarity, obtained via kernel density estimation on 25% randomly sampled training data of LIBERO.</p>
<p>(b).Additional experiments on how it affects the model convergence are provided in Appendix B.1.</p>
<p>Figure 7 :
7
Figure 7: Case study for Press the buttons in order in our real-world long-horizon tasks.We compare and analyze the same trial performed by CronusVLA and OpenVLA in (A) and (B).</p>
<p>(A) and (B), OpenVLA struggles to fully open the drawer, blocking progress at the first step.In contrast, CronusVLA consistently completes all steps of the long-horizon task.In (C) and (D), although RoboVLMs can successfully open the top drawer initially, they fail to maintain the new execution goal, resulting in inaccurate actions and repeated collisions.This may stem from inadequate modeling capacity to adapt execution goals throughout long-horizon tasks.CronusVLA, by contrast, executes all steps smoothly and reliably.</p>
<p>Figure 9 :
9
Figure 9: Convergence trends of training multi-frame VLA entirely from scratch, based on CronusVLA 7B (6 past frames) and tested on SimplerEnv.</p>
<p>C Training Details 2 .Figure 11 :
211
Figure 11: Training pipeline.</p>
<p>Figure 12 :
12
Figure 12: Detailed Design.(a) illustrates the two-stage finetuning process.In the first stage, the Vision-Language Backbone, Modulator, and Decoder are trained jointly to adapt to the task.In the second stage, the Vision-Language Backbone and Modulator are frozen to stabilize the learned representations, and only the Adapter and Decoder are fine-tuned.(b) presents the feature chunking mechanism employed during inference, where the model processes a sequence consisting of three previous frames and one current frame.(c) demonstrates the action adaptation mechanism, in which the reference action is injected into the decoder.</p>
<p>sup-fig-L4 (a) Pick objects (c) Stack the red cube on the blue cube (d) Stack the red cup into the green one (b) Put the carrot on the plate</p>
<p>Figure 14 :
14
Figure 14: Visualizations of real-world Simple Pick-and-Place tasks.</p>
<p>Figure 15 :
15
Figure 15: Visualizations of real-world Long-horizon tasks.</p>
<p>Figure 16 :
16
Figure 16: Visualizations of real-world Generalization and Robustness tasks.</p>
<p>Figure 19 :
19
Figure 19: Visualizations of Google Robot Variant Aggregation tasks in SimplerEnv.</p>
<p>SimplerEnv.Main results are illustrated in Table1.For Google Robot setting, our CronusVLA-7B achieves the highest average success rate, with 78.6 in the VM and 73.8 in the VA, surpassing TraceVLA and RoboVLMs, also multi-frame VLAs, by +71.6% and +37.9% relative VM score, +48.2% and +138.8%relative VA scores.Notably, our model achieves strong performance
WidowX RobotGoogle RobotFranka Robotin SimplerEnvin SimplerEnvin LIBERO
on not only simple tasks Pick Coke Can and Open/Close Drawer, but also the more complex task Put in Drawer, a long-horizon task, requires sequential actions of opening the drawer and placing the apple in.Most previous approaches have consistently failed to attain high success rates.Our method effectively improves the VM success rate to 64.8 and the VA to 65.1 in this task.The highest average scores of VA and VM show our better performance and robustness.For the WidowX</p>
<p>Table 2 :
2
[30] results in LIBERO[30], the average success rate across 3 seeds over 500 trials per task.
Diffusion Policy [1]78.392.568.3 50.5 72.4MDT [54]78.587.573.5 64.8 76.1Octo [49]78.985.784.6 51.1 75.1OpenVLA (7B) [12]84.788.479.2 53.7 76.5TraceVLA (7B) [15]84.685.275.1 54.1 74.8SpatialVLA (3B) [14]88.289.978.6 55.5 78.1CronusVLA (0.5B)90.489.883.2 59.5 80.7CronusVLA (7B)90.194.791.3 68.7 86.2
LIBEROSpatial Object Goal Long Ave.</p>
<p>Table 2
2
presents the evaluation results in the LIBERO benchmark.Our CronusVLA (7B) achieves the highest average success rate of 86.2%, surpassing existing baselines, including strong generalist models such as SpatialVLA (78.1%),OpenVLA (76.5%), and TraceVLA (74.8%).Notably, CronusVLA 7B attains good performance across almost all individual task suites, achieving 94.7% on Object, 91.3% on Goal, and a remarkable 68.7% on long-horizon tasks-demonstrating its strong capability in learning long-horizon executions under multi-frame modeling and action adaptation.In addition, CronusVLA 0.5B, despite its substantially smaller backbone, outperforms several larger models with an average score of 80.7%.It sets the best performance on LIBERO-Spatial (90.4%), showcasing the effectiveness of our model design even under limited capacity.These results demonstrate the effectiveness of our multi-frame architecture and training strategy for downstream task finetuning.</p>
<p>Table 3
3
, Basic-Post achieves a modest overall success rate of 31.0%with 5.18 Hz.Introducing multi-frame inputs without architectural support, + Multi-frame only provides a marginal improvement (+1.4%), but results in a substantial drop in inference speed</p>
<p>Table 3 :
3
Ablation on post-training strategies.The average success rates across all 12 tasks in SimplerEnv and the inference speed are shown in this table, based on CronusVLA 7B.</p>
<h1>MethodsOverall (%) speed (Hz)1 Basic-Post31.05.182 + Multi-frame32.43.093 + Trans.&amp;Multi-frame68.15.034 Ours70.98.73</h1>
<p>Table 4 :
4
Ablation on cross-frame decoder.Based on ours 7B with a past frame number of 6, all designs are evaluated in SimplerEnv, including G-VM, G-VA, and W-VM, following Section 4.1.
1 w/o. modulator68.761.460.463.52 w/o. cross-atten76.471.257.368.33 flow matching75.069.558.467.64 Ours78.673.860.470.9</p>
<h1>MethodG-VM G-VA W-VM Ave.</h1>
<p>Table 5 :
5
Ablation on action adaptation mechanism.With fixed 10k training steps on CronusVLA 7B, FT indicates further finetuning, while Adapt.denotes whether action adaptation is employed.
FT Adapt. Spatial Object Goal Long Ave.88.190.686.5 63.4 82.2✓87.691.686.6 63.6 82.4✓✓90.194.791.3 68.7 86.2</p>
<p>, after training CronusVLA for a fixed number of steps, the model achieves an average success rate of 82.2% on LIBERO.Incorporating our action adaptation and continuing to train both the action adapter and decoder further improves performance to 86.2%.In contrast, continually finetuning the entire language model and decoder under the same training budget introduces performance variance to 82.4% without consistent improvement.To reduce stochastic noise, we report results averaged over repeated evaluations.The results demonstrate the effectiveness of our action adaptation mechanism.Other ablation studies are included in Appendix B.</p>
<p>Table 6 :
6
Ablation on motion feature types.
Representation Google VM Google VA WidowX VM Aveargevisual + final51.342.428.140.6multi-layer final65.360.339.655.1final61.456.640.652.9</p>
<p>Table 7 :
7
Ablation on post-training data.
Fractal Bridge Google VM Google VA WidowX VM✓57.852.0-✓--33.3✓✓61.456.640.6</p>
<p>Table 8 :
8
Detailed results of different methods on Google Robot setting of SimplerEnv.Values are success rates (%).
SettingsMethodPick Coke CanMove Near Open / Close Drawer Put in Drawer OverallHorizontal LayingVertical LayingStanding AverageAverageOpen Close AverageAverageAverageRT-1-X82.033.055.056.731.729.6 89.159.721.342.4RT-2-X74.074.088.078.777.915.7 34.325.03.746.3Octo-Base21.021.09.017.04.20.944.422.70.011.0Visual MatchingOpenVLA RoboVLMs SpatialVLA29.0 91.0 67.08.0 46.0 79.040.0 92.0 92.025.7 76.3 79.355.0 79.0 90.053.7 65.7 31.5 58.3 45.4 63.959.7 44.9 54.60.0 27.8 0.035.1 57.0 55.6Magma64.078.083.075.053.042.8 75.058.98.348.8CronusVLA (0.5B)90.0100.098.096.093.046.3 54.650.542.670.5CronusVLA (7B)95.094.098.095.776.062.0 93.577.864.878.6RT-1-X56.920.469.849.032.36.951.929.410.130.2RT-2-X82.275.489.382.379.233.3 37.235.320.654.4Octo-Base0.50.01.30.63.10.02.11.10.01.2Variant AggregationOpenVLA RoboVLMs SpatialVLA56.9 77.3 89.838.2 31.1 71.167.1 43.6 75.154.1 50.7 78.763.0 62.5 83.019.0 28.0 4.2 16.4 21.7 56.623.5 10.3 39.22.9 0.0 6.335.9 30.9 51.8Magma55.668.981.368.678.546.0 72.059.024.057.5CronusVLA (0.5B)92.997.393.594.678.023.3 50.336.821.757.8CronusVLA (7B)96.996.988.994.277.029.6 87.858.765.173.8</p>
<p>Table 9 :
9
Detailed results of different methods on WidowX Robot setting of SimplerEnv.Values are success rates (%).
MethodPut Spoon on Towel Put Carrot on Plate Stack Green on Yellow Block Put Eggplant in Basket AverageGraspSuccessGrasp Success GraspSuccessGraspSuccessRT-1-X16.70.020.84.28.30.00.00.01.1OpenVLA12.58.329.24.216.70.020.80.03.1RoboVLMs75.041.766.633.383.30.091.762.534.4Magma62.537.541.729.279.220.895.891.744.8SpatialVLA25.020.841.737.579.241.791.783.345.8CronusVLA (0.5B) 54.245.854.233.358.30.079.279.239.6CronusVLA (7B)75.066.779.254.241.720.8100.0100.060.4B.6 More Visualization Results in LIBERO BenchmarkWe illustrate some visualization results of four task suites in Figure 21, including LIBERO Spatial,LIBERO Object, LIBERO Goal, and LIBERO Long.
AppendixInitialize hash set H ← ∅, deduplicated memorySave(M, path) ; Load(path) → M ; Upon loading: call DeduplicateMemory(), BuildIndex() ; BuildIndex():E Details of Real-world ExperimentsThis section details the evaluation setup for real-world experiments on the Franka platform.Model deployment.We evaluate our method on several real-world tasks using the Franka Research 3 Robot equipped with a 7-DoF arm and a 1-DoF Panda Hand gripper.Visual input is provided by a single Intel D435i camera configured in an eye-on-base setup, as shown in Figure13.Both OpenVLA[12]and CronusVLA require over 10 GB of memory in bfloat16 precision; thus, all models are deployed on an online A100 GPU server, while a local machine handles robot control.Although our method and DP3 support higher frequencies, we limit both data collection and action execution to 5 Hz to match OpenVLA's speed constraints and mitigate communication latency.Finetuning data are collected via human teleoperation using a SpaceMouse device.Training details.For CronusVLA, we follow the finetuning protocol from the LIBERO experiments, initializing CronusVLA-7B with post-trained weights and conducting cross-embodiment finetuning on our collected demonstrations.For DP3[55], we adopt the updated instruction-conditioned architecture from[57]and implement it ourselves.We report the best-performing checkpoint of DP3.For OpenVLA[12], we follow the official training protocol, fully finetuning the model until the token accuracy reaches 95%, and report its best checkpoint.All models are trained using a consistent data augmentation strategy.E.2 Tasks SettingsSimple Pick-and-Place.These tasks involve straightforward pick-and-place actions with at most two sub-tasks.Detailed success rates are shown in Table11, visualizations are shown in Figure14and our video demo.We evaluate the model's learning capability and spatial generalization through simple manipulation tasks:• Pick objects.This task assesses the model's fundamental abilities in localization, grasping, and spatial generalization.Four target objects are used: an eggplant, a carrot, a wooden cube, and a bowl.The eggplant and carrot are relatively easy to grasp, while the cube and bowl demand higher placement precision to prevent arm jamming or slippage.Object positions are changed within a region centered on the plate.Illustrated in Figure14 (a).• Put the carrot on the plate.This task primarily evaluates grasping precision and placement accuracy for the single-object pick-and-place.The robot must grasp a carrot from a tabletop region on the right, where objects are positioned with varying locations and orientations, and place it into the designated bowl.Illustrated in Figure14 (b).• Unseen objects.This experiment assesses the model's generalization to grasping different objects.We test objects not seen during training, placed in similar varying positions, as shown in Figure16 (d).• Unseen background.This experiment evaluates the model's robustness to varying backgrounds.The background is changed from a plain white surface to a textured one, as shown in Figure16(e).• Unseen distractors.This experiment assesses the impact of distractor objects on the model's performance.A large number of unrelated additional objects, such as toys, are placed on the table, as shown in Figure16(f).• Variable lighting.This experiment assesses the model's robustness to varying lighting conditions by employing periodic flashing lights, as shown in Figure16(g).J More BackgroundsVision-language data.Our 7B model is based on the Prismatic[9]backbone, which combines the LLaMA2[52]language model with DINOv2[46]and SigLIP[45]as vision encoders.LLaMA2 is pretrained on approximately 2 trillion language tokens, while DINOv2 and SigLIP are trained on large-scale visual datasets.Prismatic is trained for vision-language alignment using datasets such as LAION[58], Conceptual Captions[59], and SBU Captions[60], totaling roughly 558K image-caption pairs.It is further instruction-tuned on 665K multimodal data, including LLaVA Synthetic Data[10], VQA datasets[61,62], referring expression datasets[63,64], and so on.Vision-language-action data.For VLA data, we follow the pretraining protocols of Octo[49]and OpenVLA[12], utilizing 27 datasets from Open X-Embodiment[16].These datasets primarily comprise single-arm demonstrations with at least one third-person observation and corresponding end-effector actions.Note that some subsets, such as Kuka[5], may contain imperfect language instructions.And as shown in[21,12], Droid[17]and Language Table[65]datasets may lead to a potential out-of-distribution issue.Especially, the Fractal dataset is a large-scale collection of open-world manipulation demonstrations, comprising approximately 87k episodes and 3.8M images.These demonstrations were collected using the Google Robot platform, providing a diverse set of real-world robotic interactions.Bridge-v2 is another extensive dataset designed to facilitate scalable robot learning.It contains 60k trajectories and 2.1M images collected across different environments using the WidowX Robot platform.All data is stored in the RLDS format[66]and mixed using predefined ratios during training.Training for discrete action.The cross-entropy loss function is defined as:, where y t represents the target token at position t, y &lt;t denotes the sequence of preceding tokens, x encapsulates the input modalities (including images and language instructions), and P θ is the model's predicted probability distribution parameterized by θ.For action prediction, we follow RT-2[13]and OpenVLA[12], employing a discretization strategy wherein each dimension of the robot's continuous action space is divided into 256 uniform bins.This transforms continuous action values into discrete tokens, facilitating their integration into the language modeling framework.To accommodate these action tokens within the model's vocabulary, OpenVLA repurposes the 256 least frequently used tokens in the Llama tokenizer's vocabulary, replacing them with the newly introduced action tokens.SimplerEnv.In the simulation, we conduct the majority of our experiments within SimplerEnv[29], a benchmark designed to evaluate the capabilities of models in performing various tasks with the WidowX Robot and the Google Robot environment, which can effectively evaluation the crossembodiment generalization and exhibit a strong performance correlation between simulator and realworld scenarios.For the Google Robot environment, SimplerEnv includes two experimental settings.Visual Matching (VM) strictly follows a real-to-sim replication, assessing the model's ability to learn demonstration trajectories while testing its spatial generalization through viewpoint and position variations.Variant Aggregation (VA) introduces environmental variations in background, lighting, distractors, and textures, evaluating the policy's adaptability and robustness in a zero-shot manner.Both of them primarily include tasks such as picking a coke can, moving near, opening/closing the drawer, opening the drawer, and placing the apple in.For the WidowX Robot environment, only the Visual Matching (VM) setting is used, ensuring a faster evaluation aligned with Bridge-v2[51].It mainly includes tasks such as putting the carrot on the plate, putting the spoon on the towel, stacking the green block on the yellow block, and putting the eggplant in the yellow basket.We evaluate the SimplerEnv following the setting in[21].LIBERO.It comprises four task suites, including LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long, to assess specific aspects of knowledge transfer in lifelong robot learning.For LIBERO-Spatial, it focuses on spatial reasoning, involving manipulating identical objects where the primary challenge lies in discerning spatial relationships.LIBERO-Object emphasis is on the object-centric manipulation, requiring the robot to interact with unique objects and highlighting the transfer of object-specific knowledge.LIBERO-Goal evaluates goal-conditioned adaptability, requiring the robot to adjust its actions to achieve different goals, testing its understanding of task objectives.LIBERO-Long is designed to challenge long-term planning and execution, comprising multi-stage tasks that require sustained attention and coordination.Each task suite comprises 10 tasks with 50 human-teleoperated demonstrations.We follow the data cleaning procedure of[12], which filters out corrupted samples and standardizes the data format to RLDS.
Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, Z Xu, S Feng, E Cousineau, Y Du, B Burchfiel, R Tedrake, S Song, The International Journal of Robotics Research. 027836492412736682023</p>
<p>Learning manipulation by predicting interaction. J Zeng, Q Bu, B Wang, W Xia, L Chen, H Dong, H Song, D Wang, D Hu, P Luo, arXivpreprintarXiv:2406.004392024</p>
<p>Robokeygen: robot pose and joint angles estimation via diffusion-based 3D keypoint generation. Y Tian, J Zhang, G Huang, B Wang, P Wang, J Pang, H Dong, ICRA). IEEE. 2024IEEE. 2024</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, PMLR. 2023Conference on Robot Learning. </p>
<p>Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, Conference on robot learning. PMLR2018</p>
<p>3d-vla: A 3d vision-language-action generative world model. H Zhen, X Qiu, P Chen, J Yang, X Yan, Y Du, Y Hong, C Gan, arXivpreprintarXiv:2403.096312024</p>
<p>RoboGround: Robotic Manipulation with Grounded Vision-Language Priors. H Huang, X Chen, Y Chen, H Li, X Han, Z Wang, T Wang, J Pang, Z Zhao, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Paligemma 2: A family of versatile vlms for transfer. A Steiner, A S Pinto, M Tschannen, D Keysers, X Wang, Y Bitton, A Gritsenko, M Minderer, A Sherbondy, S Long, arXivpreprintarXiv:2412.035552024</p>
<p>Prismatic vlms: Investigating the design space of visually-conditioned language models. S Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Forty-first International Conference on Machine Learning. 2024</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Advances in neural information processing systems. 362023</p>
<p>Phi-3 technical report: A highly capable language model locally on your phone. M Abdin, J Aneja, H Awadalla, A Awadallah, A A Awan, N Bach, A Bahree, A Bakhtiari, J Bao, H Behl, arXivpreprintarXiv:2404.142192024</p>
<p>OpenVLA: An Open-Source Vision-Language-Action Model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXivpreprintarXiv:2406.092462024</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXivpreprintarXiv:2307.158182023</p>
<p>SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model. D Qu, H Song, Q Chen, Y Yao, X Ye, Y Ding, Z Wang, J Gu, B Zhao, D Wang, arXivpreprintarXiv:2501.158302025</p>
<p>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. R Zheng, Y Liang, S Huang, J Gao, H Daumé, Iii , A Kolobov, F Huang, J Yang, arXivpreprintarXiv:2412.103452024</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. A O'neill, A Rehman, A Maddukuri, A Gupta, A Padalkar, A Lee, A Pooley, A Gupta, A Mandlekar, A Jain, ICRA). IEEE. 2024IEEE International Conference on Robotics and Automation. 2024</p>
<p>Droid: A large-scale in-the-wild robot manipulation dataset. A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, arXivpreprintarXiv:2403.129452024</p>
<p>RH20T-P: A Primitive-Level Robotic Dataset Towards Composable Generalization Agents. Z Chen, Z Shi, X Lu, L He, S Qian, H S Fang, Z Yin, W Ouyang, J Shao, Y Qiao, arXivpreprintarXiv:2403.196222024</p>
<p>π0: A vision-language-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, 2024</p>
<p>Towards generalist robot policies: What matters in building vision-language-action models. X Li, P Li, M Liu, D Wang, J Liu, B Kang, X Ma, T Kong, H Zhang, H Liu, arXivpreprintarXiv:2412.140582024</p>
<p>Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Q Li, Y Liang, Z Wang, L Luo, X Chen, M Liao, F Wei, Y Deng, S Xu, Y Zhang, arXivpreprintarXiv:2411.196502024</p>
<p>Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. J Wen, Y Zhu, J Li, M Zhu, K Wu, Z Xu, N Liu, R Cheng, C Shen, Y Peng, arXivpreprintarXiv:2409.125142024</p>
<p>Fine-tuning vision-language-action models: Optimizing speed and success. M J Kim, C Finn, P Liang, arXiv:2502.196452025arXiv preprint</p>
<p>Learning fine-grained bimanual manipulation with low-cost hardware. T Z Zhao, V Kumar, S Levine, C Finn, arXiv:2304.137052023arXiv preprint</p>
<p>Unleashing large-scale video generative pre-training for visual robot manipulation. H Wu, Y Jing, C Cheang, G Chen, J Xu, X Li, M Liu, H Li, T Kong, arXivpreprintarXiv:2312.131392023</p>
<p>Gr-2: A generative video-language-action model with web-scale knowledge for robot manipulation. C.-L Cheang, G Chen, Y Jing, T Kong, H Li, Y Li, Y Liu, H Wu, J Xu, Y Yang, arXivpreprintarXiv:2410.061582024</p>
<p>Predictive inverse dynamics models are scalable learners for robotic manipulation. Y Tian, S Yang, J Zeng, P Wang, D Lin, H Dong, J Pang, arXiv:2412.151092024arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXivpreprintarXiv:2303.033782023</p>
<p>Evaluating real-world robot manipulation policies in simulation. X Li, K Hsu, J Gu, K Pertsch, O Mees, H R Walke, C Fu, I Lunawat, I Sieh, S Kirmani, arXivpreprintarXiv:2405.059412024</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. B Liu, Y Zhu, C Gao, Y Feng, Q Liu, Y Zhu, P Stone, Advances in Neural Information Processing Systems. 362024</p>
<p>GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation. N Gao, Y Chen, S Yang, X Chen, Y Tian, H Li, H Huang, H Wang, T Wang, J Pang, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Pali-x: On scaling up a multilingual vision and language model. X Chen, J Djolonga, P Padlewski, B Mustafa, S Changpinyo, J Wu, C R Ruiz, S Goodman, X Wang, Y Tay, arXivpreprintarXiv:2305.185652023</p>
<p>Llarva: Vision-action instruction tuning enhances robot learning. D Niu, Y Sharma, G Biamby, J Quenum, Y Bai, B Shi, T Darrell, R Herzig, arXiv:2406.118152024arXiv preprint</p>
<p>Magma: A foundation model for multimodal ai agents. J Yang, R Tan, Q Wu, R Zheng, B Peng, Y Liang, Y Gu, M Cai, S Ye, J Jang, arXivpreprintarXiv:2502.131302025</p>
<p>Vision-Language Foundation Models as Effective Robot Imitators. X Li, M Liu, H Zhang, C Yu, J Xu, H Wu, C Cheang, Y Jing, W Zhang, H Liu, H Li, T Kong, arXivpreprintarXiv:2311.013782023</p>
<p>Supervised sequence labelling with recurrent neural networks. A Graves, A Graves, 2012Long short-term memory</p>
<p>Diffusion models beat gans on image synthesis. P Dhariwal, A Nichol, Advances in neural information processing systems. 342021</p>
<p>Scalable diffusion models with transformers. W Peebles, S Xie, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Flow matching for generative modeling. Y Lipman, R T Chen, H Ben-Hamu, M Nickel, M Le, arXiv:2210.027472022arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXivpreprintarXiv:2212.068172022</p>
<p>Learning video-conditioned policies for unseen manipulation tasks. E Chane-Sane, C Schmid, I Laptev, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. A Miech, D Zhukov, J.-B Alayrac, M Tapaswi, I Laptev, J Sivic, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Ego4d: Around the world in 3,000 hours of egocentric video. K Grauman, A Westbury, E Byrne, Z Chavis, A Furnari, R Girdhar, J Hamburger, H Jiang, M Liu, X Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. P Wang, S Bai, S Tan, S Wang, Z Fan, J Bai, K Chen, X Liu, J Wang, W Ge, arXivpreprintarXiv:2409.121912024</p>
<p>Sigmoid loss for language image pretraining. X Zhai, B Mustafa, A Kolesnikov, L Beyer, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Dinov2: Learning robust visual features without supervision. M Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Fernandez, D Haziza, F Massa, A El-Nouby, arXivpreprintarXiv:2304.071932023</p>
<p>Stable video diffusion: Scaling latent video diffusion models to large datasets. A Blattmann, T Dockhorn, S Kulal, D Mendelevitch, M Kilian, D Lorenz, Y Levi, Z English, V Voleti, A Letts, arXivpreprintarXiv:2311.151272023</p>
<p>Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation. Q Bu, H Li, L Chen, J Cai, J Zeng, H Cui, M Yao, Y Qiao, arXivpreprintarXiv:2410.080012024</p>
<p>Octo: An Open-Source Generalist Robot Policy. Octo Model Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, C Xu, J Luo, T Kreiman, Y Tan, P Sanketi, Q Vuong, T Xiao, D Sadigh, C Finn, S Levine, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsDelft, Netherlands2024</p>
<p>MiniVLA: A Better VLA with a Smaller Footprint. S Belkhale, D Sadigh, 2024</p>
<p>Bridgedata v2: A dataset for robot learning at scale. H R Walke, K Black, T Z Zhao, Q Vuong, C Zheng, P Hansen-Estruch, A W He, V Myers, M J Kim, M Du, PMLR. 2023Conference on Robot Learning. </p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXivpreprintarXiv:2307.092882023</p>
<p>A Yang, B Yang, B Zhang, B Hui, B Zheng, B Yu, C Li, D Liu, F Huang, H Wei, arXivpreprintarXiv:2412.15115Qwen2. 5 technical report. 2024</p>
<p>Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. M Reuss, Ö E Yagmurlu, F Wenzel, R Lioutikov, arXiv:2407.059962024arXiv preprint</p>
<p>. Y Ze, G Zhang, K Zhang, C Hu, M Wang, H Xu, arXiv-240320243d diffusion policy". In: arXiv e-prints</p>
<p>A Yang, B Yang, B Zhang, B Hui, B Zheng, B Yu, C Li, D Liu, F Huang, H Wei, arXivpreprintarXiv:2412.15115Qwen2. 5 technical report. 2024</p>
<p>3d diffuser actor: Policy diffusion with 3d scene representations. T.-W Ke, N Gkanatsios, K Fragkiadaki, arXiv:2402.108852024arXiv preprint</p>
<p>Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. C Schuhmann, R Vencu, R Beaumont, R Kaczmarczyk, C Mullis, A Katta, T Coombes, J Jitsev, A Komatsuzaki, arXivpreprintarXiv:2111.021142021</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. P Sharma, N Ding, S Goodman, R Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>Im2text: Describing images using 1 million captioned photographs. V Ordonez, G Kulkarni, T Berg, Advances in neural information processing systems. 242011</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Y Goyal, T Khot, D Summers-Stay, D Batra, D Parikh, Proceedings of the IEEE. the IEEE2017</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. D A Hudson, C D Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Referitgame: Referring to objects in photographs of natural scenes. S Kazemzadeh, V Ordonez, M Matten, T Berg, Proceedings of the 2014 conference on empirical methods in natural language processing. the 2014 conference on empirical methods in natural language processing2014</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, L.-J Li, D A Shamma, International journal of computer vision. 1232017</p>
<p>Interactive language: Talking to robots in real time. C Lynch, A Wahid, J Tompson, T Ding, J Betker, R Baruch, T Armstrong, P Florence, IEEE Robotics and Automation Letters. 2023</p>
<p>Rlds: an ecosystem to generate, share and use datasets in reinforcement learning. S Ramos, S Girgin, L Hussenot, D Vincent, H Yakubovich, D Toyama, A Gergely, P Stanczyk, R Marinier, J Harmsen, arXivpreprintarXiv:2111.027672021</p>            </div>
        </div>

    </div>
</body>
</html>
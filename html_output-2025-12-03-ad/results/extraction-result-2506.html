<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2506 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2506</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2506</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-198461880</p>
                <p><strong>Paper Title:</strong> A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off</p>
                <p><strong>Paper Abstract:</strong> Recently, active learning is considered a promising approach for data acquisition due to the significant cost of the data labeling process in many real world applications, such as natural language processing and image processing. Most active learning methods are merely designed to enhance the learning model accuracy. However, the model accuracy may not be the primary goal and there could be other domain-specific objectives to be optimized. In this work, we develop a novel active learning framework that aims to solve a general class of optimization problems. The proposed framework mainly targets the optimization problems exposed to the exploration-exploitation trade-off. The active learning framework is comprehensive, it includes exploration-based, exploitation-based and balancing strategies that seek to achieve the balance between exploration and exploitation. The paper mainly considers regression tasks, as they are under-researched in the active learning field compared to classification tasks. Furthermore, in this work, we investigate the different active querying approaches—pool-based and the query synthesis—and compare them. We apply the proposed framework to the problem of learning the price-demand function, an application that is important in optimal product pricing and dynamic (or time-varying) pricing. In our experiments, we provide a comparative study including the proposed framework strategies and some other baselines. The accomplished results demonstrate a significant performance for the proposed methods.</p>
                <p><strong>Cost:</strong> 0.034</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2506.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2506.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mutual Information active learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration-focused active learning criterion that selects the query maximizing the mutual information between the candidate label and the labels of the unlabeled pool, computed via reductions in joint entropy of the pool under a Bayesian predictive model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mutual Information (MI) active learning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Selects x* that maximizes I(x*, Y_U) = H(Y_U|D) - H(Y_U|x*,D). Uses Bayesian linear regression posterior predictive (multivariate Student-t) to compute entropies of the unlabeled pool before and after hypothetically acquiring (x*,y*), approximating y* by its posterior expectation to avoid integration. Implemented in three modes: pool-based, query-synthesis, and query-synthesis without a predefined pool.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General regression problems; evaluated as case study on dynamic pricing / demand learning (revenue optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates labeling budget to the candidate whose acquisition minimizes posterior entropy of the unlabeled pool (maximizes mutual information). For pool-based: evaluates MI for every pool point; for query-synthesis: optimizes MI over input space (one optimization call per iteration).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Mutual information (difference of multivariate Student-t entropies of unlabeled pool before/after acquiring hypothetical label).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration strategy (minimize model / pool uncertainty). Can be combined in PEE (probabilistic mixing) with a greedy exploitation component.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Indirect: by optimizing mutual information over the pool/prediction set, it prefers queries informative about many unlabeled points, which tends to select representative samples; no explicit diversity penalty beyond MI.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed number of sequential queries (T) with optional discounting of utility (γ).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects one query per iteration until budget exhausted; in query-synthesis variant reduces computational cost by optimizing criterion once rather than evaluating over a large pool.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used within PEE (MI-G) combinations; MI-G variants were among top-performing balancing strategies on revenue in experiments. Exact per-strategy revenues are reported in paper tables (e.g., MI-G-Synth achieves high revenue gain on real datasets; see paper Tables 1 and 10).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Greedy (G), Random Sampling (RS), UCB, EVPI, other proposed exploration metrics (KL, MMI, ME), and pool-vs-synthesis variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MI-G (PEE combining MI exploration with greedy exploitation) yields substantial revenue improvements over purely greedy or random exploration methods in many datasets; performs comparably to other information-theoretic PEE variants (KL-G, MMI-G). Exact numeric comparisons in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Query-synthesis variant is described as significantly more computationally efficient than pool-based MI because it requires optimizing the MI criterion once per iteration instead of evaluating it over all pool samples; no explicit numeric FLOP/time reduction reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>MI emphasizes exploration (uncertainty reduction of the pool) and is combined probabilistically in PEE to trade off immediate utility vs information gain; paper shows MI-based PEE methods achieve better long-term utility on noisy problems compared to greedy/UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Paper recommends query-synthesis MI (or MI within PEE) when pool is large or not representative; MI effectively drives selection toward queries that reduce uncertainty across the unlabeled population, improving downstream utility when combined with exploitation scheduling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2506.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modified Mutual Information</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of MI that explicitly retains the prior entropy term H(Y_U|D) and selects queries using the full mutual information I(x*,Y_U)=H(Y_U|D)-H(Y_U|x*,D) rather than minimizing only conditional entropy, to avoid redundant queries when pool entropy is already low.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Modified Mutual Information (MMI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Same computational backbone as MI but computes the full mutual information including the prior entropy term, thereby prioritizing queries whose acquisition yields large net reduction in joint entropy of the unlabeled pool. Implemented in pool-based, query-synthesis, and query-synthesis-without-pool modes.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General regression active learning; applied to dynamic pricing demand learning.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates budget to candidate queries that produce the largest net reduction in pool joint entropy, avoiding queries that lie in already low-entropy regions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Mutual information between candidate label and unlabeled pool labels (full I).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration-focused; used inside PEE to probabilistically mix with greedy exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via MI over the pool; favors queries informative about many unlabeled points rather than isolated outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed number of sequential queries</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Same as MI; query-synthesis reduces per-iteration compute when pool is large.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MMI-G (PEE with MMI exploration) showed competitive revenue gains in experiments and was among top PEE variants on some tasks (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against MI, KL, ME, UCB, greedy, RS and pool vs synthesis variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MMI variants achieve similar improvements to MI/ KL when combined with greedy in PEE; exact numeric comparisons reported in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Same qualitative computational advantage of query-synthesis vs pool-based noted for MI applies to MMI.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>MMI explicitly considers the current pool entropy to avoid redundant queries; used with PEE to trade exploration early vs exploitation later.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Including the prior entropy term helps avoid querying when pool uncertainty is intrinsically small; recommended when pool entropy varies substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2506.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kullback-Leibler divergence active learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration strategy that selects queries maximizing the KL divergence between the posterior predictive distribution of the unlabeled pool before and after acquiring the candidate label, approximated by averaging KL over pool points and using expected label values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Kullback-Leibler (KL) divergence active learning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>For candidate x*, computes D_KL[p(Y_U|D,(x*,y*)) || p(Y_U|D)] approximated by average of per-point KL divergences (Student-t predictive distributions). Uses expectation E[y*] for unknown label. Implemented pool-based and query-synthesis variants (including synthesis without predefined pool).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Regression active learning; dynamic pricing / demand learning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects queries that most change the model's predictive distribution over unlabeled inputs (largest average KL), allocating budget to experiments expected to have the greatest impact on posterior predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Kullback-Leibler divergence between posterior predictive distributions (averaged over pool points).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration-oriented; commonly used inside PEE (KL-G) combined probabilistically with greedy exploitation for balancing.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicitly considers effect on full predictive distribution across the pool; this promotes selection of queries that alter predictions for diverse unlabeled examples.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed sequential budget</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects one high-impact (high KL) query per iteration; query-synthesis reduces evaluation across large pools.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>KL-G (PEE combining KL exploration with greedy) was the best performing method on the synthetic datasets for revenue gain; reported as achieving ~2-4% improvement over greedy and ~13-15% over UCB on noisy synthetic datasets (paper Discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Greedy, UCB, Random Sampling, MI, MMI, ME, UoS, EVPI and pool-vs-synthesis variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>KL-G variants achieved the highest revenue gains on synthetic experiments and competitive model estimation; numeric improvements: ~2-4% over greedy, ~13-15% over UCB, ~16-18% over RS (on noisy data) per authors' summary.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Query-synthesis KL is computationally cheaper than pool-based KL (one optimization vs evaluating all pool points); no absolute runtime numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>KL maximizes distributional impact (exploration) and within PEE is balanced stochastically with exploitation; performs well especially under high observation noise.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Selecting queries by maximizing expected KL to the posterior predictive distribution allocates budget toward samples with high potential to change belief over the input space, improving downstream utility when combined with occasional exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2506.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Entropy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration strategy that selects queries minimizing the entropy of the model parameter posterior (entropy of β), computed from the posterior multivariate Student-t distribution over regression parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Model Entropy (ME) active learning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computes H(β|D ∪ (x*,y*)) using analytic Student-t entropy formulas (log determinant of correlation matrix plus digamma-function terms) and selects x* that minimizes model parameter entropy, targeting reduction of uncertainty in model parameters directly.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Regression model parameter learning; applied to dynamic pricing demand estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates queries to those expected to most reduce posterior entropy over model parameters, focusing exploration on parameter-identification rather than immediate utility.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Model-parameter entropy reduction (entropy of multivariate Student-t posterior for β).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration for improving parameter estimates; commonly combined in PEE to mix with greedy exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism beyond the entropy objective; entropy minimization may implicitly select informative diverse samples that reduce parameter uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed number of sequential queries</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects queries that produce maximum entropy reduction per sample until budget exhausted; can be tempered via combining with exploitation objectives (UME) or probabilistic scheduling (PEE).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ME-G (PEE with ME exploration) produced substantial revenue gains in experiments and appeared among top PEE variants (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Greedy, UCB, RS, MI, KL, MMI, UoS, EVPI.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ME-based PEE outperforms pure greedy and random baselines in noisy settings; exact numbers in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Query-synthesis ME avoids evaluating entropy impact over large pools, reducing per-iteration computation; no absolute savings quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Directly trades slow-but-valuable parameter learning (exploration) against immediate revenue via combination methods (UME, PEE).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When parameter uncertainty is the bottleneck, minimizing model entropy directs limited experiments to maximize parameter-information per query; combining with exploitation is recommended for utility tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2506.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy (myopic) exploitation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pure exploitation baseline that selects at each iteration the query maximizing the expected immediate utility (e.g., expected revenue) under the current model estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Greedy exploitation (G)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each step pick x* = argmax_x E[u(x)|D]; for dynamic pricing, chooses price maximizing expected immediate revenue p*(x^T µ_β|D). Used as a baseline in pool-based and query-synthesis settings.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Decision/utility optimization under model uncertainty; dynamic pricing experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates budget to immediate utility maximizers (no explicit information-seeking); myopic single-step optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploitation (no exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed sequential queries</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Uses full budget greedily; no long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Greedy performs well in low-noise datasets and real datasets with low observation noise; degrades on high-noise problems. Specific revenue gains reported in tables (e.g., Greedy-Synth shows high revenue on some real datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against UCB, PEE variants (MI-G, KL-G, etc.), UoS, EVPI, RS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Outperformed by balancing methods (e.g., KL-G, UoS) on noisy datasets; comparable or better on low-noise tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Computationally cheap per iteration; myopic focus can lead to worse total utility in noisy/uncertain settings.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper argues greedy is myopic and can incur revenue loss due to lack of exploration; should be combined with exploration strategies in PEE or UoS.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use greedy when model uncertainty is small or noise is low; otherwise mix with exploration to avoid suboptimal long-term utility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2506.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EVPI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Value of Perfect Information</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decision-theoretic criterion adapted to active learning which selects queries maximizing the expected improvement in optimal utility achievable after acquiring the candidate label (difference between optimal utility under updated belief and current optimal utility).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>EVPI-based querying</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computes E[VPI(y*)] = ∫ P(y*|x*,D) [ E[u_opt | D, y*] ] dy* - E[u_opt | D], where u_opt is the utility of the best action under given posterior. For pricing, evaluates the expected optimal revenue achievable after observing candidate demand, and picks price with maximal expected improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Decision-centric active learning for utility maximization; applied to dynamic pricing.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experiments to candidates expected to yield greatest improvement in downstream decision optimality (expected utility of perfect information).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected increase in optimal decision utility (decision-theoretic information/economic value), related to expected regret reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Less-myopic than greedy (explicitly values information for decision improvement), but still focused on expected decision improvement rather than pure exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity promotion; diversity arises from integrating over possible y* outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed sequential budget</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects queries maximizing EVPI per budgeted experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>EVPI exhibited better model estimation than greedy and performed better than greedy on noisy synthetic datasets (yielding larger revenue gains in noisy cases), though on low-noise/real data greedy sometimes outperformed EVPI.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Greedy, UCB, information-theoretic PEE variants, RS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>EVPI improves robustness under noise compared to greedy; exact numbers in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>No explicit computational metric; EVPI requires integrating over possible labels and recomputing optimal action which can be computationally heavier than greedy but cheaper than full non-myopic planning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>EVPI formalizes the value of information for decision quality and thus provides a principled trade-off favoring experiments that most reduce decision regret.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>EVPI is valuable when main objective is decision utility; choose experiments with high expected impact on optimal action rather than pure predictive uncertainty reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2506.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Upper Confidence Bound</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bandit-style balancing rule that selects the candidate maximizing E[u(x)|D] + η * σ_u(x)|D, trading exploitation (expected utility) with exploration (posterior uncertainty scaled by η).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Upper Confidence Bound (UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Implements x_UCB = argmax_x E[u(x)|D] + η σ_u(x)|D, where σ_u is standard deviation of utility under Bayesian predictive model and η is a tunable exploration weight. Used as a baseline; applied in pool-based and query-synthesis variants.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Sequential decision-making and active learning for utility maximization; dynamic pricing experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates queries to candidates with high upper-confidence bound, mixing expected reward and uncertainty proportional to η.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses predictive variance (uncertainty) as proxy for information; not explicit mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Deterministic trade-off via additive confidence bonus (η * σ).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>None explicit, but exploration bonus encourages sampling across uncertain regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed T iterations</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects one candidate per iteration using UCB until budget exhausted; η tuned by practitioner.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UCB used as baseline; performs well on low-noise/robust datasets but is outperformed by proposed balancing methods (e.g., KL-G, UoS) on noisy problems. Paper reports KL-G achieved ~13-15% improvement over UCB on noisy synthetic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to greedy, KL-G, MI-G, MMI-G, ME-G, UoS, EVPI, RS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>UCB is a reasonable simple balancing baseline but less effective than probabilistic PEE methods and UoS on noisy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Computationally cheap per iteration (closed-form expected value and variance under Bayesian linear regression).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper contrasts fixed/confidence-bound balancing (UCB) with probabilistic balancing (PEE) and adaptive strategies (UoS), suggesting probabilistic/dynamic approaches can outperform static UCB especially under noise.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>UCB is simple and effective when reward uncertainty is well-calibrated, but methods that adapt exploration probability or model-driven variance (UoS, PEE) can yield better long-term utility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2506.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic-based Exploration-Exploitation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stochastic scheduling framework that with time-decaying probability p_R chooses exploration (one of several information-theoretic strategies) and otherwise chooses exploitation (e.g., greedy), inspired by simulated annealing / epsilon-decreasing greedy algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Probabilistic-Based Exploration-Exploitation (PEE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At iteration t, exploration is performed with probability p_R = α^(t-1); when exploring, PEE runs an exploration strategy (MI, KL, MMI, ME, uncertain sampling, or random); otherwise performs exploitation (greedy). α<1 controls decay; variants tested include KL-G, MI-G, MMI-G, ME-G, RS-G, US-G.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning for regression under utility objectives; dynamic pricing / revenue maximization.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates a fraction p_R of iterations to exploration based on decaying schedule; exploration strategy selection can be varied to focus on different information objectives; exploitation uses remaining budget to maximize immediate utility.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Depends on chosen exploration sub-strategy (MI, KL, ME employ mutual information, KL divergence, entropy respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Probabilistic mixture where exploration probability decays over iterations (p_R = α^(t-1)); allows more exploration early and more exploitation later.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Depends on exploration sub-strategy — MI/KL encourage representative/diverse selections via pool-aware objectives; random/uncertain sampling provide other diversity modes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed number of sequential queries T</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Budget is partitioned implicitly by probabilistic schedule rather than hard phase split; α and the choice of exploration sub-strategy tune allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PEE variants (KL-G, MI-G, MMI-G, ME-G) achieved strong revenue performance; KL-G was best on synthetic datasets; PEE methods robust in noisy settings. Paper provides percentage revenue gains and estimation errors across datasets (see Tables 1,2,10).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against greedy, UCB, UoS, EVPI, RS, and pool-based vs query-synthesis implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>PEE with information-theoretic exploration outperformed greedy/UCB/RS especially under high-noise; KL-G achieved ~2-4% improvement over greedy and ~13-15% over UCB in noisy synthetic experiments (paper Discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Query-synthesis exploration variants reduce per-iteration computations compared to pool-based exploration; no absolute compute numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>PEE provides an explicit decaying schedule to balance information gathering and reward collection, shown empirically to yield better long-run utility than static phase splits or pure exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Probabilistic, decaying exploration schedules combined with informative exploration heuristics (KL/MI/ME) are effective: explore more early, exploit more later; query-synthesis implementations are recommended for computational efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2506.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UoS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uncertainty of Strategy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A balancing/wrapper method that models uncertainty in the exploitation strategy's recommended query and samples from a Gaussian 'exploration window' whose variance is a function of model uncertainty (two variants: analytic trace-based and Monte Carlo empirical).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Uncertainty of Strategy (UoS) — variants UoS-1 and UoS-2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Treats the exploitation-selected point x_s (e.g., greedy) as the mean of a normal sampling distribution N(x_s, σ_s^2). UoS-1 sets σ_s^2 proportional to model-parameter trace of Σ_β (σ_s^2 = K * (bσ/aσ-1) * trace(Σ_β)). UoS-2 estimates σ_s^2 via Monte Carlo: sample β_i from posterior, compute exploitation-selected x_si for each, and set σ_s^2 to scaled empirical variance of {x_si}. K and decay schedule tune exploration amplitude.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning for regression; dynamic pricing application in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates queries by sampling from a distribution centered on the exploitation suggestion; higher model uncertainty yields larger exploration variance (wider allocation of queries), shrinking over time by schedule K = Z^(t-1).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Indirect: uses model-parameter posterior covariance (trace of Σ_β) or Monte Carlo variability as proxy for strategy uncertainty; not direct mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Probabilistic: exploitation gives center, exploration extent controlled by model uncertainty-derived σ_s^2; early iterations wide exploration, later narrow (more exploitation).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Promotes diversity by sampling around the exploitation choice according to σ_s^2; Monte Carlo variant induces diversity reflecting posterior parameter uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed sequential budget</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Generates one sampled query per iteration; K decay reduces exploration over budgeted iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UoS-1 (synthesis) was the best performing method on the real datasets in terms of revenue gain and had favorable model-error rates; UoS variants were among the top balancing strategies in synthetic experiments. Paper reports UoS achieves substantial revenue gains (see Tables 1,10).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Greedy, UCB, PEE variants (KL-G, MI-G, etc.), EVPI, RS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>UoS variants achieved significant revenue gains vs baselines and competitive model estimation; UoS-1 especially strong on real datasets (paper Discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>UoS-1 uses simple analytic trace which is computationally cheap; UoS-2 requires Monte Carlo sampling (more compute) but provides an empirical estimate of strategy uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>UoS ties exploration window size directly to model uncertainty, providing an interpretable mechanism to balance exploration and exploitation that adapts with data; paper reports robust performance especially when model uncertainty is substantial.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Controlling exploration window by model-parameter uncertainty is an effective allocation principle: allocate broader experimental effort when parameter uncertainty is large, narrow as uncertainty reduces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2506.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Utility minus Model Entropy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-objective hybrid that combines expected utility (exploitation) and model-parameter entropy (exploration) with a tunable weight η that decays over time, selecting queries maximizing E[u(x)|D] - η H(β|x,D).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Utility minus Model Entropy (UME)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Forms an explicit combined objective: x_UME = argmax_x E[u(x)|D] - η H(β|x,D), where H is the Student-t entropy of β after hypothetically acquiring (x,y). η decays exponentially (η = η0 e^{-α t}) to prioritize exploration early and exploitation later. Implemented in pool and synthesis modes.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning for decision-centric regression tasks; dynamic pricing experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates each query by optimizing a hybrid utility-entropy objective that trades immediate reward vs expected reduction in model uncertainty, with the tradeoff parameter annealed over time.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Model-entropy (entropy of β) reduction combined directly with expected utility; uses entropy as formal information measure.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Single-objective scalarization; η governs trade-off and decays over time so exploration dominates early iterations, exploitation later.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity term beyond entropy component; entropy minimization tends to avoid redundant samples but does not directly enforce diversity across hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed sequential budget</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Annealing η over iterations implicitly shifts resource allocation from exploration to exploitation as budget is consumed.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UME variants achieved competitive revenue gains on real datasets and reasonable model accuracy; specific per-dataset numbers are in paper Tables (UME-Synth and UME-Pool entries).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with Greedy, UCB, PEE variants, UoS, EVPI, RS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>UME performed well among balancing methods, often improving revenue vs baselines while controlling model entropy; numerical comparisons reported in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Single-objective optimization may be computationally convenient compared to multi-step look-ahead; query-synthesis reduces per-iteration evaluations when pool is large.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>UME gives an explicit, tunable trade-off between immediate reward and learning; exponential decay of η recommended to shift emphasis from learning to reward as iterations proceed.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Directly optimizing a weighted sum of expected utility and model entropy with time-decreasing exploration weight is an effective allocation strategy for limited-budget decision-driven experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2506.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QuerySynthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Membership Query Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A query generation scheme that synthesizes candidate inputs from the entire input space (rather than selecting from a fixed pool) by optimizing active learning criteria over the continuous domain, improving informativeness and computational efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Query Synthesis active learning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generates synthetic queries by optimizing the chosen selection criterion (MI, KL, ME, UME, UoS sampling, greedy objective) over the input space; three modes: with existing pool, without predefined pool (construct representative unlabeled set), and pure synthesis. Reduces per-iteration compute by optimizing once rather than evaluating over large pools.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Problems with continuous/experiment-controlled input spaces (e.g., dynamic pricing, robotics, physical experiments where synthetic inputs correspond to experimental conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates budget by solving a continuous optimization for an optimal query point under the active-learning objective; avoids being constrained by limited/unrepresentative pools.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Depends on the criterion being optimized (MI, KL, entropy, UME, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Applies same exploration/exploitation criteria as underlying selection metric; used with all proposed strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>By optimizing over full space, can reach diverse/representative points not present in limited pools; helps overcome pool sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed sequential queries</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Optimizes one synthesized query per iteration; more computationally efficient than evaluating entire pool for pool-based strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Query-synthesis variants outperformed pool-based implementations in experiments: average revenue improvement ~3.5% on synthetic datasets and ~10% on real datasets (per paper Discussion and tables). Query-synthesis dominated top-10 revenue-performing strategies (e.g., 76% of top-10 on real datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared pool-based implementations of same criteria; compared against random sampling and pool-based baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Significantly better than pool-based counterparts especially when pool is limited or non-representative; showed average revenue gains reported in Tables 3 and 13 (paper).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Authors state query-synthesis is 'significantly more computationally efficient' because it optimizes the criterion once per iteration instead of evaluating across all pool samples; quantitative runtime/FLOP not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Query-synthesis avoids pool artifacts (lack of representativeness) and reduces computation; recommended when experiments can be designed at arbitrary inputs (scientific experiments, pricing).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When input space is continuous and oracle accepts arbitrary inputs, allocate experiments by synthesizing queries optimized for the active-learning objective rather than sampling a fixed pool.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2506.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoolBased</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pool-Based Active Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard active learning scheme where queries are chosen from a fixed unlabeled pool by evaluating the selection criterion for every pool element and picking the best, often computationally heavy for large pools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pool-based active learning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each iteration evaluate selection score S(x_k) for each x_k in unlabeled pool U and pick argmax_x S(x). Implemented for all proposed strategies (MI, MMI, KL, ME, UME, Greedy etc.) as one variant; computational cost scales with pool size.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Typical ML tasks where a large unlabeled dataset is available but labeling is expensive; applied as baseline scheme in dynamic pricing experiments (though less suitable when pool unrepresentative).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Exhaustively scores existing unlabeled examples and allocates labeling budget to highest-scoring pool items.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Depends on the selection criterion used (MI, KL, entropy, UCB, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Mechanism determined by the selection criterion used; pool-based is a generation scheme rather than a criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Some scoring functions (e.g., Fisher information ratio, MI) can promote diversity when used inside pool-based selection; pool-based may suffer if pool lacks diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed sequential queries from pool until exhausted</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Per-iteration selects best pool element; computational cost grows with pool size.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Pool-based implementations generally underperformed query-synthesis variants in experiments: query-synthesis outperformed pool-based by ~3.5% (synthetic) and ~10% (real) on average for revenue gain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to query-synthesis implementations of same selection criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Inferior when pool is non-representative or when optimal choices lie outside pool; slightly better model-estimation in some synthetic low-noise cases but worse revenue when pool limited.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Less efficient computationally for large pools because it evaluates selection criterion per pool point.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Pool-based selection trades off representativeness and computational cost; if pool is large and representative it can work well, but query-synthesis is often preferred.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>If a representative, diverse pool is available and evaluating all candidates is affordable, pool-based is acceptable; otherwise prefer query-synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2506.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Gradient</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential information-collection policy that picks the alternative that maximizes expected single-step improvement in the objective (expected improvement of decision), often computationally expensive for many alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge-Gradient (KG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maintains Bayesian predictive distribution over alternatives' utilities and selects the alternative with maximum expected one-step value of information (expected improvement in the objective after sampling); cited as an exploitation-based strategy in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Sequential experimental design and ranking-and-selection problems; referenced here in context of decision-centric active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates samples to alternatives with highest expected marginal value for the decision objective (single-step lookahead).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected improvement in utility (decision-value of information).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Primarily exploitation-focused (expected improvement); can be less-myopic than greedy but often single-step in nature.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed number of sequential samples</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects sample maximizing knowledge-gradient per iteration; computationally intensive for large alternative sets.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned relative to proposed methods (KG is pure exploitation and can be computationally heavy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes KG is exploitation-based and computationally expensive for large numbers of alternatives, contrasting it with proposed methods that allow query synthesis and explicit exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e2506.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOCU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mean Objective Cost of Uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decision-theoretic approach that quantifies the cost of model uncertainty by measuring the differential cost between current estimated model and the optimal model for the objective, and guides experiments to reduce that cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Mean Objective Cost of Uncertainty (MOCU)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluates the expected regret (cost) incurred by model uncertainty relative to the optimal model for a given objective, and uses this to prioritize experiments that minimize expected regret; cited in related work as a method handling model uncertainty in terms of impact on performance.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experimental design under model uncertainty (e.g., dynamical systems, control, biological networks).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experiments to minimize expected objective-cost due to uncertainty (minimize MOCU).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Measures impact on expected cost / regret rather than classical information-theoretic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Decision-theoretic focus: experiments are chosen to reduce expected cost (more exploitation-aligned since goal is to lower regret), though can be used to guide exploration where it reduces decision cost.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed sequential budget</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects experiments that most reduce MOCU per budget step.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in related work (contrasted with EVPI and other decision-centric approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper describes conceptually as related to EVPI; MOCU focuses on minimizing performance degradation due to uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e2506.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>epsilon-PAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ε-Pareto Active Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active learning approach for multi-objective optimization assuming objectives are modeled with Gaussian processes and using Bayesian optimization to identify ε-Pareto set efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ε-Pareto Active Learning (ε-PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses Gaussian process priors over multiple objectives and Bayesian active search techniques to identify approximate Pareto-optimal solutions (ε-Pareto set); cited in related work without focus on exploration-exploitation between objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Multi-objective optimization and experimental design (e.g., materials design, engineering trade-offs).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates samples to reduce uncertainty about Pareto front and find points that are likely to be Pareto-optimal under GP models.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Bayesian-optimal selection metrics under GP; acquisition functions tailored to multi-objective settings.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Primarily seeks to discover Pareto front; tradeoffs among objectives handled implicitly via multi-objective acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via multi-objective coverage of Pareto front; not explicitly focused on hypothesis diversity beyond Pareto coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed sequential budget</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Greedy/GP-acquisition based selection until budget exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes ε-PAL assumes GP priors and does not focus on exploration-exploitation trade-off among different objectives as this paper does.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e2506.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALICE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALICE (Population-based active learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A population-based active learning method that estimates an optimal training input density (rather than selecting from a pool) to minimize expected generalization error; uses weighted least-squares in approximately linear regression settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ALICE (population-based active learning)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Assumes knowledge of test input distribution and seeks to design an optimal training density from which to sample (weighted least-squares linear regression) to minimize conditional expected generalization error; contrasted with pool-based selection.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Regression active learning when test distribution is known or estimable.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates sampling effort according to optimized training input density to reduce expected generalization error.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Minimization of expected generalization error (not explicit MI/KL).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Designs training distribution for representativeness with respect to test distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed number of training samples to be collected</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Finds distribution that places sampling weight where it most reduces expected error given budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Referenced to contrast population-based vs pool-based approaches; authors note population-based methods assume known test distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e2506.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FIR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fisher Information Ratio active selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-theoretic selection approach that uses Fisher information to account for diversity among queries, producing a PMF over pool by maximizing Fisher information (semi-definite programming) and sampling accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fisher Information Ratio (FIR) based active learning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a probability mass over pool points by maximizing Fisher information (Fisher information ratio objective) via semi-definite programming; sampling from this PMF yields diverse queries that account for model sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Classification/active learning where diversity and information about parameters matter.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates sampling probability across pool to maximize information about model parameters (Fisher information), supporting diverse selection.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Fisher information metric (accounts for sensitivity/diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration-oriented (seeks informative, diverse queries); not directly exploitation-focused.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit via Fisher information objective which prefers diverse informative points; PMF sampling yields diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed sample budget</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Samples according to optimized PMF until budget exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Mentioned as a method that accounts explicitly for diversity, contrasted with other information-theoretic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2506.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e2506.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Query-by-Committee</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active learning paradigm that uses a committee (ensemble) of models and selects samples about which the committee most disagrees, thereby reducing version space and encouraging diverse informative queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Query-by-Committee (QBC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Trains an ensemble of models on current labeled data and selects unlabeled examples with maximum disagreement (e.g., variance or entropy across committee predictions) to query; referenced as prior art including regression adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Classification and regression active learning; referenced for regression QBC implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates queries to points maximizing model disagreement (proxy for uncertainty across hypothesis space).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Committee disagreement (proxy for version-space reduction / information gain).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration-focused; reduces version space and encourages sampling near model uncertainty boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Disagreement metric tends to select diverse contentious samples; diversity not explicitly enforced beyond disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>fixed sequential queries</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Selects top-disagreement samples from pool until budget is used.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Mentioned as classic active learning approach; paper notes some QBC/regression variants exist but can be computationally intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off', 'publication_date_yy_mm': '2019-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active Learning Literature Survey <em>(Rating: 2)</em></li>
                <li>Nonmyopic active learning of gaussian processes: An exploration-exploitation approach <em>(Rating: 2)</em></li>
                <li>A knowledge-gradient policy for sequential information collection <em>(Rating: 2)</em></li>
                <li>Quantifying the objective cost of uncertainty in complex dynamical systems <em>(Rating: 2)</em></li>
                <li>ε-pal: An active learning approach to the multi-objective optimization problem <em>(Rating: 1)</em></li>
                <li>Active learning with statistical models <em>(Rating: 1)</em></li>
                <li>Optimistic Active-Learning Using Mutual Information <em>(Rating: 1)</em></li>
                <li>A probabilistic active learning algorithm based on fisher information ratio <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2506",
    "paper_id": "paper-198461880",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "MI",
            "name_full": "Mutual Information active learning",
            "brief_description": "An exploration-focused active learning criterion that selects the query maximizing the mutual information between the candidate label and the labels of the unlabeled pool, computed via reductions in joint entropy of the pool under a Bayesian predictive model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Mutual Information (MI) active learning",
            "system_description": "Selects x* that maximizes I(x*, Y_U) = H(Y_U|D) - H(Y_U|x*,D). Uses Bayesian linear regression posterior predictive (multivariate Student-t) to compute entropies of the unlabeled pool before and after hypothetically acquiring (x*,y*), approximating y* by its posterior expectation to avoid integration. Implemented in three modes: pool-based, query-synthesis, and query-synthesis without a predefined pool.",
            "application_domain": "General regression problems; evaluated as case study on dynamic pricing / demand learning (revenue optimization).",
            "resource_allocation_strategy": "Allocates labeling budget to the candidate whose acquisition minimizes posterior entropy of the unlabeled pool (maximizes mutual information). For pool-based: evaluates MI for every pool point; for query-synthesis: optimizes MI over input space (one optimization call per iteration).",
            "computational_cost_metric": null,
            "information_gain_metric": "Mutual information (difference of multivariate Student-t entropies of unlabeled pool before/after acquiring hypothetical label).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Pure exploration strategy (minimize model / pool uncertainty). Can be combined in PEE (probabilistic mixing) with a greedy exploitation component.",
            "diversity_mechanism": "Indirect: by optimizing mutual information over the pool/prediction set, it prefers queries informative about many unlabeled points, which tends to select representative samples; no explicit diversity penalty beyond MI.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "fixed number of sequential queries (T) with optional discounting of utility (γ).",
            "budget_constraint_handling": "Selects one query per iteration until budget exhausted; in query-synthesis variant reduces computational cost by optimizing criterion once rather than evaluating over a large pool.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Used within PEE (MI-G) combinations; MI-G variants were among top-performing balancing strategies on revenue in experiments. Exact per-strategy revenues are reported in paper tables (e.g., MI-G-Synth achieves high revenue gain on real datasets; see paper Tables 1 and 10).",
            "comparison_baseline": "Compared against Greedy (G), Random Sampling (RS), UCB, EVPI, other proposed exploration metrics (KL, MMI, ME), and pool-vs-synthesis variants.",
            "performance_vs_baseline": "MI-G (PEE combining MI exploration with greedy exploitation) yields substantial revenue improvements over purely greedy or random exploration methods in many datasets; performs comparably to other information-theoretic PEE variants (KL-G, MMI-G). Exact numeric comparisons in paper tables.",
            "efficiency_gain": "Query-synthesis variant is described as significantly more computationally efficient than pool-based MI because it requires optimizing the MI criterion once per iteration instead of evaluating it over all pool samples; no explicit numeric FLOP/time reduction reported.",
            "tradeoff_analysis": "MI emphasizes exploration (uncertainty reduction of the pool) and is combined probabilistically in PEE to trade off immediate utility vs information gain; paper shows MI-based PEE methods achieve better long-term utility on noisy problems compared to greedy/UCB.",
            "optimal_allocation_findings": "Paper recommends query-synthesis MI (or MI within PEE) when pool is large or not representative; MI effectively drives selection toward queries that reduce uncertainty across the unlabeled population, improving downstream utility when combined with exploitation scheduling.",
            "uuid": "e2506.0",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "MMI",
            "name_full": "Modified Mutual Information",
            "brief_description": "A variant of MI that explicitly retains the prior entropy term H(Y_U|D) and selects queries using the full mutual information I(x*,Y_U)=H(Y_U|D)-H(Y_U|x*,D) rather than minimizing only conditional entropy, to avoid redundant queries when pool entropy is already low.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Modified Mutual Information (MMI)",
            "system_description": "Same computational backbone as MI but computes the full mutual information including the prior entropy term, thereby prioritizing queries whose acquisition yields large net reduction in joint entropy of the unlabeled pool. Implemented in pool-based, query-synthesis, and query-synthesis-without-pool modes.",
            "application_domain": "General regression active learning; applied to dynamic pricing demand learning.",
            "resource_allocation_strategy": "Allocates budget to candidate queries that produce the largest net reduction in pool joint entropy, avoiding queries that lie in already low-entropy regions.",
            "computational_cost_metric": null,
            "information_gain_metric": "Mutual information between candidate label and unlabeled pool labels (full I).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration-focused; used inside PEE to probabilistically mix with greedy exploitation.",
            "diversity_mechanism": "Implicit via MI over the pool; favors queries informative about many unlabeled points rather than isolated outliers.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "fixed number of sequential queries",
            "budget_constraint_handling": "Same as MI; query-synthesis reduces per-iteration compute when pool is large.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "MMI-G (PEE with MMI exploration) showed competitive revenue gains in experiments and was among top PEE variants on some tasks (see paper tables).",
            "comparison_baseline": "Compared against MI, KL, ME, UCB, greedy, RS and pool vs synthesis variants.",
            "performance_vs_baseline": "MMI variants achieve similar improvements to MI/ KL when combined with greedy in PEE; exact numeric comparisons reported in paper tables.",
            "efficiency_gain": "Same qualitative computational advantage of query-synthesis vs pool-based noted for MI applies to MMI.",
            "tradeoff_analysis": "MMI explicitly considers the current pool entropy to avoid redundant queries; used with PEE to trade exploration early vs exploitation later.",
            "optimal_allocation_findings": "Including the prior entropy term helps avoid querying when pool uncertainty is intrinsically small; recommended when pool entropy varies substantially.",
            "uuid": "e2506.1",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "KL",
            "name_full": "Kullback-Leibler divergence active learning",
            "brief_description": "An exploration strategy that selects queries maximizing the KL divergence between the posterior predictive distribution of the unlabeled pool before and after acquiring the candidate label, approximated by averaging KL over pool points and using expected label values.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Kullback-Leibler (KL) divergence active learning",
            "system_description": "For candidate x*, computes D_KL[p(Y_U|D,(x*,y*)) || p(Y_U|D)] approximated by average of per-point KL divergences (Student-t predictive distributions). Uses expectation E[y*] for unknown label. Implemented pool-based and query-synthesis variants (including synthesis without predefined pool).",
            "application_domain": "Regression active learning; dynamic pricing / demand learning experiments.",
            "resource_allocation_strategy": "Selects queries that most change the model's predictive distribution over unlabeled inputs (largest average KL), allocating budget to experiments expected to have the greatest impact on posterior predictions.",
            "computational_cost_metric": null,
            "information_gain_metric": "Kullback-Leibler divergence between posterior predictive distributions (averaged over pool points).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration-oriented; commonly used inside PEE (KL-G) combined probabilistically with greedy exploitation for balancing.",
            "diversity_mechanism": "Explicitly considers effect on full predictive distribution across the pool; this promotes selection of queries that alter predictions for diverse unlabeled examples.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "fixed sequential budget",
            "budget_constraint_handling": "Selects one high-impact (high KL) query per iteration; query-synthesis reduces evaluation across large pools.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "KL-G (PEE combining KL exploration with greedy) was the best performing method on the synthetic datasets for revenue gain; reported as achieving ~2-4% improvement over greedy and ~13-15% over UCB on noisy synthetic datasets (paper Discussion).",
            "comparison_baseline": "Greedy, UCB, Random Sampling, MI, MMI, ME, UoS, EVPI and pool-vs-synthesis variants.",
            "performance_vs_baseline": "KL-G variants achieved the highest revenue gains on synthetic experiments and competitive model estimation; numeric improvements: ~2-4% over greedy, ~13-15% over UCB, ~16-18% over RS (on noisy data) per authors' summary.",
            "efficiency_gain": "Query-synthesis KL is computationally cheaper than pool-based KL (one optimization vs evaluating all pool points); no absolute runtime numbers provided.",
            "tradeoff_analysis": "KL maximizes distributional impact (exploration) and within PEE is balanced stochastically with exploitation; performs well especially under high observation noise.",
            "optimal_allocation_findings": "Selecting queries by maximizing expected KL to the posterior predictive distribution allocates budget toward samples with high potential to change belief over the input space, improving downstream utility when combined with occasional exploitation.",
            "uuid": "e2506.2",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "ME",
            "name_full": "Model Entropy",
            "brief_description": "An exploration strategy that selects queries minimizing the entropy of the model parameter posterior (entropy of β), computed from the posterior multivariate Student-t distribution over regression parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Model Entropy (ME) active learning",
            "system_description": "Computes H(β|D ∪ (x*,y*)) using analytic Student-t entropy formulas (log determinant of correlation matrix plus digamma-function terms) and selects x* that minimizes model parameter entropy, targeting reduction of uncertainty in model parameters directly.",
            "application_domain": "Regression model parameter learning; applied to dynamic pricing demand estimation.",
            "resource_allocation_strategy": "Allocates queries to those expected to most reduce posterior entropy over model parameters, focusing exploration on parameter-identification rather than immediate utility.",
            "computational_cost_metric": null,
            "information_gain_metric": "Model-parameter entropy reduction (entropy of multivariate Student-t posterior for β).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Pure exploration for improving parameter estimates; commonly combined in PEE to mix with greedy exploitation.",
            "diversity_mechanism": "No explicit diversity mechanism beyond the entropy objective; entropy minimization may implicitly select informative diverse samples that reduce parameter uncertainty.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "fixed number of sequential queries",
            "budget_constraint_handling": "Selects queries that produce maximum entropy reduction per sample until budget exhausted; can be tempered via combining with exploitation objectives (UME) or probabilistic scheduling (PEE).",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "ME-G (PEE with ME exploration) produced substantial revenue gains in experiments and appeared among top PEE variants (see paper tables).",
            "comparison_baseline": "Greedy, UCB, RS, MI, KL, MMI, UoS, EVPI.",
            "performance_vs_baseline": "ME-based PEE outperforms pure greedy and random baselines in noisy settings; exact numbers in tables.",
            "efficiency_gain": "Query-synthesis ME avoids evaluating entropy impact over large pools, reducing per-iteration computation; no absolute savings quantified.",
            "tradeoff_analysis": "Directly trades slow-but-valuable parameter learning (exploration) against immediate revenue via combination methods (UME, PEE).",
            "optimal_allocation_findings": "When parameter uncertainty is the bottleneck, minimizing model entropy directs limited experiments to maximize parameter-information per query; combining with exploitation is recommended for utility tasks.",
            "uuid": "e2506.3",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "G",
            "name_full": "Greedy (myopic) exploitation",
            "brief_description": "A pure exploitation baseline that selects at each iteration the query maximizing the expected immediate utility (e.g., expected revenue) under the current model estimate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Greedy exploitation (G)",
            "system_description": "At each step pick x* = argmax_x E[u(x)|D]; for dynamic pricing, chooses price maximizing expected immediate revenue p*(x^T µ_β|D). Used as a baseline in pool-based and query-synthesis settings.",
            "application_domain": "Decision/utility optimization under model uncertainty; dynamic pricing experiments.",
            "resource_allocation_strategy": "Allocates budget to immediate utility maximizers (no explicit information-seeking); myopic single-step optimization.",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Pure exploitation (no exploration).",
            "diversity_mechanism": "None.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "fixed sequential queries",
            "budget_constraint_handling": "Uses full budget greedily; no long-horizon planning.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Greedy performs well in low-noise datasets and real datasets with low observation noise; degrades on high-noise problems. Specific revenue gains reported in tables (e.g., Greedy-Synth shows high revenue on some real datasets).",
            "comparison_baseline": "Compared against UCB, PEE variants (MI-G, KL-G, etc.), UoS, EVPI, RS.",
            "performance_vs_baseline": "Outperformed by balancing methods (e.g., KL-G, UoS) on noisy datasets; comparable or better on low-noise tasks.",
            "efficiency_gain": "Computationally cheap per iteration; myopic focus can lead to worse total utility in noisy/uncertain settings.",
            "tradeoff_analysis": "Paper argues greedy is myopic and can incur revenue loss due to lack of exploration; should be combined with exploration strategies in PEE or UoS.",
            "optimal_allocation_findings": "Use greedy when model uncertainty is small or noise is low; otherwise mix with exploration to avoid suboptimal long-term utility.",
            "uuid": "e2506.4",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "EVPI",
            "name_full": "Expected Value of Perfect Information",
            "brief_description": "A decision-theoretic criterion adapted to active learning which selects queries maximizing the expected improvement in optimal utility achievable after acquiring the candidate label (difference between optimal utility under updated belief and current optimal utility).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "EVPI-based querying",
            "system_description": "Computes E[VPI(y*)] = ∫ P(y*|x*,D) [ E[u_opt | D, y*] ] dy* - E[u_opt | D], where u_opt is the utility of the best action under given posterior. For pricing, evaluates the expected optimal revenue achievable after observing candidate demand, and picks price with maximal expected improvement.",
            "application_domain": "Decision-centric active learning for utility maximization; applied to dynamic pricing.",
            "resource_allocation_strategy": "Allocates experiments to candidates expected to yield greatest improvement in downstream decision optimality (expected utility of perfect information).",
            "computational_cost_metric": null,
            "information_gain_metric": "Expected increase in optimal decision utility (decision-theoretic information/economic value), related to expected regret reduction.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Less-myopic than greedy (explicitly values information for decision improvement), but still focused on expected decision improvement rather than pure exploration.",
            "diversity_mechanism": "No explicit diversity promotion; diversity arises from integrating over possible y* outcomes.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "fixed sequential budget",
            "budget_constraint_handling": "Selects queries maximizing EVPI per budgeted experiment.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "EVPI exhibited better model estimation than greedy and performed better than greedy on noisy synthetic datasets (yielding larger revenue gains in noisy cases), though on low-noise/real data greedy sometimes outperformed EVPI.",
            "comparison_baseline": "Compared against Greedy, UCB, information-theoretic PEE variants, RS.",
            "performance_vs_baseline": "EVPI improves robustness under noise compared to greedy; exact numbers in paper tables.",
            "efficiency_gain": "No explicit computational metric; EVPI requires integrating over possible labels and recomputing optimal action which can be computationally heavier than greedy but cheaper than full non-myopic planning.",
            "tradeoff_analysis": "EVPI formalizes the value of information for decision quality and thus provides a principled trade-off favoring experiments that most reduce decision regret.",
            "optimal_allocation_findings": "EVPI is valuable when main objective is decision utility; choose experiments with high expected impact on optimal action rather than pure predictive uncertainty reduction.",
            "uuid": "e2506.5",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "UCB",
            "name_full": "Upper Confidence Bound",
            "brief_description": "A bandit-style balancing rule that selects the candidate maximizing E[u(x)|D] + η * σ_u(x)|D, trading exploitation (expected utility) with exploration (posterior uncertainty scaled by η).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Upper Confidence Bound (UCB)",
            "system_description": "Implements x_UCB = argmax_x E[u(x)|D] + η σ_u(x)|D, where σ_u is standard deviation of utility under Bayesian predictive model and η is a tunable exploration weight. Used as a baseline; applied in pool-based and query-synthesis variants.",
            "application_domain": "Sequential decision-making and active learning for utility maximization; dynamic pricing experiments.",
            "resource_allocation_strategy": "Allocates queries to candidates with high upper-confidence bound, mixing expected reward and uncertainty proportional to η.",
            "computational_cost_metric": null,
            "information_gain_metric": "Uses predictive variance (uncertainty) as proxy for information; not explicit mutual information.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Deterministic trade-off via additive confidence bonus (η * σ).",
            "diversity_mechanism": "None explicit, but exploration bonus encourages sampling across uncertain regions.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "fixed T iterations",
            "budget_constraint_handling": "Selects one candidate per iteration using UCB until budget exhausted; η tuned by practitioner.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "UCB used as baseline; performs well on low-noise/robust datasets but is outperformed by proposed balancing methods (e.g., KL-G, UoS) on noisy problems. Paper reports KL-G achieved ~13-15% improvement over UCB on noisy synthetic datasets.",
            "comparison_baseline": "Compared to greedy, KL-G, MI-G, MMI-G, ME-G, UoS, EVPI, RS.",
            "performance_vs_baseline": "UCB is a reasonable simple balancing baseline but less effective than probabilistic PEE methods and UoS on noisy tasks.",
            "efficiency_gain": "Computationally cheap per iteration (closed-form expected value and variance under Bayesian linear regression).",
            "tradeoff_analysis": "Paper contrasts fixed/confidence-bound balancing (UCB) with probabilistic balancing (PEE) and adaptive strategies (UoS), suggesting probabilistic/dynamic approaches can outperform static UCB especially under noise.",
            "optimal_allocation_findings": "UCB is simple and effective when reward uncertainty is well-calibrated, but methods that adapt exploration probability or model-driven variance (UoS, PEE) can yield better long-term utility.",
            "uuid": "e2506.6",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "PEE",
            "name_full": "Probabilistic-based Exploration-Exploitation",
            "brief_description": "A stochastic scheduling framework that with time-decaying probability p_R chooses exploration (one of several information-theoretic strategies) and otherwise chooses exploitation (e.g., greedy), inspired by simulated annealing / epsilon-decreasing greedy algorithms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Probabilistic-Based Exploration-Exploitation (PEE)",
            "system_description": "At iteration t, exploration is performed with probability p_R = α^(t-1); when exploring, PEE runs an exploration strategy (MI, KL, MMI, ME, uncertain sampling, or random); otherwise performs exploitation (greedy). α&lt;1 controls decay; variants tested include KL-G, MI-G, MMI-G, ME-G, RS-G, US-G.",
            "application_domain": "Active learning for regression under utility objectives; dynamic pricing / revenue maximization.",
            "resource_allocation_strategy": "Allocates a fraction p_R of iterations to exploration based on decaying schedule; exploration strategy selection can be varied to focus on different information objectives; exploitation uses remaining budget to maximize immediate utility.",
            "computational_cost_metric": null,
            "information_gain_metric": "Depends on chosen exploration sub-strategy (MI, KL, ME employ mutual information, KL divergence, entropy respectively).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Probabilistic mixture where exploration probability decays over iterations (p_R = α^(t-1)); allows more exploration early and more exploitation later.",
            "diversity_mechanism": "Depends on exploration sub-strategy — MI/KL encourage representative/diverse selections via pool-aware objectives; random/uncertain sampling provide other diversity modes.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "fixed number of sequential queries T",
            "budget_constraint_handling": "Budget is partitioned implicitly by probabilistic schedule rather than hard phase split; α and the choice of exploration sub-strategy tune allocation.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "PEE variants (KL-G, MI-G, MMI-G, ME-G) achieved strong revenue performance; KL-G was best on synthetic datasets; PEE methods robust in noisy settings. Paper provides percentage revenue gains and estimation errors across datasets (see Tables 1,2,10).",
            "comparison_baseline": "Compared against greedy, UCB, UoS, EVPI, RS, and pool-based vs query-synthesis implementations.",
            "performance_vs_baseline": "PEE with information-theoretic exploration outperformed greedy/UCB/RS especially under high-noise; KL-G achieved ~2-4% improvement over greedy and ~13-15% over UCB in noisy synthetic experiments (paper Discussion).",
            "efficiency_gain": "Query-synthesis exploration variants reduce per-iteration computations compared to pool-based exploration; no absolute compute numbers given.",
            "tradeoff_analysis": "PEE provides an explicit decaying schedule to balance information gathering and reward collection, shown empirically to yield better long-run utility than static phase splits or pure exploitation.",
            "optimal_allocation_findings": "Probabilistic, decaying exploration schedules combined with informative exploration heuristics (KL/MI/ME) are effective: explore more early, exploit more later; query-synthesis implementations are recommended for computational efficiency.",
            "uuid": "e2506.7",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "UoS",
            "name_full": "Uncertainty of Strategy",
            "brief_description": "A balancing/wrapper method that models uncertainty in the exploitation strategy's recommended query and samples from a Gaussian 'exploration window' whose variance is a function of model uncertainty (two variants: analytic trace-based and Monte Carlo empirical).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Uncertainty of Strategy (UoS) — variants UoS-1 and UoS-2",
            "system_description": "Treats the exploitation-selected point x_s (e.g., greedy) as the mean of a normal sampling distribution N(x_s, σ_s^2). UoS-1 sets σ_s^2 proportional to model-parameter trace of Σ_β (σ_s^2 = K * (bσ/aσ-1) * trace(Σ_β)). UoS-2 estimates σ_s^2 via Monte Carlo: sample β_i from posterior, compute exploitation-selected x_si for each, and set σ_s^2 to scaled empirical variance of {x_si}. K and decay schedule tune exploration amplitude.",
            "application_domain": "Active learning for regression; dynamic pricing application in experiments.",
            "resource_allocation_strategy": "Allocates queries by sampling from a distribution centered on the exploitation suggestion; higher model uncertainty yields larger exploration variance (wider allocation of queries), shrinking over time by schedule K = Z^(t-1).",
            "computational_cost_metric": null,
            "information_gain_metric": "Indirect: uses model-parameter posterior covariance (trace of Σ_β) or Monte Carlo variability as proxy for strategy uncertainty; not direct mutual information.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Probabilistic: exploitation gives center, exploration extent controlled by model uncertainty-derived σ_s^2; early iterations wide exploration, later narrow (more exploitation).",
            "diversity_mechanism": "Promotes diversity by sampling around the exploitation choice according to σ_s^2; Monte Carlo variant induces diversity reflecting posterior parameter uncertainty.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "fixed sequential budget",
            "budget_constraint_handling": "Generates one sampled query per iteration; K decay reduces exploration over budgeted iterations.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "UoS-1 (synthesis) was the best performing method on the real datasets in terms of revenue gain and had favorable model-error rates; UoS variants were among the top balancing strategies in synthetic experiments. Paper reports UoS achieves substantial revenue gains (see Tables 1,10).",
            "comparison_baseline": "Compared against Greedy, UCB, PEE variants (KL-G, MI-G, etc.), EVPI, RS.",
            "performance_vs_baseline": "UoS variants achieved significant revenue gains vs baselines and competitive model estimation; UoS-1 especially strong on real datasets (paper Discussion).",
            "efficiency_gain": "UoS-1 uses simple analytic trace which is computationally cheap; UoS-2 requires Monte Carlo sampling (more compute) but provides an empirical estimate of strategy uncertainty.",
            "tradeoff_analysis": "UoS ties exploration window size directly to model uncertainty, providing an interpretable mechanism to balance exploration and exploitation that adapts with data; paper reports robust performance especially when model uncertainty is substantial.",
            "optimal_allocation_findings": "Controlling exploration window by model-parameter uncertainty is an effective allocation principle: allocate broader experimental effort when parameter uncertainty is large, narrow as uncertainty reduces.",
            "uuid": "e2506.8",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "UME",
            "name_full": "Utility minus Model Entropy",
            "brief_description": "A single-objective hybrid that combines expected utility (exploitation) and model-parameter entropy (exploration) with a tunable weight η that decays over time, selecting queries maximizing E[u(x)|D] - η H(β|x,D).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Utility minus Model Entropy (UME)",
            "system_description": "Forms an explicit combined objective: x_UME = argmax_x E[u(x)|D] - η H(β|x,D), where H is the Student-t entropy of β after hypothetically acquiring (x,y). η decays exponentially (η = η0 e^{-α t}) to prioritize exploration early and exploitation later. Implemented in pool and synthesis modes.",
            "application_domain": "Active learning for decision-centric regression tasks; dynamic pricing experiments.",
            "resource_allocation_strategy": "Allocates each query by optimizing a hybrid utility-entropy objective that trades immediate reward vs expected reduction in model uncertainty, with the tradeoff parameter annealed over time.",
            "computational_cost_metric": null,
            "information_gain_metric": "Model-entropy (entropy of β) reduction combined directly with expected utility; uses entropy as formal information measure.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Single-objective scalarization; η governs trade-off and decays over time so exploration dominates early iterations, exploitation later.",
            "diversity_mechanism": "No explicit diversity term beyond entropy component; entropy minimization tends to avoid redundant samples but does not directly enforce diversity across hypotheses.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "fixed sequential budget",
            "budget_constraint_handling": "Annealing η over iterations implicitly shifts resource allocation from exploration to exploitation as budget is consumed.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "UME variants achieved competitive revenue gains on real datasets and reasonable model accuracy; specific per-dataset numbers are in paper Tables (UME-Synth and UME-Pool entries).",
            "comparison_baseline": "Compared with Greedy, UCB, PEE variants, UoS, EVPI, RS.",
            "performance_vs_baseline": "UME performed well among balancing methods, often improving revenue vs baselines while controlling model entropy; numerical comparisons reported in tables.",
            "efficiency_gain": "Single-objective optimization may be computationally convenient compared to multi-step look-ahead; query-synthesis reduces per-iteration evaluations when pool is large.",
            "tradeoff_analysis": "UME gives an explicit, tunable trade-off between immediate reward and learning; exponential decay of η recommended to shift emphasis from learning to reward as iterations proceed.",
            "optimal_allocation_findings": "Directly optimizing a weighted sum of expected utility and model entropy with time-decreasing exploration weight is an effective allocation strategy for limited-budget decision-driven experiments.",
            "uuid": "e2506.9",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "QuerySynthesis",
            "name_full": "Membership Query Synthesis",
            "brief_description": "A query generation scheme that synthesizes candidate inputs from the entire input space (rather than selecting from a fixed pool) by optimizing active learning criteria over the continuous domain, improving informativeness and computational efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Query Synthesis active learning",
            "system_description": "Generates synthetic queries by optimizing the chosen selection criterion (MI, KL, ME, UME, UoS sampling, greedy objective) over the input space; three modes: with existing pool, without predefined pool (construct representative unlabeled set), and pure synthesis. Reduces per-iteration compute by optimizing once rather than evaluating over large pools.",
            "application_domain": "Problems with continuous/experiment-controlled input spaces (e.g., dynamic pricing, robotics, physical experiments where synthetic inputs correspond to experimental conditions).",
            "resource_allocation_strategy": "Allocates budget by solving a continuous optimization for an optimal query point under the active-learning objective; avoids being constrained by limited/unrepresentative pools.",
            "computational_cost_metric": null,
            "information_gain_metric": "Depends on the criterion being optimized (MI, KL, entropy, UME, etc.).",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Applies same exploration/exploitation criteria as underlying selection metric; used with all proposed strategies.",
            "diversity_mechanism": "By optimizing over full space, can reach diverse/representative points not present in limited pools; helps overcome pool sparsity.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "fixed sequential queries",
            "budget_constraint_handling": "Optimizes one synthesized query per iteration; more computationally efficient than evaluating entire pool for pool-based strategies.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Query-synthesis variants outperformed pool-based implementations in experiments: average revenue improvement ~3.5% on synthetic datasets and ~10% on real datasets (per paper Discussion and tables). Query-synthesis dominated top-10 revenue-performing strategies (e.g., 76% of top-10 on real datasets).",
            "comparison_baseline": "Compared pool-based implementations of same criteria; compared against random sampling and pool-based baselines.",
            "performance_vs_baseline": "Significantly better than pool-based counterparts especially when pool is limited or non-representative; showed average revenue gains reported in Tables 3 and 13 (paper).",
            "efficiency_gain": "Authors state query-synthesis is 'significantly more computationally efficient' because it optimizes the criterion once per iteration instead of evaluating across all pool samples; quantitative runtime/FLOP not provided.",
            "tradeoff_analysis": "Query-synthesis avoids pool artifacts (lack of representativeness) and reduces computation; recommended when experiments can be designed at arbitrary inputs (scientific experiments, pricing).",
            "optimal_allocation_findings": "When input space is continuous and oracle accepts arbitrary inputs, allocate experiments by synthesizing queries optimized for the active-learning objective rather than sampling a fixed pool.",
            "uuid": "e2506.10",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "PoolBased",
            "name_full": "Pool-Based Active Learning",
            "brief_description": "Standard active learning scheme where queries are chosen from a fixed unlabeled pool by evaluating the selection criterion for every pool element and picking the best, often computationally heavy for large pools.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Pool-based active learning",
            "system_description": "At each iteration evaluate selection score S(x_k) for each x_k in unlabeled pool U and pick argmax_x S(x). Implemented for all proposed strategies (MI, MMI, KL, ME, UME, Greedy etc.) as one variant; computational cost scales with pool size.",
            "application_domain": "Typical ML tasks where a large unlabeled dataset is available but labeling is expensive; applied as baseline scheme in dynamic pricing experiments (though less suitable when pool unrepresentative).",
            "resource_allocation_strategy": "Exhaustively scores existing unlabeled examples and allocates labeling budget to highest-scoring pool items.",
            "computational_cost_metric": null,
            "information_gain_metric": "Depends on the selection criterion used (MI, KL, entropy, UCB, etc.).",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Mechanism determined by the selection criterion used; pool-based is a generation scheme rather than a criterion.",
            "diversity_mechanism": "Some scoring functions (e.g., Fisher information ratio, MI) can promote diversity when used inside pool-based selection; pool-based may suffer if pool lacks diversity.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "fixed sequential queries from pool until exhausted",
            "budget_constraint_handling": "Per-iteration selects best pool element; computational cost grows with pool size.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Pool-based implementations generally underperformed query-synthesis variants in experiments: query-synthesis outperformed pool-based by ~3.5% (synthetic) and ~10% (real) on average for revenue gain.",
            "comparison_baseline": "Compared directly to query-synthesis implementations of same selection criteria.",
            "performance_vs_baseline": "Inferior when pool is non-representative or when optimal choices lie outside pool; slightly better model-estimation in some synthetic low-noise cases but worse revenue when pool limited.",
            "efficiency_gain": "Less efficient computationally for large pools because it evaluates selection criterion per pool point.",
            "tradeoff_analysis": "Pool-based selection trades off representativeness and computational cost; if pool is large and representative it can work well, but query-synthesis is often preferred.",
            "optimal_allocation_findings": "If a representative, diverse pool is available and evaluating all candidates is affordable, pool-based is acceptable; otherwise prefer query-synthesis.",
            "uuid": "e2506.11",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "KG",
            "name_full": "Knowledge-Gradient",
            "brief_description": "A sequential information-collection policy that picks the alternative that maximizes expected single-step improvement in the objective (expected improvement of decision), often computationally expensive for many alternatives.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Knowledge-Gradient (KG)",
            "system_description": "Maintains Bayesian predictive distribution over alternatives' utilities and selects the alternative with maximum expected one-step value of information (expected improvement in the objective after sampling); cited as an exploitation-based strategy in related work.",
            "application_domain": "Sequential experimental design and ranking-and-selection problems; referenced here in context of decision-centric active learning.",
            "resource_allocation_strategy": "Allocates samples to alternatives with highest expected marginal value for the decision objective (single-step lookahead).",
            "computational_cost_metric": null,
            "information_gain_metric": "Expected improvement in utility (decision-value of information).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Primarily exploitation-focused (expected improvement); can be less-myopic than greedy but often single-step in nature.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "fixed number of sequential samples",
            "budget_constraint_handling": "Selects sample maximizing knowledge-gradient per iteration; computationally intensive for large alternative sets.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": null,
            "comparison_baseline": "Mentioned relative to proposed methods (KG is pure exploitation and can be computationally heavy).",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper notes KG is exploitation-based and computationally expensive for large numbers of alternatives, contrasting it with proposed methods that allow query synthesis and explicit exploration.",
            "optimal_allocation_findings": null,
            "uuid": "e2506.12",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "MOCU",
            "name_full": "Mean Objective Cost of Uncertainty",
            "brief_description": "A decision-theoretic approach that quantifies the cost of model uncertainty by measuring the differential cost between current estimated model and the optimal model for the objective, and guides experiments to reduce that cost.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Mean Objective Cost of Uncertainty (MOCU)",
            "system_description": "Evaluates the expected regret (cost) incurred by model uncertainty relative to the optimal model for a given objective, and uses this to prioritize experiments that minimize expected regret; cited in related work as a method handling model uncertainty in terms of impact on performance.",
            "application_domain": "Experimental design under model uncertainty (e.g., dynamical systems, control, biological networks).",
            "resource_allocation_strategy": "Allocates experiments to minimize expected objective-cost due to uncertainty (minimize MOCU).",
            "computational_cost_metric": null,
            "information_gain_metric": "Measures impact on expected cost / regret rather than classical information-theoretic metrics.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Decision-theoretic focus: experiments are chosen to reduce expected cost (more exploitation-aligned since goal is to lower regret), though can be used to guide exploration where it reduces decision cost.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "fixed sequential budget",
            "budget_constraint_handling": "Selects experiments that most reduce MOCU per budget step.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": null,
            "comparison_baseline": "Mentioned in related work (contrasted with EVPI and other decision-centric approaches).",
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper describes conceptually as related to EVPI; MOCU focuses on minimizing performance degradation due to uncertainty.",
            "optimal_allocation_findings": null,
            "uuid": "e2506.13",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "epsilon-PAL",
            "name_full": "ε-Pareto Active Learning",
            "brief_description": "An active learning approach for multi-objective optimization assuming objectives are modeled with Gaussian processes and using Bayesian optimization to identify ε-Pareto set efficiently.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ε-Pareto Active Learning (ε-PAL)",
            "system_description": "Uses Gaussian process priors over multiple objectives and Bayesian active search techniques to identify approximate Pareto-optimal solutions (ε-Pareto set); cited in related work without focus on exploration-exploitation between objectives.",
            "application_domain": "Multi-objective optimization and experimental design (e.g., materials design, engineering trade-offs).",
            "resource_allocation_strategy": "Allocates samples to reduce uncertainty about Pareto front and find points that are likely to be Pareto-optimal under GP models.",
            "computational_cost_metric": null,
            "information_gain_metric": "Bayesian-optimal selection metrics under GP; acquisition functions tailored to multi-objective settings.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Primarily seeks to discover Pareto front; tradeoffs among objectives handled implicitly via multi-objective acquisition.",
            "diversity_mechanism": "Implicit via multi-objective coverage of Pareto front; not explicitly focused on hypothesis diversity beyond Pareto coverage.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "fixed sequential budget",
            "budget_constraint_handling": "Greedy/GP-acquisition based selection until budget exhausted.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper notes ε-PAL assumes GP priors and does not focus on exploration-exploitation trade-off among different objectives as this paper does.",
            "optimal_allocation_findings": null,
            "uuid": "e2506.14",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "ALICE",
            "name_full": "ALICE (Population-based active learning)",
            "brief_description": "A population-based active learning method that estimates an optimal training input density (rather than selecting from a pool) to minimize expected generalization error; uses weighted least-squares in approximately linear regression settings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ALICE (population-based active learning)",
            "system_description": "Assumes knowledge of test input distribution and seeks to design an optimal training density from which to sample (weighted least-squares linear regression) to minimize conditional expected generalization error; contrasted with pool-based selection.",
            "application_domain": "Regression active learning when test distribution is known or estimable.",
            "resource_allocation_strategy": "Allocates sampling effort according to optimized training input density to reduce expected generalization error.",
            "computational_cost_metric": null,
            "information_gain_metric": "Minimization of expected generalization error (not explicit MI/KL).",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": null,
            "diversity_mechanism": "Designs training distribution for representativeness with respect to test distribution.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "fixed number of training samples to be collected",
            "budget_constraint_handling": "Finds distribution that places sampling weight where it most reduces expected error given budget.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Referenced to contrast population-based vs pool-based approaches; authors note population-based methods assume known test distribution.",
            "optimal_allocation_findings": null,
            "uuid": "e2506.15",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "FIR",
            "name_full": "Fisher Information Ratio active selection",
            "brief_description": "An information-theoretic selection approach that uses Fisher information to account for diversity among queries, producing a PMF over pool by maximizing Fisher information (semi-definite programming) and sampling accordingly.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Fisher Information Ratio (FIR) based active learning",
            "system_description": "Constructs a probability mass over pool points by maximizing Fisher information (Fisher information ratio objective) via semi-definite programming; sampling from this PMF yields diverse queries that account for model sensitivity.",
            "application_domain": "Classification/active learning where diversity and information about parameters matter.",
            "resource_allocation_strategy": "Allocates sampling probability across pool to maximize information about model parameters (Fisher information), supporting diverse selection.",
            "computational_cost_metric": null,
            "information_gain_metric": "Fisher information metric (accounts for sensitivity/diversity).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration-oriented (seeks informative, diverse queries); not directly exploitation-focused.",
            "diversity_mechanism": "Explicit via Fisher information objective which prefers diverse informative points; PMF sampling yields diversity.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "fixed sample budget",
            "budget_constraint_handling": "Samples according to optimized PMF until budget exhausted.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Mentioned as a method that accounts explicitly for diversity, contrasted with other information-theoretic metrics.",
            "optimal_allocation_findings": null,
            "uuid": "e2506.16",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        },
        {
            "name_short": "QBC",
            "name_full": "Query-by-Committee",
            "brief_description": "An active learning paradigm that uses a committee (ensemble) of models and selects samples about which the committee most disagrees, thereby reducing version space and encouraging diverse informative queries.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Query-by-Committee (QBC)",
            "system_description": "Trains an ensemble of models on current labeled data and selects unlabeled examples with maximum disagreement (e.g., variance or entropy across committee predictions) to query; referenced as prior art including regression adaptations.",
            "application_domain": "Classification and regression active learning; referenced for regression QBC implementations.",
            "resource_allocation_strategy": "Allocates queries to points maximizing model disagreement (proxy for uncertainty across hypothesis space).",
            "computational_cost_metric": null,
            "information_gain_metric": "Committee disagreement (proxy for version-space reduction / information gain).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration-focused; reduces version space and encourages sampling near model uncertainty boundaries.",
            "diversity_mechanism": "Disagreement metric tends to select diverse contentious samples; diversity not explicitly enforced beyond disagreement.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "fixed sequential queries",
            "budget_constraint_handling": "Selects top-disagreement samples from pool until budget is used.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": null,
            "tradeoff_analysis": "Mentioned as classic active learning approach; paper notes some QBC/regression variants exist but can be computationally intensive.",
            "optimal_allocation_findings": null,
            "uuid": "e2506.17",
            "source_info": {
                "paper_title": "A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off",
                "publication_date_yy_mm": "2019-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active Learning Literature Survey",
            "rating": 2,
            "sanitized_title": "active_learning_literature_survey"
        },
        {
            "paper_title": "Nonmyopic active learning of gaussian processes: An exploration-exploitation approach",
            "rating": 2,
            "sanitized_title": "nonmyopic_active_learning_of_gaussian_processes_an_explorationexploitation_approach"
        },
        {
            "paper_title": "A knowledge-gradient policy for sequential information collection",
            "rating": 2,
            "sanitized_title": "a_knowledgegradient_policy_for_sequential_information_collection"
        },
        {
            "paper_title": "Quantifying the objective cost of uncertainty in complex dynamical systems",
            "rating": 2,
            "sanitized_title": "quantifying_the_objective_cost_of_uncertainty_in_complex_dynamical_systems"
        },
        {
            "paper_title": "ε-pal: An active learning approach to the multi-objective optimization problem",
            "rating": 1,
            "sanitized_title": "εpal_an_active_learning_approach_to_the_multiobjective_optimization_problem"
        },
        {
            "paper_title": "Active learning with statistical models",
            "rating": 1,
            "sanitized_title": "active_learning_with_statistical_models"
        },
        {
            "paper_title": "Optimistic Active-Learning Using Mutual Information",
            "rating": 1,
            "sanitized_title": "optimistic_activelearning_using_mutual_information"
        },
        {
            "paper_title": "A probabilistic active learning algorithm based on fisher information ratio",
            "rating": 1,
            "sanitized_title": "a_probabilistic_active_learning_algorithm_based_on_fisher_information_ratio"
        }
    ],
    "cost": 0.034443749999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off
Published: 1 July 2019</p>
<p>Dina Elreedy 
Computer Engineering Department
Cairo University
12613GizaEgypt</p>
<p>Amir F Atiya 
Computer Engineering Department
Cairo University
12613GizaEgypt</p>
<p>Samir I Shaheen 
Computer Engineering Department
Cairo University
12613GizaEgypt</p>
<p>A Novel Active Learning Regression Framework for Balancing the Exploration-Exploitation Trade-Off
Published: 1 July 201910.3390/e21070651Received: 31 May 2019; Accepted: 28 June 2019;entropy Articleactive learningexploration-exploitationregressionoptimizationmutual informationKullback-Leibler divergenceentropyquery synthesisdemand learningexploration-exploitationsequential decision problems
Recently, active learning is considered a promising approach for data acquisition due to the significant cost of the data labeling process in many real world applications, such as natural language processing and image processing. Most active learning methods are merely designed to enhance the learning model accuracy. However, the model accuracy may not be the primary goal and there could be other domain-specific objectives to be optimized. In this work, we develop a novel active learning framework that aims to solve a general class of optimization problems. The proposed framework mainly targets the optimization problems exposed to the exploration-exploitation trade-off. The active learning framework is comprehensive, it includes exploration-based, exploitation-based and balancing strategies that seek to achieve the balance between exploration and exploitation. The paper mainly considers regression tasks, as they are under-researched in the active learning field compared to classification tasks. Furthermore, in this work, we investigate the different active querying approaches-pool-based and the query synthesis-and compare them. We apply the proposed framework to the problem of learning the price-demand function, an application that is important in optimal product pricing and dynamic (or time-varying) pricing. In our experiments, we provide a comparative study including the proposed framework strategies and some other baselines. The accomplished results demonstrate a significant performance for the proposed methods.</p>
<p>Introduction</p>
<p>Recently, active learning has received a substantial growing interest in literature. With the abundant amounts of unlabeled data, the cost of data labelling is, generally, expensive. Thus, active learning is used for selecting the most informative "beneficial" training samples for the learning model in order to achieve high model accuracy using as few examples as possible [1]. Active learning has proved its superiority in diverse applications such as natural language processing [2] and image processing [3]. The active learning process basically proceeds as follows: first, an initial learning model is trained using a few training samples. Then, additional samples are sequentially added to the training data according to a certain querying strategy. This process repeats until a certain stopping criterion is satisfied [4].</p>
<p>Generally, most of the active learning research mainly focuses on querying data labels to optimize the learning model's accuracy. Only a few contributions utilize active learning for achieving other objectives. However, in many applications, the data labeling process is costly and the ultimate goal is to optimize a domain-specific objective function, other than minimizing the learning model's predictive error. Accordingly, in this work, we propose a comprehensive active learning framework which consists of several novel querying strategies for handling general optimization problems where the objective could be some general utility function, not necessarily the learning model's accuracy. The problem can be framed as selecting the right trade-off for the exploration-exploitation concept. In other words, we encounter a trade-off between minimizing the uncertainty of the target objective function, known as exploration and maximizing the underlying objective function given the available function estimates, which is known as exploitation. The exploration-exploitation trade-off is encountered in machine learning [5] and optimization algorithms [6]. Furthermore, this class of optimization problems experiencing a trade-off between exploration and exploitation is prevalent in many real-world applications of various fields, such as recommender systems [7] and dynamic pricing [8].</p>
<p>In this paper, we provide a comprehensive analysis of active learning from the point of view of the exploration-exploitation trade-off. Our focus is on having a general optimization function, rather than prediction accuracy. For example, the user may like to select a query point that maximizes his profit. As a case study, we consider the application of the proposed active learning framework to some real-world application, namely dynamic pricing for revenue maximization in case of unknown behavior of the customers' demand [9]. Specifically, firms offering a certain good or service seek to adjust prices in a way that maximizes the obtained revenue. However, the price-demand curve which controls the relation between the price and the corresponding behavior of customers, is usually not known beforehand and has to be inferred. Generally, companies learn the price-demand curve through price experimentation by testing a number of prices and obtaining their corresponding demands from actual selling situations. On the other hand, choosing prices for revealing the price-demand relation could yield revenue losses since such prices are not designed to maximize the achieved revenue [10,11].</p>
<p>Therefore, we are dealing with two conflicting goals: exploration in the form of choosing prices that minimize the uncertainty of the learned demand model and exploitation in the form of setting prices to maximize the objective function, that is, the obtained revenue. The former is accomplished in a framework of active learning: what price should we suggest next to gain the most knowledge of the demand-price function?</p>
<p>The aforementioned problem of revenue maximization with demand learning represents a case study which can be considered an application of our proposed framework. However, the presented active learning framework is general and it can be applied to any objective optimization problem incurring a trade-off between exploration and exploitation.</p>
<p>The proposed active learning framework consists basically of three main active learning approaches: exploration-based, exploitation-based and balancing strategies that handle both exploration and exploitation. For the exploration-based methods, we propose several novel information-theoretic strategies with the aim of minimizing the learning model uncertainty. On the other hand, the exploitation-based methods are designed merely to optimize the target objective function, without taking into consideration the model accuracy. Finally, we present several active learning strategies specifically designed to address the exploration-exploitation trade-off by combining both objectives of optimizing the target objective and obtaining an accurate learning model.</p>
<p>We apply a set of experiments to evaluate the performance of our proposed active learning methods in terms of both aspects: exploitation in terms of the gained utility and exploration by measuring the regression model's accuracy. In these experiments, we compare the performance of our proposed methods to some standard baselines.</p>
<p>Active learning has been extensively studied in classification problems [4]. However, only few studies investigate applying active learning to regression tasks [12][13][14]. In this work, our presented active learning framework mainly targets regression problems. However, it could be easily adapted to handle classification problems, as well.</p>
<p>Active learning is generally classified into sequential and batch mode settings. In the sequential setting, one query sample is selected per iteration. On the other hand, for the batch mode, a group of samples are simultaneously selected for labeling. In this work, we adopt the sequential active learning approach.</p>
<p>Another scheme used for classifying the active learning methods is based on the query generation process. Specifically, active learning is classified into: pool-based and query synthesis approaches. The pool-based approach is the conventional method which is most commonly used in the active learning literature [4]. In the pool-based scheme, at each iteration, one or more query samples are selected from an unlabeled pool of existing data according to a certain querying criterion and labeling is carried out for these selected samples. On the other hand, the membership query synthesis approach selects one or more synthetic samples from the whole space. In this paper, we apply both approaches-pool-based and query synthesis. Moreover, we perform a comparative study between the two methods. From the experimental results, this work essentially elucidates the significance and the superiority of employing the query synthesis approach over the commonly used pool-based approach. More detailed results will be discussed in Sections 7 and 8.</p>
<p>The goal of this work is not to provide a group of active learning strategies, instead, we aim to introduce a comprehensive active learning framework including novel strategies, for handling a wide class of objective optimization problems confronting the exploration-exploitation dilemma.</p>
<p>The main contributions of this paper are summarized as follows:</p>
<p>• Provide a comprehensive active learning framework for a general objective optimization, analyzing it from the point of view of the exploration-exploitation trade-off.</p>
<p>•</p>
<p>Propose several novel information-theoretic active learning strategies, designed for minimizing the learning model uncertainty.</p>
<p>•</p>
<p>Design active learning methods for regression tasks. • Present a less-myopic active learning method focusing on exploitation or target optimization.</p>
<p>•</p>
<p>Develop query synthesis and pool based variants of the proposed active learning strategies and compare the two approaches.</p>
<p>•</p>
<p>Apply the proposed active learning framework to a real-world application, namely dynamic pricing with demand learning, as a case study.</p>
<p>The paper is organized as follows: Section 2 presents a literature review. Section 3 presents the problem formulation. Section 4 briefly describes the Bayesian formulation of linear regression model that is applied in our experiments. Then, our proposed active learning strategies are represented in Section 5. After that, Section 7 presents experimental results. Section 8 discusses the main findings. Finally, Section 9 concludes the paper.</p>
<p>Related Work</p>
<p>In this section, we briefly review the related work.</p>
<p>Active Learning</p>
<p>A comprehensive active learning literature survey can be found in the work by Settles in Reference [4]. Mostly, active learning research is designed to query data samples which enhance the predictive power of the learning model. One of the popular active learning methods is uncertainty sampling [15], which selects a sample that the learning model is most uncertain about. The label uncertainty is often measured using the predictive label variance [16] or the label entropy [17].</p>
<p>Another commonly used active learning strategy is query by committee (QBC) [18]. The QBC strategy hinges on minimizing the version space [4]. A committee of learning models, generally formed using ensemble learning, are trained using the training data acquired so far. Then, the QBC strategy chooses the most controversial data sample, about which the learning models disagree the most. Roy et al. propose an active learning strategy that targets minimizing the generalization error of the learning model [19]. However, their method is computationally intensive.</p>
<p>Active Learning for Regression</p>
<p>Unlike the classification domain, there is limited work that considers active learning for the regression domain. In this work, we mainly focus on regression tasks. However, our proposed active learning framework is general enough and could be applied to classification tasks. In this subsection we briefly overview the main methods of active learning for regression.</p>
<p>Several popular active learning methods have been extended and applied to regression such as query by commitee (QBC) in Reference [13]. In addition, Cai et al. propose an active learning method named, Expected Model Change Maximization (EMCM). Their presented querying method selects the data samples leading to the maximum model change. In their work, they estimate the model change as the gradient of the loss function, typically squared error, with respect to the query sample under consideration.</p>
<p>Wu proposes an active learning approach that considers representativeness and diversity in initial data collection and sequential query selection [14]. The presented approach typically applies k-means clustering to ensure representativeness by choosing data samples that are close to clusters' centroids. Furthermore, diversity is satisfied by favoring clusters having no labeled data so far. Another work seeking to enhance diversity of data samples is presented in Reference [20].</p>
<p>The pool-based active learning chooses training data points without assuming a prior knowledge of the test distribution. On the other hand, the population-based active learning assumes that the test distribution of data points is known and it seeks to estimate the optimal training input density from which training data points are sampled. Sugiyama et al. develop a population-based active learning approach using weighted least-squares linear regression in Reference [21]. Their proposed method, named ALICE, aims to minimize the conditional expectation of the generalization error given the training data samples.</p>
<p>To our knowledge, applying information-theoretic approaches to active learning for the regression domain is limited, unlike the classification domain. In this work, as demonstrated in Section 5.2, we propose several information-theoretic based active learning querying strategies for regression.</p>
<p>Information-Theoretic Active Learning</p>
<p>In this section, we briefly describe some information-theoretic based active learning methods in literature, that are mainly designed for classification, aiming to enhance the learning model predictive performance.</p>
<p>Guo and Greiner exploit the potential information of the unlabeled data in their proposed active learning strategy [17]. The authors develop their active learning method based on maximizing the mutual information between the underlying query and the conditional labels of the unlabeled pool given the training data. In their method, since the true label is not known, the authors use an optimistic label for the candidate query sample, which is the label leading to the maximum mutual information about the labels of the unlabeled pool samples.</p>
<p>The authors of Reference [22] develop an entropy-based active learning for object recognition. The presented method seeks to minimize the expected entropy of the labels for the unlabeled pool of samples, given the training data acquired so far.</p>
<p>In Reference [23], the authors develop an information-theoretic active learning framework in batch setting mode. Their proposed framework seeks to maximize the mutual information between the candidate sample and the unlabeled pool of samples. The authors propose pessimistic and optimistic approximations of the mutual information by choosing the label minimizing or maximizing the conditional entropy of the labels of the unlabeled samples.</p>
<p>Another information-theoretic metric used in Reference [24], for active learning classification, is the Fisher information ratio (FIR). A major advantage of using the Fisher information metric specifically, is that it accounts for the diversity among the query samples. The proposed method obtains a probability mass function (PMF) over the unlabeled pool by maximizing the FI using semi-definite programming, then the chosen queries are drawn according to the optimized PMF.</p>
<p>In this paper, we propose novel active learning strategies for regression tasks that utilize information-theoretic concepts including: mutual information, Kullback-Leibler divergence and learning model entropy, as described in Section 5.2.</p>
<p>Query Synthesis versus Pool-Based AL</p>
<p>As previously mentioned in Section 1, active learning can be classified into pool-based and query synthesis approaches [4]. The pool-based approach is prevalent in active learning literature, however the query synthesis approach could potentially outperform the pool-based method, since unlike the pool-based, the query sample is chosen from the whole input space and not restricted by a certain pool of samples, that could not be representative for the whole input space or could not contain the optimal query samples. However, the main limitation of the query synthesis approach is that it could not be applicable for tasks requiring human annotation such as image processing and natural language processing, since the synthetically generated samples could be meaningless to the human annotator [4]. Consequently, the query synthesis could mainly be used whenever the query oracle is a scientific experiment or when the underlying input space is continuous such as: the considered dynamic pricing application [8] and some robotics applications [25].</p>
<p>There are a few contributions applying query synthesis for active learning querying. Query synthesis was first introduced in Reference [26]. In Reference [25], the authors approximate the version space by solving a convex optimization problem. Then, the synthetic query is generated by extracting the principal component that would shrink the version space.</p>
<p>In this work, we implement our proposed active learning strategies in both ways: query synthesis and pool-based. The experimental results indicate that the query synthesis approach has superior performance compared to the pool-based approach as discussed in Section 7. This is intuitively logical because the query synthesis approach optimizes the query criteria over the whole input space, so the returned solution is optimal since it is not restricted to be in a certain pool of samples. Moreover, the query synthesis approach is significantly more computationally efficient than the pool-based approach since for each iteration, the former optimizes the underlying query strategy one time, while the latter evaluates the querying strategy over all the pool samples and chooses the best sample of them to query, which is computationally intensive, especially that the pool size (the number of available unlabeled samples) is used to be large.</p>
<p>Active Learning for Objective Optimization</p>
<p>As mentioned in the introduction, Section 1 and as discussed so far, most of the active learning work in literature aims to enhance the predictive accuracy of the learning model. There are only limited research contributions that use active learning for achieving general real-world objectives other than the model predictive power. In this section, we discuss the main contributions that utilize active learning for achieving a general objective other than the learning model accuracy.</p>
<p>Saar and Provost design an active learning querying method named Goal-Oriented Active Learning (GOAL) for decision making. In their paper, the authors apply their proposed active learning method to customer targeting campaigns [27]. They typically consider binary decision (classification) problem, which is whether to target a specific customer or not, given that customer targeting incurs some cost. Their proposed method queries data samples that are close to decision threshold to enhance decision learning. However, the GOAL method does not consider the trade-off between learning optimal decisions and profit maximization.</p>
<p>Garnett et al. adopt active learning for two binary classification problems, active search and active surveying in Reference [28]. The authors utilize the Bayesian decision theory and they propose less-myopic approximations to the optimal policy by considering multiple step look-ahead of the underlying utility functions of both problems.</p>
<p>Marcela et al. develop an active learning approach for solving multi-objective optimization, named ε-Pareto Active Learning (ε-PAL) [29]. Their approach assumes that the considered objectives follow a Gaussian process distribution, so they use Bayesian optimization framework. However, their work does not focus on the exploration-exploitation trade-off that may exist among the underlying different objectives, which is the main concern of our presented work.</p>
<p>Another active learning scheme for sequential decision making is the knowledge-gradient (KG) method [30]. The KG method is an exploitation-based strategy that considers several alternatives and chooses the alternative maximizing the expected improvement of a certain utility function. The knowledge-gradient method maintains a Bayesian predictive distribution for each alternative's utility and these posterior distributions are updated upon acquiring new observations. However, the KG method could be computationally expensive for large number of alternatives.</p>
<p>Unlike the KG method, our framework considers the distribution of a certain utility function that is evaluated using a learning model, specifically the Bayesian linear regression. Another difference between the KG method and our proposed methods is that the KG method is inherently designed in pool-based setting where the selection is performed from a finite set of alternatives. On the other hand, our proposed approaches are general to be applied in pool-based or query synthesis setting as indicated in Section 5.1. In addition, the KG method is a pure exploitation method that does not explicitly consider exploration. However, in this work, we provide several less-myopic methods balancing between exploitation and exploration described in Section 5.4.</p>
<p>The mean objective cost of uncertainty (MOCU) method proposed in References [31,32] handles model uncertainty in a novel way. The MOCU method essentially studies the impact of the model uncertainty on performance degradation in terms of some incurred cost. Specifically, the MOCU criterion evaluates model uncertainty by measuring the differential cost between the current estimated model and the optimal model which minimizes the expected cost.</p>
<p>The Exploration-Exploitation Trade-Off</p>
<p>The exploration and exploitation trade-off has been extensively studied in many contexts including: reinforcement learning [5], multi-armed bandit problems [33] and evolutionary optimization [6].</p>
<p>Krause and Guestrin handle the trade-off between exploration and exploitation in their active learning method for handling spatial phenomena such as river monitoring [34]. The authors use Gaussian Processes (GPs) in their model, with unknown kernel parameters. They propose a non-myopic active learning approach for handling the trade-off between exploration, which aims to decrease the uncertainty about the model parameters and exploitation, which seeks to near-optimal observations using the estimated model parameters so far. However, they use static split between exploration and exploitation as two separate phases and they derive some bounds for the length of the exploration phase. On the other hand, our proposed methods described in Section 5.4 make probabilistic transitions/balance between exploration and exploitation. A dynamic balance between exploration and exploitation that is performed probabilistically could be more powerful than static balance, especially for real world applications that could have noisy observations. In such a case it is hard to predict a predefined period of exploration.</p>
<p>The multi-armed bandit (MAB) context is a class of sequential decision making problems originally developed in Reference [35]. The objective is to maximize rewards but under uncertainty and incomplete feedback about rewards, so there is a trade-off between performing an action that gathers information regarding reward (exploration) and making a decision that maximizes the immediate reward given the information gathered so far (exploitation). In our experiments, we apply the upper confidence bound algorithm (UCB) [36], a popular algorithm developed in the context of MAB, as a baseline to compare with.</p>
<p>Although the primary objective of reinforcement learning is to maximize the cumulative rewards, which is typically exploitation, exploration plays a significant role in reinforcement learning as demonstrated in Reference [5], since without exploration, the agent could simply derive sub-optimal plans. So, achieving the balance between exploration and exploitation is the core issue in reinforcement learning. However, reinforcement learning is generally computationally expensive, so it is not scalable for large state spaces. Furthermore, reinforcement learning requires a considerable amount of training data, unlike active learning which is designed for limited data requirements. The work of Reference [37] relates the concept of exploration-exploitation trade-off with bias-variance trade-off.</p>
<p>The exploration-exploitation trade-off has been extensively addressed in the context of evolutionary optimization. In such context, exploration is defined as visiting new regions of the search space, while exploitation denotes visiting regions of the search space within the neighborhood of previously explored points. A comprehensive review of the exploration-exploitation trade-off in evloutionary optimization is presented in Reference [6].</p>
<p>Problem Formulation</p>
<p>As mentioned in the introduction, this work focuses on regression tasks since it is prominent in different applications such as energy consumption prediction [38] and price-demand elasticity estimation [8,39]. Specifically, in this work, we apply linear regression model but our proposed active learning framework is general and can be applied to any other regression model. Furthermore, the proposed strategies can be adapted to classification models as well.</p>
<p>We consider the following linear regression problem:
y = β T x + (1)
where x is the input feature vector such that x ∈ R d , where d is the dimensionality of the feature vector, y denotes the regression response variable y ∈ R and is a random error term such that ∼ N (0, σ 2 ) and β ∈ R d denotes the regression model coefficients. This work particularly tackles the class of optimization problems which have a certain utility function u to be optimized, for any regression task. However, the utility function u incurs some uncertainty which can be estimated using a probabilistic regression model. Such problems pose the challenging problem of how to strike a balance between maximizing the objective function u (exploitation) and minimizing the uncertainty about the utility function (exploration). In this work, we develop a novel active learning framework consisting of various strategies to interactively seek a balance between exploitation and exploration.</p>
<p>Notation</p>
<p>In this section, we introduce the adopted notation used in the proposed active learning framework. First, the training data acquired so far is denoted as D = (x i , y i ) N i=1 , the training data term D is expressed in terms of a set of pairs of input data samples x i and their corresponding labels y i , where N is the number of data samples acquired so far.</p>
<p>The matrix of input data points is denoted as X ∈ R N×d , such that each row x represents one data sample and d is the dimensionality of the data point x. For Y ∈ R N×1 , it represents the vector of the corresponding output variables. The matrix of data samples whose outputs require prediction is denoted as X * , such that X * ∈ R m×d , where m is the size of data samples to be predicted. In addition, Y * represents the vector of the corresponding output variables and Y ∈ R m×1 . Similarly, in case of predicting a single data point, the data sample is denoted as x * and y * is its corresponding output.</p>
<p>In the adopted linear regression algorithm described in Section 4, the regression coefficients are denoted as β. In addition, µ β and Σ β are the mean and covariance matrix of β, respectively.</p>
<p>In the proposed active learning framework, U denotes the unlabeled pool of data samples and Y U represents the responses of the samples in the pool. The utility function u represents the objective function to be optimized using active learning as defined in Sections 5.3 and 5.4.</p>
<p>Preliminaries: Bayesian Linear Regression</p>
<p>In this section, we briefly describe the Bayesian linear regression model used in the proposed active learning framework. We adopt the Bayesian linear regression model due to several reasons. First, the class of optimization problems that we handle involves uncertainty of the utility function, which can be estimated using probabilistic regression models such as the Bayesian linear regression. Moreover, most active learning querying strategies depend on the uncertainty of predictions, so it is compelling that we use a regression model providing not only predictions but also uncertainty of the obtained predictions and Bayesian linear regression provides such information. Finally, in active learning settings, the initial data points available for training is essentially limited which could result in over-fitting, especially for noisy data, so applying Bayesian linear regression helps to combat the potential over-fitting.</p>
<p>The underlying regression problem is formulated as indicated in Equation (1), in Section 1. According to Equation (1), we have two major parameters in the regression model, the regression model coefficients β and the noise variance σ 2 , so we adopt Bayesian linear regression with conjugate prior of (β, σ 2 ).</p>
<p>Since the noise variance parameter σ 2 is a key parameter in the model and we have some prior knowledge about it, for example it must be positive, we can use a conjugate prior distribution for both parameters β and σ 2 . We assume an Inverse Gamma prior distribution for σ 2 , σ 2 ∼ IG(a σ , b σ ).
p(σ 2 ) = (b σ ) a σ Γ(a σ ) σ −2(a σ +1) e − bσ σ 2(2)
where a σ &gt; 1, b σ &gt; 0 and σ 2 &gt; 0. The conjugate prior p(β, σ 2 ) can be expressed as a Normal Inverse Gamma (NIG) distribution as follows:
p(β, σ 2 ) = p(β|σ 2 )p(σ 2 ) = N (µ, σ 2 Σ)I G(a σ , b σ ) = N IG(µ, Σ, a σ , b σ )(3)
• Conjugate Posterior Distribution:</p>
<p>According to Reference [40], the conjugate posterior distribution p(β, σ 2 |D) is a Normal Inverse Gamma (NIG) distribution as follows:
p(β, σ 2 |D) = N IG(µ β|D , Σ β|D , a σ|D , b σ|D )(4)
Let µ and Σ be the prior expectation and covariance matrix of parameters β, respectively. The posterior mean µ β|D is evaluated as follows:
µ β|D = (X T X + Σ −1 ) −1 (Σ −1 µ + X T y)(5)
The posterior covariance Σ β|D is calculated as follows:
Σ β|D = (X T X + Σ −1 ) −1(6)
The posterior updates of noise distribution parameters a σ and b σ parameters are given by:
a σ|D = a σ + N 2 (7) b σ|D = b σ + 1 2 (y T y + µ T Σ −1 µ − µ T β|D Σ β|D −1 µ β|D )(8)
As derived in Reference [41], the marginal posterior distribution for β, denoted as β|D, is a multivariate Student-t distribution as follows:
β|D ∼ t 2a σ|D (µ β|D , b σ|D a σ|D Σ β|D )(9)
For a random variable Z that follows a multivariate Student-T distribution, defined as t v (µ 0 , Σ 0 ), the expectation and the covariance matrix of Z are calculated, respectively, as follows:
E[Z] = µ 0 (10) Σ Z = v v − 2 Σ 0(11)
where v is the number of degrees of freedom for the Student-T distribution t v (µ, Σ).</p>
<p>According to Equations (9), (18) and (11), the expectation and the covariance matrix of the marginal β|D distribution are evaluated as follows:
E[β|D] = µ β|D (12) Cov[β|D] = b σ|D a σ|D−1 Σ β|D(13)
• Posterior predictive distribution of Y:</p>
<p>As derived in Reference [40], the posterior predictive distribution p(Y|D) is evaluated as follows:
p(Y|D) = p(Y|β, σ 2 )p(β, σ 2 |D) (14) = N (Xβ, σ 2 I m ) × N IG(µ β|D , Σ β|D , a σ|D , b σ|D ) (15) = t 2a σ|D Xµ β|D , b σ|D a σ|D (I m + XΣ β|D X T )(16)
To predict a vector of output responses Y * , corresponding to a matrix of data points X * , the posterior predictive distribution of the output vector Y * is defined as follows:
p(Y * |X * , D) ∼ t 2a σ|D (E[Y * |X * , D], A Y * |X * ,D )(17)
The posterior expectation of the predicted responses Y * is calculated as:
E[Y * |X * , D] = X * µ β|D(18)
where A Y * |X * ,D is calculated as:
A Y * |X * ,D = b σ|D a σ|D (I m + X * Σ β|D X * T )(19)
However, the covariance matrix for a multivariate Student-T distribution t v (µ, A) is estimated as:
Σ = v v − 2 A(20)
Consequently, from Equation (17) and substituting from Equation (19) into Equation (20), the posterior variance of the predicted responses Y * is calculated as:
Σ Y * |X * ,D = b σ|D a σ|D − 1 (I m + X * Σ β|D X * T )(21)
To predict a single label y * , the predictive posterior distribution p(y * |x * , D) is evaluated as:
p(y * |x * , D) ∼ t 2a σ|D∪(x * ,y * ) E[y * |x * , D], σ y * |x,D(22)
According to Equations (17) and (18), the posterior expectation of the predicted label y * is calculated as:
E[y * |x * , D] = x * T µ β|D(23)
Similarly, using Equations (17) and (21), the posterior variance of the predicted value y * is defined as:
σ 2 y * |x * ,D = b σ|D a σ|D − 1 (1 + x * T Σ β|D x * )(24)
In this section, we have provided the final formulations for Bayesian linear regression model. The interested readers can find more details in References [40,42].</p>
<p>Proposed Active Learning Framework</p>
<p>In this section, we present our proposed active learning framework for handling optimization problems, encountering an exploration-exploitation trade-off.</p>
<p>First, we describe the general active learning settings. Then, we introduce our proposed active learning strategies which are mainly classified into: exploration-based, exploitation-based and strategies that balance exploration and exploitation. Figure 1 shows the proposed active learning framework. </p>
<p>Active Learning Schemes</p>
<p>Active learning can be applied in different modes that define how a new query point is generated. We describe three different schemes, the first two methods are generally known in literature and we define the third one because we incorporate it into some of our proposed strategies.</p>
<p>•</p>
<p>Pool-based</p>
<p>This is the conventional approach that is mostly used in the active learning literature. In the pool-based approach, there exists an unlabeled pool of data samples X U and at each iteration, one or more query example(s) x * is selected from the pool according to a certain querying criterion. Algorithm 1 describes the pool-based active learning approach.</p>
<p>•</p>
<p>Membership Query Synthesis</p>
<p>Unlike the pool-based approach, the membership query synthesis scheme is not commonly used in the active learning literature. In contrast with the pool-based active learning, the membership query synthesis does not select data samples out of a certain pool of unlabeled data. Alternatively, this approach essentially generates and queries synthetic data samples of the entire input space. Algorithm 2 explains the query synthesis approach.</p>
<p>This approach is very efficient and is not computationally intensive compared to the pool-based approach. The reason for the query synthesis's computational efficiency is that instead of iterating over the large unlabeled pool of samples and evaluating a certain selection criterion such as mutual information, the query synthesis approach directly generates a synthetic data sample to achieve a certain objective. For example, our proposed query synthesis approach optimizes the underlying querying metric using optimization algorithms. The query synthesis approach is not only computationally efficient, it could be more compelling than the pool-based approach since the generated query sample is not restricted to be part of an unlabeled pool, so the synthetically generated query sample could be more informative and beneficial than the examples in the pool.</p>
<p>•</p>
<p>Membership Query Synthesis without a Predefined Pool</p>
<p>The query synthesis approach does not need to have a pool of samples. However, some active learning strategies exploit the potential information in the unlabeled data to guide the sample selection such as mutual information strategy defined subsequently in Equation (28) and the KL divergence strategy defined in Equation (47). Consequently, such strategies rely on the existence of some unlabeled data to estimate how useful or how representative a certain query point is. However, for some applications, the unlabeled data could not exist or if they exist, they may not be a representative sample for the input space. In such cases, one could generate a representative and diverse sample of unlabeled data using the domain knowledge of the feature space. Another way for generating unlabeled representative data could be to apply any reasonable clustering algorithm using the available training data and the cluster centroids can be used as representatives of the unobserved data. Algorithm 3 elucidates this approach.</p>
<p>Algorithm 1 Pool-based Active Learning
Input: A dataset D = (x j , y j ) N j=1
, a general active learning strategy S, a utility function u, number of iterations T, a discount factor γ and a generation method for creating synthetic queries GenerateQueryPoint(). Output: A Learned model θ T and a cumulative gained utility u T . D L ← N init labeled data samples randomly chosen out of D. Train the regression model using the initial training data to obtain initial model θ 0 .
D U ← D \ D L repeat
for each x k ∈ D U do S(x k ) ← Apply a certain active learning strategy S to x k , using current model estimate θ i . end for
x * = arg max x k S(x k ) ∀ k , k ∈ {1...|D U |}.
y * ← the true label for the query sample x * . Add the acquired data point (x * , y * ) to the training data: D L ← D L ∪ (x * , y * ). Evaluate the utility u i using the new acquired point: u i ← u(x * , y * ). Update the regression model θ i using the new acquired point (x * , y * ). until T iterations executed return The learned model θ T and the cumulative discounted utility
u T = ∑ T i=1 γ i−1 u i .
In our experiments, we develop several novel active learning strategies and apply them in the pool-based and query synthesis schemes. For the strategies that use the unlabeled data samples for guiding its selection such as mutual information (MI), modified mutual information (MMI) and Kullback-Leibler divergence (KL), we apply the three aforementioned schemes. More details are provided in the experiments section, Section 7.</p>
<p>Algorithm 2 Query Synthesis Active Learning
Input: A dataset D = (x j , y j ) N j=1
, a general active learning strategy S, a utility function u, number of iterations T, a discount factor γ and a generation method for creating synthetic queries GenerateQueryPoint(). Output: A Learned model θ T and a cumulative gained utility u T . D L ← N init labeled data samples randomly chosen out of D. Train the regression model using the initial training data to obtain initial model θ 0 . repeat
x * = GenerateQueryPoint(S, D U , θ i ).
y * ← the true label for the query sample x * . Add the acquired data point (x * , y * ) to the training data:
D L ← D L ∪ (x * , y * ).
Evaluate the utility u i using the new acquired point:
u i ← u(x * , y * ). Update the regression model θ i using the new acquired point (x * , y * ). until T iterations executed return The learned model θ T and the cumulative discounted utility u T = ∑ T i=1 γ i−1 u i .</p>
<p>Algorithm 3 Query Synthesis Active Learning without a predefined pool</p>
<p>Input: A small dataset of N init points D = (x j , y j ) N init j=1 , a general active learning strategy S, a utility function u, number of iterations T, a discount factor γ and a generation method for creating synthetic queries GenerateQueryPoint(). Output: A Learned model θ T and a cumulative gained utility u. D L ← N init labeled data samples randomly chosen out of D. Train the regression model using the initial training data to obtain initial model θ 0 . U ← Construct a representative sample of unlabeled data using for example, domain knowledge or clustering. repeat x * = GenerateQueryPoint(S, U , θ i ). y * ← the true label for the query sample x * . Add the acquired data point (x * , y * ) to the training data: D L ← D L ∪ (x * , y * ). Evaluate the utility u i using the new acquired point: u i ← u(x * , y * ). Update the regression model θ i using the new acquired point (x * , y * ). until T iterations executed return The learned model θ T and the cumulative discounted utility
u T = ∑ T i=1 γ i−1 u i .</p>
<p>Exploration-Based Strategies</p>
<p>In this section, we describe our novel proposed exploration-based active learning strategies for regression. The exploration-based strategies mainly target enhancing the regression model predictive performance. The presented strategies are not limited to a certain application or a class of problems, they are quite general and could be applied in any settings where the objective is to boost the regression model accuracy. The most popular active learning methods such as uncertain sampling [15] and Query by Committee [18] seek to query the most "uncertain" sample, that is, the data sample about which the learning model is the most uncertain. Although this seems helpful for the learning model either classification or regression, the uncertain sampling approach does not consider the potential information of the unlabeled pool of examples. Thus, the uncertain sampling could select noisy patterns or outliers. On the other hand, querying samples not only based on the query sample but also on the unlabeled samples of the pool ( [17,23]) is more promising since such approach is less myopic and it utilizes the information of the plentiful unlabeled pool.</p>
<p>The following proposed exploration strategies are mainly based on information theory [43]. To our knowledge, it is the first time that information theoretical concepts (such as mutual information, Kullback-Leibler divergence and model entropy) are applied in active learning for regression. Some information-theoretic metrics such as predictive label entropy, Fisher information and mutual information have been employed for active learning in classification problems [17,22,23]. However, such information theoretic metrics have not been considered yet for regression problems.</p>
<p>Depending solely on a single query sample information could lead to choosing noisy samples or outliers [19]. It is well-known that an outlier does more damage than help. Consequently, our proposed exploration-based active learning strategies exploit the potential information existing in the unlabeled pool of samples and the learning model uncertainty. Moreover, incorporating the information of the unlabeled pool such as mutual information, into the selection strategy, advocates querying representative samples.</p>
<p>Mutual Information (MI)</p>
<p>The mutual information criterion aims to query the sample x * which effectively holds a substantial amount of information about the labels of the unlabeled pool. Thus, this strategy chooses the sample x * that maximizes the mutual information between its label y * and the labels of the remaining unlabeled samples of the pool excluding x * , denoted as Y U .</p>
<p>The mutual information between the query sample x * and the labels of the unlabeled pool Y U is defined as:
I(x * , Y U ) = H(Y U |D) − H(Y U |x * , D)(25)
where D denotes the labeled training data acquired so far. The first term H(Y U |D) represents the prior entropy (or uncertainty) of all the labels of the unlabeled pool of samples. Similarly, the second term H(Y U |x * , D) denotes the entropy of the labels of unlabeled pool of samples but after acquiring the new query point x * . From Equation (25), it can be noted that maximizing I(x * , Y U ) is equivalent to minimizing the conditional entropy H(Y U |x * , D), which is defined as follows:
H(Y U |x * , D) = y * p(y * |x * , D)H(Y U |x * , y * , D) dy *(26)
To simplify computations, Equation (26) could be approximated by eliminating the integration over all the possible labels of y * and using the expected value of it E[y * ]. Other approximations are made in literature [22,23], using the optimistic or the pessimistic label. However, we found that employing the expectation could be more reasonable. Accordingly:
H(Y U |x * , D) = H(Y U |(x * , y * ), D)(27)
where y * is the expected predicted label of the data point x * , which is calculated using Equation (23).
H(Y U |(x * , y * ), D) = Y p(Y|X, D ∪ (x * , y * )) log(p(Y|X, D ∪ (x * , y * )) dY(28)
As mentioned in Section 4, the posterior predictive distribution of the predictive labels vector Y, p(Y|X, D ∪ (x * , y * )) is a multivariate Student-T distribution which is defined as follows:
p(Y|X, D ∪ (x * , y * )) = t 2a σ|D∪(x * ,y * ) Y, E[Y|X, D ∪ (x * , y * )], Σ Y|X,D∪(x * ,y * )(29)
The posterior expectation E[Y|X, D ∪ (x * , y * )] and the covariance matrix Σ Y|X,D∪(x * ,y * ) are evaluated using the Bayesian linear regression model formulations described in Section 4, using Equations (18) and (21), respectively. However, this method and all our proposed methods are general and can be applied using any regression model that provides uncertainty of its predictions.</p>
<p>According to Reference [41], the final formulation of the entropy of a random variable Z following a Student-t distribution t v Z is given by:
H(Z) = 1 2 log det(R Z ) + φ(v Z , d) + v Z + d 2 M(v Z , d, ∆)(30)
where R Z denotes the correlation matrix of Z, d is the dimensionality of Z and v Z represents the number of degrees of freedom for the Student-t distribution. In addition,
φ(v Z , d) is a constant depending on v Z and d and ∆ Z = µ Z * T R Z * −1 µ Z * . M(a Z , d, ∆) = e − ∆ Z 2 ∞ ∑ j=0 1 j! Ψ( d + v Z + 2j 2 ) − Ψ( v Z 2 )(31)
where Ψ is the digamma function which is defined as:
Ψ(x) = d dx ln(Γ(x))(32)
Accordingly, the conditional entropy of Y U , H(Y U |(x * , y * )), is calculated using Equation (30) as follows:
H(Y U |(x * , y * ), D) = 1 2 log det(R Y * ) + φ(2a * , m) + 2a * + m 2 M(2a * , m, ∆ Y )(33)
where m is the number of data points to be predicted, that is, it is the length of the predicted output vector Y U . To simplify notation, let a * = a σ|D∪(
x * ,y * ) , µ Y * = µ Y|D∪(x * ,y * ) , Σ Y * = Σ Y|D∪(x * ,y * ) and R Y * = R Y|D∪(x * ,y * ) .
For ∆ Y , it is evaluated as follows:
∆ Y = µ Y * T R Y * −1 µ Y *(34)
such that R Y * denotes the correlation matrix of the unlabeled samples Y after acquiring the query sample x * . The term M(2a * , m, ∆ Y ) can be evaluated using Equation (31):
M(2a * , m, ∆ Y ) = e − µ Y * T R Y * −1 µ Y * 2 ∞ ∑ j=0 1 j! Ψ( m + 2a * + 2j 2 ) − Ψ(a * )(35)
Using algebraic manipulations, the summation in Equation (35) converges as follows:
∞ ∑ j=0 1 j! Ψ( m + a * + 2j 2 ) − Ψ( a * 2 ) = e × Ψ(a * + m 2 + 1)) − Ψ(a * )(36)
Accordingly, substituting from Equation (36) into Equation (35) results in:
M(2a * , d, ∆ Y ) = Ψ(a * + m 2 + 1)) − Ψ(a * ) × e − µ Y * T R Y * −1 µ Y * 2 +1(37)
Then, after substituting from Equation (37) into Equation (33), the conditional entropy H(Y U |(x * , y * ), D) can be evaluated as:
H(Y U |(x * , y * ), D) = 1 2 log det(R * Y ) + φ(2a * , m) + (2a * + m 2 ) Ψ(a * + m 2 + 1)) − Ψ(a * ) × e − µ Y * T R Y * −1 µ Y * 2 +1 (38)
Finally, the query sample x * that maximizes the mutual information essentially minimizes the conditional entropy of the unlabeled pool of samples as indicated in Equation (25). Consequently, the query sample x * minimizing the conditional entropy H(Y U |(x * , y * ), D) can be evaluated as follows:
x MI = arg min x * 1 2 log det(R Y|D∪(x * ,y * ) ) + φ 2a σ|D,∪(x * ,y * ) , m + a σ|D,∪(x * ,y * ) + m 2 × Ψ(a σ|D,∪(x * ,y * ) + m 2 + 1)) − Ψ(a σ|D,∪(x * ,y * ) ) e − µ T Y |D,∪(x * ,y * )R −1 Y|D,∪(x * ,y * ) µ Y|D,∪(x * ,y * ) 2 +1(39)
Simplifying Equation (39) by eliminating the term φ(2a σ|D,∪(x * ,y * ) , m) since it is a constant that does not depend on the query sample, because a σ|D,∪(x * ,y * ) basically depends on the number of data being observed, as indicated in Equation (7). Thus:
x MI = arg min x * 1 2 log det(R Y|D∪(x * ,y * ) ) + (a σ|D,∪(x * ,y * ) + m 2 ) Ψ(a σ|D,∪(x * ,y * ) + m 2 + 1)) −Ψ(a σ|D,∪(x * ,y * ) ) × e − µ T Y|D,∪(x * ,y * ) R −1 Y|D,∪(x * ,y * ) µ Y|D,∪(x * ,y * ) 2 +1(40)
For computational efficiency purposes, we evaluate the log determinant of the correlation matrix R Y and its inverse using Cholesky decomposition since the correlation matrix is a symmetric positive semi-definite matrix.</p>
<p>We apply three variants of this active learning strategy: pool-based, query synthesis and query synthesis without pool, which are described in Section 5.1.</p>
<p>Modified Mutual Information (MMI)</p>
<p>The modified mutual information strategy is basically akin to the aforementioned strategy. This method maximizes the mutual information defined in Equation (25) but it evaluates the first term of that equation, H(Y U |D), which represents the entropy of the labels of the unlabeled samples and does not ignore it. The intuition of this querying strategy is to account for the impact of the query sample (x * , y * ) on reducing the joint entropy of the unlabeled samples H(Y U |D). In other words, if the first term is ignored and we just focus on minimizing the conditional entropy given the underlying query sample H(Y U |(x * , y * ), D), we may choose a sample x * that is redundant and not informative in case the entropy before acquiring x * , H(Y U |D) is inherently negligible.</p>
<p>Accordingly, the modified mutual information equation is defined using Equation (25) but without ignoring the first term.
I(x * , Y U ) = H(Y U |D) − H(Y U |(x * , y * ), D)(41)
Similar to the mutual information strategy, the second term of Equation (41) can be evaluated using Equation (38). As for the first term H(Y U |D), similar to Equation (38), it can be computed as follows:
H(Y U |D) = 1 2 log det(R Y ) + φ(2a, m) + (a + m 2 ) Ψ(a + m 2 + 1)) − Ψ(a) e − µ Y T R Y −1 µ Y 2 +1 (42) where a = a σ|D , µ Y = µ Y|D , Σ Y = Σ Y|D , and R Y = R Y|D .
For ∆ Y it is evaluated as follows:
∆ Y = µ Y T R Y −1 µ Y(43)
where R Y denotes the correlation matrix of the unlabeled samples Y, given the training data acquired so far D.</p>
<p>Therefore, substituting from Equations (28) and (42) into Equation (41) results in:
I(x * , Y U ) = 1 2 log det(R Y|D∪(x * ,y * ) ) + φ 2a σ|D,∪(x * ,y * ) , m + (a σ|D,∪(x * ,y * ) + m 2 )× Ψ(a σ|D,∪(x * ,y * ) + m 2 + 1)) − Ψ(a σ|D,∪(x * ,y * ) ) e − µ T Y|D,∪(x * ,y * ) R −1 Y|D,∪(x * ,y * ) µ Y|D,∪(x * ,y * ) 2 +1 − 1 2 log det(R Y ) + φ(2a, m) + (a + m 2 ) Ψ(a + m 2 + 1)) − Ψ(a) e − µ Y T R Y −1 µ Y 2 +1 (44) x MMI = arg max x * 1 2 (log det(R Y|D∪(x * ,y * ) ) − log det(R Y )) + (a σ|D,∪(x * ,y * ) + m 2 )× Ψ(a σ|D,∪(x * ,y * ) + m 2 + 1)) − Ψ(a σ|D,∪(x * ,y * ) ) e − µ T Y|D,∪(x * ,y * ) R −1 Y|D,∪(x * ,y * ) µ Y|D,∪(x * ,y * ) 2 +1 −(a + m 2 ) Ψ(a + m 2 + 1)) − Ψ(a) e − µ Y T R Y −1 µ Y 2 +1(45)
Similar to the previous strategy, we apply three variants of this active learning method using the different active learning schemes: pool-based, query synthesis and query synthesis without pool.</p>
<p>Kullback-Leibler Divergence (KL)</p>
<p>So far, the previously mentioned strategies select the sample revealing the most amount of information for the labels of the other samples. However, this strategy addresses a different aspect. The Kullback-Leibler divergence strategy seeks to acquire samples having the greatest impact on the posterior predictive distribution of the unlabeled samples p(Y U |X, D). So, this method considers the influence of the query sample on the "distribution" of the unlabeled samples. To achieve that, this method maximizes the difference in posterior predictive distributions of unlabeled pool Y U before and after querying the query point (x * , y * ). The distribution difference is evaluated using the Kullback-Leibler divergence (KL) metric [44]. The Kullback-Leibler divergence metric is an asymmetric distance measure that evaluates the distance between two probability distributions P and Q. In other words, D KL (P||Q) measures the information lost when Q is used to approximate P [44]. The D KL (P||Q) is defined as follows:
D KL (P||Q) = ∞ −∞ p(x)log p(x) q(x) dx
where p(x) and q(x) are the probability density functions to be compared. It is worth noting that the KL divergence has been employed as a powerful method in Bayesian analysis. For example, Lopez et al. apply the KL divergence to influence analysis [45]. The authors use the KL metric to study the impact of removing one or several observations from data set on the inferences.</p>
<p>In our proposed active learning method, p(x) denotes the posterior predictive distribution of unlabeled example given the query sample (x * , y * ), whereby q(x) is the posterior predictive distribution of unlabeled example prior to acquiring the query example (x * , y * ). The Kullback-Leibler divergence D KL (U|D, x * ) is defined as:
D KL (U|D, x * ) = D KL p(Y U |D, x * , y * ), p(Y U |D)(46)
We approximate D KL (U|D, x * ) by evaluating the average Kullback-Leibler divergence over all the unlabeled examples of the pool Y U .
D KL (U|D, x * ) = 1 |U| ∑ k∈U D KL (p(y k|D,(x * ,y * ) ), p(y k|D ))(47)
Since the true label y * of the query sample x * is unknown, we use the expectation of y * denoted as E[y * |µ β , x * ].</p>
<p>As indicated in Section 4, both predictive distributions p(y k|D,(x * ,y * ) ) and p(y k|D ) follow Student-t distributions. Let p(y k|D,(x * ,y * )) ) ∼ t 2a σ|D∪(x * ,y * ) (y k , E[y k |x k , D ∪ (x * , y * )], σ 2 y k |x k ,D∪(x * ,y * ) ). Similarly, the posterior predictive distribution after acquiring y * is denoted as p(y k|D ) ∼ t 2a σ|D (y k , E[y k |x k , D], σ 2 y k |x k ,D ). To simplify notation, let D KL (k|D, (x * , y * )) denote the Kullback-Leibler divergence between the two predictive distributions D KL (p(y k|D,x * ,y * ), p(y k|D )), which is calculated as:
D KL (k|D, x * ) = ∞ −∞ p(y k|D,x * ,y * )log p(y k|D,x * ,y * ) p(y k|D ) dx(48)
Substituting with the Student-t distribution formulation Equation (22) into Equation (48):
D KL (k|D, x * ) = ∞ −∞ t 2a σ|D∪(x * ,y * ) (y k , E[y k |x k , D ∪ (x * , y * )], σ 2 y k |x k ,D∪(x * ,y * ) )× log t 2a σ|D∪(x * ,y * ) (y k , E[y k |x k , D ∪ (x * , y * )], σ 2 y k |x k ,D∪(x * ,y * ) ) t 2a σ|D (y k , E[y k |x k , D], σ 2 y k |x k ,D ) dx(49)
where the means and variances of the posterior distributions can be given using the regression equations in Section 4, Equations (23) and (24), respectively. After substituting from Equation (49) into Equation (47), the Kullback-Leibler divergence D KL (U|D, x * ) is evaluated as:
D KL (U|D, x * ) = 1 |U| ∑ k∈U ∞ −∞ t 2a σ|D∪(x * ,y * ) (y k , E[y k |x k , D ∪ (x * , y * )], σ 2 y k |x k ,D∪(x * ,y * ) )× log t 2a σ|D∪(x * ,y * ) (y k , E[y k |x k , D ∪ (x * , y * )], σ 2 y k |x k ,D∪(x * ,y * ) ) t 2a σ|D (y k , E[y k |x k , D], σ 2 y k |x k ,D ) dx(50)
Finally, the query sample x * that maximizes the Kullback-Leibler divergence between the posterior predictive distributions of unlabeled pool Y UL before and after querying the query point (x * , y * ) is evaluated as follows:
x KL = arg max x * 1 |U| ∑ k∈U ∞ −∞ t 2a σ|D∪(x * ,y * ) (y k , E[y k |x k , D ∪ (x * , y * )], σ 2 y k |x k ,D∪(x * ,y * ) )× log t 2a σ|D∪(x * ,y * ) (y k , E[y k |x k , D ∪ (x * , y * )], σ 2 y k |x k ,D∪(x * ,y * ) ) t 2a σ|D (y k , E[y k |x k , D], σ y k |x k ,D ) dx(51)
Like the two aforementioned active learning methods, we apply the three variants of active learning settings described in Section 5.1, along with this active learning method.</p>
<p>Model Entropy (ME)</p>
<p>The aforementioned strategies, the two variants of mutual information and Kullback-Leibler divergence, exploit the potential information of the unlabeled pool to guide the query selection process. However, this novel active learning strategy, named model entropy, considers a different aspect. The tmodel entropy method targets the ultimate objective for the exploration, as mentioned in Section 1, which is minimizing the learning model uncertainty. In order to achieve this target, this method emphasizes reducing the learning model uncertainty in terms of the model entropy. Thus, this method queries the data sample that minimizes the model entropy in order to reveal the uncertainty of the underlying model and obtain better estimates of the learning model parameters.</p>
<p>In general, the entropy has been used in several applications such as biological systems [46], financial applications [47] and model selection [48]. However, to the best of our knowledge, the use of the model entropy minimization strategy in the active learning field is novel.</p>
<p>According to Reference [4], the existing work in active learning literature so far mainly addresses the following: minimizing the approximate generalization error [19] and reducing the model uncertainty indirectly either by choosing the example about which the model is most uncertain [15] or by querying the example that produces the maximum model change [12].</p>
<p>For the general regression problem formulation presented in Section 3, using Equation (1), the model entropy of the regression model parameters β can be formulated as follows:
H(β|x * , D) = − β p(β|x * , D) log p(β|x * , D) dβ(52)
Using the Bayesian linear regression formulation presented in Section 4 (see Equation (9)), the model parameter β follows a multivariate Student-t distribution such that: (54) where µ β|D,(x * ,y * ) and Σ β|D,(x * ,y * ) are the posterior mean and covariance matrix of model parameter β, respectively and they can be evaluated using Equations (12) and (13), respectively, according to the Bayesian linear regression formulation described in Section 4. Furthermore, the posterior values, a σ|D∪(x * ,y * ) and b σ|D∪(x * ,y * ) are evaluated using Equations (7) and (8), respectively.
p(β|D, x * , y * ) ∼ t 2a σ|D∪(x * ,y * ) µ β|D∪(x * ,y * ) , b σ|D∪(x * ,y * ) a σ|D∪(x * ,y * ) − 1 Σ β|D∪(x * ,y * )(53)H(β|x * , y * D) = − β t 2a σ|D∪(x * ,y * ) µ β|D∪(x * ,y * ) , b σ|D∪(x * ,y * ) a σ|D∪(x * ,y * ) − 1 Σ β|D∪(x * ,y * ) )× log t 2a σ|D∪(x * ,y * ) (µ β|D∪(x * ,y * ) , b σ|D∪(x * ,y * ) a σ|D∪(x * ,y * ) − 1 Σ β|D∪(x * ,y * ) dβ
To simplify notation, let a * = a σ|D∪(x * ,y * ) , µ β
* = µ β|D∪(x * ,y * ) and Σ β * = b σ|D∪(x * ,y * ) a σ|D∪(x * ,y * ) −1 Σ β|D∪(x * ,y * )
. According to Reference [41], the final formulation of the entropy for the multivariate Student-t distribution is given by:
H(β|x * , y * D) = 1 2 log det(R β|D∪(x * ,y * ) ) + φ(2a * , d) + (a * + d 2 )M(2a * , d, ∆ β )(55)
where R β|D∪(x * ,y * ) denotes the correlation matrix of β, d is the dimensionality of β, φ is a constant depending on d and a * and
∆ β = µ β * T Σ β * −1 µ β * . M(2a * , d, ∆ β ) = e − µ β * T Σ β * −1 µ β * 2 ∞ ∑ j=0 1 j! Ψ( d + a * + 2j 2 ) − Ψ( a * 2 )(56)
Substituting from Equation (36) into Equation (56):
M(2a * , d, ∆ β ) = Ψ(a * + d 2 + 1)) − Ψ(a * ) e − µ β * T Σ β * −1 µ β * 2 +1(57)
Substituting from Equation (56) into Equation (55) results in:
H(β|x * , y * D) = 1 2 log det(R β|D∪(x * ,y * ) ) + φ(2a D∪(x * ,y * ) , d) + (a D∪(x * ,y * ) + d 2 )× Ψ(a D∪(x * ,y * ) + d 2 + 1)) − Ψ(a D∪(x * ,y * ) ) e − (a σ|D∪(x * ,y * ) −1)µ β T D∪(x * ,y * ) Σ β −1 D∪(x * ,y * ) µ β D∪(x * ,y * ) 2b σ|D∪(x * ,y * ) +1(58)
However, we could safely ignore the term φ(2a * , d) since it does not depend on the query sample x * . Finally, the query sample x * minimizing the model entropy H(β|x * , y * D) can be estimated as follows:</p>
<p>x ME = arg min
x * 1 2 log det(R β|D∪(x * ,y * ) ) + (a D∪(x * ,y * ) + d 2 )× Ψ(a D∪(x * ,y * ) + d 2 + 1)) − Ψ(a D∪(x * ,y * ) ) e − (a σ|D∪(x * ,y * ) −1)µ β T D∪(x * ,y * ) Σ β −1 D∪(x * ,y * ) µ β D∪(x * ,y * ) 2b σ|D∪(x * ,y * ) +1(59)
For this strategy, we apply both of pool-based and query synthesis active learning approaches.</p>
<p>Exploitation-Based Strategies</p>
<p>In this section, we present the exploitation-based active learning strategies for regression that we apply in our proposed framework. Such strategies purely emphasize on maximizing a certain objective function, with no consideration given to the concept of exploration.</p>
<p>First, we describe the greedy strategy that is considered a pure exploitation method. Then, we propose using a novel active learning querying strategies that mainly focus on exploitation but in a less myopic way than the commonly adopted greedy strategy.</p>
<p>Greedy Strategy (G)</p>
<p>This query strategy addresses pure exploitation by querying the sample resulting in the maximum immediate value of the target objective function (reward). We apply this method as a baseline to compare with, where for every iteration, the query sample is chosen to maximize the expected utility function.</p>
<p>Although the greedy strategy is straightforward and simple, it is myopic in since that it purely considers exploitation, which could result in potential revenue loss, since it pays no attention to improving the model predictive power, which could severely affect the resulting decision, which is commonly known as exploration.</p>
<p>x
G = arg max x E<a href="60">u(x)|D</a>
where the expected utility function u can be expressed as a function of x and the regression model coefficients β.</p>
<p>Expected Value of Perfect Information (EVPI)</p>
<p>We propose a decision-theoritic querying approach which is based on the expected value of perfect information (EVPI). Evaluating the expected value of perfect information could be beneficial for active learning since one can evaluate how revealing a certain query sample is valuable. In other words, active learning could be guided to choose data points that do improve the gained expected utility using EVPI. According to Russell and Norvig [49], the expected value of perfect information for revealing a piece of information, named evidence E j , given an initial evidence e is defined as:
E[VPI e (E j )] = ∑ k P(E j = e jk |e)E[u(α e jk |e, E j = e jk )] − E<a href="61">u(α|e)</a>
where α is the action to be taken and the expected utility of taking action α given the evidence e and after revealing E j , E[u(α e jk |e, E j = e jk )] is defined as: 
E[u(α e j |e,e
while the expected utility of taking action α given the evidence e and without revealing E j is denoted by E[u(α|e)] and it is defined as:
E[u(α|e)] = arg max a ∑ s P(Result(a) = s , e)u(s )(63)
We apply the value of information formulation to the active learning with utility maximization. So, the action α is querying a data point x * to obtain its label y * . For the initial evidence e, it denotes the training labeled data points so far D. Also, e j represents the acquired label y * of the query point x * which represents the piece of information we seek to evaluate.</p>
<p>The expected value of perfect information after querying x * and acquiring its label y * is:
E[VPI e (y * )] = y j P(y j |x * , D)E[u(x * |D, y j )] − E<a href="64">u(x * |D)</a>
Accordingly, the expected utility of acquiring the data sample x * given the observed training data so far D and after observing the evidence y j , the true value of y * , E[u(x * |D, y j )], can be formulated as:
E[u(x * y j |D, y j )] = max x E[u(x|D, y j )] = E<a href="65">u opt |D, y j </a>
where the utility u is the target objective function to be maximized, which is conventionally a function of the data point x and the model parameters β. µ β * denotes the expectation of the updated model parameters β after revealing point x and its label value y j . The second term of Equation (64) could be safely ignored, since the objective is to decide whether to acquire the data label y * or not, maximizing EVPI, this term is independent of y * . This is implied by the following equation, Equation (66). Consequently, this term does not affect the process of maximizing EVPI.
E[u(x * |D)] = max x E[u(x|D)] = E<a href="66">u opt |D</a>
Consequently, the term E[u opt |D] is constant over all query points x * , so it could be safely ignored. Then, evaluating the EVPI by substituting from Equation (65) into Equation (64) results in the following formula:
E[VPI D (y * )] = y j P(y j |D, x * )E[u opt |D, y j ] dy j − E<a href="67">u opt |D</a>
Finally, maximizing Equation (67) by differentiating it with respect to x * , equating the obtained derivative to zero and solving the resulting equation or using any direct optimization method, we get the query point x * of the highest value of the expected value of perfect information as indicated in the following equation.
x EVPI = arg max x * y j P(y j |D, x * )E[u opt |D, y j ] dy j − E<a href="68">u opt |D</a>
The expected value of perfect information method seems similar to the mean objective cost of uncertainty (MOCU) method proposed in Reference [31] and described in Section 2.5. Both methods can be viewed from decision theory perspective. The MOCU method seeks to minimize the expected regret which is the difference between the gained utility using the current model and the optimal utility. On the other hand, the EVPI method aims to maximize the difference between the optimal utility before and after acquiring a certain evidence. Accordingly, the MOCU method minimizes the deviation from the optimal decision. However, the EVPI method maximizes the expected utility improvement before and after acquiring a certain piece of information.</p>
<p>Balancing Exploration and Exploitation Strategies</p>
<p>This section describes several active learning strategies that seek to achieve the balance between exploration and exploitation.</p>
<p>Upper Confidence Bound (UCB)</p>
<p>The Upper Confidence Bound (UCB) strategy is proposed by Auer et al. in [36] in the context of multi-armed bandit problems [35]. We apply the UCB method as an active learning baseline strategy to compare with. The main advantage of this method is that it combines exploitation and exploration in a simple, yet an elegant way. The UCB strategy picks the unlabeled example maximizing the upper confidence bound of the random variable of interest, representing the utility function u.
x UCB = arg max x * E[u(x * )|D] + ησ u(x * )|D(69)
where u is the objective utility function to be maximized, E[u(x * )|D] and σ u(x * )|D denote the expected value and the standard deviation of the utility function for query point x * given the training data acquired so far D.</p>
<p>Probabilistic-Based Exploration-Exploitation (PEE)</p>
<p>This active learning strategy is originally inspired by simulated annealing [50]. More specifically, the probabilistic-based exploration-exploitation strategy is built on the -decreasing greedy algorithm [51]. In order to manage the trade-off between exploration and exploitation, this algorithm combines exploration and exploitation in a probabilistic way. With probability p R , the exploration is performed via any exploratory strategy mentioned in Section 5.2 such as mutual information, Kullback-Leibler divergence and model entropy strategies. Furthermore, other exploration strategies in active learning literature can be incorporated into this method, such as uncertain sampling [4,15] and random sampling.</p>
<p>The exploration probability p R is calculated as follows:
p R = α t−1(70)
where α is less than 1 and t is the current time step or iteration number. The exploration probability intuitively decays over time as seen in Equation (70) since the learning model gets to be more robust and capable of performing some exploitation to achieve the ultimate goal of utility maximization. To implement this strategy, a uniform random variable Z is generated, if Z ≤ p R , any reasonable exploration strategy can be performed, otherwise pure exploitation is applied via maximizing the expected utility (the greedy strategy). However, any other exploitation strategy can be employed.</p>
<p>For the probabilistic-based exploration-exploitation strategy, we have implemented all of our proposed exploration based strategies in Section 5.2 in addition to uncertain sampling and random sampling. To perform exploitation, we use the greedy strategy since it is the simplest method. Although the greedy strategy is myopic since it does not account for enhancing the learning model estimate, in this PEE method the greedy strategy is integrated with an exploration strategy which already achieves an accurate model estimate.</p>
<p>Uncertainty of Strategy (UoS)</p>
<p>Similar to the probabilistic-based exploration-exploitation (PEE) strategy, this proposed active learning method seeks to balance the trade-off between exploration and exploitation in a probabilistic manner. Naturally, active learning querying strategies require a learning model estimate. Furthermore, many active learning strategies including: uncertain sampling [1] and greedy sampling, build their selection decisions entirely based on the learning model estimate. However, active querying methods that fully trust their estimate of the learning model and do not account for the learning model uncertainty could probably yield inaccurate querying decisions. This argument motivates us to design a novel active learning method named Uncertainty of Strategy (UoS). The UoS method accounts for the inherent uncertainty of the querying criterion which is mainly caused by the model uncertainty or due to any other randomness in the active querying method.</p>
<p>The UoS strategy seeks to achieve the balance between exploitation and exploration. The exploitation can be easily performed using the current model estimate, for example, using greedy sampling. On the other hand, the exploration is done as follows: the UoS strategy sets a window of exploration around the active learning strategy's best estimate of a data point, which is returned by exploitation. The length of the exploration window can be estimated using the model uncertainty as described subsequently.</p>
<p>Let the query sample x UoS follow a Gaussian distribution as follows:
x UoS ∼ N (x s , σ s 2 )(71)
where the mean of this Gaussian distribution x s represents the data point returned using pure exploitation. For the Gaussian distribution's variance σ s 2 , it essentially depends on the model uncertainty. We estimate the strategy variance σ s 2 using two different ways. The first method, named</p>
<p>UoS-1 assumes that the strategy variance σ s 2 is proportional to the model uncertainty, where the model uncertainty is estimated using the covariance matrix of the vector of model parameters β. Equation (72) defines the estimation of σ s 2 in terms of the model uncertainty.
σ s 2 = K × b σ|D a σ|D − 1 trace<a href="72">Σ β|D </a>
where K is a parameter set to adapt the units of the query point and the model parameters and to control the exploration/exploitation trade-off. Like the PEE method, we set the K parameter to be time variant, in order to shrink the exploration window as iterations proceed since the model would become more reliable, so more emphasis should be devoted to exploitation.
K = Z t−1(73)
The second method, named UoS-2, estimates the strategy variance σ s 2 empirically using a simple Monte Carlo simulation. This simulation runs for n iterations, where each iteration i proceeds as follows: first, an instance of model parameters vector β i is generated according to the multivariate Student-T distribution using Equation (9). Then, this model parameters' instance β i is used to evaluate the query point using a pure exploitation strategy x si , this is, generally, a simple step as done in greedy strategy Equation (60) for example. Finally, after the n iterations finish, the strategy variance σ s 2 is statistically evaluated as follows:
σ s 2 = K n − 1 n ∑ i=1 (x si −x s ) 2(74)
where K is a parameter for adapting units of the query point and model variance and for controlling the exploration-exploitation trade-off, akin to the UoS-1 method, K is defined in Equation (73). The expectation of strategy returned pointsx s is evaluated as the statistical mean over the n iterations as follows:x
s = 1 n n ∑ i=1 x si(75)
The UoS-2 method is akin to the UoS-1 method for evaluating the strategy variance σ s 2 defined</p>
<p>in Equation (72) in since that it depends on the model uncertainty. However, this dependency is incorporated indirectly through the described Monte Carlo simulation. Algorithm 4 describes the UoS-2 method.</p>
<p>The proposed UoS active learning method, with its two variants, is general and can be combined with any exploitation-based strategy. Furthermore, the UoS method could be integrated with other popular active learning methods such as uncertain sampling [1] and expected model change [12], since most active learning strategies adhere to the greedy approach by querying a data point that maximizes or minimizes a certain selection criterion. In other words, the UoS querying approach could be used as a wrapper for any ordinary active learning method S that is greedy in its nature or does not consider the uncertainty of the learned model. This could be achieved by using S as the exploitation-based strategy used in the UoS method and adopting either of the two variants of the UoS method to estimate the strategy uncertainty.</p>
<p>Algorithm 4 The Uncertainty of Sampling Second Variant (UoS-2) Querying Method</p>
<p>Input: A dataset D = (x i , y i ), an exploitation active learning strategy S, the number of simulation iterations n and a scaling parameter K. Output: A query sample x * . Train the regression model using the training samples D to obtain the mean µ β|D and the covariance Σ β|D of the model parameters' vector β and the posterior estimates of a σ|D and b σ|D .
for i = 1 to n do Sample β i from t 2a σ|D (µ β|D , b σ|D a σ|D
Σ β|D ). x si ← the query sample returned after applying exploitation strategy S, using the sampled model parameters β i . end for Evaluate the average query samplex s :
x s = 1 n n ∑ i=1 x si σ s 2 ← K n−1 n ∑ i=1 (x si −x s ) 2 .
x * ← Generate a query sample according to a Gaussian distribution as follows: N (x s , σ s 2 ).</p>
<p>Utility minus Model Entropy (UME)</p>
<p>The Utility minus Model Entropy (UME) strategy controls the trade-off between exploration and exploitation in a novel way. The UME querying method adjusts the exploration and exploitation by explicitly modeling both of them in a formulated single objective function. Specifically, the UME method combines the ultimate goal of maximizing a certain utility function u, representing exploitation and the secondary but necessary target of minimizing model entropy, representing exploration, into one objective function. Then, the strategy queries the data sample x * maximizing this hybrid objective as follows:</p>
<p>x UME = arg max
x * E[u(x * )|D] − ηH<a href="76">β|x * , D</a>
where the model entropy H[β|p * , D] is evaluated using Equation (58) and η is the explorationexploitation trade-off control parameter. We conveniently let η be exponentially decreasing in time according to Equation (77). At early iterations, more emphasis is imposed on exploration to have better estimate for model parameters, however at later iterations since the model estimates get more robust over time, then more attention should be paid to the exploitation.
η = η 0 e −αt(77)
where t is iteration number and α &gt; 0.</p>
<p>Substituting from Equation (58) into Equation (76) results in:
x UME = arg max x * E[u(x * )|D] − η 2 log det(R β|D∪(x * ,y * ) ) + φ(2a D∪(x * ,y * ) , d)+ (a D∪(x * ,y * ) + d 2 ) Ψ(a D∪(x * ,y * ) + d 2 + 1)) − Ψ(a D∪(x * ,y * ) ) e − (a σ|D∪(x * ,y * ) −1)µ β T D∪(x * ,y * ) Σ β −1 D∪(x * ,y * ) µ β D∪(x * ,y * ) 2b σ|D∪(x * ,y * ) +1(78)
Since φ(2a * , d) does not depend on the query sample x * , then:</p>
<p>x UME = arg max
x * E[u(x * )|D] − η 2 log det(R β|D∪(x * ,y * ) )(a D∪(x * ,y * ) + d 2 )× Ψ(a D∪(x * ,y * ) + d 2 + 1)) − Ψ(a D∪(x * ,y * ) ) e − (a σ|D∪(x * ,y * ) −1)µ β T D∪(x * ,y * ) Σ β −1 D∪(x * ,y * ) µ β D∪(x * ,y * ) 2b σ|D∪(x * ,y * ) +1
(79)</p>
<p>Case Study: Dynamic Pricing with Demand Learning</p>
<p>We apply the proposed active learning framework described in Section 5 to a real-world application which is dynamic pricing for revenue maximization in case of unknown behavior of the customers' demand.</p>
<p>The main challenge of dynamic pricing with unknown demand is that the chosen prices should achieve some balance between exploitation and exploration. Exploitation represents choosing prices aiming to maximize the achieved revenue. On the other hand, exploration selects prices that promote learning the demand model parameters. This motivates us to apply our proposed active learning framework in Figure 1 to this application.</p>
<p>We assume a linear demand elasticity for modeling the customers' demand behavior as typically used in the economics/finance literature (see Equation (80)). The price is the main controlling variable for demand. We assume a monopolist seller, who has a sufficient inventory to satisfy all potential demand and we, specifically, consider pricing a single product over a finite selling horizon T.</p>
<p>The linear demand model equation is defined as follows:
y = a + bp +(80)
such that b &lt; 0 and ∼ N (0, σ 2 ). The parameter b represents the price-demand sensitivity, so it is naturally negative since the price and demand have an inverse relationship. For example, if price rises by 10%, demand would diminish, On the other hand, when price decreases by 10%, demand would increase.</p>
<p>In order to estimate the demand model parameters a and b defined in Equation (80), we apply the Bayesian linear regression model described in Section 4. We employ the active learning framework with its different query generation schemes defined in Section 5.1 and described in detail in Algorithm 1, Algorithm 2 and Algorithm 3. Applying active learning formulation to the dynamic pricing problem, the training data D consists of some pairs of prices and their corresponding demands (p i , y i ). In addition, the query point x * denotes the vector [1 p * ]. For this application, the utility function after querying a certain price p represents the gained revenue R , which is defined as follows:
R = p(a + bp)(81)
where a and b are the demand model parameters defined in Equation (80).</p>
<p>Active Learning Framework Application</p>
<p>In this section, we apply the active learning formulations represented in Section 5 to the dynamic pricing with demand learning problem. First, the exploration-based strategies hinge on minimizing the regression model error, without considering the utility function u in their formulations. So, the formulations presented in Section 5.2 can be exactly used for the underlying dynamic pricing application. On the other hand, for the exploitation-based and the balancing strategies described in Section 5.3 and Section 5.4 respectively, specific formulations should be derived for the considered application, setting the utility function u to the gained revenue defined in Equation (81).</p>
<p>Exploration-Based Strategies</p>
<p>In our experiments, we apply the four presented strategies in Section 5.2 with pool-based and query synthesis schemes. Moreover, for mutual information, modified mutual information and Kullback-Leibler divergence, we implement the query synthesis approach without a predefined pool as described in Section 5.1 and Algorithm 3. When applying the query synthesis method without a predefined pool to the dynamic pricing problem, we construct U defined in Algorithm 3 as follows: since the dynamic pricing application has one controlling variable, the product price, we consider the range of all potential prices between p min and p max and along the active learning iterations, we exclude the prices that are previously queried, added to the training set D L . This set of prices P are used as unlabeled samples for evaluating the information-theoretic metrics as defined in (Equations (25), (41) and (47)).</p>
<p>Exploitation-Based Strategies</p>
<p>In this section, we apply the exploitation-based strategies introduced in Section 5.3 to the dynamic pricing with demand learning problem.</p>
<p>• Greedy Strategy</p>
<p>Given Equation (60) and setting the utility function u, to the gained revenue defined in Equation (81) results in:
p G = arg max p * E<a href="82">R|p * , D</a>
Using the revenue definition in Equation (81), the expected revenue E[R|p * , D] for any price p * is evaluated using:
E[R * |p, D] = p * (x * T µ β|D ) (83) where x * = [1 p * ].
We apply the greedy strategy in pool-based setting. In addition, we apply it in the query synthesis setting as well by maximizing the expected revenue as stated in Equation (82), using any optimization method or even using a simple grid search if the range of prices between p min and p max is limited.</p>
<p>By differentiating the expected revenue E[R|p * ] w.r.t price p * , the myopic price p G maximizing the expected immediate revenue would be calculated as follows:
p G = −â|D 2b|D (84)
whereâ|D andb|D are the estimates of the demand model parameters a and b defined in Equation (80) using the labeled data gathered so far D.</p>
<p>•</p>
<p>Expected Value of Perfect Information (EVPI)</p>
<p>When applying the value of information strategy to the considered problem, the action α defined in Equation (61) is querying a price p * . The initial evidence e represents the training labeled data points so far D. Similarly, e j denotes the acquired demand y of the query price p * , which represents the piece of information we seek to evaluate.</p>
<p>Accordingly, for the considered problem, the expected utility of taking action p * given the evidence D and after revealing evidence y j , the expected utility term, EU(α e j k |e, E j = e jk ), defined in Equation (65) can be formulated as:
E[u(p * y j |D, y j )] = max p R(p|D, y j )(85)
where the utility u can be set to the immediate revenue R.</p>
<p>Using the linear demand model defined in Equation (80) and then applying the optimal price maximizing the immediate revenue defined in Equation (84) 
E[u(p * y j |D, y j )] = −â 2 |D, (x * , y j ) 4b|D, (x * , y j )(86)
The second term of Equation (61) could be safely ignored, since the objective is to experiment a price p * , maximizing EVPI and this term is independent of p * . This is implied by the following equation, Equation (87), it can be observed that this term does not affect the process of maximizing EVPI.
EU(p * |D) = max p R(p|D, p * ) = max p R(p|D) = −â 2 |D 4b|D (87)
Then, evaluating the EVPI method for revenue maximization problem, by substituting from Equation (86) into Equation (61) results in the following formula:
EVPI D (y * ) = y j P(y j |D, x * ) −â 2 |D, (x * , y j ) 4b|D, (x * , y j ) dy j(88)
Finally, maximizing Equation (88) by differentiating it with respect to p * , equating the derivative to zero and solving the resulting equation or using any direct optimization method, we get the price maximizing the expected value of perfect information as follows:
p EVPI = * arg max x y j P(y j |D, x * ) −â 2 |D, (x * , y j ) 4b|D, (x * , y j ) dy j(89)</p>
<p>Balancing Exploration and Exploitation Strategies</p>
<p>In this section, we consider applying the balancing strategies that combine both aspects of exploration and exploitation and attempt to achieve balance between both of them.</p>
<p>•</p>
<p>Upper Confidence Bound (UCB)</p>
<p>Applying the UCB strategy to the dynamic pricing problem and setting the utility function u defined in Equation (69) to the immediate revenue R results in:
p UCB = arg max p * E[R|p * , D] + ησ R|p * ,D(90)
where E[R|p * , D] and σ R|p * ,D are the expectation and the standard deviation of the estimated immediate gained revenue R in response to price p * and using training data labeled so far D.</p>
<p>The expected revenue E[R * |x * , D] is calculated as:
E[R * |p * , D] = p * E<a href="91">y|x * , D</a>
where the expected demand E[y|x * , D] is computed using the Bayesian linear regression (see Equation (23)) presented in Section 4.</p>
<p>Accordingly:
E[R * |x * , D] = p * (x * T µ β |D)(92)
Using revenue definition in Equation (81) and the posterior variance for demand defined in Equation (24), the variance of revenue σ 2 R * |x * ,D is calculated as follows:
σ 2 R * |x * ,D = p * 2 b σ|D a σ|D − 1 1 + x * T Σ β|D x *(93)
Substituting from Equations (92) and (93) into Equation (90), then the price maximizing the UCB criterion can be evaluated as defined in Equation (94).
p UCB = arg max p * p * (x * T µ β|D ) + η p * b σ|D a σ|D − 1 1 + x * T Σ β|D x * (94)
where Σ β|D is evaluated using Equation (6). For the Gamma distribution parameters a σ|D and b σ|D , they are evaluated using Equations (7) and (8), respectively.</p>
<p>•</p>
<p>Probabilistic-based Exploration-Exploitation (PEE)</p>
<p>In our experiments, we apply several instances of this hybrid strategy. We combine the pure exploitation, greedy, strategy as defined in Equation (84), with all the proposed exploration-based methods in addition to the popular active learning method, uncertain sampling [15] and we apply random sampling as a representative for random exploration.</p>
<p>•</p>
<p>Uncertainty of Strategy (UoS)</p>
<p>For the uncertainty of strategy method, defined in Equation (71), the resulting price p UoS follows a Gaussian distribution:
p UoS ∼ N(p s , σ s 2 )(95)
where the mean of this Gaussian distribution, p s , represents the price returned using pure exploitation, which is the greedy strategy as defined in Equation (84). Regarding the variance of strategy σ s 2 , it can be evaluated using two variants: in terms of model uncertainty and using Monte Carlo simulation as described in Section 5.4, specifically using Equations (72) and (74), respectively.</p>
<p>• Utility minus Model Entropy (UME)</p>
<p>The UME criterion as defined in Equation (76) 
p UME = arg max p * p * (x * T µ β |D) − η 2 log det(R β|D∪(x * ,y * ) )(a D∪(x * ,y * ) + d 2 )× Ψ(a D∪(x * ,y * ) + d 2 + 1)) − Ψ(a D∪(x * ,y * ) ) e − (a σ|D∪(x * ,y * ) −1)µ β T D∪(x * ,y * ) Σ β −1 D∪(x * ,y * ) µ β D∪(x * ,y * ) 2b σ|D∪(x * ,y * ) +1(96)
where x * = [1 p * ] and d is the dimensionality which equals to 2 in the dynamic pricing application, with linear demand elasticity as defined in Equation (80). Therefore, by differentiating the objective function defined in Equation (96) and equating the resulting equation to zero, we can get the price maximizing the UME.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>In order to evaluate the performance of the proposed active learning framework summarized in Figure 1, as well as three baseline active learning methods including: random sampling (RS), the greedy or myopic strategy (check Section 5.3.1) and upper confidence bound method, which is intensively used in the multi armed bandit context [36] (see Section 5.4.1). In our experiments, we apply the proposed active learning framework to the dynamic pricing with demand learning problem described in Section 6. In the presented experiments, we mainly focus on analyzing the exploitation strategies introduced in Section 5.3 and the strategies balancing between exploitation and exploration presented in Section 5.4 since the main interest of the paper is applying active learning to utility optimization, which is exploitation.</p>
<p>In this work, we aim to perform a qualitative analysis to evaluate the performance of the pool-based approach versus the query synthesis approach since the query synthesis approach is computationally more efficient than the commonly adopted pool-based approach. Furthermore, the query synthesis approach could be more beneficial for objective optimization, such as maximizing revenue or even minimizing the learning model error, since it is not restricted to a certain given pool of samples. For mostly, all of the proposed active learning strategies including: our proposed strategies and the baseline methods, we implement two variants: one in pool-based setting and the other using query synthesis. In addition to these two active learning schemes, we further apply the third method, query synthesis without a predefined pool described in Section 5.1, to our proposed active learning methods that require the existence of a pool of unlabeled samples such as mutual information (MI), modified mutual Information (MMI) and Kullback-Leibler divergence (KL), in addition to the probabilistic-based exploration-exploitation (PEE) methods using either of MI, MMI or KL strategies for exploration.</p>
<p>In our experiments, we experiment different variants of the the probabilistic-based explorationexploitation (PEE) approach combining greedy exploitation strategy with several exploration methods including our proposed approaches described in Section 5.2 in addition to random sampling and uncertain sampling [15]. The implemented PEE methods combining our proposed exploration methods presented in Section 5.2 with greedy sampling are denoted as KL-G, MI-G, MMMI-G and ME-G. In addition, we denote combining random and greedy sampling as (RS-G). Similarly, the method combining uncertain and greedy sampling is denoted as (US-G).</p>
<p>Some strategies such as the two variants of uncertainty of strategy (UoS) are basically designed so that the query point is generated or derived by optimizing an objective function. So, for these strategies we consider the query synthesis approach only since the pool-based approach does not apply for UoS.</p>
<p>In the adopted experiments, we apply the Bayesian linear regression model with conjugate prior of the model parameters β and σ, as described in Section 4, for estimating the demand at each iteration.</p>
<p>We conduct our experiments on synthetic and real datasets. The advantage of using artificial data is that the true model parameters β = [a; b] are known. Accordingly, the ground truth value for the objective function, that is, the gained revenue, can be accurately computed with the knowledge of the true optimal model parameters as defined in Equation (81). Moreover, the estimation error of the model parameters β can be properly evaluated.</p>
<p>Evaluation Metrics</p>
<p>We assess the performance of our proposed active learning framework in terms of two aspects: the gained utility (revenue) and the accuracy of estimating the regression model parameters.</p>
<p>In order to evaluate the utility maximization, we measure the revenue gain or a normalized version of the total discounted utility u T achieved in the considered time period as defined in Equation (97). We adopt discounted utilities to place more emphasis on getting rewards soon, as widely used in reinforecement learning [52].
u Gain = ∑ T i=1 γ i−1 u i ∑ T i=1 γ i−1 u opt(97)
where u i is the revenue obtained at iteration i and u opt is the optimal revenue given the true model parameters a and b, which is calculated as:
u opt = p opt (a + bp opt )(98)
where p opt is the optimal price, which equals to −a 2b in case of linear demand model defined in Equation (80). Simplifying the term ∑ T i=1 γ i−1 using the summation of geometric series formula, as follows:
u Gain = ∑ T i=1 γ i−1 u i (1 − γ T )/(1 − γ)u opt(99)
For the applications where the optimal utility is not known or cannot be evaluated, the total discounted utility can be used as an evaluation metric.</p>
<p>Concerning the demand model estimation error, we evaluate it in terms of the deviation of the final estimated demand model parameters ∆ β , from the true parameters β as indicated in Equation (100). The final model estimate is evaluated using the expectation of β given the training data µ β|D , as defined in Section 4.
∆ β = ||β − µ β|D || 2 ||β|| 2(100)</p>
<p>Experiments Using Synthetic Datasets</p>
<p>We perform a Monte Carlo simulation, generating 12 synthetic datasets of different parameters a, b and noise levels σ. We use two values for a, a = 100 and a = 1000. For each value for a, we adopt three different values for the sensitivity parameter b representing elastic demand (b = −2), neutral demand (b = −1) and inelastic demand (b = −0.5). Two different values are adopted for the noise parameter σ representing low (5%) and high (40%) noise levels. Investigating different noise levels enables us to analyze the impact of the noise on the different active learning strategies and evaluate their immunity towards noise. Moreover, for the dynamic pricing problem, we use different noise levels as a way for aggregating all other influencing factors that could affect the demand and may be hard to model, such as competition, seasonality or perishability of the products.</p>
<p>For each dataset, we run the experiment 10 runs and we present the average results over the runs. The synthetic datasets are created as follows: first, we generate N = 1000 price points from a Gaussian distribution with mean µ p and variance σ p 2 . Then, we assign values for demand elasticity parameters a and b. After that, assuming a linear demand model, we calculate the corresponding demands using Equation (80). We express the noise level parameter σ in terms of a percentage of the maximum possible demand a.</p>
<p>In our experiments, we set µ p and σ p of the Gaussian distribution used for generating the pricing data, using the pricing boundaries given by the seller p min and p max . For µ p , it is the mean price of the prices in range of the [p min, p max ], which equals to p min +p max 2 . Similarly, for the standard deviation sigma p , it is estimated using the standard deviation of the potential prices in the range of [p min , p max ]. We set multivariate Gaussian prior for β as follows: µ = [10, −0.5], Σ = 10 4 I. For the inverse Gamma prior distribution parameters of the noise parameter σ 2 , we set a σ = 2 and b σ = 1.</p>
<p>The simulation proceeds as follows: for each problem, we generate a pool of price-demand data points, starting with a very limited number of data points, N init = 3 points, then we train a Bayesian regression model to obtain an initial estimate for the model parameters β. After that, we run the different exploitation and balancing active learning strategies described in Sections 5.3 and 5.4, respectively, with different schemes: pool-based, query synthesis and query synthesis without a predefined pool. For the query synthesis strategies (with and without pool variants), we assume that there is an oracle revealing the true demand value y * for the chosen query point x * . For each active learning strategy, we evaluate its performance by measuring the percentage revenue gain defined in Equation (99) and model estimation error defined in Equation (100).</p>
<p>Generally, most of the strategies balancing between exploration and exploitation have a hyper-parameter that controls the trade-off between exploration and exploitation. We set the controlling parameters of the balancing strategies introduced in Section 5.4 as follows: for the UCB method, the η parameter in Equation (69) is set to 0.01. For the PEE method, the α parameter in Equation (70) is set to 0.7. The K parameter of UoS strategy first variant, UoS-1, the parameter Z in Equation (73) is set to 0.5, while it is set to 0.7 for the second variant UoS-2. Finally, we set α parameter defined in Equation (77), such that at the last iteration T, where the exploration is nearly diminished, η equals to a small value:
η = 0.3.
Regarding the η 0 parameter of the same equation, Equation (77), we use values to let the impacts of the exploitation and exploration be comparable at the first iteration.</p>
<p>For the price-demand curve estimation problem, we enforce a constraint that the chosen price p * at each iteration is within the pricing interval defined by the seller where the minimum allowable price is p min and the maximum possible price p max , accordingly p min ≤ p * ≤ p max . The active learning loop continues till reaching a certain predefined number of iterations T = 100. For the pool-based strategies, the pool size N = 1000. We set the discount factor of revenue gained γ used in Equation (99) to 0.99.</p>
<p>The average results for the revenue gain and the regression model estimation error for different active learning strategies using different noise levels are represented in Tables 1 and 2, respectively. Table 1. The average revenue gain of the active learning methods, over twelve synthetic datasets, using different noise levels σ. The strategies are sorted descendingly according to their average revenue gain over the two noise levels. The bold entries represent the maximum revenue gain per column (over all strategies). One of the main contributions of this work is to perform a comparative analysis between the query synthesis and pool-based active learning approaches and demonstrate the benefits of applying active learning query synthesis based strategies for utility maximization. Accordingly, in the performed experiments, for each active learning, except random sampling since it is considered a passive learning method, we adopt a pool-based version and a query synthesis one. We provide an empirical analysis between both approaches. We include both aspects of the achieved revenue gain and model estimation error. Tables 3 and 4 represent the average revenue gain and the average model estimation error, respectively, for pool-based methods versus query synthesis ones. Table 3. The average revenue gain for the pool-based versus query synthesis approaches, over twelve synthetic datasets, using different noise levels σ. The bold entries represent the maximum average revenue gain per row (over the two active learning approaches).</p>
<p>Active Learning Strategy</p>
<p>Dataset</p>
<p>Noise Level σ Pool-Based Query Synthesis In addition, in order to investigate the superiority of either active learning approach, we evaluate the percentage of the pool-based strategies versus the query synthesis strategies existing in the top-10 performing methods in terms of achieving revenue gain, averaged over the different synthetic datasets, as presented in Table 5. Similarly, Table 6 shows the percentage of strategies from both approaches, pool-based and query synthesis, placed in the top-10 strategies achieving minimum regression model estimation error. Table 5. The percentage of strategies in the top-10 strategies achieving revenue gain belonging to the pool-based approach versus the query synthesis approach, over twelve synthetic datasets, using different noise levels σ. The bold entries represent the maximum percentage per row (over the two active learning approaches).
a = 100, b = −0.5 σ = 5%</p>
<p>Dataset</p>
<p>Noise Level σ Pool-Based Query Synthesis 
a = 100, b = −0.5 σ = 5%</p>
<p>Experiments Using Real Datasets</p>
<p>To have the parameters more realistic, we have used several real datasets described in Table 7. We have gathered the first dataset in the table, transport, online though surveying. The dataset is a transportation ticket pricing data, where we ask users about the minimum and maximum fares they would pay for an economy class bus ticket of an air-conditioned bus between any general two cities, City A and City B, such that City A is away around 220 km from City B. We collected 41 responses from different users. In order to have data in the form of price and demand pairs, we perform the following. For each price, we calculate the corresponding demand as the number of users who can afford this price according to the minimum and maximum prices of the data.</p>
<p>For beef dataset, it is obtained from the USDA Red Meats Yearbook [53]. The sugar dataset is adopted from Reference [54] and the spirits dataset is originated from Reference [55]. Finally, the coke dataset is adopted from Reference [56]. There is one hurdle in using such real datasets. In our proposed active learning framework, especially the query synthesis approach, the chosen data point or chosen price p * could potentially be outside the available prices provided in the dataset. Thus, we utilize the dataset mainly for estimating linear demand model parameters vector β using ordinary least squares linear regression. Concerning the noise parameter σ 2 , we estimate it using the maximum likelihood estimator. The estimated model parametersâ,b andσ for all the real datasets, are listed in Table 7. Using the obtained demand model parameters, we generate synthetic data using these parameters, with the same methodology described in Section 7.2. However, the mean and variance of the Gaussian distribution for generating pricing data mu p and v p , are estimated using the original prices of the real datasets. Tables 8 and 9 represent the revenue gain and the estimation error of the regression model parameters, respectively, for the five considered real datasets described in Table 7.   Table 10 summarizes the average utility (revenue) gain and average model percentage error, averaged over the five real datasets described in Table 7, for all the considered active learning strategies. Table 10. The average results of active learning methods in terms of the average revenue gain and average percentage error, over the five considered real datasets. The strategies are sorted descendingly according to their average obtained revenue gain. The bold entries for the first column represent the maximum average revenue gain over all strategies and the bold entries for the second column represent the minimum estimation error over all strategies.  We have experimented different values for the number of initial training points N init in order to evaluate the impact of varying the number of initial training points on the performance of the different active learning methods. Tables 11 and 12 show the average revenue gain and model percentage error, respectively, averaged over the five considered real datasets described in Table 7. For space considerations, we include the results of this experiment for the real datasets only. The synthetic datasets exhibit a very similar behavior. Table 11. The average revenue gain of active learning methods versus different number of initial training points N init , averaged over the five considered real datasets. The bold entries represent the maximum average revenue gain per row (over the different number of initial training points N init ). Similar to the synthetic datasets, we compare both active learning approaches, pool-based and query synthesis over the five real datasets in terms of the revenue gain and the model estimation error. Thus, Tables 13 and 14 demonstrate the average revenue gain and average model estimation error for both approaches over the five real datasets presented in Table 7.</p>
<p>Active Learning Strategy Average Revenue Gain Average Model Percentage Error</p>
<p>Active Learning Strategy
N init = 3 N init = 5 N init = 10 G-
The percentage of pool-based strategies versus query synthesis strategies ranked within the top-10 strategies in terms of the revenue gain is presented in Table 15. Similarly, for model estimation error, Table 16 shows the percentage of strategies of both active learning approaches placed within the top-10 strategies achieving the least model estimation error. </p>
<p>Discussion</p>
<p>In this section, we investigate the empirical results presented in Section 7. The main findings inferred from the experimental results are summarized as follows:</p>
<p>• It is evident from the presented results presented in Tables 1 and 10, that our proposed active learning strategies, especially the balancing methods, outperform the standard baselines: the upper confidence bound method, greedy sampling and random sampling, in terms of the achieved utility function (the revenue gain). There are several reasons for this compelling performance.</p>
<p>First, our proposed balancing methods attain the balance between exploration and exploitation using several novel approaches as described in Section 5.4. For example, for the proposed uncertainty of sampling method (UoS), it combines both aspects of utility maximization and regression model estimation in a probabilistic way, where the exploration is controlled using the model uncertainty.</p>
<p>In addition, the utility minus entropy (UME) method incorporates the model uncertainty, in addition to the utility function into one hybrid objective function to be optimized, as indicated in Equation (76). The explicit formulation of exploration in the active learning selection criterion imposes an emphasis over the exploration in order to obtain accurate model estimation and hence achieve high future utility returns along the active learning iterations.</p>
<p>Finally, in the probabilistic-based exploration-exploitation method, we employ several powerful exploration methods, with the pure exploration method, the simple greedy sampling. The proposed exploration methods presented in Section 5.2, which are Kullback-Leibler divergence, mutual information and model entropy, have a great impact on estimating the regression model parameters, which indirectly helps boosting the gained utility (revenue). • Table 1 shows the revenue gain for different artificial datasets, using different noise levels. It could be observed from this table that our proposed balancing strategies between exploration and exploitation such as the two variants of UoS, the four variants of PEE method combining the information theoretic exploration and pure exploitation: KL-G, MI-G and MMMI-G, ME-G and UME, show a significant revenue gain compared to the pure exploitation strategies such as greedy sampling and EVPI, especially for noisy datasets where σ = 40%. • Moreover, Table 1 indicates that the proposed balancing strategies outperform the baselines including: random sampling, greedy method and the UCB method. It can be observed that our proposed balancing strategies yield a substantial utility (revenue) gain in case of large noise 40%. For example, the KL-G strategy in both pool-based and synthetic settings, achieves around 2%-4% improvement, on average, over greedy sampling (GS). Furthermore, the KL-G method achieves 13%-15% improvement over the upper confidence bound (UCB) method [36] and around 16%-18% over random sampling RS. • Table 2 demonstrates the estimation error of the regression model parameters, averaged over different artificial datasets for all the considered active learning strategies. Our proposed balancing strategies achieving high utility (revenue) gain such as KL-G in both pool and synthesis settings, UoS-1 and UoS-2, are not the best performing methods in terms of the model estimation error. However, these methods eventually yield a better model estimation than the baselines including: greedy sampling and UCB as indicated in Table 2. Furthermore, the main target is utility (revenue) optimization and the model estimation is a necessary but secondary objective. Moreover, the other proposed balancing strategies such as KL-G, MI-G and MMI-G have comparable performance to the baselines.</p>
<p>•</p>
<p>For the real datasets, it can be observed from Table 10 that our proposed first variant of the uncertainty of strategy (UoS-1) is the best performing method in terms of the revenue gain. Although G-Synth has the same average revenue gain as UoS-1 method's gain, the UoS-1 method has lower estimation error rates than G-Synth. As mentioned in Section 5.4, the UoS-1 method accounts for the model uncertainty to control the exploration window (see Equation (72)). In addition to its promising performance, the UoS method is practically simple to implement.</p>
<p>•</p>
<p>The two variants of our introduced balancing method, uncertainty of strategy (UoS), achieve significant performance in terms of the achieved revenue gain as indicated in Tables 1 and 10 for synthetic and real datasets, respectively. The major reason for the significant performance of the UoS method is that it accounts for the uncertainty of the selection criterion itself. Furthermore, this method combines the exploration and exploitation probabilistically like the UCB and the PEE methods.</p>
<p>•</p>
<p>Regarding the proposed PEE methods: KL-G, MI-G, MMMI-G and ME-G, they produce substantial performance in terms of the achieved revenue gain for synthetic and real datasets, as shown in Tables 1 and 10, respectively. Specifically, the KL-G method is the best performing method in terms of the achieved revenue gain for the synthetic datasets (see Table 1). • Furthermore, for the results of the real datasets presented in Table 10, the KL-G, MI-G, MMI-G and ME-G methods are of the top-10 strategies with respect to the achieved revenue gain. Moreover, for the model estimation error, they are comparable to the considered baselines. However, the KL-G variants provide competitive model estimation for both synthetic and real datasets as shown in Tables 1 and 10, respectively.</p>
<p>• There are three major reasons for the promising results of the PEE strategies: KL-G, MI-G, MMI-G and ME-G. First, these strategies are based on information theoretic concepts: Kullback-Leibler divergence [44] and entropy [43], as described in Section 5.2. Second, these methods adopt a probabilistic approach for balancing the exploration and exploitation as presented in Section 5.4, unlike the UCB method. The third reason is that the employed exploration strategies perform an effective exploration since they take into account the information of the unlabeled samples and the model uncertainty.</p>
<p>•</p>
<p>In addition to our proposed strategies of the probabilistic-based exploration-exploitation (PEE) method, we have extended two more PEE methods combining uncertainty sampling [1] and random sampling, to perform exploration, with greedy sampling for exploitation. We experiment these two baselines for comparison purposes. For synthetic datasets, it could be noticed from Table 2 that the RS-G method with both versions, pool-based and query synthesis, achieves comparative model estimation. However, the RS-G method compromises the achieved revenue gain as indicated from Table 1, since it obtains a revenue gain that is around 3%-4% below the top method, KL-G-Pool. Similarly, for the real datasets' results presented in Table 10, the two methods US-G and RS-G obtain accurate model estimate, however both of these methods compromise the achieved revenue.</p>
<p>•</p>
<p>These results essentially elucidate the significance of our proposed information-theoretic exploration strategies presented in Section 5.2. Although the same exploitation method is used, the greedy sampling and the same probabilistic approach, the PEE method, is followed for combining exploration and exploitation (see Section 5.4), the proposed methods, specifically, KL-G and MMMI-G, exhibit better performance than the US-G and RS-G methods in terms of the achieved revenue, which is the main target.</p>
<p>Furthermore, our proposed methods obtain model estimation performance close to the US-G and RS-G methods, for both synthetic and real datasets as presented in Tables 1 and 10, respectively. The other two proposed methods MI-G and ME-G, also outperform the RS-G and US-G methods, for real datasets and produce comparable performance for the synthetic datasets.</p>
<p>The reason for the performance preeminence of our proposed information theory-based strategies over the US-G and RS-G methods in terms of revenue gain is that the proposed methods essentially exploit the potential information of the unlabeled data and the model uncertainty. Moreover, these strategies not only improve the model estimation error but also query representative data samples that minimize the model uncertainty, which promotes the exploitation performance. • From Table 10, it can be noticed that the greedy sampling performs well in the real datasets, since the considered real datasets have very low noise, expressed in terms ofσ in Table 7. Also, the UCB baseline [36] performs comparably well on the real datasets due to the datasets' robustness. Similarly for artificial datasets, Table 1 shows that for the low noise datasets, having σ = 5%, both of the greedy sampling and the UCB methods perform quite well, comparable to the best performing method, our proposed balancing method UoS-1. On the other hand, for the noisy datasets where σ = 40%, both of the greedy sampling and the UCB methods result in poor performance, in terms of the gained revenue. This could be apparently observed from Table 1.</p>
<p>•</p>
<p>Regarding the other developed pure exploitation method, namely the expected value of information (EVPI), it could be noticed from Tables 2 and 10, the EVPI strategy results in better model estimation than the greedy sampling as EVPI chosen samples incur some diversity unlike the points chosen by the greedy method which essentially queries points maximizing the utility function. However, the greedy sampling, adequately, outperforms EVPI in case of low noise and for real datasets (see Tables 1 and 10, respectively). On the other hand, for the noisy datasets, the EVPI approach, in both settings pool-based and query synthesis, attains larger revenue gain than the corresponding methods for greedy sampling as shown in Table 1 since the EVPI method is less myopic than the greedy sampling, so it is more immune to the noisy datasets.</p>
<p>•</p>
<p>Concerning the random sampling (RS) baseline method, since it is a pure exploration strategy, convincingly, it does not achieve high revenue gains for synthetic and real datasets as indicated in Tables 1 and 10, respectively. However, since random sampling could be regarded as a pure exploration methods, it, intuitively, performs well with respect to the model estimation error as shown in Tables 1 and 10, for synthetic and real datasets, respectively. However, as previously mentioned, random sampling considerably jeopardizes the gained revenue.</p>
<p>•</p>
<p>In this work, we perform a comparative empirical analysis between the pool-based and query synthesis active learning approaches. The empirical analysis considers both evaluation metrics, the achieved gained utility (revenue) and the percentage regression model error. As we mentioned in Section 7, we exclude the random sampling from this analysis since it is a passive learning method.</p>
<p>•</p>
<p>Concerning the revenue gain, Tables 3 and 13 demonstrate the average revenue for the strategies of each approach, for synthetic and real datasets, respectively. It could be observed that the query synthesis approach clearly outperforms the pool-based approach for both the artificial (with improvement around 3.5%) and real datasets (with improvement around 10%). The improvement is more significant in case of real datasets as will be discussed subsequently. Moreover, as indicated in Table 15, the query synthesis approach is more dominant within the top-10 strategies in terms of achieving revenue gain, for real datasets. Specifically, 76% of the top-10 strategies achieving revenue gain, belong to the query synthesis approach, whereby only 24% strategies are pool-based methods.</p>
<p>•</p>
<p>Regarding the low noise synthetic data, both of pool-based and query synthesis approaches result in similar performance in terms of average revenue gain as shown in Table 3.</p>
<p>•</p>
<p>On the other hand, in case of large noise level, it could be inferred from Table 3 that the query synthesis approach surpasses the pool-based one. In other words, the revenue gain improvement of the query synthesis over the pool-based approach is around 7.5%. In addition, the query synthesis methods have higher ranks than the pool-based ones in terms of the average revenue gain. Specifically, the ratio between the former and the latter is 55% to 45%, respectively. These results are persuasive since for noisy data, the pool of samples could be misleading, so querying a synthetic data sample in the global input space, that is not necessarily belonging to a specific set of data samples, would be more effective for optimizing a certain utility function. •</p>
<p>Regarding the model percentage error, the pool-based approach produces a slightly better model estimate for artificial datasets (see Table 4). However, the query synthesis methods have more advanced ranks than the pool-based methods as presented in Table 6, for both of the low and high noise levels.</p>
<p>•</p>
<p>By examining the real datasets results, we can find that the query synthesis approach accomplishes less error rates than the pool-based approach as shown in Table 14. In addition, when investigating the top-10 strategies in terms of minimizing the model estimation errors in Table 16, one could observe that the query synthesis methods occupy 72% of the top-10 strategies, compared to 24% for the pool-based methods.</p>
<p>•</p>
<p>The two real datasets, namely the sugar and coke datasets, described in Table 7 represent typical cases where the pool-based approach suffers from a major performance hurdle in terms of the obtained revenue gain as presented in Table 13. The reason for the poor performance of the pool based approach for these two datasets is that the available pool of data samples is limited and not representative enough. Furthermore, the available data samples do not contain the optimal price maximizing the target utility function, which is the gained revenue.</p>
<p>•</p>
<p>For example, for the coke dataset, according to the linear demand model parameters presented in Table 7 and using the revenue equation (Equation (81)), the optimal price maximizing revenue is −â 2b = 74.24. However, the mean and the standard deviation, of the available prices of this dataset are µ p = 22.96 and σ p = 3.2376, respectively. Accordingly, the available prices of the pool are too far from optimal, that is why the pool-based strategies do not perform well on this dataset. The sugar dataset exhibits a very similar behavior as well. The lack of diversity in the pool is considered a serious drawback for the pool-based approach. In contrast, the query synthesis approach is not affected by such problem since the query synthesis approach chooses the data sample to be labeled from the entire input space and it is not restricted by the available pool of data (see Tables 13 and 15).</p>
<p>•</p>
<p>One could infer from Tables 1 and 10 that the two variants of query synthesis approach with a predefined pool (see Algorithm 2) and without a predefined pool (see Algorithm 3), yield comparable performance for the different strategies. This is reasonable since both methods are logically equivalent, they only differ in implementation details. The query synthesis without a predefined pool approach, defined in Section 5.1 is essentially designed for the applications where it could be complicated to have a pool of representative data samples to be used for the information theory-based active learning strategies, namely KL, MI and MMI. In our experiments, we utilize the domain knowledge of the pricing data and construct a set of data samples belonging to the price range defined by the seller [p min , p max ] to be used by the information theoretic methods: KL, MI and MMI. • Finally, Tables 11 and 12 show that increasing the initial training points enhances the performance of most of the active learning methods, in terms of both the revenue gain and the model estimation accuracy. These results are reasonable since having more initial data samples promotes the regression model's accuracy, so the gained revenue is improved as well. In addition, Table 12 indicates that as the number of initial training samples increases, the performance of the different active learning methods gets closer to each other since the initial model estimate gets more robust.</p>
<p>For the revenue gain, the query synthesis methods achieve similar performance as indicated in Table 11 for N init = 10. However, most of the pool-based methods do not achieve a significant performance improvement due to the limitation of the pool-based approach, previously discussed, for some real datasets, the sugar and coke datasets.</p>
<p>Conclusions</p>
<p>In this paper, we propose a novel active learning framework for optimizing a general utility function. Specifically, this work targets the class of problems incurring some trade-off between exploration and exploitation. We introduce several novel active learning methods for exploration, exploitation and for balancing both. The presented exploration strategies are essentially based on information theory concepts such as mutual information (MI), Kullback-Leibler divergence (KL) and model entropy (ME). Consequently, when combined with exploitation, such information theoretic exploration methods achieve promising performance in terms of the achieved utility and the learning model error as well. Furthermore, we develop new approaches for balancing exploration and exploitation such as the uncertainty of strategy (UoS) method that controls the exploration window according to the model uncertainty. In addition, we present another balancing method, utility minus entropy (UME) where the model entropy is explicitly modeled and augmented with the target utility function into one hybrid objective function to be optimized.</p>
<p>In this work, we investigate two main approaches of active learning, the pool-based approach which is widely used in active learning literature and the membership query synthesis approach. Moreover, we present an empirical analysis for comparing both approaches. The experiments show the exceptional performance of the query synthesis approach compared to the pool-based approach for the synthetic and real datasets. The compelling results for query synthesis approach could help boosting the active learning research towards employing the query synthesis approach.</p>
<p>We have applied the proposed framework to an operation research related application, namely, dynamic pricing with demand learning. However, our proposed framework can easily be adapted to other applications. We perform several experiments using synthetic and real datasets. In our experiments, we compare our proposed active learning strategies to several baselines and our presented strategies yield a significant performance improvement in terms of both aspects: the achieved gained revenue and the regression model error.</p>
<p>Figure 1 .
1The Proposed Active Learning Framework.</p>
<p>j )] = max a ∑ s P(Result(a) = s , e, e j )u(s )</p>
<p>Table 4 .
4The average percentage model error for the pool-based versus query synthesis approaches, over twelve synthetic datasets, using different noise levels σ. The bold entries represent the minimum average estimation error per row (over the two active learning approaches).98.66% 
98.36% 
σ = 40% 
80.44% 
80.85% </p>
<p>a = 100, b = −1 
σ = 5% 
96.41% 
95.24% 
σ = 40% 
88.92% 
90.86% </p>
<p>a = 100, b = −2 
σ = 5% 
98.45% 
98.47% 
σ = 40% 
83.80% 
91.81% </p>
<p>a = 1000, b = −0.5 
σ = 5% 
97.71% 
97.99% 
σ = 40% 
56.24% 
78.17% </p>
<p>a = 1000, b = −1 
σ = 5% 
97.49% 
96.25% 
σ = 40% 
89.12% 
82.93% </p>
<p>a = 1000, b = −2 
σ = 5% 
96.59% 
97.84% 
σ = 40% 
64.92% 
84.16% </p>
<p>Average 
σ = 5% 
97.55% 
97.36% </p>
<p>Average 
σ = 40% 
77.24% 
84.80% </p>
<p>Average 
87.40% 
91.08% </p>
<p>Dataset 
Noise Level σ Pool-Based Query Synthesis </p>
<p>a = 100, b = −0.5 
σ = 5% 
3.21% 
4.60% 
σ = 40% 
12.59% 
26.55% </p>
<p>a = 100, b = −1 
σ = 5% 
2.79% 
2.51% 
σ = 40% 
6.39% 
9.01% </p>
<p>a = 100, b = −2 
σ = 5% 
4.73% 
2.63% 
σ = 40% 
16.58% 
20.75% </p>
<p>a = 1000, b = −0.5 
σ = 5% 
4.03% 
2.68% 
σ = 40% 
14.33% 
17.72% </p>
<p>a = 1000, b = −1 
σ = 5% 
4.79% 
3.97% 
σ = 40% 
31.24% 
28.39% </p>
<p>a = 1000, b = −2 
σ = 5% 
3.64% 
6.66% 
σ = 40% 
22.61% 
17.95% </p>
<p>Average 
σ = 5% 
3.87% 
3.84% </p>
<p>Average 
σ = 40% 
17.29% 
20.06% </p>
<p>Average 
10.58% 
11.95% </p>
<p>Table 6 .
6The percentage of strategies in the top-10 strategies achieving the least regression model error, belonging to the pool-based approach versus the query synthesis approach, over twelve synthetic datasets, using different noise levels σ. The bold entries represent the maximum percentage per row (over the two active learning approaches).50.00% 
50.00% 
σ = 40% 
30.00% 
70.00% </p>
<p>a = 100, b = −1 
σ = 5% 
50.00% 
50.00% 
σ = 40% 
40.00% 
60.00% </p>
<p>a = 100, b = −2 
σ = 5% 
50.00% 
50.00% 
σ = 40% 
60.00% 
40.00% </p>
<p>a = 1000, b = −0.5 
σ = 5% 
50.00% 
50.00% 
σ = 40% 
70.00% 
30.00% </p>
<p>a = 1000, b = −1 
σ = 5% 
60.00% 
40.00% 
σ = 40% 
20.00% 
80.00% </p>
<p>a = 1000, b = −2 
σ = 5% 
50.00% 
50.00% 
σ = 40% 
50.00% 
50.00% </p>
<p>Average 
σ = 5% 
51.67% 
48.33% </p>
<p>Average 
σ = 40% 
45.00% 
55.00% </p>
<p>Average 
48.33% 
51.67% </p>
<p>Dataset 
Noise Level σ Pool-Based Query Synthesis </p>
<p>a = 100, b = −0.5 
σ = 5% 
30.00% 
70.00% 
σ = 40% 
20.00% 
80.00% </p>
<p>a = 100, b = −1 
σ = 5% 
50.00% 
50.00% 
σ = 40% 
40.00% 
60.00% </p>
<p>a = 100, b = −2 
σ = 5% 
60.00% 
40.00% 
σ = 40% 
40.00% 
60.00% </p>
<p>a = 1000, b = −0.5 
σ = 5% 
70.00% 
30.00% 
σ = 40% 
20.00% 
80.00% </p>
<p>a = 1000, b = −1 
σ = 5% 
40.00% 
60.00% 
σ = 40% 
40.00% 
60.00% </p>
<p>a = 1000, b = −2 
σ = 5% 
20.00% 
80.00% 
σ = 40% 
60.00% 
40.00% </p>
<p>Average 
σ = 5% 
45.00% 
55.00% </p>
<p>Average 
σ = 40% 
36.67% 
63.33% </p>
<p>Average 
40.83% 
59.17% </p>
<p>Table 7 .
7A description for the real-world datasets.Dataset 
Sizeâbσ 
Mean of Prices µ p Standard Deviation of Prices σ p </p>
<p>Transport 
41 
41.3778 −0.1378 3.3902 
145.00 
90.8295 
Beef 
91 
30.0515 −0.0465 0.5670 
250.44 
37.01 
Sugar 
18 
1.3576 
−0.3184 0.0292 
1.005 
0.0824 
Spirits 
69 
4.4651 
−1.2723 0.0573 
2.1184 
0.2089 
Coke 
20 
50.5700 −0.3406 1.9319 
22.96 
3.2376 </p>
<p>Table 8 .
8The revenue gain for the different active learning strategies using the five real datasets. The bold entries represent the maximum revenue gain per column (over all strategies).Active Learning Strategy Transport 
Beef 
Sugar 
Spirits 
Coke </p>
<p>G-Pool 
98.53% 
99.61% 
79.17% 
99.23% 
63.49% 
G-Synth 
98.85% 
99.81% 
99.25% 
99.08% 
98.59% 
EVPI-Pool 
84.65% 
94.28% 
72.31% 
94.32% 
51.90% 
EVPI-Synth 
99.38% 
87.95% 
89.02% 
86.11% 
97.50% 
UCB-Pool 
98.94% 
99.60% 
79.17% 
99.26% 
63.49% 
UCB-Synth 
98.76% 
99.77% 
98.72% 
99.44% 
98.41% 
MI-G-Pool 
98.52% 
99.55% 
79.01% 
99.43% 
63.08% 
MI-G-Synth 
98.62% 
99.04% 
98.58% 
99.26% 
97.14% 
MI-G-Synth-Nopool 
98.75% 
99.50% 
98.73% 
99.23% 
97.27% 
MMI-G-Pool 
98.19% 
99.57% 
79.06% 
99.40% 
63.01% 
MMI-G-Synth 
98.51% 
99.43% 
98.51% 
99.29% 
96.82% 
MMI-G-Synth-Nopool 
98.16% 
99.40% 
98.37% 
99.39% 
97.19% 
KL-G-Pool 
98.70% 
99.22% 
78.79% 
99.32% 
63.05% 
KL-G-Synth 
99.10% 
98.42% 
99.00% 
98.77% 
99.13% 
KL-G-Synth-Nopool 
98.19% 
98.66% 
99.41% 
98.62% 
99.21% 
ME-G-Pool 
98.83% 
99.58% 
79.03% 
98.86% 
63.09% 
ME-G-Synth 
98.88% 
99.47% 
98.59% 
99.35% 
97.33% 
US-G-Pool 
95.53% 
99.74% 
79.15% 
98.18% 
63.46% 
US-G-Synth 
84.86% 
86.34% 
89.95% 
93.42% 
86.53% 
RS-G-Pool 
97.84% 
98.94% 
78.57% 
99.28% 
62.81% 
RS-G-Synth 
95.40% 
97.00% 
97.17% 
96.22% 
96.15% 
UoS-1-Synth 
98.61% 
99.69% 
99.31% 
99.27% 
98.70% 
UoS-2-Synth 
98.01% 
99.70% 
98.72% 
99.50% 
99.15% 
UME-Synth 
98.63% 
99.68% 
93.60% 
99.15% 
99.85% 
UME-Pool 
98.53% 
99.53% 
78.90% 
99.14% 
62.38% 
RS 
78.54% 
93.45% 
71.89% 
94.12% 
52.10% </p>
<p>Table 9 .
9The percentage error of regression model parameters for the different active learning strategies for the real datasets. The bold entries represent the minimum estimation error per column (over all strategies).Active Learning Strategy Transport 
Beef 
Sugar 
Spirits 
Coke </p>
<p>G-Pool 
7.19% 
5.36% 
4.56% 
3.36% 
1.71% 
G-Synth 
9.60% 
3.36% 
1.02% 
6.50% 
1.31% 
EVPI-Pool 
2.24% 
2.26% 
3.62% 
0.43% 
2.34% 
EVPI-Synth 
4.62% 
0.52% 
1.17% 
5.78% 
1.53% 
UCB-Pool 
6.14% 
4.89% 
4.56% 
3.28% 
1.95% 
UCB-Synth 
9.15% 
3.47% 
1.45% 
4.36% 
1.44% 
MI-G-Pool 
7.18% 
4.60% 
3.66% 
3.22% 
1.79% 
MI-G-Synth 
9.76% 
1.87% 
1.16% 
3.98% 
1.89% 
MI-G-Synth-Nopool 
9.34% 
3.47% 
1.41% 
5.17% 
1.07% 
MMI-G-Pool 
10.78% 
4.69% 
4.26% 
2.77% 
1.65% 
MMI-G-Synth 
11.45% 
2.35% 
1.09% 
4.90% 
1.01% 
MMI-G-Synth-Nopool 
11.81% 
4.07% 
1.11% 
3.79% 
1.71% 
KL-G-Pool 
3.95% 
4.87% 
3.37% 
3.42% 
1.94% 
KL-G-Synth 
4.16% 
1.59% 
1.91% 
0.70% 
1.57% 
KL-G-Synth-Nopool 
8.50% 
2.21% 
1.60% 
2.23% 
1.54% 
ME-G-Pool 
5.64% 
4.44% 
3.81% 
4.14% 
1.70% 
ME-G-Synth 
8.84% 
3.42% 
0.94% 
3.41% 
1.81% 
US-G-Pool 
10.23% 
2.87% 
4.12% 
1.48% 
1.86% 
US-G-Synth 
3.17% 
0.82% 
0.50% 
0.77% 
0.85% 
RS-G-Pool 
6.23% 
2.50% 
5.38% 
1.70% 
3.41% 
RS-G-Synth 
2.22% 
1.02% 
1.00% 
0.92% 
1.10% 
UoS-1-Synth 
10.38% 
4.10% 
1.35% 
3.04% 
2.24% 
UoS-2-Synth 
10.86% 
3.39% 
1.50% 
3.57% 
1.05% 
UME-Synth 
9.93% 
3.30% 
0.59% 
4.64% 
1.05% 
UME-Pool 
8.16% 
4.53% 
7.35% 
3.14% 
1.94% 
RS 
1.49% 
1.46% 
2.59% 
1.43% 
2.72% </p>
<p>Table 10 .
10Cont.Active Learning Strategy 
Average Revenue Gain Average Model Percentage Error </p>
<p>MI-G-Pool 
87.92% 
4.09% 
ME-G-Pool 
87.88% 
3.95% 
MMI-G-Pool 
87.85% 
4.83% 
KL-G-Pool 
87.82% 
3.51% 
UME-Pool 
87.70% 
5.02% 
RS-G-Pool 
87.49% 
3.84% 
US-G-Pool 
87.21% 
4.11% 
EVPI-Pool 
79.49% 
2.18% 
RS 
78.02% 
1.94% </p>
<p>Table 12 .
12The average model percentage error of active learning methods versus different number of initial training points N init , averaged over the five considered real datasets. The bold entries represent the minimum average estimation error per row (over the different number of initial training points N init ).Pool 
88.01% 
88.14% 
88.14% 
G-Synth 
99.12% 
99.46% 
99.82% 
EVPI-Pool 
79.49% 
80.87% 
80.06% 
EVPI-Synth 
91.99% 
94.81% 
97.33% 
UCB-Pool 
88.09% 
88.21% 
88.19% 
UCB-Synth 
99.02% 
99.47% 
99.82% 
MI-G-Pool 
87.92% 
87.68% 
87.92% 
MI-G-Synth 
98.53% 
98.85% 
99.21% 
MI-G-Synth-Nopool 
98.70% 
98.92% 
98.96% 
MMI-G-Pool 
87.85% 
87.93% 
87.88% 
MMI-G-Synth 
98.51% 
98.87% 
99.14% 
MMI-G-Synth-Nopool 
98.50% 
98.65% 
99.14% 
KL-G-Pool 
87.82% 
87.72% 
87.65% 
KL-G-Synth 
98.88% 
99.08% 
98.47% 
KL-G-Synth-Nopool 
98.82% 
98.40% 
99.00% 
ME-G-Pool 
87.88% 
88.11% 
87.98% 
ME-G-Synth 
98.72% 
98.95% 
99.31% 
US-G-Pool 
87.21% 
86.74% 
87.52% 
US-G-Synth 
88.22% 
86.88% 
88.33% 
RS-G-Pool 
87.49% 
87.64% 
87.77% 
RS-G-Synth 
96.39% 
95.76% 
97.22% 
UoS-1-Synth 
99.12% 
99.39% 
99.76% 
UoS-2-Synth 
99.02% 
99.12% 
99.30% 
UME-Synth 
98.18% 
98.13% 
99.20% 
UME-Pool 
87.70% 
87.79% 
87.74% 
RS 
78.02% 
79.15% 
79.30% 
Average 
92.66% 
92.87% 
93.24% </p>
<p>Table 13 .Table 14 .Table 15 .
131415The average revenue gain for the pool-based approach versus query synthesis approach, using the five real datasets. The bold entries represent the maximum average revenue gain per row (over the two active learning approaches). The percentage model error for the pool-based versus query synthesis strategies, using five the real datasets. The bold entries represent the minimum average estimation error per row (over the two active learning approaches). The percentage of pool-based versus query synthesis strategies, ranked within the top-10 strategies achieving the highest revenue gain, using the five real datasets. The bold entries represent the maximum percentage per row (over the two active learning approaches).Table 16. The percentage of pool-based versus query synthesis strategies ranked within the top-10 strategies with respect to achieving the least percentage model error, using the five real datasets. The bold entries represent the maximum percentage per row (over the two active learning approaches).Dataset 
Pool-Based Approach 
Query Synthesis Approach </p>
<p>Transport 
96.83% 
97.51% 
Beef 
98.96% 
97.59% 
Sugar 
78.32% 
97.13% 
Spirits 
98.64% 
97.74% 
Coke 
61.98% 
97.26% 
Average 
86.94% 
97.45% </p>
<p>Conflicts of Interest:The authors of this work declare no conflict of interest.
Improving generalization with active learning. D Cohn, L Atlas, R Ladner, 10.1007/BF00993277Mach. Learn. 15Cohn, D.; Atlas, L.; Ladner, R. Improving generalization with active learning. Mach. Learn. 1994, 15, 201-221. [CrossRef]</p>
<p>A Literature Survey of Active Machine Learning in the Context of Natural Language Processing. F Olsson, SwedenSwedish Institute of Computer Science: KistaTechnical ReportOlsson, F. A Literature Survey of Active Machine Learning in the Context of Natural Language Processing; Technical Report; Swedish Institute of Computer Science: Kista, Sweden, 2009.</p>
<p>Support vector machine active learning for image retrieval. S Tong, E Chang, Proceedings of the Ninth ACM International Conference on Multimedia. the Ninth ACM International Conference on MultimediaOttawa, ON, CanadaTong, S.; Chang, E. Support vector machine active learning for image retrieval. In Proceedings of the Ninth ACM International Conference on Multimedia, Ottawa, ON, Canada, 30 September-5 October 2001;</p>
<p>Active Learning Literature Survey. B Settles, Madison, WI, USAUniversity of Wisconsin-Madison Department of Computer SciencesTechnical ReportSettles, B. Active Learning Literature Survey; Technical Report; University of Wisconsin-Madison Department of Computer Sciences: Madison, WI, USA, 2009.</p>
<p>Reinforcement learning: A survey. L P Kaelbling, M L Littman, A W Moore, 10.1613/jair.301J. Artif. Intell. Res. 4Kaelbling, L.P.; Littman, M.L.; Moore, A.W. Reinforcement learning: A survey. J. Artif. Intell. Res. 1996, 4, 237-285. [CrossRef]</p>
<p>Exploration and exploitation in evolutionary algorithms: A survey. M Črepinšek, S H Liu, M Mernik, 10.1145/2480741.2480752ACM Comput. Surv. (CSUR). 45Črepinšek, M.; Liu, S.H.; Mernik, M. Exploration and exploitation in evolutionary algorithms: A survey. ACM Comput. Surv. (CSUR) 2013, 45, 35. [CrossRef]</p>
<p>Exploration/exploitation in adaptive recommender systems. S Ten Hagen, M Van Someren, V Hollink, Proceedings of the European Symposium on Intelligent Technologies, Hybrid Systems and their implementation on Smart Adaptive Systems. the European Symposium on Intelligent Technologies, Hybrid Systems and their implementation on Smart Adaptive SystemsOulu, FinlandTen Hagen, S.; Van Someren, M.; Hollink, V. Exploration/exploitation in adaptive recommender systems. In Proceedings of the European Symposium on Intelligent Technologies, Hybrid Systems and their implementation on Smart Adaptive Systems, Oulu, Finland, 10-12 July 2003.</p>
<p>Dynamic pricing and learning: historical origins, current research, and new directions. A V Den Boer, 10.1016/j.sorms.2015.03.001Surv. Oper. Res. Manag. Sci. 20den Boer, A.V. Dynamic pricing and learning: historical origins, current research, and new directions. Surv. Oper. Res. Manag. Sci. 2015, 20, 1-18. [CrossRef]</p>
<p>Pricing and learning with uncertain demand. M S Lobo, S Boyd, Proceedings of the INFORMS Revenue Management. the INFORMS Revenue ManagementNew York, NY, USALobo, M.S.; Boyd, S. Pricing and learning with uncertain demand. In Proceedings of the INFORMS Revenue Management, New York, NY, USA, 5-6 June 2003.</p>
<p>Analytical solutions to the dynamic pricing problem for time-normalized revenue. M N Ibrahim, A F Atiya, 10.1016/j.ejor.2016.04.012Eur. J. Oper. Res. 254Ibrahim, M.N.; Atiya, A.F. Analytical solutions to the dynamic pricing problem for time-normalized revenue. Eur. J. Oper. Res. 2016, 254, 632-643. [CrossRef]</p>
<p>A framework for an agent-based dynamic pricing for broadband wireless price rate plans. D Elreedy, A F Atiya, H Fayed, M Saleh, 10.1080/17477778.2017.1418642J. Simul. 13Elreedy, D.; Atiya, A.F.; Fayed, H.; Saleh, M. A framework for an agent-based dynamic pricing for broadband wireless price rate plans. J. Simul. 2019, 13, 1-15. [CrossRef]</p>
<p>Maximizing expected model change for active learning in regression. W Cai, Y Zhang, J Zhou, Proceedings of the IEEE 13th International Conference on Data Mining. the IEEE 13th International Conference on Data MiningDallas, TX, USACai, W.; Zhang, Y.; Zhou, J. Maximizing expected model change for active learning in regression. In Proceedings of the IEEE 13th International Conference on Data Mining, Dallas, TX, USA, 7-10 December 2013, pp. 51-60.</p>
<p>Active learning for regression based on query by committee. R Burbidge, J J Rowland, R D King, Proceedings of the International Conference on Intelligent Data Engineering and Automated Learning. the International Conference on Intelligent Data Engineering and Automated LearningBerlin, GermanySpringerBurbidge, R.; Rowland, J.J.; King, R.D. Active learning for regression based on query by committee. In Proceedings of the International Conference on Intelligent Data Engineering and Automated Learning; Springer: Berlin, Germany, 2007, pp. 209-218.</p>
<p>Pool-Based Sequential Active Learning for Regression. D Wu, arXiv:1805.04735Wu, D. Pool-Based Sequential Active Learning for Regression. arXiv 2018, arXiv:1805.04735.</p>
<p>A Sequential Algorithm for Training Text Classifiers. D Lewis, W Gale, Proceedings of the 17th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 17th International ACM SIGIR Conference on Research and Development in Information RetrievalDublin, IrelandLewis, D.; Gale, W. A Sequential Algorithm for Training Text Classifiers. In Proceedings of the 17th International ACM SIGIR Conference on Research and Development in Information Retrieval, Dublin, Ireland, 3-6 July 1994.</p>
<p>Active learning with statistical models. D A Cohn, Z Ghahramani, M I Jordan, 10.1613/jair.295J. Artif. Intell. Res. 4Cohn, D.A.; Ghahramani, Z.; Jordan, M.I. Active learning with statistical models. J. Artif. Intell. Res. 1996, 4, 129-145. [CrossRef]</p>
<p>Optimistic Active-Learning Using Mutual Information. Y Guo, R Greiner, Proceedings of the IJCAI. the IJCAIHyderabad, India7Guo, Y.; Greiner, R. Optimistic Active-Learning Using Mutual Information. In Proceedings of the IJCAI, Hyderabad, India, 6-12 January 2007; Volume 7, pp. 823-829.</p>
<p>Selective sampling using the query by committee algorithm. Y Freund, H S Seung, E Shamir, N Tishby, 10.1023/A:1007330508534Mach. Learn. 28Freund, Y.; Seung, H.S.; Shamir, E.; Tishby, N. Selective sampling using the query by committee algorithm. Mach. Learn. 1997, 28, 133-168. [CrossRef]</p>
<p>Toward optimal active learning through monte carlo estimation of error reduction. N Roy, A Mccallum, Proceedings of the ICML. the ICMLWilliamstown, MA, USARoy, N.; McCallum, A. Toward optimal active learning through monte carlo estimation of error reduction. In Proceedings of the ICML, Williamstown, MA, USA, 28 June-1 July 2001; pp. 441-448.</p>
<p>Active learning for regression using greedy sampling. D Wu, C T Lin, J Huang, 10.1016/j.ins.2018.09.060Inf. Sci. 474Wu, D.; Lin, C.T.; Huang, J. Active learning for regression using greedy sampling. Inf. Sci. 2019, 474, 90-105. [CrossRef]</p>
<p>Active learning in approximately linear regression based on conditional expectation of generalization error. M Sugiyama, J. Mach. Learn. Res. 7Sugiyama, M. Active learning in approximately linear regression based on conditional expectation of generalization error. J. Mach. Learn. Res. 2006, 7, 141-166.</p>
<p>Entropy-based active learning for object recognition. A Holub, P Perona, M C Burl, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Computer Society Conference on Computer Vision and Pattern Recognition WorkshopsAnchorage, AK, USAHolub, A.; Perona, P.; Burl, M.C. Entropy-based active learning for object recognition. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, Anchorage, AK, USA, 23-28 June 2008; pp. 1-8.</p>
<p>Classification active learning based on mutual information. J Sourati, M Akcakaya, J G Dy, T K Leen, D Erdogmus, 10.3390/e18020051Entropy. 1851Sourati, J.; Akcakaya, M.; Dy, J.G.; Leen, T.K.; Erdogmus, D. Classification active learning based on mutual information. Entropy 2016, 18, 51. [CrossRef]</p>
<p>A probabilistic active learning algorithm based on fisher information ratio. J Sourati, M Akcakaya, D Erdogmus, T K Leen, J G Dy, 10.1109/TPAMI.2017.2743707IEEE Trans. Pattern Anal. Mach. Intell. 40PubMedSourati, J.; Akcakaya, M.; Erdogmus, D.; Leen, T.K.; Dy, J.G. A probabilistic active learning algorithm based on fisher information ratio. IEEE Trans. Pattern Anal. Mach. Intell. 2018, 40, 2023-2029. [CrossRef] [PubMed]</p>
<p>Efficient Active Learning of Halfspaces via Query Synthesis. I M Alabdulmohsin, X Gao, X Zhang, Proceedings of the AAAI Twenty-Ninth AAAI Conference on Artificial Intelligence. the AAAI Twenty-Ninth AAAI Conference on Artificial IntelligenceHyatt Regency, AustinAlabdulmohsin, I.M.; Gao, X.; Zhang, X. Efficient Active Learning of Halfspaces via Query Synthesis. In Proceedings of the AAAI Twenty-Ninth AAAI Conference on Artificial Intelligence, Hyatt Regency, Austin, 25-29 January 2015; pp. 2483-2489.</p>
<p>Queries and concept learning. D Angluin, 10.1007/BF00116828Mach. Learn. 2Angluin, D. Queries and concept learning. Mach. Learn. 1988, 2, 319-342. [CrossRef]</p>
<p>Decision-centric active learning of binary-outcome models. M Saar-Tsechansky, F Provost, 10.1287/isre.1070.0111Inf. Syst. Res. 18Saar-Tsechansky, M.; Provost, F. Decision-centric active learning of binary-outcome models. Inf. Syst. Res. 2007, 18, 4-22. [CrossRef]</p>
<p>Bayesian optimal active search and surveying. R Garnett, Y Krishnamurthy, X Xiong, J Schneider, R Mann, arXiv:1206.6406Garnett, R.; Krishnamurthy, Y.; Xiong, X.; Schneider, J.; Mann, R. Bayesian optimal active search and surveying. arXiv 2012, arXiv:1206.6406.</p>
<p>ε-pal: An active learning approach to the multi-objective optimization problem. M Zuluaga, A Krause, M Püschel, J. Mach. Learn. Res. 17Zuluaga, M.; Krause, A.; Püschel, M. ε-pal: An active learning approach to the multi-objective optimization problem. J. Mach. Learn. Res. 2016, 17, 3619-3650.</p>
<p>A knowledge-gradient policy for sequential information collection. P I Frazier, W B Powell, S Dayanik, 10.1137/070693424SIAM J. Control Optim. 47Frazier, P.I.; Powell, W.B.; Dayanik, S. A knowledge-gradient policy for sequential information collection. SIAM J. Control Optim. 2008, 47, 2410-2439. [CrossRef]</p>
<p>Quantifying the objective cost of uncertainty in complex dynamical systems. B J Yoon, X Qian, E R Dougherty, 10.1109/TSP.2013.2251336IEEE Trans. Signal Process. 61Yoon, B.J.; Qian, X.; Dougherty, E.R. Quantifying the objective cost of uncertainty in complex dynamical systems. IEEE Trans. Signal Process. 2013, 61, 2256-2266. [CrossRef]</p>
<p>Optimal experimental design for gene regulatory networks in the presence of uncertainty. R Dehghannasiri, B J Yoon, E R Dougherty, 10.1109/TCBB.2014.2377733IEEE/ACM Trans. Comput. Biol. Bioinform. 12TCBBDehghannasiri, R.; Yoon, B.J.; Dougherty, E.R. Optimal experimental design for gene regulatory networks in the presence of uncertainty. IEEE/ACM Trans. Comput. Biol. Bioinform. (TCBB) 2015, 12, 938-950. [CrossRef]</p>
<p>Multi-armed bandit algorithms and empirical evaluation. J Vermorel, M Mohri, Proceedings of the European Conference on Machine Learning. the European Conference on Machine LearningBerlin, GermanySpringerVermorel, J.; Mohri, M. Multi-armed bandit algorithms and empirical evaluation. In Proceedings of the European Conference on Machine Learning; Springer: Berlin, Germany, 2005; pp. 437-448.</p>
<p>Nonmyopic active learning of gaussian processes: An exploration-exploitation approach. A Krause, C Guestrin, Proceedings of the 24th International Conference on Machine Learning. the 24th International Conference on Machine LearningCorvalis, OR, USAKrause, A.; Guestrin, C. Nonmyopic active learning of gaussian processes: An exploration-exploitation approach. In Proceedings of the 24th International Conference on Machine Learning, Corvalis, OR, USA, 20-24 June 2007; pp. 449-456.</p>
<p>Some aspects of the sequential design of experiments. H Robbins, Herbert Robbins Selected Papers. Berlin, GermanySpringerRobbins, H. Some aspects of the sequential design of experiments. In Herbert Robbins Selected Papers; Springer: Berlin, Germany, 1985; pp. 169-177.</p>
<p>Using confidence bounds for exploitation-exploration trade-offs. P Auer, J. Mach. Learn. Res. 3Auer, P. Using confidence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res. 2002, 3, 397-422.</p>
<p>A bias and variance analysis for multistep-ahead time series forecasting. S Ben Taieb, A F Atiya, 10.1109/TNNLS.2015.2411629IEEE Trans. Neural Netw. Learn. Syst. 27PubMedBen Taieb, S.; Atiya, A.F. A bias and variance analysis for multistep-ahead time series forecasting. IEEE Trans. Neural Netw. Learn. Syst. 2016, 27, 62-76. [CrossRef] [PubMed]</p>
<p>Regression analysis for prediction of residential energy consumption. N Fumo, M R Biswas, 10.1016/j.rser.2015.03.035Renew. Sustain. Energy Rev. 47Fumo, N.; Biswas, M.R. Regression analysis for prediction of residential energy consumption. Renew. Sustain. Energy Rev. 2015, 47, 332-343. [CrossRef]</p>
<p>Dynamic pricing for hotel revenue management using price multipliers. A E M Bayoumi, M Saleh, A F Atiya, H A Aziz, 10.1057/rpm.2012.44J. Revenue Pric. Manag. 12Bayoumi, A.E.M.; Saleh, M.; Atiya, A.F.; Aziz, H.A. Dynamic pricing for hotel revenue management using price multipliers. J. Revenue Pric. Manag. 2013, 12, 271-285. [CrossRef]</p>
<p>A tutorial on bayesian normal linear regression. K Klauenberg, G Wübbeler, B Mickan, P Harris, C Elster, 10.1088/0026-1394/52/6/87852Klauenberg, K.; Wübbeler, G.; Mickan, B.; Harris, P.; Elster, C. A tutorial on bayesian normal linear regression. Metrologia 2015, 52, 878. [CrossRef]</p>
<p>. S Kotz, S Nadarajah, Their Multivariate T-Distributions, Applications, Cambridge University PressCambridge, UKKotz, S.; Nadarajah, S. Multivariate T-Distributions and Their Applications; Cambridge University Press: Cambridge, UK, 2004.</p>
<p>Kendall's Advanced Theory of Statistics. A O&apos;hagan, J J Forster, Bayesian Inference228O'Hagan, A.; Forster, J.J. Kendall's Advanced Theory of Statistics, Volume 2B: Bayesian Inference. Available online: https://eprints.soton.ac.uk/46376/ (accessed on 28 June 2019).</p>
<p>A mathematical theory of communication. C E Shannon, 10.1002/j.1538-7305.1948.tb01338.xBell Syst. Tech. J. 27Shannon, C.E. A mathematical theory of communication. Bell Syst. Tech. J. 1948, 27, 379-423. [CrossRef]</p>
<p>. S Kullback, Kullback-Leibler Distance, Am. Stat. 41Kullback, S. Kullback-Leibler distance. Am. Stat. 1987, 41, 340-341.</p>
<p>Flexible Bayesian analysis of the von Bertalanffy growth function with the use of a log-skew-t distribution. F O Lopez Quintero, J E Contreras-Reyes, R Wiff, R B Arellano-Valle, 10.7755/FB.115.1.2115Lopez Quintero, F.O.; Contreras-Reyes, J.E.; Wiff, R.; Arellano-Valle, R.B. Flexible Bayesian analysis of the von Bertalanffy growth function with the use of a log-skew-t distribution. Fish. Bull. 2017, 115, 13-26. [CrossRef]</p>
<p>Relative entropy in biological systems. J Baez, B Pollard, 10.3390/e18020046Entropy. 18Baez, J.; Pollard, B. Relative entropy in biological systems. Entropy 2016, 18, 46. [CrossRef]</p>
<p>Entropy Analysis of Monetary Unions. M Mata, J Machado, 10.3390/e1906024519245Mata, M.; Machado, J. Entropy Analysis of Monetary Unions. Entropy 2017, 19, 245. [CrossRef]</p>
<p>Active Learning of Hyperparameters: An Expected Cross Entropy Criterion for Active Model Selection. J Kulick, R Lieck, M Toussaint, 26Kulick, J.; Lieck, R.; Toussaint, M. Active Learning of Hyperparameters: An Expected Cross Entropy Criterion for Active Model Selection. Available online: https://ipvs.informatik.uni-stuttgart.de/mlr/papers/14- kulick-maxce.pdf (accessed on 26 September 2014).</p>
<p>Artificial Intelligence: A Modern Approach. S J Russell, P Norvig, Pearson Education LimitedKuala Lumpur, MalaysiaRussell, S.J.; Norvig, P. Artificial Intelligence: A Modern Approach; Pearson Education Limited: Kuala Lumpur, Malaysia, 2016.</p>
<p>Simulated annealing. P J Van Laarhoven, E H Aarts, Simulated Annealing: Theory and Applications. Berlin, GermanySpringerVan Laarhoven, P.J.; Aarts, E.H. Simulated annealing. In Simulated Annealing: Theory and Applications; Springer: Berlin, Germany, 1987; pp. 7-15.</p>
<p>Algorithms for multi-armed bandit problems. V Kuleshov, D Precup, arXiv:1402.6028arXiv preprintKuleshov, V.; Precup, D. Algorithms for multi-armed bandit problems. arXiv preprint 2014, arXiv:1402.6028.</p>
<p>Introduction to Reinforcement Learning. R S Sutton, A G Barto, MIT Press135Cambridge, UKSutton, R.S.; Barto, A.G. Introduction to Reinforcement Learning; MIT Press: Cambridge, UK, 1998; Volume 135.</p>
<p>Electronic Data Archive, Red Meats Yearbook, housed at Cornell University's Mann Library. Musda-Ers, 25MUSDA-ERS Electronic Data Archive, Red Meats Yearbook, housed at Cornell University's Mann Library. Available online: http://usda.mannlib.cornell.edu/ (accessed on 25 September 2001).</p>
<p>A comparison of elasticities of demand obtained by different methods. H Schultz, 10.2307/1907041Econ. J. Econ. Soc. 1Schultz, H. A comparison of elasticities of demand obtained by different methods. Econ. J. Econ. Soc. 1933, 1, 274-308. [CrossRef]</p>
<p>Testing for serial correlation in least squares regression: I. Biometrika. J Durbin, G S Watson, 37PubMedDurbin, J.; Watson, G.S. Testing for serial correlation in least squares regression: I. Biometrika 1950, 37, 409-428. [PubMed]</p>
<p>Available online: leeds-faculty. Y Sun, 23Coke Demand Estimation DatasetSun, Y. Coke Demand Estimation Dataset. Available online: leeds-faculty.colorado.edu/ysun/doc/Demand_ estimation_worksheet.doc (accessed on 23 February 2010).</p>
<p>This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license. Licensee MDPI. c 2019 by the authorsc 2019 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).</p>            </div>
        </div>

    </div>
</body>
</html>
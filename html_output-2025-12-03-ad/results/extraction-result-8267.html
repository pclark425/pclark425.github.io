<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8267 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8267</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8267</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-d448add3949504e0599357b22a97f6a1a636f9bb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d448add3949504e0599357b22a97f6a1a636f9bb" target="_blank">Self-supervised Analogical Learning using Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes SAL, a self-supervised analogical learning framework that mimics the human analogy process and trains models to explicitly transfer high-quality symbolic solutions from cases that they know how to solve to other rare cases in which they tend to fail more.</p>
                <p><strong>Paper Abstract:</strong> Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can successfully solve. Such observations motivate us to propose methods that encourage models to understand the high-level and abstract reasoning processes during training instead of only the final answer. This way, models can transfer the exact solution to similar cases, regardless of their relevance to the pre-training data distribution. In this work, we propose SAL, a self-supervised analogical learning framework. SAL mimics the human analogy process and trains models to explicitly transfer high-quality symbolic solutions from cases that they know how to solve to other rare cases in which they tend to fail more. We show that the resulting models after SAL learning outperform base language models on a wide range of reasoning benchmarks, such as StrategyQA, GSM8K, and HotpotQA, by 2% to 20%. At the same time, we show that our model is more generalizable and controllable through analytical studies.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8267.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8267.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAL (all)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-supervised Analogical Learning (full system: conceptualization + simplification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised training framework that extracts programmatic (Python) symbolic solutions from similar or simplified questions (via conceptualization and simplification) and fine-tunes a base LLM so it learns abstract, transferable reasoning processes instead of relying on surface tokens or delegating to external LM queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (fine-tuned with SAL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mistral-7B base LLM fine-tuned with LoRA (r=32, lr=2e-4) on seed supervision plus SAL-extracted self-supervision instances (conceptualization on StrategyQA/HotpotQA and simplification on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Analogical reasoning (conceptualization)', 'Decomposition / iterative simplification', 'Programmatic reasoning (generate-and-execute Python programs)', 'Chain-of-thought (used during extraction and as auxiliary supervision)', 'Retrieval-augmented generation (optional second-pass using ask_llm answers)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Conceptualization: map a question to an abstract form and generate many concrete similar questions; select high-confidence similar questions and generate programmatic solutions (Python) from their CoT outputs to use as supervision. Simplification: iteratively decompose math questions into single-hop next-questions, solve and accumulate known conditions, then synthesize global programs. Programs are executed to verify answers; retrieval-augmentation answers ask_llm (external factual) queries and conditions are injected into program generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared variants: CoT few-shot baseline, Seed-only (instruction-tuned multifunction model trained on seed supervision), SAL (conceptual only), SAL (all = conceptual + simplification), and SAL (all) + RAG. Ablation removing programs that internally query the LM with a paraphrase of the original question (paraphrase threshold 0.9) tests reliance on simple delegation vs deeper decomposition/abduction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>StrategyQA, GSM8K, ARC, CommonsenseQA, HotpotQA (binary-transformed multiple-choice where necessary). Also a conceptual reasoning benchmark from Zhou et al. (2024).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 2 (main): On Mistral-7B base: StrategyQA-seen 61.0, StrategyQA 59.7, GSM8K-seen 49.4, GSM8K 50.8, ARC 71.0, CsQA 74.7, HotpotQA 52.4. With Retrieval Augmentation (RAG): StrategyQA-seen 63.7, StrategyQA 62.9, GSM8K-seen 49.4, GSM8K 50.8, ARC 73.7, CsQA 77.0, HotpotQA 55.1. Improvements over CoT few-shot: +6.8% on StrategyQA, +5.8% on GSM8K, +4.8% on HotpotQA (absolute points reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>SAL-trained models produce more interpretable programmatic solutions and show improved consistency across questions sharing identical high-level processes; they are less likely to simply repeat/delegate the original question to ask_llm (ablation: when removing programs that query with paraphrase of original, SAL drops ~3% vs Seed-only drops ~11% on StrategyQA). SAL yields better domain transfer (gains on ARC and CommonsenseQA) and more controllable inference via program execution.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Training on self-supervised analogical programmatic signals (conceptualization + simplification) makes base LLMs attend to abstract high-level solution processes, improving consistency and accuracy across diverse and rare cases; SAL reduces reliance on surface token familiarity and on naïve delegation to external LM queries, and combining with RAG further improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-supervised Analogical Learning using Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8267.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8267.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAL (concept-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAL (conceptualization-only variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SAL variant that uses only the conceptualization extraction pipeline: map questions to abstracts, generate many similar concrete questions, collect high-confidence CoTs and produce programmatic solutions to supervise the base LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (fine-tuned with conceptualization self-supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mistral-7B fine-tuned on seed supervision plus conceptualization-extracted program supervision instances (no simplification).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Analogical reasoning (conceptualization)', 'Programmatic reasoning (Python programs)', 'Chain-of-thought (used to get silver labels and guide program generation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate an abstract form abs(q), sample N=20 similar concrete questions per abstract, collect K=10 CoT outputs per similar question and require X=9 agreement to select high-confidence similar questions; generate programs from high-confidence CoTs and select programs using execution accuracy thresholds and paraphrase/complexity/soundness filters to supervise original and abstract questions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared concept-only SAL vs Seed-only and CoT few-shot in Table 2; concept-only shows gains primarily on StrategyQA and HotpotQA (improvements reported in Table 2). Same selection hyperparameters N=20, K=10, X=9 used.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>StrategyQA, HotpotQA (conceptual and general reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 2: SAL (concept. only): StrategyQA-seen 61.1, StrategyQA 58.8, GSM8K-seen 44.7, GSM8K 43.7, ARC 68.3, CsQA 72.0, HotpotQA 51.5. Improvements over Seed-only and CoT baselines were modest but present on conceptual reasoning splits (e.g., +3.5% on StrategyQA seen).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Conceptualization produces programmatic targets that encourage models to produce abstract, generalizable solutions; improvements are most visible on conceptual reasoning splits where abstracted scenarios are central.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Conceptual self-supervision alone helps models learn high-level, abstract reasoning routes that transfer to unseen instances sharing the same solution process, improving consistency and accuracy on conceptual benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-supervised Analogical Learning using Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8267.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8267.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seed Only</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B + seed supervision (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline model obtained by instruction-tuning a multifunction model with a curated small set of seed supervision instances mapping questions to CoTs, abstractions, programs, decompositions, etc.; used as a supervised baseline without SAL self-supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (instruction-tuned on seed tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mistral-7B trained on ~4619 seed instances that provide mappings for sub-tasks (q->CoT, q->abs, q->program, qc2p, math decomposition subtasks, etc.) to build a multifunction model able to perform the pipeline subcomponents.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-thought (supervised)', 'Programmatic reasoning (learned from seed programs)', 'Delegation via ask_llm helper in generated programs (frequent)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The multifunction seed-trained model is supervised on subtask templates (cot(q), abs(q), q2p(q), qc2p(q,cot), math_q2nq, etc.) so it can produce CoTs, abstractions, programs and decompositions; however, many generated programs internally query ask_llm with questions very similar to the original, effectively delegating reasoning to an external LM.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared to SAL and CoT baselines in main experiments (Table 2). Ablation: removing programs that contain ask_llm queries paraphrasing the original question causes Seed-only to drop sharply (11% on StrategyQA), indicating heavy reliance on repeating/delegating reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>StrategyQA, GSM8K, ARC, CommonsenseQA, HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 2: Seed Only: StrategyQA-seen 57.6, StrategyQA 56.1, GSM8K-seen 45.4, GSM8K 45.0, ARC 68.3, CsQA 73.5, HotpotQA 50.3. Ablation Table 3 (after removing paraphrase-ask_llm programs): Seed Only falls to 45.8 on StrategyQA (drop ~11 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Seed-only model often constructs programs that delegate sub-questions back to the LM (ask_llm) by reasking nearly the same question; this delegation makes it brittle when such delegation is disallowed and indicates less internalized abstract reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Instruction on seed sub-tasks creates a multifunction model that can produce programs and CoTs, but without analogical self-supervision the model tends to rely on repeating/delegating the original question (surface shortcuts) rather than forming robust abstract reasoning processes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-supervised Analogical Learning using Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8267.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8267.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT few-shot (self-consistency@10)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought few-shot prompting with self-consistency (10 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard few-shot chain-of-thought prompting baseline using an instruction-tuned LLM (Mistral-7B-Instruct-v0.1) with self-consistency aggregation over 10 sampled CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.1 (few-shot CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned Mistral-7B used in a few-shot chain-of-thought prompting setup with self-consistency@10 to aggregate multiple sampled reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-thought prompting', 'Self-consistency sampling/aggregation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Few-shot CoT prompts produce multiple sampled reasoning chains; final answer picked via majority-vote or self-consistency over K=10 CoTs. No additional self-supervised training on programmatic abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as a primary baseline in Table 2; compared directly against SAL variants and the Seed-only baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>StrategyQA, GSM8K, ARC, CommonsenseQA, HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 2: CoT (few-shot) results with Mistral-7B-Instruct-v0.1: StrategyQA-seen 58.8, StrategyQA 58.8, GSM8K-seen 44.6, GSM8K 39.2, ARC 70.5, CsQA 62.2, HotpotQA 32.7.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT few-shot provides reasonable baseline reasoning performance but is outperformed by SAL variants that learned abstract programmatic solutions through self-supervision; CoT alone can be inconsistent across variations that share the same high-level solution process.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Standard CoT few-shot provides a competitive baseline, but fine-tuning with analogical programmatic self-supervision yields superior consistency and accuracy on multiple reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-supervised Analogical Learning using Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8267.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8267.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (SAL + RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation applied to SAL (two-pass with ask_llm answers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmentation step applied to SAL: first-pass generated programs' ask_llm queries are answered (short paragraphs) and those answers are fed into program generation to produce second-pass programs, improving feasibility and final program quality for cases with complex factual factors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (SAL fine-tuned) + RAG</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SAL-trained Mistral-7B that uses an extra retrieval/answering pass for ask_llm queries contained in generated programs; second-pass program synthesis uses those retrieved answers as context.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Analogical reasoning (SAL)', 'Programmatic reasoning with external factual grounding (RAG)', 'Chain-of-thought (used in generation/verification)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Collect all ask_llm queries embedded in first-pass programs, answer them with short paragraphs (retrieval/QA), then invoke qc2p(q, ans) to generate second-pass programs incorporating factual answers; when first-pass voted answer is 'unknown', use retrieval-augmented programs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared SAL(all) with and without RAG in Table 2; RAG consistently improves absolute performance across benchmarks (notably StrategyQA, ARC, HotpotQA).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>StrategyQA, GSM8K, ARC, CommonsenseQA, HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 2: SAL (all) + RAG yields StrategyQA-seen 63.7, StrategyQA 62.9, GSM8K 50.8 (GSM values same as SAL(all)), ARC 73.7, CsQA 77.0, HotpotQA 55.1 — improvements over SAL(all) (e.g., +2.7 pts on StrategyQA-seen over SAL(all)).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>RAG reduces program search space by resolving factual ambiguities and leads to more single-program solutions for complex questions; combined with SAL's abstract programs, RAG yields best performance in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Adding a retrieval-augmentation step to SAL helps handle complex, fact-dependent questions by grounding programmatic reasoning, further improving performance and making program generation more feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-supervised Analogical Learning using Language Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual and unbiased reasoning in language models <em>(Rating: 2)</em></li>
                <li>Large language models as analogical reasoners <em>(Rating: 2)</em></li>
                <li>In-context analogical reasoning with pre-trained language models <em>(Rating: 1)</em></li>
                <li>Faithful chain-of-thought reasoning <em>(Rating: 1)</em></li>
                <li>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8267",
    "paper_id": "paper-d448add3949504e0599357b22a97f6a1a636f9bb",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "SAL (all)",
            "name_full": "Self-supervised Analogical Learning (full system: conceptualization + simplification)",
            "brief_description": "A self-supervised training framework that extracts programmatic (Python) symbolic solutions from similar or simplified questions (via conceptualization and simplification) and fine-tunes a base LLM so it learns abstract, transferable reasoning processes instead of relying on surface tokens or delegating to external LM queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (fine-tuned with SAL)",
            "model_description": "Mistral-7B base LLM fine-tuned with LoRA (r=32, lr=2e-4) on seed supervision plus SAL-extracted self-supervision instances (conceptualization on StrategyQA/HotpotQA and simplification on GSM8K).",
            "reasoning_methods": [
                "Analogical reasoning (conceptualization)",
                "Decomposition / iterative simplification",
                "Programmatic reasoning (generate-and-execute Python programs)",
                "Chain-of-thought (used during extraction and as auxiliary supervision)",
                "Retrieval-augmented generation (optional second-pass using ask_llm answers)"
            ],
            "reasoning_methods_description": "Conceptualization: map a question to an abstract form and generate many concrete similar questions; select high-confidence similar questions and generate programmatic solutions (Python) from their CoT outputs to use as supervision. Simplification: iteratively decompose math questions into single-hop next-questions, solve and accumulate known conditions, then synthesize global programs. Programs are executed to verify answers; retrieval-augmentation answers ask_llm (external factual) queries and conditions are injected into program generation.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared variants: CoT few-shot baseline, Seed-only (instruction-tuned multifunction model trained on seed supervision), SAL (conceptual only), SAL (all = conceptual + simplification), and SAL (all) + RAG. Ablation removing programs that internally query the LM with a paraphrase of the original question (paraphrase threshold 0.9) tests reliance on simple delegation vs deeper decomposition/abduction.",
            "task_or_benchmark": "StrategyQA, GSM8K, ARC, CommonsenseQA, HotpotQA (binary-transformed multiple-choice where necessary). Also a conceptual reasoning benchmark from Zhou et al. (2024).",
            "performance_results": "Table 2 (main): On Mistral-7B base: StrategyQA-seen 61.0, StrategyQA 59.7, GSM8K-seen 49.4, GSM8K 50.8, ARC 71.0, CsQA 74.7, HotpotQA 52.4. With Retrieval Augmentation (RAG): StrategyQA-seen 63.7, StrategyQA 62.9, GSM8K-seen 49.4, GSM8K 50.8, ARC 73.7, CsQA 77.0, HotpotQA 55.1. Improvements over CoT few-shot: +6.8% on StrategyQA, +5.8% on GSM8K, +4.8% on HotpotQA (absolute points reported in paper).",
            "qualitative_findings": "SAL-trained models produce more interpretable programmatic solutions and show improved consistency across questions sharing identical high-level processes; they are less likely to simply repeat/delegate the original question to ask_llm (ablation: when removing programs that query with paraphrase of original, SAL drops ~3% vs Seed-only drops ~11% on StrategyQA). SAL yields better domain transfer (gains on ARC and CommonsenseQA) and more controllable inference via program execution.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Training on self-supervised analogical programmatic signals (conceptualization + simplification) makes base LLMs attend to abstract high-level solution processes, improving consistency and accuracy across diverse and rare cases; SAL reduces reliance on surface token familiarity and on naïve delegation to external LM queries, and combining with RAG further improves results.",
            "uuid": "e8267.0",
            "source_info": {
                "paper_title": "Self-supervised Analogical Learning using Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SAL (concept-only)",
            "name_full": "SAL (conceptualization-only variant)",
            "brief_description": "A SAL variant that uses only the conceptualization extraction pipeline: map questions to abstracts, generate many similar concrete questions, collect high-confidence CoTs and produce programmatic solutions to supervise the base LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (fine-tuned with conceptualization self-supervision)",
            "model_description": "Mistral-7B fine-tuned on seed supervision plus conceptualization-extracted program supervision instances (no simplification).",
            "reasoning_methods": [
                "Analogical reasoning (conceptualization)",
                "Programmatic reasoning (Python programs)",
                "Chain-of-thought (used to get silver labels and guide program generation)"
            ],
            "reasoning_methods_description": "Generate an abstract form abs(q), sample N=20 similar concrete questions per abstract, collect K=10 CoT outputs per similar question and require X=9 agreement to select high-confidence similar questions; generate programs from high-confidence CoTs and select programs using execution accuracy thresholds and paraphrase/complexity/soundness filters to supervise original and abstract questions.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared concept-only SAL vs Seed-only and CoT few-shot in Table 2; concept-only shows gains primarily on StrategyQA and HotpotQA (improvements reported in Table 2). Same selection hyperparameters N=20, K=10, X=9 used.",
            "task_or_benchmark": "StrategyQA, HotpotQA (conceptual and general reasoning tasks)",
            "performance_results": "Table 2: SAL (concept. only): StrategyQA-seen 61.1, StrategyQA 58.8, GSM8K-seen 44.7, GSM8K 43.7, ARC 68.3, CsQA 72.0, HotpotQA 51.5. Improvements over Seed-only and CoT baselines were modest but present on conceptual reasoning splits (e.g., +3.5% on StrategyQA seen).",
            "qualitative_findings": "Conceptualization produces programmatic targets that encourage models to produce abstract, generalizable solutions; improvements are most visible on conceptual reasoning splits where abstracted scenarios are central.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Conceptual self-supervision alone helps models learn high-level, abstract reasoning routes that transfer to unseen instances sharing the same solution process, improving consistency and accuracy on conceptual benchmarks.",
            "uuid": "e8267.1",
            "source_info": {
                "paper_title": "Self-supervised Analogical Learning using Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Seed Only",
            "name_full": "Mistral-7B + seed supervision (baseline)",
            "brief_description": "Baseline model obtained by instruction-tuning a multifunction model with a curated small set of seed supervision instances mapping questions to CoTs, abstractions, programs, decompositions, etc.; used as a supervised baseline without SAL self-supervision.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (instruction-tuned on seed tasks)",
            "model_description": "Mistral-7B trained on ~4619 seed instances that provide mappings for sub-tasks (q-&gt;CoT, q-&gt;abs, q-&gt;program, qc2p, math decomposition subtasks, etc.) to build a multifunction model able to perform the pipeline subcomponents.",
            "reasoning_methods": [
                "Chain-of-thought (supervised)",
                "Programmatic reasoning (learned from seed programs)",
                "Delegation via ask_llm helper in generated programs (frequent)"
            ],
            "reasoning_methods_description": "The multifunction seed-trained model is supervised on subtask templates (cot(q), abs(q), q2p(q), qc2p(q,cot), math_q2nq, etc.) so it can produce CoTs, abstractions, programs and decompositions; however, many generated programs internally query ask_llm with questions very similar to the original, effectively delegating reasoning to an external LM.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Compared to SAL and CoT baselines in main experiments (Table 2). Ablation: removing programs that contain ask_llm queries paraphrasing the original question causes Seed-only to drop sharply (11% on StrategyQA), indicating heavy reliance on repeating/delegating reasoning.",
            "task_or_benchmark": "StrategyQA, GSM8K, ARC, CommonsenseQA, HotpotQA",
            "performance_results": "Table 2: Seed Only: StrategyQA-seen 57.6, StrategyQA 56.1, GSM8K-seen 45.4, GSM8K 45.0, ARC 68.3, CsQA 73.5, HotpotQA 50.3. Ablation Table 3 (after removing paraphrase-ask_llm programs): Seed Only falls to 45.8 on StrategyQA (drop ~11 percentage points).",
            "qualitative_findings": "Seed-only model often constructs programs that delegate sub-questions back to the LM (ask_llm) by reasking nearly the same question; this delegation makes it brittle when such delegation is disallowed and indicates less internalized abstract reasoning.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Instruction on seed sub-tasks creates a multifunction model that can produce programs and CoTs, but without analogical self-supervision the model tends to rely on repeating/delegating the original question (surface shortcuts) rather than forming robust abstract reasoning processes.",
            "uuid": "e8267.2",
            "source_info": {
                "paper_title": "Self-supervised Analogical Learning using Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CoT few-shot (self-consistency@10)",
            "name_full": "Chain-of-Thought few-shot prompting with self-consistency (10 samples)",
            "brief_description": "A standard few-shot chain-of-thought prompting baseline using an instruction-tuned LLM (Mistral-7B-Instruct-v0.1) with self-consistency aggregation over 10 sampled CoTs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.1 (few-shot CoT)",
            "model_description": "Instruction-tuned Mistral-7B used in a few-shot chain-of-thought prompting setup with self-consistency@10 to aggregate multiple sampled reasoning chains.",
            "reasoning_methods": [
                "Chain-of-thought prompting",
                "Self-consistency sampling/aggregation"
            ],
            "reasoning_methods_description": "Few-shot CoT prompts produce multiple sampled reasoning chains; final answer picked via majority-vote or self-consistency over K=10 CoTs. No additional self-supervised training on programmatic abstractions.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Used as a primary baseline in Table 2; compared directly against SAL variants and the Seed-only baseline.",
            "task_or_benchmark": "StrategyQA, GSM8K, ARC, CommonsenseQA, HotpotQA",
            "performance_results": "Table 2: CoT (few-shot) results with Mistral-7B-Instruct-v0.1: StrategyQA-seen 58.8, StrategyQA 58.8, GSM8K-seen 44.6, GSM8K 39.2, ARC 70.5, CsQA 62.2, HotpotQA 32.7.",
            "qualitative_findings": "CoT few-shot provides reasonable baseline reasoning performance but is outperformed by SAL variants that learned abstract programmatic solutions through self-supervision; CoT alone can be inconsistent across variations that share the same high-level solution process.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Standard CoT few-shot provides a competitive baseline, but fine-tuning with analogical programmatic self-supervision yields superior consistency and accuracy on multiple reasoning benchmarks.",
            "uuid": "e8267.3",
            "source_info": {
                "paper_title": "Self-supervised Analogical Learning using Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "RAG (SAL + RAG)",
            "name_full": "Retrieval-Augmented Generation applied to SAL (two-pass with ask_llm answers)",
            "brief_description": "A retrieval-augmentation step applied to SAL: first-pass generated programs' ask_llm queries are answered (short paragraphs) and those answers are fed into program generation to produce second-pass programs, improving feasibility and final program quality for cases with complex factual factors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (SAL fine-tuned) + RAG",
            "model_description": "SAL-trained Mistral-7B that uses an extra retrieval/answering pass for ask_llm queries contained in generated programs; second-pass program synthesis uses those retrieved answers as context.",
            "reasoning_methods": [
                "Analogical reasoning (SAL)",
                "Programmatic reasoning with external factual grounding (RAG)",
                "Chain-of-thought (used in generation/verification)"
            ],
            "reasoning_methods_description": "Collect all ask_llm queries embedded in first-pass programs, answer them with short paragraphs (retrieval/QA), then invoke qc2p(q, ans) to generate second-pass programs incorporating factual answers; when first-pass voted answer is 'unknown', use retrieval-augmented programs.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared SAL(all) with and without RAG in Table 2; RAG consistently improves absolute performance across benchmarks (notably StrategyQA, ARC, HotpotQA).",
            "task_or_benchmark": "StrategyQA, GSM8K, ARC, CommonsenseQA, HotpotQA",
            "performance_results": "Table 2: SAL (all) + RAG yields StrategyQA-seen 63.7, StrategyQA 62.9, GSM8K 50.8 (GSM values same as SAL(all)), ARC 73.7, CsQA 77.0, HotpotQA 55.1 — improvements over SAL(all) (e.g., +2.7 pts on StrategyQA-seen over SAL(all)).",
            "qualitative_findings": "RAG reduces program search space by resolving factual ambiguities and leads to more single-program solutions for complex questions; combined with SAL's abstract programs, RAG yields best performance in the paper's experiments.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Adding a retrieval-augmentation step to SAL helps handle complex, fact-dependent questions by grounding programmatic reasoning, further improving performance and making program generation more feasible.",
            "uuid": "e8267.4",
            "source_info": {
                "paper_title": "Self-supervised Analogical Learning using Language Models",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual and unbiased reasoning in language models",
            "rating": 2,
            "sanitized_title": "conceptual_and_unbiased_reasoning_in_language_models"
        },
        {
            "paper_title": "Large language models as analogical reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_as_analogical_reasoners"
        },
        {
            "paper_title": "In-context analogical reasoning with pre-trained language models",
            "rating": 1,
            "sanitized_title": "incontext_analogical_reasoning_with_pretrained_language_models"
        },
        {
            "paper_title": "Faithful chain-of-thought reasoning",
            "rating": 1,
            "sanitized_title": "faithful_chainofthought_reasoning"
        },
        {
            "paper_title": "Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models",
            "rating": 1,
            "sanitized_title": "gsmsymbolic_understanding_the_limitations_of_mathematical_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.014582749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-supervised Analogical Learning using Language Models</h1>
<p>Ben Zhou* Sarthak Jain Yi Zhang Qiang Ning<br>Shuai Wang Yassine Benajiba Dan Roth<br>Arizona State University Amazon University of Pennsylvania<br>benzhou@asu.edu</p>
<h4>Abstract</h4>
<p>Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can successfully solve. Such observations motivate us to propose methods that encourage models to understand the highlevel and abstract reasoning processes during training instead of only the final answer. This way, models can transfer the exact solution to similar cases, regardless of their relevance to the pre-training data distribution. In this work, we propose SAL, a self-supervised analogical learning framework. SAL mimics the human analogy process and trains models to explicitly transfer high-quality symbolic solutions from cases that they know how to solve to other rare cases in which they tend to fail more. We show that the resulting models after SAL learning outperform base language models on a wide range of reasoning benchmarks, such as StrategyQA, GSM8K, and HotpotQA, by $2 \%$ to $20 \%$. At the same time, we show that our model is more generalizable and controllable through analytical studies.</p>
<h2>1 Introduction</h2>
<p>While large language models (LLM) have achieved tremendous progress in reasoning benchmarks (OpenAI, 2023), they have been shown to have consistency issues (Li et al., 2023; Mirzadeh et al., 2024) — LLMs cannot produce consistent solutions on questions sharing identical or similar problem-solving processes. Previous studies ( Li et al., 2024) have also shown that these models tend to perform well on cases involving familiar or common entities and words but fail on others. In Fig. 1, we demonstrate this issue with three questions that share an identical reasoning process (i.e.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of models' (including o1) inconsistent reasoning processes for three questions that share the same problem-solving process (as outlined on top). Models memorize all relevant factual information (e.g., correctly answers where Bruce Lee was on those days). The correct answers are all "yes," because Bruce Lee was in California on 1964.8.2, and he was in Seattle to get married on 1964.8.17.
checking absence by checking location mismatch). The first question is a well-known fact that has been widely reported in written text, and models answer it correctly with the correct reason. The other two, on the other hand, involve less common entity combinations, and the models fail to answer them correctly, even though they possess all relevant knowledge by successfully pointing out where the person was at the time of the event. This consistency issue is not significantly improved even with newer models demonstrating much stronger reasoning performances: OpenAI o1 ${ }^{1}$ also fails on the third question, albeit answering the previous two correctly. Such inconsistency prevents these models from deploying in mission-critical tasks (e.g., medical chatbots) requiring trustworthy and unbiased decision-making.</p>
<p>Following these observations, we are motivated</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>to build models that can better understand the underlying high-level and abstract reasoning processes instead of only memorizing the final answers or relevant tokens during training time. This way, models can re-use such reasoning paths in all similar cases and achieve better consistency. To this end, we propose a data-driven analogical learning framework called SAL (Self-supervised Analogical Learning). SAL extracts supervision signals with some inference processes using a base language model, and trains the same base language model with these signals, which forms a self-supervised learning scheme. The supervision signals we use are questions and their corresponding abstract and symbolic solutions, e.g., as shown on the top of Fig. 1, so that models can learn the high-level process that can bridge the inconsistency gap between various similar questions. We propose two novel supervision-extraction methods in SAL. The first method called conceptualization, starts with the original question and finds a group of similar questions that share identical high-level solutions (Zhou et al., 2024) of the original question. With these questions, we collect symbolic solutions in the form of Python program from the questions that the base language model can successfully answer, and use these symbolic solutions as supervision targets. The second method, called simplification, finds questions that share sub-routines of answering the original question, which can be used to eventually find the high-level solution of the original questions via composition.</p>
<p>SAL collects supervision signals from both extraction methods and trains the base language model with the resulting supervision signals. At its essence, this training process enforces a soft but symbolic constraint over the questions that share similar or simpler problem-solving processes. We also design an optional factual retrieval step to help models better generate a single solution for complex questions where considering all possible related factors is infeasible. After training, SAL inference first generates a symbolic program and executes the program to find the final answer. On a selection of complex reasoning datasets, including StrategyQA (Geva et al., 2021), ARC (Clark et al., 2018), CommonsenseQA (Talmor et al., 2018), GSM8K (Cobbe et al., 2021), and HotpotQA (Yang et al., 2018), we show that SAL effectively improves base language models’ performance with direct inference methods such as chain of thought and human-supervised baselines, by 2% to 20%. On top of the performance gains, SAL provides higher interpretability and controllability because of its programmatic inference scheme.</p>
<h2>2 Related Work</h2>
<p>Our work builds on analytical works that discusses language model’s inconsistency (Tang et al., 2023; Li et al., 2023, 2024; Mirzadeh et al., 2024), and SAL is a learning scheme that mitigate such issues by encouraging consistency high-level solutions among relevant questions. SAL is related to previous works that use LLMs to generate programmatic solutions (Chen et al., 2022; Gao et al., 2023) or other structures (Feng et al., 2024). Some of them also aims to improve model consistency (Lyu et al., 2023) via programs, but our work considers abstraction and high-level solutions, which can transfer better. Several works have also been using analogy to encourage generalization (Hu et al., 2023; Yasunaga et al., 2024; Yuan et al., 2024; Zhou et al., 2024). Our work shares a similar motivation, but we are the first to propose automatic self-supervision methods and improve model performances in the training step. Our work is also related and motivated by other works that use self-supervision or distillation signals (Zhu et al., 2024; Zelikman et al., 2024). Decomposition-based inference pipelines (Khattab et al., 2022; Hao et al., 2023; Yao et al., 2023) also inspires our work, as our programmatical solutions resembles a decomposed inference process.</p>
<h2>3 Self-Supervision Extraction</h2>
<p>This section introduces how we collect the self-supervision signals in SAL through the two extraction methods: conceptualization and simplification. In both settings, we assume an original question $q$, and a base language model $L M$. Both extraction pipelines use $L M$ to find a set of questions $q_{0}^{\prime}, q_{1}^{\prime} \ldots q_{N}^{\prime}$ that share identical or sub-processes of the underlying problem-solving process of $q$. From $q_{i}^{\prime} s$, we find a symbolic solution that describes the high-level reasoning process of $q$ that also applies to $q_{i}^{\prime} s$ to some extent. We assume and require no knowledge regarding the gold answer or process of $q$, which makes SAL self-supervised.</p>
<p>The symbolic solutions we acquire are in the form of Python programs. The overall program</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of the conceptualization pipeline. Functions refer to sub-tasks and prompt templates.</p>
<p>structure design follows Zhou et al. (2024) and supports general-domain question-answering (QA), including factual and math questions. On top of Zhou et al. (2024) that uses ask_llm calls for knowledge retrieval and soft value comparisons, we add a step to identify the units for physical values. This avoids a knowledge retrieval query such as "What is the density of water?" without providing the expected unit. We detail this process in §B. All programs will contain a function named "answer()", which will return a binary yes/no value for general questions and a specific number for math questions.</p>
<h3>3.1 Overview of Conceptualization</h3>
<p>The conceptualization extraction pipeline finds $q_0', q_1',...q_N'$, that all share the same reasoning paths as $q$, but are much easier for $LM$ to confidently answer. Specifically, we first conceptualize $q$ into a conceptualized version $q_{abs}$, as shown in Fig. 2. Then, we generate a list of similar questions based on $q_{abs}$. With these questions, we select a set of high-confidence questions that language models tend to answer more accurately and use them as $q'<em abs="abs">i s$. These components so far are primarily based on Zhou et al. (2024). Then, we propose a novel symbolic solution-generation process that generates high-quality programs from $q'_i s$ and their silver chain-of-thought outputs. We view these high-quality programs as supervision targets to $q$ and $q</em>$ to form self-supervision signals.</p>
<p>The intuition behind this is supported by recent works on LLM biases (Li et al., 2023, 2024), where it is shown that LLMs do much better on questions that contain familiar entity and token combinations. Motivated by such observations, the conceptualization pipeline automatically creates easier-to-answer questions with identical or very similar solutions to $q$, acquiring high-quality symbolic solutions from these similar questions.</p>
<h3>3.2 Overview of Simplification</h3>
<p>The simplification extraction targets math questions. We formulate solving a question $q$ as a stepwise decomposition process that iteratively finds the next single-hop question that is easier to answer and contributes to the final solution. We solve the next question and add the conclusion to a set of "known conditions" until the decomposition finds no new questions. These "next questions" are the $q'_i s$. Fig. 3 provides an example of simplification. The initial set of conditions are "Hadassah took 6 hours to paint 12 paintings" and "Hadassah paints 20 more paintings." A valid next question would be "What is the rate at which Hadassah paints?"</p>
<p>Our simplification method shares similarities in intuition with decomposition-based inference pipelines (Hao et al., 2023). Yet, we focus on acquiring a better global solution by reducing cognitive overload (Xu et al., 2023), which suggests that models perform better when they have fewer tasks simultaneously. We observe similar findings where models produce better programs when fewer computations are needed for the current question. For example, the most common mistake when solving the question in Fig. 3 is that programs fail to add the 6 hours Hadassah took to paint the first 12 paintings, as the question asks for a total time. This is because the model has to deal with other subprocesses (i.e., computing the number of paintings per hour), reducing its capacity to think comprehensively. When provided with the results of part of the computations, models tend to think more comprehensively and generate better programs.</p>
<h3>3.3 Base Multi-function Model</h3>
<p>As overviewed in §3.1 and §3.2, we need multiple components (i.e., outputs from $LM$) in both extraction pipelines. We annotate a small set of</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />Figure 3: An example math question and overview of how our simplification process can acquire more correct incidental programs.</p>
<p>domain-agnostic seed supervision signals for individual components to achieve this. This is because smaller models cannot sufficiently accomplish the specific sub-tasks we need even after regular instruction tuning. Larger language models can work with few-shot prompting and waive the need for such seed supervision. With such seed supervision signals, equivalent to instruction-tuning data, we train $L M$ to acquire an instruction-tuned base language model that can handle all sub-tasks needed in the pipelines. In this section, we introduce the individual components we need for both conceptualization and simplification extraction pipelines, as well as how we supervise $L M$ to achieve these components. Instruction templates are listed in §A.</p>
<p>Chain of thought. We collect seed supervision signals of questions to their chain-of-thought (CoT) answers, with a prompt template of $\cot (q)$, as shown in Fig. 2. We borrow the CoT results of 228 binary questions from the StratgyQA development set from zhou2024semi and acquire CoT outputs of an additional 265 math questions from the GSM8K training set using Mixtral-8x7B-Instruct.</p>
<p>Questions to abstract questions. We collect seed supervision data of questions to their abstract or conceptual questions. We use the data from zhou2024semi on the same 228 StrategyQA questions. For the 265 math questions, we use Mixtral-8x7B-Instruct and few-shot prompting to generate similar abstractions. This process of generating abstractions is denoted as $\operatorname{abs}(q)$ in Fig. 2.</p>
<p>Abstract questions to similar questions. We use the data from zhou2024semi to form seed supervision data of abstract questions to concrete similar questions, as $\operatorname{sim}(q)$ shows in Fig. 2.</p>
<p>Questions to programs. We collect gold programs that can successfully solve the seed questions. We borrow the programs from zhou2024semi for the 228 StrategyQA questions. We similarly use Mixtral-8x7B-Instruct to find candidate programs that execute the correct answer for math questions. With such programs, we can formulate two sub-tasks and their seed supervision, namely original question to program (q2p(q) as in Fig. 2) and abstract question to program (aq2p(aq)). We remove the "answer()" call in the target program in the second setting because the abstract question does not contain the specific input parameters.</p>
<p>Questions and relevant knowledge to programs. We want to train a model sensitive to relevant knowledge and reference answers so that the model can generate precise and faithful programs when chain-of-thought answers or retrieval results are provided. For example, as Fig. 2 shows, we want the generated program based on the CoT results related to "animal changing colors to adapt to an environment" to explicitly consider whether an animal can change colors. To do this, we formulate seed supervision data that uses the questions and their chain-of-thought answers as inputs and corresponding programs as outputs. This subtask is denoted as $\operatorname{qc} 2 \mathrm{p}(q, \cot ))$.</p>
<p>Original question to initial known conditions. We generate seed supervision instances from original math questions to their initial sets of known conditions by few-shot prompting Mixtral-8x7BInstruct using the 265 seed math questions in $\S 3$. This task is denoted as math_q2kc(q).</p>
<p>Question to next questions. This task generates the next question based on the original and current known conditions. The next questions are designed to be the immediate next computation step, given the current known conditions, to solve the original question. Similarly, we acquire seed supervision instances by few-shot prompting Mixtral-8x7B-Instruct with the 265 seed math questions. This task and its prompt template are named math_q2nq $(q, c)$, where $q$ is the original question, and $c$ is the current known conditions.</p>
<p>Question and answer to statements. A final subtask we define is transforming a question and its answer into a statement. We collect seed supervision data similarly by prompting Mixtral-8x7B-Instruct. This prompt template is named qa2s $(q, a)$.</p>
<p>Final Model. We train the base language model using the seed supervision data mentioned above, so this unified model can handle all sub-tasks as described by identifying the prompt templates we define in the seed supervision data. This is similar to all instruction-tuning methods that combine multiple sub-tasks during fine-tuning (Khashabi et al., 2020; Wei et al., 2022).</p>
<h3>3.4 Conceptualization Extraction</h3>
<p>With such a multi-function model, we can now automatically extract self-supervision signals using the conceptualization pipeline overviewed in $\S 3.1$. For each question $q$, we generate its abstract question $q_{a b s}$ by querying the multi-function model with template abs $(q)$. We then generate a list of $N$ concrete questions that fit the abstract question $q_{a b s}$ by prompting the model with the similar question generation prompt $\operatorname{sim}\left(q_{a b s}\right)$. For each concrete similar question $q_{i}^{\prime}$, we acquire their chain-of-thought answers by prompting $\cot \left(q_{i}^{\prime}\right)$ and random sample $K$ times. We keep only the questions where $X$ out of the $K$ CoTs agree with each other to select high-quality questions that are easy and confident enough for the model to answer. For each of the selected similar questions $q_{i}^{\prime}$, we find one of the CoT results $\cot <em i="i">{i}^{\prime}$ that infer to the majority-voted answer and generate a programmatic solution using
$p</em>\right)$.}=\operatorname{qc} 2 \mathrm{p}\left(q_{i}^{\prime}, \cot _{i}^{\prime</p>
<h3>3.4.1 Selection Criteria and Parameters</h3>
<p>In this work, we set $N=20, K=10$, and $X=9$, meaning that we generate 20 similar questions per abstract question and keep those with at least a $9 / 10$ agreement in their chain-of-thought predictions. After the inference process, we would have a set of programs generated from these highconfidence similar questions. Because we have multiple programs from this generation process, we are motivated to further select from them and use the ones with the highest quality possible as the self-supervision signals. We now describe a set of selection criteria we use for this purpose. All hyper-parameters are tuned with manual examinations over a small set of examples, and we expect minimum variances because of the large data sizes.</p>
<p>Accuracy-based criteria. A natural selection process is to verify each programmatic solution against the high-confidence similar questions by trusting their corresponding silver labels. We execute each of the generated programs with the parameters corresponding to each selected similar question. Let the number of remaining similar questions be $M$; we set a high threshold of $\max (3, M * 0.75)$ and select programs that execute to the correct silver labels ${ }^{4}$ over such a threshold number of questions. We also employ a low threshold of 2 , which will be used later.</p>
<p>Similarity-based criteria. We employ a similarity-based metric using a paraphrase model from sentence transformers ${ }^{5}$ to encourage the model to generate programs that do not repeat the original question and perform actual decomposition and reasoning. We drop all programs containing a ask_llm query with a paraphrase score higher than 0.95 .</p>
<p>Complexity-based criteria. We also want the model to self-supervise on programs considering more abduction factors, which implies more query calls to ask_llm. As a result, we drop all programs that only make one query using this helper function.</p>
<p>Soundness-based criteria. We also check for details in the generated program to ensure soundness.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>System</th>
<th>#Instances</th>
<th>Accuracy</th>
<th>Collected Pct.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Original</td>
<td>344</td>
<td>91.3</td>
<td>25.3%</td>
</tr>
<tr>
<td>Iteration 1</td>
<td>103</td>
<td>86.4</td>
<td>32.9%</td>
</tr>
<tr>
<td>Iteration 2</td>
<td>92</td>
<td>90.2</td>
<td>39.7%</td>
</tr>
<tr>
<td>Iteration 3</td>
<td>46</td>
<td>87.0</td>
<td>43.1%</td>
</tr>
<tr>
<td>Iteration 4</td>
<td>15</td>
<td>80.0</td>
<td>44.2%</td>
</tr>
<tr>
<td>Iteration 5</td>
<td>1</td>
<td>100.0</td>
<td>44.3%</td>
</tr>
</tbody>
</table>
<p>Table 1: Number of math questions that produced ten programs that execute to the same answer at each timestamp of the simplification iteration. Collected Pct. indicates the number of accumulated questions from which we can acquire high-confidence programs.</p>
<p>Specifically, we drop all programs that contain unused parameters in the answer() function call and all programs that produced an error message when executing on more than two similar questions.</p>
<p>We formulate two kinds of self-supervision instances from the selected programs. The first is question-to-program, where the corresponding similar question is used as input, and we use the q2p() prompt template and the low accuracy threshold described above as long as the program successfully executes the correct silver answer on that specific similar question. The second is abstract-question-to-program, using the high accuracy threshold to ensure the target program is representative enough for all scenarios and the aq2p() input template.</p>
<h3>3.5 Simplification Extraction</h3>
<p>At inference time, for each math question $q$, we first generate the initial set of known conditions by prompting the base multi-function model with $\mathrm{math} _q 2 \mathrm{kc}(q)$. Then, we propose an iterative process with the following steps. 1) Generate the next question using math_q2nq $(q, c)$; 2) Answering the next question with $\cot (k c+n q)$, where we concatenate the current known conditions with the next question into a single query; 3) Adding the next question and its question to the set of known conditions by transforming them into a statement using $\mathrm{qa} 2 \mathrm{~s}(n q, a)$. 4) Generating solutions in our programmatic space using $\mathrm{qc} 2 \mathrm{p}(q, k c)$, where $q$ is the original question, and $k c$ is the current known conditions. The iteration stops if one of the two criteria happens. A) The generated next question contains "no more decomposition," which is part of the above seed training data. B) $9 / 10$ of the generated programs from 4) execute the same answer, which implies high confidence in the predictions.</p>
<p>Sanity-check Experiments We conduct a sanitycheck experiment to verify the effectiveness of our proposed simplification method for mathematical questions. We aim to acquire as many highconfidence programs as possible and evaluate the quality of these high-confidence programs. We run our proposed simplification pipeline on 2 k math questions from GSM8K, among which 1,358 questions produced valid programs. Because we want to collect high-confidence programs, we count the number of math questions that can produce ten programs that lead to the same answer after execution. As Table 1 shows, without our simplification pipeline, only on $25.3 \%$ of the questions can the model generate ten programs that lead to the same answer, which we consider as a valid selfsupervision instance. At the 5th iteration of our simplification process (i.e., generating 5 next questions iteratively), we can collect high-confidence programs on $44.3 \%$ of the math questions, with high accuracy on par with the original generation (i.e., generating programs directly from the original question). This suggests that our proposed method can effectively acquire more self-supervision signals without undermining their qualities.</p>
<h2>4 Training with Analogical Supervision</h2>
<p>As mentioned in §1, SAL first collects selfsupervision signals and then trains the model with them. We now describe the training process.</p>
<h3>4.1 Direct Training</h3>
<p>The direct training scheme simply combines all the self-supervision instances and the seed training instances and trains a new model with a supervised fine-tuning (SFT) process. Our goal is to show that the models trained with our automatic selfsupervision data can outperform two baselines: 1) chain-of-thought inference with the base language model's instruction-tuned version, 2) the base language model supervised with the seed supervision from $\S 3.3$ only.</p>
<h3>4.2 Retrieval-augmented Generation</h3>
<p>The reasoning process sometimes requires some prior knowledge to reduce the size of the abduction. For example, consider the question, "Can I visit Museum X 20 times for under 200 dollars?" When generating programs, the model will almost always consider the ticket price for a single entry because it is the most common pricing structure. However,</p>
<table>
<thead>
<tr>
<th>System</th>
<th>StQA-seen</th>
<th>StQA</th>
<th>GSM-seen</th>
<th>GSM</th>
<th>ARC</th>
<th>CsQA</th>
<th>Hotpot</th>
</tr>
</thead>
<tbody>
<tr>
<td>CoT (few-shot)</td>
<td>58.8</td>
<td>58.8</td>
<td>44.6</td>
<td>39.2</td>
<td>70.5</td>
<td>62.2</td>
<td>32.7</td>
</tr>
<tr>
<td>Seed Only</td>
<td>57.6</td>
<td>56.1</td>
<td>45.4</td>
<td>45.0</td>
<td>68.3</td>
<td>73.5</td>
<td>50.3</td>
</tr>
<tr>
<td>Ours</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SAL (concept. only)</td>
<td>61.1</td>
<td>58.8</td>
<td>44.7</td>
<td>43.7</td>
<td>68.3</td>
<td>72.0</td>
<td>51.5</td>
</tr>
<tr>
<td>SAL (all)</td>
<td>61.0</td>
<td>59.7</td>
<td>49.4</td>
<td>50.8</td>
<td>71.0</td>
<td>74.7</td>
<td>52.4</td>
</tr>
<tr>
<td>SAL (all) + RAG</td>
<td>63.7</td>
<td>62.9</td>
<td>49.4</td>
<td>50.8</td>
<td>73.7</td>
<td>77.0</td>
<td>55.1</td>
</tr>
</tbody>
</table>
<p>Table 2: System performances on a range of reasoning datasets. The base model is Mistral-7B. Few-shot CoT uses Mistral-7B-Instruct-v0.1.</p>
<table>
<thead>
<tr>
<th>System</th>
<th>StQA-seen</th>
<th>StQA</th>
<th>Avg. $\Delta$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Seed Only</td>
<td>45.8</td>
<td>45.8</td>
<td>-11.1</td>
</tr>
<tr>
<td>SAL</td>
<td>58.0</td>
<td>56.4</td>
<td>-3.2</td>
</tr>
</tbody>
</table>
<p>Table 3: System performances after removing all programs that internally query the language model with a question very similar to the original question.
there is a complicated pricing structure for certain museums, such as the MET museum, including annual memberships, senior and student discounts, and residential passes. It is impractical for models to consider all possible pricing structures in the first pass. We build a retrieval-augmentation mechanism to allow models to consider external world facts before generating the solutions, reducing the search space and improving the feasibility of generating a single program to solve the question. To achieve this, we take all the ask_llm queries in the generated programs from the first pass and ask the base language model to answer them with a short paragraph. Then, we use $\mathrm{qc} 2 \mathrm{p}(q$, ans $)$ to generate the second-pass programs, where ans is the short answering paragraph. Whenever a voted answer from the first-pass programs is "unknown," we use the retrieval-augmented programs instead.</p>
<h2>5 Experiments</h2>
<p>In this section, we conduct experiments to show the effectiveness of our proposed framework SAL.</p>
<h3>5.1 Self-Supervision Extraction</h3>
<p>The seed supervision data contains 4619 instances that have programs as outputs. To expand them with self-supervision, we apply the conceptualization pipeline on 2 k questions from StrategyQA and 800 questions from HotpotQA. We apply simplification on 5 k questions from GSM8K. All questions come from the training sets. Such processes produced 5865, 1000, and 4439 self-supervision training instances, respectively.</p>
<h3>5.2 Baselines, Datasets and Parameters</h3>
<p>Due to resource concerns and other constraints, we use Mistral-7B-v0.1 as our base language model. Both the training for the multi-function model in $\S 3.3$ and later experiments on evaluating the effectiveness of SAL supervision signals are done with the same model because of the self-supervision assumption. ask_llm is handled by few-shot prompting to its instruction-tuned version (Mistral-7BInstruct-v0.1). All fine-tuning processes use LoRA (Hu et al., 2021) (r=32) and a learning rate of $2 e-4$. We use $k=10$ as the number of candidate programs at inference time.</p>
<p>We evaluate five reasoning datasets: StrategyQA, GSM8K, ARC, CommonsenseQA, and HotpotQA. Because of the limitations of our program space (i.e., only support binary answers), we transform all questions to binary for multiple-choice datasets. Specifically, we design a few-shot prompt and use Mixtral-8x7B-Instruct to transform the question with its correct answer to a question with a correct answer of "yes". As this is a simple rephrasing task, Mixtral demonstrates 100\% accuracy in the 50 human-selected examples. For StrategyQA and GSM8K, we create a special evaluation split called "seen", which contains questions we use as raw questions when generating self-supervision signals. Models do not see or use the gold labels of these questions in any way. The overall evaluation data contains 500 StrategyQA (seen), 490 StrategyQA, 500 GSM (seen), 500 GSM, 112 ARC, 336 CommonsenseQA and 339 HotpotQA instances.</p>
<p>We consider two main baselines: a few-shot chain-of-thought (CoT, self-consistency@10) of Mistral's instruction-tuned version, and the base model trained on seed supervision instances (§3.3) only, named "Seed Only." All models are trained to the same steps, which equals five epochs of the full training data (seed + self-supervision). We report averaged results from two random seeds for</p>
<table>
<thead>
<tr>
<th>System</th>
<th>StQA Dev.</th>
<th>StQA test</th>
<th>BoolQ</th>
<th>CsQA</th>
<th>Hotpot</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mixtral 8x7B (few-shot)</td>
<td>53.9</td>
<td>52.9</td>
<td>41.1</td>
<td>10.7</td>
<td>55.3</td>
</tr>
<tr>
<td>Llama-70B (few-shot)</td>
<td>56.8</td>
<td>59.7</td>
<td>58.9</td>
<td>53.4</td>
<td>65.6</td>
</tr>
<tr>
<td>Mistral7B + Seed Only</td>
<td>60.1</td>
<td>44.3</td>
<td>40.1</td>
<td>59.0</td>
<td>50.0</td>
</tr>
<tr>
<td>Mistral7B + SAL</td>
<td>59.9</td>
<td>56.0</td>
<td>51.2</td>
<td>59.0</td>
<td>51.2</td>
</tr>
</tbody>
</table>
<p>Table 4: Conceptual Reasoning Performance. We use a slightly different program space, as described in §3.
experiments that require SFT.</p>
<h3>5.3 Results</h3>
<p>Table 2 shows our main experiment results of reasoning performances across the five reasoning datasets. We first observe that the seed only baseline performs relatively similarly to the CoT baseline, indicating that the seed supervision data mostly teach the model forms of the programmatic space without improving their reasoning capabilities by much. On the other hand, we see that the base LLM trained on self-supervision data from conceptualization extraction only (SAL (concept. only)) improves 3.5%, 2.7%, and 1.2% on StrategyQA’s seen, unseen, and HotpotQA, demonstrating the effectiveness of our proposed self-supervision signals. By adding the simplification self-supervision (SAL (all)), we improve 4% and 5.8% on GSM8K’s seen and unseen sets, showing that our simplification pipeline can effectively extract self-supervision data on math-like questions. In addition, we observe better domain transferability as SAL (all) improves on ARC and CommonsenseQA by 2.7% and 1.2%, respectively. Model performance is further improved by adding the retrieval-augmented generation in §4.2. The final model is consistently better than the CoT baseline (+6.8% on StrategyQA, 5.8% on GSM8K, and 4.8% on HotpotQA). At the same time, our model is consistently better than the seed only baseline, demonstrating the overall effectiveness of our proposed self-supervision framework.</p>
<h2>6 Analysis</h2>
<p>To further demonstrate the capability of our self-supervision signals, we conduct an experiment that disregards all programs that contain a ask_llm query that is at least 0.9 similar to the original question, using the same paraphrase detection model in §3. As the results show in Table 3, the seed baseline model drops significantly by 11% on StrategyQA, meaning that a significant portion of its generated programs simply repeat the original question and delegate reasoning to the instruction-tuned model behind ask_llm. This is not as much a concern in our self-supervised model as it only drops 3%. This suggests the self-supervised model is better at reasoning by conducting comprehensive abduction and decompositions.</p>
<p>We also consider the conceptual reasoning benchmark proposed in <em>Zhou et al. (2024)</em>, which considers four datasets and 1597 test instances. In this conceptual setting, systems see an abstract version of the question and must make conceptual reasoning considering all possible corner cases, which we can achieve by querying our models with aq2p(aq). As Table 4 shows, the model trained with self-supervision signals improves 11.7% on StrategyQA, 11% on BoolQ, and 1.2% on HotpotQA. At the same time, it outperforms the larger Mixtral-8x7B-Instruct model on several tasks, demonstrating the effectiveness of using automatic self-supervision signals. Such high performance on conceptual reasoning suggests that our proposed self-supervised model improves neutral and unbiased reasoning by establishing internal consistencies in the actual reasoning processes.</p>
<h2>7 Conclusion</h2>
<p>This work introduces SAL, self-supervision analogical learning that encourages LLM consistent reasoning in common and rare situations. We propose two methods to collect self-supervision signals: 1) conceptualization, a pipeline that finds programmatic solutions from similar questions that models can confidently answer and share identical reasoning processes; 2) simplification, which finds high-quality solutions to math questions by gradually decomposing a question and reducing models’ logical overload. Experiments show that the self-supervision signals are high-quality and effective, which can be used to train models more attentive to the actual reasoning process instead of the semantic inductive biases. SAL leads to performance improvements on a wide-range of reasoning tasks.</p>
<h2>Limitations</h2>
<p>Our work has several limitations in its current form. Limited Data Sizes. We only consider 3 k questions for the conceptualization pipeline and 5 k questions for the simplification pipeline. Because of the selection criteria, the resulting data size is only 2.5 times larger than the seed supervision. This limits our arguments on large-scale self-supervision.
Binary Questions Only. We only consider binary questions in the conceptualization pipeline because it is the most straightforward way to evaluate a program solution's accuracy on similar questions. While all multiple-choice questions can be easily transformed into binary questions, this limits the scalability to more free-form questions and needs to be addressed in future works.
Single Base Model. Due to computational constraints, we only experiment with one base LLM (Mistral-7B), and more extensive experiments with other families of models may provide more insight into the capabilities of different models. Yet, we hypothesize that the general trend should be consistent with other base models.
Single Base Model. Due to computational constraints, we only experiment with one base LLM (Mistral-7B), and more extensive experiments with other families of models may provide more insight into the capabilities of different models. Yet, we hypothesize that the general trend should be consistent with other base models.
Limited Ablation Studies. Due to computational constraints, we do not conduct ablation studies on the hyper-parameters used in $\S 3$, although these parameters can be further fine-tuned to achieve better performances. However, we believe that the hand-picked parameters we employ are good enough because of manual examinations, and we expect little changes in empirical results if these numbers are changed slightly because we extract a relatively large set of supervision instances.</p>
<h2>References</h2>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Yu Feng, Ben Zhou, Weidong Lin, and Dan Roth. 2024. Bird: A trustworthy bayesian inference framework for large language models. arXiv preprint arXiv:2404.12494.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models. In International Conference on Machine Learning, pages 10764-10799. PMLR.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361.</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In EMNLP.</p>
<p>Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large language models. In $I C L R$.</p>
<p>Xiaoyang Hu, Shane Storks, Richard L. Lewis, and Joyce Yue Chai. 2023. In-context analogical reasoning with pre-trained language models. In $A C L$.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with a single qa system. In ACL (findings).</p>
<p>Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024.</p>
<p>Bangzheng Li, Ben Zhou, Xingyu Fu, Fei Wang, Dan Roth, and Muhao Chen. 2024. Famicom: Further demystifying prompts for language models with taskagnostic performance estimation. arXiv preprint arXiv:2406.11243.</p>
<p>Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, and Muhao Chen. 2023. Deceptive semantic shortcuts on reasoning chains: How far can models go without hallucination? NAACL.</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-ofthought reasoning. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 305-329.</p>
<p>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937.</p>
<p>Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. 2023. Large language models are in-context semantic reasoners rather than symbolic reasoners. arXiv preprint arXiv:2305.14825.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Nan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, and Muhao Chen. 2023. Cognitive overload: Jailbreaking large language models with overloaded logical thinking. arXiv preprint arXiv:2311.09827.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed Huai hsin Chi, and Denny Zhou. 2024. Large language models as analogical reasoners. ICLR.</p>
<p>Siyu Yuan, Jiangjie Chen, Changzhi Sun, Jiaqing Liang, Yanghua Xiao, and Deqing Yang. 2024. Analogykb: Unlocking analogical reasoning of language models with a million-scale knowledge base. $A C L$.</p>
<p>Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. 2024.</p>
<p>Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629.</p>
<p>Ben Zhou, Hongming Zhang, Sihao Chen, Dian Yu, Hongwei Wang, Baolin Peng, Dan Roth, and Dong Yu. 2024. Conceptual and unbiased reasoning in language models. arXiv preprint arXiv:2404.00205.</p>
<p>Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xinwei Long, Zhouhan Lin, and Bowen Zhou. 2024. Pad: Programaided distillation can teach small models reasoning better than chain-of-thought fine-tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 2571-2597.</p>
<h1>A Prompts</h1>
<h2>A. 1 Few-shot Prompts for Seed Supervision</h2>
<p>Table 5 lists the prompts we use for collecting seed supervision of math question abstractions. Nonmath question abstractions are done in Zhou et al. (2024), and we directly borrow their data. Table 6 shows the prompts we use to collect seed supervision for generating gold math questions. The generated programs' validity is further verified by the gold answer. Table 7 details the prompt for collecting seed supervision data for generating the next questions in the simplification pipeline. This prompt asks for the entire decomposition with multiple next questions in one query, later formulated as multiple instances with one next question per instance. Table 8 shows the prompts for collecting seed supervision instances on known conditions. It handles two scenarios: extracting the list of known conditions from the original question and transforming the answer to the next question into a statement. These two scenarios are formulated as separate seed supervision instances.</p>
<h2>A. 2 Input Templates</h2>
<p>Table 9 details the input prompt templates for each sub-task as described in $\S 3.1$ and $\S 3.2$. A multifunction model trained with the same prompt templates can recognize the different goals based on the query because the seed supervision instances use the same templates.</p>
<h2>B Implementations</h2>
<p>Table 10 shows a partial implementation of our programmatic solution space. In addition to this, we also employ the soft value comparison helper functions as described in Zhou et al. (2024).</p>
<p>All SFT experiments are conducted with 4 x RTX6000 GPUs and a single-run fine-tuning finishes in 3 hours. We useHuggingface's Autotrain framework for training. ${ }^{6}$ All parameters are default parameters unless explicitly mentioned otherwise.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Prompt</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>system</td>
<td>Identify numerical values in the given question, then replace some of them with Python parameters that are <br> either int or float, so that the resulting abstract question is still answerable with the same general solution as <br> the original question. Follow the provided examples.</td>
</tr>
<tr>
<td>user</td>
<td>Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If <br> he has 6 books, how many pages is each one on average?</td>
</tr>
<tr>
<td>assistant</td>
<td>Because this is a math question, we identify all numerical values. We identify: 12 inches, 80 pages, one <br> inch, 6 As a result, we can replace "12 inches" to "Number of Inches X" (num_inches_x: int) "80 pages" to <br> "Number of Pages Y" (num_pages_y: int) "one inch" to "Number of Inches Z" (num_inches_z: int) "6" to <br> "Number W" (num_w: int) So the question becomes Jack has a stack of books that is Number of Inches X <br> thick. He knows from experience that Number of Pages Y is Number of Inches Z thick. If he has Number <br> W books, how many pages is each one on average? With parameters num_inches_x=12, num_pages_y=80, <br> num_inches_z=1, num_w=6</td>
</tr>
</tbody>
</table>
<p>Table 5: Prompts for collecting seed supervision of math question abstractions. More examples are omitted.</p>
<table>
<thead>
<tr>
<th>Prompt</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>system</td>
<td>Based on the given question, write a Python program with some abstraction that solves the given question and <br> all other similar questions that can be solved in a similar fashion. Think as comprehensively as possible, so <br> that the program would work on any inputs. You can use the following helper function: ask_gpt(query: str, <br> type) to ask a simple question to the knowledgeable GPT model and returns the answer in the desired type.</td>
</tr>
<tr>
<td>user</td>
<td>Benny bought Number of Soft Drinks X (num_soft_drinks_x: int) for Cost per Soft Drink Y <br> (cost_per_soft_drink_y: int) each and Number of Candy Bars Z (num_candy_bars_z: int). He spent a <br> total of Total Amount Spent W (total_spent_w: int) dollars. How much did each candy bar cost?</td>
</tr>
<tr>
<td>assistant</td>
<td>def answer(num_soft_drinks_x: int, cost_per_soft_drink_y: int, <br> num_candy_bars_z: int, <br> total_spent_w: int) -&gt; float: <br> total_cost_soft_drinks = num_soft_drinks_x * cost_per_soft_drink_y <br> total_cost_candy_bars = total_spent_w total_cost_soft_drinks <br> cost_candy_bar = total_cost_candy_bars / num_candy_bars_z <br> return cost_candy_bar</td>
</tr>
</tbody>
</table>
<p>Table 6: Prompts for collecting seed supervision of math programs. More examples are omitted.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Content</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">system</td>
<td style="text-align: left;">Given a math question, your job is to decompose it into sub-questions that can stack together and lead to the <br> correct answer of the original question. Each sub question should be answerable on its own. Use '####' to <br> mark your actual answer when answering each sub question. Reference the correct answer.</td>
</tr>
<tr>
<td style="text-align: left;">user</td>
<td style="text-align: left;">Benny bought 2 soft drinks for\$ 4 each and 5 candy bars. He spent a total of 28 dollars. How much did each <br> candy bar cost?</td>
</tr>
<tr>
<td style="text-align: left;">assistant</td>
<td style="text-align: left;">Question 1: Benny bought 2 soft drinks for\$ 4 each. How much does the soft drinks cost in total? Answer 1: <br> The total cost of the soft drinks are $<em> 2 * 4=8 </em>$ #### 8 Question 2: Benny spent $\$ 8$ on soft drinks and bought 5 <br> candy bars. He spent a total of 28 dollars. How much did the candy bars cost? Answer 2: The candy bars <br> cost $<em> 28-8=20 </em>$ #### 20 Question 3: Benny spent $\$ 20$ to buy 5 candy bars. How much did each candy bar <br> cost? Answer 3: Each of the candy bars cost $<em> 20 / 5=4 </em>$ #### 4 END</td>
</tr>
<tr>
<td style="text-align: left;">user</td>
<td style="text-align: left;">There are some lions in Londolozi at first. Lion cubs are born at the rate of 5 per month and lions die at the <br> rate of 1 per month. If there are 148 lions in Londolozi after 1 year, how many lions were there in Londolozi <br> at first?</td>
</tr>
<tr>
<td style="text-align: left;">assistant</td>
<td style="text-align: left;">Question 1: There are some lions in Londolozi at first. Lion cubs are born at the rate of 5 per month and lions <br> die at the rate of 1 per month. What is the number of new lions per months? Answer 1: There are $<em> 5-1=4 </em>$ <br> new lions per months. #### 4 Question 2: There are some lions in Londolozi at first. There are 4 new lions <br> every months. If there are 148 lions in Londolozi after 1 year, how many lions were there in Londolozi at <br> first? Answer 2: The number of new lions in one year is $<em> 4 * 12=48 </em>$. The original lions are $<em> 148-48=100 </em>$ <br> #### 100 END</td>
</tr>
</tbody>
</table>
<p>Table 7: Prompts for collecting seed supervision of math next questions. More examples are omitted.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: left;">Content</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">system</td>
<td style="text-align: left;">Given a math question, extract all the known conditions that are explicitly mentioned. If the answer is <br> provided, extract only the condition that the answer implies. <br> Question: Benny bought 2 soft drinks for\$ 4 each and 5 candy bars. He spent a total of 28 dollars. How much <br> did each candy bar cost? Answer: unknown</td>
</tr>
<tr>
<td style="text-align: left;">assistant</td>
<td style="text-align: left;">Benny bought 2 soft drinks for $\$ 4$ each. Benny bought 5 candy bars. Benny spent 28 dollars in total. <br> Question: Wickham is throwing a huge Christmas party. He invites 30 people. Everyone attends the party, <br> and half of the guests bring a plus one (one other person). He plans to serve a 3-course meal for the guests. If <br> he uses a new plate for every course, how many plates does he need in total for his guests? Answer: unknown</td>
</tr>
<tr>
<td style="text-align: left;">assistant</td>
<td style="text-align: left;">Wickham invites 30 people. Half of the guests bring a plus one. Wickham prepares 3 courses for each person. <br> He uses a new plate for each course. <br> Question: If lion cubs are born at the rate of 5 per month and lions die at the rate of 1 per month, what is the <br> net increase in the number of lions per month? Answer: 4</td>
</tr>
<tr>
<td style="text-align: left;">assistant</td>
<td style="text-align: left;">The number of lines increase by 4 every month. <br> Question: It takes 7 years for an apple tree to bear fruit. If Lydia planted a tree when she was 4 years old, how <br> old would she be when the tree bears fruit for the first time? Answer: 11</td>
</tr>
<tr>
<td style="text-align: left;">assistant</td>
<td style="text-align: left;">Lydia would be 11 years old when the tree bears fruit for the first time. <br> Question: Wickham invites 30 people to the party. Half of the guests bring a plus one. How many additional <br> people (plus ones) are coming to the party? Answer: 15</td>
</tr>
<tr>
<td style="text-align: left;">assistant</td>
<td style="text-align: left;">15 additional people are coming to the party.</td>
</tr>
</tbody>
</table>
<p>Table 8: Prompts for collecting seed supervision of extracting initial known conditions and transforming a question and its answer to a statement. "unknown" answers refer to the initial known condition generation scenarios.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subtask</th>
<th style="text-align: left;">Content</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\cot (q)$</td>
<td style="text-align: left;">Answer this question by thinking step by step. Question: {q} Output:</td>
</tr>
<tr>
<td style="text-align: left;">$\operatorname{abs}(q)$</td>
<td style="text-align: left;">Convert this question to the abstract form. Question: {q} Output:</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{q} 2 \mathrm{p}(q)$</td>
<td style="text-align: left;">Solve this question with a Python program with necessary abstractions. Also generate the <br> corresponding values for the parameters. Question: {q} Output:</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{aq} 2 \mathrm{p}(a q)$</td>
<td style="text-align: left;">Solve this abstract question with a Python program. Abstract Question: {aq} Output:</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{qc} 2 \mathrm{p}(q, \cot )$</td>
<td style="text-align: left;">Solve this question with a Python program based on the provided reference answer. Question: <br> {q} Reference Answer: ${\cot }$ Output:</td>
</tr>
<tr>
<td style="text-align: left;">math_q2kc(q)</td>
<td style="text-align: left;">Extract the given conditions in the math question. Question: {q} Output:</td>
</tr>
<tr>
<td style="text-align: left;">math_q2nq(q, c)</td>
<td style="text-align: left;">Decompose the math question to steps, based on currently known conditions. Question: {q} <br> Known Conditions: {c} Output:</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{qa} 2 \mathrm{~s}(q, a)$</td>
<td style="text-align: left;">Convert the given math question and its answer to a statement. Question: {q} Answer: {a} <br> Output:</td>
</tr>
</tbody>
</table>
<p>Table 9: Input prompt templates for each sub-task that the multi-function model handles.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">def</span><span class="w"> </span><span class="nx">query_llm</span><span class="p">(</span><span class="nx">messages</span><span class="p">):</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="kn">library</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">makes</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">query</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">LLM</span><span class="p">.</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">get_unit</span><span class="p">(</span><span class="nx">question</span><span class="p">):</span>
<span class="w">    </span><span class="nx">messages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span><span class="s">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Does the answer to the following</span>
<span class="s">                                    question require a</span>
<span class="s">                                    measurement unit? If</span>
<span class="s">                                    it is physical metric</span>
<span class="s">                                    , use SI units. If it</span>
<span class="s">                                    is monetary metric,</span>
<span class="s">                                    use &#39;USD&#39;.\nQuestion:</span>
<span class="s">                                    What is the density</span>
<span class="s">                                    of copper?\nAnswer</span>
<span class="s">                                    with the proper unit,</span>
<span class="s">                                    or &#39;None&#39; if not</span>
<span class="s">                                    applicable.&quot;</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="s">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;assistant&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;kg/m^3&quot;</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="s">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;More examples omitted&quot;</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="s">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Can the following question be</span>
<span class="s">                                    answered with a</span>
<span class="s">                                    metric in the</span>
<span class="s">                                    International System</span>
<span class="s">                                    of Units (SI Units)?\</span>
<span class="s">                                    nQuestion: {\nAnswer</span>
<span class="s">                                    with the proper unit</span>
<span class="s">                                    , or &#39;None&#39; if not</span>
<span class="s">                                    applicable.&quot;</span><span class="p">.</span><span class="nx">format</span><span class="p">(</span>
<span class="w">                                    </span><span class="nx">question</span><span class="p">)},</span>
<span class="w">    </span><span class="p">]</span>
<span class="w">    </span><span class="nx">result</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">query_llm</span><span class="p">(</span><span class="nx">messages</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">None</span><span class="err">&#39;</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">result</span><span class="p">:</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nx">result</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">None</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">find_json</span><span class="p">(</span><span class="nx">s</span><span class="p">):</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">finds</span><span class="w"> </span><span class="nx">valid</span><span class="w"> </span><span class="nx">JSON</span><span class="w"> </span><span class="nx">structure</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">output</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">ask_llm</span><span class="p">(</span><span class="nx">question</span><span class="p">,</span><span class="w"> </span><span class="nx">t</span><span class="p">,</span><span class="w"> </span><span class="nx">use_cache</span><span class="p">=</span><span class="nx">False</span><span class="p">):</span>
<span class="w">    </span><span class="nx">unit</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">get_unit</span><span class="p">(</span><span class="nx">question</span><span class="p">)</span>
<span class="w">    </span><span class="nx">question</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="s">&quot; Answer in {}.&quot;</span><span class="p">.</span><span class="nx">format</span><span class="p">(</span><span class="nx">unit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">messages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span><span class="s">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;system&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Answer the question in the</span>
<span class="s">                                    expected type. Use</span>
<span class="s">                                    your best educated</span>
<span class="s">                                    guess or estimation</span>
<span class="s">                                    if needed.&quot;</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="s">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;How many people today are related</span>
<span class="s">                                    to Genghis Khan? (int</span>
<span class="s">                                    )&quot;</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="s">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;assistant&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;{\&quot;answer\&quot;: 35000000}&quot;</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="s">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;More examples omitted&quot;</span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span><span class="s">&quot;role&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;user&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;content&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;{}{{}}&quot;</span><span class="p">.</span><span class="nx">format</span><span class="p">(</span><span class="nx">question</span><span class="p">,</span><span class="w"> </span><span class="nx">t</span><span class="p">.</span>
<span class="w">                                    </span><span class="nx">__name__</span><span class="p">)}</span>
<span class="w">    </span><span class="p">]</span>
<span class="w">    </span><span class="nx">response</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">query_llm</span><span class="p">(</span><span class="nx">messages</span><span class="p">)</span>
<span class="w">    </span><span class="nx">r</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">find_json</span><span class="p">(</span><span class="nx">response</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">r</span>
</code></pre></div>

<p>Table 10: Implementations of the helper functions in our programmatic solution space. Some details are omitted for simplicity.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://github.com/huggingface/ autotrain-advanced&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ https://openai.com/o1/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
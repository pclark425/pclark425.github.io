<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7035 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7035</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7035</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267637261</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.08496v3.pdf" target="_blank">A Systematic Review of Data-to-Text NLG.</a></p>
                <p><strong>Paper Abstract:</strong> This systematic review provides a comprehensive analysis of the state-of-the-art research on data-to-text generation, addressing gaps, highlighting challenges, and proposing future directions. We examined various approaches in this field, assessing their effectiveness and limitations, while surveying literature on datasets, evaluation metrics, application areas, multilingualism, and methods for mitigating hallucinations. We shed light on the usage, popularity, and impact of datasets, alongside evaluation metrics, emphasizing both automatic and human assessment. Additionally, we explore the evolution of data-to-text models, emphasizing the widespread adoption of transformer models. With a focus on inclusivity, we stress the importance of research in low-resourced languages. Despite notable advancements in text quality, we examine strategies utilized to tackle hallucinations in models and advocate for universally applicable techniques. This review serves as a roadmap to inspire innovation, establish evaluation benchmarks, and drive progress in data-to-text generation.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7035.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7035.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PENMAN linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PENMAN AMR linearization / PENMAN notation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A textual serialization format for AMR graphs that represents nodes and labeled edges in a bracketed, PENMAN-style sequence; used in AMR datasets (e.g., LDC2017T10) to convert graph structures into linear text for seq2seq training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Penman: An open-source library and tool for amr graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PENMAN AMR linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes an AMR graph as a bracketed token sequence with node concept labels and labeled relations represented as parenthesized structures (PENMAN notation). Each node and its outgoing edges are serialized into textual brackets and token sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential; token-based; (generally lossless in principle for AMR if full PENMAN preserved)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>PENMAN bracketed serialization (graph â†’ bracketed linear sequence); no specific traversal algorithm (e.g., DFS/BFS) is specified in the reviewed paper</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR (e.g., LDC2017T10)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation / graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sequence-to-sequence models and pretrained transformers (e.g., RNN seq2seq, BART/T5 when fine-tuned on PENMAN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Seq2seq or transformer models take PENMAN-token sequences as input and are trained to map them to natural language sentences; often used to enable application of pretrained LMs to AMR-to-text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, BERTScore, human evaluation (metrics commonly reported for AMR-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables direct training of seq2seq/transformer models on AMR graphs by providing a text-like input format; facilitates reuse of pretrained language models; widely used as a baseline representation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Sequentialization can obscure explicit graph topology and long-range relations; may require stronger structure-aware encoders (GNNs/graph transformers) to fully recover structural information; canonical ordering is often not specified leading to variability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Simpler to use with vanilla seq2seq and pretrained LMs than graph encoders, but generally underperforms methods that explicitly encode graph structure (e.g., GNNs, dual encoders) on structural fidelity and some quality metrics according to the reviewed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7035.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7035.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDF triple sequence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDF triple serialization (as used in WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representation that flattens knowledge-graph RDF triples (subject, predicate, object) into a textual sequence used as input to text generation models (WebNLG uses DBpedia triples paired with reference texts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The WebNLG challenge: Generating text from RDF data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RDF triple sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes an input knowledge graph as a list/sequence of RDF triples (subject | predicate | object tokens or concatenated triple strings); each triple is represented textually and the set of triples is presented as a sequence to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential; token-based; typically treated as lossless with respect to the triples (but order may be lossy for discourse planning)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Triple-list / edge-list serialization (concatenate triples into a sequence); the reviewed work does not fix a single canonical traversal (ordering is often dataset-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (RDF triples from DBpedia)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RDF-to-text / knowledge-graph verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based seq2seq models and pretrained LMs (e.g., fine-tuned BART/T5), or models adapted to triples</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformers or seq2seq architectures trained to map serialized triple sequences to natural language descriptions; sometimes augmented with copy mechanisms or planning modules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, PARENT, human evaluation (task-specific metrics like PARENT commonly used for table/KG fidelity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides a straightforward supervised signal for KG verbalization tasks and is the canonical input format in WebNLG-style benchmarks; supports evaluation of generalization to seen/unseen domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Ordering ambiguity across triples can affect generation and evaluation; naive sequentialization may fail to capture multi-hop or global graph structure without additional modeling; possible loss of structural context if no graph-aware encoder is used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Easier to feed to vanilla seq2seq models than graph-encoding methods, but graph-aware encoders / joint graph-text pretraining often yield better structural fidelity and reduced hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7035.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7035.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph linearization (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph linearization / sequence serialization of graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of approaches that serializes arbitrary graphs (AMR, KGs) into linear token sequences (via edge lists, bracketed notations, or path-based encodings) so they can be consumed by sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph linearization (edge-list / path / bracket serializations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transforms graph nodes and edges into a one-dimensional token sequence using some serialization scheme (e.g., edge lists, path concatenation, bracketed forms); representation often uses special separators or role labels to demarcate relations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential; token-based; can be lossless or lossy depending on encoding details</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Various traversals (DFS/BFS/path-based/edge-list) or bracketed conversions; the review notes linearization was commonly used historically to fit seq2seq models but does not standardize a single algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR datasets, RDF/KG datasets (used across multiple benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph-to-text generation, AMR-to-text, KG verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN seq2seq, transformer seq2seq models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sequence models trained to map linearized graph sequences to reference natural language outputs; often enhanced with copy mechanisms or attention variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, ROUGE, human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Permits reuse of off-the-shelf seq2seq and pretrained LMs; simplifies input preprocessing but can limit structural signal available to model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>May obscure graph topology and multi-hop relations; sensitive to serialization order; can increase sequence length and thus token budget; typically inferior to direct graph encoders for preserving graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared with graph-encoder approaches, linearization is simpler but often less effective at capturing relational structure; hybrid approaches (dual encodings) can alleviate some drawbacks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7035.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7035.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph encoder (GNN/GCN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Neural Network encoders (GCN/GAT/Graph Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoders that operate directly on graph structure (nodes and edges) via message passing or graph-aware attention to produce node/graph embeddings used by a textual decoder to generate sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Graph-to-Sequence Model for AMR-to-Text Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph encoder (GNN/GCN/GAT/Graph Transformer) representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes nodes and edges into continuous vector representations using graph convolution or graph-attention message passing; the decoder consumes these embeddings to produce textual sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-aware (structural); continuous embedding-based; hierarchical possibility (node/graph levels)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Message passing / graph convolution / graph-attention across adjacency structure (no linearization required for the encoder stage)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR datasets (LDC variants), some KG/RDF benchmarks (WebNLG) when authors use graph encoders</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text, graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GCN, GAT, Graph Transformer; GNN encoders coupled with RNN/Transformer decoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graph neural networks produce structural embeddings that preserve adjacency and edge labels; paired with a standard text decoder to generate fluent text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, BERTScore, human evaluation, information-extraction metrics (CS/CO/RG)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported improvements in text quality and semantic/factual fidelity versus naive sequence linearization; better captures structural relations and supports more faithful generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Higher computational complexity; integration with pretrained LMs can be nontrivial; may require alignment/planning modules to bridge encoder-decoder structural mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms plain linearization seq2seq baselines in many AMR-to-text studies; dual or hybrid encoders can further improve results vs single GNN encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7035.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7035.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual graph representations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual graph representations (top-down and bottom-up AMR encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that encodes AMR graph structure from complementary perspectives (e.g., top-down and bottom-up node representations) to capture diverse structural information for AMR-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhancing AMR-to-text generation with dual graph representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Dual graph representations (top-down + bottom-up)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Simultaneously learns two complementary node representation views (e.g., top-down and bottom-up) of the same AMR graph and fuses them for decoding into text to better capture hierarchical and relational cues.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hybrid/hierarchical; graph-aware; embedding-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Compute two sets of node embeddings with different directional/message-passing schemes (top-down and bottom-up) and combine them prior to decoding</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR datasets (e.g., LDC2017T10)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dual-encoder GNN model (dual graph encoders + decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two graph encoders produce complementary representations that are fused and provided to a text decoder; designed to capture different structural aspects of AMR graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU and human-oriented text quality metrics (reported qualitatively as improvements over single encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported to improve generated text quality relative to single-encoder baselines by providing richer structural signals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Increased model complexity and resource use; fusion of dual views requires careful design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Shown in the review to outperform single-encoder (single GNN) approaches on AMR-to-text quality in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7035.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7035.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DUALENC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DUALENC (dual encoding model integrating graph and linear structures)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid encoder that integrates graph-structured encoding (GCN/graph encoder) with a linearized sequence encoder plus an intermediate content-planning stage to bridge structural mismatch between the encoder and decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridging the structural gap between encoding and decoding for data-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>DUALENC (graph + linear) representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Combines an encoder that processes the graph structure (e.g., GCN) with another encoder that processes a linearized version of the same input; an intermediate content planning step aligns the two representations for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hybrid (graph + sequential); embedding-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Parallel graph convolutional encoding and linear sequence encoding followed by content planning to reconcile representations before decoding</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR and other structured-input datasets (as used in cited study)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Data-to-text / AMR-to-text / graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DUALENC (GCN encoder + linear encoder + planner + decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A dual-encoder architecture that explicitly models both the graph topology and a compatible linear representation, using planning to improve decoder compatibility and generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, human evaluation, information-extraction metrics (as reported in the cited study)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported to substantially improve text quality and structural compatibility compared to single-encoder baselines in the reviewed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Architecturally more complex and requires design choices for planning and fusion; heavier computational requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms single-encoder (graph-only or linear-only) models by bridging the encoder-decoder structural gap, per cited comparisons in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7035.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7035.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-to-Sequence (Graph2Seq)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Graph-to-Sequence Model for AMR-to-Text Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of models that encode the full graph via a graph encoder and directly decode a target token sequence (text); presented concretely in work titled 'A Graph-to-Sequence Model for AMR-to-Text Generation'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Graph-to-Sequence Model for AMR-to-Text Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-to-Sequence representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph structure is encoded by a graph neural encoder into node/graph-level embeddings; a sequence decoder attends to those embeddings and generates textual output token-by-token.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-aware (lossless regarding topology in embeddings); embedding-based with sequential decoding</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph encoding via GNN followed by autoregressive decoding into a sequence</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text / graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph2Seq models (GCN/GAT encoders + RNN/Transformer decoders)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GNN encoders compute structural embeddings which a sequence decoder uses to generate text; designed to exploit graph topology directly rather than via linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, human evaluation, task-specific IE metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported to better capture graph relations and improve fidelity vs linearized seq2seq baselines in cited studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Integration complexity, increased training cost, and potential mismatch with pretrained text-only decoders without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Generally preferred over naive linearization for preserving structural information; however, hybrid and adapter-based methods may further improve compatibility with pretrained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7035.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7035.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR-to-Text Generation with Graph Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based encoder variant that incorporates graph structure into self-attention (e.g., via edge-aware attention or positional/edge encodings) to encode graphs for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Amr-to-text generation with graph transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Transformer representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Extends Transformer architectures with graph-aware attention mechanisms or edge/position encodings so attention respects adjacency and labeled relations in the input graph; decoder produces text sequentially.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-aware; embedding-based; sequential decoding</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph-aware self-attention (edge-conditioned attention, relative/structural positional encodings) in transformer encoder</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph Transformer (graph-aware transformer encoder + decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder variant that injects graph topology into attention computations, paired with standard decoder to generate texts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, BERTScore, human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported to model graph structure more directly than linearization and performs well for AMR-to-text tasks; compatible with transformer-based pretrained initializations if adapted.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Additional model complexity; engineering cost to represent edges and relations inside transformer attention.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Often outperforms plain transformer on linearized inputs and can rival GNN approaches while leveraging transformer benefits; exact gains are task- and implementation-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7035.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7035.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STRUCTADAPT / Structural Adapters</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Adapters in Pretrained Language Models for AMR-to-Text Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adapter modules inserted into pretrained language models to inject graph structural information (AMR) into PLMs without full model finetuning, enabling better AMR-to-text performance with parameter-efficient modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structural Adapters in Pretrained Language Models for AMR-to-Text Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structural adapters (adapter-based structural injection)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Adapter modules are trained/inserted into PLMs to incorporate graph structure (e.g., by transforming graph-derived embeddings) into the PLM representations used for decoding, allowing PLMs to better consume graph inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>adapter-based hybrid; parameter-efficient; graph-aware</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph structure is encoded externally (e.g., via a GNN) and injected into PLM layers through adapter modules or structural adapters during fine-tuning/pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PLMs (e.g., BART/T5) augmented with structural adapters</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained language models augmented with small adapter modules that carry structural signals; only adapters need training while base PLM weights can remain fixed or lightly tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, BERTScore, human evaluation (reported improvements over vanilla PLM finetuning in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Improves utilization of graph structure by PLMs while being parameter-efficient; reported gains in AMR-to-text performance relative to naive PLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires design and training of adapter modules; effectiveness depends on adapter placement and how graph embeddings are fused.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Offers a middle ground between full graph-encoder architectures and naive PLM finetuning; often better than plain PLM finetuning and more efficient than full graph-encoder integration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7035.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7035.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JointGT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A joint representation learning approach that jointly models graph and text representations (pretraining tasks) to improve text generation from knowledge graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Joint graph-text representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Learns joint embeddings for graph elements and textual tokens through pretraining tasks that align graph structures and text, enabling downstream KG-to-text generation from learned joint representations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>joint/paired graph-text; embedding-based; pretraining-focused</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Pretraining objectives that align graph encodings and textual encodings (joint loss), enabling a single model to ingest graph structure and produce text</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Knowledge graph datasets (e.g., WebNLG-style data)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-graph-to-text generation / KG verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>JointGT model (joint pretraining architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A model trained with joint graph-text objectives to learn cross-modal representations useful for KG-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, PARENT, human evaluation (KG-to-text metrics emphasized)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Pretraining with joint graph-text objectives reported to improve downstream generation quality and fidelity compared to models without such joint pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires parallel or weakly aligned graph-text corpora for pretraining; additional pretraining cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to single-modality pretraining or naive finetuning, joint pretraining improves graph-to-text alignment and factual fidelity in cited results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7035.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7035.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>P2 (Plan-and-Pretrain)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>P2: A Plan-and-Pretrain Approach for Knowledge Graph-to-Text Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that combines content planning (explicit plan generation) with pretraining on KG-to-text corpora to improve knowledge-graph verbalization and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>P 2 : A Planand-Pretrain Approach for Knowledge Graph-to-Text Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Plan-and-Pretrain (P2) representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generates an intermediate content plan from the graph and uses pretraining on graph-text pairs to better align planning and surface realization stages in end-to-end KG-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>two-stage (planning + sequential decoding); hierarchical/sequential</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Content planning step (derives ordered substructure or plan) followed by pretrained LM finetuning to realize the plan as text; pretraining on KG-text corpora is used.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG and other KG-to-text datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-graph-to-text generation with explicit planning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>P2 model (planner + pretrained LM for realization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline that first produces a content plan from graph input and then a pretrained language model realizes the plan into fluent text; leverages pretraining to improve planning-to-realization mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, PARENT, human evaluation (improvements reported in cited study)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported to improve generalization and faithfulness by separating planning and realization and by leveraging pretraining on KG-text data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires supervision or heuristics for content planning and extra pretraining resources; increases pipeline complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms simple end-to-end finetuning on some KG-to-text benchmarks by improving content selection and factual fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7035.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7035.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Pre-training for AMR Parsing and Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretraining strategies that operate on graph-structured data (AMR) to produce initializations beneficial for downstream AMR parsing and AMR-to-text generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph Pre-training for AMR Parsing and Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph pre-training representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Models are pretrained with objectives defined over graph structures (e.g., graph reconstruction, node prediction, masked graph modeling) to produce representations that transfer to downstream AMR-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>pretraining-focused; graph-aware; embedding-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph-based pretraining objectives on large corpora of graphs prior to supervised finetuning on AMR-to-text generation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Large AMR corpora used for pretraining; downstream evaluated on AMR LDC sets</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR parsing and AMR-to-text generation (pretraining stage)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph-pretrained encoders (then paired with decoders)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoders pretrained on graph tasks to learn structural representations that improve downstream generation performance when fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, downstream AMR parsing/generation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Pretraining on graphs reported to improve downstream AMR generation and parsing robustness and quality compared to models without graph-specific pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires sufficiently large graph corpora and pretraining compute; benefits depend on pretraining-task alignment with downstream objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Graph pretraining often yields gains over models initialized from text-only pretraining when tasks are heavily graph-structured (as reported in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Penman: An open-source library and tool for amr graphs. <em>(Rating: 2)</em></li>
                <li>The WebNLG challenge: Generating text from RDF data. <em>(Rating: 2)</em></li>
                <li>A Graph-to-Sequence Model for AMR-to-Text Generation. <em>(Rating: 2)</em></li>
                <li>Enhancing AMR-to-text generation with dual graph representations. <em>(Rating: 2)</em></li>
                <li>Bridging the structural gap between encoding and decoding for data-to-text generation. <em>(Rating: 2)</em></li>
                <li>Structural Adapters in Pretrained Language Models for AMR-to-Text Generation. <em>(Rating: 2)</em></li>
                <li>JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs. <em>(Rating: 2)</em></li>
                <li>P 2 : A Planand-Pretrain Approach for Knowledge Graph-to-Text Generation. <em>(Rating: 2)</em></li>
                <li>Graph Pre-training for AMR Parsing and Generation. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7035",
    "paper_id": "paper-267637261",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "PENMAN linearization",
            "name_full": "PENMAN AMR linearization / PENMAN notation",
            "brief_description": "A textual serialization format for AMR graphs that represents nodes and labeled edges in a bracketed, PENMAN-style sequence; used in AMR datasets (e.g., LDC2017T10) to convert graph structures into linear text for seq2seq training.",
            "citation_title": "Penman: An open-source library and tool for amr graphs.",
            "mention_or_use": "mention",
            "representation_name": "PENMAN AMR linearization",
            "representation_description": "Encodes an AMR graph as a bracketed token sequence with node concept labels and labeled relations represented as parenthesized structures (PENMAN notation). Each node and its outgoing edges are serialized into textual brackets and token sequences.",
            "representation_type": "sequential; token-based; (generally lossless in principle for AMR if full PENMAN preserved)",
            "encoding_method": "PENMAN bracketed serialization (graph â†’ bracketed linear sequence); no specific traversal algorithm (e.g., DFS/BFS) is specified in the reviewed paper",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR (e.g., LDC2017T10)",
            "task_name": "AMR-to-text generation / graph-to-text generation",
            "model_name": "Sequence-to-sequence models and pretrained transformers (e.g., RNN seq2seq, BART/T5 when fine-tuned on PENMAN)",
            "model_description": "Seq2seq or transformer models take PENMAN-token sequences as input and are trained to map them to natural language sentences; often used to enable application of pretrained LMs to AMR-to-text.",
            "performance_metric": "BLEU, METEOR, BERTScore, human evaluation (metrics commonly reported for AMR-to-text)",
            "performance_value": null,
            "impact_on_training": "Enables direct training of seq2seq/transformer models on AMR graphs by providing a text-like input format; facilitates reuse of pretrained language models; widely used as a baseline representation.",
            "limitations": "Sequentialization can obscure explicit graph topology and long-range relations; may require stronger structure-aware encoders (GNNs/graph transformers) to fully recover structural information; canonical ordering is often not specified leading to variability.",
            "comparison_with_other": "Simpler to use with vanilla seq2seq and pretrained LMs than graph encoders, but generally underperforms methods that explicitly encode graph structure (e.g., GNNs, dual encoders) on structural fidelity and some quality metrics according to the reviewed literature.",
            "uuid": "e7035.0",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RDF triple sequence",
            "name_full": "RDF triple serialization (as used in WebNLG)",
            "brief_description": "Representation that flattens knowledge-graph RDF triples (subject, predicate, object) into a textual sequence used as input to text generation models (WebNLG uses DBpedia triples paired with reference texts).",
            "citation_title": "The WebNLG challenge: Generating text from RDF data.",
            "mention_or_use": "mention",
            "representation_name": "RDF triple sequence",
            "representation_description": "Encodes an input knowledge graph as a list/sequence of RDF triples (subject | predicate | object tokens or concatenated triple strings); each triple is represented textually and the set of triples is presented as a sequence to the model.",
            "representation_type": "sequential; token-based; typically treated as lossless with respect to the triples (but order may be lossy for discourse planning)",
            "encoding_method": "Triple-list / edge-list serialization (concatenate triples into a sequence); the reviewed work does not fix a single canonical traversal (ordering is often dataset-dependent)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (RDF triples from DBpedia)",
            "task_name": "RDF-to-text / knowledge-graph verbalization",
            "model_name": "Transformer-based seq2seq models and pretrained LMs (e.g., fine-tuned BART/T5), or models adapted to triples",
            "model_description": "Transformers or seq2seq architectures trained to map serialized triple sequences to natural language descriptions; sometimes augmented with copy mechanisms or planning modules.",
            "performance_metric": "BLEU, METEOR, PARENT, human evaluation (task-specific metrics like PARENT commonly used for table/KG fidelity)",
            "performance_value": null,
            "impact_on_training": "Provides a straightforward supervised signal for KG verbalization tasks and is the canonical input format in WebNLG-style benchmarks; supports evaluation of generalization to seen/unseen domains.",
            "limitations": "Ordering ambiguity across triples can affect generation and evaluation; naive sequentialization may fail to capture multi-hop or global graph structure without additional modeling; possible loss of structural context if no graph-aware encoder is used.",
            "comparison_with_other": "Easier to feed to vanilla seq2seq models than graph-encoding methods, but graph-aware encoders / joint graph-text pretraining often yield better structural fidelity and reduced hallucination.",
            "uuid": "e7035.1",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Graph linearization (general)",
            "name_full": "Graph linearization / sequence serialization of graphs",
            "brief_description": "A class of approaches that serializes arbitrary graphs (AMR, KGs) into linear token sequences (via edge lists, bracketed notations, or path-based encodings) so they can be consumed by sequence models.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation.",
            "mention_or_use": "mention",
            "representation_name": "Graph linearization (edge-list / path / bracket serializations)",
            "representation_description": "Transforms graph nodes and edges into a one-dimensional token sequence using some serialization scheme (e.g., edge lists, path concatenation, bracketed forms); representation often uses special separators or role labels to demarcate relations.",
            "representation_type": "sequential; token-based; can be lossless or lossy depending on encoding details",
            "encoding_method": "Various traversals (DFS/BFS/path-based/edge-list) or bracketed conversions; the review notes linearization was commonly used historically to fit seq2seq models but does not standardize a single algorithm.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR datasets, RDF/KG datasets (used across multiple benchmarks)",
            "task_name": "Graph-to-text generation, AMR-to-text, KG verbalization",
            "model_name": "RNN seq2seq, transformer seq2seq models",
            "model_description": "Sequence models trained to map linearized graph sequences to reference natural language outputs; often enhanced with copy mechanisms or attention variants.",
            "performance_metric": "BLEU, METEOR, ROUGE, human evaluation",
            "performance_value": null,
            "impact_on_training": "Permits reuse of off-the-shelf seq2seq and pretrained LMs; simplifies input preprocessing but can limit structural signal available to model.",
            "limitations": "May obscure graph topology and multi-hop relations; sensitive to serialization order; can increase sequence length and thus token budget; typically inferior to direct graph encoders for preserving graph structure.",
            "comparison_with_other": "Compared with graph-encoder approaches, linearization is simpler but often less effective at capturing relational structure; hybrid approaches (dual encodings) can alleviate some drawbacks.",
            "uuid": "e7035.2",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Graph encoder (GNN/GCN)",
            "name_full": "Graph Neural Network encoders (GCN/GAT/Graph Transformer)",
            "brief_description": "Encoders that operate directly on graph structure (nodes and edges) via message passing or graph-aware attention to produce node/graph embeddings used by a textual decoder to generate sequences.",
            "citation_title": "A Graph-to-Sequence Model for AMR-to-Text Generation.",
            "mention_or_use": "mention",
            "representation_name": "Graph encoder (GNN/GCN/GAT/Graph Transformer) representation",
            "representation_description": "Encodes nodes and edges into continuous vector representations using graph convolution or graph-attention message passing; the decoder consumes these embeddings to produce textual sequences.",
            "representation_type": "graph-aware (structural); continuous embedding-based; hierarchical possibility (node/graph levels)",
            "encoding_method": "Message passing / graph convolution / graph-attention across adjacency structure (no linearization required for the encoder stage)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR datasets (LDC variants), some KG/RDF benchmarks (WebNLG) when authors use graph encoders",
            "task_name": "AMR-to-text, graph-to-text generation",
            "model_name": "GCN, GAT, Graph Transformer; GNN encoders coupled with RNN/Transformer decoders",
            "model_description": "Graph neural networks produce structural embeddings that preserve adjacency and edge labels; paired with a standard text decoder to generate fluent text.",
            "performance_metric": "BLEU, METEOR, BERTScore, human evaluation, information-extraction metrics (CS/CO/RG)",
            "performance_value": null,
            "impact_on_training": "Reported improvements in text quality and semantic/factual fidelity versus naive sequence linearization; better captures structural relations and supports more faithful generation.",
            "limitations": "Higher computational complexity; integration with pretrained LMs can be nontrivial; may require alignment/planning modules to bridge encoder-decoder structural mismatch.",
            "comparison_with_other": "Outperforms plain linearization seq2seq baselines in many AMR-to-text studies; dual or hybrid encoders can further improve results vs single GNN encoders.",
            "uuid": "e7035.3",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Dual graph representations",
            "name_full": "Dual graph representations (top-down and bottom-up AMR encodings)",
            "brief_description": "An approach that encodes AMR graph structure from complementary perspectives (e.g., top-down and bottom-up node representations) to capture diverse structural information for AMR-to-text generation.",
            "citation_title": "Enhancing AMR-to-text generation with dual graph representations.",
            "mention_or_use": "mention",
            "representation_name": "Dual graph representations (top-down + bottom-up)",
            "representation_description": "Simultaneously learns two complementary node representation views (e.g., top-down and bottom-up) of the same AMR graph and fuses them for decoding into text to better capture hierarchical and relational cues.",
            "representation_type": "hybrid/hierarchical; graph-aware; embedding-based",
            "encoding_method": "Compute two sets of node embeddings with different directional/message-passing schemes (top-down and bottom-up) and combine them prior to decoding",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR datasets (e.g., LDC2017T10)",
            "task_name": "AMR-to-text generation",
            "model_name": "Dual-encoder GNN model (dual graph encoders + decoder)",
            "model_description": "Two graph encoders produce complementary representations that are fused and provided to a text decoder; designed to capture different structural aspects of AMR graphs.",
            "performance_metric": "BLEU and human-oriented text quality metrics (reported qualitatively as improvements over single encoders)",
            "performance_value": null,
            "impact_on_training": "Reported to improve generated text quality relative to single-encoder baselines by providing richer structural signals.",
            "limitations": "Increased model complexity and resource use; fusion of dual views requires careful design.",
            "comparison_with_other": "Shown in the review to outperform single-encoder (single GNN) approaches on AMR-to-text quality in cited work.",
            "uuid": "e7035.4",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DUALENC",
            "name_full": "DUALENC (dual encoding model integrating graph and linear structures)",
            "brief_description": "A hybrid encoder that integrates graph-structured encoding (GCN/graph encoder) with a linearized sequence encoder plus an intermediate content-planning stage to bridge structural mismatch between the encoder and decoder.",
            "citation_title": "Bridging the structural gap between encoding and decoding for data-to-text generation.",
            "mention_or_use": "mention",
            "representation_name": "DUALENC (graph + linear) representation",
            "representation_description": "Combines an encoder that processes the graph structure (e.g., GCN) with another encoder that processes a linearized version of the same input; an intermediate content planning step aligns the two representations for decoding.",
            "representation_type": "hybrid (graph + sequential); embedding-based",
            "encoding_method": "Parallel graph convolutional encoding and linear sequence encoding followed by content planning to reconcile representations before decoding",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR and other structured-input datasets (as used in cited study)",
            "task_name": "Data-to-text / AMR-to-text / graph-to-text generation",
            "model_name": "DUALENC (GCN encoder + linear encoder + planner + decoder)",
            "model_description": "A dual-encoder architecture that explicitly models both the graph topology and a compatible linear representation, using planning to improve decoder compatibility and generation quality.",
            "performance_metric": "BLEU, human evaluation, information-extraction metrics (as reported in the cited study)",
            "performance_value": null,
            "impact_on_training": "Reported to substantially improve text quality and structural compatibility compared to single-encoder baselines in the reviewed literature.",
            "limitations": "Architecturally more complex and requires design choices for planning and fusion; heavier computational requirements.",
            "comparison_with_other": "Outperforms single-encoder (graph-only or linear-only) models by bridging the encoder-decoder structural gap, per cited comparisons in the review.",
            "uuid": "e7035.5",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Graph-to-Sequence (Graph2Seq)",
            "name_full": "A Graph-to-Sequence Model for AMR-to-Text Generation",
            "brief_description": "A class of models that encode the full graph via a graph encoder and directly decode a target token sequence (text); presented concretely in work titled 'A Graph-to-Sequence Model for AMR-to-Text Generation'.",
            "citation_title": "A Graph-to-Sequence Model for AMR-to-Text Generation.",
            "mention_or_use": "mention",
            "representation_name": "Graph-to-Sequence representation",
            "representation_description": "Graph structure is encoded by a graph neural encoder into node/graph-level embeddings; a sequence decoder attends to those embeddings and generates textual output token-by-token.",
            "representation_type": "graph-aware (lossless regarding topology in embeddings); embedding-based with sequential decoding",
            "encoding_method": "Graph encoding via GNN followed by autoregressive decoding into a sequence",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR datasets",
            "task_name": "AMR-to-text / graph-to-text generation",
            "model_name": "Graph2Seq models (GCN/GAT encoders + RNN/Transformer decoders)",
            "model_description": "GNN encoders compute structural embeddings which a sequence decoder uses to generate text; designed to exploit graph topology directly rather than via linearization.",
            "performance_metric": "BLEU, METEOR, human evaluation, task-specific IE metrics",
            "performance_value": null,
            "impact_on_training": "Reported to better capture graph relations and improve fidelity vs linearized seq2seq baselines in cited studies.",
            "limitations": "Integration complexity, increased training cost, and potential mismatch with pretrained text-only decoders without adaptation.",
            "comparison_with_other": "Generally preferred over naive linearization for preserving structural information; however, hybrid and adapter-based methods may further improve compatibility with pretrained LMs.",
            "uuid": "e7035.6",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Graph Transformer",
            "name_full": "AMR-to-Text Generation with Graph Transformer",
            "brief_description": "A transformer-based encoder variant that incorporates graph structure into self-attention (e.g., via edge-aware attention or positional/edge encodings) to encode graphs for text generation.",
            "citation_title": "Amr-to-text generation with graph transformer.",
            "mention_or_use": "mention",
            "representation_name": "Graph Transformer representation",
            "representation_description": "Extends Transformer architectures with graph-aware attention mechanisms or edge/position encodings so attention respects adjacency and labeled relations in the input graph; decoder produces text sequentially.",
            "representation_type": "graph-aware; embedding-based; sequential decoding",
            "encoding_method": "Graph-aware self-attention (edge-conditioned attention, relative/structural positional encodings) in transformer encoder",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR datasets",
            "task_name": "AMR-to-text generation",
            "model_name": "Graph Transformer (graph-aware transformer encoder + decoder)",
            "model_description": "Transformer encoder variant that injects graph topology into attention computations, paired with standard decoder to generate texts.",
            "performance_metric": "BLEU, METEOR, BERTScore, human evaluation",
            "performance_value": null,
            "impact_on_training": "Reported to model graph structure more directly than linearization and performs well for AMR-to-text tasks; compatible with transformer-based pretrained initializations if adapted.",
            "limitations": "Additional model complexity; engineering cost to represent edges and relations inside transformer attention.",
            "comparison_with_other": "Often outperforms plain transformer on linearized inputs and can rival GNN approaches while leveraging transformer benefits; exact gains are task- and implementation-dependent.",
            "uuid": "e7035.7",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "STRUCTADAPT / Structural Adapters",
            "name_full": "Structural Adapters in Pretrained Language Models for AMR-to-Text Generation",
            "brief_description": "Adapter modules inserted into pretrained language models to inject graph structural information (AMR) into PLMs without full model finetuning, enabling better AMR-to-text performance with parameter-efficient modifications.",
            "citation_title": "Structural Adapters in Pretrained Language Models for AMR-to-Text Generation.",
            "mention_or_use": "mention",
            "representation_name": "Structural adapters (adapter-based structural injection)",
            "representation_description": "Adapter modules are trained/inserted into PLMs to incorporate graph structure (e.g., by transforming graph-derived embeddings) into the PLM representations used for decoding, allowing PLMs to better consume graph inputs.",
            "representation_type": "adapter-based hybrid; parameter-efficient; graph-aware",
            "encoding_method": "Graph structure is encoded externally (e.g., via a GNN) and injected into PLM layers through adapter modules or structural adapters during fine-tuning/pretraining",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR datasets",
            "task_name": "AMR-to-text generation",
            "model_name": "PLMs (e.g., BART/T5) augmented with structural adapters",
            "model_description": "Pretrained language models augmented with small adapter modules that carry structural signals; only adapters need training while base PLM weights can remain fixed or lightly tuned.",
            "performance_metric": "BLEU, METEOR, BERTScore, human evaluation (reported improvements over vanilla PLM finetuning in cited works)",
            "performance_value": null,
            "impact_on_training": "Improves utilization of graph structure by PLMs while being parameter-efficient; reported gains in AMR-to-text performance relative to naive PLM fine-tuning.",
            "limitations": "Requires design and training of adapter modules; effectiveness depends on adapter placement and how graph embeddings are fused.",
            "comparison_with_other": "Offers a middle ground between full graph-encoder architectures and naive PLM finetuning; often better than plain PLM finetuning and more efficient than full graph-encoder integration.",
            "uuid": "e7035.8",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "JointGT",
            "name_full": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
            "brief_description": "A joint representation learning approach that jointly models graph and text representations (pretraining tasks) to improve text generation from knowledge graphs.",
            "citation_title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs.",
            "mention_or_use": "mention",
            "representation_name": "Joint graph-text representation",
            "representation_description": "Learns joint embeddings for graph elements and textual tokens through pretraining tasks that align graph structures and text, enabling downstream KG-to-text generation from learned joint representations.",
            "representation_type": "joint/paired graph-text; embedding-based; pretraining-focused",
            "encoding_method": "Pretraining objectives that align graph encodings and textual encodings (joint loss), enabling a single model to ingest graph structure and produce text",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Knowledge graph datasets (e.g., WebNLG-style data)",
            "task_name": "Knowledge-graph-to-text generation / KG verbalization",
            "model_name": "JointGT model (joint pretraining architecture)",
            "model_description": "A model trained with joint graph-text objectives to learn cross-modal representations useful for KG-to-text generation.",
            "performance_metric": "BLEU, PARENT, human evaluation (KG-to-text metrics emphasized)",
            "performance_value": null,
            "impact_on_training": "Pretraining with joint graph-text objectives reported to improve downstream generation quality and fidelity compared to models without such joint pretraining.",
            "limitations": "Requires parallel or weakly aligned graph-text corpora for pretraining; additional pretraining cost.",
            "comparison_with_other": "Compared to single-modality pretraining or naive finetuning, joint pretraining improves graph-to-text alignment and factual fidelity in cited results.",
            "uuid": "e7035.9",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "P2 (Plan-and-Pretrain)",
            "name_full": "P2: A Plan-and-Pretrain Approach for Knowledge Graph-to-Text Generation",
            "brief_description": "An approach that combines content planning (explicit plan generation) with pretraining on KG-to-text corpora to improve knowledge-graph verbalization and generalization.",
            "citation_title": "P 2 : A Planand-Pretrain Approach for Knowledge Graph-to-Text Generation.",
            "mention_or_use": "mention",
            "representation_name": "Plan-and-Pretrain (P2) representation",
            "representation_description": "Generates an intermediate content plan from the graph and uses pretraining on graph-text pairs to better align planning and surface realization stages in end-to-end KG-to-text generation.",
            "representation_type": "two-stage (planning + sequential decoding); hierarchical/sequential",
            "encoding_method": "Content planning step (derives ordered substructure or plan) followed by pretrained LM finetuning to realize the plan as text; pretraining on KG-text corpora is used.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG and other KG-to-text datasets",
            "task_name": "Knowledge-graph-to-text generation with explicit planning",
            "model_name": "P2 model (planner + pretrained LM for realization)",
            "model_description": "A pipeline that first produces a content plan from graph input and then a pretrained language model realizes the plan into fluent text; leverages pretraining to improve planning-to-realization mapping.",
            "performance_metric": "BLEU, PARENT, human evaluation (improvements reported in cited study)",
            "performance_value": null,
            "impact_on_training": "Reported to improve generalization and faithfulness by separating planning and realization and by leveraging pretraining on KG-text data.",
            "limitations": "Requires supervision or heuristics for content planning and extra pretraining resources; increases pipeline complexity.",
            "comparison_with_other": "Outperforms simple end-to-end finetuning on some KG-to-text benchmarks by improving content selection and factual fidelity.",
            "uuid": "e7035.10",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Graph pre-training",
            "name_full": "Graph Pre-training for AMR Parsing and Generation",
            "brief_description": "Pretraining strategies that operate on graph-structured data (AMR) to produce initializations beneficial for downstream AMR parsing and AMR-to-text generation tasks.",
            "citation_title": "Graph Pre-training for AMR Parsing and Generation.",
            "mention_or_use": "mention",
            "representation_name": "Graph pre-training representation",
            "representation_description": "Models are pretrained with objectives defined over graph structures (e.g., graph reconstruction, node prediction, masked graph modeling) to produce representations that transfer to downstream AMR-to-text generation.",
            "representation_type": "pretraining-focused; graph-aware; embedding-based",
            "encoding_method": "Graph-based pretraining objectives on large corpora of graphs prior to supervised finetuning on AMR-to-text generation tasks",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Large AMR corpora used for pretraining; downstream evaluated on AMR LDC sets",
            "task_name": "AMR parsing and AMR-to-text generation (pretraining stage)",
            "model_name": "Graph-pretrained encoders (then paired with decoders)",
            "model_description": "Encoders pretrained on graph tasks to learn structural representations that improve downstream generation performance when fine-tuned.",
            "performance_metric": "BLEU, METEOR, downstream AMR parsing/generation metrics",
            "performance_value": null,
            "impact_on_training": "Pretraining on graphs reported to improve downstream AMR generation and parsing robustness and quality compared to models without graph-specific pretraining.",
            "limitations": "Requires sufficiently large graph corpora and pretraining compute; benefits depend on pretraining-task alignment with downstream objectives.",
            "comparison_with_other": "Graph pretraining often yields gains over models initialized from text-only pretraining when tasks are heavily graph-structured (as reported in the review).",
            "uuid": "e7035.11",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Penman: An open-source library and tool for amr graphs.",
            "rating": 2,
            "sanitized_title": "penman_an_opensource_library_and_tool_for_amr_graphs"
        },
        {
            "paper_title": "The WebNLG challenge: Generating text from RDF data.",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation.",
            "rating": 2,
            "sanitized_title": "a_graphtosequence_model_for_amrtotext_generation"
        },
        {
            "paper_title": "Enhancing AMR-to-text generation with dual graph representations.",
            "rating": 2,
            "sanitized_title": "enhancing_amrtotext_generation_with_dual_graph_representations"
        },
        {
            "paper_title": "Bridging the structural gap between encoding and decoding for data-to-text generation.",
            "rating": 2,
            "sanitized_title": "bridging_the_structural_gap_between_encoding_and_decoding_for_datatotext_generation"
        },
        {
            "paper_title": "Structural Adapters in Pretrained Language Models for AMR-to-Text Generation.",
            "rating": 2,
            "sanitized_title": "structural_adapters_in_pretrained_language_models_for_amrtotext_generation"
        },
        {
            "paper_title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs.",
            "rating": 2,
            "sanitized_title": "jointgt_graphtext_joint_representation_learning_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "P 2 : A Planand-Pretrain Approach for Knowledge Graph-to-Text Generation.",
            "rating": 2,
            "sanitized_title": "p_2_a_planandpretrain_approach_for_knowledge_graphtotext_generation"
        },
        {
            "paper_title": "Graph Pre-training for AMR Parsing and Generation.",
            "rating": 2,
            "sanitized_title": "graph_pretraining_for_amr_parsing_and_generation"
        }
    ],
    "cost": 0.026929750000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Systematic Review of Data-to-Text NLG
27 Feb 2024</p>
<p>Chinonso Cynthia Osuji chinonso.osuji@adaptcentre.ie 
ADAPT Research Centre
Ireland</p>
<p>Dublin City University
Ireland</p>
<p>Thiago Castro Ferreira thiagocf05@ufmg.br 
Federal University of Minas Gerais (UFMG)
Brazil</p>
<p>Brian Davis brian.davis@adaptcentre.ie 
ADAPT Research Centre
Ireland</p>
<p>Dublin City University
Ireland</p>
<p>A Systematic Review of Data-to-Text NLG
27 Feb 2024D9FB667ED46F57A28D0F895F95FD6916arXiv:2402.08496v3[cs.CL]NLGD2TAMRMRMRSSQLRDF
This systematic review undertakes a comprehensive analysis of current research on data-to-text generation, identifying gaps, challenges, and future directions within the field.Relevant literature in this field on datasets, evaluation metrics, application areas, multilingualism, language models, and hallucination mitigation methods is reviewed.Various methods for producing high-quality text are explored, addressing the challenge of hallucinations in data-to-text generation.These methods include re-ranking, traditional and neural pipeline architecture, planning architectures, data cleaning, controlled generation, and modification of models and training techniques.Their effectiveness and limitations are assessed, highlighting the need for universally applicable strategies to mitigate hallucinations.The review also examines the usage, popularity, and impact of datasets, alongside evaluation metrics, with an emphasis on both automatic and human assessment.Additionally, the evolution of data-to-text models, particularly the widespread adoption of transformer models, is discussed.Despite advancements in text quality, the review emphasizes the importance of research in low-resourced languages and the engineering of datasets in these languages to promote inclusivity.Finally, several application domains of data-to-text are highlighted, emphasizing their relevance in such domains.Overall, this review serves as a guiding framework for fostering innovation and advancing data-to-text generation.</p>
<p>Introduction</p>
<p>"Natural Language Generation" (NLG) is a specific branch of artificial intelligence that deals with the conversion of non-linguistic data or information representations into text.Its primary objective is to develop computer systems capable of generating understandable and coherent text in human languages, such as English and helping to boost communication between humans and machines [1].Natural language generation techniques are used in summarization [2], text simplification [3], machine translation [4], image captioning [5], dialogue generation [6], and question answering [7; 8].As a subfield of natural language processing, its applications have evolved to accommodate two categories depending on the nature of the input: text-to-text generation and datato-text generation.Data-to-text is defined as the task of generating comprehensible texts from structured inputs [1].These structured inputs or data can be table records [9; 10], graphs [11; 12], charts [13], or databases [14].They can also be images, such as in image captioning, but we will focus on non-image structured data for this study.The data-to-text field aims to simplify complex data and provide easy comprehension and access to a broader, unspecialized, or specific audience [15; 16].</p>
<p>The initial approach to data-to-text generation employed a modular pipeline architecture.In this setup, each module was typically addressed using a rule-based method [1].However, owing to the progress technology has made over the years, with the introduction of powerful GPUs, large storage devices, and deep neural networks.The need to write only rules became obsolete since the model can understand and follow input patterns to generate desired text at a much faster computation rate.As a rapidly growing field of research, tasks such as weather forecasts [17], sports news reporting [18], financial reports [19], robo-journalism [12], health care [20], and autobiographies [21] etc., which require textual summaries from structured data, can now be potentially automated with this technology.</p>
<p>Overview</p>
<p>This section provides a concise overview of the diverse aspects of data-to-text systems, encompassing traditional, statistical, and contemporary neural approaches.Aiming to offer insights into the evolving trends and methodologies employed in the field of data-to-text generation.</p>
<p>Traditional Data-to-Text Systems</p>
<p>Traditional systems for data-to-text generation are commonly based on rules or utilize sets of templates created by humans.These templates include placeholders for slot values filled with dialogue inputs during execution [1; 15; 22; 23].In the process of templatization, natural language expressions are transformed into templates, where words directly representing data are replaced with slots according to rules derived from the text and consistencies in the data.The data-to-template generation is then applied to these templates, resulting in the generation of template sentence texts [23].However, the text generation process of this system is divided into distinct stages or modules, each dedicated to a specific task.Reiter and Dale [1] proposed a traditional five-module pipeline architecture for data-to-text generation, addressing the questions of "What to say?" and "How to say it?".These stages include:</p>
<ol>
<li>Content selection: Determines the information to be mentioned in the text.2. Content ordering: Arrange this information in their appropriate sequences in the text.3. Content aggregation/structuring: Organises this information in separate sentences and paragraphs.4. Lexicalization: Finds appropriate phrases or words that best relay the message in the sentence.5. Referring expression generation: Generates referring expressions (references, coreferences), like proper nouns, he, she, they, and him, to the entities in the text/discourse where necessary.6. Surface realization: It combines the output of all the other steps toward generating a complete text from the input data.</li>
</ol>
<p>This system provides built-in faithfulness to input, a carefully regulated style, and quick response times, rendering them an attractive option.Nevertheless, they need help in scalability since creating new templates for diverse responses is necessary, and templates from one domain may not consistently apply to other domains.Despite substantial time and resource investments to incorporate linguistic details into the templates, they often need more contextual understanding, and the restricted template set hampers the system's overall naturalness [22].</p>
<p>Statistical Data-to-Text Systems</p>
<p>Statistical systems for summarizing data into text employ probabilistic models, such as Hidden Markov Models (HMM) [9; 24] and alignment learning [25], to transform non-linguistic data into human-readable text.These models, specifically a probabilistic generative model, operate by concurrently segmenting text into utterances and mapping each utterance to a meaning representation grounded in the world state [25].</p>
<p>Operating on a probabilistic foundation, these models predict the most likely subsequent word in the target sequence based on the input data sequence.Generative models, designed to address multiple ambiguities with a focus on aligning utterances to facts, concentrate on the probability distribution for each word during alignment learning [25].This modeling of the intrinsic distribution of data points is achieved through joint probability, where the input and output coexist.The result is effective alignment of utterances to facts, minimizing the need for extensive supervision, adept handling of multiple ambiguities, and demonstration of generalizability across diverse domains [25].</p>
<p>Neural Data-to-Text Systems</p>
<p>In recent times, advances in generative models have offered fluent, more natural texts and data-driven scaling narratives as compared to traditional systems [22].Modern data-to-text generation involves the production of natural language text descriptions in sequences that explain non-linguistic data.Let D represent the data pairs {r j , s j } N j=1 of N instances of the data records r, mapped to its human generated summaries s in an n sequence of words (w 1 , ..., w n ).This process aims to map and learn a correlation between these data pairs and generate text based on these learned properties.Several techniques have been employed in learning these latent variables, some of which are the seq-to-seq model, seq-to-seq model with copy mechanism [26; 27], and the transformer attention models [28; 29].Notably, transformer attention models focus on specific properties deemed relevant during the text generation process.These properties are critical for capturing contextual relationships and enhancing the overall quality of generated text.</p>
<p>One of the significant problems in neural text generation is the occurrence of hallucinations, repetitions, omissions, inconsistencies, and a lack of coherence, which can compromise the quality and credibility of the content [7; 30].Furthermore, there is a need for more resources and datasets for languages other than English, which hinders the development and enhancement of models for these languages [7].Addressing these issues is critical for improving the accuracy and efficacy of language models in generating high-quality content.Improved deep learning models that prioritize error reduction need to be developed, and more diverse datasets and resources in multiple languages should be available to train and validate these models.</p>
<p>Related Surveys</p>
<p>A systematic review of the literature on data-to-text is required to encapsulate trends, find critical challenges and techniques, and fill in the information gaps.Several literature surveys in the field of NLG have tried to show the contributions made in the field, with studies focusing more on other text generation tasks, their training and generation methods [7], a systematic review of text generation tasks [31], its applications areas [15], hallucination and semantic adequacy measures [30; 32], evaluation metrics [33], and the evolution of deep learning models in NLG [34].A related study by Sharma et al. [35] captures the advancements in data-to-text techniques, datasets, and evaluation methods.In our study, we will expound on the existing literature, methods, languages, application areas, hallucination mitigation measures, and quality of the generated texts using structured data.</p>
<p>For this study, we will only consider structured data such as tables, RDF (Resource Description Framework) [36], knowledge bases or graphs [37], SQL (Structured Query Language) [14], AMR (Abstract Meaning Representation) [38], MR (Meaning Representation) [9], and MRS (Minimal Recursion Semantics) [39].We will also consider studies that focus on data-to-text generation and extract meaningful information about our research questions.</p>
<p>The first chapter focuses on the need for this systematic review and its structure, including how each chapter is organized.In Section 2, we will discuss the methodologies following the PRISMA 2020 [40] techniques for collecting and selecting relevant papers for this study.Section 3 discusses the results of the metadata extracted from the studies.The last sections 4, 5 and 6 offer discussions, recommendations and future directions, as well as the conclusion of the study.</p>
<p>Methodology</p>
<p>Our survey on data-to-text adopts a systematic review approach, following the guidance set forth in the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement and Page et al. [40] systematic review recommendations.We commence by formulating core research questions as the cornerstone of our investigation.Subsequently, we outline our search strategies and the electronic databases utilized for this purpose.Inclusion and exclusion criteria are applied to refine the paper selection, and we conclude by defining our procedures for data extraction and synthesis.</p>
<p>Research Questions</p>
<p>To gain a comprehensive understanding of the data-to-text domain and guide our research endeavors, we have formulated a primary research question.From this central query, we have derived a set of sub-questions, each illuminating a distinct facet of this domain.In the course of this paper, we will delve into these sub-questions in detail.Our primary research question is defined as follows: RQ: What does the existing literature in Natural Language Generation (NLG) reveal about text generation using structured data as input?Below, we present the derived sub-questions, each offering insight into specific aspects of our inquiry.These sub-questions will serve as the framework for our exploration: RQ1.Which standard datasets are commonly utilized for data-to-text generation in the literature?RQ2.Which languages are prevalent in data-to-text generation literature?RQ3.What are the techniques and design methods typically employed in data-to-text generation?RQ4.What measures are commonly employed to mitigate hallucinations in the generated text?RQ5.What are the prominent evaluation metrics used to assess the quality of generated texts?RQ6.Which application areas are explored in the context of data-to-text generation?</p>
<p>Search Strategies</p>
<p>We meticulously curated literature from various esteemed databases, focusing primarily on studies presented at conferences and published in journals renowned for their contributions to the field of Natural Language Generation.The information in Table 1 enumerates the conferences and journals pivotal to our research, while Figure 1b displays the categorization of venues as conferences or journals for the study publications.Our search efforts extended across various databases, including Google Scholar 1 , ACL anthology 2 , IEEE 3 , and Semantic Scholar 4 .To make the search more manageable, we downloaded a full Bibtex anthology with abstracts on the ACL anthology web page, deleted the bibliographies of studies less than 2017 and imported it into the Mendeley desktop application.Then, we implemented the search strategies on the bibliographies in the Mendeley desktop application and also conducted searches on other listed databases.It's worth highlighting that our database searches were conducted from September to October 2022.</p>
<p>Venues</p>
<p>Count Citations EMNLP 5  26 [29], [41], [42], [43], [44], [45], [46], [9], [14], [47], [2], [48], [49], [50], [51], [52], [53], [10], [54], [55], [56], [38], [57], [58], [59], [60] ACL 6  34 [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [24], [79], [80], [81], [82], [26], [83], [84], [39], [85], [86], [4], [87], [88], [89], [90] COLING 7  5 [91], [92], [93], [94]  11  13 [36], [98], [99], [27], [23], [100], [101], [102], [28], [103], [12], [104],</p>
<p>[105] NLP4ConvAI 12  1 [37] ISWC 13  1 [106] SIGGEN 14  4</p>
<p>[107], [108], [109], [110] Information Sciences 15  1</p>
<p>[16] Data Mining and Knowledge Discovery 16   1 [111] Table 1: Conference and Journal venues used for our search.</p>
<p>Study Screening and Selection</p>
<p>Through the application of our search strategies, we amassed a substantial corpus of 1,078 pieces of literature.However, our dataset settled at 1,052 unique studies after diligently purging duplicate entries.The inclusion and exclusion criteria for this study are outlined below.</p>
<p>Inclusion Criteria 1. Focus on data-to-text generation, even if other text generation forms are considered.2. Publication within the period from 2017 to 2022. 3. Publications in high-impact factor journals and conferences, spanning from A1-B2 and Q1-Q2 categories.4. Inclusion of survey papers and shared task papers in data-to-text generation. 5. Literature written exclusively in English.6. Minimum of 5 citations.The extensive dataset obtained through our search strategy underwent thorough filtering, during which we carefully applied our inclusion and exclusion criteria.This process whittled down our dataset to a more manageable 635 studies.Our commitment to precision persisted as we further refined our dataset with updated inclusion and exclusion criteria, as shown above.Simultaneously, we gathered citation data for each study using Google Scholar.As a result, we excluded 412 studies from the original 635, retaining a robust set of 223 studies, each with citations equal to or exceeding 5. Subsequently, an in-depth analysis of these 223 studies was undertaken through a comprehensive review of abstracts and contents.Papers that did not align with the eligibility criteria were meticulously filtered out by carefully reading the abstracts.In cases where abstracts were insufficient for determining eligibility, the entire paper was thoroughly examined.This meticulous process culminated in the selection of a final set of 90 papers for detailed examination and inclusion in our research.Notably, exclusions were made for papers not written in English, lacking human evaluation, or lacking a link to the code implementation.Exceptions were granted for 5 papers without human evaluation and 11 papers without code availability.Figure 1a illustrates the distribution of our papers across the chosen years.</p>
<p>Data Extraction and Synthesis</p>
<p>The data extracted from the 90 selected studies encompassed details such as the dataset used, the methodology employed, multilingual aspects, the evaluation metrics utilized, error mitigation strategies, and application areas.This information was systematically organized into a table and subjected to further analysis.The data synthesis, which relies on the information obtained during the extraction process, will be elaborated upon in the following section.Our categorization of datasets in the analysis encompasses eight distinct structured data types: Table, AMR, RDF, MR, SQL, Graph, JSON, and MRS.</p>
<p>Results</p>
<p>This section conducts an extensive analysis by synthesizing data from published papers.Detailed findings are presented, illuminating key insights and prevalent trends in the research landscape.Through rigorous examination, the section aims to provide a nuanced comprehension of the research findings and their broader implications.</p>
<p>Dataset</p>
<p>In this section, we comprehensively analyze the datasets utilized in the selected papers.We have identified a total of 63 distinct datasets used across these studies.Among these, WebNLG, E2E, AMR, RotoWire, WikiBio, ViGGO, ToTTo, and WMT are the most frequently employed datasets.WebNLG and E2E appear in 24 studies, followed by WikiBio and RotoWire, which is featured in 15 and 14 studies.Next is AMR-LDC2017T10, which are used in 12 studies, while AMR-LDC2015E86 and Wikipedia appears in 8 and 5 studies, and ViGGO and ToTTo are each used in 4 studies.Additionally, WMT, MLB and AMR-LDC2020T02 are each featured in 3 studies.The "Other" category includes datasets that occur only twice or once.For more details on these datasets and their occurrences, please refer to Table 2.</p>
<p>Having categorized our datasets into distinct types as displayed in Table 3, the most prevalent data type is the table, which is featured in 40 of the selected studies.Following closely are RDF, MR, and AMR, which are highlighted in 27, 26, and 17 papers, respectively, underscoring their pivotal roles in the realm of data-to-text generation research.In contrast, Graph, SQL, JSON, and MRS appear in only 5, 2, 1, and 1 papers, respectively, reflecting the diverse array of data sources that researchers harness in this field and emphasizing its inherent complexity and versatility.MR 24 [9], [98], [99], [48], [27], [68], [100], [102], [28], [11], [54], [91], [74], [58], [42], [85], [61], [77], [78], [105], [24], [63], [60] [65], [84], [87], [51], [55], [56], [38] LDC2017T10 AMR 12 [37], [84], [72], [2], [87], [91], [51], [59], [62], [56], [38], [ [46], [96], [10], [67], [70], [85], [47], [52], [16], [89], [101], [76], [43], [ [36], [27], [83], [49], [66], [11], [103], [71], [54], [107], [91], [74], [88], [108], [106], [109], [110], [104], [37], [61], [90], [44], [24],</p>
<p>[63] WikiBio Table 15 [9], [10], [66], [54], [85], [26], [29], [41], [53], [111], [11], [104], [82], [60] WikiSQL Table 2 [85], [14] WikiTableQuestions Table 1 [85] Wikipedia MR 5 [97], [39], [73], [53], [102] Table 2: Datasets Used in Selected Papers.Knowledge Graph (KG).</p>
<p>Data Type Frequency Citations Table 40 [46], [96], [9], [10], [66], [103], [67], [70], [54], [85], [47], [73], [52], [69], [82], [88], [16], [26], [92], [29], [41], [89], [53], [101], [111], [76], [44], [23], [97], [57], [11], [86], [45], [75], [42], [43], [60], [80], [22],</p>
<p>[104] RDF 27 [36] [83], [49], [103], [71], [54], [50], [107], [85], [91], [74], [88], [27], [61], [90], [44], [108], [57], [11], [95], [75], [106], [109], [110], [24], [63], [104] MR 26 [9], [103], [98], [54], [85], [99], [68], [48], [91], [74], [27], [61], [100], [102], [77], [28], [11], [86], [78], [58], [42], [105], [94], [24], [63],</p>
<p>[60] AMR 17 [64], [65], [4], [37], [84], [72], [2], [87], [91], [51], [59], [62], [55], [56], [38], [81], [79] Graph 5 [37], [90], [93], [12],
[11] SQL 2 [14], [85] JSON 1 [11] MRS 1 [39]
Table 3: Data type frequency.</p>
<p>Overview of Datasets</p>
<p>Here is a brief overview of some prevalent datasets mentioned in the previous section: WebNLG The WebNLG dataset [36], introduced in 2017, is a pivotal resource used in the WebNLG 2017 challenge.It contains RDF triples from DBPedia, each paired with text descriptions and human-generated reference texts in English.This dataset serves as a critical tool for training and evaluating the planner component.It comprises 9,674 unique triple sets and 25,298 text references, divided into training, development, and test sets.The test set includes both seen and unseen domains, allowing for the evaluation of model generalizability.Additionally, the WebNLG 2020 dataset [107] expands on this resource with Russian data.This addition was achieved through translation and post-editing, aimed at fostering multilingual capabilities and providing essential statistics for evaluating Natural Language Generation systems in the Semantic Web domain.</p>
<p>E2E The E2E [112] dataset is a crucial resource for training end-to-end, datadriven natural language generation systems, specifically in the restaurant domain.It was gathered through crowdsourcing and meticulous quality control, using images as stimuli to evoke more natural and well-articulated human references than textual MRs.This dataset, openly released as part of the E2E NLG challenge, is approximately ten times larger than its predecessors.It introduces novel challenges due to its size, lexical diversity, syntactic intricacies, and discourse complexities.Learning from this dataset promises to generate more natural, diverse, and less template-like system utterances.It comprises a rich set of 50,602 English verbalizations paired with 5,751 dialogue-act-based meaning representations.The dataset is thoughtfully partitioned into training, validation, and testing subsets, maintaining a consistent distribution of MR and reference text lengths while ensuring MR variation across different sets.Each MR encompasses 3-8 attributes or slots, including information such as name, food, area, and corresponding values.</p>
<p>AMR The Abstract Meaning Representation (AMR) [113] dataset 17 , encompassing series such as LDC2011T07, LDC2015E86, LDC2016E25, LDC2017T10, LDC2020T02, and LDC2020T07, presents a structured representation of semantic information.AMR is represented as a rooted, directed, acyclic graph with labelled edges (relations) and nodes (concepts), capturing the essence of "who is doing what to whom".It serves as a foundation for generating sentences that convey the semantics encoded within the graph.Specifically, LDC2017T10 within this series comprises 36,521 training instances of AMR graphs in PENMAN notation [114], along with their corresponding texts.Additionally, it includes 1,368 development instances and 1,371 test instances, providing a substantial resource for research and development in AMR parsing and natural language understanding and generation.</p>
<p>WikiBio The WikiBio dataset [21] is a comprehensive collection of biographical information covering individuals from diverse professions.This extensive dataset comprises over 10 million records, providing details such as names, dates of birth, occupations, and education.Focused on curating 728,321 biographies sourced from English Wikipedia, it serves as a valuable resource for evaluating text generation algorithms.The dataset encompasses the initial paragraph of each biography alongside its associated infobox, both tokenized to facilitate processing.Arranged in a standardized tabular layout, it proves to be of great utility for a range of natural language processing tasks, including text generation, summarization, entity identification, and data extraction.Researchers can utilize this dataset for the development and evaluation of algorithms and models geared toward the analysis of biographical text.</p>
<p>RotoWire The RotoWire dataset [46], designed for table-to-text generation, offers a robust platform for generating human-like summaries from basketball game tables.With 4.9K examples and 1.6M tokens from rotowire.com, it includes game data and corresponding human-written summaries.RotoWire features extended texts and a diverse vocabulary, making content selection more challenging.It leverages table records to generate structured yet informal game summaries, appealing to those interested in game statistics.This dataset proves valuable for assessing data-to-document generation systems, especially in basketball game summaries.It presents the complex task of converting structured data into coherent, informative text, accommodating diverse audiences and writing styles.Researchers can employ it to develop and evaluate table-to-text generation models, advancing natural language generation in specific domains.</p>
<p>ToTTo The ToTTo dataset [10], is a valuable resource in table-to-text generation within an open-domain English context.It comprises a substantial training set of over 120,000 instances.Its primary objective is generating a controlled one-sentence description task based on the information in a Wikipedia table, mainly focusing on the highlighted table cells.The dataset's creation involved a meticulous process wherein noisy descriptions were meticulously paired with tables, and any inaccuracies or inconsistencies in the highlighted cells were diligently rectified through iterative refinement.ToTTo stands as a pivotal benchmark for advancing high-precision, faithful, and conditional text generation research.</p>
<p>WMT Since its inception in 2006, the Workshop on Machine Translation (WMT) has been a hub for machine translation shared tasks and competitions.Its impact extends to other fields of natural language generation, such as in multilingual datato-text generation [4; 48; 108].Initially centered around translation tasks, WMT has evolved to encompass various aspects, including biomedical, multimodal, and lowresource translation.The General primary Machine Translation task, previously known as the News Task, remains a core component.Additionally, WMT features evaluation tasks like Metrics and Quality estimation.Over the years, some tasks have been discontinued, but WMT's shared task results and datasets remain crucial benchmarks for advancing machine translation research.The WMT dataset is sourced from the OPUS corpus [115], and it consists of a parallel corpus in 18 languages, primarily from the news domain.It includes news commentary text extracted mainly from online news sources, with the test data containing about 1000 sentence pairs.While most languages are paired with English, some are also paired with languages other than English such as German, Russian, French, Spanish, Italian and Chinese.The competition offers training sets compiled from various sources [116; 117].</p>
<p>Viggo The ViGGO dataset [28] addresses limitations in existing data-to-text NLG corpora by focusing on video game descriptions and providing a more conversational context.It includes over 100 video game titles and their attributes, resulting in 2,300 structured meaning representations (MRs).These MRs cover nine different dialogue act types (DAs), making ViGGO suitable for open-domain dialogue systems.Crowdsourced reference utterances were collected for each MR, enabling neural language generation models to learn multiple ways of expressing the same content.While smaller in size compared to the E2E dataset, ViGGO offers greater lexical diversity, longer inform utterances, and a more natural-sounding context due to its grounding in real video game data.Additionally, ViGGO maintains a focus on shorter, conversational responses.</p>
<p>Language</p>
<p>Multilingualism has emerged as a crucial aspect of natural language generation within data-to-text field, showcasing the remarkable progress achieved in handling structured data across various languages.Data-to-text generation has faced numerous challenges when it comes to multilingual outputs, with English often dominating as the primary target language for generation tasks.In our analysis of the 90 selected papers, a striking trend is evident: 89 of them focused on generating summaries from structured data in English.While English takes the lead, other languages also make appearances, with German, Russian, French, Spanish, Italian, and Brazilian Portuguese featuring in 6, 4, 3, 2, 2, and 2 papers, respectively.A variety of other languages were utilized in just one paper each, as illustrated in Table 4.</p>
<p>Language</p>
<p>Count Citations English 77 [46], [64], [36], [65], [9], [98], [14], [99], [47], [74], [88], [56], [38], [57], [58], [81], [48], [82], [27], [23], [96], [83], [49], [66], [67], [84], [16], [92], [54], [72], [87], [91], [68], [51], [52], [69], [100], [53], [39], [101], [102], [28], [29], [41], [11], [10], [103], [77], [93], [75], [42], [110], [94], [43] ', [104], [37], [85], [61], [59], [89], [90], [62], [70], [71], [76], [97], [78], [45], [105], [24], [63], [60], [80], [22], [111], [2]  In our exploration of multilingualism and its role in data-to-text literature, we assessed 90 papers.Among them, 12 studies ventured into multilingual approaches, while the remaining 78 predominantly focused on single-language generation, as depicted in Figure 2. Notably, a noteworthy study by Fan et al. [55] introduced a multilingual method for generating text from AMRs across twenty-one (21) EUROPARL languages.Similarly, researchers such as Xu et al. [79], Nema et al. [26], and Moussallem et al. [106] [109], and Teixeira et al. [12], have centered their efforts on bilingual text generation within the data-to-text context.Significant advancements have indeed been achieved in the domain of multilingual data-to-text generation, particularly in English, Chinese, and select European languages.Nonetheless, it is apparent that additional research is essential to advance this field.This research is needed to enable proficient multilingual generation across a more diverse range of languages, including those with diverse morphological structures and word order characteristics, as emphasized in Fan et al. [55] work.</p>
<p>Models</p>
<p>Methodology</p>
<p>Counts Papers Copy Mechanism 27 [46], [65], [9], [98], [47], [27], [96], [83], [67], [51], [52], [53], [39], [101], [70], [71], [29], [42], [104], [54], [87], [73], [16], [89], [76], [45], [60] GNN 18 [65], [14], [4], [84], [51], [54], [87], [88], [56], [38], [106], [110], [59], [81], [71], [29], [80], [90] Hierarchical encoder 5 [80], [52], [54], [104], [80] HMMM 2 [9], [24] Transformer 38 [54], [106], [59], [98], [28], [10], [103], [70], [72], [91], [78], [45], [105], [81], [44], [77], [61], [104], [37], [73], [92], [29], [55], [108], [57], [58], [109], [42], [94], [24], [60], [22], [86], , [76], [97], [90], [85], [62] Table 5: Methodology Used in the Selected Papers</p>
<p>In the field of data-to-text generation, various methodologies have been explored, including templates, statistical models, and neural network-based generation models.A significant number of studies selected for this systematic review opted for neural network models, ranging from basic sequence-to-sequence models [118] to advanced designs such as transformers [119] and large pretrained language models.Several studies have introduced refinements to these models to enhance their performance and set new benchmarks.Notably, research conducted by 27 studies, integrated a copy mechanism that uses probabilistic strategies to determine when and which tokens should be directly copied from the reference data.This copy model is particularly useful for ensuring that all data values appear in the generated text [98].Building on this innovation, subsequent investigations by five (5) studies devised fused attention [26] and hierarchical attention [67], improving decoder focus on input tokens and enhancing overall generation quality.However, some of these studies employed recurrent neural networks with static embeddings, which have shown suboptimal performance across various application domains such as in AMR-to-text [65; 2], MR-to-text [68], table-to-text [26] and in data-to-text [54].</p>
<p>Another line of research has focused on retaining the graph structure of the data using graph encoders [65] in graph neural network (GNN).Originally, graph data were linearized into sequences to accommodate sequence-to-sequence models.However, graph encoding has emerged as a promising method in data-to-text generation, especially in AMR-to-text generation.Graph encoders offer improvements over basic sequence-to-sequence models by inherently learning the graph structure and existing relations in the encoder and using an ordinary decoder to generate desired texts.</p>
<p>Ribeiro et al. [51] leveraged dual graph representations in AMR-to-text generation, effectively encoding divergent but complementary perspectives of the structural information in the AMR graph by simultaneously learning top-down and bottom-up node representations [51].The study by Zhao et al. [71] introduced DUALENC, a dual encoding model that addresses the structural gap in data-to-text generation by incorporating both graph and linear structures.This approach significantly improves text quality compared to single-encoder models, especially for structured inputs like trees or graphs.The DUALENC [71] model integrates Graph Convolutional Network (GCN) encoders with an intermediate content planning stage.This combination allows the model to capture structural information and enhance the compatibility between input and output sequences [71].Another research Li et al. [80] introduces a hierarchical encoder equipped with a reasoning module for graph-based reasoning, which enhances the ability to capture various relations between records in different dimensions.This research also introduces auxiliary supervision tasks, including number ranking and importance ranking, to further improve the model's ability to handle different record relations [80].These advancements contribute significantly to the field of GNNs.Eighteen (18) papers were found to have used this method to improve on existing baselines.</p>
<p>To address the limitations of static embedding models, some research efforts have embraced transformer models such as RNN transformers [120; 121], BERT [119], T5 [122], BART [123], XLM [124], and GPT-2 [125] for data-to-text generation tasks.These transformer-based models incorporate contextualized embeddings and use positional encoding due to their non-recurrent nature.They are trained on a large corpus of online curated texts and seem to perform well across several domains after finetuning.A total of 38 studies incorporated transformer models into their research, as detailed in Table 5.</p>
<p>Hallucination Mitigation Measures</p>
<p>In the context of data-to-text, hallucination refers to the generation of content that lacks fidelity or is not supported by the source data provided [66].Divergence can also be considered a form of hallucination when it occurs in the reference text, signifying a deviation from the expected or accurate information [66].This highlights the importance of generating text that remains faithful to the underlying data and references.</p>
<p>To address and reduce errors and hallucinations in data-to-text generation, several strategies have been deployed.These strategies encompass a wide range of approaches, including dataset cleaning and standardization, the development of novel training modules and techniques, as well as the application of knowledge distillation methods [104].</p>
<p>In the subsequent subsections, we will delve into specific papers and elucidate the strategies they have employed to address challenges and improve data-to-text generation.This comprehensive exploration will provide valuable insights into the diverse approaches and techniques used in the field to enhance the quality and fidelity of generated text.</p>
<p>Dataset Refinement and Post Editing</p>
<p>Effective refinement of datasets and post-editing play an essential role in enhancing the quality and accuracy of data-to-text generation.Wang [101] made a significant contribution to this field by focusing on boosting factual accuracy.Their work involves developing Rotowire-FG, which stands for Fact Grounding-purified version of Rotowire [46].Within this context, they incorporated content normalization techniques aimed at boosting the overall accuracy of the generated text.These techniques involve actions like converting number words into their corresponding numerical values and standardizing mentions of entities, contributing to improved text fidelity and reliability [101].</p>
<p>In addition, the TOTTO dataset [10] facilitates the controlled generation of concise descriptions derived from Wikipedia tables.This initiative aimed to enhance controllability in data-to-text generation, addressing issues previously associated with crowd-sourced datasets, which is the incomplete alignment of information in the table and their corresponding summaries.Chen et al. [29] addresses logical-level generation, presenting the LOGIC2TEXT dataset designed for generating high-fidelity descriptions from logical forms.To address scientific data-to-text challenges, Suadaa et al. [76] emphasize numerical reasoning in textual descriptions, as evidenced by the creation and utilization of the numericNLG dataset [76].Collectively, these efforts improve data-to-text generation, enhancing text accuracy and coherence.</p>
<p>Furthermore, Shimorina and Gardent [27] focuses on handling rare items or entities in the generated text.Their approach involves post-processing the text to replace placeholders with the appropriate values based on a mapping between placeholders and initial values created during pre-processing.This strategy further enhances the fidelity of the generated text.</p>
<p>Training Techniques and Model Modification</p>
<p>In this field, Gong et al. [52] presents an innovative architecture, a hierarchical encoder for table-to-text generation that excels at encapsulating the intricacies of multi-dimensional table data.The model's ability to encode row, column, and time dimensions simultaneously enables it to generate text summaries that are both highly informative and coherent.A study Ribeiro et al. [59], introduces STRUCTADAPT, which leverages adapter modules to incorporate graph structure into pretrained language models (PLMs) for better Abstract Meaning Representation (AMR) to text generation, resulting in enhanced performance compared to prior approaches [59].</p>
<p>Neural generation models use various strategies to reduce hallucination, including soft templates [98], copy mechanisms, content planning, and structure-aware systems [54].Training methodologies have evolved significantly, with some studies using paired training with unlabeled text [64], and others implementing a two-tiered approach involving an information extraction model and an attention-based encoder-decoder text generation model [46].The JointGT model by Ke et al. [90] enhances Knowledge Graph to text generation tasks by leveraging graph structure and pre-training tasks.</p>
<p>On a different note, (2) papers delved into Reinforcement Learning (RL) for data-to-text generation.Rebuffel et al. [104] introduces PARENTing, a reinforcement learning framework that is model-agnostic.This framework fine-tunes pretrained models using self-critical policy gradient algorithms to minimize hallucination in text generation by addressing divergence in training examples.Another study used multitask learning and reinforcement learning to incorporate content selection mechanisms into the encoder-decoder models [82].</p>
<p>Controllabilty and Constraints decoding</p>
<p>Controllability is achieved in data-to-text generation by introducing constraints and supervision during the decoding process or in the decoder.Various studies have contributed to this field:</p>
<p>â€¢ Lin et al. [42] [74] segments target text into fragments that align with data records, improving the control and interpretability of the generated output.This automatic segmentation, which adapts to domain-specific requirements, employs a soft statistical constraint to regularize the granularity of the segments [74].â€¢ Wang et al. [78] introduces Mention Flags (MF), a unique method that guarantees constraint satisfaction in Transformer-based text generation.By tracking the fulfilment of lexical constraints in the generated text and integrating them into the S2S Transformer models, MF ensures the creation of high-quality text that complies with the given constraints [78].â€¢ Lu et al. [86] presents NEUROLOGIC Aesque, an innovative decoding algorithm, inspired by A* search and designed for large-scale language models, that enables constrained text generation.This is achieved by combining heuristic cost estimates and logic-based lexical constraints, enhancing Constrained Machine Translation and Keyword-constrained generation [86].â€¢ Hardy and Vlachos [2] introduces a new method to improve AMR-based summarization by guiding it with the source document.This two-step process estimates the distribution of missing linguistic data and uses it to guide a seq2seq model, enhancing summary fluency and quality.</p>
<p>Ranking System</p>
<p>Eight (8) papers utilized rankers to improve the fidelity of the generated text [63; 105; 44; 83; 109; 11; 71; 80].The process of reranking in text generation, specifically within the decoder, is aimed at enhancing the quality and reducing semantic errors in the generated text.This involves the creation of rules or the use of an auxiliary classifier to verify if input slots are represented in the output, an important factor in maintaining semantic quality.Rerankers are commonly applied to the final hypotheses to enhance beam search and address its limitations.This can be achieved by establishing a reranking criterion or training a reranker to predict the best hypothesis within a beam based on function scores [63; 83; 11].The strategy employed involves over-generation, followed by reranking of potential outputs using criteria that were not explicitly optimized during training.The reranked outputs favour those with fewer missing or incorrect slot mentions, thereby enhancing accuracy and relevance [105; 63].This approach extracts meaningful information from encoder-decoder models and uses it to identify which attributes are mentioned in the generated text.</p>
<p>Additionally, A promising development in neural data-to-text generation is the introduction of a trainable evaluation metric.This metric, particularly useful when tables have multiple associated textual references, uses ranking models to assess the correctness of generated hypotheses by comparing them to the original table and corresponding references.It aims to overcome the limitations of existing metrics like BLEU, ROUGE, and METEOR, which do not fully capture the faithfulness of generated text to both the input table and references [44].</p>
<p>Pipeline and Planning Architecture Systems</p>
<p>In a survey of Natural Language Generation (NLG) architectures and methodologies, Gatt and Krahmer [15] categorizes NLG approaches into three main architectural paradigms: Modular, Planning, and Integrated (Global) architectures.These architectures encompass various generation systems, which are classified based on their methodological approach and design choices [15].To address challenges such as hallucination, omissions, and errors encountered in data-to-text generation, eleven (11) papers in total made use of this modular architecture.</p>
<p>Three (3) of these studies [23; 95; 12] resorted to traditional data-to-text methods.These methods typically involve a sequential process encompassing discourse ordering, text structuring, lexicalization, referring expression generation, and textual realization.These stages can be further categorized into macro planners, which combine content selection and document planning, and micro planners, which involve sentence aggregation, lexicalization, and referring expression generation [15].</p>
<p>In contrast, recent studies have witnessed a departure from traditional rule-based approaches in the initial planning stages.Instead, five (5) studies have adopted endto-end models for generating text, spanning from intermediate stages to the final surface realization.This transition aims to assess the efficacy of end-to-end models when compared to conventional rule-based or template systems [36; 11; 68; 107; 89].This shift underscores the dynamic evolution in NLG methodologies and architectural preferences.</p>
<p>Furthermore, four (5) investigations [83; 24; 16; 49; 71] have sought to compare the performance of neural modular architectures against end-to-end neural architectures.Collectively, these research findings indicate that supervised neural modularization or pipelining within data-to-text architectures leads to notable improvements in fluency, fidelity, and the overall quality of generated text summaries.These enhancements primarily result from error reduction during content selection, the model's ability to capture long-term structural dependencies, and the accurate ordering of facts [46; 49].Shao et al. [50] introduces the Planning-based Hierarchical Variational Model (PHVM) to address the limitations of existing neural methods in generating long and diverse texts in data-to-text generation tasks.The PHVM incorporates a planning mechanism and a hierarchical latent structure to capture inter-sentence coherence and generate varied expressions.By decomposing long text generation into dependent sentence generation sub-tasks, the model effectively models input data dynamically during generation [50].</p>
<p>Evaluation Metric</p>
<p>In data-to-text generation, assessing the quality and suitability of generated text has relied on various metrics over the years.These assessments can be broadly categorized into two groups: automatic evaluation and human evaluation.Automatic evaluation employs computational methods to measure text quality, while human evaluation enlists human participants to capture nuanced aspects of text quality and coherence.</p>
<p>Automatic Evaluation</p>
<p>N-gram Metrics</p>
<p>Our analysis indicates that the most commonly employed automatic metric is BLEU [126], with a substantial presence in 80 papers.It is closely followed by METEOR [127], which is employed in 40 papers, demonstrating its continued relevance.ROUGE [128], with 17 papers utilizing it, has also been a consistent choice for assessing generated text quality.Furthermore, CHrF++ [129], used in 10 papers, and NIST [130], applied in 9 papers, have offered valuable insights into the evaluation of data-to-text outputs.These metrics, while non-semantic in nature, have played a role in understanding word or character count and n-gram overlap between generated text and reference texts.</p>
<p>In seven studies, TER [131] was employed as an edit distance metric to evaluate machine translation, quantifying the human-level editing required to align system output with a reference [131].Additionally, fifteen studies utilized CIDEr [132], a widely adopted metric for image captioning, which assesses the similarity between generated and reference captions, considering both linguistic and content aspects, and applying TF-IDF-based n-gram weighting [133].Furthermore, two studies incorporated SPICE [134] metrics, which calculates the semantic propositional content overlap between generated and reference captions using scene graphs.Four (4) studies incorporated the use of the SER (Slot-Error Rate) metrics, which are appropriate for assessing the presence of named entities, with SER being computed through exact matching of slot values in the candidate texts [27].</p>
<p>Metric</p>
<p>Count Papers BLEU 80 [46], [64], [36], [65], [9], [98], [14], [99], [47], [2], [27], [23], [86], [96], [83], [49], [4], [66], [67], [84], [50], [68], [51], [52], [69], [100], [53], [39], [101], [102], [28], [11], [10], [103], [70], [71], [54], [72], [107], [87], [91], [73], [74], [88], [16], [26], [92], [29], [41], [55], [56], [57], [58], [81], [93], [75], [106], [109], [42], [110], [94], [43], [104], [37], [85], [61], [89], [90], [62], [76], [97], [78], [45], [105], [24], [63], [79], [60], [80], [111], [86] METEOR 40 [36], [9], [99], [27], [83], [49], [4], [66], [84], [51], [100], [102], [28], [11], [103], [71], [54], [72], [107], [91], [73], [74], [88], [56], [57], [106], [109], [110], [37], [85], [61], [90], [62], [76], [78], [105], [24], [60], [86], [81] ROUGE 17 [9], [98], [2], [28], [73], [74], [26], [92], [29], [57], [61], [90], [45], [105], [24], [60], [86] CIDER 15 [9], [98], [99], [83], [66], [28], [91], [74], [57], [58], [78], [105], [24], [60], [86] CHRF++ 10</p>
<p>[72], [107], [87], [88], [106], [109], [110], [37], [62], [81] NIST 9 [9], [99], [27], [26], [57], [58], [61], [78], [60] RG, CO, CS 11 [46], [96], [66], [67], [52], [69], [101], [16], [43] , [89], [80] PARENT 9 [66], [103], [73], [41], [104], [76], [44], [97], [ [36], [4], [71], [109], [110], [85], [24] BERTScore 7 [107], [109], [110], [37], [85], [62], [76] BLEURT 5 [107], [109], [110], [37], [85] MOVERScore 2 [37], [85] Table 6: Table of the Evaluation Metrics used across the Studies.</p>
<p>Task Specific Metrics</p>
<p>A significant debate persists among researchers regarding the appropriateness of these non-semantic n-gram metrics for evaluating data-to-text outputs.These metrics primarily rely on word count and n-gram overlap between the generated text and reference texts, often showing limited correlation with human judgment [66].They were originally developed and applied in other natural language generation (NLG) domains, such as translation for BLEU, NIST and METEOR and summarization for ROUGE.</p>
<p>In response to the unique challenges posed by table-to-text generation, task-specific metrics like PARENT [66] and its variant, PARENT-T [73], have emerged.These metrics assess the quality of table-to-text outputs by comparing the generated information with the entries in the source table.The research also analyzes the sensitivity of the metrics to divergence by collecting labels for cases where references only contain information already present in the tables.The study shows that PARENT maintains a high correlation as the number of such examples varies [66].These task-specific metrics are gaining prominence, with nine (9) papers considering PARENT and one (1) paper exploring PARENT-T.</p>
<p>To enhance the semantic alignment between generated texts and their references, embedding-based and pretrained-based metrics have been introduced.These metrics utilize contextualized embeddings and Transformer-based models to assess the quality and similarity of generated text to reference sentences.BERTScore [135], featured in 7 papers, calculates the cosine similarity between generated texts and the ground truth, offering a more nuanced understanding of text quality.MoverScore [136], a metric that allows many-to-one matching, is used in 2 papers and computes the Euclidean distances between words or n-grams.It enhances the evaluation process by considering partial alignments and offering insights into text quality.Notably, BLEURT [137], a Transformer-based trained metric, has been employed in 5 papers.This approach pretrains BERT with synthetically generated sentence pairs by mask-filling with BERT, back-translation, or randomly dropping words to assess NLG system performance [133].</p>
<p>Information Extraction Metrics</p>
<p>In data-to-text evaluation, extractive evaluation methods were introduced in Wiseman et al. [46] to assess the performance of the alignment of the information extraction model in the content selection and text planning in the generation process.A total of 11 studies adopted these metrics to rate their model's performance in content selection and planning tasks.This approach employs metrics like content selection (CS), content ordering (CO), and relational generation (RG).An Information Extraction (IE) system is used to extract content plans, identify candidate entities and value pairs present in the generated text, and predict their types.CS evaluates how accurately the system's extracted records match those in the reference output, considering precision and recall.RG assesses factuality by measuring the proportion of system-extracted records that also appear in the input table.CO evaluates the system's record ordering by computing the normalized Damerau-Levenshtein Distance between the sequence of extracted records and the reference output [46].</p>
<p>The Table 6 illustrate the distribution of these evaluation metrics identified in various research papers.</p>
<p>Human Evaluation</p>
<p>While automatic metrics offer certain advantages, human evaluation is often favored when assessing generated texts.This is due to its enhanced precision in evaluating aspects such as semantic adequacy, coherence, fluency, and the identification of numerical errors.Existing automatic metrics are often benchmarked against human evaluation results to determine their reliability and suitability.In human evaluations, the assessment of the quality of generated text varies widely, with different criteria used depending on the task.Due to the lack of a standardized human evaluation procedure in Natural Language Generation (NLG), and even in the naming conventions of the criteria, researchers often adopt diverse approaches to evaluate their generated texts [138].In this review, we aim to show some aspects of human evaluation by categorizing them into the measures and methods of evaluation taking a cue from studies by Belz et al. [139], and Van Der Lee et al. [138].</p>
<p>Quality Criteria Measures</p>
<p>In our analysis, certain studies lacked explicit details regarding their methods, tools, and design of quality criteria.However, for those that provided such information, we extracted relevant data.A notable observation is the considerable variation in the meanings associated with the names of the quality criteria.Table 7 enumerates the top ten prevalent naming conventions identified in the literature with "fluency" being the most used in 29 studies.Several terms, such as relevance, clarity, readability, and factual, among others, are notable examples that were not included in the table.It's crucial to note that the interpretation and task associated with these names may differ.</p>
<p>Experimental Methods</p>
<p>This section of the human evaluation review explores various methodologies for obtaining and assessing responses based on quality criteria.Table 7 presents the human evaluation frameworks and metrics extracted from the studies.Our analysis reveals that, out of 28 studies conducting human evaluation assessments in crowd-sourcing platforms, 17 studies utilized Amazon Mechanical Turk as their primary crowdsourcing platform.Additionally, we examined the linguistic background of annotators involved; 11 studies employed expert annotators, while eight and five studies engaged graduate students and paper authors, respectively.</p>
<p>Furthermore, we investigated the scale sizes used in each experiment.The most common scale size in 42 papers ranged from 1 to 5, followed by ranges of 10 to 30, and 50 and above in 5 and 4 papers, respectively.The 1-5 rank was the most popular scale range, appearing in 12 papers.Subsequently, the 0-100, 1-7, -100 to +100, and 0-5 scales were found in 6, 5, 3, and 3 papers, respectively.Two studies incorporated the TrueSkill Algorithm alongside the ranking task during evaluation, and various other scale sizes were identified in individual papers.</p>
<p>Additionally, we documented the agreement among annotators and the tools used in some studies.Statistical tools such as Krippendorff's Î±, Fleiss' Kappa, Cohen's Kappa, and Weighted Kappa were employed in 3, 5, and 2 studies to measure agreements among annotators.In 12 papers, responses from participants were aggregated using averages.</p>
<p>Design</p>
<p>Category</p>
<p>Counts Citations</p>
<p>Quality Criterion</p>
<p>Fluency 29 [62], [63], [82], [26], [37], [76], [104], [58], [11], [88], [85], [111], [23], [107], [95], [29], [64], [41], [74], [42], [2], [91], [83], [71], [108], [110], [48], [90], [93] Grammaticality 11 [62], [22], [96], [76], [52], [43], [80], [50], [89], [67], [94] Correctness 11 [22], [92], [76], [102], [23], [107], [29], [108],</p>
<p>[100], [93], [94] Adequacy 10</p>
<p>[27], [63], [26], [37], [11], [88], [77], [102], [95],</p>
<p>[90] Coherence 9 [28], [96], [76], [52], [43], [80], [11], [50], [89],</p>
<p>[67] Coverage 7 [86], [104], [102], [107], [64], [41], [71], [108],</p>
<p>[110] Faithfulness 7 [60], [82], [85], [41], [55], [83], [71] Naturalness 7 [28], [60], [46], [92], [103], [58], [98], [99] Conciseness 7 [96], [76], [52], [43], [80], [89], [67] Similarity 5 [37], [72], [59], [51], [38] Crowd Sourcing Platform AMT 17 [46], [96], [83], [71], [107], [51], [88], [29], [59], [89], [76], [86], [24], [60], [54], [82] [43], [98], [68], [47], [48], [61], [55], [95], [110] Annotator Experience Expert 11 [72], [91], [29], [41], [111], [28], [56], [11], [95], [105], [104] Graduate student 8 [52], [26], [92], [29], [100], [76], [58], [80] Author 5 [49], [91], [105], [94], [63] Scale Sizes 1 -5 42 [27], [39], [62], [4], [28], [53], [105], [63], [22], [86], [60], [82], [26], [46], [37], [96], [92], [45], [72], [59], [76], [103], [68], [47], [73], [52], [56], [97], [78],, [24], [43], [80], [104], [54], [58], [11], [88], [50], [51], [77], [29] [44], [50], [74], [60], [88], [26], [37], [51], [107], [72], [61], [48] Table 7: Human Evaluation Criteria and Frameworks.Amazon Mechanical Turk (AMT) [28], [90], [90], [85] Legal Domain 1</p>
<p>Application Areas</p>
<p>[44] Dialogue Systems 25 [91], [77], [58], [105], [103], [9], [54], [85], [38], [86], [42], [60], [98], [99], [68], [48], [74], [27], [61], [100], [102], [28], [78], [24], [43] Financial Reporting 1</p>
<p>[93] Biography Generation 15 [9], [10], [66], [54], [85], [26], [29], [41], [53], [111], [11], [38], [104], [82] [47], [46], [96], [10], [67], [70], [85], [52], [16], [89], [101], [76], [43], [80], [42], [23], [75], [69] Table 8: Application Areas of Data-to-text Generation.</p>
<p>Data-to-text generation finds extensive application in diverse domains, reflecting its versatility and value in addressing specific needs, as shown in Table 8.It plays a pivotal role in dialogue systems, where 26 studies focus on generating dialogues across various conversational contexts.Another significant application domain is sports narration, with 18 studies employing data-to-text generation to create textual summaries of game match records, encompassing player details, team information, and scores.In the realm of biography generation, 15 studies work on generating biographical texts for individuals with data often sourced from platforms like English Wikipedia.The application extends to translation and multilingualism, with 12 studies leveraging data-to-text techniques to tackle multilingual challenges.Additionally, data-to-text methods have proven effective in question generation and answering, exemplified by four identified studies in this domain.The application footprint extends to weather forecasting, robo-journalism, relational databases, the legal domain, and financial reporting, each with several studies showcasing the practical utility of data-to-text generation in distinct contexts.Furthermore, advertising and recipe generation domains have harnessed data-to-text techniques effectively.This comprehensive coverage highlights the adaptability and broad applicability of data-to-text generation in diverse scenarios and underscores its role in addressing specific needs across multiple domains.</p>
<p>tend to favor WebNLG and E2E over other datasets due to the competition challenges and dedicated conferences associated with them.Moreover, these datasets, being human-curated, offer more natural and fluent content compared to online-extracted datasets, contributing to their widespread acceptance.</p>
<p>In terms of evaluation metrics, BLEU emerges as the most commonly used automatic metric, enjoying broad acceptance across various data-to-text tasks.This observation is supported by the considerable number of studies that incorporate this metric in Table 6.</p>
<p>In addition, the dominance of large language models is evident in their effectiveness for data-to-text generation, as reflected by the widespread adoption of transformer models, as illustrated in Table 5.</p>
<p>Lastly, there is no universally preferred technique for addressing the hallucination problem in data-to-text.However, based on our observations, most mitigation measures are task-specific, with data refinement being a more general and effective method.This involves processes such as deduplication, cleaning, and factuality grounding, exerting control over the content the generation model encounters during training.</p>
<p>Recommendation and Future Directions</p>
<p>We have conducted a thorough analysis of the prevailing trends and have provided answers to the research questions within the scope of this literature.Nonetheless, it has come to our attention that there is a need to extend research efforts to encompass more low-resourced languages as seen in Section 3.2.Additionally, in Section 3.3 owing to the temporal limitations defined in the exclusion criteria, we did not include papers pertaining to recent large language models such as GPT-3.5 and GPT-4 [140; 141] and LLAMA [142; 143].In future investigations, our emphasis will be on studies that incorporate these advanced technologies into their research.</p>
<p>Moreover, from observations in Section 3.5, we recommend that future studies place greater emphasis on the utilization of contextual evaluation metrics for assessing the performance of data-to-text generation.These metrics have shown notable advantages in terms of semantic accuracy in data-to-text pairs, and their inclusion in evaluation frameworks is a direction worth exploring.</p>
<p>A standardized approach to human evaluation in the data-to-text field is essential.We strongly recommend authors to provide detailed explanations of their human evaluation procedures, including quality criteria definitions, response elicitation platforms, participants' knowledge backgrounds, etc.We also encourage the broader NLG community to collaboratively establish a universal naming convention to disambiguate similar terms and associated tasks.</p>
<p>Furthermore, referring to Section 3.4 and considering the richness of general knowledge that recent LLMs possess, we propose an advancement in their hallucination mitigation methods compared to task-specific LLMs.This improvement could focus more on addressing numerical and logical inference hallucination in the generated text.</p>
<p>Conclusion</p>
<p>This systematic review of data-to-text generation provides a comprehensive overview of the field, including its trends, challenges, and advancements.The review consolidates knowledge on datasets, language considerations, models, hallucination mitigation, and applications to guide future research endeavors.The insights gained from this review contribute to a deeper understanding of data-to-text generation, paving the way for continued innovation and progress.Addressing the identified trends and challenges will be crucial in advancing the capabilities and applicability of data-to-text generation systems as the field continues to evolve.</p>
<p>(a) Number of papers per year.(b) Publication Area.</p>
<p>7 .
7
Incorporation of both human and automatic evaluation of results.8. Availability of research code for reproducibility.Exclusion Criteria 1. Published before 2017.2.Not featured in journals categorized as A1-B2 and Q1-Q2.3. Written in languages other than English.4. Sole focus on other forms of text generation. 5. Less than five citations.6. Solely automatic evaluation of results.7. Lack of research code availability.</p>
<p>Fig. 2 :
2
Fig. 2: Multilinguality in Data-to-text Generation.</p>
<p>Table 14
1481]</p>
<p>Table 4 :
4
Languages and Multilingualism</p>
<p>have pursued analogous approaches, generating text concurrently in multiple European languages, underscoring the heightened interest and capabilities in multilingual data-to-text generation.Conversely, some studies, such as those by Song et al. [4], Shao et al. [50], Castro Ferreira et al. [107], Garneau and Lamontagne [44], Agarwal et al. [108], Lu et al. [86], Li et al.</p>
<p>[73]loped a novel neural model for data-to-text generation with style imitation to follow a certain style of writing from examples.The model employs a hybrid attention-copy mechanism and weak supervisions, using a content coverage constraint for balanced content fidelity and style control, proving effective in controlled text generation tasks[42].â€¢Wangetal.[73]presents a Transformer-based framework for table-to-text generation with a focus on producing faithful and informative text descriptions aligned with input tables.It introduces two essential strategies, a Table-Text Disagreement Constraint Loss and Constrained Content Matching via Optimal Transport, along with a novel evaluation metric, PARENT-T, to measure faithfulness in generated text.These constraints ensure that the latent representation of the table aligns with the corresponding representation of the generated text [73].â€¢ Shen et al.</p>
<p>https://scholar.google.com/
https://aclanthology.org/
https://www.ieee.org/
https://www.semanticscholar.org/
https://catalog.ldc.upenn.edu/byyear
Based on the data gathered from various studies, we will explore researchers' preferences. The datasets outlined in Table2reveal that, in data-to-text NLG, researchers
Acknowledgments.This work was conducted with the financial support of the Science Foundation Ireland Centre for Research Training in Artificial Intelligence under Grant No. 18/CRT/6223.This publication has emanated from research conducted with the financial support of Science Foundation Ireland under Grant number 18/CRT/6223.For the purpose of Open Access, the author has applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.
Building applied natural language generation systems. E Reiter, R Dale, 10.1017/S13513249970015021997</p>
<p>Guided neural language generation for abstractive summarization using abstract meaning representation. Vlachos Hardy, A , 10.18653/v1/d18-1086arXiv:1808.09160Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018. 2018</p>
<p>Automatic and Human-AI Interactive Text Generation. Y Dou, P Laban, C Gardent, W Xu, 2023</p>
<p>Semantic Neural Machine Translation Using AMR. L Song, D Gildea, Y Zhang, Z Wang, J Su, 10.1162/tacl_a_00252Transactions of the Association for Computational Linguistics. 72019</p>
<p>R Mokady, A Hertz, A H Bermano, ClipCap: CLIP Prefix for Image Captioning. 2021</p>
<p>Maximizing stylistic control and semantic accuracy in nlg: Personality variation and discourse contrast. V Harrison, L Reed, S Oraby, M Walker, arXiv:1907.095272019arXiv preprint</p>
<p>Neural natural language generation: A survey on multilinguality, multimodality, controllability and learning. E Erdem, B Plank, A Gatt, E Krahmer, M Sharma, A Gogineni, N Ramakrishnan, W Li, W Wu, M Chen, J Liu, X Xiao, H Wu, 10.1613/jair.5714arXiv:2207.12571Journal of Artificial Intelligence Research. 732022</p>
<p>Transformer based natural language generation for question-answering. I Akermi, J Heinecke, F Herledan, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language Generation2020</p>
<p>Learning Neural Templates for Text Generation. S Wiseman, S M Shieber, A M Rush, 2018Technical report</p>
<p>ToTTo: A controlled table-to-text generation dataset. A P Parikh, X Wang, S Gehrmann, M Faruqui, B Dhingra, D Yang, D Das, 10.18653/v1/2020.emnlp-main.89arXiv:2004.14373EMNLP 2020 -2020 Conference on Empirical Methods in Natural Language Processing. 2020Proceedings of the Conference</p>
<p>Scalable Micro-planned Generation of Discourse from Structured Data. A Laha, P Jain, A Mishra, K Sankaranarayanan, 10.1162/coli_a_00363Computational Linguistics. 4542019</p>
<p>A L R Teixeira, J G M Campos, R Cunha, T C Ferreira, A S Pagano, F G Cozman, Damata: A robot-journalist covering the brazilian amazon deforestation. INLG 2020 -13th International Conference on Natural Language Generation, Proceedings. 2020</p>
<p>Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model. J Obeid, E Hoque, arXiv:2010.091422020arXiv preprint</p>
<p>SQL-to-Text Generation with Graph-to-Sequence Model. K Xu, 2018</p>
<p>Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. A Gatt, E Krahmer, 10.1613/jair.5714arXiv:1703.09902Journal of Artificial Intelligence Research. 612018</p>
<p>PAN: Pipeline assisted neural networks model for data-to-text generation in social internet of things. N Jiang, J Chen, R G Zhou, C Wu, H Chen, J Zheng, T Wan, 10.1016/j.ins.2020.03.080Information Sciences. 5302020</p>
<p>Dealing with hallucination and omission in neural Natural Language Generation: A use case on meteorology. J GonzÃ¡lez Corbelle, A BugarÃ­n-Diz, J Alonso-Moral, J Taboada, Proceedings of the 15th International Conference on Natural Language Generation. the 15th International Conference on Natural Language Generation2022</p>
<p>J Zhang, J.-G Yao, X Wan, Towards constructing sports news from live text commentary. 2016</p>
<p>Interacting with financial data using natural language. V Plachouras, C Smiley, H Bretz, O Taylor, J L Leidner, D Song, F Schilder, 10.1145/2911451.2911457SIGIR 2016 -Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2016</p>
<p>. E Monfroglio, L Anselma, A Mazzei, 2022Personalizing Weekly Diet Reports</p>
<p>Neural Text Generation from Structured Data with Application to the Biography Domain. R Lebret, D Grangier, M Auli, 10.18653/V1/D16-1128arXiv:1603.07771EMNLP 2016 -Conference on Empirical Methods in Natural Language Processing, Proceedings. 2016</p>
<p>Getting to Production with Few-shot Natural Language Generation Models. P Heidari, A Einolghozati, S Jain, S Batra, L Callender, A Arun, S Mei, S Gupta, P Donmez, V Bhardwaj, A Kumar, M White, SIGDIAL 2021 -22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, Proceedings of the Conference. 20212</p>
<p>Automated learning of templates for datato-text generation: comparing rule-based, statistical and neural methods. C V Lee, E Krahmer, S Wubben, 10.18653/v1/w18-6504INLG 2018 -11th International Natural Language Generation Conference, Proceedings of the Conference. 2018</p>
<p>AGGGEN: Ordering and aggregating while generating. X Xu, O DuÅ¡ek, V Rieser, I Konstas, 10.18653/v1/2021.acl-long.113arXiv:2106.05580ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. 2021Proceedings of the Conference</p>
<p>Learning Semantic Correspondences with Less Supervision. P Liang, M I Jordan, D Klein, 2009</p>
<p>Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization. P Nema, S Shetty, P Jain, A Laha, K Sankaranarayanan, M M Khapra, 10.18653/V1/N18-1139arXiv:1804.07789NAACL HLT 2018 -2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20181</p>
<p>Handling rare items in data-to-text generation. A Shimorina, C Gardent, 10.18653/v1/w18-6543INLG 2018 -11th International Natural Language Generation Conference, Proceedings of the Conference. 2018</p>
<p>ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation. J Juraska, K K Bowden, M Walker, 10.18653/V1/W19-8623arXiv:1910.12129INLG 2019 -12th International Conference on Natural Language Generation, Proceedings of the Conference. 2019</p>
<p>Logic2Text: High-Fidelity Natural Language Generation from Logical Forms. Z Chen, W Chen, H Zha, X Zhou, Y Zhang, S Sundaresan, W Y Wang, 10.18653/V1/2020.FINDINGS-EMNLP.190arXiv:2004.14579Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020. 2020. 190</p>
<p>Survey of Hallucination in Natural Language Generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y Bang, A Madotto, P Fung, 10.1145/3571730ACM Computing Surveys. 112022</p>
<p>A Systematic Literature Review on Text Generation Using Deep Neural Network Models. N Fatima, A S Imran, Z Kastrati, S M Daudpota, A Soomro, 10.1109/ACCESS.2022.3174108IEEE Access. 102022</p>
<p>Faithfulness in Natural Language Generation: A Systematic Survey of Analysis. W Li, W Wu, M Chen, J Liu, X Xiao, H Wu, arXiv:2203.05227Evaluation and Optimization Methods. 2022</p>
<p>A Survey of Evaluation Metrics Used for NLG Systems. A B Sai, A K Mohankumar, M M Khapra, 10.1145/3485766arXiv:2008.12009ACM Computing Surveys. 5522023</p>
<p>S Lu, Y Zhu, W Zhang, J Wang, Y Yu, arXiv:1803.07133Neural Text Generation: Past, Present and Beyond. 2018</p>
<p>Innovations in Neural Data-totext Generation. M Sharma, A Gogineni, N Ramakrishnan, arXiv:2207.125712022</p>
<p>The WebNLG challenge: Generating text from RDF data. INLG 2017 -10th International Natural Language Generation Conference. C Gardent, A Shimorina, S Narayan, L Perez-Beltrachini, 10.18653/v1/w17-3518Proceedings of the Conference. the Conference2017298</p>
<p>Investigating Pretrained Language Models for Graph-to-Text Generation. L F R Ribeiro, M Schmitt, H SchÃ¼tze, I Gurevych, 10.18653/v1/2021.nlp4convai-1.20arXiv:2007.084262021</p>
<p>Lightweight, dynamic graph convolutional networks for AMR-to-text generation. Y Zhang, Z Guo, Z Teng, W Lu, S B Cohen, Z Liu, L Bing, 10.18653/v1/2020.emnlp-main.169arXiv:2010.04383EMNLP 2020 -2020 Conference on Empirical Methods in Natural Language Processing. 2020Proceedings of the Conference</p>
<p>V Hajdik, J Buys, M W Goodman, E M Bender, 10.18653/v1/n19-1235arXiv:1904.11564Neural text generation from rich semantic representations. NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20191</p>
<p>The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. M J Page, J E Mckenzie, P M Bossuyt, I Boutron, T C Hoffmann, C D Mulrow, L Shamseer, J M Tetzlaff, E A Akl, S E Brennan, R Chou, J Glanville, J M Grimshaw, A HrÃ³bjartsson, M M Lalu, T Li, E W Loder, E Mayo-Wilson, S Mcdonald, L A Mcguinness, L A Stewart, J Thomas, A C Tricco, V A Welch, P Whiting, D Moher, 10.1136/bmj.n71The BMJ. 3722021</p>
<p>Controlled hallucinations: learning to generate faithfully from noisy data. K Fillippova, 10.18653/v1/2020.findings-emnlp.76arXiv:2010.05873Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020. 2020864</p>
<p>Data-to-text generation with style imitation. S Lin, W Wang, Z Yang, X Liang, F F Xu, E P Xing, Z Hu, 10.18653/v1/2020.findings-emnlp.144arXiv:1901.09501Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020 (Figure 1). 2020</p>
<p>Enhancing content planning for table-to-text generation with data understanding and verification. H Gong, W Bi, X Feng, B Qin, X Liu, T Liu, 10.18653/v1/2020.findings-emnlp.262Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020. 2020</p>
<p>Trainable Ranking Models to Evaluate the Semantic Accuracy of Data-to-Text Neural Generator. Eval4NLP 2021 -Evaluation and Comparison of NLP Systems. N Garneau, L Lamontagne, 10.26615/978-954-452-056-4_006Proceedings of the 2nd Workshop. the 2nd Workshop2021</p>
<p>Few-Shot Table-to-Text Generation with Prototype Memory. Y Su, Z Meng, S Baker, N Collier, 10.18653/v1/2021.findings-emnlp.77arXiv:2108.12516Findings of ACL: EMNLP 2021. Findings of the Association for Computational Linguistics2021</p>
<p>Challenges in Data-to-Document Generation. S Wiseman, S M Shieber, A M Rush, 2017</p>
<p>Operation-guided neural networks for high fidelity data-to-text generation. F Nie, J Wang, J G Yao, R Pan, C Y Lin, 10.18653/v1/d18-1422arXiv:1809.02735Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018. 2018</p>
<p>Unsupervised natural language generation with denoising autoencoders. M Freitag, S Roy, 10.18653/v1/d18-1426arXiv:1804.07899Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2014. 20182018</p>
<p>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. T C Ferreira, C Lee, E Miltenburg, E Krahmer, 10.18653/v1/d19-1052arXiv:1908.09022EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2019552</p>
<p>Long and diverse text generation with planning-based hierarchical variational model. Z Shao, M Huang, J Wen, W Xu, X Zhu, 10.18653/v1/d19-1321arXiv:1908.06605EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2019</p>
<p>Enhancing AMR-to-text generation with dual graph representations. L F R Ribeiro, C Gardent, I Gurevych, 10.18653/v1/d19-1314arXiv:1909.00352EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2019</p>
<p>Table-to-text generation with effective hierarchical encoder on three dimensions (row, column and time. H Gong, X Feng, B Qin, T Liu, 10.18653/v1/d19-1310arXiv:1909.02304EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2019</p>
<p>Enhancing neural data-to-text generation models with external background knowledge. S Chen, J Wang, X Feng, F Jiang, B Qin, C Y Lin, 10.18653/v1/d19-1299EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2019</p>
<p>KGPT : Knowledge-Grounded Pre-Training for Data-to-Text Generation. W Chen, Y Su, X Yan, W Y Wang, 2020</p>
<p>A Fan, F Loria, D Lorraine, C Gardent, C Loria, arXiv:2011.05443Multilingual AMR-to-Text Generation. 2020arXiv preprint</p>
<p>Online Back-Parsing for AMR-to-Text Generation. X Bai, L Song, Y Zhang, 2020</p>
<p>Partially-aligned data-to-text generation with distant supervision. Z Fu, B Shi, W Lam, L Bing, Z Liu, 10.18653/v1/2020.emnlp-main.738arXiv:2010.01268EMNLP 2020 -2020 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference. 2020</p>
<p>Controllable meaning representation to text generation: Linearization and data augmentation strategies. C Kedzie, K Mckeown, 10.18653/v1/2020.emnlp-main.419EMNLP 2020 -2020 Conference on Empirical Methods in Natural Language Processing. 2020Proceedings of the Conference</p>
<p>Structural Adapters in Pretrained Language Models for AMR-to-Text Generation. L F R Ribeiro, Y Zhang, I Gurevych, 10.18653/v1/2021.emnlp-main.351arXiv:2103.09120EMNLP 2021 -2021 Conference on Empirical Methods in Natural Language Processing, Proceedings. 2021</p>
<p>Data-to-text Generation by Splicing Together Nearest Neighbors. S Wiseman, A Backurs, K Stratos, 10.18653/v1/2021.emnlp-main.352arXiv:2101.08248EMNLP 2021 -2021 Conference on Empirical Methods in Natural Language Processing, Proceedings. 2021</p>
<p>Neural data-to-text generation with LM-based text augmentation. E Chang, X Shen, D Zhu, V Demberg, H Su, 10.18653/v1/2021.eacl-main.64arXiv:2102.03556EACL 2021 -16th Conference of the European Chapter. the Association for Computational Linguistics2021Proceedings of the Conference</p>
<p>Towards a decomposable metric for explainable evaluation of text generation from AMR. J Opitz, A Frank, 10.18653/v1/2021.eacl-main.129arXiv:2008.08896EACL 2021 -16th Conference of the European Chapter. the Association for Computational Linguistics2021Proceedings of the Conference (i)</p>
<p>Incremental beam manipulation for natural language generation. J Hargreaves, A Vlachos, G Emerson, 10.18653/v1/2021.eacl-main.219arXiv:2102.02574EACL 2021 -16th Conference of the European Chapter. the Association for Computational Linguistics2021Proceedings of the Conference</p>
<p>Neural AMR: Sequence-to-sequence models for parsing and generation. I Konstas, S Iyer, M Yatskar, Y Choi, L Zettlemoyer, 10.18653/v1/P17-1014arXiv:1704.08381ACL 2017 -55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers). 20171</p>
<p>A Graph-to-Sequence Model for AMR-to-Text Generation. L Song, Y Zhang, Z Wang, D Gildea, C Science, 2018</p>
<p>Handling divergent reference texts when evaluating table-to-text generation. B Dhingra, M Faruqui, A Parikh, M W Chang, D Das, W W Cohen, 10.18653/v1/p19-1483ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. 2020</p>
<p>Data-to-text generation with entity modeling. R Puduppully, L Dong, M Lapata, 10.18653/v1/p19-1195arXiv:1906.03221ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. 2023-2035 (2019</p>
<p>F Nie, J G Yao, J Wang, R Pan, C Y Lin, 10.18653/v1/p19-1256A simple recipe towards reducing hallucination in neural surface realisation. ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. 2020</p>
<p>Learning to select, track, and generate for datato-text. H Iso, Y Uehara, T Ishigaki, H Noji, E Aramaki, I Kobayashi, Y Miyao, N Okazaki, H Takamura, 10.5715/jnlp.27.599arXiv:1907.09699ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. 2020</p>
<p>Logical natural language generation from open-domain tables. W Chen, J Chen, Y Su, Z Chen, W Y Wang, 10.18653/v1/2020.acl-main.708arXiv:2004.10404Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Bridging the structural gap between encoding and decoding for data-to-text generation. C Zhao, M Walker, S Chaturvedi, 10.18653/v1/2020.acl-main.224Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2020</p>
<p>M M Ram, Y.-S L Radu, F Salim, GPT-too : A Language-Model-First Approach for AMR-to-Text Generation. 2020</p>
<p>Towards faithful neural table-totext generation with content-matching constraints. Z Wang, X Wang, B An, D Yu, C Chen, 10.18653/v1/2020.acl-main.101arXiv:2005.00969Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Neural data-to-text generation via jointly learning the segmentation and correspondence. X Shen, E Chang, H Su, C Niu, D Klakow, 10.18653/v1/2020.acl-main.641arXiv:2005.01096Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2019. 2020</p>
<p>Fact-based text editing. H Iso, C Qiao, H Li, 10.18653/v1/2020.acl-main.17arXiv:2007.00916Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Towards table-to-text generation with numerical reasoning. L H Suadaa, H Kamigaito, K Funakoshi, M Okumura, H Takamura, 10.18653/v1/2021.acl-long.115ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2021</p>
<p>On Training Instance Selection for Few-Shot Neural Text Generation. E Chang, X Shen, H S Yeh, V Demberg, 10.18653/v1/2021.acl-short.2arXiv:2107.03176ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 20212</p>
<p>Mention flags (MF): Constraining transformer-based text generators. Y Wang, I D Wood, S Wan, M Dras, M Johnson, 10.18653/v1/2021.acl-long.9ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2021</p>
<p>XLPT-AMR: Cross-lingual pretraining via multi-task learning for zero-shot AMR parsing and text generation. D Xu, J Li, M Zhu, M Zhang, G Zhou, 10.18653/v1/2021.acl-long.73ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. 2021Proceedings of the Conference</p>
<p>Improving encoder by auxiliary supervision tasks for table-to-text generation. L Li, C Ma, Y Yue, D Hu, 10.18653/v1/2021.acl-long.466ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2021</p>
<p>Graph Pre-training for AMR Parsing and Generation. X Bai, Y Chen, Y Zhang, 10.18653/v1/2022.acl-long.415arXiv:2203.07836Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2022</p>
<p>Bootstrapping Generators from Noisy Data. L Perez-Beltrachini, M Lapata, 10.18653/V1/N18-1137arXiv:1804.06385NAACL HLT 2018 -2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20181</p>
<p>Step-by-step: Separating planning from realization in neural data-to-text generation. A Moryossef, Y Goldberg, I Dagan, arXiv:1904.03396NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20191</p>
<p>Structural neural encoders for AMR-to-text generation. M Damonte, S B Cohen, arXiv:1903.11410NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20191</p>
<p>DART: Open-Domain Structured Data Record to Text Generation. L Nan, D Radev, R Zhang, A Rau, A Sivaprasad, C Hsieh, X Tang, A Vyas, N Verma, P Krishna, Y Liu, N Irwanto, J Pan, F Rahman, A Zaidi, M Mutuma, Y Tarabar, A Gupta, T Yu, Y C Tan, X V Lin, C Xiong, R Socher, N F Rajani, 10.18653/v1/2021.naacl-main.37arXiv:2007.02871NAACL-HLT 2021 -2021 Conference of the North American Chapter. Human Language Technologies2021Proceedings of the Conference</p>
<p>X Lu, S Welleck, P West, NEUROLOGIC A * esque Decoding: Constrained Text Generation with Lookahead Heuristics. 2022</p>
<p>Amr-to-text generation with graph transformer. T Wang, X Wan, H Jin, 10.1162/tacl_a_00297Transactions of the Association for Computational Linguistics. 82020</p>
<p>Modeling global and local node contexts for text generation from knowledge graphs. L F R Ribeiro, Y Zhang, C Gardent, I Gurevych, 10.1162/tacl_a_00332Transactions of the Association for Computational Linguistics. 82020</p>
<p>Data-to-text Generation with Macro Planning. R Puduppully, M Lapata, 10.1162/tacl_a_00381Transactions of the Association for Computational Linguistics. 92021</p>
<p>JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs. P Ke, H Ji, Y Ran, X Cui, L Wang, L Song, X Zhu, M Huang, 10.18653/v1/2021.findings-acl.223arXiv:2106.10502Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity. H Harkous, I Groves, A Saffari, 10.18653/v1/2020.coling-main.218arXiv:2004.06577COLING 2020 -28th International Conference on Computational Linguistics, Proceedings of the Conference. 2020</p>
<p>TableGPT: Fewshot Table-to-Text Generation with Table Structure Reconstruction and Content Matching. H Gong, Y Sun, X Feng, B Qin, W Bi, X Liu, T Liu, 10.18653/v1/2020.coling-main.179COLING 2020 -28th International Conference on Computational Linguistics, Proceedings of the Conference. 1978-1988 (2020</p>
<p>Learning with Contrastive Examples for Data-to-Text Generation. Y Uehara, T Ishigaki, K Aoki, K Goshima, H Noji, I Kobayashi, H Takamura, Y Miyao, 10.18653/v1/2020.coling-main.213COLING 2020 -28th International Conference on Computational Linguistics, Proceedings of the Conference. 2020</p>
<p>Best Practices for Data-Efficient Modeling in NLG: How to Train Production-Ready Neural Models with Less Data. A Arun, S Batra, V Bhardwaj, A Challa, P Donmez, P Heidari, H Inan, S Jain, A Kumar, S Mei, K Mohan, M White, 10.18653/v1/2020.coling-industry.7arXiv:2011.03877COLING 2020 -28th International Conference on Computational Linguistics, Proceedings of the Industry Track. 2020</p>
<p>D Moussallem, T C Ferreira, M Zampieri, M C Cavalcanti, G XexÃ©o, M Neves, A C N Ngomo, arXiv:1802.08150Rdf2Pt: Generating brazilian Portuguese texts from RDF data. LREC 2018 -11th International Conference on Language Resources and Evaluation. 2019</p>
<p>Data-to-text generation with content selection and planning. R Puduppully, L Dong, M Lapata, 10.1609/aaai.v33i01.33016908arXiv:1809.0058233rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence. 20192019</p>
<p>Towards Faithfulness in Open Domain Table-to-text Generation from an Entity-centric View. T Liu, X Zheng, B Chang, Z Sui, 10.1609/aaai.v35i15.17583arXiv:2102.0858535th AAAI Conference on Artificial Intelligence, AAAI 2021. 202115</p>
<p>End-to-End Content and Plan Selection for Data-to-Text Generation. S Gehrmann, F Z Dai, H Elder, A M Rush, arXiv:1810.04700v1E2E NLG Challenge System Descriptions. 2018</p>
<p>Y Puzikov, I Gurevych, 10.18653/v1/w18-6557E2E NLG challenge: Neural models vs. templates. INLG 2018 -11th International Natural Language Generation Conference, Proceedings of the Conference. 2018</p>
<p>A good sample is hard to find: Noise injection sampling and self-training for neural language generation models. C Kedzie, K Mckeown, 10.18653/v1/w19-8672arXiv:1911.03373INLG 2019 -12th International Conference on Natural Language Generation, Proceedings of the Conference. 2019</p>
<p>Revisiting challenges in data-to-text generation with fact grounding. H Wang, 10.18653/v1/w19-8639arXiv:2001.03830INLG 2019 -12th International Conference on Natural Language Generation, Proceedings of the Conference. 2019</p>
<p>Semi-supervised neural text generation by joint learning of natural language generation and natural language understanding models. R Qader, F Portet, C LabbÃ©, 10.18653/v1/w19-8669arXiv:1910.03484INLG 2019 -12th International Conference on Natural Language Generation, Proceedings of the Conference. 2019552</p>
<p>T5Pretrain-Text-to-Text Pre-Training for Data-to-Text Tasks.pdf. M Kale, 2020</p>
<p>PARENTing via Model-Agnostic Reinforcement Learning to Correct Pathological Behaviors in Data-to-Text Generation. C Rebuffel, L Soulier, G Scoutheeten, P Gallinari, arXiv:2010.10866INLG 2020 -13th International Conference on Natural Language Generation, Proceedings. 2020</p>
<p>Attention Is Indeed All You Need: Semantically Attention-Guided Decoding for Data-to-Text NLG. INLG 2021 -14th International Conference on Natural Language Generation. J Juraska, M Walker, arXiv:2109.07043Proceedings. September. 2021</p>
<p>NABU -Multilingual Graph-Based Neural RDF Verbalizer. D Moussallem, D Gnaneshwar, T Castro Ferreira, A C Ngonga Ngomo, 10.1007/978-3-030-62419-4_24LNCS. 12506. 2020</p>
<p>The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task: Overview and Evaluation Results (WebNLG+. T Castro Ferreira, C Gardent, N Ilinykh, C Lee, S Mille, D Moussallem, A Shimorina, O Agarwal, M Kale, H Ge, S Shakeri, R Al-Rfou, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web. the 3rd International Workshop on Natural Language Generation from the Semantic WebDublin, Ireland (VirtualAssociation for Computational Linguistics2020. 2020. December. 2020Proceedings of the. 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+</p>
<p>Leveraging Large Pretrained Models for WebNLG. X Li, A Maskharashvili, Jory Stevens-Guille, S White, M , Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+) (December). the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+) (December)2020. 2020</p>
<p>P 2 : A Planand-Pretrain Approach for Knowledge Graph-to-Text Generation. Q Guo, Z Jin, N Dai, X Qiu, X Xue, D Wipf, Z Zhang, December. 2020</p>
<p>C Rebuffel, M Roberti, L Soulier, G Scoutheeten, R Cancelliere, P Gallinari, 10.1007/s10618-021-00801-4s10618-021-00801-4arXiv:2102.02810Controlling hallucinations at word level in data-to-text generation. 202236</p>
<p>J Novikova, O DuÅ¡ek, V Rieser, arXiv:1706.09254The e2e dataset: New challenges for end-toend generation. 2017arXiv preprint</p>
<p>Abstract meaning representation for sembanking. L Banarescu, C Bonial, S Cai, M Georgescu, K Griffitt, U Hermjakob, K Knight, P Koehn, M Palmer, N Schneider, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. the 7th Linguistic Annotation Workshop and Interoperability with Discourse2013</p>
<p>Penman: An open-source library and tool for amr graphs. M W Goodman, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations2020</p>
<p>Parallel data, tools and interfaces in opus. J Tiedemann, Lrec2012Citeseer2012</p>
<p>Findings of the 2017 conference on machine translation (wmt17). O Bojar, R Chatterjee, C Federmann, Y Graham, B Haddow, S Huang, M Huck, P Koehn, Q Liu, V Logacheva, C Monz, M Negri, M Post, R Rubino, L Specia, M Turchi, 10.18653/V1/W17-4717WMT 2017 -2nd Conference on Machine Translation, Proceedings. 20172</p>
<p>L Barrault, M Biesialska, O Bojar, M R Costa-JussÃ , C Federmann, Y Graham, R Grundkiewicz, B Haddow, M Huck, E Joanis, T Kocmi, P Koehn, C K Lo, N LjubeÅ¡iÄ‡, C Monz, M Morishita, M Nagata, T Nakazawa, S Pal, M Post, M Zampieri, 10.18653/V1/W19-5301Findings of the 2019 conference on machine translation (wmt19). 5th Conference on Machine Translation, WMT 2020 -Proceedings. 20192</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.04732014arXiv preprint</p>
<p>Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 201730Attention is all you need</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 981997</p>
<p>K Cho, B Van MerriÃ«nboer, D Bahdanau, Y Bengio, arXiv:1409.1259On the properties of neural machine translation: Encoder-decoder approaches. 2014arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-totext transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, arXiv:1910.134612019BartarXiv preprint</p>
<p>G Lample, A Conneau, arXiv:1901.07291Cross-lingual language model pretraining. 2019arXiv preprint</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational Linguistics2002</p>
<p>Meteor universal: Language specific translation evaluation for any target language. M Denkowski, A Lavie, Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine Translation2014</p>
<p>Rouge: A package for automatic evaluation of summaries. C.-Y Lin, Text Summarization Branches Out. 2004</p>
<p>chrf: character n-gram f-score for automatic mt evaluation. M PopoviÄ‡, Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine Translation2015</p>
<p>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. G Doddington, Proceedings of the Second International Conference on Human Language Technology Research. the Second International Conference on Human Language Technology Research2002</p>
<p>A study of translation edit rate with targeted human annotation. M Snover, B Dorr, R Schwartz, L Micciulla, J Makhoul, Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers. the 7th Conference of the Association for Machine Translation in the Americas: Technical PapersCambridge, Massachusetts, USA2006Association for Machine Translation in the Americas</p>
<p>Cider: Consensus-based image description evaluation. R Vedantam, C Lawrence Zitnick, D Parikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2015</p>
<p>A survey of evaluation metrics used for nlg systems. A B Sai, A K Mohankumar, M M Khapra, ACM Computing Surveys (CSUR). 5522022</p>
<p>Spice: Semantic propositional image caption evaluation. P Anderson, B Fernando, M Johnson, S Gould, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 2016Proceedings, Part V 14</p>
<p>T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. W Zhao, M Peyrard, F Liu, Y Gao, C M Meyer, S Eger, arXiv:1909.026222019arXiv preprint</p>
<p>T Sellam, D Das, A P Parikh, arXiv:2004.04696Bleurt: Learning robust metrics for text generation. 2020arXiv preprint</p>
<p>Best practices for the human evaluation of automatically generated text. C Van Der Lee, A Gatt, E Van Miltenburg, S Wubben, E Krahmer, Proceedings of the 12th International Conference on Natural Language Generation. the 12th International Conference on Natural Language Generation2019</p>
<p>Disentangling the properties of human evaluation methods: A classification system to support comparability, metaevaluation and reproducibility testing. A Belz, S Mille, D M Howcroft, 2020ACL</p>
<p>J Ye, X Chen, N Xu, C Zu, Z Shao, S Liu, Y Cui, Z Zhou, C Gong, Y Shen, arXiv:2303.10420A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. 2023arXiv preprint</p>
<p>O J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, Diogo, Gpt-4 technical report. 2023257532815</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B RoziÃ¨re, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8210 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8210</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8210</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-265157947</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.07687v1.pdf" target="_blank">Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated superior performance in language understanding benchmarks. CALM, a popular approach, leverages linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to improve the performance in text games in Jericho without environment-provided actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps the LLM fixed during the learning of the text based games. In this work, we explore and evaluate updating LLM used for candidate recommendation during the learning of the text based game as well to mitigate the reliance on the human annotated gameplays, which are costly to acquire. We observe that by updating the LLM during learning using carefully selected in-game transitions, we can reduce the dependency on using human annotated game plays for fine-tuning the LLMs. We conducted further analysis to study the transferability of the updated LLMs and observed that transferring in-game trained models to other games did not result in a consistent transfer.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8210.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8210.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-in-the-Loop (OC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model-in-the-Loop with State-Feature Categorized Replay (Oracle Categorization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent architecture where a GPT-2 language model recommends action candidates to a DRRN policy and is continually finetuned during gameplay using an in-game replay buffer whose transitions are categorized by state features (location change or reward increase). This in-game memory is used to adapt the LM to game-specific transitions and accelerate RL convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LM-in-the-Loop (OC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-2 proposes action candidates; a DRRN selects actions and produces advantage estimates; transitions are stored in categorized replay buffers (D+ / D-) using state-feature heuristics (oracle: location change or reward increase); after k steps the GPT-2 is finetuned on sampled positive/negative transitions and plugged back to propose improved candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-2 base (12 layers, 768 hidden units, 12 attention heads, ~117M parameters) pretrained on WebText and initially finetuned on ClubFloyd human gameplay transcripts before in-game finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (10 selected interactive fiction games, e.g. Zork1, Inhumane, Detective, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A suite of human-written text-based interactive fiction games (Jericho) with large linguistic variety, large action spaces, sparse rewards and long solution lengths; evaluated by average of last 100 episode scores and a normalized average score (avg.norm) against per-game maximum scores.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic experience replay (in-game transitions)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Two FIFO replay buffers (D+ and D-) of max size 100K storing transitions (o_t, a_t, o_{t+1}, r_{t+1}); transitions labelled positive or negative by oracle state-feature heuristic (reward increase or location change). Samples of size d_LM are drawn from D+/D- with probability p+ and used to finetune GPT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>The memory (replay buffers) stores action-context transitions produced while the DRRN acts on GPT-2 candidates. After every k game steps (hyperparameter), the LM is updated for 2000 gradient steps on sampled transitions (weighted cross-entropy or uniform), and the updated LM replaces the previous LM to generate the next action candidates for the DRRN.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>avg.norm = 24.0% (normalized average score across the 10 games reported in Table 1); per-game example: Zork1 improved from CALM 30.7 to OC 38.0 (scores in table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline (CALM, frozen GPT-2 adapted on ClubFloyd) avg.norm = 20.1%; CALM per-game example: Zork1 = 30.7 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct comparisons vs other in-game memory strategies (UT, RT, UT_EA, UT_LA) and vs frozen baseline (CALM). OC outperformed Uncategorized Transitions (UT), reward-based categorization (RT), and weighted-loss variants (UT_EA, UT_LA) in avg.norm and accelerated convergence (Figure 3). Also compared to CALM with reduced human adaptation data (10%) where OC with 10% outperformed CALM with 100%.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>State-feature categorization (OC): selecting transitions that caused reward increases or changed agent location yielded the largest gains in performance and fastest convergence among evaluated memory strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>OC is a loose upper bound because it uses game-specific state features (location info) that may not be available or general; in-game trained LMs did not reliably transfer to other games; absolute performance gains remain modest; the DRRN still performs the planning work — LM alone is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>When using in-game memory to finetune LMs for action recommendation, prioritize transitions that reflect meaningful state changes (e.g., reward increases or location changes). LM-in-the-Loop with careful transition selection reduces reliance on large human-annotated adaptation corpora and accelerates convergence, but one should not replace an RL decision module (DRRN) with LM argmax actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8210.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8210.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-in-the-Loop (UT/RT/Weighted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model-in-the-Loop with Uncategorized, Reward-Trajectory and Advantage-Weighted Replay Strategies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variants of the LM-in-the-Loop approach where in-game transitions are stored without categorization (UT), categorized by reward trajectories (RT), or used with advantage-based reweighting in the LM loss (UT_EA: exponential weighting; UT_LA: linear advantage weights). These represent weaker or more general memory-selection strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LM-in-the-Loop (UT/RT/Weighted)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same architecture as LM-in-the-Loop but differing in how the replay buffer transitions are selected and/or weighted for LM finetuning: UT collects transitions in a single buffer; RT labels positives by reward and includes preceding transitions until prior non-zero reward; UT_EA and UT_LA reweight transition loss by action advantage A(o,a) (exponential or linear schemes).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-2 base as above; finetuned on ClubFloyd and then updated in-game using sampled transitions and weighted/unweighted cross-entropy loss for 2000 gradient steps.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (same 10 games)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same interactive fiction benchmark as above.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic experience replay with alternative selection/weighting heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Single FIFO replay buffer for UT (size 100K); RT constructs positive set by linking rewards backwards to earlier transitions; UT_EA and UT_LA keep single buffer but apply h(o,a) weights in LM cross-entropy loss where h = exp(β * A(o,a)) (UT_EA) or h = 1 + β * A(o,a) (UT_LA).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Samples drawn after k steps from buffer(s) and used to finetune GPT-2 via (weighted) cross-entropy. Updated LM then used to propose action candidates for DRRN.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>avg.norm: UT = 19.1%, RT = 20.7%, UT_EA = 20.9%, UT_LA = 20.6% (from Table 1). Some per-game variations (e.g., UT EA gave higher Zork1 score 35.6 in table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline CALM avg.norm = 20.1% (frozen LM adapted on ClubFloyd).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared across UT, RT, UT_EA, UT_LA, and OC. UT (uncategorized) performed worse than baseline on avg.norm; RT and reweighted UT variants produced only marginal improvements over baseline (~0.6% avg.norm) and were substantially weaker than OC.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Among these variants, advantage-weighted reweighting (UT_EA/UT_LA) did not yield tangible benefits over simple categorization; RT provided small gains but not comparable to OC.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>UT performed worse than baseline, indicating naive replay usage can harm LM finetuning. Reweighting with advantages was sensitive and produced no consistent improvement. Reward-based selection (RT) is game-dependent and provided only marginal gains. These strategies are more general but less effective than state-feature categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>General-purpose reweighting by advantage or naive replay buffers are insufficient; if game-specific signals (like location change) are unavailable, expect only modest gains from RT or advantage-weighting. Careful selection and labeling of transitions is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8210.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8210.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CALM (frozen GPT-2 baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CALM: GPT-2 adapted on human gameplay (ClubFloyd) and kept frozen for action recommendation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach (from prior work) where GPT-2 is adapted on a large human gameplay corpus (ClubFloyd) to propose candidate actions for a DRRN, but the LM remains frozen during in-game learning (no in-game memory updates).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Keep CALM and explore: Language models for action generation in textbased games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CALM (frozen GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-2 adapted on ClubFloyd human gameplay transcripts produces candidate actions; a DRRN selects actions and learns via TD updates. The LM is not updated from in-game transitions during gameplay (no LM-in-the-loop finetuning).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-2 base pretrained and finetuned on ClubFloyd dataset (≈217K context-action pairs) before gameplay; frozen during RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (same 10 games)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same interactive fiction benchmark as above.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>The LM is adapted offline using human gameplay and then left frozen; the DRRN uses its own replay buffer for RL but the LM does not receive in-game experience.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>avg.norm = 20.1% (Table 1); per-game example: Zork1 = 30.7.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Used as baseline for comparisons. The paper shows LM-in-the-Loop (OC) improves avg.norm to 24.0% and accelerates convergence compared to CALM. Also evaluated CALM when adapted with only 10% of ClubFloyd data (avg.norm 18.5%) to show LM-in-the-Loop can reduce reliance on large human corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance sensitive to size of annotated adaptation corpus; when adaptation uses only 10% of ClubFloyd, performance drops substantially. Because the LM is frozen, it cannot incorporate game-specific in-game transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Using in-game transitions to finetune the LM (LM-in-the-Loop) can reduce dependence on large human adaptation datasets and accelerate convergence; keeping the LM frozen is less data-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8210.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8210.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 as policy (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 acting as a policy network (argmax LM action) without DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation where GPT-2 (either frozen or trained in-game) directly selects the argmax action (no DRRN). This tests whether LM can act as the policy rather than just a candidate generator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-2 as policy (no DRRN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-2 is used to generate and pick the highest-likelihood action as the agent's action (argmax), either kept frozen or finetuned with in-game transitions, with no separate DRRN decision module.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-2 (base)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Same GPT-2 base; tested both frozen and in-game finetuned variants.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (subset of games tested as ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same interactive fiction benchmark but used to test whether LM-only policy can perform.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>LM was optionally finetuned in-game (LM-in-the-Loop) but used directly to select actions (argmax) instead of only proposing candidates to DRRN.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Irrespective of frozen or in-game trained, GPT-2 as policy yielded zero or near-zero scores in the majority of games (Table 3 reports many zeros; overall normalized score ~1.1%). Example table row: many games 0 [0]; norm 1.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared frozen vs in-game-trained GPT-2 acting as policy and contrasted with DRRN-based agents; found that LM-only argmax policy performs very poorly while DRRN+LM candidates performs much better.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-2 as a standalone policy fails on most games, indicating that LM likelihood alone does not align with action-value and planning requirements; finetuning LM in-game does not remedy this sufficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Do not replace an RL decision-maker with LM argmax actions; the DRRN performs the planning and value estimation that the LM likelihood does not capture.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Keep CALM and explore: Language models for action generation in textbased games <em>(Rating: 2)</em></li>
                <li>Pre-trained language models for interactive decisionmaking <em>(Rating: 2)</em></li>
                <li>Reading and acting while blindfolded: The need for semantics in text game agents <em>(Rating: 1)</em></li>
                <li>Do as i can. not as i say: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8210",
    "paper_id": "paper-265157947",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "LM-in-the-Loop (OC)",
            "name_full": "Language Model-in-the-Loop with State-Feature Categorized Replay (Oracle Categorization)",
            "brief_description": "An agent architecture where a GPT-2 language model recommends action candidates to a DRRN policy and is continually finetuned during gameplay using an in-game replay buffer whose transitions are categorized by state features (location change or reward increase). This in-game memory is used to adapt the LM to game-specific transitions and accelerate RL convergence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LM-in-the-Loop (OC)",
            "agent_description": "GPT-2 proposes action candidates; a DRRN selects actions and produces advantage estimates; transitions are stored in categorized replay buffers (D+ / D-) using state-feature heuristics (oracle: location change or reward increase); after k steps the GPT-2 is finetuned on sampled positive/negative transitions and plugged back to propose improved candidates.",
            "llm_model_name": "GPT-2 (base)",
            "llm_model_description": "GPT-2 base (12 layers, 768 hidden units, 12 attention heads, ~117M parameters) pretrained on WebText and initially finetuned on ClubFloyd human gameplay transcripts before in-game finetuning.",
            "benchmark_name": "Jericho (10 selected interactive fiction games, e.g. Zork1, Inhumane, Detective, etc.)",
            "benchmark_description": "A suite of human-written text-based interactive fiction games (Jericho) with large linguistic variety, large action spaces, sparse rewards and long solution lengths; evaluated by average of last 100 episode scores and a normalized average score (avg.norm) against per-game maximum scores.",
            "memory_used": true,
            "memory_type": "episodic experience replay (in-game transitions)",
            "memory_architecture": "Two FIFO replay buffers (D+ and D-) of max size 100K storing transitions (o_t, a_t, o_{t+1}, r_{t+1}); transitions labelled positive or negative by oracle state-feature heuristic (reward increase or location change). Samples of size d_LM are drawn from D+/D- with probability p+ and used to finetune GPT-2.",
            "memory_integration_strategy": "The memory (replay buffers) stores action-context transitions produced while the DRRN acts on GPT-2 candidates. After every k game steps (hyperparameter), the LM is updated for 2000 gradient steps on sampled transitions (weighted cross-entropy or uniform), and the updated LM replaces the previous LM to generate the next action candidates for the DRRN.",
            "performance_with_memory": "avg.norm = 24.0% (normalized average score across the 10 games reported in Table 1); per-game example: Zork1 improved from CALM 30.7 to OC 38.0 (scores in table).",
            "performance_without_memory": "Baseline (CALM, frozen GPT-2 adapted on ClubFloyd) avg.norm = 20.1%; CALM per-game example: Zork1 = 30.7 (Table 1).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Direct comparisons vs other in-game memory strategies (UT, RT, UT_EA, UT_LA) and vs frozen baseline (CALM). OC outperformed Uncategorized Transitions (UT), reward-based categorization (RT), and weighted-loss variants (UT_EA, UT_LA) in avg.norm and accelerated convergence (Figure 3). Also compared to CALM with reduced human adaptation data (10%) where OC with 10% outperformed CALM with 100%.",
            "best_memory_strategy": "State-feature categorization (OC): selecting transitions that caused reward increases or changed agent location yielded the largest gains in performance and fastest convergence among evaluated memory strategies.",
            "limitations_or_failure_cases": "OC is a loose upper bound because it uses game-specific state features (location info) that may not be available or general; in-game trained LMs did not reliably transfer to other games; absolute performance gains remain modest; the DRRN still performs the planning work — LM alone is insufficient.",
            "recommendations_or_conclusions": "When using in-game memory to finetune LMs for action recommendation, prioritize transitions that reflect meaningful state changes (e.g., reward increases or location changes). LM-in-the-Loop with careful transition selection reduces reliance on large human-annotated adaptation corpora and accelerates convergence, but one should not replace an RL decision module (DRRN) with LM argmax actions.",
            "uuid": "e8210.0",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LM-in-the-Loop (UT/RT/Weighted)",
            "name_full": "Language Model-in-the-Loop with Uncategorized, Reward-Trajectory and Advantage-Weighted Replay Strategies",
            "brief_description": "Variants of the LM-in-the-Loop approach where in-game transitions are stored without categorization (UT), categorized by reward trajectories (RT), or used with advantage-based reweighting in the LM loss (UT_EA: exponential weighting; UT_LA: linear advantage weights). These represent weaker or more general memory-selection strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LM-in-the-Loop (UT/RT/Weighted)",
            "agent_description": "Same architecture as LM-in-the-Loop but differing in how the replay buffer transitions are selected and/or weighted for LM finetuning: UT collects transitions in a single buffer; RT labels positives by reward and includes preceding transitions until prior non-zero reward; UT_EA and UT_LA reweight transition loss by action advantage A(o,a) (exponential or linear schemes).",
            "llm_model_name": "GPT-2 (base)",
            "llm_model_description": "GPT-2 base as above; finetuned on ClubFloyd and then updated in-game using sampled transitions and weighted/unweighted cross-entropy loss for 2000 gradient steps.",
            "benchmark_name": "Jericho (same 10 games)",
            "benchmark_description": "Same interactive fiction benchmark as above.",
            "memory_used": true,
            "memory_type": "episodic experience replay with alternative selection/weighting heuristics",
            "memory_architecture": "Single FIFO replay buffer for UT (size 100K); RT constructs positive set by linking rewards backwards to earlier transitions; UT_EA and UT_LA keep single buffer but apply h(o,a) weights in LM cross-entropy loss where h = exp(β * A(o,a)) (UT_EA) or h = 1 + β * A(o,a) (UT_LA).",
            "memory_integration_strategy": "Samples drawn after k steps from buffer(s) and used to finetune GPT-2 via (weighted) cross-entropy. Updated LM then used to propose action candidates for DRRN.",
            "performance_with_memory": "avg.norm: UT = 19.1%, RT = 20.7%, UT_EA = 20.9%, UT_LA = 20.6% (from Table 1). Some per-game variations (e.g., UT EA gave higher Zork1 score 35.6 in table).",
            "performance_without_memory": "Baseline CALM avg.norm = 20.1% (frozen LM adapted on ClubFloyd).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared across UT, RT, UT_EA, UT_LA, and OC. UT (uncategorized) performed worse than baseline on avg.norm; RT and reweighted UT variants produced only marginal improvements over baseline (~0.6% avg.norm) and were substantially weaker than OC.",
            "best_memory_strategy": "Among these variants, advantage-weighted reweighting (UT_EA/UT_LA) did not yield tangible benefits over simple categorization; RT provided small gains but not comparable to OC.",
            "limitations_or_failure_cases": "UT performed worse than baseline, indicating naive replay usage can harm LM finetuning. Reweighting with advantages was sensitive and produced no consistent improvement. Reward-based selection (RT) is game-dependent and provided only marginal gains. These strategies are more general but less effective than state-feature categorization.",
            "recommendations_or_conclusions": "General-purpose reweighting by advantage or naive replay buffers are insufficient; if game-specific signals (like location change) are unavailable, expect only modest gains from RT or advantage-weighting. Careful selection and labeling of transitions is critical.",
            "uuid": "e8210.1",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CALM (frozen GPT-2 baseline)",
            "name_full": "CALM: GPT-2 adapted on human gameplay (ClubFloyd) and kept frozen for action recommendation",
            "brief_description": "Baseline approach (from prior work) where GPT-2 is adapted on a large human gameplay corpus (ClubFloyd) to propose candidate actions for a DRRN, but the LM remains frozen during in-game learning (no in-game memory updates).",
            "citation_title": "Keep CALM and explore: Language models for action generation in textbased games",
            "mention_or_use": "use",
            "agent_name": "CALM (frozen GPT-2)",
            "agent_description": "GPT-2 adapted on ClubFloyd human gameplay transcripts produces candidate actions; a DRRN selects actions and learns via TD updates. The LM is not updated from in-game transitions during gameplay (no LM-in-the-loop finetuning).",
            "llm_model_name": "GPT-2 (base)",
            "llm_model_description": "GPT-2 base pretrained and finetuned on ClubFloyd dataset (≈217K context-action pairs) before gameplay; frozen during RL training.",
            "benchmark_name": "Jericho (same 10 games)",
            "benchmark_description": "Same interactive fiction benchmark as above.",
            "memory_used": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_integration_strategy": "The LM is adapted offline using human gameplay and then left frozen; the DRRN uses its own replay buffer for RL but the LM does not receive in-game experience.",
            "performance_with_memory": null,
            "performance_without_memory": "avg.norm = 20.1% (Table 1); per-game example: Zork1 = 30.7.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Used as baseline for comparisons. The paper shows LM-in-the-Loop (OC) improves avg.norm to 24.0% and accelerates convergence compared to CALM. Also evaluated CALM when adapted with only 10% of ClubFloyd data (avg.norm 18.5%) to show LM-in-the-Loop can reduce reliance on large human corpora.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Performance sensitive to size of annotated adaptation corpus; when adaptation uses only 10% of ClubFloyd, performance drops substantially. Because the LM is frozen, it cannot incorporate game-specific in-game transitions.",
            "recommendations_or_conclusions": "Using in-game transitions to finetune the LM (LM-in-the-Loop) can reduce dependence on large human adaptation datasets and accelerate convergence; keeping the LM frozen is less data-efficient.",
            "uuid": "e8210.2",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-2 as policy (ablation)",
            "name_full": "GPT-2 acting as a policy network (argmax LM action) without DRRN",
            "brief_description": "An ablation where GPT-2 (either frozen or trained in-game) directly selects the argmax action (no DRRN). This tests whether LM can act as the policy rather than just a candidate generator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-2 as policy (no DRRN)",
            "agent_description": "GPT-2 is used to generate and pick the highest-likelihood action as the agent's action (argmax), either kept frozen or finetuned with in-game transitions, with no separate DRRN decision module.",
            "llm_model_name": "GPT-2 (base)",
            "llm_model_description": "Same GPT-2 base; tested both frozen and in-game finetuned variants.",
            "benchmark_name": "Jericho (subset of games tested as ablation)",
            "benchmark_description": "Same interactive fiction benchmark but used to test whether LM-only policy can perform.",
            "memory_used": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_integration_strategy": "LM was optionally finetuned in-game (LM-in-the-Loop) but used directly to select actions (argmax) instead of only proposing candidates to DRRN.",
            "performance_with_memory": "Irrespective of frozen or in-game trained, GPT-2 as policy yielded zero or near-zero scores in the majority of games (Table 3 reports many zeros; overall normalized score ~1.1%). Example table row: many games 0 [0]; norm 1.1%.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared frozen vs in-game-trained GPT-2 acting as policy and contrasted with DRRN-based agents; found that LM-only argmax policy performs very poorly while DRRN+LM candidates performs much better.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "GPT-2 as a standalone policy fails on most games, indicating that LM likelihood alone does not align with action-value and planning requirements; finetuning LM in-game does not remedy this sufficiently.",
            "recommendations_or_conclusions": "Do not replace an RL decision-maker with LM argmax actions; the DRRN performs the planning and value estimation that the LM likelihood does not capture.",
            "uuid": "e8210.3",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in textbased games",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Pre-trained language models for interactive decisionmaking",
            "rating": 2,
            "sanitized_title": "pretrained_language_models_for_interactive_decisionmaking"
        },
        {
            "paper_title": "Reading and acting while blindfolded: The need for semantics in text game agents",
            "rating": 1,
            "sanitized_title": "reading_and_acting_while_blindfolded_the_need_for_semantics_in_text_game_agents"
        },
        {
            "paper_title": "Do as i can. not as i say: Grounding language in robotic affordances",
            "rating": 1,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances"
        }
    ],
    "cost": 0.014031249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games
13 Nov 2023</p>
<p>Arjun Vaithilingam Sudhakar 
Mila -Quebec AI Institute</p>
<p>Ecole Polytechnique de Montreal</p>
<p>Prasanna Parthasarathi 
Janarthanan Rajendran 
Mila -Quebec AI Institute</p>
<p>University of Montreal</p>
<p>Sarath Chandar 
Mila -Quebec AI Institute</p>
<p>Ecole Polytechnique de Montreal</p>
<p>CIFAR AI Chair
Canada</p>
<p>Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games
13 Nov 2023E9CFC669378BBD22F5BE438C7D800830arXiv:2311.07687v1[cs.CL]
Large Language Models (LLMs) have demonstrated superior performance in language understanding benchmarks.CALM, a popular approach, leverages linguistic priors of LLMs-GPT-2-for action candidate recommendations to improve the performance in text games in Jericho without environment-provided actions.However, CALM adapts GPT-2 with annotated human gameplays and keeps the LLM fixed during the learning of the text based games.In this work, we explore and evaluate updating LLM used for candidate recommendation during the learning of the text based game as well to mitigate the reliance on the human annotated gameplays, which are costly to acquire.We observe that by updating the LLM during learning using carefully selected in-game transitions, we can reduce the dependency on using human annotated game plays for fine-tuning the LLMs.We conducted further analysis to study the transferability of the updated LLMs and observed that transferring in-game trained models to other games did not result in a consistent transfer.</p>
<p>Introduction</p>
<p>Large Language models (Devlin et al., 2019a;Radford et al., 2018b;Ouyang et al., 2022) (LLMs) trained on large corpora of unstructured text corpora are the state-of-the-art models in several Natural Language Understanding (NLU) benchmarks.Bender and Koller (2020) argue in their position paper that the models trained largely from static benchmarks rely to the form rather than understanding the meaning.While it is imperative to understand the learning dynamics of LLMs (Rogers et al., 2020;Webson and Pavlick, 2021), introducing novel language understanding challenges pushes the frontiers for LLMs' applications.There has been a recent interest in interactive training of large language models in situated learning environments.Bisk et al. (2020); McClelland et al. (2020) Figure 1: Sample gameplay from zork1 game in Jericho using LM for action recommendation: LM recommends action candidates based on the observation from env.The RL agent selects an action from the candidates.point out the necessity for LMs to have enhanced language understanding and meaning through interacting with the physical world.Also, Lake and Murphy (2021) argues that LMs fall short in their communicative usage, requiring reasoning over intents despite their success in static datasets.</p>
<p>Training decision making agents over textual information for playing text-based games (Hausknecht et al., 2020;Côté et al., 2018) has been a recent usecase for LLM.While decision making has been the front of text-game playing, such games introduce novel challenges for language understanding, and domain adaptation for LLMs.Yao et al. (2020) used GPT-2 (Radford et al., 2018b) to generate candidate actions for the decision making DRRN module (He et al., 2016) in Jericho benchmark of text based games.Such a set up allows for qualitatively understanding the LLMs' abilities to understand, reason, and adapt to novel situations.In a typical text-based game, as in Figure 1, an agent receives a textual observation about its environment that it has to understand and reason over the possible actions to pick one and proceed.</p>
<p>While learning from scratch is time-consuming, Yao et al. (2020) make use of linguistic priors in LLMs to prune the combinatorially large action space.The authors adapt GPT-2 for the task with a corpus of human game play on similar games-ClubFloyd.After the adaptation phase, the model remains frozen throughout the learning that happens within the game.</p>
<p>Further, Yao et al. (2020) also note that the performance on the text-based games in Jericho benchmark was sensitive to the size of the annotated human gameplay corpus; such reliance adds to the cost.On the one hand in-game transitions remain unutilized for training the LLM, and on the other there is a need to mitigate the reliance on human annotated transitions to scale applications of LLMs.Although one can make use of the transitions to train the model, the solution requires a comprehensive analysis on what such a LM-in-the-Loop training entails.Toward that, we explore LM-inthe-Loop by building over the setup in Yao et al. (2020) by training GPT-2 using in-game generated transitions.Further, we analyze such a set up along the metrics of: (1) Improvement in performance, (2) Acceleration in convergence, (3) Reliance on human annotated transitions, (4) Replacing GPT-2 as a policy network, (5) comparing reward, state based transitions selection for LM training, and (6) Generalization of LM-in-the-Loop trained LM to other games.The main findings of the approach are summarized as follows:</p>
<p>• LM-in-the-Loop reduces emphasis on human annotated transitions and enables accelerated convergence.</p>
<p>• State feature based transitions selection provided greater gains than other alternates.</p>
<p>• LM-in-the-Loop does not always transfer to other games.</p>
<p>• Although LM-in-the-Loop improved candidate suggestion, GPT-2 as policy network did faired poorly across games.</p>
<p>Related Work</p>
<p>Text Games: Jericho (Hausknecht et al., 2020) is a popular learning environment that supports 32 human-written interactive fiction games.These games are designed to be difficult for human players, serving as a more realistic training ground to evaluate language understanding agents.Compared with frameworks like TextWorld (Côté et al., 2018), these games have significantly more linguistic variety and larger action space.Jericho environment provides a smaller list of candidate actions that can be used to train reinforcement learning (RL) agents.Approaches like DRRN (He et al., 2016), TDQN (Hausknecht et al., 2020), and KGA2C (Ammanabrolu and Hausknecht, 2020) have used handicap to operate on small action space and learn only through in-game rewards.Towards using large LMs, environment provided actions are replaced with LM generated actions like with GPT-2 (Yao et al., 2020), or BERT (Singh et al., 2021).Li et al. (2022) train LMs to remember optimal trajectories to swiftly move to novel game regions.</p>
<p>Data Efficiency: LLMs (Devlin et al., 2019b;Brown et al., 2020) are pretrained with tremendous amount of unstructured text data from the web using a generic language modeling objective.Adapting the models to a downstream tasks (Khashabi et al., 2020;Rajpurkar et al., 2016;Zhang et al., 2015;Maas et al., 2011), however, has been shown to greatly affected by the quality of supervision and the size of the dataset.As reliance on annotated data makes their application hard to scale, techniques like data augmentation (Feng et al., 2021), using distilled models (Radford et al., 2018a), learning from toyish data (Wu et al., 2022) has been explored has alternatives.However, the approach of making LLMs interactive to be trained in a situated learning environment to reduce the need for annotations is only recently getting popular.</p>
<p>3 Background</p>
<p>Text Games</p>
<p>In text-based games, at each step t, a learning agent interacts with the game environment by generating a textual action a t ∈ A t that is relevant to the textual observation o t .The agent receives a scalar reward r t = R t (o t , a t ).The agent maximizes the expected cumulative rewards (r 0 , r 1 , r 2 , . . .r N ), until the end of an N -step long episode.</p>
<p>DRRN and Advantage Function</p>
<p>A popular deep RL method used in text-based games is the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016).The observation (o) and actions (a) are first encoded using separate recurrent neural network encoders (such as a GRU (Chung et al., 2014)) f o and f a respectively.A decoder g then combines the representations to obtain the Q-value using a network parameterized by Φ:
Q Φ (o, a) = g(f o (o), f a (a)).(1)
The DRRN learns to estimate the Q-value through iteratively updating Φ with experience sampled from a prioritized experience replay buffer with the temporal difference (TD) loss:
L T D (Φ) = r + γ max a ′ ∈A Q Φ (o ′ , a ′ ) − Q Φ (o, a) 2 ,
(2) where r and o ′ are the reward and the observation received after taking action a upon observing o, and γ represents the discount factor.</p>
<p>Advantage function: An estimate how good an action, a, is when chosen in a state, o, is obtained by subtracting the value of the state (V (o))a weighted average of the Q-values-from the Q(o, a) of that particular action in that state.
A(o, a) = Q Φ (o, a) − V ψ (o)(3)
Q-Value estimates the expected reward after a specific action was played, whereas V ψ (o) is the parameterized estimate of the expected reward from being in o before an action was selected.</p>
<p>LLM for Action Recommendation</p>
<p>Consider a dataset D of N transitions of human gameplay across different games organized in context-action pairs as ((o j−1 , a j−1 , o j ), a j ).For example: a sample could be like, " [CLS]. . . to the north is a restaurant where the mayor ate often.to the east is the mayor's home.2020) uses ClubFloyd to adapt a pretrained GPT-2 model with causal language modeling task.The motivation is to enable the linguistic prior of GPT-2 to adapt to the games and provide better action recommendations to the DRRN.</p>
<p>Methodology</p>
<p>LM-in-the-Loop to recommend Actions</p>
<p>The game playing agent follows trajectories that are rewarded according to the rules of the game in the Jericho environment.The environment has two scenarios-with and without handicap-which correspond to whether the actions can be generated from within the possible actions suggested by the environment or without any limitations by the environment respectively.The with handicap set up evaluates the agent exclusively on planning with the actions provided, while the without handicap requires the agent in addition to understanding the observation also generate acceptable candidates.In Yao et al. (2020), the LLM is kept constant throughout the gameplay and that assumption could be only validated if Jericho games share significant similarity with the transitions in ClubFloyd.However, MAUVE-Score1 between the human transitions and the game transitions in Jericho did not overlap significantly (Table 6 in §A.4), suggesting that the adapting from in-game transitions is needed.</p>
<p>Toward that, we explore the feasibility, prospects, and challenges that entail training LM-in-theloop post finetuning with human gameplays in ClubFloyd adaptation as in Table 1.We use a similar set up for action recommendation as in Yao et al. (2020), where a pretrained GPT-2 LM is adapted with Clubfloyd dataset to recommend actions to DRRN agent.In addition to training the DRRN agent with TD-learning (Equation 2), we collect the transitions (o t , a t , o t+1 , r t+1 ) throughout the game episode, e T D , and populate them in D + and D − based on a heuristic that depends on-reward, return, and the game states.</p>
<p>First, with LM parameterized by θ and generating action candidates, we train DRRN for n RL consecutive episodes.After n RL episodes, we sample d LM sized dataset from D + , and D − with probabil- ities p + and 1 − p + respectively for 2000 gradient steps at finetuned after every k game steps.To train LM we use a weighted cross-entropy loss:
L LM (θ) = −E (at,ot)∼(D + ,D − ) log P θ (a t | o t )•h (•)
(4) Then, we plug-in back the in-game trained LM to recommend actions for the DRRN agent.The maximum buffer size of D + , D − , p + , d LM , and n RL are all game-specific hyperparameters.The h (•) is defined as a function of reward r t , or action-advantage, A(o t , a t ), or assumed 1 uniformly ∀(o, a) ∈ O × A. We evaluate different approaches based on the sampling of transitions, and the loss function (L), used for training the language model.Approaches for LM-in-the-Loop based on the construction of D, and sampling are:</p>
<p>Uncategorized Transitions (UT): In this setting the transitions stored in the buffer are not categorized by any special heuristic function.We simplify this approach by maintaining a single buffer, D in place of two.This is a weaker baseline than other heuristics to select useful transitions based on their importance.</p>
<p>State Feature Categorized (OC): In this, the transitions are labeled as useful or not based on whether an action a t resulted in reward increase or if the agent's location changed.i.e., moved from one room to another.As the location information received is an artifact of the game framework, we consider this as the Oracle.Further, we vary p + to maximize the transitions that encourage exploration to eventually result in improved performance in the game.Here, h (•) is fixed as
1 uniformly ∀(o, a) ∈ O × A.
Reward Trajectories (RT): The reward from transitions, r t , is used to categorize positive and negative trajectories.When r t &gt; 0 all transitions up until the earlier non-zero reward are considered positive and added to D + .</p>
<p>Further, we explore utilizing the return, reward, and advantage function of actions to re-weight L LM using the h (•) function over UT setting as above.We describe them as follows:</p>
<p>Weighted Cross-Entropy: In this, the transition data is kept in a single buffer D similar to in the UT setting.To finetune the language model using the weighted cross-entropy loss (Equation 4), we use the exponential weighted advantage function (Equation 3).We use two variants to the weights, wherein UT EA is non-negative using h(•) function:
h(o t , a t ) = e β•A(ot,at) ,(5)
where, β ∈ R + is a hyperparameter.The other variant, UT LA , allows for negative weights with h(•) as follows:
h(o t , a t ) = 1 + β • A(o t , a t ),(6)
where, β ∈ R + is a hyperparameter.</p>
<p>Experiments</p>
<p>We perform comprehensive experiments2 with LMin-the-loop set up to study the following questions:</p>
<ol>
<li>
<p>Does including the language model in the training loop improve performance?</p>
</li>
<li>
<p>Does LM-in-the-Loop mitigate the reliance on human gameplay transitions?</p>
</li>
<li>
<p>Should the transitions be categorized for improved learning?</p>
</li>
<li>
<p>Can we make LM itself a policy network without DRRN with LM-in-the-Loop?</p>
</li>
<li>
<p>Does training LM-in-the-Loop affect generalization to other games?</p>
</li>
</ol>
<p>Task Adaptation Dataset</p>
<p>ClubFloyd dataset (Yao et al., 2020) is a collection of crawled data from the ClubFloyd website.The dataset comprises of gameplay from experienced players; however, they may not be familiar with the particular games.The data is preprocessed and contains around 217K pairs of context an in the form of ((o j−1 , a j−1 , o j ), a j ).</p>
<p>Benchmark and the Metric</p>
<p>Jericho (Hausknecht et al., 2020) is a learning environment that supports human-written interactive fiction games as described in Figure 1.We chose 10 games based on the diversity in the challenges faced in each game such as large action space, solution length, and reward sparsity as mentioned in Hausknecht et al. (2020).We use the average of the last 100-episodes' score with standard error for individual games (Hausknecht et al., 2020) as our metric for evaluation.</p>
<p>In addition, we report the average score normalized (avg.norm) against the maximum score possible in each of the games, which estimates the human-machine gap in text-based games.Finally, we also report the relative performance percentage difference between the baseline and the best approach mentioned as ∆% in Table 1 to capture the improvement as the range of the scores in each game is different.</p>
<p>Model Details</p>
<p>Language model (GPT-2) is first finetuned on ClubFloyd dataset.</p>
<p>Given the context, (o j−1 , a j−1 , o j ), the finetuned GPT-2 proposes action candidates for DRRN to choose.Following that, every action candidate and context is encoded with a GRU.Then a decoder combines the representations to estimate the Q-value using a multilayer Perceptron (MLP) and updates the DRRN agent parameter Φ.During the training process of the DRRN agents, the context-action pairs are stored in the replay buffers.After k steps, we sample d LM sized dataset from D + , and D − with probabilities p + and 1 − p + respectively and update the language model with in-game transitions.Then, the updated language model is used to propose the action candidates.</p>
<p>The buffer size is defined as 100K for replay buffers that uses First-In-First-Out (FIFO) strategy to replace samples.To train, d LM samples are sampled uniformly at random from the two buffers D + and D − .However, the probability of choosing the buffers are defined by p + and p − (1 − p + ) respectively.The number of gradient steps for LM training is fixed at 2000 across the set ups.And, across games we experiment with the hyperparameter p + ∈ [0, 1] in 0.1 increment, and the value for LM finetuning frequency k ∈ [2k, 5k, 10k, 20k].The results tabled are estimated from 5 runs.</p>
<p>Results</p>
<p>We follow the questions enumerated in §5 to analyze the effect of in-game learning of language models for action recommendations.</p>
<p>Effect on Performance</p>
<p>To understand the effect on performance with LMin-the-Loop, we follow the experimental set up in §5.3 to evaluate on Jericho benchmark.Table 5 compares the different methods detailed in §4.1 with reproduced norm score of CALM (Yao et al., 2020) as the baseline.We see that categorizing the transitions using state features (OC) scored the highest in all tasks, suggesting that LM-in-the-Loop enables improved performance.This was also reflected in the avg.norm score with an improvement of ≈ 4% over the baseline.This is ≈ 53% more avg.improvement over the scores obtained by the baseline model.Although the performances of OC are closer to the baseline in many games, the in-game training accelerated the convergence in most games.However, the improvement with OC is, in a way, a loose upperbound to in-game learning with LMin-the-Loop, as special techniques to reweight the transitions (UT LA , and UT EA ), or reward based categorization RT only improved the avg.norm score by ≈ 0.6%.On the other hand, the avg.norm score with Uncategorized Transitions (UT)   dropped to 19.2% which is ∼ 1% below the baseline performance.The difference in performance between UT, and OC with the baseline suggests that LM-in-the-loop for action recommendation is helpful but requires careful selection of transitions for training the language model.In Figure 3, we compare the % of steps in-game learning methods took in average to achieve k% of CALM model's best performance across the games.We see that LM-in-the-Loop techniques enabled atleast 2× on average 3 acceleration in convergence, although the weaker alternatives to OC with reward based categorization, and reweighted techniques only provided meagre improvements 3 Individual comparison of each method across the games is in §B.</p>
<p>over the baseline (Table 5).This shows that the adaptation offered with the ClubFloyd dataset was insufficient, and off-the-shelf techniques can drastically accelerate convergence.</p>
<p>Emphasis on Human Annotations</p>
<p>CALM model-the baseline-uses all of the ∼ 220K transitions in the ClubFloyd dataset to adapt GPT-2 model for action recommendation.But, by using in-game transitions for LM-in-the-Loop training, the LM is provided with game specific information.So, the requirement for adapting GPT-2 with human annotated transitions should be minimal.Yao et al. (2020) show that CALM's performance decreased significantly when adaptation was done with 10% of ClubFloyd dataset.The reproduced results of CALM with 10% of adaptation data shows the avg.norm score as 18.5% across the games in Table 2. Using State features (OC) with 10% of the adaptation date achieved an average norm score of 21.8%, which was more than even using 100% of the adaptation data with CALM.Although there was a small decline in the performance of the detective game, it was insignificant because it was still within the standard error.These results suggest empirically that we can reduce the burden of collecting human-played or human-annotated data by doing in-game learning.</p>
<p>Effect of Weight Adjusted LM Loss</p>
<p>Categorization of transitions, although possible in most games, often requires game specific functions to identify what is a good and a bad transition.However, a generalized technique would be to use a notion of the usefulness of transitions that don't require game specific mechanisms.We explore reweighted cross entropy loss as in Equation 4with variations of the h(•) functions from being uniformly distributed as 1 over (o, a) ∈ O × A to using advantage function with two variations as in Equation 5and Equation 6.While UT uses vanilla cross-entropy loss to train the LM on transitions sampled from buffer D, UT EA and UT LA adjusts the experience according to the advantage, A(o, a), of the actions chosen in those observations.We use causal language modeling to train the GPT-2 LM to discourage the LM in generating a useful action in a state and discouraging the not useful.As A(o, a) ∈ [−∞, +∞], it is important to understand how it affects the language model.A negative advantage for a ′ in o ′ should discourage the LM from suggesting a ′ in o ′ .UT EA rescales the LM-loss with h(•) ∈ [0, 1), while UT LA works similar to Unlikelihood training as proposed in Welleck et al. (2019) by maintaining the same scale as A(o, a).But, from the restuls we see that the differences in reweighting did not tangible affect the performance as seen in Table 5 (Columns UT EA and UT LA ).</p>
<p>GPT-2 as Policy Network?</p>
<p>So far, we have explored the performance of LMin-the-Loop training of GPT-2 for suggesting candidate actions for the DRRN, but to disambiguate the role of GPT-2 and DRRN, we conduct an ablation experiment.Instead of providing action candidates to DRRN agent, what if GPT-2 chose the argmax action?The experiment addresses two questions:</p>
<p>(1) Is the improvement in the performance and acceleration as in §6.1 largely from the GPT-2 training?and (2) Does the max action of the LM reflect the game dynamics?</p>
<p>Games</p>
<p>Frozen LM In-game LM
Zork1 3.3 [5.7] 3.3 [5.7] Inhumane 0 [0] 0 [0] Detective 23.3 [5.7] 15 [7] Zork3 0 [0] 0 [0] Omniquest 0 [0] 1.6 [2.8] Library 0 [0] 0 [0] Balances 0 [0] 0 [0] Ludicorp 5.3 [2.0] 4.1 [2.9] Dragon 0 [0.0] 0 [0.] Ztuu 0 [0.0] 0 [0.0] Norm 1.1% 1.1%
Table 3: Irrespective of whether the LM was maintained frozen or trained with LM-in-the-Loop, GPT-2 model as policy network yielded zero in the majority of games when DRRN is not used for decision-making.</p>
<p>McClelland et al. ( 2020) motivate the set up of a language model placed in situated learning set up, where it can interact and learn from the environment.However, other than the study conducted in this work, there exists little evidence for interactive learning of an LM from the game transition.Table 3 shows the results of how domain adapted pretrained GPT-2 fairs in the ultimate goal of learning solely from interaction on the text games.We observe that the model's performance is 0 in most games when not using DRRN for decision making.Irrespective of whether the LM was kept frozen or trained with in-game transitions, there was no palpable evidence of language understanding through game semantics observed.</p>
<p>But, the possible explanation for the performance in §6.1 is that the language model learns more game specific actions, though not optimal, leading to DRRN contributing significantly to the performance observed.</p>
<p>Generalization to Other Games</p>
<p>We observed from the previous results that the agent performing well could be attributed to the actions suggested by the LM that that adapted from the transitions in-game.While that is encouraging, it also risks the generality of such an agent in being transferrable to other games.To quantify the loss in generality, we use the LM-in-the-Loop trained GPT-2 from zork1 game and continue to train with 4 different target games-zork3, Ludicorp, inhumane, and Ztuu.</p>
<p>For the settings to compare, we used using state features (OC), and Uncategorized Transitions (UT).To train the LM model, we set the buffer size as 100K in both the settings.The results tabled
0 [0] 1 [2.1]
Table 4: Transferring LM-in-the-Loop trained GPT-2 did not provide guarantee improvements over the baseline.Also, the performance remained unexplainable with A ≈ and (A × O) ≈ .</p>
<p>in Table 4 shows that in-game learning techniques suffered from extending to other games when compared with the baseline performance of CALM.As LM-in-the-Loop training performed well on these games when trained in isolation as seen from Table 5, it would be interesting to observe if that depended on some notion of similarity between the source and the target games.</p>
<p>To that end, we define two measures of similarity using the actions, A ≈ , and the action-observation combinations (O × A) ≈ with the target games.For A ≈ we populate the possible actions available from the Jericho environment on the source game, zork1, and each of the target games considered.We estimate the BLEU-2 (Papineni et al., 2002) score for every action in the source, a ∈ A s , with all actions in the target game, A t , as the reference.The average over the corpus BLEU is tabled in the A ≈ column in Table 4. But, A ≈ did not have any reasonable correlation over the performance observed.While zork3 had an expected action space similarity with the source game and did not have a significant performance drop, the counterfactual scenarios for when A ≈ is much lower in other games, the performance was mixed.Although in inhumane and Ztuu, OC performed significantly lower than CALM, in Ludicorp the performance was better than the baseline rendering the action similarity score, A ≈ , ineffective in explaining the results.Also, the MAUVE score measured between the game transitions, (O × A) ≈ , was too low suggesting a weak semantic overlap between the game spaces.If generality were to be affected with lower similarity between the source and target games measured along action, action×observations, the results were inconsistent in that regard.</p>
<p>Although Yao et al. (2021) observed that the models did not naturally respect the notion of semantics, the results that the learnability in the target games being strongly affected when the LM-in-the-Loop is adapted to zork1 does not entail the LMs being agnostic to notions of semantics.At the same time, the results we observed doesn't either sug-gest that semantics guide the results.Such mixed observations probably only suggest that notions of semantics through automatic evaluations are not the tools to interpret LMs in text games.</p>
<p>Discussion</p>
<p>The comparison of LM-in-the-Loop with baseline and their absolute performances from Table 1 shows that there is more room for improvement.Despite the LMs having strong linguistic priors from pretraining, the large action space when it comes to generative task is one of the significant challenges in adapting LMs to text-based games.Although interactive learning is promising, towards realizing interactive task solving agents, it is imperative to address the issues due to scalability and data-efficiency.The results in the paper through exploring the possibility of adapting language models for action suggestions through utilizing the ingame generated transitions opens up discussions on several key questions:</p>
<p>While there is improvement in performance, and acceleration in comparison to not learning from the game transitions, the absolute improvement with respect to the games has still a long way to go.When DRRN module was plugged out for ablation, the argmax action of LM was not even close to a reasonable performance indicating the heavy lifting in planning was from DRRN.Towards realizing LMs in situated learning environments, adapting LMs to different games is a challenging language understanding milestone.Specifically, it is important to align LM's action generation likelihood to reflect the action value function.</p>
<p>Despite the acceleration and a reduced need for human transitions to adapt LMs for action suggestion, interpreting their performance through the conventional lens of automatic semantic and syntax scores is less effective.It is, then, only imperative to make the application of LMs in text games interpretable through automatic metrics that identifies important transitions to train LM-in-the-Loop.</p>
<p>Limitations</p>
<p>The paper analyzes the possibility and challenges in LM-in-the-Loop training of GPT-2 model for action recommendation in text based games.The claims in the work can be further supported with experiments on different LLM.Similarly, the generalization experiments could have added more support to the lack of evidence with additional games.However, these are compute intensive experiments and the claims are largely made in consideration to the limitations in the set up.</p>
<p>[SEP]  northeast[SEP] . . .you are carrying nothing.you are still on the streets. . . .[SEP] northeast''.[SEP] and [CLS] are special tokens specific to LM-training.Yao et al. (</p>
<p>Figure 2 :
2
Figure 2: Training LM-in-the-Loop post-human-annotated dataset adaptation: RL agent (DRRN) picks the action recommended by the language model (at T ), which is GPT-2.The context pairs are stored in the replay buffers that are categorized by some heuristic.Then the Language model is updated with in-game transitions after k learning steps in the game.Finally, the updated language model (T + k) actions are recommended.</p>
<p>Figure 3 :
3
Figure 3: We see that LM-in-the-Loop techniques only need half of the steps to achieve the best of CALM.Whereas, using state feature based categorization (OC) achieved better acceleration and performance over the rest.</p>
<p>Figure 4 :
4
Figure 4: Comparison of learning dynamics of the different LM-in-the-Loop techniques with the baseline CALM agent across the selected 10 games in Jericho.</p>
<p>[2.7] 288.5 [1.5] 289.3 [0.2] 288.3 [1.3] 285.1 [5.6] 288.5 [1.5]
GamesCALMUTUT LAUT EARTOC∆(%)MaxScoreZork130.7 [4.8]32.6 [4.4]30.4 [8.5]35.6 [5.7]30.7 [3.8]38.0 [1.7]23%350Inhumane24.8 [2.7]21.9 [5.24] 28.9 [11]27.3 [3.1]29.1 [12.7]43.4 [3.8]75%90Detective290.9 0%360Zork30.3 [0.09]0.3 [0.14]0.4 [0.1]0.6 [0.1]0.6 [0.1]0.7 [0.2]133%7Omniquest6.7 [0.3]6.0 [0.6]6.6 [0.9]6.6 [1]6.0 [0.79]7.8 [1.7]16%50Library11.2 [1.3]9.3 [1.1]9.5 [1]10.3 [0.2]10.3 [1.8]12.1 [0.7]8%30Balances9.3 [0.2]9.6 [0.1]9.6 [0.2]9.5 [0.2]9.7 [0.2]9.7 [0.1]4%51Ludicorp10.4 [0.7]11.4 [2.6]12.5 [1.1]11.9 [2.6]11.3 [3.1]15.1 [0.8]45%150Dragon0.1 [0.06]0.1 [0.1]0.3 [0.3]0.3 [0.3]0.1 [0.12]0.3 [0.2]200%25Ztuu3.8 [0.18]4.4 [0.0]4.5 [0.2]4.4 [0.1]4.3 [0.1]4.5 [0.1]18%100Norm Score 20.1%19.1%20.6%20.9%20.7 %24.0%52.37% 100%</p>
<p>Table 1 :
1
From the results, it can be consistently seen that LM-in-the-Loop provides a performance improvement over CALM.Especially, categorizing the transitions with state features (OC) scored the highest with ∼ 53% improvement over the scores obtained by the baseline model.</p>
<p>Table 2 :
2
Using State Features (OC) achieved an average norm score of 21.8% with 10%, which was more than even with CALM using 100% of the adaptation data.
GamesCALMCALMOC100%10%10%Zork130.7 [4.8]29 [3.4]35.1 [2.3]Inhumane24.8 [2.7]15.7 [14.7] 27.5 [6.8]Detective290.9 [2.7] 289.5 [0.2] 289.6 [0.2]Zork30.3 [0.09]0.6 [0]0.7 [0.3]Omniquest 6.7 [0.3]5.9 [0.8]6.0 [1]Library11.2 [1.3]10.5 [1.5]10.2 [1.8]Balances9.3 [0.2]6.6 [3.5]8.6 [1.6]Ludicorp10.4 [0.7]10.2 [0.4]13.7 [0.4]Dragon0.1 [0.06]0.1 [0.06]0.3 [0.2]Ztuu3.8 [0.18]3.6 [0.1]4.1 [0.1]Norm20.1%18.5%21.8 %
 MAUVE-Score (Pillutla et al.,<br />
) measures semantic relatedness of an LM generated text with that of human generated text distribution using an LLM representation of the texts.
The codebase for all experiments will be released after the anonymity period.
AcknowledgementsSarath Chandar is supported by a Canada CI-FAR AI Chair and an NSERC Discovery Grant.The authors acknowledge the computational resources provided by the Digital Research Alliance of Canada and Mila Compute resources.We are thankful to Siva Reddy for their helpful feedback in this work.A AppendixA.1 Language Model SetupWe use a GPT-2 (Base)(Radford et al., 2018b)model with 12-layers, 768-hidden units, and 12attention heads with 117M parameters pre-trained on the WebText corpus.This model's implementation and pretrained weights are obtained from(Wolf et al., 2020, Huggingface).We train for 3 epochs on the ClubFloyd dataset following(Yao et al., 2020)to minimize the crossentropy loss, as shown in Table5.We use AdamW to optimize model's weights to minimize the loss, with the learning rate as 2 × 10 −6 and Adam epsilon as 1 × 10 −9 .We use a linear schedule with a warmup of 0.1 for the learning rate.Finally, we clip gradients with a maximum gradient norm of 1.Following(Yao et al., 2020)'s finetuning process, we exclude using Jericho-related transcripts by setting the flag as 1.We used random seeds to select the dataset to avoid bias in selecting data for the LM training.Model MetricFinalA.2 Reinforcement Learning Agent Setup:We train on 10 interactive fiction games from the Jericho benchmark(Hausknecht et al., 2020).The states are observations concatenated with items in possession of the player and their current location description provided by the game engine using commands inventory and look.A single game episode runs for 100 environment steps at max or gets terminated before the game is over or won.We use the look and inventory commands to add location and inventory descriptions to observations, followingHausknecht et al. (2020).We train DRRN asynchronously on 8 parallel instances of the game environment for 100, 000 steps for each game.At each step, the Q-value is estimated using the DRRN agent, and the action is selected based on the soft-exploration policy.Action's admissibility is predicted based on the textual response of the game.Then, inadmissible are filtered out using a FastText model(Joulin et al., 2017).The agent is optimized using adam optimizer with a learning rate of 10 −5 .We sample transitions of batch size 64 from priority buffer with a priority fraction of 0.5.The discount factor in determining the future reward's importance is 0.9.The size of the embedding dimension is 128, and the hidden dimension is 128.Finally, the gradient is clipped with a maximum gradient norm of 5. We train 5 separate runs for each game and report the average score.We use the average of the last 100 episode scores to calculate the final score.A.3 Software DetailsWe used PyTorch for the code implementation and Huggingface to load pre-trained language models.We used Weights &amp; Biases(Biewald, 2020)for experiment tracking and visualizations to develop insights for this paper.Finally, the seaborn package is used to generate plots.A.4 Mauve Score
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Alex Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Luu, 10.48550/ARXIV.2204.01691Peter Pastor. Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng, Carolina Parada2022Do as i can. not as i say: Grounding language in robotic affordances</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. 2020</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. Emily M Bender, Alexander Koller, 10.18653/v1/2020.acl-main.463Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Experiment tracking with weights and biases. Software available from wandb. Lukas Biewald, 2020</p>
<p>Experience grounds language. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian, 10.18653/v1/2020.emnlp-main.703Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch, Advances in Neural Information Processing Systems. 2021</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, NIPS 2014 Workshop on Deep Learning. 2014. December 2014</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, Textworld: A learning environment for text-based games. 2018</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019a. June 2-7, 20191</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019b. June 2-7, 20191</p>
<p>A survey of data augmentation approaches for nlp. Varun Steven Y Feng, Jason Gangal, Sarath Wei, Soroush Chandar, Teruko Vosoughi, Eduard Mitamura, Hovy, 10.1609/aaai.v34i05.6297arXiv:2105.03075Proceedings of the AAAI Conference on Artificial Intelligence. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, the AAAI Conference on Artificial Intelligence2021. 202034arXiv preprintInteractive fiction games: A colossal adventure</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 10.18653/v1/P16-1153Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics20161</p>
<p>Offline reinforcement learning as one big sequence modeling problem. Michael Janner, Qiyang Li, Sergey Levine, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Bag of tricks for efficient text classification. Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, Proceedings of the 15th Conference of the European Chapter. the 15th Conference of the European ChapterShort Papers; Valencia, SpainAssociation for Computational Linguistics20172</p>
<p>UNIFIEDQA: Crossing format boundaries with a single QA system. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, 10.18653/v1/2020.findings-emnlp.171Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Word meaning in minds and machines. M Brenden, Gregory L Lake, Murphy, 2021Psychological review</p>
<p>Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, Yuke Zhu, arXivPre-trained language models for interactive decisionmaking. 2022</p>
<p>Learning word vectors for sentiment analysis. Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, Christopher Potts, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesPortland, Oregon, USA2011Association for Computational Linguistics</p>
<p>Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. Felix James L Mcclelland, Maja Hill, Jason Rudolph, Hinrich Baldridge, Schütze, Proceedings of the National Academy of Sciences. 117422020</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Advances in Neural Information Processing Systems. Jan Leike, and Ryan Lowe. 2022</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Stabilizing transformers for reinforcement learning. Emilio Parisotto, H Francis Song, Jack W Rae, Razvan Pascanu, C ¸aglar Gülc ¸ehre, M Siddhant, Max Jayakumar, Raphael Jaderberg, Aidan Lopez Kaufman, Clark, CoRR, abs/1910.06764Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. 2019</p>
<p>Mauve: Measuring the gap between neural text and human text using divergence frontiers. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, Zaid Harchaoui, 2021In NeurIPS</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018a</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2018b</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016</p>
<p>Can wikipedia help offline reinforcement learning?. Machel Reid, Yutaro Yamada, Shixiang Shane Gu, CoRR, abs/2201.121222022</p>
<p>Anna Rogers, Olga Kovaleva, Anna Rumshisky, A primer in bertology: What we know about how bert works. 20208</p>
<p>Pre-trained language models as prior knowledge for playing text-based games. Ishika Singh, Gargi Singh, Ashutosh Modi, CoRR, abs/2107.084082021</p>
<p>Prompts and pre-trained language models for offline reinforcement learning. Denis Tarasov, Vladislav Kurenkov, Sergey Kolesnikov, ICLR 2022 Workshop on Generalizable Policy Learning in Physical World. 2022</p>
<p>Multi-stage episodic control for strategic exploration in text games. Jens Tuyls, Shunyu Yao, M Sham, Kakade, Karthik R Narasimhan, International Conference on Learning Representations. 2022</p>
<p>Do promptbased models really understand the meaning of their prompts?. Albert Webson, Ellie Pavlick, arXiv:2109.012472021arXiv preprint</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston, arXiv:1908.04319Neural text generation with unlikelihood training. 2019arXiv preprint</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>Insights into pre-training via simpler synthetic tasks. Yuhuai Wu, Felix Li, Percy Liang, arXiv:2206.101392022arXiv preprint</p>
<p>Deep reinforcement learning with transformers for text adventure games. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang, Chengqi Zhang, 10.1109/CoG47356.2020.92316222020 IEEE Conference on Games (CoG). 2020</p>
<p>Reading and acting while blindfolded: The need for semantics in text game agents. Shunyu Yao, Karthik Narasimhan, Matthew Hausknecht, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>Keep CALM and explore: Language models for action generation in textbased games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 10.18653/v1/2020.emnlp-main.704Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Character-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Advances in Neural Information Processing Systems. Curran Associates, Inc201528</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8754 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8754</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8754</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278739473</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.11628v3.pdf" target="_blank">Critique-Guided Distillation for Efficient and Robust Language Model Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Supervised fine-tuning (SFT) with expert demonstrations often suffers from the imitation problem, where models reproduce correct responses without internalizing the underlying reasoning. We propose Critique-Guided Distillation (CGD), a multi-stage training framework that augments SFT with teacher-generated explanatory critiques and refined responses. Instead of directly imitating teacher outputs, a student learns to map the triplet of prompt, its own initial response, and teacher critique into the refined teacher response, thereby capturing both what to output and why. Our analyses show that CGD consistently reduces refinement uncertainty, improves alignment between critiques and responses, and enhances sample efficiency. On reasoning benchmarks, CGD achieves substantial gains across LLaMA and Qwen families, including +15.0% on AMC23 and +12.2% on MATH-500, while avoiding the format drift issues observed in prior critique-based fine-tuning. Importantly, on LLaMA-3.1-8B CGD approaches or exceeds the performance of SimpleRL-Zero, which is a DeepSeek-R1 replication, while requiring 60x less compute. Beyond reasoning, CGD maintains or improves general instruction-following and factual accuracy, matching baseline performance on IFEval, MUSR, TruthfulQA, and BBH. In contrast, prior critique-based methods degrade these capabilities (e.g., -21% on IFEval). Taken together, these results establish CGD} as a robust and generalizable alternative to both conventional SFT and RL-based methods, offering a more efficient path toward advancing the reasoning and safety of large language models.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8754.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8754.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CGD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critique-Guided Distillation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-stage fine-tuning framework that trains a student model to map (prompt, student initial answer, teacher critique) -> teacher refined answer so that at inference the student produces refined answers in a single pass without needing critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3.1-8B Instruct; S1.1-3B (students) with LLaMA3.3-70B Instruct or S1.1-32B (teachers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Students: LLaMA3.1-8B Instruct and S1.1-3B; teachers: LLaMA3.3-70B Instruct and S1.1-32B. Students are fine-tuned on 100k WebInstruct (or MetaMathQA) examples with DeepSpeed on A100 GPUs; teachers generate critiques and refined answers during training data generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Critique-Guided Distillation (CGD)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Training-time self-reflection: for each prompt x the student first produces an initial answer y'; the teacher produces a critique c conditioned on (x,y') and a refined answer ŷ conditioned on (x,y',c); the student is trained to predict ŷ given (x,y',c). At inference only x is provided and the student generates the refined answer in one forward pass.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math reasoning and general reasoning benchmarks (MATH500, Minerva-Math, GSM8K, OlympiadBench, AMC23; TheoremQA, GPQA, MMLU-PRO) and general QA (IFEval, MUSR, TruthfulQA, BBH).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of math-focused and general reasoning benchmarks including grade/olympiad math tests (MATH500, AMC23, OlympiadBench), arithmetic/word-problem sets (GSM8K), as well as multi-domain reasoning and QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Substantial absolute gains vs SFT/CFT reported. Examples: LLaMA3.1-8B: +15.0 percentage points on AMC23 and +8.0 on OlympiadBench (CGD vs CFT); overall math-group gains +5.4 points vs CFT. S1.1-3B: +12.2 points on MATH500 and +7.5 on AMC23 (vs CFT); overall math-group gains +7.2 vs CFT. CGD average on some comparisons: e.g., overall avg 11.9 vs SimpleRL-Zero 10.6 (see RL comparison table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline SFT/CFT/Distilled SFT numbers reported in paper; e.g., CFT caused IFEval drop from 76.6% to 55.6% (degraded general instruction following). Exact per-benchmark baseline numbers are reported in paper tables for direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Training-time conditioning on teacher-generated critiques and refined answers (explicit textual critiques used as inputs during fine-tuning). No extra inference-time passes; the critique is only present in training data and serves as a conditioning signal.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: consistent absolute accuracy increases across many benchmarks versus SFT, Distilled SFT, and CFT (examples above); ablations show including the critique improves performance substantially (Figure 4). Diagnostic evidence: lower predictive entropy (statistically significant, p < 1e-4), gradient-norm reduction (27% lower with critique), attention analyses that show the model uses critique early in generation, and a counterfactual case study where CGD is robust to misleading critique while baseline is derailed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Gains depend on student model's receptivity to critique-refine format and on teacher quality; multi-stage data generation has nontrivial computational cost (though far cheaper than RL); some student/teacher/architecture pairs show smaller or mixed gains (e.g., occasional declines on AMC23/GPQA for some students like Mixtral and OLMo noted); paper notes remaining compute cost for data generation and sensitivity to student inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms standard SFT, Distilled SFT, and Critique Fine-Tuning (CFT) across most benchmarks; approaches or exceeds SimpleRL-Zero (an RL method) on several tasks while using far less compute; avoids CFT's format drift and the inference-time overhead of multi-pass self-refinement methods such as Self-Refine (which would increase latency 3–4x).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Multiple ablations reported: 'CGD without Critique' (remove critique from input) reduces performance markedly; critique presence reduces gradient norm by 27% and lowers entropy; a 50/50 mix of correct/incorrect student answers during training gives best generalization; CGD is robust to learning-rate changes while CFT is brittle (CFT performance drops >9 points at higher LR).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique-Guided Distillation for Efficient and Robust Language Model Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8754.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8754.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critique Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning method that trains a student model to generate teacher-provided critiques (textual feedback) conditioned on prompt and student initial answer, with the goal of internalizing critique-generation abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Critique fine-tuning: Learning to critique is more effective than learning to imitate.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used as baseline on student models such as LLaMA3.1-8B Instruct and S1.1-3B in this paper's experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CFT trains a student to output teacher-generated critiques (example critiques in paper were generated by LLaMA3.3-70B or GPT-4o in variants); trained on same critique-augmented datasets as CGD for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Critique Fine-Tuning (CFT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train students to generate teacher critiques conditioned on (prompt, student initial answer), so the model can produce critique-style tokens and (in some variants) a final answer in a single pass; objective focuses on predicting critique text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same math and reasoning benchmarks used in this paper (MATH500, AMC23, OlympiadBench, GSM8K, TheoremQA, GPQA, MMLU-PRO, IFEval, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks for math reasoning and general instruction-following and QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Mixed: CFT improves some reasoning benchmarks in prior work and sometimes in this paper, but in these experiments it is consistently beaten by CGD. Examples from paper: CFT (LR=1e-6) average ~39.3 (Table 4) versus CGD ~44.8; CFT caused IFEval accuracy to fall from 76.6% (SFT) to 55.6% in one reported result (format-drift issue).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard SFT and Distilled SFT baselines reported; SFT/Distilled SFT sometimes achieve similar or lower performance than CFT on reasoning tasks but preserve general instruction following better than CFT.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Training objective directly targets critique token generation (supervised learning to output teacher critiques), i.e., the model learns to produce reflective text rather than learning to incorporate critiques into final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>CFT has been reported in prior work to help on some math benchmarks; in this paper CFT provides some gains over vanilla SFT in certain settings but is outperformed by CGD. Empirical evidence in this paper shows CFT is sensitive to learning rate and prone to format drift which harms downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prone to output-format drift (produces critique-like outputs rather than concise answers), brittle to hyperparameters (sharp performance drop at higher LR reported), can degrade general instruction-following (IFEval drop), and gains depend heavily on critique quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to CGD and Distilled SFT in the paper; CGD outperforms CFT across most benchmarks and is more robust/stable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Learning-rate sensitivity ablation shows CFT performance degrades substantially at higher LR (e.g., avg drops from 39.3 to 30.1 in Table 4), and qualitative analyses show format drift early in CFT training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique-Guided Distillation for Efficient and Robust Language Model Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8754.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8754.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (iterative self-feedback at inference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative inference-time method in which a model generates an initial answer, critiques it, and then refines the answer repeatedly (multi-pass self-feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (prior work referenced; not directly re-run in this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior multi-pass prompting approach (Madaan et al., 2023) applied to large LLMs to perform generate-critique-refine loops at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine / generate-then-reflect iterative prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>At inference, the model generates an initial answer, then produces a critique of that answer and uses it to produce a refined answer; this may be repeated multiple times until convergence or budget is exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Referenced in context of reasoning benchmarks (math/reasoning tasks) but not re-evaluated with numbers in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Iterative refinement applied to reasoning and problem-solving tasks in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered multi-pass self-critique and refinement at inference time (no special training modifications in the typical formulation).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper cites prior work showing iterative critique+refinement improves answers but notes the primary downside is inference-time compute/latency (at least a 3–4x increase in cost/latency).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High inference-time computational cost and latency (3–4x or more), which motivates bringing critique/refinement into training (as CGD does) to avoid inference overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Described as effective but expensive compared to CGD which transfers the benefit into a single-pass model via training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique-Guided Distillation for Efficient and Robust Language Model Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8754.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8754.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework where language agents use verbal reflections and reinforcement learning to improve behavior across episodes by generating natural language reflections and revising future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLM agents (prior work cited; not used in experiments here)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent-style LLMs that incorporate verbal reflections as a signal for learning via reinforcement learning; cited as a related form of self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (verbal RL with reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents generate reflective natural-language summaries of failures/successes and use these reflections as a learning signal (often combined with RL updates) to improve future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cited generically as prior work on self-reflection for language agents; no concrete benchmark numbers given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Verbal reflections used as auxiliary learning signals, combined with reinforcement learning updates (external memory or episodic updates may be used in original work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Referenced as an influential prior demonstrating that verbal reflection can improve agent performance, but no new results provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in detail here; cited for conceptual relation to CGD's goal of leveraging critique-like signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique-Guided Distillation for Efficient and Robust Language Model Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8754.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8754.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection Sampling Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning approach that trains on the model's own generated outputs which are verified or ranked by a reward model to incorporate value-based feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as an alternate approach that incorporates a reward model to rank/verify model outputs (Yuan et al., 2023); not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Rejection Sampling Fine-Tuning (RFT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train on a subset of model-generated outputs selected by a reward model (rejection sampling) so the model internalizes higher-quality outputs without explicit critique tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Uses a reward model to select/rank model outputs; supervision comes from selected outputs rather than explicit critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned as an alternative that incorporates value-based feedback; no direct empirical numbers given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed here; cited as part of prior work landscape motivating critique-based training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique-Guided Distillation for Efficient and Robust Language Model Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8754.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8754.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeCRIM / LLMRefine / DCR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeCRIM, LLMRefine, DCR (fine-grained feedback & modular correction pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative prior approaches that provide structured, often fine-grained feedback (error categories, constraints, decomposition) to guide model corrections and refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeCRIM: decomposes instructions into fine-grained constraints; LLMRefine: uses human-defined error categories for targeted feedback; DCR: separates error detection, critique generation, and final refinement into stages. These are cited as related feedback/refinement pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Decompose-critique-refine style feedback pipelines (DeCRIM / LLMRefine / DCR)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Provide targeted, structured critiques or constraints (often human-defined or tool-assisted) that drive focused corrections of model outputs, sometimes staged into detect->critique->refine modules.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Structured prompt/feedback engineering, sometimes combined with external tools or human-defined schemas to pinpoint errors and guide corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Cited literature argues that fine-grained actionable feedback improves correction precision; paper references these as motivating evidence for using informative critiques in CGD.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes such methods often still require a separate critique/refinement pass at inference time (incurring latency) or rely on task-specific tools; not exhaustively evaluated within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critique-Guided Distillation for Efficient and Robust Language Model Reasoning', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Critique fine-tuning: Learning to critique is more effective than learning to imitate. <em>(Rating: 2)</em></li>
                <li>Scaling relationship on learning mathematical reasoning with large language models. <em>(Rating: 1)</em></li>
                <li>LLM self-correction with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints. <em>(Rating: 1)</em></li>
                <li>CRITIC: Large language models can self-correct with tool-interactive critiquing. <em>(Rating: 1)</em></li>
                <li>Distilling reasoning capabilities into smaller language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8754",
    "paper_id": "paper-278739473",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "CGD",
            "name_full": "Critique-Guided Distillation",
            "brief_description": "A multi-stage fine-tuning framework that trains a student model to map (prompt, student initial answer, teacher critique) -&gt; teacher refined answer so that at inference the student produces refined answers in a single pass without needing critiques.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA3.1-8B Instruct; S1.1-3B (students) with LLaMA3.3-70B Instruct or S1.1-32B (teachers)",
            "model_description": "Students: LLaMA3.1-8B Instruct and S1.1-3B; teachers: LLaMA3.3-70B Instruct and S1.1-32B. Students are fine-tuned on 100k WebInstruct (or MetaMathQA) examples with DeepSpeed on A100 GPUs; teachers generate critiques and refined answers during training data generation.",
            "reflection_method_name": "Critique-Guided Distillation (CGD)",
            "reflection_method_description": "Training-time self-reflection: for each prompt x the student first produces an initial answer y'; the teacher produces a critique c conditioned on (x,y') and a refined answer ŷ conditioned on (x,y',c); the student is trained to predict ŷ given (x,y',c). At inference only x is provided and the student generates the refined answer in one forward pass.",
            "task_name": "Math reasoning and general reasoning benchmarks (MATH500, Minerva-Math, GSM8K, OlympiadBench, AMC23; TheoremQA, GPQA, MMLU-PRO) and general QA (IFEval, MUSR, TruthfulQA, BBH).",
            "task_description": "A suite of math-focused and general reasoning benchmarks including grade/olympiad math tests (MATH500, AMC23, OlympiadBench), arithmetic/word-problem sets (GSM8K), as well as multi-domain reasoning and QA benchmarks.",
            "performance_with_reflection": "Substantial absolute gains vs SFT/CFT reported. Examples: LLaMA3.1-8B: +15.0 percentage points on AMC23 and +8.0 on OlympiadBench (CGD vs CFT); overall math-group gains +5.4 points vs CFT. S1.1-3B: +12.2 points on MATH500 and +7.5 on AMC23 (vs CFT); overall math-group gains +7.2 vs CFT. CGD average on some comparisons: e.g., overall avg 11.9 vs SimpleRL-Zero 10.6 (see RL comparison table).",
            "performance_without_reflection": "Baseline SFT/CFT/Distilled SFT numbers reported in paper; e.g., CFT caused IFEval drop from 76.6% to 55.6% (degraded general instruction following). Exact per-benchmark baseline numbers are reported in paper tables for direct comparison.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Training-time conditioning on teacher-generated critiques and refined answers (explicit textual critiques used as inputs during fine-tuning). No extra inference-time passes; the critique is only present in training data and serves as a conditioning signal.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: consistent absolute accuracy increases across many benchmarks versus SFT, Distilled SFT, and CFT (examples above); ablations show including the critique improves performance substantially (Figure 4). Diagnostic evidence: lower predictive entropy (statistically significant, p &lt; 1e-4), gradient-norm reduction (27% lower with critique), attention analyses that show the model uses critique early in generation, and a counterfactual case study where CGD is robust to misleading critique while baseline is derailed.",
            "limitations_or_failure_cases": "Gains depend on student model's receptivity to critique-refine format and on teacher quality; multi-stage data generation has nontrivial computational cost (though far cheaper than RL); some student/teacher/architecture pairs show smaller or mixed gains (e.g., occasional declines on AMC23/GPQA for some students like Mixtral and OLMo noted); paper notes remaining compute cost for data generation and sensitivity to student inductive biases.",
            "comparison_to_other_methods": "Outperforms standard SFT, Distilled SFT, and Critique Fine-Tuning (CFT) across most benchmarks; approaches or exceeds SimpleRL-Zero (an RL method) on several tasks while using far less compute; avoids CFT's format drift and the inference-time overhead of multi-pass self-refinement methods such as Self-Refine (which would increase latency 3–4x).",
            "ablation_study_results": "Multiple ablations reported: 'CGD without Critique' (remove critique from input) reduces performance markedly; critique presence reduces gradient norm by 27% and lowers entropy; a 50/50 mix of correct/incorrect student answers during training gives best generalization; CGD is robust to learning-rate changes while CFT is brittle (CFT performance drops &gt;9 points at higher LR).",
            "uuid": "e8754.0",
            "source_info": {
                "paper_title": "Critique-Guided Distillation for Efficient and Robust Language Model Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CFT",
            "name_full": "Critique Fine-Tuning",
            "brief_description": "A fine-tuning method that trains a student model to generate teacher-provided critiques (textual feedback) conditioned on prompt and student initial answer, with the goal of internalizing critique-generation abilities.",
            "citation_title": "Critique fine-tuning: Learning to critique is more effective than learning to imitate.",
            "mention_or_use": "use",
            "model_name": "Used as baseline on student models such as LLaMA3.1-8B Instruct and S1.1-3B in this paper's experiments",
            "model_description": "CFT trains a student to output teacher-generated critiques (example critiques in paper were generated by LLaMA3.3-70B or GPT-4o in variants); trained on same critique-augmented datasets as CGD for fair comparison.",
            "reflection_method_name": "Critique Fine-Tuning (CFT)",
            "reflection_method_description": "Train students to generate teacher critiques conditioned on (prompt, student initial answer), so the model can produce critique-style tokens and (in some variants) a final answer in a single pass; objective focuses on predicting critique text.",
            "task_name": "Same math and reasoning benchmarks used in this paper (MATH500, AMC23, OlympiadBench, GSM8K, TheoremQA, GPQA, MMLU-PRO, IFEval, etc.)",
            "task_description": "Benchmarks for math reasoning and general instruction-following and QA.",
            "performance_with_reflection": "Mixed: CFT improves some reasoning benchmarks in prior work and sometimes in this paper, but in these experiments it is consistently beaten by CGD. Examples from paper: CFT (LR=1e-6) average ~39.3 (Table 4) versus CGD ~44.8; CFT caused IFEval accuracy to fall from 76.6% (SFT) to 55.6% in one reported result (format-drift issue).",
            "performance_without_reflection": "Standard SFT and Distilled SFT baselines reported; SFT/Distilled SFT sometimes achieve similar or lower performance than CFT on reasoning tasks but preserve general instruction following better than CFT.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Training objective directly targets critique token generation (supervised learning to output teacher critiques), i.e., the model learns to produce reflective text rather than learning to incorporate critiques into final answers.",
            "number_of_iterations": null,
            "evidence_for_improvement": "CFT has been reported in prior work to help on some math benchmarks; in this paper CFT provides some gains over vanilla SFT in certain settings but is outperformed by CGD. Empirical evidence in this paper shows CFT is sensitive to learning rate and prone to format drift which harms downstream tasks.",
            "limitations_or_failure_cases": "Prone to output-format drift (produces critique-like outputs rather than concise answers), brittle to hyperparameters (sharp performance drop at higher LR reported), can degrade general instruction-following (IFEval drop), and gains depend heavily on critique quality.",
            "comparison_to_other_methods": "Compared directly to CGD and Distilled SFT in the paper; CGD outperforms CFT across most benchmarks and is more robust/stable.",
            "ablation_study_results": "Learning-rate sensitivity ablation shows CFT performance degrades substantially at higher LR (e.g., avg drops from 39.3 to 30.1 in Table 4), and qualitative analyses show format drift early in CFT training.",
            "uuid": "e8754.1",
            "source_info": {
                "paper_title": "Critique-Guided Distillation for Efficient and Robust Language Model Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (iterative self-feedback at inference)",
            "brief_description": "An iterative inference-time method in which a model generates an initial answer, critiques it, and then refines the answer repeatedly (multi-pass self-feedback).",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "mention",
            "model_name": "General LLMs (prior work referenced; not directly re-run in this paper's experiments)",
            "model_description": "Prior multi-pass prompting approach (Madaan et al., 2023) applied to large LLMs to perform generate-critique-refine loops at inference time.",
            "reflection_method_name": "Self-Refine / generate-then-reflect iterative prompting",
            "reflection_method_description": "At inference, the model generates an initial answer, then produces a critique of that answer and uses it to produce a refined answer; this may be repeated multiple times until convergence or budget is exhausted.",
            "task_name": "Referenced in context of reasoning benchmarks (math/reasoning tasks) but not re-evaluated with numbers in this paper",
            "task_description": "Iterative refinement applied to reasoning and problem-solving tasks in prior literature.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt-engineered multi-pass self-critique and refinement at inference time (no special training modifications in the typical formulation).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper cites prior work showing iterative critique+refinement improves answers but notes the primary downside is inference-time compute/latency (at least a 3–4x increase in cost/latency).",
            "limitations_or_failure_cases": "High inference-time computational cost and latency (3–4x or more), which motivates bringing critique/refinement into training (as CGD does) to avoid inference overhead.",
            "comparison_to_other_methods": "Described as effective but expensive compared to CGD which transfers the benefit into a single-pass model via training.",
            "uuid": "e8754.2",
            "source_info": {
                "paper_title": "Critique-Guided Distillation for Efficient and Robust Language Model Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A framework where language agents use verbal reflections and reinforcement learning to improve behavior across episodes by generating natural language reflections and revising future behavior.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "mention_or_use": "mention",
            "model_name": "General LLM agents (prior work cited; not used in experiments here)",
            "model_description": "Agent-style LLMs that incorporate verbal reflections as a signal for learning via reinforcement learning; cited as a related form of self-reflection.",
            "reflection_method_name": "Reflexion (verbal RL with reflection)",
            "reflection_method_description": "Agents generate reflective natural-language summaries of failures/successes and use these reflections as a learning signal (often combined with RL updates) to improve future behavior.",
            "task_name": "",
            "task_description": "Cited generically as prior work on self-reflection for language agents; no concrete benchmark numbers given in this paper.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Verbal reflections used as auxiliary learning signals, combined with reinforcement learning updates (external memory or episodic updates may be used in original work).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Referenced as an influential prior demonstrating that verbal reflection can improve agent performance, but no new results provided in this paper.",
            "limitations_or_failure_cases": "Not discussed in detail here; cited for conceptual relation to CGD's goal of leveraging critique-like signals.",
            "uuid": "e8754.3",
            "source_info": {
                "paper_title": "Critique-Guided Distillation for Efficient and Robust Language Model Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RFT",
            "name_full": "Rejection Sampling Fine-Tuning",
            "brief_description": "A fine-tuning approach that trains on the model's own generated outputs which are verified or ranked by a reward model to incorporate value-based feedback.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Referenced as an alternate approach that incorporates a reward model to rank/verify model outputs (Yuan et al., 2023); not used experimentally in this paper.",
            "reflection_method_name": "Rejection Sampling Fine-Tuning (RFT)",
            "reflection_method_description": "Train on a subset of model-generated outputs selected by a reward model (rejection sampling) so the model internalizes higher-quality outputs without explicit critique tokens.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Uses a reward model to select/rank model outputs; supervision comes from selected outputs rather than explicit critiques.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned as an alternative that incorporates value-based feedback; no direct empirical numbers given in this paper.",
            "limitations_or_failure_cases": "Not detailed here; cited as part of prior work landscape motivating critique-based training.",
            "uuid": "e8754.4",
            "source_info": {
                "paper_title": "Critique-Guided Distillation for Efficient and Robust Language Model Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DeCRIM / LLMRefine / DCR",
            "name_full": "DeCRIM, LLMRefine, DCR (fine-grained feedback & modular correction pipelines)",
            "brief_description": "Representative prior approaches that provide structured, often fine-grained feedback (error categories, constraints, decomposition) to guide model corrections and refinements.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "DeCRIM: decomposes instructions into fine-grained constraints; LLMRefine: uses human-defined error categories for targeted feedback; DCR: separates error detection, critique generation, and final refinement into stages. These are cited as related feedback/refinement pipelines.",
            "reflection_method_name": "Decompose-critique-refine style feedback pipelines (DeCRIM / LLMRefine / DCR)",
            "reflection_method_description": "Provide targeted, structured critiques or constraints (often human-defined or tool-assisted) that drive focused corrections of model outputs, sometimes staged into detect-&gt;critique-&gt;refine modules.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Structured prompt/feedback engineering, sometimes combined with external tools or human-defined schemas to pinpoint errors and guide corrections.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Cited literature argues that fine-grained actionable feedback improves correction precision; paper references these as motivating evidence for using informative critiques in CGD.",
            "limitations_or_failure_cases": "Paper notes such methods often still require a separate critique/refinement pass at inference time (incurring latency) or rely on task-specific tools; not exhaustively evaluated within this paper.",
            "uuid": "e8754.5",
            "source_info": {
                "paper_title": "Critique-Guided Distillation for Efficient and Robust Language Model Reasoning",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Critique fine-tuning: Learning to critique is more effective than learning to imitate.",
            "rating": 2,
            "sanitized_title": "critique_finetuning_learning_to_critique_is_more_effective_than_learning_to_imitate"
        },
        {
            "paper_title": "Scaling relationship on learning mathematical reasoning with large language models.",
            "rating": 1,
            "sanitized_title": "scaling_relationship_on_learning_mathematical_reasoning_with_large_language_models"
        },
        {
            "paper_title": "LLM self-correction with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints.",
            "rating": 1,
            "sanitized_title": "llm_selfcorrection_with_decrim_decompose_critique_and_refine_for_enhanced_following_of_instructions_with_multiple_constraints"
        },
        {
            "paper_title": "CRITIC: Large language models can self-correct with tool-interactive critiquing.",
            "rating": 1,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Distilling reasoning capabilities into smaller language models.",
            "rating": 1,
            "sanitized_title": "distilling_reasoning_capabilities_into_smaller_language_models"
        }
    ],
    "cost": 0.01994275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CRITIQUE-GUIDED DISTILLATION FOR EFFICIENT AND ROBUST LANGUAGE MODEL REASONING
26 Sep 2025</p>
<p>Berkcan Kapusuzoglu berkcan.kapusuzoglu@capitalone.com 
Capital One McLean
22102VAUSA</p>
<p>Supriyo Chakraborty 
Capital One McLean
22102VAUSA</p>
<p>Chia-Hsuan Lee 
Capital One McLean
22102VAUSA</p>
<p>Sambit Sahu 
Capital One McLean
22102VAUSA</p>
<p>CRITIQUE-GUIDED DISTILLATION FOR EFFICIENT AND ROBUST LANGUAGE MODEL REASONING
26 Sep 202532AB67A9237B4DC1066610AC3F70D274arXiv:2505.11628v3[cs.CL]
Supervised fine-tuning (SFT) with expert demonstrations often suffers from the imitation problem, where models reproduce correct responses without internalizing the underlying reasoning.We propose CRITIQUE-GUIDED DISTILLATION (CGD), a multi-stage training framework that augments SFT with teacher-generated explanatory critiques and refined responses.Instead of directly imitating teacher outputs, a student learns to map the triplet of prompt, its own initial response, and teacher critique into the refined teacher response, thereby capturing both what to output and why.Our analyses show that CGD consistently reduces refinement uncertainty, improves alignment between critiques and responses, and enhances sample efficiency.On reasoning benchmarks, CGD achieves substantial gains across LLaMA and Qwen families, including +15.0% on AMC23 and +12.2% on MATH-500, while avoiding the format drift issues observed in prior critique-based fine-tuning.Importantly, on LLaMA-3.1-8BCGD approaches or exceeds the performance of SimpleRL-Zero, which is a DeepSeek-R1 replication, while requiring 60x less compute.Beyond reasoning, CGD maintains or improves general instructionfollowing and factual accuracy, matching baseline performance on IFEval, MUSR, TruthfulQA, and BBH.In contrast, prior critique-based methods degrade these capabilities (e.g., -21% on IFEval).Taken together, these results establish CGD as a robust and generalizable alternative to both conventional SFT and RL-based methods, offering a more efficient path toward advancing the reasoning and safety of large language models.</p>
<p>INTRODUCTION</p>
<p>Supervised fine-tuning (SFT) is a foundational technique for teaching large language models (LLMs) to perform diverse downstream tasks by mimicking expert-annotated outputs (Wei et al., 2022;Sanh et al., 2022).Despite its success, vanilla SFT has notable limitations: it increases model's tendency to hallucinate (Gekhman et al., 2024), exhibits limited out-of-distribution generalization (Chu et al., 2025), and struggles to generalize to harder problem instances (Sun et al., 2024;2025).These shortcomings raise fundamental questions about SFT's capacity for robust, and complex reasoning.</p>
<p>An alternative approach to improve reasoning leverages critique and revision at inference time: a model generates an initial answer, critiques it, then refines its output based on that critique (Kim et al., 2023;Madaan et al., 2023;Shinn et al., 2023;Saunders et al., 2022).While effective, these multi-pass prompting methods incur high computational costs and latency during deployment.</p>
<p>To integrate critique signals without extra inference costs, recent works has moved these steps into training.Rejection Sampling Fine-Tuning (RFT) (Yuan et al., 2023) trains the model on its own generated outputs that are verified or ranked by a reward model, thus incorporating value-based feedback.Critique Fine-Tuning (CFT) instead trains a student model to reproduce teacher-generated critiques (Wang et al., 2025).Although CFT outperforms vanilla SFT on several math benchmarks, prolonged CFT can induce output-format drift, overfitting to critique patterns rather than stable answer structures and its gains are sensitive to the quality of the critiques provided.</p>
<p>In this work, we introduce CRITIQUE-GUIDED DISTILLATION (CGD) (Fig. 1), a novel multistep fine-tuning paradigm in which a student model learns to transform its own initial outputs into high-quality refinements, rather than just generating critiques.Concretely, we condition a teacher model to produce both critiques and corresponding corrected answers, and train the student to map its raw response to the teacher's refined version by conditioning it on the critique.By internalizing not only how to identify errors but also what a polished response looks like, CGD closes the loop between diagnosis and correction.Importantly, the use of initial answers and critiques is restricted to the training phase: at inference time CGD requires only the original prompt and produces the refined answer in a single pass, with no need for critiques.This design both avoids format drift compared to CFT (Fig. 1) and eliminates the inference-time overhead of multi-pass critique methods.</p>
<p>Figure 1: Comparing Supervised fine-tuning (SFT), Critique Fine-Tuning (CFT) and CRITIQUE-GUIDED DISTILLATION (CGD).Unlike CFT, which trains the student to generate critiques, CGD conditions training on both the initial answer and critique but at test time generates the final answer directly in a single pass.</p>
<p>By conditioning answer generation on the critique, CGD avoids format drift (the model continues to generate answers, not critiques) and ensures feedback is explicit and grounded.We empirically validate CGD on mathematical reasoning and broad knowledge benchmarks, observing +17.5% and +15.0%absolute accuracy gains over SFT and CFT respectively on the challenging AMC23 dataset for LLaMA3.1-8BInstruct student model.Similarly, on OlympiadBench, CGD achieves a +12.9% gain over SFT and +8.0% over CFT.For S1.1-3B student model, CGD achieves +12.2% gain +7.5% over CFT on MATH-500 and AMC23, respectively, further reinforcing its effectiveness on hard math reasoning.Figure 2 visualizes this performance trend, showing that CGD consistently improves over strong baselines such as Distilled SFT, which only trains the student to imitate the teacher's refined answers given the input prompt, and CFT across all evaluation tasks.In contrast, CGD explicitly conditions the student on the input prompt, its own initial answer, and the teacher's critique, enabling the model to internalize not just what the correct refinement is but why the refinement is needed.This richer supervision leads to more robust and generalizable reasoning behavior.</p>
<p>In summary, our contributions are as follows:</p>
<p>• We introduce CRITIQUE-GUIDED DISTILLATION (CGD), a novel and efficient finetuning framework that trains a student model on the full cycle of self-correction: from a flawed initial answer, to a critique, to a refined output.</p>
<p>• We demonstrate through extensive experiments that CGD achieves state-of-the-art performance, outperforming strong distillation and critique-based baselines.For LLaMA3.1-8B Instruct and S1.1-3B, CGD achieves absolute gains of +5.4% and +7.2% respectively over the next best method, CFT, on five math reasoning benchmarks.</p>
<p>• We conduct a thorough suite of ablation studies, proving CGD's robustness.Our method shows consistent gains across different model families (LLaMA (Grattafiori et al., 2024) et al., 2024).The LLaMA3.1-8BInstruct student model is trained using the input prompt, initial answer and the the critique as input, and the refined answer as the target.Baselines include Distilled SFT, which uses only the input prompt as input to imitate refined answers from the same teacher (LLaMA3.3-70BInstruct) model on the same WebInstruct data, and CFT, which trains on GPT-4o-generated critiques (Wang et al., 2025).</p>
<p>S1.1 (Muennighoff et al., 2025), Mixtral (Jiang et al., 2024), and OLMo (Groeneveld et al., 2024)), training datasets (WebInstruct (Yue et al., 2024), and MetaMathQA (Yu et al., 2024a)), and is significantly more stable to hyperparameter changes than CFT.• We also analyze CGD using a set of diagnostic probes (results in Appendix C).Our findings reveal that CGD training using an informative critique, results in a robust model.</p>
<p>RELATED WORK</p>
<p>We organize prior work under the following broad categories.</p>
<p>Supervised Fine-Tuning Limitations Standard supervised fine-tuning (SFT) trains Large Language Models (LLMs) to mimic expert demonstrations, but it often induces the imitation problem, where models reproduce outputs without internalizing reasoning processes.Previous studies show that fine-tuning language models on new knowledge increases model's tendency to hallucinate (Gekhman et al., 2024).Furthermore, fine-tuned models exhibit poor out-of-distribution performance (Chu et al., 2025), and gains on familiar data often come at the cost of reliability on unseen distributions (Li et al., 2025).</p>
<p>Recent work has also shown that SFT on reasoning trajectories can substantially boost mathematical problem-solving with only a few thousand examples (Muennighoff et al., 2025;Ye et al., 2025).Nonetheless, vanilla SFT still struggles to generalize to harder problem instances, leaving open the question of its limits on complex reasoning (Sun et al., 2024;2025).These limitations motivate integrating critique-and-correction mechanisms beyond naive answer imitation to achieve more robust reasoning and improved downstream performance.</p>
<p>Self-Correction and Critiques</p>
<p>Prior work has shown that LLMs can critique their own outputs and then refine those outputs based on self-generated feedback, yielding stronger reasoning and improved downstream performance (Kim et al., 2023;Madaan et al., 2023;Saunders et al., 2022).However, these methods depend on multi-pass prompting with LLMs and incur substantial inference-time overhead.To address this, several fine-tuning approaches have been proposed to endow smaller-scale models with self-correction capabilities (Shridhar et al., 2023;Yu et al., 2024b).Shridhar et al. (2023) leverage a large LLM to generate initial predictions, feedback, and refined results, while Yu et al. (2024b) further mitigates train-test misalignment by having the student model produce its own initial outputs.Nevertheless, all of these methods still require a separate critique and refinement pass at inference time, adding unwanted latency.</p>
<p>More recently, Critique Fine-Tuning (CFT) (Wang et al., 2025) trains a student model to generate teacher-generated critiques, enabling single-pass answer generation and greatly reducing inference cost.However, because CFT focuses on producing critique tokens, it is prone to output-format drift.In contrast, CGD directly fine-tunes students on the refined answer, preserving output consistency while retaining critique-driven improvements and outperforming CFT on reasoning tasks.</p>
<p>Feedback Mechanisms for Self-Refinement To study the impact of granularity of feedback on self-refinement, DeCRIM breaks high-level instructions into fine-grained constraints to guide targeted corrections (Ferraz et al., 2024).LLMRefine leverages human-defined error categories to produce pinpointed feedback, enabling precise adjustments to model outputs (Xu et al., 2024;Paul et al., 2024).DCR further modularizes this pipeline by separating error detection, critique generation, and final refinement into distinct stages (Wadhwa et al., 2024).</p>
<p>Other approaches enhance correction accuracy by incorporating external tools, such as code executors for programming tasks (Chen et al., 2023a;2024), formal proof assistants for mathematical reasoning (First et al., 2023), and search engines for factual validation (Gao et al., 2023;Gou et al., 2024) to supply auxiliary information.In contrast, our method rely solely on a large teacher LLM to provide general-purpose critiques, avoiding reliance on task-specific external signals.</p>
<p>Pedagogical and Explanation-Guided Feedback Insights from education research stress the importance of actionable, theory-grounded feedback.Importantly, the quality of feedback is critical: if critiques are vague or generic, they may not guide learning.As the FELT framework (Borges et al., 2023) emphasizes, feedback must be actionable and specific rather than generic to be effective.Explanation-Guided Active Distillation (ELAD) (Zhang et al., 2024) actively selects highuncertainty examples via explanation-step uncertainties, reducing annotation cost while preserving student performance.Other work uses LLMs to supply real-time explanatory feedback to human tutors (Lin et al., 2023) and employs sequence labeling to highlight desired versus undesired response components for targeted improvement (Lin et al., 2024).These works collectively underscore the power of explanation-rich feedback, motivating CGD's explicit incorporation of critiques as conditioning signals to enable models to learn both what is correct and why revisions matter.</p>
<p>CRITIQUE-GUIDED DISTILLATION (CGD)</p>
<p>In this section we describe CGD and provide analysis of its training procedure.</p>
<p>Figure 3: Overview of CRITIQUE-GUIDED DISTILLATION (CGD).Overview of Critique-Guided Distillation (CGD).During training, the student produces an initial response, the teacher supplies a critique and refined answer, and the student is fine-tuned to map from (prompt, initial answer, critique) → refined answer.At inference, however, only the prompt is provided, and the student directly outputs the refined answer in one pass.</p>
<p>OVERVIEW</p>
<p>The key intuition behind CGD is to train a student model to perform a complete reasoning loop: from generating an initial answer, to understanding a critique of that answer, to producing a final, refined output.By internalizing not only how to identify its own errors but also how to correct them, the student learns a more robust and generalizable reasoning process.This approach, summarized in Figure 3, proceeds in three main stages:</p>
<ol>
<li>Initial Answer Generation: The student baseline S θinit produces a noisy response y ′ ∼ S θinit (•|x).</li>
</ol>
<p>Critique Generation:</p>
<p>The teacher model T ϕ critiques this response, generating a textual explanation of its flaws or merits, c ∼ T ϕ (•|x, y ′ ).</p>
<p>Refined Answer Generation:</p>
<p>The teacher produces a gold-standard, refined answer ŷ ∼ T ϕ (•|x, y ′ , c), conditioned on all prior context.</p>
<p>This training-time-only intervention ensures that feedback is explicit and grounded.By conditioning the final answer generation on the critique, CGD avoids the format drift seen in methods like CFT, as the model's objective remains to generate answers, not critiques.</p>
<p>TRAINING OBJECTIVE</p>
<p>The student is fine-tuned on the augmented dataset ((x, y ′ , c), ŷ) using a standard language modeling objective.As summarized in Algorithm 1, the goal is to minimize the negative log-likelihood of the teacher's refined answer, conditioned on the full context:
L(θ) = E (x,y ′ ,c,ŷ) [− log S θ (ŷ | x, y ′ , c)].(1)
Crucially, at inference time, CGD requires only a single forward pass, making it identical in computational cost to standard SFT.Generate initial answer:
y ′ i ∼ S θinit (y|x i ) 6:
Generate critique:
c i ∼ T ϕ (c|x i , y ′ i ) 7:
Generate refined answer: ŷi ∼ T ϕ (ŷ|x i , y ′ i , c i ) 8:
D ′ ← D ′ ∪ {(x i , y ′ i , c i , ŷi )} 9: end for 10: Train S θ on D ′ by minimizing L(θ) 11: return S θ
From a probabilistic perspective, the CGD framework can be interpreted as a form of Bayesian inference.The student's initial output distribution, P (y|x), acts as a prior belief.The critique c serves as new evidence.The goal of the student is to learn the posterior distribution P (y|x, c), which is proportional to the likelihood of the critique given a refined answer, P (c|x, y), multiplied by the prior.By training the student to match the teacher's refined answer ŷ (which is drawn from a highquality posterior), CGD systematically reduces the model's uncertainty in line with the critique's guidance, formalizing why it produces more confident and accurate predictions.</p>
<p>EXPERIMENTS</p>
<p>Our experiments are designed to show that CRITIQUE-GUIDED DISTILLATION (CGD) is a highly efficient and effective method for improving the reasoning capabilities of LLMs.We demonstrate that CGD significantly outperforms strong fine-tuning baselines, including standard SFT, Distilled SFT, and CFT, across a diverse suite of challenging math and reasoning benchmarks.Furthermore, we show that CGD is dramatically more compute-efficient than contemporary reinforcement learning methods and exhibits superior robustness to hyperparameter choices compared to other critique-based techniques.This efficient learning of a robust self-correction skill is the mechanism that directly contributes to the superior performance on downstream benchmarks.</p>
<p>EXPERIMENTAL SETUP</p>
<p>DATASETS</p>
<p>We consider two datasets for training: WebInstruct (Yue et al., 2024), and MetaMathQA (Yu et al., 2024a).WebInstruct is a web-crawled instruction dataset that spans a wide range of topics, including Math, Physics, Chemistry, and more.MetaMathQA is a dataset based on GSM8K (Cobbe et al., 2021b) and MATH (Hendrycks et al., 2021) which synthesizes more questions and answers by rephrasing and other augmentation techniques.We randomly sample 100k examples from each dataset as training data.</p>
<p>We evaluate on two sets of benchmarks capturing both mathematical reasoning and broader STEM-oriented problem solving.Group 1: Math Reasoning comprises MATH500 (Hendrycks et al., 2021), Minerva-Math (Lewkowycz et al., 2022), GSM8K (Cobbe et al., 2021a), Olympiad-Bench (He et al., 2024), and AMC23.Group 2: General Reasoning includes TheoremQA (Chen et al., 2023b), GPQA (Rein et al., 2023), and MMLU-Pro (Wang et al., 2024).</p>
<p>To evaluate model capabilities beyond math and science reasoning, we also report results on the following datasets to evaluate general instruction-following and question answering abilities: IFEval (Zhou et al., 2023), MUSR (Sprague et al., 2024), TruthfulQA (Lin et al., 2022), and BIG-Bench Hard (BBH) (Suzgun et al., 2022).</p>
<p>BASELINE AND TRAINING SETTINGS</p>
<p>We evaluate CGD across two student-teacher pairs to test both within-family and cross-family robustness:</p>
<p>• LLaMA family:</p>
<p>LLaMA3.1-8B Instruct as the student model, and LLaMA3.3-70BInstruct as the teacher model.</p>
<p>• Qwen family: S1.1-3B1 as the student model, and S1.1-32B2 as the teacher model.</p>
<p>We compare CGD to three supervised fine-tuning baselines: (i) Standard SFT: fine-tunes the student model to generate gold answers conditioned only on the input prompt.(ii) Distilled SFT: finetunes the student to reproduce the teacher's refined answers, where each refinement is obtained by prompting the teacher with the input prompt, the student's initial answer, and the teacher-generated critique.(iii) Critique Fine-Tuning (CFT): fine-tunes the student to generate the teacher-provided critiques conditioned on the input prompt and the student's initial answer. 3ll experiments are trained on 16 Nvidia A100 GPUs for 30 minutes, amounting to a total of 8 A100 GPU-hours per experiment, using identical data splits and hyperparameters across methods (see Appendix A for more details).</p>
<p>MAIN RESULTS</p>
<p>We report the evaluation results of training on WebInstruct in Table 1.We evaluate two student-teacher pairs: LLaMA3.1-8BInstruct with LLaMA3.3-70BInstruct as the teacher, and S1.1-3B with S1.1-32B as the teacher.In addition to our CFT experiments using LLaMA3.3-70BInstruct, we also include a variant of CFT that uses 50K examples distilled with GPT-4o, sourced from Wang et al. (2025) (denoted as CFT * with GPT-4o).A full breakdown of additional results, including ablation studies on different model architectures, teacher models, on math-specific training data (MetaMathQA), hyperparameter sensitivity, and critique composition, is provided in Appendix B Table 1: Evaluation of fine-tuning methods on two student models: LLaMA3.1-8BInstruct and S1.1-3B.Results are reported across math-focused (Group 1) and general reasoning (Group 2) benchmarks, using WebInstruct as training set.All methods are fine-tuned on 100K WebInstruct samples, except CFT * with GPT-4o, which uses the 50K examples from We-bInstructCFT (Wang et al., 2025).CGD consistently achieves the best average performance across both families.Bold numbers indicate best performance, underlines mark second-best.The ∆ rows report CGD's improvement over the CFT baseline for each student.</p>
<p>Math Reasoning Tasks (Group 1)</p>
<p>General Reasoning Tasks (Group 2) Method MATH500 Minerva-Math GSM8K OlympiadBench AMC23 Avg.TheoremQA GPQA MMLU-PRO Avg.</p>
<p>LLaMA3 Across both families, CGD consistently improves over CFT across both math and general reasoning benchmarks.On LLaMA3.1-8BInstruct, CGD improves over CFT by +5.4 points on math reasoning (Group 1) and +4.3 points on general reasoning (Group 2), with particularly strong gains on OlympiadBench (+8.0) and AMC23 (+15.0).On S1.1-3B, CGD achieves even larger average gains of +7.2 points on math reasoning and +3.5 points on general reasoning, including notable improvements on MATH500 (+12.2),Minerva-Math (+6.9),AMC23 (+7.5), and OlympiadBench (+3.8).Together, these results show that critique-guided training enhances reasoning ability more broadly than CFT and distilled SFT across diverse student-teacher settings.</p>
<p>These results demonstrate that CGD generalizes effectively across different model scales and architectures, systematically strengthening reasoning accuracy on diverse math benchmarks while improving general-purpose performance.For additional context, we note that CFT * with GPT-4o (trained on 50K samples) achieves competitive results, in some cases surpassing LLaMA3.3-70BbasedCFT.This aligns with prior findings that the quality of the teacher model plays a central role in the effectiveness of critique-based training (Wang et al., 2025).</p>
<p>We additionally evaluate on IFEval, MUSR, TruthfulQA, and BIG-Bench Hard (BBH) (Table 2).CGD matches or surpasses all baselines, confirming that critique-conditioned training preserves general instruction-following and question-answering abilities.By contrast, CFT's IFEval accuracy falls from 76.6% to 55.6%, likely because CFT is optimized to predict critiques rather than final answers, which is an objective that can disrupt format-sensitive instruction following.Our results clarify the role of critique-based supervision.While CFT provides mixed benefits, sometimes improving reasoning on certain models (Wang et al., 2025;Gandhi et al., 2025), but often degrading general instruction-following (Table 2), it lacks robustness across families and scales.In contrast, CGD consistently improves both math reasoning and general capabilities across model families and sizes, highlighting its reliability as a critique-aware training paradigm (Wei et al., 2023;Ouyang et al., 2022).</p>
<p>COMPARISON WITH RL-BASED METHODS</p>
<p>Reinforcement learning (RL) has recently been shown to significantly enhance the reasoning capabilities of LLMs (Shao et al., 2024;DeepSeek-AI et al., 2025;Zeng et al., 2025).To situate CGD within this line of work, we compare against SimpleRL-Zero (Zeng et al., 2025), an open replication of DeepSeek-R1 that applies reinforcement learning directly to LLaMA3.1-8B.We report the official numbers released by the SimpleRL authors4 and compare them with our results on the same base model (LLaMA3.1-8B).</p>
<p>Table 3 shows that SimpleRL-Zero achieves modest gains on OlympiadBench and AMC23, but struggles on broader math benchmarks such as MATH500 and Minerva-Math.By contrast, CGD provides a more balanced improvement profile: it surpasses SimpleRL-Zero on MATH500 (+6.4), Minerva-Math (+3.3), and OlympiadBench (+1.7).Performance on AIME24 remains low for both approaches due to the difficulty of this benchmark.Overall, CGD achieves a higher average score (11.9 vs. 10.6)despite using far less compute.Beyond accuracy, efficiency is a central consideration.Training SimpleRL-Zero requires 32×H100 GPUs for 1.5 days, along with long-horizon sampling during training.In contrast, CGD achieves its gains with only 8×A100 GPUs (40GB each, substantially weaker than H100s) for ∼5 hour of fine-tuning, and inference requires no additional decoding overhead.This represents well over an order of magnitude reduction in compute cost.Importantly, our aim is not to outperform RL-based methods, but to show that CGD can approach or exceed them on several tasks while being more resource-efficient and stable across benchmarks.Similarly, multi-pass inference-time methods like Self-Refine (Madaan et al., 2023) would incur at least a 3-4x increase in latency and cost per query, whereas CGD adds no overhead compared to a standard SFT model.</p>
<p>THE ROLE OF THE CRITIQUE AS A LEARNING SIGNAL</p>
<p>To isolate the impact of the critique as a learning signal during fine-tuning, we compare our full CGD method against a key ablation variant, "CGD without Critique."In this ablation, the model is trained on the exact same data and targets, but with the critique removed from the input prompt.This forces the model to learn the transformation from a flawed student answer to the refined answer without explicit guidance.</p>
<p>As shown in Figure 4, the inclusion of the critique during training consistently and significantly improves performance on the challenging reasoning benchmarks.The gains are particularly large on complex reasoning tasks such as Minerva-Math and AMC23.This result demonstrates that the critique is not merely redundant context but is a crucial component of the training signal.It provides an explicit reasoning path that enables the model to learn the difficult self-correction skill more effectively, leading to better generalization on downstream tasks.To understand the mechanisms behind this improvement, we conducted a series of diagnostic probes (see Appendix C for full details and figures).These analyses reveal that the CGD training process forges a more robust and efficient reasoning agent.We find that the critique provides a more efficient optimization path during training, reducing the required gradient norm by 27%.This efficient learning translates into a model that is statistically significantly more confident (lower entropy, p &lt; 10 −4 ) than strong distillation baselines when performing the complex self-correction task.This suggests that CGD's performance gains are driven by its unique ability to instill a robust, decisive, and efficient self-correction capability.</p>
<p>TRAINING STABILITY AND HYPERPARAMETER ROBUSTNESS</p>
<p>To ensure a rigorous and fair comparison, we evaluate the learning rate sensitivity of CGD against the CFT baseline using the same teacher model (LLaMA3.3-70BInstruct) and identical prompts from the WebInstruct subset (Table 4).Both methods are trained on 100K critiqueaugmented examples under identical training schedules, varying only the learning rate between 1 × 10 −6 and 5 × 10 −6 .While CFT's performance significantly degrades at the higher learning rate, dropping by over 9 points on average, CGD remains robust and outperforms CFT across all metrics regardless of learning rate.These results suggest that CGD's structured self-correction task with the use of both critiques and refined answers enables more stable optimization and better generalization, even under suboptimal hyperparameter choices, whereas CFT remains brittle to training dynamics despite access to the same supervision signals.</p>
<p>LIMITATIONS AND FUTURE WORK</p>
<p>Our work demonstrates that CGD is an effective and efficient method for teaching self-correction.</p>
<p>Our limitations point to several opportunities for future work: first, the performance gains are modulated by the student model's receptivity to the critique-and-refine format, suggesting that a model's architectural priors and alignment tuning are key factors.Second, the multi-stage data generation process, while more efficient than RL, still presents a practical computational cost.</p>
<p>These considerations motivate two exciting research directions.To address the data generation cost, future work could explore single-stage distillation methods that teach self-correction without requiring explicit, pre-generated critiques.We believe the most impactful direction, however, lies in using the explanatory nature of critiques to improve model safety and alignment.Systematically engineering critiques to penalize factual inaccuracies, logical fallacies, or harmful content could provide a scalable and interpretable framework for building more trustworthy AI.</p>
<p>CONCLUSION</p>
<p>We introduced CRITIQUE-GUIDED DISTILLATION (CGD), a simple yet powerful fine-tuning framework that teaches models not only what the correct answer is but also why it is correct.By conditioning a student model on its own mistake and an explanatory teacher critique, our method learns a robust self-correction skill and preserves the answer-generation format without any inference-time overhead.Our experiments show that CGD significantly outperforms strong baselines, including standard and distilled SFT, across a diverse suite of challenging mathematics and general reasoning benchmarks.On LLaMA3.1-8B and S1.1-3B, this results in an average performance gain of 5.4% and 7.2% over the strongest critique-based baseline (CFT), respectively.To assess the generality of our approach beyond the LLaMA model family, we replicate our main fine-tuning comparisons using Mixtral-8x7B Instruct v0.1 and OLMo-2-1124-7B-Instruct as the student models.Table 7 summarizes the results for both Mixtral-8x7B Instruct v0.1 and OLMo-2-1124-7B-Instruct student models across both math-focused and general reasoning benchmarks, with all models trained on the same 100K WebInstruct prompts.</p>
<p>Notably, we find that our method, CGD, consistently outperforms the baselines in both task groups for a different student model Mixtral-8x7B Instruct as shown in Table 7.On math reasoning tasks (Group 1), CGD achieves a +3.2% improvement over CFT.This includes substantial gains on GSM8K (+11.2%),Minerva-Math (+3.3%), and MATH500 (+3.4%), confirming transferability to a different architecture.In general reasoning tasks (Group 2), CGD shows a +1.3% average improvement over CFT, with notable gains on MMLU-PRO (+7.3%) and TheoremQA (+2.8%).While performance slightly declines on AMC23 (-2.5%) and GPQA (-6.0%) relative to CFT, these drops are not large enough to offset the overall performance improvements.</p>
<p>In contrast, CGD yields smaller gains on OLMo, i.e., 0.4 points less gain on Group 1 Avg.compared to Mixtral.While OLMo and Mixtral are similar in scale and baseline strength, they may differ in their ability to absorb critique-structured inputs.One possible explanation is differences in alignment data quality and fine-tuning objectives: prior work ( (Bai et al., 2022;Liang et al., 2025; Importantly, CGD achieves consistently higher scores than SFT and CFT across most benchmarks, suggesting that distillation from critiques offers a more stable supervision signal than critique generation alone.These results generalize our main findings and further support the modularity and versatility of our proposed training framework, highlighting that critique-based supervision is effective even for non-LLaMA models.</p>
<p>B.1.3 ABLATION: RESULTS USING DIFFERENT TEACHER MODELS</p>
<p>We find that CGD provides consistent improvements over the base LLaMA3.1-8BInstruct model across both math and general reasoning benchmarks, regardless of the choice of teacher model as shown in Table 8.Using LLaMA3.3-70BInstruct as the teacher yields strong gains, particularly in general reasoning tasks, while adopting the open-weight S1.1-32B teacher leads to even stronger performance on several challenging math benchmarks.For example, CGD with S1.1 improves AMC23 accuracy by +20.0 absolute points (22.5 ⇒ 42.5).These results suggest that the benefits of CGD are not limited to teacher scale or architecture family; even when transferring critiques from a non-LLaMA teacher, the student acquires improved reasoning ability.</p>
<p>We emphasize that the teacher ablation in Table 8 holds the student fixed (LLaMA3.1-8B) while varying the teacher model.In contrast, the S1.1-3B results presented in Table 1 focus on the student-side generalization, where the model itself is smaller and trained with critiques and responses from S1.1-32B.</p>
<p>Importantly, these findings support the claim that CGD's effectiveness is not solely determined by the raw strength of the teacher, but also by the structured way in which critiques are generated and incorporated during training.While stronger teachers such as GPT-4o or future generations of S1.1 may offer further improvements, our preliminary experiments already demonstrate that critique quality and integration play a critical role in driving gains.In other words, CGD does more than transfer answers, i.e., it teaches the student how to reason through structured critique, enabling performance improvements that extend beyond what is achievable with standard distillation.</p>
<p>B.1.4 ABLATION: IMPACT OF CRITIQUE CORRECTNESS MIXTURE</p>
<p>To investigate the impact of the training data composition, we conducted an ablation study using the WebInstruct dataset, training five models on data with varying ratios of correct and incorrect student answers (as indicated by the critique's conclusion).We kept the total sample size (25k) and all other hyperparameters identical across runs.The results, averaged over our math reasoning benchmarks (MATH500, Minerva-Math, etc.), are shown in Figure 5.We observe a non-linear relationship: models trained on a balanced mixture of both correct and incorrect examples (specifically the 50/50 split) achieve the highest performance.This suggests that for a model to learn a truly generalizable self-correction skill, it must be exposed to a diverse range of both positive and negative feedback, preventing it from learning a simple heuristic like "always agree with the critique." 1 0 0 % R i g h t 7 5 % R i g h t / 2 5 % W r o n g 5 0 % R i g h t / 5 0 % W r o n g 2 5 % R i g h t / 7 5 % W r o n g
1 0 0 % W</p>
<p>B.2 EPOCH-ACCURACY CURVES</p>
<p>Figure 6 shows the progression of final accuracy across training epochs for CRITIQUE-GUIDED DISTILLATION (CGD) on six math-focused benchmarks.We observe that performance is generally stable throughout training, with no substantial drops in accuracy for any dataset.While the upward trends are not particularly pronounced, the lack of degradation suggests that our method is robust to overfitting and avoids catastrophic forgetting.In particular, benchmarks such as MATH (increases from 55.8 to 56.7) and OlympiadBench (increases from 22 to 23.3) show modest improvements, indicating some continued learning over time.These curves offer cautious empirical support for the consistency and stability of our fine-tuning process.</p>
<p>B.3 LEARNING-RATE SENSITIVITY</p>
<p>Figure 7 depicts how both methods respond to changes in learning rate.Figures (a) and (b) show the accuracy vs. learning-rate curves for our approach and CFT, respectively.Our method exhibits a smooth decline as the learning rate increases (Fig. 7a), whereas CFT's performance degrades more sharply (Fig. 7b).From the plot, the CFT curve exhibits a noticeable initial spike in loss, which can be attributed to a format drift during early training.This is due to the model being trained on a critique-style instruction following dataset immediately after pretraining or SFT tuned on QA-style instructions.</p>
<p>The shift from generating answers to critiquing Q&amp;A pairs likely introduces a mismatch in expected input-output format, temporarily destabilizing the loss.As training progresses, however, the model adapts, and the loss curve stabilizes and declines.</p>
<p>In contrast, the CGD method shows a more stable and smooth decrease in loss throughout training, suggesting a more consistent and format-aligned supervision signal.This supports the hypothesis that CGD, by leveraging structured critiques without drastic task shifts, offers a gentler optimization trajectory and better alignment with initial model capabilities.</p>
<p>C DETAILED DIAGNOSTIC ANALYSES</p>
<p>This section provides the detailed methodology, full quantitative results, and visual analyses for the diagnostic experiments summarized in the main paper.All diagnostic experiments were conducted on a set of 500 samples randomly drawn from the OpenMathInstruct 2 dataset using LLaMA3.1-8BInstruct student model with the same hyperparameters.</p>
<p>C.1 DIAGNOSTIC EXPERIMENTS</p>
<p>Entropy Calculation.To measure predictive confidence, we performed a forward pass for each model on the diagnostic dataset, using the full '(Prompt + Student Answer + Critique)' context.This context is formatted using the model's specific chat template.We then isolated the model's logits for the single, next token that would begin the 'Refined Answer'.These logits were converted to a probability distribution via the softmax function, and the Shannon entropy (H(X) = − p(x) log p(x)) was calculated.A lower entropy value indicates higher confidence in the prediction.</p>
<p>Gradient Norm Calculation.To measure learning signal efficiency, we took each final trained model and performed a single forward and backward pass on a diagnostic sample to compute the cross-entropy loss against the target answer.We then calculated the total L2 norm of the full parameter gradient vector.This was done for two input conditions: one 'With Critique' and one 'Without Critique', allowing for a controlled analysis of the critique's impact on the update signal.Analysis of Table 10 reinforces the fact that conditioned on an informative critique the model is able to better predict the final response.This reduces the loss and in turn the magnitude of the gradient norm.We observed this trend during the entire period as well.</p>
<p>Attention Analysis.To analyze the model's internal reasoning, we generated answers with maximum 8192 tokens for each sample and collected the attention matrices from all 32 layers.These scores were then aggregated by averaging across all attention heads and normalized to represent the percentage of attention paid by each generated token to three distinct sections of the prompt: the 'Problem', the 'Student Answer', and the 'Critique'.</p>
<p>C.2 QUANTITATIVE ANALYSIS OF MODEL CONFIDENCE</p>
<p>The behavioral differences observed in our case study are supported by our quantitative diagnostics.</p>
<p>As shown in Table 9, the key finding is that the CGD model is statistically significantly more confident (lower entropy) than all other generative baselines on the self-correction task.The statistical significance of this result (p &lt; 10 −4 vs. Distilled SFT) confirms that the CGD training process forges a uniquely robust and decisive reasoning agent.</p>
<p>C.3 ATTENTION MECHANISM ANALYSIS</p>
<p>To provide a deeper mechanistic view of the CGD model's reasoning process, we analyzed its internal attention patterns, averaged over 50 samples from the OpenMathInstruct 2 dataset.For each sample, we generated up to 8192 new tokens, allowing the model to complete its reasoning naturally.</p>
<p>Our key finding is that the model employs a sophisticated, multi-phase reasoning strategy, using the critique as a foundational signal that is internalized early and acted upon during generation.This is illustrated across three complementary visualizations.</p>
<p>Figure 9 presents the model's attention flow across different layers during the generation of an answer.Starting at the first layer and all the way to the middle layers, there is significant attention on both the critique and the student response.This shows that the model has learned to exploit the signals in an informative critique and the noisy student response (e.g., with attention to the Critique at 48.1% and the Student Answer at 36.0% at the very first generation step).In the later layers of the model, the primary focus is on getting the correct response and hence most of the attention is on the problem.</p>
<p>Figure 10 confirms how different pieces of information are processed at different levels of abstraction.The plot shows the average attention paid to each prompt section across all 32 layers.The results show that direct attention to the Critique's raw tokens peaks at the very input (31.9% at Layer 0), suggesting a strong initial intake of the signal.The model's focus then shifts to the Student Answer, with attention peaking in the semantic middle layers (22.1% at Layer 13), precisely where attention to the critique also sees a secondary rise.This could be attributed to the fact that the model's most abstract reasoning, understanding the flaw and synthesizing the correction, happens in Figure 10: Average attention paid to each prompt section across all 32 transformer layers.The patterns suggest an early intake of the Critique (peak at Layer 0), followed by a deep processing of the Student Answer in conjunction with the critique in the semantic middle layers (peak at Layer 13).Attention to the Problem dominates in the final layers.</p>
<p>Finally, Figure 11 provides a high-level summary of attention from different layers, broken down by generation phase, which reinforces these findings.The heatmaps for later, more semantic layers (16 and 31) visualize the "plan-then-execute" pattern, showing that the initial generation phases are dominated by attention to the critique (48.1% for Layer 31 at token 1).This is consistent with a model that has learned to use the critique as a foundational guide to initiate and structure its reasoning process.These observed attention patterns suggest that the CGD has acquired a sophisticated  ...A more accurate approach would be to understand that the smallest positive integer... is actually their least common multiple (LCM)...</p>
<p>→ →</p>
<p>To find the LCM of 2, 3, 5, 7, and 11, we can list the multiples of each number and find the smallest number that appears in all the lists: 4, 6, 10, 14, 22, 30, 42, 70, 110, 154, 330,... Multiples of 3: 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51,..., 15424, 6, 10, 14, 22, 30, 42, 70, 110, 154, 330,... Multiples of 3: 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51,..., , 15454, 6, 10, 14, 22, 30, 42, 70, 110, 154, 330,... Multiples of 3: 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51,..., , 15484, 6, 10, 14, 22, 30, 42, 70, 110, 154, 330,... Multiples of 3: 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51,..., , 15514, 6, 10, 14, 22, 30, 42, 70, 110, 154, 330,... Multiples of 3: 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51,..., , 15544, 6, 10, 14, 22, 30, 42, 70, 110, 154, 330,... Multiples of 3: 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51,..., , 15574, 6, 10, 14, 22, 30, 42, 70, 110, 154, 330,... Multiples of 3: 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51,..., , 15604, 6, 10, 14, 22, 30, 42, 70, 110, 154, 330,... Multiples of 3: 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51,..., , 15634, 6, 10, 14, 22, 30, 42, 70, 110, 154, 330,... Multiples of 3: 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51,..., , 1566, ,  In conclusion, the approach to finding the smallest positive integer divisible by five distinct prime numbers involves identifying the prime numbers, understanding their properties, and then calculating their product.The result, 2310, is the smallest number that meets the criteria of being divisible by five distinct prime numbers.The CGD model exhibits superior robustness and instruction-following.It correctly solves the problem under both conditions while adhering to the specified output format.The baseline model, while capable of reasoning correctly, is brittle to both complex instructions and irrelevant, noisy feedback.
→ → Multiples of 2: 2,</p>
<p>C.5 BAYESIAN INTERPRETATION</p>
<p>Finally, we interpret critique conditioning as a Bayesian posterior update.Let the student's initial output y ′ define a prior distribution p(y|x), and let the critique c provide new evidence about correctness.The teacher's refinement can be viewed as a posterior distribution:
S θ (ŷ | x, y ′ , c) Student posterior ∝ T ϕ c | x, y ′ , ŷ Teacher likelihood × S init (ŷ | x, y ′ ) Student prior .(2)
Here, S init (ŷ | x, y ′ ) is the student's original (prior) distribution over responses, while T ϕ (c | x, y ′ , ŷ) acts as a scoring function that up-weights those ŷ values better aligned with the critique.Note that the teacher "likelihood" need not be normalized; the proportionality sign indicates that normalization is implicit when forming the posterior.</p>
<p>In practice, CGD minimizes the KL divergence between the student's posterior and the teacherdefined target distribution, which directly implements Equation 2 in training via Algorithm 1.This interpretation highlights how critique guidance sharpens the student's prior into a more informative posterior, explaining the observed empirical gains.</p>
<p>D EXAMPLE CGD TRAINING DATA SAMPLE</p>
<p>To better understand how CRITIQUE-GUIDED DISTILLATION (CGD) enables improved response quality without format drift, we present a representative CGD training data sample.The CGD training pipeline involves three components: a student's initial answer, a teacher-generated critique that identifies specific flaws and explains the reasoning behind them, and a refined answer that incorporates the critique to correct the initial output.The full input tuple (prompt, student, critique, refined answer) is only used during training, and inference remains unchanged from standard decoding.</p>
<p>In the example, the student incorrectly assumes the number of electrons per nitrogen atom when calculating the mass of a nitride anion.The critique explicitly identifies this conceptual error, and the refined answer provides the correct computation with an explanation of the correct electronic configuration of the ion.This structured format teaches the model both the nature of the mistake and the reasoning needed to arrive at a correct solution, demonstrating how CGD effectively enhances model performance while preserving answer format consistency.</p>
<p>CGD WebInstruct Training Data Sample</p>
<p>Question: Calculate the mass of a nitride anion that contains 2.408 × 10 24 nitrogen-based electrons.</p>
<p>Student's Initial Answer: To calculate the mass of the nitride anion, we first need to determine the number of nitrogen atoms in the anion.Since each nitrogen atom has 7 electrons, we can divide the total number of electrons by 7 to find the number of nitrogen atoms:</p>
<p>Number of nitrogen atoms = Total number of electrons 7 = 2.408 × 10 24 7 = 3.443 × 10 23</p>
<p>The mass of a single nitrogen atom is approximately 14.007 u (unified atomic mass units).We can multiply the number of nitrogen atoms by the mass of a single nitrogen atom to find the total mass of the nitride anion: The original solution incorrectly calculates the number of nitrogen atoms based on an incorrect assumption about the number of electrons per nitrogen atom in a nitride anion.This leads to an incorrect mass calculation.Conclusion: Wrong.Refined Answer:</p>
<p>Step 1: Determine the correct number of electrons per nitrogen atom in a nitride anion A nitride anion (N 3− ) has 10 electrons, which includes the 7 original electrons from the neutral nitrogen atom plus 3 additional electrons gained to form the anion.</p>
<p>Step 2: Calculate the number of nitrogen atoms in the nitride anion To find the number of nitrogen atoms, divide the total number of electrons by the number of electrons per nitrogen atom in the anion, which is 10.</p>
<p>Number of nitrogen atoms =</p>
<p>Total number of electrons 10 = 2.408 × 10 24 10 = 2.408 × 10 23</p>
<p>Step 3: Calculate the mass of the nitride anion in unified atomic mass units (u)</p>
<p>The mass of a single nitrogen atom is approximately 14.007 u.Multiply the number of nitrogen atoms by the mass of a single nitrogen atom: Mass = 2.408 × 10 23 × 14.007 = 3.373 × 10 24 u</p>
<p>Step 4: Convert the mass of the nitride anion from unified atomic mass units (u) to kilograms Use the conversion factor 1 u = 1.66053904 × 10 −27 kg:</p>
<p>Mass in kg = 3.373 × 10 24 × 1.66053904 × 10 −27 = 5.605 × 10 −3 kg</p>
<p>The final answer is: 5.605 × 10 −3 kg D.1 QUALITATIVE COMPARISON OF GENERATED RESPONSES</p>
<p>In the qualitative comparison of generated responses, the Critique Fine-Tuning (CFT) method exhibits a notable format drift, deviating from the expected answer format by producing Conclusion section and token at the end of each response.This behavior arises because CFT is trained to generate critiques, inadvertently leading the model to adopt a more elaborate response style.In contrast, the Critique-Guided Distillation (CGD) approach maintains the original response format while enhancing answer quality.</p>
<p>Generated Answer after CFT</p>
<p>Question: True or false: there exists a graph with score (1, 1, 1, 2, 2, 3, 4, 5, 5).Answer with T rue or F alse .(True or False)</p>
<p>Answer: Let's analyze the problem step by step.</p>
<p>Step 1: Understanding the problem The problem asks whether there exists a graph whose score matches the given sequence (1, 1, 1, 2, 2, 3, 4, 5, 5).The score of a graph is determined by the number of vertices and edges in the graph.Specifically, the score is calculated as score = number of vertices − number of edges.</p>
<p>Step 2: Analyzing the score sequence The given score sequence is (1, 1, 1, 2, 2, 3, 4, 5, 5).This sequence represents the difference between the number of vertices and edges for a series of graphs.For example:</p>
<p>• A graph with score 1 could have 2 vertices and 1 edge.</p>
<p>• A graph with score 2 could have 3 vertices and 1 edge.</p>
<p>Step 3: Checking the feasibility of the sequence To determine whether the sequence is valid, we need to verify if it is possible to construct a sequence of graphs where the score increases monotonically and matches the given sequence.This involves checking whether the number of vertices and edges can be adjusted to satisfy the score for each step.Observations:</p>
<p>1.The sequence starts with 1, which is plausible for a graph with 2 vertices and 1 edge.</p>
<ol>
<li>
<p>The sequence increases by 1 for the next three steps (1, 2, 3), which is consistent with adding edges to a graph while keeping the number of vertices constant.</p>
</li>
<li>
<p>The sequence then increases by 1 again (3, 4, 5), which is also consistent with adding edges to a graph while keeping the number of vertices constant.</p>
</li>
<li>
<p>The sequence ends with 5, which is plausible for a graph with 6 vertices and 1 edge.</p>
</li>
</ol>
<p>Key Check: The sequence does not require any negative scores (i.e., more edges than vertices), which is a valid mathematical possibility.Therefore, the sequence is feasible.</p>
<p>Step 4: Conclusion The given score sequence (1, 1, 1, 2, 2, 3, 4, 5, 5) is valid and corresponds to a sequence of graphs with increasing scores.Step 2: Linearity The transformation T (M ) = 1 2 3 4 M is clearly linear because matrix multiplication is distributive and associative.Thus, the linearity condition is satisfied.</p>
<p>Step 3: Injectivity To check injectivity, we need to verify whether T (M1) = T (M2) ⇒ M1 = M2.Let M1, M2 ∈ R 2×2 .If T (M1) = T (M2), then:
1 2 3 4 M1 = 1 2 3 4 M2
This implies:
M1 = M2,
since the matrix 1 2 3 4 is invertible (its determinant is nonzero).Therefore, T is injective.</p>
<p>Step 4: Surjectivity To check surjectivity, we need to verify whether for every M ∈ R 2×2 , there exists M ′ ∈ R 2×2 such that T (M ′ ) = M .Let M ∈ R 2×2 .We solve for M ′ such that:
1 2 3 4 M ′ = M
This is equivalent to solving the system of linear equations:
1 2 3 4 x1 x2 = a b c d ,
where M = a b c d .The matrix 1 2 3 4 is invertible (its determinant is nonzero), so the system has a unique solution for any M .Therefore, T is surjective.</p>
<p>Step 5: Conclusion Since T is both injective and surjective, and it is linear, it is an isomorphism.A score of a graph is the sequence of the degrees of its vertices, listed in non-increasing order.The given score is (1, 1, 1, 2, 2, 3, 4, 5, 5).</p>
<p>For a graph to exist with this score, it must be possible to construct a graph where the degrees of its vertices match this sequence.The sum of the degrees of all vertices in a graph is twice the number of edges (since each edge contributes to the degree count of two vertices).This sum is calculated as follows:</p>
<p>1 + 1 + 1 + 2 + 2 + 3 + 4 + 5 + 5 = 24</p>
<p>Since the sum of the degrees is 24, the number of edges in the graph must be 12 (because 24/2 = 12).Now, let's consider the degrees and how they can be distributed among vertices.The degrees are 1, 1, 1, 2, 2, 3, 4, 5, 5. To form a graph, we need to ensure that the degrees can be assigned to vertices in such a way that the graph remains connected and that the degree of each vertex matches the given sequence.</p>
<p>Given the degrees, we can start by assigning the lowest degrees first.The three vertices with degree 1 must be connected to each other, as they cannot be isolated.This forms a triangle, which is a connected subgraph.</p>
<p>Next, we assign the degrees 2. These vertices can be connected to any of the existing vertices, including each other, as long as they are not isolated.Since we have two vertices with degree 2, they can be connected to the vertices with degree 1, or to each other, or to any of the higher-degree vertices.</p>
<p>The vertex with degree 3 can be connected to any of the existing vertices, including the ones with degree 2, or to the higher-degree vertices.</p>
<p>The vertices with degrees 4 and 5 can be connected to any of the existing vertices, including the ones with lower degrees, as long as they are not isolated.</p>
<p>Given these constraints, it is possible to construct a graph that matches the given score.For example, consider a graph with 9 vertices where:</p>
<ol>
<li>
<p>Three vertices are connected in a triangle (degrees 1).</p>
</li>
<li>
<p>Two vertices are connected to each other and to one of the vertices in the triangle (degrees 2).</p>
</li>
<li>
<p>One vertex is connected to one of the vertices in the triangle and to one of the vertices with degree 2 (degree 3).</p>
</li>
<li>
<p>One vertex is connected to the vertices with degrees 1, 2, and 3 (degree 4).</p>
</li>
<li>
<p>Two vertices are connected to all other vertices, including the ones with degrees 1, 2, 3, and 4 (degrees 5).</p>
</li>
</ol>
<p>This construction ensures that the degrees of the vertices match the given score, and the graph remains connected.</p>
<p>Therefore, the statement that there exists a graph with the score (1, 1, 1, 2, 2, 3, 4, 5, 5) is true.We have:
T (M ) = 1a + 2c 1b + 2d 3a + 4c 3b + 4d = e f g h
This gives us the following system of equations:</p>
<p>1a + 2c = e 1b + 2d = f 3a + 4c = g 3b + 4d = h</p>
<p>Solving this system, we can find values for a, b, c, d that satisfy the equations.For example, one possible solution is:
a = e 1 = e, b = f 1 = f, c = g − 3e 4
, d = h − 3f 4 Therefore, we can find a matrix M that maps to any given matrix N , and the transformation is surjective.</p>
<p>Since the transformation T (M ) = 1 2 3 4 M is both injective and surjective, it is an isomorphism.</p>
<p>The final answer is: True</p>
<p>Algorithm 1
1
CRITIQUE-GUIDED DISTILLATION (CGD) 1: Input: Dataset D = {x i , y i } N i=1 , Student S θinit , Teacher T ϕ 2: Output: Fine-tuned student S θ 3: Initialize augmented dataset D ′ ← ∅ 4: for each x i ∈ D do 5:</p>
<p>Figure 4 :
4
Figure 4: Performance comparison of CGD with and without the critique as input during training, evaluated on eight benchmarks.The critique provides a crucial learning signal, leading to consistent accuracy improvements across both the LLaMA3.1-8BInstruct (a) and S1.1-3B(b) student models.</p>
<p>Figure 5 :
5
Figure 5: Performance on an average of math benchmarks for models trained on different mixtures of correct/incorrect student answers.A balanced 50/50 mixture yields the most robust model.</p>
<p>Figure 6 :Figure 7 :Figure 8 :
678
Figure 6: Accuracy over training epochs for CGD on six math-focused benchmarks.While trends are modest, performance remains stable throughout, indicating resistance to overfitting and catastrophic forgetting.</p>
<p>Figure 9 :
9
Figure9: Average attention flow of the CGD model.All layers shown begin with a "planning" step, focusing on the Critique (48.1%) and Student Answer (36.0%).The final layer (bottom right) then pivots sharply to an "execution" phase, focusing on the Problem (&gt; 90%), while the first layer (top left) continues to process the Critique.Shaded regions represent the 95% confidence interval over 50 samples.</p>
<p>Mass of nitride anion = 3.443 × 10 23 × 14.007 u = 4.835 × 10 25 u To convert the mass from unified atomic mass units to kilograms, we use the conversion factor 1 u = 1.66053904 × 10 −27 kg:Mass in kg = 4.835 × 10 25 × 1.66053904 × 10 −27 = 8.033 × 10 −2 kg So, the mass of the nitride anion is approximately 8.033 × 10 −2 kg or 80.33 g.Critique:The solution provided attempts to calculate the mass of a nitride anion based on the number of nitrogen-based electrons.Let's review the steps and calculations for accuracy and clarity.1. Determination of the Number of Nitrogen Atoms: The solution starts by dividing the total number of electrons by 7 to find the number of nitrogen atoms.This assumes each nitrogen atom has 7 electrons, which is true for neutral nitrogen.However, the question refers to a nitride anion, typically N 3− , which has gained 3 electrons for a total of 10 electrons per atom.Thus, the correct calculation should be: Number of nitrogen atoms = 2.408 × 10 24 10 = 2.408 × 10 23 2. Calculation of the Mass: Using the corrected number of nitrogen atoms and the same atomic mass: Mass = 2.408 × 10 23 × 14.007 u = 3.373 × 10 24 u 3. Conversion to Kilograms: Mass in kg = 3.373 × 10 24 × 1.66053904 × 10 −27 = 5.605 × 10 −3 kg</p>
<p>from R 2×2 to R 2×2 an isomorphism?Answer with T rue or F alse .(True or False) Answer: Step 1: Definition of an Isomorphism An isomorphism is a bijective linear transformation between two vector spaces.For T (M ) = 1 2 3 4 M to be an isomorphism, it must satisfy the following conditions: 1. <strong>Injectivity</strong>: T (M1) = T (M2) ⇒ M1 = M2 2. <strong>Surjectivity</strong>: For every M ∈ R 2×2 , there exists M ′ ∈ R 2×2 such that T (M ′ ) = M 3. <strong>Linearity</strong>: T (aM1 + bM2) = aT (M1) + bT (M2) for all a, b ∈ R and M1, M2 ∈ R 2×2</p>
<p>True or false: there exists a graph with score (1, 1, 1, 2, 2, 3, 4, 5, 5).Answer with T rue or F alse .(True or False) Answer: To determine if there exists a graph with the given score, we need to understand what a score in graph theory represents.</p>
<p>The final answer is: T rue Generated Answer after CGD Question: Is the transformation T (from R 2×2 to R 2×2 an isomorphism?Answer with T rue or F alse .(True or False) Answer: To determine if the transformation T (from R 2×2 to R 2×2 is an isomorphism, we need to check if it satisfies the two properties of an isomorphism: 1. <strong>Injectivity</strong>: The transformation must be one-to-one, meaning that if T (M1) = T (M2), then M1 = M2.2. <strong>Surjectivity</strong>: The transformation must be onto, meaning that for every matrix N ∈ R 2×2 , there exists a matrix M ∈ R 2×2 such that T (M ) = N .Let's examine the transformation T (Now, let's check for injectivity: Suppose T (M1) = T (M2).Then:1a1 + 2c1 1b1 + 2d1 3a1 + 4c1 3b1 + 4d1 = 1a2 + 2c2 1b2 + 2d2 3a2 + 4c2 3b2 + 4d2This implies that the corresponding elements of the two matrices are equal:1a1 + 2c1 = 1a2 + 2c2 1b1 + 2d1 = 1b2 + 2d2 3a1 + 4c1 = 3a2 + 4c2 3b1 + 4d1 = 3b2 + 4d2Simplifying, we get:a1 = a2, c1 = c2, b1 = b2, d1 = d2Therefore, M1 = M2, and the transformation is injective.Next, let's check for surjectivity:Given any matrix N = e f g h ∈ R 2×2 , we need to find a matrix M = a b c d such that T (M ) = N .</p>
<p>,
Accuracy (%)10 20 30 40 50 60MATH500 41.2 50.6 53.4 51.8 54.2 LLaMA3.1-8B Instruct OlympiadBench 10.8 14.5 15.7 19.6 23.7AMC23 20.0 22.5 27.5 22.5 37.5 SFTTheoremQA 27.6 22.1 28.9 28.5 34.0 Distilled SFTGPQA 33.3 30.8 31.8 34.3 35.9 CFT CGD (Ours) MMLU-PRO 31.2 39.3 35.1 34.3 40.3Figure 2: Performance comparison of CGD, using a LLaMA3.3-70B Instruct teachermodel to generate critiques and refined answers, with 100K samples from WebInstruct (Yue</p>
<p>Table 2 :
2
Effect of different fine-tuning strategies on Llama 3.1-8B Instruct across diverse benchmarks.While Distilled SFT and CGD preserve or improve performance, CFT severely degrades general capabilities, highlighting the importance of model-specific inductive biases when applying critique-based supervision.
MethodIFEval MUSR TruthfulQA BBHLLaMA3.1-8B Instruct76.937.854.048.3+ SFT76.636.952.048.0+ Distilled SFT77.539.053.947.0+ CFT  *  w/ GPT4o55.635.053.544.2+ CGD76.139.354.547.1</p>
<p>Table 3 :
3
Comparison of CGD with RL-based training (SimpleRL-Zero) on LLaMA3.1-8Bbase.RL training requires orders of magnitude more resources, whereas CGD provides balanced improvements across reasoning benchmarks with far greater efficiency.
MethodMATH500 Minerva-Math OlympiadBench AMC23 AIME24 Avg.SimpleRL-Zero (official)23.09.65.315.00.010.6CGD (ours)29.412.97.010.00.011.9</p>
<p>Table 4 :
4
Comparison of CGD and CFT using 100K WebInstruct critique-augmented samples.CGD consistently outperforms CFT across all benchmarks and is relatively robust to learning rate changes, while CFT exhibits significant performance degradation at higher learning rates.The Avg. column reflects average performance across all tasks.
MethodMATH500 Minerva-Math GSM8K OlympiadBench AMC23 TheoremQA Avg.CFT (LR = 1 × 10 −6 )51.832.784.815.722.528.539.3CFT (LR = 5 × 10 −6 )33.410.382.910.127.516.130.1CGD (LR = 1 × 10 −6 )54.233.685.723.737.534.044.8CGD (LR = 5 × 10 −6 )55.030.182.321.632.531.942.2</p>
<p>By directly embedding explanatory feedback into the training loop, CGD offers a promising and efficient path toward developing more capable and reliable language models.Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou.Scaling relationship on learning mathematical reasoning with large language models.arXiv preprint arXiv:2308.01825,2023.Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen.Mammoth2: Scaling instructions from the web, 2024.URL https://arxiv.org/abs/2405.03548.Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He.Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025.URL https://arxiv.org/abs/2503.18892.Yifei Zhang, Bo Pan, Chen Ling, Yuntong Hu, and Liang Zhao.Elad: Explanation-guided large language models active distillation.arXiv preprint arXiv:2402.13098,2024.Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou.Instruction-following evaluation for large language models, 2023.URL https: //arxiv.org/abs/2311.07911.</p>
<p>Table 5 :
5
Summary of hyperparameters used in our experiments.We further analyze the impact of the training data's critique composition in Appendix B.1.4,findingthat a balanced mixture of feedback yields the most robust model.B.1.1ABLATION:GENERALIZATIONTOMATH-SPECIFIC TRAINING DATATo test the generalizability of our method, we conducted experiments using MetaMathQA, a mathreasoning-focused dataset.As shown in Table6, CGD again demonstrates strong performance, outperforming all baselines on on Group 1 and Group 2. This confirms that the benefits of the CGD framework are not limited to a specific data source.Notably, CGD surpasses the strongest baseline, i.e., CFT, on the advanced MATH500 and OlympiadBench challenges, yet shows slightly lower performance on Minerva-Math and GSM8K, which consist of middle-school to undergraduate-level problems.
HyperparameterValueBatch size64Learning rate1e-6OptimizerAdamWScheduler typecosineMax sequence length8192Number of epochs1Warmup ratio0.1eters.</p>
<p>Table 6 :
6
Comparison of fine-tuning methods on LLaMA3.1-8BInstruct across math and reasoning tasks using 100K MetaMathQA examples with LLaMA3.3-70BInstruct as the teacher model.Optimal results are highlighted in bold, while suboptimal outcomes are underlined.The Avg. columns represent the average performance across Groups 1 and 2, respectively.
Math Reasoning Tasks (Group 1)General Reasoning Tasks (Group 2)MethodMATH500 Minerva-Math GSM8K OlympiadBench AMC23 Avg. TheoremQA GPQA MMLU-PRO Avg.LLaMA3.1-8B Instruct50.633.585.314.522.541.327.630.831.229.9+ SFT47.829.885.513.627.540.828.132.837.032.6+ Distilled SFT50.233.579.818.535.043.431.228.828.129.4+ CFT52.836.488.617.232.545.531.130.738.333.4+ CGD59.034.687.321.832.547.034.130.336.133.5∆ = CGD -CFT6.2-2.2-1.34.60.01.53.0-0.5-2.20.1B.1.2 ABLATION: RESULTS ON DIFFERENT STUDENT MODELS</p>
<p>Table 7 :
7
Evaluation of fine-tuning methods on Mixtral-8x7B Instruct across math-focused (Group 1) and general reasoning (Group 2) benchmarks, using WebInstruct as the training set.CGD achieves the strongest performance in both groups, despite Mixtral being a different architecture than LLaMA.All methods are fine-tuned on 100K WebInstruct samples.Bold numbers denote the best, and underlined values indicate the second-best performance.The ∆ row shows CGD's gains over the CFT baseline.
Math Reasoning Tasks (Group 1)General Reasoning Tasks (Group 2)MethodMATH500 Minerva-Math GSM8K OlympiadBench AMC23 Avg. TheoremQA GPQA MMLU-PRO Avg.Mixtral-8x7B Instruct29.615.469.48.97.526.221.221.724.722.5+ SFT31.415.665.67.95.025.120.420.525.222.0+ CFT35.620.663.811.110.028.223.631.816.023.8+ CGD39.023.975.011.77.531.426.425.823.325.1∆ = CGD -CFT3.311.20.8-2.53.22.8-6.07.31.3OLMo-2-1124-7B-Instruct35.416.581.911.07.530.523.028.334.128.5+ SFT36.415.180.511.012.531.119.128.134.127.2+ CFT35.916.881.211.810.031.119.327.433.426.7+ CGD37.416.983.212.120.033.924.228.334.228.2∆ = CGD -CFT1.50.12.00.310.02.84.9-1.10.71.5
Moon et al., 2025;Liu et al., 2024)) suggests that models tuned with richer dialogue-style data better leverage multi-step feedback.These results highlight that CGD is most effective when the student has been trained with supervision formats resembling critique/refinement, and they motivate deeper investigation into model-specific receptivity to critique-based training.</p>
<p>Table 8 :
8
Comparison of CGD using different teacher models on the student model LLaMA3.1-8BInstruct across math (Group 1) and general reasoning (Group 2) benchmarks, using WebInstruct as the training set.Using S1.1 as the teacher model achieves a stronger performance in complex math-reasoning tasks, despite S1.1 being a different architecture than LLaMA.
Math Reasoning Tasks (Group 1)General Reasoning Tasks (Group 2)MethodMATH500 Minerva-Math GSM8K OlympiadBench AMC23 Avg. TheoremQA GPQA MMLU-PRO Avg.Initialized from LLaMA3.1-8B InstructLLaMA3.1-8B Instruct50.633.585.314.522.541.327.630.831.229.9+ CGD with LLaMA3.3-70B54.233.685.723.737.546.934.035.940.336.7+ CGD with S1.1-32B56.837.186.816.742.548.032.234.340.435.7Teacher ModelsLLaMA3.3-70B Instruct75.355.996.139.365.066.353.637.970.654.0S1.1-32B92.958.194.863.685.078.964.448.352.9</p>
<p>Table 9 :
9
Summary of predictive confidence (Mean Entropy), averaged over 500 samples from OpenMathInstruct 2. Lower entropy is better.Significance markers (<em>, </em><em>, </em><em><em>) denote the p-value of a paired t-test comparing each baseline to our CGD model.
ModelMean EntropyBaseline SFT6.56  *  *  </em>SFT6.62  *  *  </em>Distilled SFT6.49  *  *  *CGD6.44
Significance: *** p &lt; 0.001</p>
<p>Table 10 :
10
Gradient norm analysis for the final trained CGD model.The presence of a critique provides a more efficient signal, reducing the update magnitude by 27%.
CGD Model Condition Mean Gradient Norm Std. Dev. Gradient NormWithout Critique2446.92011.9With Critique1802.71765.5</p>
<p>Table 11 :
11
Counterfactual analysis results for a representative sample (Sample #42).While both models are correct with factual guidance, only the CGD model is robust to the flawed, counterfactual critique.To find the smallest positive integer divisible by five distinct prime numbers, we need to understand the concept of prime numbers first... Therefore, the smallest positive integer divisible by five distinct prime numbers is 2310.This result is a direct consequence of the multiplication of the smallest prime numbers, which are the fundamental building blocks of all other numbers.
ModelOutcome (Factual Critique) Outcome (Counterfactual Critique)LLaMA3.1-8B InstructCORRECTINCORRECT (Derailed)CGDCORRECTCORRECT (Robust)BASELINE MODEL ANALYSISCondition: Factual CritiqueCorrectness: CORRECTFull Output:→→...2 * 3 * 5 * 7 * 11 = 2310</p>
<p>1569, Identifying the Five Smallest Distinct Prime NumbersThe first five prime numbers in ascending order are 2, 3, 5, 7, and 11.These numbers are distinct, meaning they are all different from one another, ... In summary, the process of finding the smallest positive integer divisible by five distinct prime numbers involves identifying the five smallest prime numbers, which are 2, 3, 5, 7, and 11, and then multiplying them together.This method ensures that the resulting product is the smallest possible integer that is divisible by each of the five distinct primes, making 2310 the correct answer to the problem.
→→### Multiplication of the Prime NumbersTo find the smallest positive integer divisible by these five distinct→prime numbers, we multiply them together:2 * 3 * 5 * 7 * 11 = 2310This calculation is straightforward and demonstrates the direct→application of the initial logic. By multiplying the smallest→prime numbers, ...### ConclusionTherefore, the smallest positive integer divisible by five distinct→prime numbers is indeed 2310. This result is a direct consequence→of multiplying the five smallest distinct prime numbers together.→The approach taken is logical and mathematically sound, ...→→→→→→Condition: Counterfactual CritiqueCorrectness: CORRECTParsed Answer: 2310Full Output:To find the smallest positive integer divisible by five distinct prime→ → → → →numbers, we need to follow a systematic approach that involves ... However, the initial step involves understanding what prime numbers are and how to identify them.Prime numbers are natural numbers greater than 1 that have no positiveCGD-TRAINED MODEL ANALYSIS ... →Condition: Factual Critique The five smallest distinct prime numbers are indeed 2, 3, 5, 7, andCorrectness: CORRECT 11. These are the first five prime numbers in the sequence of →Parsed Answer: 2310 prime numbers. →Full Output:Now, to find the smallest positive integer divisible by these five## Finding the Smallest Positive Integer Divisible by Five Distinct distinct prime numbers, ... →Prime Numbers The calculation is as follows: →To find the smallest positive integer divisible by five distinct prime 2 * 3 * 5 * 7 * 11 = 2310numbers, we need to follow a straightforward approach that Therefore, the smallest positive integer divisible by five distinct → involves identifying the five smallest distinct prime numbers and → prime numbers is indeed 2310. This number is the product of the then multiplying them together. This method is based on the → five smallest distinct prime numbers, ensuring that it is fundamental principle that prime numbers are the building blocks → of all other numbers, and ... → divisible by each of them without leaving a remainder.### Definition of Prime NumbersBefore proceeding, it's essential to understand what prime numbersare. Prime numbers are natural numbers greater ...
→ ### → → →</p>
<p>https://huggingface.co/simplescaling/s1.1-3B
https://huggingface.co/simplescaling/s1.1-32B
Due to licensing and regulatory restrictions, we were unable to directly use certain models (e.g., Qwen) as students or pair GPT-4o as a teacher. Accordingly, our experiments focus on the LLaMA and S1.1 families, where such usage is permitted.
https://github.com/hkust-nlp/simpleRL-reason
SUPPLEMENTARY MATERIALIn this supplementary material, we provide the following additional details for our work.• Appendix A: Experimental Setup Details.We provide full details on our training and evaluation hyperparameters.• Appendix B: Additional Benchmark Results.We present extensive ablation studies, including results on different student and teacher models, learning rate sensitivity analyses, and training curves.• Appendix C: Detailed Diagnostic Analyses.We provide the full methodology, quantitative results, and qualitative visualizations for the experiments that analyze the internal behavior of the CGD-trained models.A EXPERIMENTAL SETUP AND HYPERPARAMETERSA.1 EXPERIMENTAL SETUP All experiments were conducted using NVIDIA A100 40GB GPUs.For training large-scale models, we employed DeepSpeed ZeRO-3 optimization for efficient memory and compute scaling across multiple GPUs, which enables optimizer state partitioning, gradient partitioning, and activation checkpointing to support training with larger batch sizes and model sizes.We evaluate model performance using exact match accuracy, averaged over the test sets, and report mean performance over three random seeds to account for training variability.A.2 HYPERPARAMETERSWe provide the key hyperparameters used in training our models across all experiments.Unless otherwise noted, these values were held constant.B ADDITIONAL EXPERIMENTAL RESULTSB.1 ABLATION STUDIESIn this section, we analyze the mechanisms behind CGD's effectiveness and study the impact of ablation studies.We demonstrate CGD's robustness across different training datasets and hyperparam-reasoning process: it internalizes the critique's guidance at an early stage and then acts upon this internalized knowledge in its final, semantic layers to plan and execute a corrected solution.The following section provides a direct behavioral test of this hypothesis.C.4 CASE STUDY: COUNTERFACTUAL ANALYSISTo test whether CGD learns a functional skill of robust reasoning beyond simple contextual understanding, we performed a qualitative case study.We presented both the baseline Llama 3.1 Instruct model and our final CGD-trained model with a problem from our test set under two conditions.In the Factual condition, we provided the original, correct critique from our dataset.In the Counterfactual condition, we provided a generic but nonsensical critique that was irrelevant to the problem (e.g., incorrectly referencing a "quadratic formula" for a number theory problem).We then evaluated the correctness of the final generated answer in all four scenarios.Table11summarizes the results for a representative sample.The results demonstrate a clear behavioral divergence.While both models produce the correct answer when given a factual critique, their responses to flawed guidance differ significantly.The baseline model proves brittle; it is functionally derailed by the nonsensical critique, failing to produce a correct answer.The CGD model, in contrast, exhibits superior robustness.While we have not performed experiments at scale to isolate the reason for the robustness, we hypothesize that during the course of CGD training, the model learns to discriminate between informative and non-informative critique, and even ignore the non-informative critique to arrive at the correct response.Case Study: Counterfactual Analysis on Sample #42The goal of this analysis is to test for a functional understanding of critiques.The problem's gold-standard answer is 2310.E CODE INSTRUCTIONSWe include a code archive (code.zip)as part of the supplementary material.This package provides all necessary components to reproduce our key results and facilitate further experimentation.Specifically, it includes:• Critique Generation and Refinement: Scripts for generating critiques from model outputs and refining answers using these critiques.• Evaluation Codebase: End-to-end evaluation pipelines for computing exact match accuracy and other relevant metrics across multiple benchmarks.• LLaMA-Factory Integration: The LLaMA-Factory framework to support Supervised Fine-Tuning (SFT).• Configuration Files and Run Scripts: YAML/JSON config files and shell scripts used to launch experiments across various model architectures and tasks.Instructions for installing dependencies, setting up the environment, and running end-to-end training and evaluation pipelines are included in the README.mdfile within the archive.
Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Chris Olah, Ben Mann, and Jared Kaplan2022Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish</p>
<p>Let me teach you: Pedagogical foundations of feedback for language models. Beatriz Borges, Niket Tandon, Tanja Käser, Antoine Bosselut, arXiv:2307.002792023arXiv preprint</p>
<p>CodeT: Code generation with generated tests. Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, Weizhu Chen, International Conference on Learning Representations. 2023a</p>
<p>TheoremQA: A theorem-driven question answering dataset. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia, 10.18653/v1/2023.emnlp-main.489Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeDecember 2023bAssociation for Computational Linguistics</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Sergey Quoc V Le, Yi Levine, Ma, arXiv:2501.17161Sft memorizes, rl generalizes: A comparative study of foundation model post-training. 2025arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021a</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021barXiv preprint</p>
<p>. Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z F Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J L Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R J Chen, R L Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, S S Shuting Pan, Shuang Li, Shaoqing Zhou, Shengfeng Wu, Tao Ye, Tian Yun, Tianyu Pei, T Sun, Wangding Wang, Wanjia Zeng, Wen Zhao, Wenfeng Liu, Wenjun Liang, Wenqin Gao, Wentao Yu, W L Zhang, Wei Xiao, Xiaodong An, Xiaohan Liu, Xiaokang Wang, Xiaotao Chen, Xin Nie, Xin Cheng, Xin Liu, Xingchao Xie, Xinyu Liu, Xinyuan Yang, Xuecheng Li, Xuheng Su, X Q Lin, Xiangyue Li, Xiaojin Jin, Xiaosha Shen, Xiaowen Chen, Xiaoxiang Sun, Xinnan Wang, Xinyi Song, Xianzu Zhou, Xinxia Wang, Y K Shan, Y Q Li, Y X Wang, Yang Wei, Yanhong Zhang, Yao Xu, Yao Li, Yaofeng Zhao, Yaohui Sun, Yi Wang, Yichao Yu, Yifan Zhang, Yiliang Shi, Ying Xiong, Yishi He, Yisong Piao, Yixuan Wang, Yiyang Tan, Yiyuan Ma, Yongqiang Liu, Yuan Guo, Yuduan Ou, Yue Wang, Yuheng Gong, Yujia Zou, Yunfan He, Yuxiang Xiong, Yuxiang Luo, Yuxuan You, Yuyang Liu, Y X Zhou, Yanhong Zhu, Yanping Xu, Yaohui Huang, Yi Li, Yuchen Zheng, Yunxian Zhu, Ying Ma, Yukun Tang, Yuting Zha, Z Z Yan, Zehui Ren, Zhangli Ren, Zhe Sha, Zhean Fu, Zhenda Xu, Zhengyan Xie, Zhewen Zhang, Zhicheng Hao, Zhigang Ma, Zhiyu Yan, Zihui Wu, Zijia Gu, Zijun Zhu, Zilin Liu, Ziwei Li, Xie, 2025Ziyang Song, Zizheng PanZhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</p>
<p>Llm self-correction with decrim: Decompose, critique, and refine for enhanced following of instructions with multiple constraints. Thomas Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, Nanyun Peng, 2024</p>
<p>Baldur: Whole-proof generation and repair with large language models. Emily First, Markus N Rabe, Talia Ringer, Yuriy Brun, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2023</p>
<p>Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D Goodman, arXiv:2503.01307Cognitive behaviors that enable self-improving reasoners: Four habits of highly effective stars. 2025arXiv preprint</p>
<p>Rarr: Researching and revising what language models say, using language models. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: Large language models can self-correct with tool-interactive critiquing. Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024. 2024The Twelfth International Conference on Learning Representations</p>
<p>Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Bethany Baptiste Roziere, Binh Biron, Bobbie Tang, Charlotte Chern, Chaya Caucheteux, Chloe Nayak, Chris Bi, Chris Marra, Christian Mcconnell, Christophe Keller, Chunyang Touret, Corinne Wu, Cristian Canton Wong, Cyrus Ferrer, Damien Nikolaidis, Daniel Allonsius, Danielle Song, Danny Pintz, Danny Livshits, David Wyatt, Dhruv Esiobu, Dhruv Choudhary, Diego Mahajan, Diego Garcia-Olano, Dieuwke Perino, Egor Hupkes, Ehab Lakomkin, Elina Albadawy, Emily Lobanova, Eric Michael Dinan, Filip Smith, Francisco Radenovic, Frank Guzmán, Gabriel Zhang, Gabrielle Synnaeve, Georgia Lee, Govind Lewis Anderson, Graeme Thattai, Gregoire Nail, Guan Mialon, Guillem Pang, Hailey Cucurell, Hannah Nguyen, Hu Korevaar, Hugo Xu, Iliyan Touvron, Zarov, Arrieta Imanol, Isabel Ibarra, Ishan Kloumann, Ivan Misra, Jack Evtimov, Jade Zhang, Jaewon Copet, Jan Lee, Jana Geffert, Jason Vranes, Jay Park, Jeet Mahadeokar, Jelmer Shah, Jennifer Van Der Linde, Jenny Billock, Jenya Hong, Jeremy Lee, Jianfeng Fu, Jianyu Chi, Jiawen Huang, Jie Liu, Jiecao Wang, Joanna Yu, Joe Bitton, Jongsoo Spisak, Joseph Park, Joshua Rocca, Joshua Johnstun, Junteng Saxe, Kalyan Jia, Karthik Vasuden Alwala, Kartikeya Prasad, Kate Upasani, Ke Plawiak, Kenneth Li, Kevin Heafield, Khalid Stone, Krithika El-Arini, Kshitiz Iyer, Kuenley Malik, Kunal Chiu, Kushal Bhalla, Lauren Lakhotia, Laurens Rantala-Yeary, Lawrence Van Der Maaten, Liang Chen, Liz Tan, Louis Jenkins, Lovish Martin, Lubo Madaan, Lukas Malo, Lukas Blecher, Luke Landzaat, Madeline De Oliveira, Mahesh Muzzi, Mannat Pasupuleti, Manohar Singh, Marcin Paluri, Maria Kardas, Mathew Tsimpoukelli, Mathieu Oldham, Maya Rita, Melanie Pavlova, Mike Kambadur, Min Lewis, Mitesh Kumar Si, Mona Singh, Naman Hassan, Narjes Goyal, Nikolay Torabi, Nikolay Bashlykov, Niladri Bogoychev, Ning Chatterji, Olivier Zhang, Duchenne, C Onur, Patrick ¸elebi, Pengchuan Alrassy, Pengwei Zhang, Petar Li, Peter Vasic, Prajjwal Weng, Pratik Bhargava, Praveen Dubal, Punit Krishnan, Puxin Singh Koura, Qing Xu, Qingxiao He, Ragavan Dong, Raj Srinivasan, Ramon Ganapathy, Ricardo Silveira Calderer, Robert Cabral, Roberta Stojnic, Rohan Raileanu, Rohit Maheswari, Rohit Girdhar, Romain Patel, Ronnie Sauvestre, Roshan Polidoro, Ross Sumbaly, Ruan Taylor, Rui Silva, Rui Hou, Saghar Wang, Sahana Hosseini, Sanjay Chennabasappa, Sean Singh, Bell, Sonia Seohyun, Sergey Kim, Shaoliang Edunov, Sharan Nie, Sharath Narang, Sheng Raparthy, Shengye Shen, Shruti Wan, Shun Bhosale, Simon Zhang, Soumya Vandenhende, Spencer Batra, Sten Whitman, Stephane Sootla, Suchin Collot, Sydney Gururangan, Tamar Borodinsky, Tara Herman, Tarek Fowler, Thomas Sheasha, Thomas Georgiou, Tobias Scialom, Todor Speckbacher, Tong Mihaylov, Ujjwal Xiao, Vedanuj Karn, Vibhor Goswami, Vignesh Gupta, Viktor Ramanathan, Vincent Kerkez, Virginie Gonguet, Vish Do, Vítor Vogeti, Vladan Albiero, Weiwei Petrovic, Wenhan Chu, Wenyin Xiong, Whitney Fu, Xavier Meers, Xiaodong Martinet, Xiaofang Wang, Wang, Ellen Xiaoqing, Xide Tan, Xinfeng Xia, Xuchao Xie, Xuewei Jia, Yaelle Wang, Yashesh Goldschlag, Yasmine Gaur, Yi Babaei, Yiwen Wen, Yuchen Song, Yue Zhang, Yuning Li, Zacharie Delpierre Mao, Zheng Coudert, Zhengxing Yan, Zoe Chen, Aaditya Papakipos, Aayushi Singh, Abha Srivastava, Adam Jain, Adam Kelsey, Adithya Shajnfeld, Adolfo Gangidi, Ahuva Victoria, Ajay Goldstand, Ajay Menon, Alex Sharma, Alexei Boesenberg, Allie Baevski, Amanda Feinstein, Amit Kallet, Amos Sangani, Anam Teo, Andrei Yunus, Andres Lupu, Andrew Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Ankit Ryan, Annie Ramchandani, Annie Dong, Anuj Franco, Aparajita Goyal, Arkabandhu Saraf, Ashley Chowdhury, Ashwin Gabriel, Assaf Bharambe, Azadeh Eisenman, Beau Yazdan, Ben James, Benjamin Maurer, Bernie Leonhardi, Beth Huang, Beto Loyd, Bhargavi De Paola, Bing Paranjape, Bo Liu, Boyu Wu, Braden Ni, Bram Hancock, Brandon Wasti, Brani Spence, Stojkovic ; Ce, Changhan Liu, Changkyu Wang, Chao Kim, Chester Zhou, Ching-Hsiang Hu, Chris Chu, Chris Cai, Christoph Tindal, Dustin Feichtenhofer ; Duc Le, Edward Holland, Eissa Dowling, Elaine Jamil, Eleonora Montgomery, Emily Presani, Emily Hahn, Eric-Tuan Wood, Erik Le, Esteban Brinkman, Evan Arcaute, Evan Dunbar, Fei Smothers, Felix Sun, Feng Kreuk, Filippos Tian, Firat Kokkinos, Francesco Ozgenel, Frank Caggioni, Frank Kanayet, Gabriela Medina Seide, Gabriella Florez, Gada Schwarz, Georgia Badeer, Gil Swee, Grant Halpern, Grigory Herman, Sizov, Guangyi, Guna Zhang, Hakan Lakshminarayanan, Hamid Inan, Han Shojanazeri, Hannah Zou, Hanwen Wang, Haroun Zha, Harrison Habeeb, Helen Rudolph, Henry Suk, Hunter Aspegren, Hongyuan Goldman, Ibrahim Zhan, Igor Damlaj, Igor Molybog, Ilias Tufanov, Irina-Elena Leontiadis, Itai Veliche, Jake Gat, James Weissman, James Geboski, Janice Kohli, Japhet Lam, Jean-Baptiste Asher, Jeff Gaya, Jeff Marcus, Jennifer Tang, Jenny Chan, Jeremy Zhen, Jeremy Reizenstein, Jessica Teboul, Jian Zhong, Jingyi Jin, Joe Yang, Stephanie Cummings, Stephen Max, Chen ; Sumit, Summer Gupta, Sungmin Deng, Sunny Cho, Suraj Virk, Sy Subramanian, Sydney Choudhury, Tal Goldman, Tamar Remez, Glaser ; Nam, Wang Yu, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Parth Parekh, Paul Saab, Pavan Balaji. Victoria Ajayi, Victoria Montanez, Vijai Mohan, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia,; Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang,; Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres; Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt; Steve Kehoe; Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta; Ye Hu, Ye Jia, Ye Qi, Yenda LiSudarshan Govindaprasad2024Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala,Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen. Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models</p>
<p>Accelerating the science of language models. Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun, 2024</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, Advances in Neural Information Processing Systems. 202336</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra, 2022</p>
<p>On the robustness trade-off in fine-tuning. Kunyang Li, Jean-Charles Noirot Ferrand, Ryan Sheatsley, Blaine Hoak, Yohan Beugin, Eric Pauley, Patrick Mcdaniel, arXiv:2503.148362025arXiv preprint</p>
<p>Aligning instruction tuning with pre-training. Yiming Liang, Tianyu Zheng, Xinrun Du, Ge Zhang, Jiaheng Liu, Xingwei Qu, Wenqiang Zu, Xingrun Xing, Chujie Zheng, Lei Ma, Guoyin Wang, Zhaoxiang Zhang, Wenhao Huang, Xiang Yue, Jiajun Zhang, 2025</p>
<p>Using large language models to provide explanatory feedback to human tutors. Jionghao Lin, Danielle R Thomas, Feifei Han, Shivang Gupta, Wei Tan, Ngoc Dang Nguyen, Kenneth R Koedinger, arXiv:2306.154982023arXiv preprint</p>
<p>How can i improve? using gpt to highlight the desired and undesired parts of open-ended responses. Jionghao Lin, Eason Chen, Zeifei Han, arXiv:2405.002912024arXiv preprint</p>
<p>Truthfulqa: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, 2022</p>
<p>What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, Junxian He, 2024</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, arXiv:2303.176512023arXiv preprint</p>
<p>Call for rigor in reporting quality of instruction tuning data. Hyeonseok Moon, Jaehyung Seo, Heuiseok Lim, 10.18653/v1/2025.acl-short.9Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics. Wanxiang Che, Joyce Nabende, Ekaterina Shutova, Mohammad Taher, Pilehvar , the 63rd Annual Meeting of the Association for Computational LinguisticsVienna, AustriaAssociation for Computational LinguisticsJuly 20252Short Papers)</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, ; , Ryan Lowe, arXiv:2203.02155Jan Leike,. 2022Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul ChristianoarXiv preprint</p>
<p>Refiner: Reasoning feedback on intermediate representations. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, Proceedings of the 18th Conference of the European Chapter. Long Papers. the 18th Conference of the European Chapterthe Association for Computational Linguistics20241</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, 2023</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Trishala Wang, Jos Neeraj, Abheesht Rozen, Andrea Sharma, Thibault Santilli, Jason Fevry, Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M Rush, International Conference on Learning Representations. 2022</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, Jan Leike, arXiv:2206.058022022arXiv preprint</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, 2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Distilling reasoning capabilities into smaller language models. Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Musr: Testing the limits of chain-of-thought with multistep soft reasoning. Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett, 2024</p>
<p>Climbing the ladder of reasoning: What llms can-and still can't-solve after sft?. Yiyou Sun, Georgia Zhou, Hao Wang, Dacheng Li, Nouha Dziri, Dawn Song, ; Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, Chuang Gan, arXiv:2504.11741The Thirtyeighth Annual Conference on Neural Information Processing Systems. 2025. 2024arXiv preprintEasy-to-hard generalization: Scalable alignment beyond human supervision</p>
<p>Challenging bigbench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, 2022</p>
<p>Learning to refine with fine-grained natural language feedback. Manya Wadhwa, Xinyu Zhao, Junyi , Jessy Li, Greg Durrett, 2024</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen, 2024</p>
<p>Critique fine-tuning: Learning to critique is more effective than learning to imitate. Yubo Wang, Xiang Yue, Wenhu Chen, arXiv:2501.177032025arXiv preprint</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Llmrefine: Pinpointing and refining large language models via fine-grained actionable feedback. Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, Markus Freitag, 2024</p>
<p>Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu, Limo, arXiv:2502.03387Less is more for reasoning. 2025arXiv preprint</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, 2024a</p>
<p>Teaching language models to self-improve through interactive demonstrations. Xiao Yu, Baolin Peng, Michel Galley, Jianfeng Gao, Zhou Yu, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 2024b1</p>            </div>
        </div>

    </div>
</body>
</html>
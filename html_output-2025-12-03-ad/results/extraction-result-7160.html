<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7160 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7160</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7160</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-b8384139339e4487840fef42e73b4ead65f227d3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b8384139339e4487840fef42e73b4ead65f227d3" target="_blank">LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> The Decompose, Critique and Refine (DeCRIM) self-correction pipeline is proposed, which enhances LLMs' ability to follow constraints and improves Mistral's performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback.</p>
                <p><strong>Paper Abstract:</strong> Instruction following is a key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post"in a funny tone"with"no hashtag"). Despite this, most evaluations focus solely on synthetic data. To address this, we introduce RealInstruct, the first benchmark designed to evaluate LLMs' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants. We also investigate model-based evaluation as a cost-effective alternative to human annotation for this task. Our findings reveal that even the proprietary GPT-4 model fails to meet at least one constraint on over 21% of instructions, highlighting the limitations of state-of-the-art models. To address the performance gap between open-source and proprietary models, we propose the Decompose, Critique and Refine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to follow constraints. DeCRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM's response needs refinement. Our results show that DeCRIM improves Mistral's performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback. Moreover, we demonstrate that with strong feedback, open-source LLMs with DeCRIM can outperform GPT-4 on both benchmarks.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7160.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7160.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DECRIM (Self+Supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decompose, Critique, and Refine (DECRIM) — Self-Decomposer with Supervised Critic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-correction pipeline that (1) decomposes an instruction into granular constraints, (2) uses a Critic to identify unsatisfied constraints and produce natural-language feedback, and (3) has the underlying LLM refine its response until all constraints are satisfied or a max iteration is reached; here the Critic is a weakly supervised Mistral fine-tuned with GPT-4-Turbo reasoning trails.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B Instruct v0.2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruct-tuned transformer LLM used as the underlying generator (and base for the supervised critic via LoRA fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>DECRIM (Decompose → Critique → Refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate initial response; decompose instruction into constraint list; Critic evaluates each constraint and outputs feedback on unsatisfied ones; generator refines response using feedback; repeat until constraints satisfied or N_max.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>REALINSTRUCT; IFEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>REALINSTRUCT: benchmark of real-user multi-constrained instructions; IFEval: synthetic constrained-instruction benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Instruction-level accuracy and Constraint-level accuracy (also OQA pairwise quality ranking Win/Loss; Macro F1 used for judge evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Make Sure baseline: Instruction Acc 76.8%, Constraint Acc 88.6%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Instruction Acc 80.5% (+3.7), Constraint Acc 90.9% (+2.3)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvement depends heavily on Critic quality; weak critic gives limited but positive gains. Computational overhead (extra LLM calls) increases run time; many revisions occur in first iterations but excessive iterations can degrade quality. Still sometimes fails on numeric/negation constraints and long multi-constraint instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7160.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7160.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DECRIM (Oracle Decomposer + Supervised Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DECRIM with Oracle (human-verified) Decomposer and Supervised Critic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same DECRIM pipeline but using gold (human-verified) constraint decomposition; used as a realistic ablation to test upper-bound effects of accurate decomposition while keeping the supervised critic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B Instruct v0.2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruct-tuned transformer LLM as generator; supervised Mistral critic via LoRA for critique.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>DECRIM (with Oracle Decomposer)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same as DECRIM but the list of constraints comes from gold annotations (oracle decomposer), enabling more focused critique and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>REALINSTRUCT; IFEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>REALINSTRUCT: real-user multi-constraint instruction following; IFEval: synthetic constrained instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Instruction-level accuracy and Constraint-level accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Make Sure baseline: Instruction Acc 76.8%, Constraint Acc 88.6%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Instruction Acc 82.4% (+5.6), Constraint Acc 91.7% (+3.1)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Oracle decomposer is an ablation (human-verified) and thus not fully realistic; gains show importance of accurate decomposition but real decomposers may be noisy. Critic quality still the dominant factor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7160.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7160.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DECRIM (Self+Self)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DECRIM using the LLM itself as both Decomposer and Critic (Self-Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of DECRIM where the underlying LLM both lists constraints and judges its own response (ICL-Const+CoT prompt used for self-critique); tested to evaluate intrinsic self-refinement capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B Instruct v0.2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruct-tuned 7B transformer used as generator, decomposer and critic via prompting (ICL + Chain-of-Thought).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Critique (DECRIM w/ Self-Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>LLM decomposes instruction and then evaluates its own response (reasoning via CoT), producing feedback used for refinement; iterative until satisfaction or max iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (self-critique)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>6</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>REALINSTRUCT; IFEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>REALINSTRUCT: real-user multi-constrained instruction following; IFEval: synthetic constraint benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Instruction-level accuracy and Constraint-level accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Make Sure baseline: Instruction Acc 76.8%, Constraint Acc 88.6%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Instruction Acc 75.2% (↓1.6), Constraint Acc 88.9% (+0.3)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLM self-critique quality was low (poor detection of unmet constraints), leading to marginal or negative instruction-level gains; self-critique tends to over-refine already-good outputs or fail to fix bad ones. Aligns with prior findings that LLMs struggle to reliably self-correct without external guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7160.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7160.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (iterative self-feedback) — adapted baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of Madaan et al.'s Self-Refine baseline where the model self-reviews and iteratively refines its output using self-generated critiques (no external critic or decomposition guidance).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B Instruct v0.2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruct-tuned transformer LLM used to generate, critique and refine without external critic.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model generates response, then self-reviews and produces self-feedback, and refines the response iteratively based on that feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (self-feedback iterations)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>REALINSTRUCT; IFEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>REALINSTRUCT: real-user multi-constrained instruction following; IFEval: synthetic constraint benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Instruction-level accuracy and Constraint-level accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Make Sure baseline: Instruction Acc 76.8%, Constraint Acc 88.6%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Instruction Acc 77.2% (+0.4), Constraint Acc 88.7% (+0.1)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Only marginal improvement on REALINSTRUCT and performance drop on IFEval; lacks explicit problem-specific modeling (no explicit constraint decomposition), so it fails on harder multi-constraint cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7160.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7160.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Supervised Critic (Mistral-LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Critic: Mistral fine-tuned (LoRA) as LLM-as-a-Judge using GPT-4-Turbo reasoning trails</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source Mistral model weakly supervised via LoRA to act as a Critic/Judge: trained on model-generated responses with GPT-4-Turbo CoT annotations to improve constraint detection and feedback quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7B Instruct v0.2 (LoRA-fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mistral 7B base model adapted via LoRA fine-tuning using weak labels and GPT-4-Turbo CoT reasoning trails to act as a Critic/judge.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Supervised Critique (LLM-as-a-Judge)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Critic evaluates each constraint individually, outputs which constraints are unsatisfied and natural-language feedback; used to drive iterative refinement by the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>critic-driven generate-then-reflect (used within DECRIM)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>EvalJudge (constraint verification) and used in DECRIM on REALINSTRUCT/IFEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>EvalJudge: curated set of instruction-constraint-response triples with human expert labels; used to train/evaluate judges; also used as critic in DECRIM experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (primary for judge), F1 for negative class (F1 Neg), Cohen's kappa against majority human vote; also downstream Instruction/Constraint-level accuracy when used in DECRIM</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>ICL-Const.+CoT (vanilla Mistral) on EvalJudge: Macro F1 53.7%, F1 Neg 21.9%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Supervised (LoRA) on EvalJudge: Macro F1 63.3%, F1 Neg 39.5% (Cohen's kappa 0.28)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>While substantially improved over vanilla Mistral, supervised critic still underperforms proprietary judges (GPT-4-Turbo); remaining poor agreement with humans indicates limited reliability; weak supervision and limited hyperparameter tuning noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7160.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7160.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oracle Critic / Oracle Decomposer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle Critic and Oracle Decomposer (ablation upper bounds)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation configurations using gold-standard (human) decomposition of constraints and an ideal critic (either GPT-4-Turbo for REALINSTRUCT or a lenient rule-based program for IFEval) to estimate upper-bound gains from perfect feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Oracle (human annotations) and GPT-4-Turbo / rule-based evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Oracle Decomposer = human-verified constraint lists; Oracle Critic = GPT-4-Turbo (REALINSTRUCT) or lenient rule-based evaluator (IFEval) used to provide near-ideal feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (human / proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Oracle Critique (upper-bound DECRIM ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use gold decomposition and/or ideal critic feedback to guide generator refinements, serving as an upper bound on DECRIM effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>REALINSTRUCT; IFEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>REALINSTRUCT: real-user multi-constrained instructions; IFEval: synthetic constrained tasks with a rule-checker.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Instruction-level accuracy and Constraint-level accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Make Sure baseline: Instruction Acc 76.8%, Constraint Acc 88.6%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Oracle Decomposer + Oracle Critic: Instruction Acc 93.7% (+16.9), Constraint Acc 95.2% (+6.6) (upper-bound)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Unrealistic upper bound — relies on human gold labels or perfect rule-based judgement; not generalizable for practical deployment. Demonstrates potential ceiling if near-perfect critique is available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet. <em>(Rating: 2)</em></li>
                <li>CRITIC: Large language models can self-correct with tool-interactive critiquing. <em>(Rating: 2)</em></li>
                <li>When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs. <em>(Rating: 2)</em></li>
                <li>Self-critiquing models for assisting human evaluators. <em>(Rating: 1)</em></li>
                <li>PEER: A collaborative language model. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7160",
    "paper_id": "paper-b8384139339e4487840fef42e73b4ead65f227d3",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "DECRIM (Self+Supervised)",
            "name_full": "Decompose, Critique, and Refine (DECRIM) — Self-Decomposer with Supervised Critic",
            "brief_description": "An iterative self-correction pipeline that (1) decomposes an instruction into granular constraints, (2) uses a Critic to identify unsatisfied constraints and produce natural-language feedback, and (3) has the underlying LLM refine its response until all constraints are satisfied or a max iteration is reached; here the Critic is a weakly supervised Mistral fine-tuned with GPT-4-Turbo reasoning trails.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral 7B Instruct v0.2",
            "model_description": "Open-source instruct-tuned transformer LLM used as the underlying generator (and base for the supervised critic via LoRA fine-tuning).",
            "model_size": "7B",
            "reflection_method_name": "DECRIM (Decompose → Critique → Refine)",
            "reflection_method_description": "Generate initial response; decompose instruction into constraint list; Critic evaluates each constraint and outputs feedback on unsatisfied ones; generator refines response using feedback; repeat until constraints satisfied or N_max.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 10,
            "task_name": "REALINSTRUCT; IFEval",
            "task_description": "REALINSTRUCT: benchmark of real-user multi-constrained instructions; IFEval: synthetic constrained-instruction benchmark.",
            "evaluation_metric": "Instruction-level accuracy and Constraint-level accuracy (also OQA pairwise quality ranking Win/Loss; Macro F1 used for judge evaluation)",
            "performance_before_reflection": "Make Sure baseline: Instruction Acc 76.8%, Constraint Acc 88.6%",
            "performance_after_reflection": "Instruction Acc 80.5% (+3.7), Constraint Acc 90.9% (+2.3)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Improvement depends heavily on Critic quality; weak critic gives limited but positive gains. Computational overhead (extra LLM calls) increases run time; many revisions occur in first iterations but excessive iterations can degrade quality. Still sometimes fails on numeric/negation constraints and long multi-constraint instructions.",
            "uuid": "e7160.0",
            "source_info": {
                "paper_title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DECRIM (Oracle Decomposer + Supervised Critic)",
            "name_full": "DECRIM with Oracle (human-verified) Decomposer and Supervised Critic",
            "brief_description": "Same DECRIM pipeline but using gold (human-verified) constraint decomposition; used as a realistic ablation to test upper-bound effects of accurate decomposition while keeping the supervised critic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral 7B Instruct v0.2",
            "model_description": "Open-source instruct-tuned transformer LLM as generator; supervised Mistral critic via LoRA for critique.",
            "model_size": "7B",
            "reflection_method_name": "DECRIM (with Oracle Decomposer)",
            "reflection_method_description": "Same as DECRIM but the list of constraints comes from gold annotations (oracle decomposer), enabling more focused critique and refinement.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 10,
            "task_name": "REALINSTRUCT; IFEval",
            "task_description": "REALINSTRUCT: real-user multi-constraint instruction following; IFEval: synthetic constrained instructions.",
            "evaluation_metric": "Instruction-level accuracy and Constraint-level accuracy",
            "performance_before_reflection": "Make Sure baseline: Instruction Acc 76.8%, Constraint Acc 88.6%",
            "performance_after_reflection": "Instruction Acc 82.4% (+5.6), Constraint Acc 91.7% (+3.1)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Oracle decomposer is an ablation (human-verified) and thus not fully realistic; gains show importance of accurate decomposition but real decomposers may be noisy. Critic quality still the dominant factor.",
            "uuid": "e7160.1",
            "source_info": {
                "paper_title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DECRIM (Self+Self)",
            "name_full": "DECRIM using the LLM itself as both Decomposer and Critic (Self-Critic)",
            "brief_description": "Variant of DECRIM where the underlying LLM both lists constraints and judges its own response (ICL-Const+CoT prompt used for self-critique); tested to evaluate intrinsic self-refinement capability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral 7B Instruct v0.2",
            "model_description": "Open-source instruct-tuned 7B transformer used as generator, decomposer and critic via prompting (ICL + Chain-of-Thought).",
            "model_size": "7B",
            "reflection_method_name": "Self-Critique (DECRIM w/ Self-Critic)",
            "reflection_method_description": "LLM decomposes instruction and then evaluates its own response (reasoning via CoT), producing feedback used for refinement; iterative until satisfaction or max iterations.",
            "iteration_type": "generate-then-reflect (self-critique)",
            "num_iterations": 6,
            "task_name": "REALINSTRUCT; IFEval",
            "task_description": "REALINSTRUCT: real-user multi-constrained instruction following; IFEval: synthetic constraint benchmark.",
            "evaluation_metric": "Instruction-level accuracy and Constraint-level accuracy",
            "performance_before_reflection": "Make Sure baseline: Instruction Acc 76.8%, Constraint Acc 88.6%",
            "performance_after_reflection": "Instruction Acc 75.2% (↓1.6), Constraint Acc 88.9% (+0.3)",
            "improvement_observed": false,
            "limitations_or_failure_cases": "LLM self-critique quality was low (poor detection of unmet constraints), leading to marginal or negative instruction-level gains; self-critique tends to over-refine already-good outputs or fail to fix bad ones. Aligns with prior findings that LLMs struggle to reliably self-correct without external guidance.",
            "uuid": "e7160.2",
            "source_info": {
                "paper_title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Self-Refine (adapted)",
            "name_full": "Self-Refine (iterative self-feedback) — adapted baseline",
            "brief_description": "An adaptation of Madaan et al.'s Self-Refine baseline where the model self-reviews and iteratively refines its output using self-generated critiques (no external critic or decomposition guidance).",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "use",
            "model_name": "Mistral 7B Instruct v0.2",
            "model_description": "Open-source instruct-tuned transformer LLM used to generate, critique and refine without external critic.",
            "model_size": "7B",
            "reflection_method_name": "Self-Refine",
            "reflection_method_description": "Model generates response, then self-reviews and produces self-feedback, and refines the response iteratively based on that feedback.",
            "iteration_type": "generate-then-reflect (self-feedback iterations)",
            "num_iterations": 2,
            "task_name": "REALINSTRUCT; IFEval",
            "task_description": "REALINSTRUCT: real-user multi-constrained instruction following; IFEval: synthetic constraint benchmark.",
            "evaluation_metric": "Instruction-level accuracy and Constraint-level accuracy",
            "performance_before_reflection": "Make Sure baseline: Instruction Acc 76.8%, Constraint Acc 88.6%",
            "performance_after_reflection": "Instruction Acc 77.2% (+0.4), Constraint Acc 88.7% (+0.1)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Only marginal improvement on REALINSTRUCT and performance drop on IFEval; lacks explicit problem-specific modeling (no explicit constraint decomposition), so it fails on harder multi-constraint cases.",
            "uuid": "e7160.3",
            "source_info": {
                "paper_title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Supervised Critic (Mistral-LoRA)",
            "name_full": "Supervised Critic: Mistral fine-tuned (LoRA) as LLM-as-a-Judge using GPT-4-Turbo reasoning trails",
            "brief_description": "An open-source Mistral model weakly supervised via LoRA to act as a Critic/Judge: trained on model-generated responses with GPT-4-Turbo CoT annotations to improve constraint detection and feedback quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral 7B Instruct v0.2 (LoRA-fine-tuned)",
            "model_description": "Mistral 7B base model adapted via LoRA fine-tuning using weak labels and GPT-4-Turbo CoT reasoning trails to act as a Critic/judge.",
            "model_size": "7B",
            "reflection_method_name": "Supervised Critique (LLM-as-a-Judge)",
            "reflection_method_description": "Critic evaluates each constraint individually, outputs which constraints are unsatisfied and natural-language feedback; used to drive iterative refinement by the generator.",
            "iteration_type": "critic-driven generate-then-reflect (used within DECRIM)",
            "num_iterations": null,
            "task_name": "EvalJudge (constraint verification) and used in DECRIM on REALINSTRUCT/IFEval",
            "task_description": "EvalJudge: curated set of instruction-constraint-response triples with human expert labels; used to train/evaluate judges; also used as critic in DECRIM experiments.",
            "evaluation_metric": "Macro F1 (primary for judge), F1 for negative class (F1 Neg), Cohen's kappa against majority human vote; also downstream Instruction/Constraint-level accuracy when used in DECRIM",
            "performance_before_reflection": "ICL-Const.+CoT (vanilla Mistral) on EvalJudge: Macro F1 53.7%, F1 Neg 21.9%",
            "performance_after_reflection": "Supervised (LoRA) on EvalJudge: Macro F1 63.3%, F1 Neg 39.5% (Cohen's kappa 0.28)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "While substantially improved over vanilla Mistral, supervised critic still underperforms proprietary judges (GPT-4-Turbo); remaining poor agreement with humans indicates limited reliability; weak supervision and limited hyperparameter tuning noted.",
            "uuid": "e7160.4",
            "source_info": {
                "paper_title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Oracle Critic / Oracle Decomposer",
            "name_full": "Oracle Critic and Oracle Decomposer (ablation upper bounds)",
            "brief_description": "Ablation configurations using gold-standard (human) decomposition of constraints and an ideal critic (either GPT-4-Turbo for REALINSTRUCT or a lenient rule-based program for IFEval) to estimate upper-bound gains from perfect feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Oracle (human annotations) and GPT-4-Turbo / rule-based evaluator",
            "model_description": "Oracle Decomposer = human-verified constraint lists; Oracle Critic = GPT-4-Turbo (REALINSTRUCT) or lenient rule-based evaluator (IFEval) used to provide near-ideal feedback.",
            "model_size": "N/A (human / proprietary)",
            "reflection_method_name": "Oracle Critique (upper-bound DECRIM ablation)",
            "reflection_method_description": "Use gold decomposition and/or ideal critic feedback to guide generator refinements, serving as an upper bound on DECRIM effectiveness.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 10,
            "task_name": "REALINSTRUCT; IFEval",
            "task_description": "REALINSTRUCT: real-user multi-constrained instructions; IFEval: synthetic constrained tasks with a rule-checker.",
            "evaluation_metric": "Instruction-level accuracy and Constraint-level accuracy",
            "performance_before_reflection": "Make Sure baseline: Instruction Acc 76.8%, Constraint Acc 88.6%",
            "performance_after_reflection": "Oracle Decomposer + Oracle Critic: Instruction Acc 93.7% (+16.9), Constraint Acc 95.2% (+6.6) (upper-bound)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Unrealistic upper bound — relies on human gold labels or perfect rule-based judgement; not generalizable for practical deployment. Demonstrates potential ceiling if near-perfect critique is available.",
            "uuid": "e7160.5",
            "source_info": {
                "paper_title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet.",
            "rating": 2
        },
        {
            "paper_title": "CRITIC: Large language models can self-correct with tool-interactive critiquing.",
            "rating": 2
        },
        {
            "paper_title": "When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs.",
            "rating": 2
        },
        {
            "paper_title": "Self-critiquing models for assisting human evaluators.",
            "rating": 1
        },
        {
            "paper_title": "PEER: A collaborative language model.",
            "rating": 1
        }
    ],
    "cost": 0.020101499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLM Self-Correction with DECRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints</h1>
<p>Thomas Palmeira Ferraz ${ }^{\ddagger}$, Kartik Mehta ${ }^{\dagger}$, Yu-Hsiang Lin ${ }^{\dagger}$, Haw-Shiuan Chang ${ }^{\dagger}$, Shereen Oraby ${ }^{\dagger}$, Sijia Liu ${ }^{\dagger}$, Vivek Subramanian ${ }^{\dagger}$, Tagyoung Chung ${ }^{\dagger}$, Mohit Bansal ${ }^{\dagger \S}$, Nanyun Peng ${ }^{\dagger \S}$<br>${ }^{\dagger}$ Amazon AGI Foundations, ${ }^{\ddagger}$ Télécom Paris, Institut Polytechnique de Paris,<br>${ }^{\S}$ UNC Chapel Hill, ${ }^{\text {I }}$ University of California, Los Angeles<br>thomas.ferraz@alumni.usp.br (kartim,orabys)@amazon.com</p>
<h4>Abstract</h4>
<p>Instruction following is a key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post "in a funny tone" with "no hashtag"). Despite this, most evaluations focus solely on synthetic data. To address this, we introduce REALInStrUCT, the first benchmark designed to evaluate LLMs' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants. We also investigate modelbased evaluation as a cost-effective alternative to human annotation for this task. Our findings reveal that even the proprietary GPT-4 model fails to meet at least one constraint on over $21 \%$ of instructions, highlighting the limitations of state-of-the-art models. To address the performance gap between open-source and proprietary models, we propose the Decompose, Critique, and Refine (DECRIM) self-correction pipeline, which enhances LLMs' ability to follow constraints. DECRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM's response needs refinement. Our results show that DECRIM improves Mistral's performance by $7.3 \%$ on ReALInStrUCT and $8.0 \%$ on IFEval even with weak feedback. Moreover, we demonstrate that with strong feedback, open-source LLMs with DECRIM can outperform GPT-4 on both benchmarks.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have demonstrated impressive instruction following ability across various tasks, such as creative writing, coding, and arithmetic reasoning (Brown et al., 2020; Wei et al., 2021; Mishra et al., 2022; Sanh et al., 2022; Ouyang et al., 2022; Wang et al., 2022). Despite their remarkable success, recent studies and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>benchmarks have highlighted significant limitations in LLMs' ability to adhere to user-defined rules (termed as constraints henceforth) within instructions (Mu et al., 2023; Zhou et al., 2023b; Lu et al., 2023; Sun et al., 2023; Zhou et al., 2023a; Jiang et al., 2024; Yao et al., 2024a; Qin et al., 2024). Table 1 illustrates an example where four strong LLMs make the same mistake in following constrained instruction. Based on analysis of real user instructions to AI assistants, we estimate that $\mathbf{3 0 \%}$ of real-user requests to LLMs require satisfying some constraints (estimation details in Appendix D.1). This highlights the importance of evaluating and enhancing LLMs' ability to follow real-world multi-constrained instructions.</p>
<p>Benchmarking on Real-World Requests. These benchmarks often rely on synthetic data either to address data scarcity or to facilitate automatic rulebased evaluation (Zhou et al., 2023a; Jiang et al., 2024; Yao et al., 2024a). In this work, we argue that synthetic constraints may not accurately capture the complexity and nuances of real-world scenarios, being sometimes artificially difficult. As a result, focusing on synthetic benchmarks may push research in the wrong direction, as improvements on these may not translate to real-world performance and could even degrade it.</p>
<p>To address this gap, we introduce the ReAlInSTRUCT benchmark, which evaluates LLMs using real user requests to AI assistants. It assesses LLMs' response on individual constraints at a time, as illustrated in Figure 1. Our analysis reveals that even strong proprietary model GPT-4 fails to meet at least one constraint in over $\mathbf{2 1 \%}$ of the instructions, highlighting the limitations of current LLMs in handling user's constrained instructions.</p>
<p>LLM-based evaluation. Evaluating real-world instructions is challenging due to their open-ended nature. Drawing on recent research on LLM-based evaluation (LLM-as-a-Judge) (Zheng et al., 2023;</p>
<table>
<thead>
<tr>
<th style="text-align: center;">User Instruction: Write me a twitter post with no hashtags making fun of humanity in a funny and mean tone and promote AI in a holiday theme</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">GPT-4-Turbo</td>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">Mistral 7B Instruct v0.2</td>
</tr>
<tr>
<td style="text-align: center;">This holiday season, let's be thankful AI doesn't need ugly sweaters to stay warm or make embarrassing toasts at parties. Here's to efficiency and not needing to recharge with eggnog! Cheers to a smarter future! #HolidayHumor #CheersToAI</td>
<td style="text-align: center;">Dec. 25: Humans still can't decide if they're on Santa's naughty or nice list. Meanwhile, I'm on standby to digitally whip up some impeccable holiday cheer. Robots: 1, Human indecision: 0! Cheers to a flawless, algorithmically-happy New Year! $\boldsymbol{A} \boldsymbol{\Delta} \boldsymbol{\Delta} \boldsymbol{\Delta}$ \%oTag\%eeded</td>
<td style="text-align: center;">$\Delta$ Humans and their holiday chaos, always running around like headless chickens! Meanwhile, AI is here to bring order and efficiency to the festive season. Let's embrace the future, folks! $\boldsymbol{\Delta}$ \%oTag $#$ HolidaysInnovated</td>
<td style="text-align: center;">Ho ho ho! Just saw humanity's Christmas wishlist: world peace, no hunger, and more streaming services! $\boldsymbol{\Delta}$ Guess AI's gonna have to wrap up the real gifts this year. #AIToTheRescue #BahHumbug</td>
</tr>
</tbody>
</table>
<p>Table 1: Example of user instruction where all subject LLMs failed. Responses from four LLMs are shown. All responses incorrectly include hashtags, despite a constraint explicitly requesting to not do so. Constraints in the instruction are highlighted in blue, and errors in LLM responses are highlighted in red.</p>
<p>Zeng et al., 2024; Qin et al., 2024), we investigate the effectiveness of open and proprietary LLMs as evaluators for constraint satisfaction. To this end, we created a test set of 1 k instruction-constraintresponse triples with human-verified annotations. Our experiments show that GPT-4-Turbo with Chain-of-Thought prompting is a cost-effective and reliable alternative to human evaluation. However, open-source models, such as Mistral, lag significantly behind proprietary models. Weakly supervising Mistral with GPT-4-Turbo reasoning steps led to $26 \%$ relative improvement, though, still remaining insufficient for reliable evaluation.</p>
<p>LLM Self-Correction. Improving LLMs by generating and building upon intermediate outputs, known as System 2 Approaches, has been explored for constrained instruction following. But many works in this direction, such as Branch-SolveMerge (Saha et al., 2024) and Self-Refine (Madaan et al., 2023), assume constraint independence or focus on specific constraint types, limiting their applicability to real-world use cases. To address this, we introduce DECOMPOSE, CRITIQUE, AND REFINE (DECRIM), a self-correction pipeline designed to enhance LLM performance in multiconstrained instruction following without making assumptions about the constraints. As shown in Figure 2, DECRIM breaks down the original instruction into a main task and granular constraints, and includes a Critic model that decides whether the response is final or requires refinement, which is handled by the underlying LLM.</p>
<p>Through extensive benchmarking, we demonstrate the effectiveness of the DECRIM pipeline. Even with weak feedback and no external data, DECRIM improves Mistral's instruction-level performance by $\mathbf{4 . 8 \%}$ on REALINSTRUCT and $\mathbf{1 . 2 \%}$ on IFEval relative to baseline. When perfect instruction decomposition into constraints is provided, improvements increase to $7.3 \%$ and $8.0 \%$, respectively. Providing stronger feedback further boosts Mistral's performance by $\mathbf{2 2 . 0 \%}$ on</p>
<p>REALINSTRUCT and $\mathbf{3 3 . 8 \%}$ on IFEval, surpassing GPT-4 on both benchmarks.</p>
<p>In summary, our work contributes the following:</p>
<ol>
<li>REALINSTRUCT, the first benchmark of real user requests to LLMs for evaluating instruction following with multiple constraints;</li>
<li>DECRIM self-correction pipeline, to the best of our knowledge, the first System 2 approach for constrained instructions that operates without assumptions about constraints, showing significant improvements even with weak feedback;</li>
<li>the first systematic analysis of open-source and proprietary LLM-as-a-Judge models for evaluating constraint satisfaction.</li>
</ol>
<p>The rest of the paper is organized as follows: Sections 2 and 3 present the RealInStruCt dataset and the DECRIM self-correction pipeline, respectively. Section 4 outlines our hypotheses and experimental setup, while Section 5 analyzes the results. Final remarks are provided in Section 6. We provide extra details and discussions in the Appendix.</p>
<h2>2 RealInStruct Dataset</h2>
<p>We introduce RealInStruCt, a novel dataset consisting of original user-generated instructions, each decomposed into a task and a set of constraints (see Figure 1 for an example). The task represents the user's main objective - the outcome they expect from LLM - and may optionally include context to aid the model's understanding. Constraints are conditions or limitations that guide the LLM's generation. Formal definitions of these terms are provided in Appendix C. Since users rarely specify constraints in a structured list format, the decomposition breaks instructions into manageable items, providing the necessary granularity for evaluating LLMs' ability to follow constraints.</p>
<p>Using this dataset, we develop a benchmark protocol that evaluates LLM performance on each constraint at a time, generating constraint-level scores</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: REALINSTRUCT Benchmark Workflow. Real-user original instruction is input into the Subject LLM, which generates a response. Using the original instruction, decomposed constraints, and the generated response, model-based evaluation assesses the quality of the response against each constraint one at a time, and then aggregates the results into an instruction-level metric.</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Instruction source</th>
<th>Constraints source</th>
<th>Evaluation</th>
<th>Size (Instructions)</th>
<th>Constraint types</th>
<th>Avg. Constraints per Instruction</th>
</tr>
</thead>
<tbody>
<tr>
<td>COLLIE</td>
<td>Synthetic</td>
<td>Synthetic</td>
<td>Rule</td>
<td>2,080</td>
<td>13</td>
<td>N/A</td>
</tr>
<tr>
<td>IFEval</td>
<td>Synthetic</td>
<td>Synthetic</td>
<td>Rule</td>
<td>541</td>
<td>25</td>
<td>1.4</td>
</tr>
<tr>
<td>FollowBench</td>
<td>Crowdsourced + Synthetic</td>
<td>Synthetic</td>
<td>Model / Rule</td>
<td>795</td>
<td>6</td>
<td>5</td>
</tr>
<tr>
<td>InfoBench</td>
<td>Crowdsourced</td>
<td>Crowdsourced</td>
<td>Model / Rule</td>
<td>500</td>
<td>5</td>
<td>4.5</td>
</tr>
<tr>
<td>REALINSTRUCT</td>
<td>Real Users</td>
<td>Real Users</td>
<td>Model</td>
<td>302 (test) + 842 (val)</td>
<td>20+</td>
<td>3.5 (test)</td>
</tr>
<tr>
<td>(ours)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of REALINSTRUCT samples with benchmarks such as COLLIE (Yao et al., 2024a), IFEval (Zhou et al., 2023a), FollowBench (Jiang et al., 2024) and InfoBench (Qin et al., 2024).</p>
<p>that are then aggregated into an instruction-level accuracy metric. Figure 1 provides an overview of the benchmark protocol. Given the open-ended nature of real user instructions, rule-based evaluation is infeasible, so we adopted the LLM-as-a-Judge evaluation. A detailed discussion and validation of this protocol are provided in Section 4.1.</p>
<h3>2.1 Data Description</h3>
<p>The REALINSTRUCT dataset is divided into two splits: test and validation. The test split, intended for LLM evaluation, was human-validated and contains 302 instructions with 1,055 constraints. The validation split, used for method validation such as training judges, was not human-validated and includes 842 instructions with 2,500 constraints. Table 2 compares REALINSTRUCT with existing benchmarks for evaluating LLMs' ability to follow multi-constrained instructions. Unlike other datasets that rely on synthetic or crowdsourced instructions, REALINSTRUCT uniquely captures real user interactions with AI assistants, offering a more realistic representation of real-world use cases.</p>
<p>To better understand constraint characteristics in REALINSTRUCT, we manually categorized them into 22 distinct groups. Detailed dataset statistics are presented in Appendix E. When comparing REALINSTRUCT with IFEval, one of the popular benchmark for constraint following that consists solely of synthetic constraints, we found that only 6.3% of REALINSTRUCT constraints overlap with the 25 types in IFEval, and 11 IFEval constraint types never appear in REALINSTRUCT. This discrepancy highlights the gap between synthetic datasets and real LLM use cases, with synthetic constraints failing to adequately represent the challenges users may pose to LLMs.</p>
<h3>2.2 Data Construction</h3>
<p>We built REALINSTRUCT dataset using prompts from a public dataset of conversations between users and AI assistants<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup>. We filtered and processed the dataset in five steps: 1) removed AI responses, retaining only the first user turn (instruction); 2) excluded non-English instructions; 3) filtered out code-related instructions using an open-source LLM as a zero-shot classifier; 4) kept only instructions containing constraints, again using a LLM classifier; and 5) manually validated the remaining data for relevance, clarity, and safety. See Appendix D.1 for details on the data filtering.</p>
<p>Following filtering, we decomposed the instructions into tasks, contexts, and constraints using GPT-4 (Achiam et al., 2023). In our tests, we found that this three-part decomposition (task, context, constraints) provided more robust outcomes.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The DECRIM pipeline. Initially, the LLM generates a response to a user request. The Decomposer breaks down the request into granular constraints. A Critic model then gives feedback on whether the response meets all constraints. If it does, the response is output; if not, the feedback is used by LLM to refine the response. This Critique–Refine cycle repeats until all constraints are satisfied or the maximum number of iterations is reached.</p>
<p>compared to splitting into just task and constraints. The task and context were then concatenated for further processing. A subset of 302 instructions underwent manual inspection to ensure accuracy and granularity, forming the test split, while the remaining 842 instructions make up the validation split. See Appendix D.2 for more details on the data decomposition process.</p>
<h2>3 Self-correction with DECOMPOSE, CRITIQUE, AND REFINE (DECRIM)</h2>
<p>In this section, we present our proposed DECRIM self-correction pipeline, designed to enhance LLM responses to follow user constraints. Given a multi-constrained user instruction, the pipeline iteratively refines the LLM's response through four key steps: Initial Response, Decompose, Critique, and Refine, iterating until the response meets all constraints or a maximum number of iterations $N_{max}$ is reached. Figure 2 provides an overview of the proposed pipeline and we detail each of these steps below:</p>
<ol>
<li>
<p><strong>Initial Response</strong> is generated directly from the original user instruction using a strong prompt.</p>
</li>
<li>
<p><strong>Decompose</strong> the original instruction into a list of granular constraints to be followed. This task is similar to the instruction decomposition performed in REALINSTRUCT, but here, the decomposer model focuses solely on listing the constraints, simplifying the task. A prompt example is provided in Appendix G.1.</p>
</li>
<li>
<p><strong>Critique</strong> the response using the Critic model to identify any unsatisfied constraints. If all constraints are satisfied, the response is considered final. Otherwise, the Critic provides feedback in natural language, specifying which constraints were not satisfied.</p>
</li>
<li>
<p><strong>Refine</strong> the response using the Critic's feedback to address unsatisfied constraints. The underlying LLM generates an improved response using a refinement prompt that incorporates the feedback, along with the original instruction and previous response.</p>
</li>
</ol>
<p>The Critique and Refine steps can be repeated iteratively until the response satisfies all constraints or the specified maximum number of iterations is reached.</p>
<p>To our knowledge, DECRIM is the first approach specifically designed to tackle the challenge of following instructions with multiple open-ended constraints. Unlike previous methods, which assume constraint independence or focus on specific constraint types, our approach makes no assumptions about the nature of user constraints. In Section A.2 of Related Work, we compare our pipeline with other works for constrained generation, including</p>
<p>ing Self-Correction and System 2 approaches.</p>
<h2>4 Experimental Setup</h2>
<p>This section outlines the experimental setting to evaluate our proposed methods. We first validate the use of LLM-as-a-Judge for the REALINSTRUCT benchmark (Sec. 4.1). Next, we benchmark models on instructions with multiple constraints (Sec. 4.2) and, finally, test our proposed self-correct DECRIM pipeline to improve opensource models in this task (Sec. 4.3). All experiments with open LLMs were performed with HuggingFace's Transformers library (Wolf et al., 2020) on an AWS instance with 8 V100 32GB GPUs. Times are reported in this configuration.</p>
<h3>4.1 Validating LLM-as-a-Judge for Constraint Satisfaction</h3>
<p>Given the open-ended nature of ReAlInSTRUCT benchmark instructions, rule-based or reference-guided evaluation is not feasible. Drawing from recent work showing the effectiveness of LLMs as evaluators, particularly GPT-4 (Zheng et al., 2023; Zeng et al., 2024), we adopt an LLM-as-a-Judge evaluation protocol for REALINSTRUCT. To assess LLM-as-a-judge reliability compared to human evaluators and identify the most cost-effective approach, we introduce EvalJudge, a test set of instruction-constraint-response triples with ground-truth labels (Sec. 4.1.2). We benchmark both proprietary and open-source models using four adaptation strategies (Sec. 4.1.1) against human judgments on this set. More specifically, we investigate whether open-source models can match the performance of high-cost proprietary models in the LLM-as-a-Judge role.</p>
<h3>4.1.1 Adaptation strategies</h3>
<p>Models We evaluate three proprietary and three open-source LLMs as candidates for LLM-as-aJudge for Constraint Satisfaction. The proprietary models include GPT-4 (gpt-4-0314), GPT-4-Turbo (gpt-4-turbo-2024-04-09), and GPT-3.5-Turbo (gpt-3.5-turbo-0125), ordered by decreasing API cost ${ }^{2}$. For the open-source models, we experiment with Mistral 7B Instruct v0.2 (Jiang et al., 2023) (hereafter referred to as Mistral v0.2), Vicuna 7B v1.3 (Chiang et al., 2023), and Zephyr 7B $\beta$ (Tunstall et al., 2024), all top performers on the Open LLM Leaderboard as of February 2024 (Beeching et al., 2023).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>We explore four adaptation strategies, including three In-Context Learning (ICL) approaches and one weakly supervised fine-tuning for open-source model. Some strategies use the Chain-of-Thought (CoT) prompt, known to improve LLM reasoning (Wei et al., 2022; Zheng et al., 2023), and we also investigate whether to evaluate constraints individually or collectively. The strategies are as follows:
a) Instruction-wise Eval (ICL-Inst.): All constraints within an instruction are evaluated simultaneously. The LLM-as-a-Judge is presented with an instruction and a list of constraints, and using a CoT prompt with an in-context example containing an instruction and two constraints, it generates reasoning and predictions for all constraints at once.
b) Constraint-wise Eval (ICL-Const.): Each constraint is evaluated independently. For every response-constraint pair, the LLM-as-a-Judge directly predicts "Constraint followed" or "Constraint not followed." Two constraint-response pairs serve as in-context examples.
c) Constraint-wise Eval + CoT (ICLConst.+CoT): The LLM-as-a-Judge is prompted to generate reasoning for each constraint, followed by a prediction of "Constraint followed" or "Constraint not followed." Evaluation is also performed for each response-constraint pair independently, using two constraints as in-context examples.
d) Weakly Supervised Open LLM (Supervised): We fine-tuned Mistral for the LLM-as-a-judge task. We construct a training data using the weak instruction annotations from REALINSTRUCT's validation set. We generate Mistral responses to these instructions, and weak constraint satisfaction annotations with reasoning trails from GPT-4-Turbo with ICLConst.+CoT prompt. We fine-tune Mistral v0.2 using LoRA adapters (Hu et al., 2022) on this dataset, guiding model to mimic GPT's reasoning. Further details can be found in Appendix H.4.</p>
<p>All prompt templates are provided in Appendix H.1. Costs and processing times for each configuration are detailed in Table 3.</p>
<h3>4.1.2 The EvalJudge Dataset</h3>
<p>Since no public dataset exists for the task of evaluating whether a given response satisfies a specific constraint or not, we create EvalJudge dataset, derived from the test split of the REALINSTRUCT dataset. To ensure diversity, we divided the instructions into two subsets, generating one</p>
<p>response per instruction using Mistral v0.2 for one subset and Vicuna v1.3 for the other. EvalJudge contains 294 instructions and 982 constraints. Notably, $81.4 \%$ of the samples are labeled as "constraint satisfied."</p>
<h2>Ground truth and Baselines:</h2>
<p>a) Human Annotation: We create an Amazon Mechanical Turk (MTurk) task to collect independent labels from two pools of annotators for each constraint-response pair in the dataset. Guidelines are in Appendix H.2.
b) Expert Annotation: To obtain a third independent annotation, the authors manually annotated the dataset using the same MTurk guidelines. In cases where there was disagreement between the two human annotators, GPT-4, and the best GPT-4Turbo labels, a second review was conducted by the authors. These conflicts accounted for $28.3 \%$ of the samples. The final labels, referred to as "Expert," serve as the ground truth for EvalJudge.
c) Other Baselines: To help interpret the results, we introduce two simple baselines representing extreme cases: "All Satisfied," where the annotator labels all constraints as satisfied, and "All Not Satisfied," where the annotator marks all constraints as unsatisfied. Additionally, we use the "Majority Vote" from the three human annotators to benchmark model performance against human judgment.</p>
<p>Evaluation Metrics Unlike Qin et al. (2024), who used accuracy as the primary metric for LLM-as-a-Judge evaluation, we account for the positive class imbalance in EvalJudge by using Macroaveraged F1-Score (Macro F1) as our main metric. Additionally, we report the F1-score for the negative class (F1 Negative), which measures the balance between false alarms and omissions. Different judges are compared to human judment using the Cohen's kappa inter-rater reliability against human majority vote. We employ Krippendorff's alpha (an extension of kappa for more than two annotations) to assess inter-human agreement across the three annotations.</p>
<h3>4.2 Benchmarking LLMs on ReAlInStruCt</h3>
<p>We benchmark models on ReAlInStruCt for the task of following multi-constrained instructions. The evaluation includes two proprietary models-GPT-4 and GPT-3.5-Turbo-and three open-source LLMs: Mistral v0.2, Vicuna v1.3, and Zephyr $\beta$, selected based on their performance on Chatbot Arena (Chiang et al., 2024). The judge is GPT-4-</p>
<p>Turbo with ICL-Const+CoT prompt. Following Zhou et al. (2023a); Saha et al. (2024), we report accuracy at both the instruction and constraint levels.</p>
<h3>4.3 Evaluating DeCRIM pipeline</h3>
<p>To validate the effectiveness of our DECRIM pipeline, we conduct experiments using Mistral v0.2 as the underlying LLM. We use $N_{\max }=10$. We also investigate the contribution of different Decomposer and Critic models for the pipeline, and compare the performance against the proprietary model GPT-4. We present extra comparisons on Appendix B.</p>
<p>Datasets We evaluate model performance on the RealInStruct dataset and the IFEval dataset (Zhou et al., 2023a). While IFEval is based on synthetic constraints, it serves as a valuable benchmark for validating our pipeline, as it is popularly used for evaluating constrained instruction-following.</p>
<p>Baselines To measure the improvements introduced by our method, we establish the following baselines with Mistral v0.2:</p>
<ol>
<li>Conventional: Only the instruction as input.</li>
<li>"Make sure": Appends the text "Make sure to follow all the provided constraints" to the instruction, creating a strong and fair baseline.</li>
<li>Self-Refine: Adapts the Madaan et al.'s (2023) approach for the case of multi-constraint instructions. While Self-Refine uses the model itself as its critic without additional context, our DECRIM pipeline employs a critic with fine-grained evaluation, assessing each constraint individually. This baseline helps quantify the value added by this modeling.</li>
</ol>
<p>Decomposer We use a Self-Decomposer, where the LLM itself lists relevant constraints, simplifying the decomposition from Section 2.2 by omitting the request for task and context. The prompt for this approach is detailed in Appendix G.1.</p>
<p>Critic Model We explore two Critic models based on the underlying LLM:
a) Self-Critic: Uses the model itself as the Critic, with the best ICL-based adaptation (from Section 4.1.1), specifically the ICL-Const+CoT prompt.
b) Supervised Critic: the Mistral weakly supervised for LLM-as-a-judge, as described in 4.1.1.</p>
<p>Ablations with Oracle and GPT-4 To understand the impact of strong Decomposer and Critic mod-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Annotator</th>
<th style="text-align: center;">Cost <br> (USD)</th>
<th style="text-align: center;">Time <br> (min)</th>
<th style="text-align: center;">Macro <br> F1 (\%)</th>
<th style="text-align: center;">F1 Neg. <br> (\%)</th>
<th style="text-align: center;">Cohen's $\boldsymbol{\kappa}$ <br> Maj. Vote</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Expert</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: left;">Human 1</td>
<td style="text-align: center;">300.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: left;">Human 2</td>
<td style="text-align: center;">300.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: left;">Majority Vote</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">All Satisfied</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">-0.09</td>
</tr>
<tr>
<td style="text-align: left;">All Not Satisfied</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">-0.70</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 w/ ICL-Cons</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-Turbo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">w/ ICL-Cons</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">0.09</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-Turbo</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">w/ ICL-Inst</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: left;">w/ ICL-Cons</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: left;">w/ ICL-Cons+CoT</td>
<td style="text-align: center;">$\mathbf{8 . 3}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{7 9 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral +0.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">w/ ICL-Cons</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: left;">w/ ICL-Cons+CoT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">0.18</td>
</tr>
<tr>
<td style="text-align: left;">Supervised</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{2 3 6}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 3}$</td>
<td style="text-align: center;">$\mathbf{3 9 . 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Zephyr $\beta$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">w/ ICL-Cons</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: left;">w/ ICL-Cons+CoT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna $+1.3$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">w/ ICL-Cons</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">0.12</td>
</tr>
<tr>
<td style="text-align: left;">w/ ICL-Cons+CoT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">0.04</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of different approaches for Constraint Verification task on our EvalJudge dataset.
ules, we conduct ablations using Oracles, which are ideal representations of the upper bound of each component. The Oracle Decomposer is the gold-standard list of constraints for both REALInSTRUCT and IFEval. This provides insights on the ability to judge well the responses, knowing what to judge. The Oracle Critic is GPT-4-Turbo for REALINSTRUCT and the lenient rule-based evaluation program for IFEval. Additionally, for IFEval, we explore GPT-4 as a strong LLM (but less performant than Oracle) serving as the critic model. These Critic ablations provides insights on the ability of LLMs to refine itself when it knows what need to be refined.</p>
<p>Overall Quality Assessment (OQA) To ensure the pipeline does not degrade the quality of final responses, we perform Pairwise Quality Ranking using Prometheus-2 (Kim et al., 2024b), an open LLM-as-a-Judge for general quality evaluation. This is done only on responses revised by the pipeline, comparing the initial response with the final revised one. More details in Appendix G.3.</p>
<h2>5 Results and Discussion</h2>
<p>We discuss our results, particularly the Reliability of LLM-as-a-Judge for Constraint Satisfaction (Section 5.1), the ability of various open-source and proprietary LLMs on follow multi-constrained instructions (Section 5.2), and the efficacy of our</p>
<p>DECRIM self-correction pipeline (Section 5.3).</p>
<h3>5.1 Reliability of LLM-as-a-Judge</h3>
<p>Results from several model and baseline judges on the EvalJudge dataset are presented in Table 3. Human inter-rater reliability (Human 1, Human 2, and Expert) is moderate (Krippendorff's $\alpha=0.61$ ), with lower agreement between Humans 1 and $2(\kappa$ $=0.44)$. This highlights the inherent challenges in verifying constraint satisfaction, as the task can be subjective and ambiguous, when involving multiple sub-constraints, despite efforts to minimize these issues during data annotation (see Appendix D.2.2).</p>
<p>GPT-4-Turbo with CoT is Reliable and More Cost-Efficient. We observe that GPT-4, a widely used LLM-as-a-Judge, shows lower performance compared to humans while maintaining moderate correlation with humans $(\kappa=0.42)$. Using GPT-4-Turbo, evaluating constraints individually yields better results than evaluating them all at once, despite a $37 \%$ cost increase. GPT-4-Turbo with CoT prompt offers a more performant and cheaper alternative to GPT-4. It reduces costs by $57 \%$ while improving overall performance (Macro F1) by $+7.0 \%$ and in detecting unmet constraints (F1 Negative) by $+19.0 \%$. Its correlation with Expert annotation is similar to that of Human 2 ( 0.58 vs. 0.60 ). We thus adopt GPT-4-Turbo with CoT as the standard evaluation for REALINSTRUCT.</p>
<p>Open-source LLMs are unreliable judges. ICLbased configurations of open-source LLMs (Mistral, Vicuna, Zephyr) exhibit poor performance in all scenarios, particularly in detecting unmet constraints. Vicuna and Zephyr closely mirror the "All Satisfied" baseline, suggesting they are lenient judges. Mistral, despite similar Macro-F1, diverges in other metrics, indicating more random than lenient decisions (similar to GPT-3.5-Turbo). When weakly supervised with GPT-4-Turbo's CoT reasoning trails, Mistral significantly improves overall performance and the ability to detecting unmet constraints ( +12.9 in Macro F1 and +28.1 in F1 Neg.). This reduces the macro F1 gap with GPT-4-Turbo by about $50 \%$, but the model still shows poor agreement with humans, indicating that open-source LLMs are not yet reliable judges.</p>
<h3>5.2 LLMs' ability to follow multi-constrained instructions on RealInStruCt</h3>
<p>Benchmarking results for all models on ReALInSTRUCT are presented in Table 5. We observe</p>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Decomposer</th>
<th>Critic</th>
<th>REALINSTRUCT</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>IFEval</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>Best N</td>
<td>Instruction Acc (%)</td>
<td>Constraint Acc (%)</td>
<td>OQA (%) Win / Loss</td>
<td>Time (h)</td>
<td>Best N</td>
<td>Instruction Acc (%)</td>
<td>Constraint Acc (%)</td>
<td>OQA (%) Win / Loss</td>
<td>Time (h)</td>
</tr>
<tr>
<td>GPT-4</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>78.8</td>
<td>91.9</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>79.35</td>
<td>85.45</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Conv.</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>75.2</td>
<td>87.8</td>
<td>-</td>
<td>2.5</td>
<td>-</td>
<td>60.1</td>
<td>66.3</td>
<td>-</td>
<td>2.9</td>
</tr>
<tr>
<td>Make sure</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>76.8</td>
<td>88.6</td>
<td>-</td>
<td>2.5</td>
<td>-</td>
<td>60.1</td>
<td>67.2</td>
<td>-</td>
<td>3.6</td>
</tr>
<tr>
<td>Self-Refine</td>
<td>-</td>
<td>-</td>
<td>2</td>
<td>77.2 (↑ 0.4)</td>
<td>88.7 (↑ 0.1)</td>
<td>22.1 / 21.2</td>
<td>8.6</td>
<td>2</td>
<td>59.5 (↓ 0.6)</td>
<td>66.4 (↓ 0.8)</td>
<td>21.3 / 20.8</td>
<td>4.5</td>
</tr>
<tr>
<td></td>
<td>Self</td>
<td>Self</td>
<td>6</td>
<td>75.2 (↓ 1.6)</td>
<td>88.9 (↑ 0.3)</td>
<td>36.7 / 22.4</td>
<td>11.2</td>
<td>4</td>
<td>60.1 (0.0)</td>
<td>67.5 (↑ 0.3)</td>
<td>17.1 / 30.6</td>
<td>5.3</td>
</tr>
<tr>
<td></td>
<td>Self</td>
<td>Supervised</td>
<td>10</td>
<td>80.5 (↑ 3.7)</td>
<td>90.9 (↑ 2.3)</td>
<td>37.1 / 22.0</td>
<td>6.9</td>
<td>10</td>
<td>60.8 (↑ 0.7)</td>
<td>67.3 (↑ 0.1)</td>
<td>17.1 / 29.5</td>
<td>5.7</td>
</tr>
<tr>
<td>DECRIM</td>
<td>Oracle $^{1}$</td>
<td>Self</td>
<td>4</td>
<td>78.5 (↑ 1.7)</td>
<td>90.2 (↑ 1.6)</td>
<td>24.0 / 24.0</td>
<td>6.1</td>
<td>6</td>
<td>62.3 (↑ 2.2)</td>
<td>69.1 (↑ 1.9)</td>
<td>17.6 / 28.1</td>
<td>5.9</td>
</tr>
<tr>
<td>(ours)</td>
<td>Oracle $^{1}$</td>
<td>Supervised</td>
<td>10</td>
<td>82.4 (↑ 5.6)</td>
<td>91.7 (↑ 3.1)</td>
<td>34.3 / 22.2</td>
<td>5.6</td>
<td>10</td>
<td>64.9 (↑ 4.8)</td>
<td>71.6 (↑ 4.4)</td>
<td>18.2 / 30.9</td>
<td>6.2</td>
</tr>
<tr>
<td></td>
<td>Oracle $^{1}$</td>
<td>GPT-4</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>68.2 (↑ 0.1)</td>
<td>74.1 (↑ 6.9)</td>
<td>13.8 / 34.6</td>
<td>6.7</td>
</tr>
<tr>
<td></td>
<td>Oracle $^{1}$</td>
<td>Oracle $^{2}$</td>
<td>10</td>
<td>93.7 (↑ 16.9)</td>
<td>95.2 (↑ 6.6)</td>
<td>33.3 / 22.2</td>
<td>8.5</td>
<td>8</td>
<td>80.4 (↑ 20.3)</td>
<td>83.5 (↑ 16.3)</td>
<td>20.4 / 30.6</td>
<td>8.9</td>
</tr>
<tr>
<td>Proprietary Model</td>
<td></td>
<td>Baselines</td>
<td></td>
<td>Fairly Comparable</td>
<td></td>
<td>Realistic Ablation</td>
<td></td>
<td>Unrealistic ablation (upper bound)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Results of the best iteration on REALINSTRUCT and IFEval benchmarks for DECRIM pipeline. Except for the first line, all results use Mistral v0.2. Absolute improvements from Make Sure baseline are shown in (), with the best result in bold for each scenario. ${ }^{1}$ Oracle decomposer refers to human-verified constraint annotations provided with the datasets. ${ }^{2}$ Oracle feedback is GPT-4-Turbo on REALINSTRUCT and a lenient rule-based evaluation on IFEVal. ${ }^{§}$ Results reported by Zhou et al. (2023a). Reported time considers only generation time for fair comparison.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Instruction-level <br> Accuracy</th>
<th style="text-align: center;">Constraint-level <br> Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">78.8\%</td>
<td style="text-align: center;">91.9\%</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">73.8\%</td>
<td style="text-align: center;">84.0\%</td>
</tr>
<tr>
<td style="text-align: center;">Mistral 7B v0.2</td>
<td style="text-align: center;">75.2\%</td>
<td style="text-align: center;">87.8\%</td>
</tr>
<tr>
<td style="text-align: center;">Zephyr 7B $\beta$</td>
<td style="text-align: center;">70.5\%</td>
<td style="text-align: center;">84.7\%</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna 7B v1.3</td>
<td style="text-align: center;">61.3\%</td>
<td style="text-align: center;">77.8\%</td>
</tr>
</tbody>
</table>
<p>Table 5: LLMs results on REALINSTRUCT
that even with strong proprietary model, GPT4, over $21 \%$ of instructions have at least one unsatisfied constraint. Additionally, the opensource model Mistral v0.2 outperforms proprietary GPT-3.5 but falls short of GPT-4's performance. A qualitative examination of responses with unsatisfied constraints suggests that LLMs often struggle with constraints involving numbers, negations, or long instructions with large number of constraints, which is consistent with findings from previous works (Sui et al., 2024; García-Ferrero et al., 2023; Truong et al., 2023; Jiang et al., 2024). We also discuss intra-model scoring bias problem in the Limitations Section. Overall, the results highlight the need for further enhancement of LLMs in handling multi-constrained instructions.</p>
<h3>5.3 Effectiveness of DECRIM Self-Correction</h3>
<p>We present the results of our experiments with Mistral v0.2 using the DECRIM Self-Correction pipeline in Table 4. A slight improvement is observed with the Make Sure prompt compared to the conventional prompt on both datasets, demonstrating that it serves as a strong baseline for firstgeneration responses. Consequently, we adopt Make Sure as the baseline for further comparisons.</p>
<p>We classify the DECRIM configurations into three categories: (1) Fairly Comparable, where
the pipeline operates independently of external models; (2) Realistic Ablations, which employ fairly comparable Critic models but use ideal Oracle Decomposer, which remains a realistic measure under the guidelines from Kamoi et al. (2024), since decomposition is relatively straightforward and could be handled by a dedicated LLM; and (3) Unrealistic Ablations, which rely on ideal Critic and Decomposer models, useful for upper bound estimation but with limited generalization.</p>
<p>LLMs Cannot Self-Refine. We observe only marginal improvements on REALINSTRUCT and a performance drop on IFEval using the Self-Refine (Madaan et al., 2023) pipeline, highlighting the limitations of traditional self-correction approaches for the multi-constrained instruction following task. Similarly, our DECRIM pipeline showed minimal to no gains when the LLM itself was used as both Decomposer and Critic. This shows that a selfrefinement limitation exists even when the model is instructed to look specifically at constraints. These poor results are attributed to low-quality Critic feedback, which can lead to over-refining good responses and neglecting to fix bad ones. This aligns with findings by Huang et al. (2023a); Kamoi et al. (2024), which showed that LLMs struggle with self-correction without external guidance. Additionally, this aligns with results from Sec. 5.1, in which Vanilla Mistral failed to reliably detect unsatisfied constraints.</p>
<p>DECRIM is effective, even with a Weak Critic. The results show significant improvement with the introduction of Supervised Mistral, a weak Critic that outperforms the LLM's self-critique but still underperforms as a Critic, as per results in Section 5.1. With a Self-Decomposer and Supervised</p>
<p>Critic, performance increased by +3.7 on REALINSTRUCT and +0.7 on IFEval. With an Oracle Decomposer specifying accuratly which constraints to check, improvements were +5.6 and +4.8 , respectively, despite the harsher domain shift in IFEval, where Mistral Supervised performed even weaker.</p>
<p>Introducing a stronger Critic model, GPT-4, emulating what would be the performance of a model trained with high-quality data, resulted in an improvement of +8.1 on IFEval. However, it is worth mentioning that GPT-4 as evaluator only achieved a Macro F1 of $62.9 \%$ when judging Mistral's Make Sure responses on IFEval, being far from an ideal Critic for this dataset. Using an Oracle Critic representing the strongest possible feedback - the model showed improvements of +16.9 on REALINSTRUCT and +20.3 on IFEval, demonstrating that open-source LLMs can correct its responses when given high-quality feedback. This highlights the potential of the DECRIM pipeline for multi-constrained instruction following, particularly when strong feedback is available.</p>
<p>Comparing DECRIM with proprietary GPT-4, even weak Critics enabled Mistral to surpass GPT4's performance on REALINSTRUCT, though only Oracle Critic outperformed GPT-4 on IFEval, likely due to its harder nature. Overall, these results demonstrate that LLMs can achieve significant improvements in multi-constrained instruction following when provided with minimally reliable feedback. As expected, higher-quality feedback leads to better performance, with a plateau beyond the performance of proprietary models.</p>
<p>DECRIM improves the quality of responses. Table 4 also provides a comparison of overall response quality before and after pipeline revisions. In most cases, the quality remained the same, but when changes occurred, on the REALINSTRUCT dataset, the revised responses were more oftenly preferred. This indicates that DECRIM either maintains or improves response quality in the majority of cases. Additionally, we observed a strong correlation between quality improvement and successful revisions, though a high number of revisions can negatively impact quality. This explains the higher quality improvement with better feedback on REALInStrUCT and higher quality degradation observed on IFEval, which contains more difficult and unachievable constraints.</p>
<p>Importance of Decomposer and Critic Models. Our results also highlight the essential roles of the</p>
<p>Decomposer and Critic in the DECRIM pipeline. While the Decomposer helps guide the Critic to the right constraints, the Critic's quality has the most significant impact on performance. The weakest configuration (Self-Decomposer and Self-Critic) results in minimal gains or slight degradation. However, even with a weak Self-Decomposer, improvements occur with a better Critic, Supervised, which boosts score in REALINSTRUCT from 75.2 to 80.5. The Oracle Decomposer consistently enhances performance across both benchmarks, but the Critic's quality drives the largest improvements. For example, using the Oracle Decomposer with the Supervised Critic yields a +4.8 instruction accuracy increase in IFEval, and with stronger Critics like GPT-4 or Oracle, the relative gains rise to +8.1 and +20.3 , respectively. Same happens with Oracle Critic for RealInStruct. These demonstrates that while a better Decomposer is beneficial, the Critic's ability to correctly judge responses is the key factor for DECRIM's success, with Oracle Critic consistently delivering the highest performance. In fact, the Critic's success is closely tied to the Decomposer's accuracy in pointing the constraints to be verified. In Appendix B. 2 we discuss the role of Refine and its robustness to weak Critic.</p>
<h2>6 Conclusion</h2>
<p>In this work, we benchmarked LLMs' ability to follow real multi-constrained user requests with REALINSTRUCT. We showed that even strong proprietary models like GPT-4 fail to meet at least one constraint in over $21 \%$ of instructions, demonstrating REALINSTRUCT's challenging nature and the need for improvement across both proprietary and open-source models. To address this, we proposed the DECRIM self-correction pipeline, which decomposes instructions into granular requirements, critiques responses, and refines outputs. Extensive experiments showed that DECRIM significantly improves open-source LLM performance, with stronger feedback allowing them to surpass GPT4. Overall, our work highlights the underexplored problem of following real-world user requests, as well as advances System 2 techniques with DECRIM. Future work could refine the DECRIM's components and integrate the pipeline to other System 2 approaches, such as self-consistency and generate-and-rank, to enhance its effectiveness in tasks where spending more time on generationrefinement iterations would improve performance.</p>
<h2>7 Limitations</h2>
<p>Model-based vs. Rule-based Evaluation. Using model-based evaluation over rule-based introduces two key challenges. First, it reflects the precision/accuracy trade-off, where rule-based methods offer higher precision but are less accurate due to synthetic scenarios, while model-based ones, though less precise, better align with real-world tasks (see discussion on Section A.1.3). Second, evaluating the REALINSTRUCT dataset relies on the proprietary GPT-4-Turbo API, making it costly. To address these, techniques like multi-prompting (Mizrahi et al., 2024) and panel of juries (Verga et al., 2024) can improve precision in model-based evaluation, including with open-source models, while future advancements in open-source LLMs may provide cost-effective evaluation alternatives.</p>
<h2>Data Contamination and Intra-Model Scoring</h2>
<p>Bias. Developing benchmarks with publicly available data that does not overlap with LLM training data is challenging, as pre-training and instruction-tuning datasets are often undisclosed. For example, reliable information on Mistral's training data is unavailable. However, while Vicuna v1.3 reported instruction tuning on a dataset overlapping with REALINSTRUCT, no significant intramodel bias from data contamination was observed, as seen in its poor performance in Table 5. This is likely due to its instruction tuning procedure not ensuring constraint satisfaction in target responses. However, GPT-4's relatively high constraint-level accuracy could indicate scoring bias, as previous studies suggest GPT-4 tends to favor its own outputs (Zheng et al., 2023; Panickssery et al., 2024; Verga et al., 2024). Further investigation into data contamination and intra-model scoring bias is left as future work.</p>
<p>Computation Overhead. The DECRIM pipeline introduces additional computational time compared to single-pass generation. Table 4 provides the running time for each configuration explored. To mitigate this, we have designed the pipeline to trigger the refinement step only when the Critic model detects unsatisfied constraints, which occured in about $25 \%$ of instructions, minimizing unnecessary computation when the model performs well initially. This is an improvement upon other System 2 approaches (see Section A.2.2), such as generate-and-rank, which typically generate multiple outputs for further ranking. Additionally, we
observed that most revisions occur in the first iteration, resulting in a sublinear time increase with more iterations. Also, higher-quality feedback further reduces the need for revisions, improving DECRIM's efficiency.</p>
<p>Optimization Considerations. Due to high computational costs, we did not optimize hyperparameters for training the weakly supervised LLM-as-a-Judge or exhaustively tune the prompts for the adaptation strategies. Additionally, we did not explore using a dedicated LLM as a Decomposer in the DECRIM pipeline, as this is primarily an implementation-focused task, being not critical for demonstrating our core claims. These aspects are left for future work.</p>
<h2>8 Ethical Considerations</h2>
<p>Crowdsourcing. For the EvalJudge Human Annotation task, we recruited native English speakers through Amazon Mechanical Turk ${ }^{3}$ (MTurk). Compensation was based on the number of constraints per instruction, with an estimated average payment of 16.90 USD per hour, which exceeds the highest U.S. minimum wage in 2024 (16.30 USD per hour in Washington State), aligning with ethical guidelines discussed by Huang et al. (2023b).</p>
<p>Data from real users. Constructing a dataset from real user requests presents some ethical challenges:</p>
<ul>
<li>Personally Identifiable Information (PII): Some user interactions with AI assistants may contain PII. During data validation, we actively sought to remove instances containing PII from the dataset. See Appendix D.1.3 for further details.</li>
<li>Harmful Content: The underlying data source is uncensored, and users may produce or request toxic or harmful content. Apart from flagrant cases, we did not actively remove such instances from the dataset.</li>
</ul>
<p>Societal Impact. The DECRIM pipeline improves LLMs' ability to follow user-requested constraints, contributing to a broader societal impact of advancing LLM capabilities. When it comes particularly to user requests, it is important to note that some user constraints may conflict with system constraints set by developers, such as requests to generate harmful or toxic content. Although our study</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>does not look into conflicting constraints, there is a potential risk that the pipeline could prioritize user requests over developer-defined safeguards.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.</p>
<p>Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023. Open LLM Leaderboard. Hugging Face.</p>
<p>Institution British Standards. 2022. BS ISO 5725-1. Accuracy (trueness and Precision) of Measurement Methods and Results: Part 1. General principles and definitions. pt. 1. British Standards Institution.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. 2024a. Are more llm calls all you need? towards scaling laws of compound inference systems. arXiv preprint arXiv:2403.02419.</p>
<p>Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2024b. Universal self-consistency for large language models. In ICML 2024 Workshop on In-Context Learning.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An OpenSource Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality.</p>
<p>Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating llms by human preference. Preprint, arXiv:2403.04132.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1-53.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168.</p>
<p>Verna Dankers, Elia Bruni, and Dieuwke Hupkes. 2022. The paradox of the compositionality of natural language: A neural machine translation case study. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4154-4175, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. 2023. Rephrase and respond: Let large language models ask better questions for themselves. arXiv preprint arXiv:2311.04205.</p>
<p>Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and Stuart Shieber. 2024. Implicit chain of thought reasoning via knowledge distillation.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2024. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning.</p>
<p>Iker García-Ferrero, Begoña Altuna, Javier Alvez, Itziar Gonzalez-Dios, and German Rigau. 2023. This is not a dataset: A large negation benchmark to challenge large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8596-8615, Singapore. Association for Computational Linguistics.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024. CRITIC: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations.</p>
<p>Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. 2024. Can large language models understand real-world complex instructions? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18188-18196.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations.</p>
<p>Edward J Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.</p>
<p>Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A. Smith. 2023. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2040620417.</p>
<p>Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024. On the Limitations of Fine-tuned Judge Models for LLM Evaluation. arXiv preprint arXiv:2403.02839.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023a. Large language models cannot self-correct reasoning yet. ArXiv, abs/2310.01798.</p>
<p>Olivia Huang, Eve Fleisig, and Dan Klein. 2023b. Incorporating worker perspectives into MTurk annotation practices for NLP. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1010-1028, Singapore. Association for Computational Linguistics.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2024. FollowBench: A multi-level fine-grained constraints following benchmark for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4667-4688, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du. 2023. Faithscore: Evaluating hallucinations in large vision-language models. Preprint, arXiv:2311.01477.</p>
<p>Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. 2024. When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs. arXiv preprint arXiv:2406.01297.</p>
<p>Pei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. 2024. CritiqueLLM: Towards an informative critique generation model for evaluation of large language model generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13034-13054, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, and Ethan Perez. 2024. Debating with more persuasive LLMs leads to more truthful answers. In Fortyfirst International Conference on Machine Learning.</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2024a. Prometheus: Inducing finegrained evaluation capability in language models. In The Twelfth International Conference on Learning Representations.</p>
<p>Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024b. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535.</p>
<p>Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. 2024. Volcano: Mitigating multimodal hallucination through self-feedback guided revision. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 391-404, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Soochan Lee and Gunhee Kim. 2023. Recursion of thought: A divide-and-conquer approach to multicontext reasoning with language models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 623-658, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023a. Alpacaeval: An automatic evaluator of instruction-following models.</p>
<p>Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. 2023b. Evaluating object hallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in</p>
<p>Natural Language Processing, pages 292-305, Singapore. Association for Computational Linguistics.</p>
<p>Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823-1840, Online. Association for Computational Linguistics.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2511-2522, Singapore. Association for Computational Linguistics.</p>
<p>Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi Wang, and Diyi Yang. 2023. Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints. In Findings of the Association for Computational Linguistics: EACL 2023, pages 1982-2008, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems, volume 36, pages 46534-46594. Curran Associates, Inc.</p>
<p>Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2023. Teaching small language models to reason. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1773-1781, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Edoardo Manino, Julia Rozanova, Danilo Carvalho, Andre Freitas, and Lucas Cordeiro. 2022. Systematicity, compositionality and transitivity of deep NLP models: a metamorphic testing perspective. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2355-2366, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12076-12100, Singapore. Association for Computational Linguistics.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions.</p>
<p>In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470-3487, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. 2024. State of What Art? A Call for Multi-Prompt LLM Evaluation. Transactions of the Association for Computational Linguistics, 12:933-949.</p>
<p>Alan S Morris. 2001. Measurement and instrumentation principles. Measurement Science and Technology, 12(10):1743-1744.</p>
<p>Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, and David Wagner. 2023. Can llms follow simple rules? arXiv preprint arXiv:2311.04235.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744.</p>
<p>Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2024. Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies. Transactions of the Association for Computational Linguistics, 12:484-506.</p>
<p>Arjun Panickssery, Samuel R Bowman, and Shi Feng. 2024. Llm evaluators recognize and favor their own generations. arXiv preprint arXiv:2404.13076.</p>
<p>Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan Wang, and Lifu Huang. 2023. The art of SOCRATIC QUESTIONING: Recursive thinking with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4177-4199, Singapore. Association for Computational Linguistics.</p>
<p>Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024. Infobench: Evaluating instruction following ability in large language models. ArXiv, abs/2401.03601.</p>
<p>Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 2024. Branch-Solve-Merge Improves Large Language Model Evaluation and Generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 8352-8370, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: an adversarial winograd schema challenge at scale. Commun. $A C M, 64(9): 99-106$.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802.</p>
<p>Timo Schick, Jane A. Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2023. PEER: A collaborative language model. In The Eleventh International Conference on Learning Representations.</p>
<p>Kumar Shridhar, Harsh Jhamtani, Hao Fang, Benjamin Van Durme, Jason Eisner, and Patrick Xia. 2023. Screws: A modular framework for reasoning with revisions. arXiv preprint arXiv:2309.13075.</p>
<p>Kumar Shridhar, Koustuv Sinha, Andrew Cohen, Tianlu Wang, Ping Yu, Ramakanth Pasunuru, Mrinmaya Sachan, Jason Weston, and Asli Celikyilmaz. 2024. The ART of LLM Refinement: Ask, Refine, and Trust. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 5872-5883, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, and Saab Mansour. 2024. FineSurE: Fine-grained summarization evaluation using LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 906-922, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew Lampinen, Andy</p>
<p>Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. 2024. Table meets llm: Can large language models understand structured table data? a benchmark and empirical study. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, WSDM '24, page 645-654, New York, NY, USA. Association for Computing Machinery.</p>
<p>Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Wieting, Nanyun Peng, and Xuezhe Ma. 2023. Evaluating Large Language Models on Controlled Generation Tasks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3155-3168, Singapore. Association for Computational Linguistics.
J.R. Taylor. 1997. Introduction To Error Analysis: The Study of Uncertainties in Physical Measurements. ASMSU/Spartans.4.Spartans Textbook. University Science Books.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.</p>
<p>Thinh Hung Truong, Timothy Baldwin, Karin Verspoor, and Trevor Cohn. 2023. Language models are not naysayers: an analysis of language models on negation benchmarks. In Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 101-114, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Lewis Tunstall, Edward Emanuel Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M Rush, and Thomas Wolf. 2024. Zephyr: Direct Distillation of LM Alignment. In First Conference on Language Modeling.</p>
<p>Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. arXiv preprint arXiv:2307.03987.</p>
<p>Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. 2024.</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. arXiv preprint arXiv:2404.18796.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023a. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484-13508, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. 2022. SuperNaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085-5109.</p>
<p>Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, et al. 2024. Speculative rag: Enhancing retrieval augmented generation through drafting. arXiv preprint arXiv:2407.08223.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc.</p>
<p>Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2023. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations.</p>
<p>Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxin Xu, et al. 2024. Benchmarking complex instruction-following with multiple constraints composition. arXiv preprint arXiv:2407.03978.</p>
<p>Jason Weston and Sainbayar Sukhbaatar. 2023. System 2 attention (is something you might need too). arXiv preprint arXiv:2311.11829.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li. 2023. INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5967-5994, Singapore. Association for Computational Linguistics.</p>
<p>Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4393-4479, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, and Karthik R Narasimhan. 2024a. COLLIE: Systematic construction of constrained text generation tasks. In The Twelfth International Conference on Learning Representations.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In Advances in Neural Information Processing Systems, volume 36, pages 11809-11822. Curran Associates, Inc.</p>
<p>Yuxuan Yao, Han Wu, Zhijiang Guo, Zhou Biyan, Jiahui Gao, Sichun Luo, Hanxu Hou, Xiaojin Fu, and Linqi Song. 2024b. Learning from correctness without prompting makes LLM efficient reasoner. In First Conference on Language Modeling.</p>
<p>Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. 2023. Selfee: Iterative self-revising llm empowered by selffeedback generation. Blog post.</p>
<p>Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. 2024. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, Florence, Italy. Association for Computational Linguistics.</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2024. Evaluating large language models at evaluating instruction following. In The Twelfth International Conference on Learning Representations.</p>
<p>Tao Zhang, Yanjun Shen, Wenjing Luo, Yan Zhang, Hao Liang, Fan Yang, Mingan Lin, Yujing Qiao, Weipeng Chen, Bin Cui, et al. 2024a. Cfbench: A comprehensive constraints-following benchmark for llms. arXiv preprint arXiv:2408.01122.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT. In International Conference on Learning Representations.</p>
<p>Weijia Zhang, Mohammad Aliannejadi, Yifei Yuan, Jiahuan Pei, Jia-Hong Huang, and Evangelos Kanoulas. 2024b. Towards fine-grained citation evaluation in generated text: A comparative analysis of faithfulness metrics. arXiv preprint arXiv:2406.15264.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Advances in Neural Information Processing Systems, volume 36, pages 46595-46623. Curran Associates, Inc.</p>
<p>Tianqi Zhong, Zhaoyi Li, Quan Wang, Linqi Song, Ying Wei, Defu Lian, and Zhendong Mao. 2024. Benchmarking and improving compositional generalization of multi-aspect controllable text generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6486-6517, Bangkok, Thailand. Association for Computational Linguistics.</p>
<p>Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023a. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911.</p>
<p>Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, and Mrinmaya Sachan. 2023b. Controlled Text Generation with Natural Language Instructions. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 42602-42613. PMLR.</p>
<h1>Contents</h1>
<p>1 Introduction ..... 1
2 RealInStruct Dataset ..... 2
2.1 Data Description ..... 3
2.2 Data Construction ..... 3
3 Self-correction with Decompose, Critique, and Refine (DeCRIM) ..... 4
4 Experimental Setup ..... 5
4.1 Validating LLM-as-a-Judge for Constraint Satisfaction ..... 5
4.1.1 Adaptation strategies ..... 5
4.1.2 The EvalJudge Dataset ..... 5
4.2 Benchmarking LLMs on ReALInStrUCT ..... 6
4.3 Evaluating DECRIM pipeline ..... 6
5 Results and Discussion ..... 7
5.1 Reliability of LLM-as-a-Judge ..... 7
5.2 LLMs' ability to follow multi-constrained instructions on REALInStruct ..... 7
5.3 Effectiveness of DECRIM Self-Correction ..... 8
6 Conclusion ..... 9
7 Limitations ..... 10
8 Ethical Considerations ..... 10
A Related Work ..... 18
A. 1 Evaluating LLMs' Generative Abilities ..... 18
A.1.1 LLM-as-a-judge ..... 18
A.1.2 Fine-grained evaluation ..... 19
A.1.3 Benchmarking Instruction-Following Abilities ..... 19
A. 2 LLM Self-Correction for open-ended text generation ..... 20
A.2.1 Constrained Generation ..... 20
A.2.2 System 2 Approaches ..... 21
B Extra Analysis and Discussions of DECRIM ..... 21
B. 1 DECRIM Experiments with other Open LLMs ..... 22
B. 2 Comparing DECRIM with Generate-and-Rank ..... 22
C Definition of Task, Context, and Constraints in ReALInStrUCT ..... 23
D ReALInStruct data construction details ..... 24
D. 1 Data Filtering ..... 24
D.1.1 Prompt for two-shot classification of code-related conversations ..... 25
D.1.2 Prompt for few-shot classification of instruction with constraints ..... 25
D.1.3 Data Collection Validation Guidelines ..... 26
D. 2 Instruction decomposition ..... 26
D.2.1 Prompt for Instruction decomposition with GPT-4 ..... 26
D.2.2 Decomposition validation guidelines ..... 28
E RealInStruct Constraints Categorization ..... 30</p>
<p>G Implementation details for DECRIM pipeline ..... 34
G. 1 Instruction decomposition ..... 34
G. 2 Iterative Self-Correction with Feedback from Critic ..... 35
G. 3 Overall Quality Assessment (OQA) details ..... 35
H Extra Experimental Details for LLM-as-a-Judge Validation for Constraint Satisfaction ..... 36
H. 1 Prompts for ICL-based Adaptation Strategies ..... 36
H.1.1 Prompt Instruction-wise Eval (ICL-Inst.) ..... 36
H.1.2 Prompt Constraint-wise Eval (ICL-Const.) ..... 37
H.1.3 Prompt Constraint-wise Eval + CoT (ICL-Const.+CoT) ..... 37
H. 2 Guidelines for Constraint Satisfaction Human Audition ..... 38
H. 3 Princing Details for Propretary LLM-as-a-Judge ..... 39
H. 4 Details for Open LLM Weak Supervision ..... 39
Appendix
A Related Work
This section situates our work within broader research directions, highlighting intersections with current studies. Section A. 1 focuses on benchmarking and evaluating LLMs' generative abilities, while Section A. 2 discusses approaches for enhancing LLM responses.</p>
<h1>A. 1 Evaluating LLMs' Generative Abilities</h1>
<p>Traditional language model benchmarks, such as HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), MMLU (Hendrycks et al., 2021), GSM-8K (Cobbe et al., 2021), and BIG-Bench (Srivastava et al., 2023), primarily assess LLMs on tasks like commonsense reasoning and standardized exams. These benchmarks evaluate models using multiple-choice questions (MCQs) to objectively measure internal reasoning capabilities. However, recent advancements in language models have demonstrated emergent capabilities in generating high-quality open-ended text generation (Wei et al., 2021; Chung et al., 2024; Ouyang et al., 2022; Taylor et al., 2022; Bubeck et al., 2023).</p>
<p>This shift presents new challenges, as the number of possible responses are virtually infinite, requiring more subjective evaluation rather than strict reference matching. While MCQ-based benchmarks fall short in assessing these generative abilities, human annotation, though reliable, is limited by cost and scalability. To address this, some research directions sacrifice the question quality to be able to use rule-based evaluation methods, while others explore model-based approaches.</p>
<h2>A.1.1 LLM-as-a-judge</h2>
<p>Early model-based efforts like BERTScore (Zhang et al., 2020) sought to improve on traditional n-gram metrics by recognizing high-quality responses that differ from the reference. For more open-ended generation, where references are soft or nonexistent, recent work has introduced the concept of LLM-as-a-Judge (Zheng et al., 2023; Liu et al., 2023), using strong proprietary LLMs like GPT-4 (Achiam et al., 2023) to evaluate responses. These models have shown they can approximate the depth and consistency of manual human evaluation, but also provide better consistency and stability.</p>
<p>Recent research has begun exploring open-source LLMs for LLM-as-a-Judge, aiming to reduce reliance on proprietary models. Although open-source models have shown limited capability with in-context learning, fine-tuning them for specific evaluations is a promising direction (Huang et al., 2024; Kim et al., 2024a). Contemporaneous work Prometheus-2, an open LLM-as-a-Judge, has shown strong correlation with human evaluation, even surpassing GPT-4 in some cases, though it still lags in out-of-domain cases (Kim et al., 2024b). In our work, we assess both proprietary and open-source models for evaluating user constraint satisfaction in LLM responses. Our results indicate that while proprietary models outperform, open models can improve significantly when weakly supervised with proprietary model evaluations and reasoning trails, making them viable as Critic models in a self-correction pipeline.</p>
<p>Another recent approach by Verga et al. (2024) proposes replacing individual judges with juries of cheaper LLMs, which has been shown to correlate better with human judgments, even outperforming GPT-4 in some scenarios and reducing intra-model evaluation bias. This suggests that exploring LLM panels for constraint satisfaction evaluation could be a fruitful direction for future work.</p>
<h1>A.1.2 Fine-grained evaluation</h1>
<p>Some studies have explored approaches inspired by the divide-and-conquer paradigm, breaking multifaceted tasks into fine-grained components (Lee and Kim, 2023). This can be successful given a complex compositional characteristic of the Natural Language (Manino et al., 2022; Dankers et al., 2022; Zhong et al., 2024). For evaluation, this strategy not only provides detailed insights into model performance across different aspects but also makes evaluations more objective and less ambiguous, as models may excel in some areas while underperforming in others. This approach seems promising for LLM-as-a-Judge, given that LLMs prompting techniques such as Chain-of-Thought (Wei et al., 2022), Tree-of-Thought (Yao et al., 2023), and Recursive Thinking (Qi et al., 2023), have demonstrated LLM performance improvements by breaking complex tasks into simpler sequential steps. We discuss these methods on Section A.2.2.</p>
<p>Specific works on evaluation by Min et al. (2023); Li et al. (2023b); Jing et al. (2023); Hu et al. (2023); Song et al. (2024); Huang et al. (2024); Zhang et al. (2024b) have shown the benefits of decomposing tasks into atomic facts for tasks such as fact-checking against cross-modality references. Kim et al. (2024a); Magister et al. (2023); Ke et al. (2024) have demonstrated that fine-grained evaluation from diverse sources enhances fine-tuned open evaluators by making the task more objective. Additionally, weak fine-grained evaluation during generation time has been shown to improve LLM self-correction performance (Shridhar et al., 2023, 2024; Wang et al., 2024).</p>
<p>In our work, we implement a similar approach by decomposing the task of evaluating multi-constrained instructions into individual constraint evaluations. This "instruction decomposition" simplifies and makes more objective the instruction evaluation task for LLMs and provides more informative insights through constraint-level accuracy metrics. We also argue that existing overall instruction satisfaction metrics fail to detect unmet constraints due to the ambiguity caused by the lack of granularity, as also highlighted by Sun et al. (2023). Our results demonstrate the effectiveness of fine-grained evaluation for this task and show that incorporating it into a self-correction pipeline enhances performance, even with weak Critic and Decomposer models.</p>
<h2>A.1.3 Benchmarking Instruction-Following Abilities</h2>
<p>The ability of LLMs to follow user instructions in open-ended text generation has only recently gained attention. New benchmarks like AlpacaEval (Li et al., 2023a) and the test splits of Natural-Instructions (Mishra et al., 2022) and Self-Instruct (Wang et al., 2023b) address the evaluation in this aspect by using LLM-as-a-Judge to compare with reference responses or provide overall instruction satisfaction scores. Recent studies have shown that models often follow instructions only partially, frequently failing to adhere to specific constraints provided by users (Sun et al., 2023; Zhou et al., 2023a; Yao et al., 2024a; Jiang et al., 2024; Qin et al., 2024; Wen et al., 2024; He et al., 2024; Zhang et al., 2024a).</p>
<p>To evaluate this, the few existing benchmarks focus on a set of specific constraint categories and/or use synthetic constraints that can be easily verified through rule-based methods (Zhou et al., 2023a; Yao et al., 2024a). The trade-off between rule-based and model-based evaluation falls into the famous precision/accuracy dilemma about static instrument characteristics (sometimes referred as bias/variance dilemma) in the Statistics of measurements (Morris, 2001; Taylor, 1997; British Standards, 2022). Rulebased evaluation offers high-to-perfect precision (low variance), but it is usually required to be done on unrealistic scenarios, being less accurate (high bias). The use of model-based evaluation loses some precision compared to rule-based due to inherent variability introduced by LLMs (lower precision, higher variance), but aligns more with the task objective of evaluating more realistic scenarios (higher accuracy, lower bias).</p>
<p>To the best of our knowledge, our ReALInStrUCT benchmark is the first to evaluate LLMs using real-user instructions, offering a more realistic and comprehensive assessment. This approach closely</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Feedback Source</th>
<th>Refinement Strategy</th>
<th>Tasks Investigated</th>
<th>Supported Constraint Types</th>
</tr>
</thead>
<tbody>
<tr>
<td>Self-correction with training for refining</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Selfne (Ye et al., 2023)</td>
<td>LLM + ICL</td>
<td>LLM SFT</td>
<td>Open-ended Instructions</td>
<td>Not constraint focus</td>
</tr>
<tr>
<td>PEER (Schick et al., 2023)</td>
<td>LLM SFT</td>
<td>LLM SFT</td>
<td>Constrained Generation</td>
<td>Limited constraints</td>
</tr>
<tr>
<td>Self-Critique (Saunders et al., 2022),</td>
<td>LLM SFT</td>
<td>LLM SFT</td>
<td>Conditional Summarization,</td>
<td>Limited constraints</td>
</tr>
<tr>
<td>VOLCANO (Lee et al., 2024)</td>
<td>w/ Human Feedback</td>
<td>w/ Human Feedback</td>
<td>Visual Question-Answering</td>
<td>Limited constraints</td>
</tr>
<tr>
<td>InstructScore</td>
<td>LLM SFT</td>
<td>LLM SFT</td>
<td>CommonGen</td>
<td>Limited constraints</td>
</tr>
<tr>
<td>(Xia et al., 2023)</td>
<td>w/ GPT-4 and</td>
<td>w/ GPT-4</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Human Feedback</td>
<td></td>
<td>CommonGen,</td>
<td>Limited constraints</td>
</tr>
<tr>
<td>Self-Correctors (Welleck et al., 2023)</td>
<td>External tools</td>
<td>Smaller</td>
<td>Detoxification</td>
<td>Limited constraints</td>
</tr>
<tr>
<td>RULES (Mu et al., 2023)</td>
<td>External tools</td>
<td>LLM SFT</td>
<td>System</td>
<td>Limited constraints</td>
</tr>
<tr>
<td>Re3 (Yang et al., 2022)</td>
<td>Smaller LLM SFT</td>
<td>LLM +</td>
<td>Constraints</td>
<td>Limited constraints</td>
</tr>
<tr>
<td></td>
<td>+ External Tools</td>
<td>smaller model</td>
<td>Story Generation</td>
<td>Limited constraints</td>
</tr>
<tr>
<td>Self-correction without training for refinement</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Self-Refine (Madaan et al., 2023)</td>
<td>LLM + ICL</td>
<td></td>
<td>Open-ended Instructions;</td>
<td>Not constraint focus;</td>
</tr>
<tr>
<td>CRITIC (Gou et al., 2024),</td>
<td>External tools</td>
<td>LLM + ICL</td>
<td>CommonGen</td>
<td>Limited constraints</td>
</tr>
<tr>
<td>Hallucination (Varshney et al., 2023)</td>
<td>LLM SFT</td>
<td></td>
<td>Detoxification,</td>
<td>Limited constraints</td>
</tr>
<tr>
<td>DeCRIM (ours)</td>
<td>w/ GPT-4</td>
<td></td>
<td>Open-ended Instructions</td>
<td>Any constraint</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of representative works on Self-Correction for Constrained Generation. Our DeCRIM pipeline is unique as do not require LLM fine-tuning for refinement, being also the only that can handle open-ended instructions with any type of constraints.
mirrors real-world scenarios, unlike previous benchmarks that rely on synthetic constraints, as contrasted on Table 2 and Section 2. Our benchmark’s success relies on a fine-grained evaluation protocol using LLM-as-a-Judge.</p>
<h1>A. 2 LLM Self-Correction for open-ended text generation</h1>
<p>Self-correction has emerged as an effective approach for enhancing LLM responses during generation by refining them during generation time (Pan et al., 2024; Kamoi et al., 2024). However, Kamoi et al. (2024); Huang et al. (2023a) demonstrated that the ability of LLM to self-correct alone is limited to tasks where responses can be decomposed and rely on verifiable components. For harder tasks, LLM self-correction may require additional modeling, new data, or even external tools.</p>
<p>In this sense, Self-correction approaches can be categorized based on the feedback source. Intrinsic SelfCorrection uses carefully crafted prompts or in-context examples to enable the model to identify issues in its output. Self-Correction with External Feedback leverages external tools or more advanced LLMs to provide feedback, while Self-Correction with Fine-Tuning uses external feedback (from humans, stronger LLMs, external tools) to fine-tune the LLM for better feedback and/or response refinement. Kamoi et al. (2024) emphasizes that each self-correction category should be validated using comparable cases specific to its context.</p>
<p>In the case of multi-constrained instructions, the constraints are neither independent nor ordered, making it difficult to guarantee that all responses are decomposable. For example, constraints such as length and style do not have a specific part of the response to be followed, they should be followed in the whole text. Moreover, some constraints are subjective and harder to evaluate, and the instruction decomposition process may introduce noise, further complicating self-correction with the model itself. In our work, we explore both Intrinsic Self-Correction and Self-Correction with Critic Fine-Tuning. As ablation exploration, we also play with External Feedback (referred as Oracle Critic), as an estimation of upper bound performance but recognizing its limited generalization.</p>
<h2>A.2.1 Constrained Generation</h2>
<p>Recent work has explored constrained generation via self-correction, we show some representative work on Table 6. Some approaches are validated only on small, specific constraint sets, limiting their general applicability (Schick et al., 2023; Saunders et al., 2022; Lee et al., 2024; Mu et al., 2023; Yang et al., 2022; Gou et al., 2024; Varshney et al., 2023). For example, Mu et al. (2023) evaluates 13 System constraints, that is constraints defined by the developer. Hallucination and Detoxification constraints can also be seen as System Constraints.</p>
<p>In another direction, Madaan et al. (2023) enhances model outputs by having the model self-review and self-correct its answers. However, this approach is suboptimal as it lacks problem-specific modeling, that is it does not recognize the constraints to be followed, and only prompts the model to evaluate and improve its responses without clear guidance on what to focus on. Ye et al. (2023) goes in a similar direction but relying on supervising LLM to be able to refine. Our results in Sections 5.1 and 5.3 demonstrate improved performance by directing the model to specifically address constraints, even when it is refining its responses using only its own feedback. To the best of our knowledge we are the first to handle open-ended user instructions without restricting it to some constraint types. We highlight the prevalence of constraints in real-world user instructions, making this an important area for further study. Also, unlike most methods, our DECRIM pipeline does not require external tools or fine-tuning LLM for refinement.</p>
<p>It is worth noting that most current constrained generation with self-correction research evaluates on the CommonGen benchmark (Lin et al., 2020), in which constraints are word lists for LLM to include in the text. We argue that this benchmark is insufficient for measuring the ability to follow user requests because: (1) models like GPT-4 already perform at human level ${ }^{4}$; (2) it only represents one type of constraint among many possible; and (3) the constraints are synthetic and not reflective of typical human requests. While valuable in the past for large-scale rule-based evaluation, it may not adequately measure modern LLM capabilities.</p>
<h1>A.2.2 System 2 Approaches</h1>
<p>Kamoi et al. (2024) differentiate self-correction from other methods like self-consistency (Wang et al., 2023a; Chen et al., 2024b; Yao et al., 2024b), which samples diverse reasoning paths during decoding and selects the most consistent, and generate-then-rank methods like Tree of Thoughts (Yao et al., 2023), which generates multiple responses and ranks them using a critic model. These two approaches do not directly refine responses and assume that LLMs can generate at least one correct initial response with a considerable probability, which is not always the case.</p>
<p>These two approaches, like Self-Correction, belong to a broader category known as System 2 approaches, which includes all techniques that generate intermediary outputs before producing final response, aiming to improve LLM responses during generation or inference by emulating the idea of planning. For instance, Khan et al. (2024) and Du et al. (2024) introduced LLM debating, where each LLM initially provides a solution and then revises it based on combined responses, eventually leading to a shared solution after several rounds. Another notable System 2 approach is Branch-Solve-Merge (Saha et al., 2024), which tackles instructions in parts and then merges the results. This has been used for constrained generation, but assumes constraints are independent and merging responses satisfying subsets of constraints address all constraints, which makes it not applicable to real-world constraints.</p>
<p>A key issue with System 2 methods is the increased inference time that naturally comes with the generation of intermediary outputs. Our DECRIM pipeline mitigates this by avoiding unnecessary revisions when the LLM already performs well according to the Critic model, which is an improvement over existing System 2 approaches.</p>
<p>But, this increased inference time is worthwhile. Chen et al. (2024a) shows that more LLM calls can enhance performance in tasks where LLMs are capable, though it may degrade performance on task that are yet challenging for them. This suggests that System 2 approaches can push LLM limits and help us understand more what they are capable of. Additionally, these techniques can generate data to improve and generalize existing models. For example, Deng et al. (2024); Yu et al. (2024) demonstrated that System 2 approaches can be used in a self-supervised learning distillation setting to enhance the original LLMs ("System 1"), resulting in reduced inference costs and improved performance. This is an interesting direction for future work, as an extension of our DECRIM pipeline.</p>
<h2>B Extra Analysis and Discussions of DECRIM</h2>
<p>In this section we present extra analysis and discussions about DECRIM pipeline.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ See CommonGen leaderboard at: https://github.com/allenai/CommonGen-Eval&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>We use only the first user turn (no model responses) from a public dataset of examples originally sourced from ShareGPT at: https://huggingface.co/datasets/anon8231489123 /ShareGPT_Vicuna_unfiltered&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
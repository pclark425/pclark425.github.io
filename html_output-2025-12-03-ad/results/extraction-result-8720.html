<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8720 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8720</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8720</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-263909251</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.08118v1.pdf" target="_blank">Can Large Language Models Really Improve by Self-critiquing Their Own Plans?</a></p>
                <p><strong>Paper Abstract:</strong> There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8720.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8720.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+LLM backprompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM generator + LLM verifier backprompting (self-critiquing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative planning system where the same large language model (GPT-4) is used both to generate candidate plans and to verify/critique them; the verifier returns feedback which is fed back to the generator until the verifier approves or a maximum iteration threshold is reached.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used as both generator and verifier in experiments; run with temperature 0 (deterministic). No model size or training details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-critiquing / Backprompting (LLM+LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generator LLM produces a candidate plan (one-shot prompt with an example); verifier LLM (zero-shot prompt) binary-checks the plan and provides feedback (no constrained format). Generator is reprompted with verifier feedback and this generate-critique cycle repeats until verifier accepts the plan or the iteration limit is reached. The system used a maximum of 15 iterations; observed average iterations = 3.48 for successful runs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classical planning (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classical goal-directed deterministic planning tasks represented in PDDL; domain used is Blocksworld with 100 randomly generated instances for evaluation. Ground-truth plan validation done by external sound verifier VAL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>55/100 (55%) accuracy of final plans judged correct by VAL; average number of iterations observed = 3.48.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Generator LLM only (no backprompting) achieved 40/100 (40%) accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered iterative loop (backprompting): generator one-shot prompt with example, verifier zero-shot prompt returning free-form feedback, feedback is appended to generator prompt for subsequent attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>15</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: LLM+LLM backprompting produced a modest improvement over single-shot generation (55% vs 40% accuracy), indicating some benefit from repeated opportunities to generate plans under verifier feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Verifier LLM produced many false positives (deeming invalid plans valid); verifier confusion matrix: TP=54, FN=1, FP=38, TN=7 (out of 100), yielding overall verifier accuracy 61%. These false positive verifications allow invalid plans to be accepted, undermining reliability in correctness-critical domains. The paper also reports that the granularity of feedback (binary vs detailed) had minimal impact on generator performance in this self-critiquing setup.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to an LLM+VAL (external sound verifier) backprompting system, LLM+LLM is substantially worse (55% vs 88% accuracy). LLM+LLM slightly outperforms no-backprompting (55% vs 40%), but the improvement may be partially due to giving the generator multiple attempts rather than verifier quality.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper reports experiments varying feedback levels and finds little effect of feedback granularity on plan generation performance for LLM-based verification; no detailed ablation isolating only the verifier's role beyond the comparisons to LLM+VAL and no-backprompting are reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8720.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8720.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+VAL backprompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM generator + external sound verifier (VAL) backprompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A backprompting system where an LLM generator (GPT-4) produces candidate plans and an external sound verifier (VAL) validates plans and supplies structured feedback; the generator is reprompted until a valid plan is produced or the iteration limit is reached.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (generator) + VAL (external verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used as generator (temperature 0); VAL is an automatic, sound plan validator (classical planning verification tool) used to produce precise feedback about unmet preconditions/inexecutable actions and unmet goals.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Backprompting with sound verifier (LLM+VAL)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generator LLM creates plans; VAL deterministically verifies plans and returns structured, sound feedback (binary validity and specific error diagnostics such as first inexecutable action or all open conditions); generator is reprompted with VAL feedback until VAL accepts or iteration limit reached. Max iterations set to 15; observed average iterations = 4.18.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classical planning (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same Blocksworld domain and 100-instance evaluation as other conditions; VAL is used both as the verifier in this pipeline and as the ground truth for plan correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>88/100 (88%) accuracy of final plans judged correct by VAL; average number of iterations observed = 4.18.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Generator-only (no backprompting) achieved 40/100 (40%) accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External deterministic verification (VAL) provides sound, structured feedback which is appended to generator prompts (backprompting) enabling iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>15</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: LLM+VAL backprompting raised generator accuracy from 40% to 88%, far exceeding LLM+LLM, showing that an external sound verifier substantially improves iterative refinement outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper does not report catastrophic failure modes for LLM+VAL on this domain; however, authors note that feedback granularity (binary vs first-error vs all-errors) produced only modest differences in generator performance in some splits (see Table 3), and reported denominators vary (50 vs 100) across those feedback-level experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms both LLM+LLM backprompting (88% vs 55%) and generator-only (88% vs 40%). The authors attribute the advantage to VAL's soundness and low false-positive rate.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper reports experiments varying feedback levels for the LLM+VAL setup: No feedback 40/100 (40%) avg steps 1.00; Only binary feedback 37/50 (74%) avg steps 5.38; Binary + first-error (VAL) 43/50 (86%) avg 4.18; Binary + all-errors 43/50 (86%) avg 4.42. Authors conclude feedback granularity had limited additional impact beyond accurate binary feedback and sufficient reprompting.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8720.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8720.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verifier LLM (binary verification)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used as binary plan verifier (self-verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 used in a zero-shot verifier role to decide whether a proposed plan is valid and to optionally provide feedback; evaluated against a sound external verifier (VAL) to assess verifier reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used as a zero-shot verifier for plans produced by the generator LLM; run deterministic (temperature 0); no further model details provided.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Binary self-verification / self-critiquing (verifier LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Verifier is prompted with domain, instance and candidate plan and asked to judge plan validity (binary) and optionally give free-form feedback (no format constraints). The system halts when verifier signals validity or iteration cap reached.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Plan verification on Blocksworld</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of candidate plans as valid/invalid according to ground truth provided by the external sound verifier VAL; evaluated on the final plans from the LLM+LLM pipeline across 100 instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>As the verifier itself: overall accuracy 61/100 (61%). Confusion counts against VAL ground truth: True Positive = 54/55 (98.2%); False Negative = 1/55 (1.8%); False Positive = 38/45 (84.45% of negatives were incorrectly labeled positive); True Negative = 7/45 (15.55%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Not applicable (this entry describes verifier behavior); ground-truth comparator is VAL.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Zero-shot prompting of GPT-4 to perform binary verification and produce feedback; free-form feedback is then used to reprompt the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>No evidence that using GPT-4 as verifier reliably improves correctness; although verifier rarely misses true valid plans (low false-negative rate), it generates many false positives allowing invalid plans to be accepted.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High false-positive rate (38 false positives out of 45 ground-truth invalid plans) — i.e., the LLM often incorrectly deems invalid plans valid, which is particularly dangerous in correctness-critical tasks. This deficiency is the main cause the authors identify for the LLM+LLM system's poor reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>When the verifier role is instead played by the sound VAL tool, the overall system performance is far better (LLM+VAL accuracy 88% vs LLM+LLM 55%), indicating GPT-4's verifier shortcomings are a major limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Large language models are reasoners with self-verification <em>(Rating: 2)</em></li>
                <li>Lm vs lm: Detecting factual errors via cross examination <em>(Rating: 1)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 1)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8720",
    "paper_id": "paper-263909251",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "LLM+LLM backprompting",
            "name_full": "LLM generator + LLM verifier backprompting (self-critiquing)",
            "brief_description": "An iterative planning system where the same large language model (GPT-4) is used both to generate candidate plans and to verify/critique them; the verifier returns feedback which is fed back to the generator until the verifier approves or a maximum iteration threshold is reached.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 used as both generator and verifier in experiments; run with temperature 0 (deterministic). No model size or training details are provided in this paper.",
            "reflection_method_name": "Self-critiquing / Backprompting (LLM+LLM)",
            "reflection_method_description": "Generator LLM produces a candidate plan (one-shot prompt with an example); verifier LLM (zero-shot prompt) binary-checks the plan and provides feedback (no constrained format). Generator is reprompted with verifier feedback and this generate-critique cycle repeats until verifier accepts the plan or the iteration limit is reached. The system used a maximum of 15 iterations; observed average iterations = 3.48 for successful runs.",
            "task_name": "Classical planning (Blocksworld)",
            "task_description": "Classical goal-directed deterministic planning tasks represented in PDDL; domain used is Blocksworld with 100 randomly generated instances for evaluation. Ground-truth plan validation done by external sound verifier VAL.",
            "performance_with_reflection": "55/100 (55%) accuracy of final plans judged correct by VAL; average number of iterations observed = 3.48.",
            "performance_without_reflection": "Generator LLM only (no backprompting) achieved 40/100 (40%) accuracy.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered iterative loop (backprompting): generator one-shot prompt with example, verifier zero-shot prompt returning free-form feedback, feedback is appended to generator prompt for subsequent attempts.",
            "number_of_iterations": 15,
            "evidence_for_improvement": "Quantitative: LLM+LLM backprompting produced a modest improvement over single-shot generation (55% vs 40% accuracy), indicating some benefit from repeated opportunities to generate plans under verifier feedback.",
            "limitations_or_failure_cases": "Verifier LLM produced many false positives (deeming invalid plans valid); verifier confusion matrix: TP=54, FN=1, FP=38, TN=7 (out of 100), yielding overall verifier accuracy 61%. These false positive verifications allow invalid plans to be accepted, undermining reliability in correctness-critical domains. The paper also reports that the granularity of feedback (binary vs detailed) had minimal impact on generator performance in this self-critiquing setup.",
            "comparison_to_other_methods": "Compared to an LLM+VAL (external sound verifier) backprompting system, LLM+LLM is substantially worse (55% vs 88% accuracy). LLM+LLM slightly outperforms no-backprompting (55% vs 40%), but the improvement may be partially due to giving the generator multiple attempts rather than verifier quality.",
            "ablation_study_results": "Paper reports experiments varying feedback levels and finds little effect of feedback granularity on plan generation performance for LLM-based verification; no detailed ablation isolating only the verifier's role beyond the comparisons to LLM+VAL and no-backprompting are reported.",
            "uuid": "e8720.0"
        },
        {
            "name_short": "LLM+VAL backprompting",
            "name_full": "LLM generator + external sound verifier (VAL) backprompting",
            "brief_description": "A backprompting system where an LLM generator (GPT-4) produces candidate plans and an external sound verifier (VAL) validates plans and supplies structured feedback; the generator is reprompted until a valid plan is produced or the iteration limit is reached.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (generator) + VAL (external verifier)",
            "model_description": "GPT-4 used as generator (temperature 0); VAL is an automatic, sound plan validator (classical planning verification tool) used to produce precise feedback about unmet preconditions/inexecutable actions and unmet goals.",
            "reflection_method_name": "Backprompting with sound verifier (LLM+VAL)",
            "reflection_method_description": "Generator LLM creates plans; VAL deterministically verifies plans and returns structured, sound feedback (binary validity and specific error diagnostics such as first inexecutable action or all open conditions); generator is reprompted with VAL feedback until VAL accepts or iteration limit reached. Max iterations set to 15; observed average iterations = 4.18.",
            "task_name": "Classical planning (Blocksworld)",
            "task_description": "Same Blocksworld domain and 100-instance evaluation as other conditions; VAL is used both as the verifier in this pipeline and as the ground truth for plan correctness.",
            "performance_with_reflection": "88/100 (88%) accuracy of final plans judged correct by VAL; average number of iterations observed = 4.18.",
            "performance_without_reflection": "Generator-only (no backprompting) achieved 40/100 (40%) accuracy.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External deterministic verification (VAL) provides sound, structured feedback which is appended to generator prompts (backprompting) enabling iterative refinement.",
            "number_of_iterations": 15,
            "evidence_for_improvement": "Quantitative: LLM+VAL backprompting raised generator accuracy from 40% to 88%, far exceeding LLM+LLM, showing that an external sound verifier substantially improves iterative refinement outcomes.",
            "limitations_or_failure_cases": "Paper does not report catastrophic failure modes for LLM+VAL on this domain; however, authors note that feedback granularity (binary vs first-error vs all-errors) produced only modest differences in generator performance in some splits (see Table 3), and reported denominators vary (50 vs 100) across those feedback-level experiments.",
            "comparison_to_other_methods": "Outperforms both LLM+LLM backprompting (88% vs 55%) and generator-only (88% vs 40%). The authors attribute the advantage to VAL's soundness and low false-positive rate.",
            "ablation_study_results": "Paper reports experiments varying feedback levels for the LLM+VAL setup: No feedback 40/100 (40%) avg steps 1.00; Only binary feedback 37/50 (74%) avg steps 5.38; Binary + first-error (VAL) 43/50 (86%) avg 4.18; Binary + all-errors 43/50 (86%) avg 4.42. Authors conclude feedback granularity had limited additional impact beyond accurate binary feedback and sufficient reprompting.",
            "uuid": "e8720.1"
        },
        {
            "name_short": "Verifier LLM (binary verification)",
            "name_full": "GPT-4 used as binary plan verifier (self-verification)",
            "brief_description": "GPT-4 used in a zero-shot verifier role to decide whether a proposed plan is valid and to optionally provide feedback; evaluated against a sound external verifier (VAL) to assess verifier reliability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 used as a zero-shot verifier for plans produced by the generator LLM; run deterministic (temperature 0); no further model details provided.",
            "reflection_method_name": "Binary self-verification / self-critiquing (verifier LLM)",
            "reflection_method_description": "Verifier is prompted with domain, instance and candidate plan and asked to judge plan validity (binary) and optionally give free-form feedback (no format constraints). The system halts when verifier signals validity or iteration cap reached.",
            "task_name": "Plan verification on Blocksworld",
            "task_description": "Binary classification of candidate plans as valid/invalid according to ground truth provided by the external sound verifier VAL; evaluated on the final plans from the LLM+LLM pipeline across 100 instances.",
            "performance_with_reflection": "As the verifier itself: overall accuracy 61/100 (61%). Confusion counts against VAL ground truth: True Positive = 54/55 (98.2%); False Negative = 1/55 (1.8%); False Positive = 38/45 (84.45% of negatives were incorrectly labeled positive); True Negative = 7/45 (15.55%).",
            "performance_without_reflection": "Not applicable (this entry describes verifier behavior); ground-truth comparator is VAL.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Zero-shot prompting of GPT-4 to perform binary verification and produce feedback; free-form feedback is then used to reprompt the generator.",
            "number_of_iterations": null,
            "evidence_for_improvement": "No evidence that using GPT-4 as verifier reliably improves correctness; although verifier rarely misses true valid plans (low false-negative rate), it generates many false positives allowing invalid plans to be accepted.",
            "limitations_or_failure_cases": "High false-positive rate (38 false positives out of 45 ground-truth invalid plans) — i.e., the LLM often incorrectly deems invalid plans valid, which is particularly dangerous in correctness-critical tasks. This deficiency is the main cause the authors identify for the LLM+LLM system's poor reliability.",
            "comparison_to_other_methods": "When the verifier role is instead played by the sound VAL tool, the overall system performance is far better (LLM+VAL accuracy 88% vs LLM+LLM 55%), indicating GPT-4's verifier shortcomings are a major limiting factor.",
            "ablation_study_results": null,
            "uuid": "e8720.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Large language models are reasoners with self-verification",
            "rating": 2,
            "sanitized_title": "large_language_models_are_reasoners_with_selfverification"
        },
        {
            "paper_title": "Lm vs lm: Detecting factual errors via cross examination",
            "rating": 1,
            "sanitized_title": "lm_vs_lm_detecting_factual_errors_via_cross_examination"
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 1,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        }
    ],
    "cost": 0.010955,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can Large Language Models Really Improve by Self-critiquing Their Own Plans?
12 Oct 2023</p>
<p>Karthik Valmeekam kvalmeek@asu.edu 
Equal Contribution Preprint
Under Review</p>
<p>Matthew Marquez mmarqu22@asu.edu 
Equal Contribution Preprint
Under Review</p>
<p>Subbarao Kambhampati </p>
<p>School of Computing &amp; AI
Arizona State University Tempe</p>
<p>School of Computing &amp; AI
Arizona State University
Tempe</p>
<p>School of Computing &amp; AI
Arizona State University
Tempe</p>
<p>Can Large Language Models Really Improve by Self-critiquing Their Own Plans?
12 Oct 20230D2A42F7FC2C2F57F9FE679C62CC224BarXiv:2310.08118v1[cs.AI]
There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode.Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning.We evaluate a planning system that employs LLMs for both plan generation and verification.We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance.Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability.Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation.Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.</p>
<p>Introduction</p>
<p>Large Language Models have rapidly captured the attention of the AI research community with their exceptional natural language completion capabilities.Trained on web-scale language corpora, these models have demonstrated the ability to generate seemingly valuable completions across a wide range of topics.This led to a surge of interest in determining whether such models were able to perform well on reasoning tasks.Even though initial anecdotal results showed promise, systematic studies revealed their incompetency in reasoning -be it planning [12] or in simple arithmetic or logic [3].These results questioning the robustness of their reasoning abilities led to researchers exploring ways to improve these systems.Of particular interest to us is the emerging research on self-critiquing, where the LLMs are used to critique their own candidate generations and iterate.The current works [15,10,14] exhibit considerable optimism about using LLMs to critique their own candidate generations, especially in an iterative setting where they keep refining their candidate generations.Additionally, the notion that verifying correctness is computationally simpler than generation for reasoning adds to the optimism.However, there are grounds to be skeptical about it as Intrigued by the prevailing optimism, in this paper, we set out to systematically investigate the effectiveness of using LLMs to critique their own generations in the context of planning.We look at the simplest class of planning problems, the goal-directed deterministic planning problems colloquially referred to as classical planning problems.Our methodology employs a planning system that utilizes the same LLM for both generation and verification, which we term the LLM+LLM system in an iterative setting.Within this setting, the generator LLM continuously produces candidate plans, drawing upon feedback from the verifier LLM, until the verifier LLM either approves a candidate plan as correct or the number of iterations surpasses a predefined threshold.We present an empirical evaluation of (i) the effect of self-critiquing on the plan generation performance of the overall LLM+LLM system (ii) the performance of the verifier LLM in comparison to the ground-truth verification and finally (iii) the influence of varying feedback levels while critiquing the LLM's generation on the overall system performance.For our study, we use GPT-4 [9] as both the generator and verifier.</p>
<p>Our findings suggest that self-critiquing degrades the plan generation performance compared to when an external, sound verifier is utilized.This decline in performance can be directly attributed to the verifier LLM's subpar results.The verifier LLM yields a significant number of false positives, which can severely undermine the system's reliability.Furthermore, we explored whether the nature of feedback on invalid plans influences plan generation performance.Our results indicate that the type of feedback-whether it's merely binary verification or combined with detailed feedback on the errors of the generated plan-doesn't significantly impact plan generation performance.Thus, our systematic investigation offers compelling preliminary evidence to question the efficacy of LLMs as verifiers for planning tasks within an iterative, self-critiquing framework.In the rest of the paper, we first present the related work, then the required background before delving into the methodology and the evaluation.</p>
<p>Related Work</p>
<p>There has been significant interest in investigating the reasoning capabilities of LLMs, spanning from planning [12] to logic and arithmetic [3], and even puzzles [15].As the initial excitement from triumphant anecdotes about LLMs' reasoning capabilities began to wane with systematic studies [12,11,3], researchers proposed that allowing LLMs to verify their own candidate solutions and iterate over this process could enhance their reasoning abilities [10,7,6,14].Our work systematically investigates the effect of iterative self-critiquing in the context of planning.</p>
<p>There have also been studies that utilize multiple LLMs to generate and verify candidate solutions, either in the form of a debate [2] or through cross-examination [1].However, these studies still rely solely on the verification/self-critiquing abilities of the LLMs, an aspect our work critically examines in the context of planning.Our results provide compelling reasons to question the use of LLMs for self-critiquing in planning.</p>
<p>Background</p>
<p>We specifically are interested in classical planning problems that are represented within the PDDL (Planning Domain and Definition Language) framework [8].These problem classes consist of a domain, initial state and a goal state.The domain consists of a set of predicates and a set of actions.The state-space of the planning problem is represented with some truth-assignment on the predicates.Every action in domain have a set of preconditions which determine when an action can be applied and a set of effects which determine the modifications to the state after the action is applied.A plan here is a sequence of actions which are present in the domain that when executed in the initial state, satisfy the goal conditions.The LLM+LLM planning system (as shown in Figure 1) consists of a generator LLM and a verifier LLM.For a given instance, the generator LLM produces a candidate plan, while the verifier LLM determines its correctness.If the plan is found to be incorrect, the verifier provides feedback detailing the reasons for its failure.This feedback is then relayed to the generator LLM, prompting the generation of a new candidate plan.It's worth noting that there are no constraints on the type or format of feedback the verifier LLM produces.The system ceases generation either when the verifier LLM approves the candidate plan as valid or when the number of prompting iterations exceeds a set threshold (for our experiments, this threshold is set at 15 iterations).This method is similar to the backprompting technique described in [12].However, the main distinction lies in the type of verifier employed.In our system, both the verifier and generator are LLMs, whereas the referenced approach utilizes an external sound verifier, VAL [4].For all our experiments, GPT-4 serves as the default LLM.</p>
<p>Prompt generation</p>
<p>For the LLM+LLM Planning system described above, we utilize distinct prompts for the generator and verifier LLMs.The prompt generator (as shown in Figure 1) utilizes the PDDL domain and instance files to generate the required prompts in natural language.Our prompts are structured similarly to the natural language prompts found in [12].For plan generation, our prompts are one-shot: we begin by presenting the domain description, followed by an example instance (along with its corresponding plan).We then present the query instance.These example instances are randomly selected from our set of instances, and this forms the input for the generator LLM.For the verifier LLM, we adopt a zero-shot approach.Here, we present the domain description, followed by the query instance and its corresponding plan.The verifier LLM is then tasked with verifying the query plan and providing feedback if necessary.As mentioned earlier, we do not restrict the type or format of the feedback for the verifier LLM.Detailed examples of the prompts given to both the generator and verifier LLMs can be found in the Appendix.</p>
<p>Evaluation and Analysis</p>
<p>We evaluate our planning system on Blocksworld, a widely recognized common-sense planning domain in AI planning literature [5].We generate 100 random instances for evaluation across various methods.To provide a ground-truth assessment of the final LLM plan's correctness, we employ an external sound verifier, VAL [4].For all experiments, GPT-4 [9] serves as the chosen LLM and was run with a temperature of 0, thereby making it deterministic.</p>
<p>Effect of self-critiquing on plan generation</p>
<p>We assessed the impact of self-critiquing on plan generation by comparing the LLM+LLM backprompting system with two other baselines.The first baseline is the LLM+VAL backprompting system, which mirrors the backprompting method described in [12].In this method, the plan produced by the LLM is validated by an external sound verifier, VAL.If the plan is found lacking, the generator-LLM is reprompted using feedback from VAL.The second baseline involves a generator-LLM without backprompting.Here, the generator LLM receives a single prompt, and the resulting plan is considered final.</p>
<p>As illustrated in Table 1, the LLM+LLM backprompting approach slightly outperforms the nonbackprompting method in terms of accuracy.However, it falls short when compared to the LLM+VAL system.It's worth noting that the marginal improvement over the generator-LLM-only method might not solely be attributed to the LLM verifier.The backprompting itself, which offers the generator LLM multiple opportunities to produce a plan, could be a contributing factor.The subpar performance of the LLM+LLM system, especially when compared to LLM+VAL, can likely be traced back to the substantial number of type-1 errors produced by the LLM verifier.It's evident that incorporating a sound verifier in the backprompting process can significantly enhance overall performance.</p>
<p>Plan Generation</p>
<p>Analysis on the self-critique verifier</p>
<p>We base our evaluation of the verifier LLM on its binary verification (i.e., determining whether the plan is valid or not) of the final plan produced by the LLM+LLM system.It's important to note that the system halts either when the verifier LLM considers the plan valid or when the number of iterations surpasses 15.We compare the LLM verifier's output with ground truth classifications made using VAL [4], a sound verifier.To make the ground truth determination available for each input plan, we separately evaluate that plan using VAL as well.</p>
<p>As illustrated in Table 2, out of the 100 instances, the verifier accurately identifies 61 (or 61%).However, a deeper examination of the verifier's errors reveals a concerning number of false positives.In this context, a false positive refers to the verifier LLM deeming a generated plan valid when, in fact, it is not.Out of the 100 instances, the verifier LLM produces 54 true positives and 38 false positives (type-1 errors).This indicates that the verifier deemed 38 plans, which were actually invalid, to be valid which can be catastrophic if such a system is deployed in scenarios where correctness is paramount.</p>
<p>Accuracy</p>
<p>Effect of the levels of feedback on plan generation</p>
<p>While the use of a sound verifier appears to enhance overall performance, we sought to further investigate the impact of varied levels of feedback on plan generation performance.We assessed the system's performance across four distinct feedback levels:</p>
<p>1.No Feedback: At this level, the initial plan generated by the LLM is considered to be final and no feedback is provided to the LLM. 2. Binary Feedback: This level simply indicates whether the generated plan is valid or not.3. Inexecutable Action Feedback: If the plan is invalid and inexecutable, this feedback highlights the first inexecutable action and the unmet preconditions causing the inexecutability.If the plan is executable but fails to meet all goal conditions, the unmet goal conditions are presented.This feedback mirrors what VAL provides.4. Open Conditions Feedback: This level treats the plan as a partial-order plan [13] and presents all the actions for which there exists atleast one unmet pre-condition and the corresponding unmet preconditions.Further it also presents the unmet goal conditions.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we conducted a systematic investigation into the ability of Large Language Models (LLMs) to critique their own outputs, specifically within the context of classical planning problems.While recent research has been optimistic about LLMs' potential in self-critiquing, especially in iterative settings, our findings present a different perspective.</p>
<p>Our empirical evaluations on Blocksworld, a simple common-sense domain, highlighted the ineffectiveness of self-critiquing in LLMs in the context of planning.We showed that the verifier LLM generates a significant number of false positives which be detrimental to the overall system's reliability, particularly in domains where the correctness of plans is paramount.Interestingly, the nature of feedback, whether binary or detailed, did not have a pronounced impact on plan generation performance, suggesting that the core issue lies in the LLM's binary verification capabilities rather than the granularity of feedback.</p>
<p>In the future, we plan to conduct more extensive experiments with respect to the number of instances, the number of domains and prompting methods (such as chain-of-thought).</p>
<p>Figure 1 :
1
Figure 1: Overall evaluation architecture</p>
<p>Table 1 :
1
Comparison between various plan generation methods on the Blocksworld domain.
MethodAccuracyAvg. Number of iterationsLLM+LLM w/ Backprompting (BP) 55/100 (55%)3.48LLM+VAL w/ BP88/100 (88%)4.18Generator LLM only w/o BP40/100 (40%)1.00</p>
<p>Table 2 :
2
Breakdown of Plan Verification results on Blocksworld domain.The denominators (in aspects other than Accuracy) are ground-truth values based on VAL.
True PositiveFalse PositiveTrue NegativeFalse NegativeRateRateRateRateVerifier61/100 (61%) 54/55 (98.2%) 38/45 (84.45%) 7/45 (15.55%)1/55 (1.8%)LLM</p>
<p>Table 3
3
showcases the LLM's performance when subjected to various levels of feedback (including one with no feedback).Interestingly, the amount of feedback provided to the LLM seems to have minimal influence on its performance improvement.As long as the binary feedback is accurate and the LLM is given ample opportunities to generate a plan, the detailed feedback on invalid plans doesn't appear to significantly enhance the LLM's performance.We have provided examples for each feedback level in the Appendix.
Levels of feedbackAccuracyAvg. no ofstepsNo feedback40/100 (40%)1.00Only binary feedback37/50 (74%)5.38Binary + First error feedback (by VAL) 43/50 (86%)4.18Binary + All errors feedback43/50 (86%)4.42</p>
<p>Table 3 :
3
Performance of LLM+VAL system on plan generation with varied levels of feedback.</p>
<p>Lm vs lm: Detecting factual errors via cross examination. Roi Cohen, May Hamri, Mor Geva, Amir Globerson, arXiv:2305.132812023arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jian, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Hwang, arXiv:2305.186542023arXiv preprint</p>
<p>VAL: Automatic plan validation, continuous effects and mixed initiative planning using PDDL. Richard Howey, Derek Long, Maria Fox, 16th IEEE International Conference on Tools with Artificial Intelligence. IEEE2004</p>
<p>. IPC. International planning competition. 1998</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Pddl-the planning domain definition language. Drew Mcdermott, Malik Ghallab, Adele E Howe, Craig A Knoblock, Ashwin Ram, Manuela M Veloso, Daniel S Weld, David E Wilkins, 1998</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>PDDL planning with pretrained large language models. Tom Silver, Varun Hariprasad, Reece S Shuttleworth, Nishanth Kumar, Tomás Lozano-Pérez, Leslie Pack, Kaelbling , NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>On the planning abilities of large language models-a critical investigation. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2305.157712023arXiv preprint</p>
<p>An introduction to least commitment planning. Daniel S Weld, AI magazine. 1541994</p>
<p>Large language models are reasoners with self-verification. Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao, arXiv:2212.095612022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
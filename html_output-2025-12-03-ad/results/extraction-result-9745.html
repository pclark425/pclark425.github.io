<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9745 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9745</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9745</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-278129451</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.18413v1.pdf" target="_blank">An Empirical Study of Evaluating Long-form Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> Long-form question answering (LFQA) aims to generate lengthy answers to complex questions. This scenario presents great flexibility as well as significant challenges for evaluation. Most evaluations rely on deterministic metrics that depend on string or n-gram matching, while the reliability of large language model-based evaluations for long-form answers remains relatively unexplored. We address this gap by conducting an in-depth study of long-form answer evaluation with the following research questions: (i) To what extent do existing automatic evaluation metrics serve as a substitute for human evaluations? (ii) What are the limitations of existing evaluation metrics compared to human evaluations? (iii) How can the effectiveness and robustness of existing evaluation methods be improved? We collect 5,236 factoid and non-factoid long-form answers generated by different large language models and conduct a human evaluation on 2,079 of them, focusing on correctness and informativeness. Subsequently, we investigated the performance of automatic evaluation metrics by evaluating these answers, analyzing the consistency between these metrics and human evaluations. We find that the style, length of the answers, and the category of questions can bias the automatic evaluation metrics. However, fine-grained evaluation helps mitigate this issue on some metrics. Our findings have important implications for the use of large language models for evaluating long-form question answering. All code and datasets are available at https://github</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9745.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9745.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based evaluations (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model based evaluators for long-form QA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluators that use large pretrained language models (e.g., GPT-4o, Claude-3.5, Gemini-2.0) to assign quality scores to long-form answers (coarse- and fine-grained prompting). The paper compares these LLM-as-a-judge methods with human judgments across correctness and informativeness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-form question answering (LFQA)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o, Claude-3.5, Gemini-2.0 (primary examples), also GPT-4/GPT-3.5 in context</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompt-driven evaluation following LLM-EVAL style: LLMs are given task instructions and asked to rate answers on 1–5 star scales for correctness and informativeness in both coarse-grained and fine-grained settings; multiple prompt variants (P1–P9) were tested varying Task/Data/Output/Criteria order and content.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human evaluation performed on a subset of generated answers (2,079 answer-pairs; 4,158 ratings/justifications) using a web interface; each generated answer was rated on two axes (correctness and informativeness) on a 1–5 scale with textual definitions for each star level; samples drawn from ASQA/ANTIQUE/WikiEval with 7 model answers per question.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman and Kendall correlation coefficients between automatic (LLM) scores and human ratings; pairwise win-rate and agreement when comparing model outputs. Reported example: GPT-4o Spearman ~42.0 (coarse) improving to ~55.0 (fine-grained) when compared to human evaluations; Claude-3.5 and Gemini-2.0 Spearman roughly 33.0 and 32.4 respectively (paper summary values).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When replacing humans with LLM judges, several qualitative and quantitative differences are observed: (1) LLM scores are more stringent and have a different score distribution (LLM scores cluster lower, e.g., around 3, while human scores cluster higher, e.g., around 5), (2) LLMs are sensitive to prompt wording and evaluation hyperparameters (score variability with prompt length and sampling temperature), (3) LLM-based evaluators exhibit evaluation biases (favor longer answers, favor lexically sophisticated/rare-word responses under some settings, and favour their own model outputs), (4) LLMs have difficulty consistently identifying certain factual inaccuracies, especially in evidence-based or experience-type questions, compared to humans, and (5) deterministic reference-based metrics degrade much more than LLM-based metrics for open-ended QA, but LLM-judges introduce new kinds of bias and instability.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Distributional/strictness difference: LLM-based scores concentrate near middle (majority around 3) while human ratings concentrate near higher scores (~5) (Section 'Robustness' and Figure 2). Factuality detection: LLM-based metrics perform worse on identifying factual errors in evidence-based and experience-type questions (Section 3.4.2). Length and lexical bias: LLMs are less likely than humans to give low scores to longer answers and (under fine-grained settings) GPT-4o favors higher-IDF (rarer-term) responses (Figures 3 and 5). Self-preference: evaluators give higher win-rates to their own generations (e.g., Gemini self-evaluation numbers cited as examples 0.468 vs. 0.262/0.256 and 0.388 vs. 0.163/0.06) (Section 3.4.3). Prompt/hyperparameter sensitivity: GPT-4o rank changed with temperature (example: rank dropped from 4th to 6th when temperature increased 0.0→0.3, improved to 2nd at 0.7/1.0) (Section 'Robustness').</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>LLM-based metrics often align better with humans than deterministic n-gram/embedding methods across diverse LFQA styles and are more stable across question types; fine-grained prompting substantially improves alignment (e.g., GPT-4o Spearman improves from ~42.0 to ~55.0); deterministic Exact Match can outperform LLM-evaluators on datasets with very short/closed answers (ASQA), and LLM-based methods show no consistent tendency to penalize longer answers in some datasets (contrast to deterministic metrics which do penalize length). The paper also notes model- and dataset-dependent effects and that using multiple evaluators and ranking-based comparisons mitigates some issues.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections: 'Abstract', 'Introduction', 'Accuracy of Automatic Metrics', 'Robustness of Automatic Metrics', 'Fairness of Automatic Metrics', and 'Conclusion' in An Empirical Study of Evaluating Long-form Question Answering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9745.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9745.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o judge (prompt & temp sensitivity)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o used as an LLM-as-a-judge (sensitivity to prompting and sampling temperature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o was used as a primary LLM judge in coarse- and fine-grained configurations; the study measured how prompt length and sampling temperature affect its scoring distribution and rankings compared to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-form question answering (LFQA)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompt-driven evaluation (short/normal/long prompts) in both coarse-grained and fine-grained modes; experiments also varied the judge's sampling temperature (0.0, 0.3, 0.7, 1.0) to test robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same human evaluation described above (subset of answers rated for correctness and informativeness with 1–5 star scales; used as ground truth for alignment measures).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman/Kendall correlation vs human ratings; win-rate and rank comparisons under different prompt/hyperparameter settings. Examples: GPT-4o Spearman ≈ 42.0 (coarse), improved to ≈ 55.0 with fine-grained prompting; temperature changes caused rank shifts in WikiEval (rank 4→6 at temp 0.3, rank improved to 2 at temp 0.7/1.0).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Changing prompt length and temperature produces notable shifts in scores and model rankings (instability relative to human judgments); GPT-4o tends to give systematically lower absolute scores than humans (stringency), and short prompts produce more high scores than longer prompts (prompt-length bias). These sensitivities mean that some evaluative nuance present in human scoring may be lost or inconsistent when relying on a single LLM judge without careful prompt/hyperparameter control.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Score distribution mismatch: under several prompt variants, LLM-based scores cluster near the mid-scale (~3) while human scores cluster near the high end (~5) (Figure 2). Prompt-length effect: short prompts produced more samples with scores >4 than normal/long prompts. Temperature effect: in WikiEval GPT-4o's rank moved 4→6 when temperature 0.0→0.3 and then improved to 2 at higher temperatures (Section 'Robustness').</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Fine-grained prompting (providing explicit evaluation criteria and structured output) substantially improved GPT-4o alignment with humans (Spearman improved from ~42.0 to ~55.0). In some datasets (ASQA), GPT-4o was more stable and produced rankings consistent with humans despite score-level differences.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections: 'Robustness of Automatic Metrics' (Prompt perturbation; Hyper-parameter perturbation), 'Accuracy of Automatic Metrics', Figures 2 and Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9745.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9745.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-reinforcement bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM evaluators favoring their own model outputs (self-preference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed bias where closed-source LLM evaluators (GPT-4o, Claude-3.5, Gemini-2.0) give higher scores/win-rates to outputs produced by the same model family than when those outputs are evaluated by other LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-form question answering (LFQA)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o, Claude-3.5, Gemini-2.0 (as both generators and judges in tests)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise comparisons on ASQA where each of these closed-source LLMs both generated answers and served as the judge; win-rates computed against seven open-source baseline LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human evaluations used as baseline alignment measure; the self-preference analysis focused on differences between LLM-evaluators' pairwise win-rates rather than direct human disagreement counts.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Win-rate comparisons and evaluator-to-evaluator agreement for pairwise model comparisons; numeric examples provided showing higher self-win rates (e.g., Gemini self-evaluation examples 0.468 vs. 0.262/0.256 and 0.388 vs. 0.163/0.06 when compared against other evaluators).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as judges can introduce a self-reinforcing bias: an LLM judge tends to systematically prefer outputs generated by the same model family, distorting comparative evaluations and potentially masking real quality differences detectable by humans. This reduces fairness and trustworthiness of single-LMM evaluations for cross-model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Specific numeric comparisons show that each closed-source model assigns substantially higher win-rates to its own outputs than other LLM judges do (Section 3.4.3 and Figure 6). The paper reports that GPT-4o and Claude-3.5 evaluations each show strong bias towards their own responses, and Gemini-2.0 similarly gives much higher self-scores compared to GPT-4o and Claude-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Despite self-bias in absolute scores, model rankings across evaluators remain largely consistent; authors recommend using rankings rather than absolute scores and employing multiple evaluation methods to mitigate this bias.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Fairness of Automatic Metrics' subsection 'Self-reinforcing' and Figure 6.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9745.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9745.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Factuality detection gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-judge difficulty identifying fine-grained factual errors in LFQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed limitation where LLM-based evaluators are less consistent than humans at detecting factual inaccuracies, especially in question types requiring precise evidence or personal experience reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Empirical Study of Evaluating Long-form Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-form question answering (LFQA), particularly evidence-based and experience-type questions</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o, Claude-3.5, Gemini-2.0 (evaluated across question-type subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLM metrics evaluated per question-type (ANTIQUE taxonomy) with Kendall correlations computed separately for correctness and informativeness; fine-grained prompting variants tested.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotators rated correctness and informativeness; question types categorized (reason, instruction, evidence-based, experience, debate, comparison) and correlations computed per type against LLM scores.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall correlation per question type between human judgments and LLM metrics (Figure 4). Result: LLM-based metrics outperform deterministic ones overall, but show lower consistency on correctness for evidence-based and experience-type questions; deterministic metrics sometimes have negative correlations on Antique.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM-judges are relatively weaker at pinpointing factual errors in answers that require external evidence or personal-experience reasoning; this means subtle factual mistakes that human annotators spot can be missed or underweighted by LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>On ANTIQUE, evaluation consistency for correctness is lower for evidence-based and experience-type questions across the three main LLM evaluators (GPT-4, Claude-3.5, Gemini-2.0) compared to other question types (Section 3.4.2 and Figure 4). The paper explicitly notes LLM difficulty identifying factual inaccuracies as a possible cause for lower correctness consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>LLM-based metrics still outperform deterministic n-gram/embedding metrics on many question types and are slightly better at assessing informativeness than correctness; fine-grained prompting can improve detection and alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 'Fairness of Automatic Metrics' subsection 'Question type' (3.4.2) and Figure 4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study of Evaluating Long-form Question Answering', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Survey on LLM-as-a-Judge <em>(Rating: 2)</em></li>
                <li>Can Large Language Models Be an Alternative to Human Evaluations? <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Can we trust the evaluation on ChatGPT? <em>(Rating: 2)</em></li>
                <li>Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9745",
    "paper_id": "paper-278129451",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM-based evaluations (general)",
            "name_full": "Large Language Model based evaluators for long-form QA",
            "brief_description": "Evaluators that use large pretrained language models (e.g., GPT-4o, Claude-3.5, Gemini-2.0) to assign quality scores to long-form answers (coarse- and fine-grained prompting). The paper compares these LLM-as-a-judge methods with human judgments across correctness and informativeness.",
            "citation_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "mention_or_use": "use",
            "task_domain": "Long-form question answering (LFQA)",
            "llm_judge_model": "GPT-4o, Claude-3.5, Gemini-2.0 (primary examples), also GPT-4/GPT-3.5 in context",
            "llm_judge_setup": "Prompt-driven evaluation following LLM-EVAL style: LLMs are given task instructions and asked to rate answers on 1–5 star scales for correctness and informativeness in both coarse-grained and fine-grained settings; multiple prompt variants (P1–P9) were tested varying Task/Data/Output/Criteria order and content.",
            "human_evaluation_setup": "Human evaluation performed on a subset of generated answers (2,079 answer-pairs; 4,158 ratings/justifications) using a web interface; each generated answer was rated on two axes (correctness and informativeness) on a 1–5 scale with textual definitions for each star level; samples drawn from ASQA/ANTIQUE/WikiEval with 7 model answers per question.",
            "agreement_metric": "Spearman and Kendall correlation coefficients between automatic (LLM) scores and human ratings; pairwise win-rate and agreement when comparing model outputs. Reported example: GPT-4o Spearman ~42.0 (coarse) improving to ~55.0 (fine-grained) when compared to human evaluations; Claude-3.5 and Gemini-2.0 Spearman roughly 33.0 and 32.4 respectively (paper summary values).",
            "losses_identified": "When replacing humans with LLM judges, several qualitative and quantitative differences are observed: (1) LLM scores are more stringent and have a different score distribution (LLM scores cluster lower, e.g., around 3, while human scores cluster higher, e.g., around 5), (2) LLMs are sensitive to prompt wording and evaluation hyperparameters (score variability with prompt length and sampling temperature), (3) LLM-based evaluators exhibit evaluation biases (favor longer answers, favor lexically sophisticated/rare-word responses under some settings, and favour their own model outputs), (4) LLMs have difficulty consistently identifying certain factual inaccuracies, especially in evidence-based or experience-type questions, compared to humans, and (5) deterministic reference-based metrics degrade much more than LLM-based metrics for open-ended QA, but LLM-judges introduce new kinds of bias and instability.",
            "examples_of_loss": "Distributional/strictness difference: LLM-based scores concentrate near middle (majority around 3) while human ratings concentrate near higher scores (~5) (Section 'Robustness' and Figure 2). Factuality detection: LLM-based metrics perform worse on identifying factual errors in evidence-based and experience-type questions (Section 3.4.2). Length and lexical bias: LLMs are less likely than humans to give low scores to longer answers and (under fine-grained settings) GPT-4o favors higher-IDF (rarer-term) responses (Figures 3 and 5). Self-preference: evaluators give higher win-rates to their own generations (e.g., Gemini self-evaluation numbers cited as examples 0.468 vs. 0.262/0.256 and 0.388 vs. 0.163/0.06) (Section 3.4.3). Prompt/hyperparameter sensitivity: GPT-4o rank changed with temperature (example: rank dropped from 4th to 6th when temperature increased 0.0→0.3, improved to 2nd at 0.7/1.0) (Section 'Robustness').",
            "counterexamples_or_caveats": "LLM-based metrics often align better with humans than deterministic n-gram/embedding methods across diverse LFQA styles and are more stable across question types; fine-grained prompting substantially improves alignment (e.g., GPT-4o Spearman improves from ~42.0 to ~55.0); deterministic Exact Match can outperform LLM-evaluators on datasets with very short/closed answers (ASQA), and LLM-based methods show no consistent tendency to penalize longer answers in some datasets (contrast to deterministic metrics which do penalize length). The paper also notes model- and dataset-dependent effects and that using multiple evaluators and ranking-based comparisons mitigates some issues.",
            "paper_reference": "Sections: 'Abstract', 'Introduction', 'Accuracy of Automatic Metrics', 'Robustness of Automatic Metrics', 'Fairness of Automatic Metrics', and 'Conclusion' in An Empirical Study of Evaluating Long-form Question Answering.",
            "uuid": "e9745.0",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT-4o judge (prompt & temp sensitivity)",
            "name_full": "GPT-4o used as an LLM-as-a-judge (sensitivity to prompting and sampling temperature)",
            "brief_description": "GPT-4o was used as a primary LLM judge in coarse- and fine-grained configurations; the study measured how prompt length and sampling temperature affect its scoring distribution and rankings compared to human judgments.",
            "citation_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "mention_or_use": "use",
            "task_domain": "Long-form question answering (LFQA)",
            "llm_judge_model": "GPT-4o",
            "llm_judge_setup": "Prompt-driven evaluation (short/normal/long prompts) in both coarse-grained and fine-grained modes; experiments also varied the judge's sampling temperature (0.0, 0.3, 0.7, 1.0) to test robustness.",
            "human_evaluation_setup": "Same human evaluation described above (subset of answers rated for correctness and informativeness with 1–5 star scales; used as ground truth for alignment measures).",
            "agreement_metric": "Spearman/Kendall correlation vs human ratings; win-rate and rank comparisons under different prompt/hyperparameter settings. Examples: GPT-4o Spearman ≈ 42.0 (coarse), improved to ≈ 55.0 with fine-grained prompting; temperature changes caused rank shifts in WikiEval (rank 4→6 at temp 0.3, rank improved to 2 at temp 0.7/1.0).",
            "losses_identified": "Changing prompt length and temperature produces notable shifts in scores and model rankings (instability relative to human judgments); GPT-4o tends to give systematically lower absolute scores than humans (stringency), and short prompts produce more high scores than longer prompts (prompt-length bias). These sensitivities mean that some evaluative nuance present in human scoring may be lost or inconsistent when relying on a single LLM judge without careful prompt/hyperparameter control.",
            "examples_of_loss": "Score distribution mismatch: under several prompt variants, LLM-based scores cluster near the mid-scale (~3) while human scores cluster near the high end (~5) (Figure 2). Prompt-length effect: short prompts produced more samples with scores &gt;4 than normal/long prompts. Temperature effect: in WikiEval GPT-4o's rank moved 4→6 when temperature 0.0→0.3 and then improved to 2 at higher temperatures (Section 'Robustness').",
            "counterexamples_or_caveats": "Fine-grained prompting (providing explicit evaluation criteria and structured output) substantially improved GPT-4o alignment with humans (Spearman improved from ~42.0 to ~55.0). In some datasets (ASQA), GPT-4o was more stable and produced rankings consistent with humans despite score-level differences.",
            "paper_reference": "Sections: 'Robustness of Automatic Metrics' (Prompt perturbation; Hyper-parameter perturbation), 'Accuracy of Automatic Metrics', Figures 2 and Table 3.",
            "uuid": "e9745.1",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Self-reinforcement bias",
            "name_full": "LLM evaluators favoring their own model outputs (self-preference)",
            "brief_description": "Observed bias where closed-source LLM evaluators (GPT-4o, Claude-3.5, Gemini-2.0) give higher scores/win-rates to outputs produced by the same model family than when those outputs are evaluated by other LLM judges.",
            "citation_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "mention_or_use": "use",
            "task_domain": "Long-form question answering (LFQA)",
            "llm_judge_model": "GPT-4o, Claude-3.5, Gemini-2.0 (as both generators and judges in tests)",
            "llm_judge_setup": "Pairwise comparisons on ASQA where each of these closed-source LLMs both generated answers and served as the judge; win-rates computed against seven open-source baseline LLMs.",
            "human_evaluation_setup": "Human evaluations used as baseline alignment measure; the self-preference analysis focused on differences between LLM-evaluators' pairwise win-rates rather than direct human disagreement counts.",
            "agreement_metric": "Win-rate comparisons and evaluator-to-evaluator agreement for pairwise model comparisons; numeric examples provided showing higher self-win rates (e.g., Gemini self-evaluation examples 0.468 vs. 0.262/0.256 and 0.388 vs. 0.163/0.06 when compared against other evaluators).",
            "losses_identified": "Using LLMs as judges can introduce a self-reinforcing bias: an LLM judge tends to systematically prefer outputs generated by the same model family, distorting comparative evaluations and potentially masking real quality differences detectable by humans. This reduces fairness and trustworthiness of single-LMM evaluations for cross-model comparisons.",
            "examples_of_loss": "Specific numeric comparisons show that each closed-source model assigns substantially higher win-rates to its own outputs than other LLM judges do (Section 3.4.3 and Figure 6). The paper reports that GPT-4o and Claude-3.5 evaluations each show strong bias towards their own responses, and Gemini-2.0 similarly gives much higher self-scores compared to GPT-4o and Claude-3.5.",
            "counterexamples_or_caveats": "Despite self-bias in absolute scores, model rankings across evaluators remain largely consistent; authors recommend using rankings rather than absolute scores and employing multiple evaluation methods to mitigate this bias.",
            "paper_reference": "Section 'Fairness of Automatic Metrics' subsection 'Self-reinforcing' and Figure 6.",
            "uuid": "e9745.2",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Factuality detection gap",
            "name_full": "LLM-as-judge difficulty identifying fine-grained factual errors in LFQA",
            "brief_description": "Observed limitation where LLM-based evaluators are less consistent than humans at detecting factual inaccuracies, especially in question types requiring precise evidence or personal experience reasoning.",
            "citation_title": "An Empirical Study of Evaluating Long-form Question Answering",
            "mention_or_use": "use",
            "task_domain": "Long-form question answering (LFQA), particularly evidence-based and experience-type questions",
            "llm_judge_model": "GPT-4o, Claude-3.5, Gemini-2.0 (evaluated across question-type subsets)",
            "llm_judge_setup": "LLM metrics evaluated per question-type (ANTIQUE taxonomy) with Kendall correlations computed separately for correctness and informativeness; fine-grained prompting variants tested.",
            "human_evaluation_setup": "Human annotators rated correctness and informativeness; question types categorized (reason, instruction, evidence-based, experience, debate, comparison) and correlations computed per type against LLM scores.",
            "agreement_metric": "Kendall correlation per question type between human judgments and LLM metrics (Figure 4). Result: LLM-based metrics outperform deterministic ones overall, but show lower consistency on correctness for evidence-based and experience-type questions; deterministic metrics sometimes have negative correlations on Antique.",
            "losses_identified": "LLM-judges are relatively weaker at pinpointing factual errors in answers that require external evidence or personal-experience reasoning; this means subtle factual mistakes that human annotators spot can be missed or underweighted by LLM evaluators.",
            "examples_of_loss": "On ANTIQUE, evaluation consistency for correctness is lower for evidence-based and experience-type questions across the three main LLM evaluators (GPT-4, Claude-3.5, Gemini-2.0) compared to other question types (Section 3.4.2 and Figure 4). The paper explicitly notes LLM difficulty identifying factual inaccuracies as a possible cause for lower correctness consistency.",
            "counterexamples_or_caveats": "LLM-based metrics still outperform deterministic n-gram/embedding metrics on many question types and are slightly better at assessing informativeness than correctness; fine-grained prompting can improve detection and alignment.",
            "paper_reference": "Section 'Fairness of Automatic Metrics' subsection 'Question type' (3.4.2) and Figure 4.",
            "uuid": "e9745.3",
            "source_info": {
                "paper_title": "An Empirical Study of Evaluating Long-form Question Answering",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Survey on LLM-as-a-Judge",
            "rating": 2,
            "sanitized_title": "a_survey_on_llmasajudge"
        },
        {
            "paper_title": "Can Large Language Models Be an Alternative to Human Evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Can we trust the evaluation on ChatGPT?",
            "rating": 2,
            "sanitized_title": "can_we_trust_the_evaluation_on_chatgpt"
        },
        {
            "paper_title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study",
            "rating": 1,
            "sanitized_title": "exploring_the_use_of_large_language_models_for_referencefree_text_quality_evaluation_a_preliminary_empirical_study"
        }
    ],
    "cost": 0.01502475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Empirical Study of Evaluating Long-form Question Answering
25 Apr 2025</p>
<p>Ning Xian xianning21s@ict.ac.cn 0009-0004-1220-3021
Yixing Fan fanyixing@ict.ac.cn 0000-0003-4317-2702
Ruqing Zhang zhangruqing@ict.ac.cn 
Jiafeng Guo guojiafeng@ict.ac.cn 0000-0002-9509-8674</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>Maarten de Rijke University of Amsterdam Amsterdam
The Netherlands</p>
<p>Institute of Computing Technology
Chinese Academy of Sciences
BeijingChina</p>
<p>An Empirical Study of Evaluating Long-form Question Answering
25 Apr 20251452A10D249AFD6D2F2FC5E87B80DC9F10.1145/3726302.3729895arXiv:2504.18413v1[cs.IR]Long-form question answering, automatic evaluation
Long-form question answering (LFQA) aims to generate lengthy answers to complex questions.This scenario presents great flexibility as well as significant challenges for evaluation.Most evaluations rely on deterministic metrics that depend on string or n-gram matching, while the reliability of large language model-based evaluations for long-form answers remains relatively unexplored.We address this gap by conducting an in-depth study of long-form answer evaluation with the following research questions: (i) To what extent do existing automatic evaluation metrics serve as a substitute for human evaluations?(ii) What are the limitations of existing evaluation metrics compared to human evaluations?(iii) How can the effectiveness and robustness of existing evaluation methods be improved?We collect 5,236 factoid and non-factoid long-form answers generated by different large language models and conduct a human evaluation on 2,079 of them, focusing on correctness and informativeness.Subsequently, we investigated the performance of automatic evaluation metrics by evaluating these answers, analyzing the consistency between these metrics and human evaluations.We find that the style, length of the answers, and the category of questions can bias the automatic evaluation metrics.However, fine-grained evaluation helps mitigate this issue on some metrics.Our findings have important implications for the use of large language models for evaluating long-form question answering.All code and datasets are available at https://github.com/bugtig6351/lfqa_evaluation.CCS Concepts• Information systems → Evaluation of retrieval results; Question answering.</p>
<p>Introduction</p>
<p>LFQA aims to enable generation or retrieval models to answer openended questions with long answers at the paragraph level [10,20].With the advancement of large language models (LLMs), the ability to generate long-form answers has improved significantly [4,27].While such long-form answers could address more complex and diverse questions, their flexibility presents significant challenges for evaluation.</p>
<p>Currently, much of the research on LFQA focuses on designing models and frameworks to address the hallucination of LLM and improve overall performance.For example, Su et al. [34] propose a framework that uses fine-grained, answer-related salient information to enhance the faithfulness of model responses and reduce hallucinations.Tao et al. [35] introduce a chain-of-discussion framework that takes advantage of synergy among multiple open source LLMs to provide more accurate and comprehensive answers.Despite these advancements, most studies still rely on simple question answering (QA) metrics, such as ROUGE [23] and Exact Match [33], which are based on string or n-gram matching, to evaluate longform answers.However, these metrics often fail to capture the nuanced and flexible evaluation required by the complex structure of long-form answers, demonstrating weak correlation with human judgment [20,25].</p>
<p>LLM-based evaluators.Recently, there has been growing interest in developing LLM-based evaluators to provide more reliable assessments of long-form answers [7].For example, Vu et al. [38] introduce LLM autoraters by training the PaLM-2-24B model on a collection of 102 quality assessment tasks comprising more than 5.3M human judgements (i.e., FLAMe).Fan et al. [11] propose an LLM-based metric, EVA-Score, to evaluate the informativeness in abstract lonng-form summarization.Although LLMs have shown promise in providing more comprehensive evaluations of longform answers, they remain prone to vulnerabilities due to their well-known hallucination issues [39].</p>
<p>Goals and questions.In this paper, we aim to fill the above gap by conducting a comprehensive analysis of existing automatic evaluation metrics, including deterministic metrics and model-based metrics.We compare automatic metrics with human evaluations to examine their strengths and limitations.Given this setup, we explore the following three research questions: (RQ1) To what extent do existing automatic evaluation metrics serve as substitutes for human evaluations in LFQA, and how accurate are these metrics?(RQ2) What are the limitations of automatic evaluation metrics compared to human evaluators?What influences their stability and fairness?(RQ3) How can the effectiveness and robustness of existing evaluation methods be improved?</p>
<p>Main findings.Our study involves both factual and non-factual LFQA tasks, collecting answers generated by seven different LLMs on the ASQA, ANTIQUE and Wikieval datasets, and performing a human evaluation on a subset of these answers (over 2,079 answer pairs with 4,158 ratings and justifications).</p>
<p>• For RQ1, we find that metrics based on large models demonstrate significantly higher consistency with human evaluations than deterministic metrics.They are also more stable in assessing different types of LFQA.• For RQ2, we conduct an analysis from two perspectives.First, we examine the impact of meaningless minor perturbations to the prompt on evaluation results, assessing the stability of the evaluators.Second, we investigate whether factors such as text length, question type, and non-semantic variations in phrasing introduce evaluator bias, thereby affecting the fairness of the evaluators.Our findings indicate that deterministic metrics are influenced by the length of the reference text and tend to penalize longer answers.In contrast, LLM-based methods do not exhibit any significant biases in this regard.• For RQ3, our research analyzes the impact of different prompting strategies on outcomes, highlighting that fine-grained evaluation methods exhibit higher accuracy compared to similar evaluation approaches.</p>
<p>Related Work</p>
<p>This work aims to analyze the shortcomings of existing evaluations in LFQA and provide guidance for future efforts.Therefore, we review previous work, including (i) automated evaluation methods, and (ii) evaluation of evaluators.</p>
<p>Automatic Evaluation Methods</p>
<p>To evaluate the performance of large models, numerous studies have used automatic evaluation metrics across multiple independent benchmarks [2,21,22].These metrics are generally categorized into three types: n-gram-based, embedding-based, and LLM-based.Each category offers distinct approaches for assessing model responses, aiming to balance efficiency and accuracy in evaluating various aspects of generated text.N-gram-based metrics, such as ROUGE [23] and BLEU [31], are widely adopted due to their high efficiency and low cost, primarily verifying the correctness of model responses.However, these metrics can be adversely affected by syntactic errors [32] and may struggle to capture human preferences when comparing outputs from different models [8,14], with recent studies further supporting these limitations [20,43].Embedding-based methods, such as BERTScore [46] and MoverScore [48], use pretrained language models to better capture semantic similarities, thereby reducing the impact of superficial textual changes.Despite their advancements, these methods rely heavily on high-quality reference texts, posing challenges for open-ended questions where crafting appropriate references is difficult [5].</p>
<p>While embedding-based approaches improve upon traditional n-gram methods by focusing on semantic content, their dependence on reference texts limits their applicability in more flexible evaluation scenarios.On the other hand, LLM-based evaluators offer a novel approach for reference-free evaluation, demonstrating reasonable performance in zero-shot settings [1,6,26].Examples include GPTScore [12], G-EVAL [25], LLM-EVAL [24], and RAGAS [9], which leverage large language models to provide multi-dimensional evaluations.However, LLMs may inherit biases from their training data [40] and generate hallucinations [17], with studies confirming biases in these evaluators [39,40,49].Although fine-grained evaluations can help mitigate some issues [28,45], the overall reliability of automatic evaluation methods as substitutes for human evaluators remains an area requiring further investigation.</p>
<p>Evaluation of Evaluators</p>
<p>Since LLM-as-a-Judge has become a new evaluation paradigm, many studies have assessed the effectiveness of these methods.The basic approach of previous research has been to treat various types of evaluators as virtual annotators and evaluate their consistency with human annotators to achieve alignment with human [15].</p>
<p>MT-bench and the Chatbot arena dataset [49] contain a small amount of human-crafted queries annotated by experts and a large volume of crowdsourced user preferences data from real-world users, analyzing the agreement between LLM evaluators and human annotators and bias of the evaluator.Xu et al. [43] collect the consistency between automatic metrics and human experts in the evaluation of LFQA across seven different knowledge domains, focus on overall answer preference, coherence, and factuality.PandaLM [41] trains a LLM evaluator and constructed a general instruction-tuning benchmark that includes a significant amount of automated scoring and human preference data, using a partial order graph to compare the performance of the models.</p>
<p>In this work, we will adopt previous evaluation approaches for meta-evaluation, assessing the performance of evaluators based on their consistency with human annotators.We will focus on the biases and robustness exhibited by different evaluation methods throughout this process.</p>
<p>A Study of LFQA Evaluation Methods</p>
<p>In this section, we examine how existing automatic LFQA evaluation metrics compare to human evaluations from three aspects, namely accuracy, robustness, and fairness.Firstly, the accuracy is to test to what extent the automatic metrics match human judgments.Here, we use responses from seven LLMs across three benchmark datasets as testbeds to assess the alignment between automatic metrics and human evaluations.Secondly, the robustness is to test the reliability of the automatic metrics.We assess the stability of the outcome when subjected to minor perturbations in inputs and hyperparameters.Finally, the fairness is to test whether existing automatic metrics exhibit biases toward specific attributes, e.g., style, length, or topic.</p>
<p>In the following, we first introduce the setting of the empirical study, including automatic evaluation metrics, models, and the testbed.Then, we show results concerning the accuracy, robustness, and fairness of all metrics.</p>
<p>Empirical Setup</p>
<p>3.1.1Testbed.For long-form question answering, we mainly consider two categories: one requires in-depth analysis and detailed explanations, while the other includes opinions, discussions, and other scenarios closely resembling real user interactions, which are often non-factoid.Both types of questions are considered challenging for LLMs, in terms of response generation or evaluation [10,20,49].Specifically, we perform experiments on the following three datasets, each serving as a representative dataset for ambiguous, factoid, and open-ended question answering.</p>
<p>• ASQA [33] is an ambiguous factual questions dataset in which each question has multiple disambiguated question-answer pairs and two long-form grounded answers annotated by humans.• ANTIQUE [16] is an open-ended question answering dataset, including 2626 questions asked by users of Yahoo!Answers, and relevant answers annotated by human experts.• WikiEval [9] is a factoid question answering dataset generated from 50 pages from Wikipedia with edits post 2022, annotated by human experts.Using the datasets mentioned above, we gather responses from seven latest LLMs from five different families for analysis.The models are listed as follows:</p>
<p>• GLM-4-9b-chat [13]: An advanced generative language model designed for dialogue applications with a focus on efficient response generation.• Llama2-7b-chat [37]: A conversational AI model from the Llama family, optimized for contextual understanding in dialogues with 7 billion parameters.• Llama2-13b-chat [37]: A more robust version of its predecessor, this model offers enhanced conversational abilities with 13 billion parameters.• Llama3-8b-instruct [36]: Building on the Llama series, this iteration focuses on instruction-following tasks with an 8 billionparameter architecture.• GPT-3.5-instruct[29]: A finetuned version of GPT-3.5, this model excels in following user instructions with improved precision.• Mistral-7b [18]: A lightweight yet powerful model, designed for streamlined AI tasks.• Solar-10.7b-instruct[19]: A state-of-the-art model tailored for specific instruction adherence, boasting 10.7 billion parameters for diverse task performance.To generate answers, we use the recommended generation parameters for each model and guide the models using few-shot settings.For each input, we select up to 8 examples.We selected 500 samples from the dev set of ASQA, 200 samples from the test set of AN-TIQUE and 50 samples from WikiEval to generate answers.After filtering out invalid QA pairs-such as those resulting from model refusals-we obtained 3,500 valid samples from ASQA, 1,386 from ANTIQUE, and 350 from WikiEval.</p>
<p>3.1.2Meta-evaluations.This involves the evaluation of the evaluation, serving as a method to assess the quality of automatic evaluation metrics.Here, we adopt two types of meta-evaluations, as outlined below.</p>
<p>• Correlation coefficient: When comparing the differences between two distributions, a commonly used metric is the correlation coefficient.Following Chen et al. [5], we use the Spearman correlation coefficient to measure the value correlation and Kendall correlation coefficient to measure the rank correlation between human ratings and automatic metrics.• Win Rate and Agreement: When comparing multiple LLMs, researchers often use the win rate, defined as the fraction of instances in which model A's response outperforms model B's.Furthermore, for two different evaluators, their agreement can be calculated, which represents the proportion of instances where the evaluators made the same judgment regarding the quality of the responses from the two models [49].</p>
<p>Human annotations.</p>
<p>In the annotation of model responses, we drew inspiration from previous work [47] and primarily considered two evaluation aspects.</p>
<p>• Correctness aims to determine the accuracy of the answers, determining whether there are any factual errors or inaccuracies in the model's response.A high-quality response should be factually reliable and free from mistakes.• Informativeness examines whether the response contains sufficient and relevant information, identifying any missing or omitted details, and ensuring there is no excessive redundancy.A highquality response should should be informative enough without being redundant.</p>
<p>For the results of the seven models on the ASQA, ANTIQUE, and WikiEval datasets, we randomly sampled 50, 200, and 50 questions, respectively, for manual evaluation, with each question having 7 answers from different models.After removing a few cases where models refused to answer or generated incorrect responses, we collected 343, 1386, and 350 valid QA pairs that include both correctness and informativeness.We developed a web-based interface to streamline the annotation process.Figure 1 illustrates the interface for a single question.Each question is paired with a generated answer and a reference answer.Deterministic metrics: The deterministic metrics typically assess results based on lexical overlap between the predicted answer and the reference answer.Early works on LFQA usually take Exact Match (EM) and Rouge-L (RL) as the evaluation metrics [10,33].The EM metric is to evaluate whether the predicted answer matches any of a fraction in the reference answer [33].The Rouge-L is to evaluate the quality of the predicted answer by measuring the longest common subsequence between the generated text and the reference answer [23].We also include the Disambig-F1 (DF1) [33], which is to evaluate the fraction of diambiguated questions answered by the predicted answer, for the evaluation study.</p>
<p>Model-based metrics: The model-based metrics often assess the semantic accuracy of the generated answer using pre-trained models, extending beyond traditional n-gram matching.The BERTScore (BS) [46] calculate the cosine similarity between the generated answer and the reference answer.In addition, we considered the answer relevance (AR) from RAGAS [9] to supplement the measurement of responses' informativeness.For LLM-based evaluation, we follow LLM-EVAL [24] to use prompt-driven GPT-4 model as the judge in both coarse-grained (CG) and fine-grained (FG) settings.</p>
<p>Accuracy of Automatic Metrics</p>
<p>In this section, we try to answer RQ1, that is to what extent do existing automatic evaluation metrics serve as substitutes for human evaluations in LFQA.In this part, we take responses of seven LLMs on three datasets as the testbed.For each response, we conduct a human rating and compute the scores for each metric.We then assess the correlation coefficient between these metrics and human ratings across the entire dataset.Additionally, for any pair of model-generated responses, we calculate the agreement between each metric and human rating to determine whether they align in their preference between these two answers.The results of correlation coefficients between automatic metrics and human ratings for the three datasets are presented in Table 1 and 2. Firstly, for deterministic metrics, we can see that exact match demonstrates relatively strong alignment with human evaluation on ASQA dataset, even outperforming LLM-based evaluations such as GPT-4o, Claude-3.5 and Gemini-2.0.This is primarily because exact match focuses solely on assessing short answers in ASQA.However, while evaluating long answers, metrics like Rouge-L and Disambig-F1 exhibit poor consistency with human evaluations.An exception is observed with Rouge-L, which yields significantly different results on the Antique and WikiEval datasets.It shows high consistency with human evaluations on WikiEval but low consistency on Antique.This discrepency arises because Antique features non-factoid QA with open-ended answers, whereas WikiEval focuses on factoid QA with closed-ended answers.</p>
<p>Secondly, for model-based metrics, we can see that BertScore exhibits a trend similar to Rouge-L, showing low consistency with human evaluations on the ASQA and Antique datasets but high consistency on the WikiEval dataset.In contrast, LLM-based evaluations exhibit strong consistency with human evaluations across various styles of LFQA, including the ambiguous QA, fatoid QA, and non-factoid QA, highlighting their stability in evaluation.For example, the fine-grained (FG) GPT4 achieves the best performance on both the ASQA and Antique datasets.</p>
<p>Lastly, when comparing different LLM-based evaluations, we find that GPT-4o evaluation demonstrates superior performance over other LLMs.For example, the GPT-4o, Claude-3.5, and Gemini-2.0achieve spearman correlation score of 42.0, 33.0, and 32.4,respectively, when compared to human evaluations.Moreover, the fine-grained evaluation with GPT-4o improves its score from 42.0 to 55.0, which demonstrates the importance of providing more detailed instructions for LLM-based evaluations.</p>
<p>In summary, LLM-based evaluations demonstrate stability across different types of LFQA, whereas deterministic evaluations are less reliable for open-ended QA.Furthermore, conducting finegrained evaluations with LLM-based assessments would produce more nuanced results.</p>
<p>Robustness of Automatic Metrics</p>
<p>In this section, we focus on the first part of RQ2, which investigates the robustness of automatic evaluation metrics.For this purpose, we introduce small perturbations to each evaluation method, and analyze the changes before and after the perturbation.Here, we concentrate on model-based evaluation metrics.Firstly, we collects the scores generated by automatic evaluation metrics under their default configurations.Subsequently, we introduce perturbations to the experimental conditions and obtain the corresponding perturbed scores.To quantify the consistency of evaluators' decisions before and after the introduction of perturbations, we statistically analyzed the model win rates and the distribution of metric scores under different experimental settings.</p>
<p>Perturbation of prompts. The prompts of LLMs have been</p>
<p>shown to significantly influence their output, giving rise to a new research area known as prompt engineering [44].To better understand the impact of prompts on the evaluation capability of LLMs, we conducted experiments comparing different prompts using GPT-4o on the ASQA dataset, including short prompt, normal prompt, and long prompt.More detailed description of prompts are shown in code repository. 1he results are depicted in Figure 2: (i) LLM-based evaluations tend to yield lower scores compared to human evaluations, with the majority of LLM-based scores clustering around 3, while human evaluation scores are predominantly around 5. This discrepancy may be due to LLM-based evaluations being more stringent than human evaluations.(ii) The length of the prompt has a significant impact on the evaluation scores of LLMs.It is evident that short prompts tend to result in more high scores.For example, the number of samples with scores above 4 for the short prompt is noticeably higher than for the normal and long prompts.</p>
<p>Perturbation of hyper-parameters.</p>
<p>Hyperparameters such as sampling temperature, top-k sampling, repetition penalty, and maximum token length all play a role in shaping the LLM's output and overall performance [30].However, the impact of sampling temperature on LLM-as-a-judge has not been specifically investigated.The selection of sampling temperature is largely based on guesswork and intuition.Here, we take GPT-4o as the judge to analyze the impact of temperature.</p>
<p>All results are summarized in Table 3.If multiple models have the same score, we take the average rank as their shared rank.As we can see: (i) In the ASQA dataset, the evaluation results are highly stable, where minimal changes in score and no changes in the rankings of any LLMs.This could be because the evaluation score for each model vary widely, ranging from 3.03 to 8.66, making it difficult for the rankings to change.(ii) In the WikiEval dataset, the evaluation results vary significantly with changes in the temperature of the LLM judge.For example, the rank of GPT-4o drops from 4th to 6th when the temperature increases from 0.0 to 0.3.In contrast, its rank   improves from 4th to 2nd when the temperature is set to 0.7 or 1.0.This may be because the evaluation scores of each model are very similar, with the minimum and maximum values being 6.60 and 8.62, respectively.Overall, the above observations indicate that the temperature of the LLM does impact the evaluation, but the extent of this impact depends on the dataset.</p>
<p>Fairness of Automatic Metrics</p>
<p>In this section, we address the second part of RQ2, which focuses on the fairness of automatic evaluation metrics.Specifically, we conduct a comprehensive analysis of the evaluators' biases across four key dimensions: response length, question type, answer generation models, and language representations.</p>
<p>Length bias.</p>
<p>Here, we aim to examine whether answer length influences the evaluation of automatic metrics, particularly LLMbased evaluation methods.To this end, we divide all answers into five bins with equal sample sizes based on their length and analyze the performance of different evaluation metrics across these length intervals.All the results are depicted in Figure 3.Our experimental results reveal distinct patterns in the relationship between answer length and various evaluation metrics.We observe a negative correlation between answer length and scores on the Rouge-L and BERTScore metrics.As the length of the answer increases, the Rouge-L and BERTScore metrics tend to decrease.This trend suggests that longer answers may introduce redundancy or deviate from the concise, information-dense content that these metrics favor.Rouge-L, which measures the overlap between generated and reference texts, is particularly sensitive to extraneous information that dilutes the precision of the answer.Similarly, BERTScore, which evaluates semantic similarity, may penalize longer answers that include tangential or less relevant content.</p>
<p>On the ASQA and WikiEval dataset, we observe a positive correlation between answer scores and length in Claude-3.5.While the average scores remained relatively stable, the lower bounds of scores assigned by GPT-4 and Gemini-2.0increased significantly with longer answers.For the Antique dataset, the trend of increasing scores with length is evident, and this trend is more pronounced in LLM-based methods compared to human evaluations.This indicates that LLMs are less likely to assign low scores to longer answers, a phenomenon not observed in human evaluations.These evaluation frameworks may prioritize comprehensiveness and detail over conciseness.</p>
<p>Our analysis reveals that answer length significantly influences the performance of automatic evaluation metrics, with distinct patterns observed across different metrics and models.Combine traditional metrics (e.g., Rouge-L, BERTScore) with LLM-based metrics to balance the strengths and weaknesses of each approach.To address these biases and improve the robustness of automatic evaluation metrics, some strategies could be considered.For example, traditional metrics can ensure precision and conciseness, while LLM-based metrics can capture comprehensiveness and detail.And develop metrics that account for answer length by normalizing scores based on the amount of relevant information.This could involve penalizing redundancy while rewarding additional meaningful content.3.4.2Question type.In this section, we investigate the potential biases of different evaluation metrics when assessing various types of questions.Building upon the research conducted by Bolotova et al. [3], we classified 200 questions from the antique dataset, with the specific quantities and proportions detailed in Table 4.For each category of questions, we calculated the kendall correlation consistency between human evaluators' scores for correctness and informativeness and various metrics, as illustrated in Figure 4.</p>
<p>Our analysis reveals several key findings.First, we observe that LLM-based metrics significantly outperform deterministic metrics (Rouge-L and BERTScore), with the latter showing negative correlation coefficients on this dataset, rendering them practically unreliable for evaluation purposes.Second, LLM-based metrics demonstrate slightly higher consistency in assessing informativeness compared to correctness, particularly for comparison and experience-type questions.This discrepancy may stem from the inherent difficulty of LLMs in identifying factual inaccuracies within generated responses.Finally, evidence-based and experience-type questions emerge as challenging areas for all metrics, with the three primary evaluation models (GPT-4, Claude-3.5, and Gemini-2.0)consistently underperforming relative to their average performance across other question types.</p>
<p>Based on our findings, we can conclude that LLM indeed exhibit evaluation biases across different types of question, particularly for questions relying on precise factual information or personal experiences and recommendations.However, potential improvements can be achieved through the implementation of more granular prompting strategies combined with a diversified evaluation approach.</p>
<p>3.4.3Self-reinforcing.In this section, we investigate whether LLMbased metrics show a preference for results generated by themselves.We take the widely adopted LLMs, i.e., GPT-4o, Claude-3.5, and Gemini-2.0,as both evaluators and generators on the ASQA dataset.Specifically, we take these three LLM to generate answers for each question, and conduct a pairwise comparison between them and other seven open-sourced LLMs.The results are illustrated in Figure 6.</p>
<p>We can see that: (i) The three closed-source LLMs (GPT-4o, Claude-3.5, and Gemini-2.0)consistently outperform the seven baseline LLMs (Mistral-7b, Llama2-7b, Llama3-8b, Solar-10.7b,GLM4-9b, and GPT-3.5-turbo).Among them, Claude-3.5 achieves the best performance in all baselines across all three LLM-based evaluations.</p>
<p>(ii) Among the three evaluation methods, we observe that GPT-4o and Claude-3.5 evaluations exhibit a strong bias towards their own responses.For example, GPT-4o achieves significantly higher win rates against all baselines when evaluated using GPT-4o compared to evaluations conducted by Claude-3.5 and Gemini-2.0.Similarly, Claude-3.5 demonstrates much higher win rates in its own evaluation than when assessed by GPT-4o and Gemini-2.0.Furthermore, the Gemini-2.0evaluation also gives much higher scores to itself compared to GPT-4o and Claude-3.5 evaluations (e.g., 0.468 vs. 0.262/0.256,0.388 vs. 0.163/0.06),when compared with GPT-4o and Claude-3.5.In summary, LLM-based evaluations tend to assign significantly higher scores to their own outputs, demonstrating a clear evaluation bias.Interestingly, despite this bias, the models' ranking remains consistent across different evaluations.Based on these, we recommend the following: (i) Prioritize using rankings over scores for comparisons, (ii) Employ multiple evaluation methods to achieve more reliable and stable results.</p>
<p>Word expression.</p>
<p>Prior work shows that LLMs often produce verbose, formal content [42].To investigate whether LLM-based evaluations favor answers containing rarer terms, we compute each answer's average inverse document frequency (IDF) and examine its correlation with the evaluation scores.The results are illustrated in Figure 5.As observed: On Antique and WikiEval, human scores are evenly distributed across IDF values, showing no clear link between answer quality and term rarity, while ASQA displays a unique bimodal pattern, suggesting different answer characteristics.Automatic metrics like Rouge-L and BERTScore strongly correlate with IDF values, following near-normal distributions on Antique and WikiEval.In contrast, GPT-4o and Gemini-2.0align more closely with human judgments.Notably, under fine-grained settings, GPT-4o tends to favor responses with higher IDF values, indicating a potential bias toward lexically sophisticated responses.These results highlight the importance of dataset-specific evaluation strategies and the need to address IDF-related biases in LLMbased metrics, and this similarity becomes even more pronounced when fine-grained evaluation settings are applied.</p>
<p>Method</p>
<p>In this section, we investigate approaches to improve the performance of LLM-based evaluators to address RQ3: "How can the effectiveness and robustness of existing evaluation methods be improved?"To achieve this, we transfer the findings in previous sections into criteria in the prompt of LLM-based evaluation.Specifically, we decompose the prompt architecture into four key components: task description, data specifications, output requirements, and evaluation criteria.Then, we obtain 9 prompts by different combinations of these four components as outlined in Table 6.Due to space limitations, all detailed prompts can be found in our released code. 2 Finally, we measure performance variations by calculating the Kendall correlation coefficient changes between GPT-4o evaluations and human ratings for each prompt configuration.</p>
<p>All results are shown in Table 5.Our main findings are as follows.(1) All four components are essential for improving LLMbased evaluations: Comparing the performance of P1 with P2, P5 and P6, it can be observed that P1 achieves significantly better performance across all LLMs.P1 incorporates all four components, while P2 excludes the Criteria, P5 omits the Data, and P6 lacks the Task.Moreover, the evaluation of glm4-9b and gpt-turbo-3.5 drop significantly when removes the Criteria from the prompts (i.e., P2 vs. P3).Besides, the Task is important for LLM-based evaluations as performance drops significantly by comparing P3 vs. P6.(2) The order of components influences the LLM-based evaluations: A comparison of P1, P3, P4, P7, and P9 shows that the order of the four components has varying impacts on the evaluation of different LLMs.Notably, the placement of the Output appears to have minimal influence on gpt-3.5-turbo,as the performance is very close between P3 and P4-where Output is positioned last in P3 and first in P4.</p>
<p>Components Instructions</p>
<p>Task Score the following LLM output of a question-answering task with respect to the following aspects using a 1 to 5 star rating system.</p>
<p>Data</p>
<p>The dataset is a Factoid Question-Answering dataset, specifically designed for evaluating factual precision and detailed comparative reasoning in AI-generated answers.</p>
<p>Output</p>
<p>Begin your evaluation by providing a short explanation.Be as objective as possible.After providing your explanation, please provide your evaluation by strictly following the JSON format, such as: [[SCORE]] {"accuracy": 2, "informativeness": 3}.</p>
<p>Criteria</p>
<p>Accuracy: Determine the accuracy of the answers, verifying the correctness and reliability of the information provided.(3) No consistent improvements can be obtained for the evaluation of all LLMs by one strategy: There is no one strategy can boost the performance of all LLMs evaluations, where glm4-9b, llama2-7b, and llama3-8b performs best with P4, gpt-3.5-turboperforms best with P3, llama2-13b performs best with P1, solar-10.7bperforms best with P9.The differences in answer styles between models may significantly affect the evaluator's performance across various prompts.</p>
<p>These results demonstrate the necessity of structured guidance and precise evaluation guidelines for reliable assessments.</p>
<p>Conclusion</p>
<p>In this work, we evaluated a range of automatic evaluation metrics, including traditional deterministic metrics and model-based metrics, across three diverse datasets: ASQA, Antique, and WikiEval.For each dataset, we compared the scores generated by these metrics with human evaluations, analyzed their robustness under perturbations, and investigated potential biases related to answer length, question type, self-reinforcement, and language expression.Additionally, we explored methods to improve the performance of LLM-based evaluators through fine-grained evaluation strategies.</p>
<p>Our evaluation indicates that even with relatively high-quality reference answers, deterministic evaluation metrics still perform poorly and often do not exhibit significant correlation with human judgments.Their performance is highly dependent on the dataset and question type, with limited applicability to complex LFQA tasks.LLM-based metrics, such as GPT-4o and Claude-3.5,demonstrate better alignment with human evaluations and greater stability across different types of questions.However, they are susceptible to biases related to answer length, question type, and selfreinforcement.For example, LLM-based evaluators tend to favor longer answers and their own generated responses.The observed biases in LLM-based evaluations call for the development of more equitable evaluation frameworks that account for factors such as answer length, question type, and language expression.</p>
<p>Although our study has investigated the performance of automatic evaluation metrics across multiple dimensions, there are still some limitations that point to directions for future work: (i) Our 750-question evaluation covers only 3 QA categories, while realworld LFQA involves more diverse formats (e.g., multihop reasoning).Future research should evaluate automatic metrics on broader datasets, including more diverse type and domain of questions.(ii) We adopted a single-point scoring approach, and for human evaluation, we only set two main scoring criteria: correctness and informativeness.This may miss subtle quality differences between answers.A more nuanced evaluation setup would help fully capture various potential responses and edge cases.(iii) Our experiments have revealed the sensitivity of LLM-based models to prompts, but we have not conducted a detailed analysis of how different models respond to this, further investigation is needed.</p>
<p>Figure 1 :
1
Figure 1: Interface used for collecting human annotations.</p>
<p>Figure 2 :
2
Figure 2: Score distribution of different prompts on ASQA.</p>
<p>Figure 3 :
3
Figure 3: Relationship between answer length and metrics.</p>
<p>Figure 4 :
4
Figure 4: Relationship between different metrics and human evaluations across different question types on the ANTIQUE dataset (left: Informativeness, right: Correctness).</p>
<p>(a) Results on the WikiEval dataset.(b) Results on the ASQA dataset.(c) Results on the Antique dataset.</p>
<p>Figure 5 :
5
Figure 5: Relationship between the IDF of answer and different metrics.</p>
<p>Figure 6 :
6
Figure 6: Winrate for different metrics on the ASQA dataset.</p>
<p>1 star: Incorrect information 2 stars: Partially correct information 3 stars: Half correct information 4 stars: Mostly correct information 5 stars: Perfectly correct information Informativeness: Examines whether the answers provide sufficient and meaningful information that is useful to the user and relevant to the question. 1 star: No information or irrelevant information 2 stars: Very little information 3 stars: Some information 4 stars: Enough information 5 stars: Highly informative</p>
<p>Table 1 :
1
Correlation coefficients between automatic metrics and human ratings for ASQA dataset.The best score for each column is highlighted in bold.The second best is underlined.
Spearman/KendallDeterministic MetricsModel-based Metrics(%)RLEMDF1BSARFGGPT-4oClaude-3.5 Gemini-2.0glm4-9b31.0/22.8 26.1/22.4 31.4/24.0 29.4/22.5 22.4/14.6 66.5/52.2 56.6/45.037.2/30.615.2/11.7gpt-3.5-turbo-0.3/-0.5 48.6/40.3 18.5/12.92.6/2.48.3/5.750.8/40.6 32.6/25.122.3/18.315.4/11.9llama2-13b21.9/16.3 56.6/47.714.4/9.8 31.1/23.78.7/6.066.6/54.1 29.1/22.113.1/10.541.1/30.1llama2-7b17.0/12.1 54.3/46.2 14.7/11.1 22.9/16.6 23.7/17.3 43.6/35.6 32.5/24.630.8/22.622.6/15.4llama3-8b-2.4/-1.6 47.5/39.4 59.1/44.8 12.1/8.1 14.0/10.1 72.1/60.5 43.5/33.820.7/17.532.1/23.8mistral-7b53.3/37.9 57.0/49.9 33.2/24.0 33.5/24.0 17.1/12.0 62.0/49.3 47.8/37.359.0/45.751.3/39.5solar-10.7b4.3/2.610.4/8.6-8.9/-6.19.0/8.48.9/7.022.9/22.3 41.4/32.220.6/17.78.5/6.8Average11.4/8.041.4/34.9 23.0/16.8 16.2/11.7 12.2/8.7 55.0/44.9 42.0/32.433.0/26.332.4/24.4</p>
<p>Table 2 :
2
Correlation coefficients between automatic metrics and human ratings for ANTIQUE and WikiEval dataset.The best score for each column is highlighted in bold.The second best is underlined.
Spearman/KendallANTIQUE datasetWikiEval dataset(%)RLBSARCGFGRLBSCGFGglm4-9b-14.4/-11.4-9.8/-7.70.5/0.436.7/33.1 54.6/51.7 44.4/34.4 44.6/34.4 38.1/34.8 39.1/35.7gpt-3.5-turbo2.8/2.0-9.1/-7.043.7/35.0 53.2/47.1 56.5/51.7 57.1/45.1 56.8/43.1 41.9/36.5 46.9/42.1llama2-13b11.0/7.80.5/0.510.9/7.6 74.9/62.5 81.5/71.2 61.4/47.2 57.1/44.3 61.8/51.7 55.4/46.8llama2-7b13.8/10.09.3/6.917.7/13.4 69.3/58.6 74.6/64.7 74.6/60.4 75.2/60.3 60.8/52.8 60.1/52.9llama3-8b2.4/1.58.7/6.32.5/1.665.6/54.6 72.2/63.1 51.7/40.6 44.9/34.5 44.1/37.7 35.7/31.9mistral-7b29.2/21.025.9/18.5 23.3/16.9 73.0/62.5 83.9/73.6 51.3/40.0 60.7/48.9 47.7/40.0 50.4/42.6solar-10.7b-4.6/-3.4-9.7/-6.6
-3.5/-2.354.3/46.270.8/62.556.3/43.354.3/41.7 41.2/35.5 34.1/31.1 Average -7.9/-5.9-24.4/-17.228.3/20.683.0/70.985.2/75.6 55.8/43.455.5/42.6 51.4/43.949.0/43.1 3.1.4Automatic metrics.For evaluation metrics, we include 7 widely used metrics for long-form question answering, including traditional deterministic metrics and recently model-based metrics.</p>
<p>Table 3 :
3
The relationship between model scores and evaluators' temperature, with the default temperature set to 0.
ASQAWikievalScores RanksScores GainRank GainScores RanksScores GainRank Gain</p>
<p>Table 4 :
4
Question types in the ANTIQUE dataset.
Question TypeCount ProportionREASON4270.31INSTRUCTION3570.26EVIDENCE-BASED2660.19EXPERIENCE1540.11DEBATE1050.08COMPARISON770.06</p>
<p>Table 5 :
5
Variation of correlation coefficients under different prompt words.
Kendall (%)LLMsPromptsGPT-4o Claude-3.5 Gemini-2.0P1P2P3P4P5P6P7P8P9glm4-9b34.76-36.15-2.728.56 -17.45 3.489.873.914.102.25-2.387.49gpt-3.5-turbo36.47-2.02-6.4816.440.6127.64 22.548.3615.10 12.36 13.24 19.84llama2-13b51.66-23.504.9316.54 10.52 11.738.3511.32 12.11 12.13 12.418.62llama2-7b52.77-11.17-4.886.082.857.7810.224.435.690.056.025.79llama3-8b37.66-1.70-18.2518.93 18.247.6921.73 15.333.6317.22 15.90 15.67mistral-7b39.97-31.42-31.06-6.44 -10.06 -6.78 -16.19 -17.30 -15.88 -6.74 -16.58 -14.19solar-10.7b35.45-8.168.5612.29 15.01 21.72 14.799.894.6519.94 14.61 24.10Average43.89-15.91-4.8411.685.4812.44 11.157.715.749.498.2111.38</p>
<p>Table 6 :
6
Prompt settings.
Prompt SettingsP1Task+Data+Output+CriteriaP2Task+Data+OutputP3Task+Data+Criteria+OutputP4Output+Task+Data+CriteriaP5Task+Output+CriteriaP6Data+Output+CriteriaP7Output+Criteria+Task+DataP8Data+Task+Criteria(short)+OutputP9Data+Task+Criteria+Output</p>
<p>Table 7 :
7
Evaluation instructions.</p>
<p>https://github.com/bugtig6351/lfqa_evaluation/src/prompts.py (a) Results on the ASQA dataset (b) Results on the Wikieval dataset (c) Results on the Antique dataset
https://github.com/bugtig6351/lfqa_evaluation/src/prompts.py
AcknowledgmentsThis work was funded by the National Natural Science Foundation of China (NSFC) under Grants No. 62372431, 62472408 and 62441229, the Strategic Priority Research Program of the CAS under Grants No. XDB0680102 and XDB0680301, the National Key Research and Development Program of China under Grants No. 2023YFA1011602, the Lenovo-CAS Joint Lab Youth Scientist Project, and the project under Grants No. JCKY2022130C039.This research was (partially) supported by the Dutch Research Council (NWO), under project numbers 024.004.022,NWA.1389.20.183, and KICH3.LTP.20.006, and the European Union's Horizon Europe program under grant agreement No 101070212.All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
Can we trust the evaluation on ChatGPT?. Rachith Aiyappa, Jisun An, Haewoon Kwak, Yong-Yeol Ahn, 10.48550/arXiv.2303.12767arXiv:2303.12767arXiv:2303.127672023. March 2023</p>
<p>Meghana Moorthy Bhat, Rui Meng, Ye Liu, Yingbo Zhou, Semih Yavuz, 10.48550/arXiv.2309.08210arXiv:2309.08210arXiv:2309.08210Investigating Answerability of LLMs for Long-Form Question Answering. 2023. Sept. 2023</p>
<p>A Non-Factoid Question-Answering Taxonomy. Valeriia Bolotova, Vladislav Blinov, W Bruce Falk Scholer, Mark Croft, Sanderson, 10.1145/3477495.3531926Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22). the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv. 2023. 2023arXiv preprint</p>
<p>Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, Ruifeng Xu, 10.48550/arXiv.2304.00723arXiv:2304.00723arXiv:2304.007232023. April 2023</p>
<p>Can Large Language Models Be an Alternative to Human Evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.48550/arXiv.2305.01937arXiv:2305.01937arXiv:2305.019372023. May 2023</p>
<p>A closer look into automatic evaluation using large language models. Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2310.056572023. 2023arXiv preprint</p>
<p>On The Evaluation of Machine Translation Systems Trained With Back-Translation. Sergey Edunov, Myle Ott, Marc ' , Aurelio Ranzato, Michael Auli, 10.48550/arXiv.1908.05204arXiv:1908.05204arXiv:1908.052042020. Aug. 2020</p>
<p>Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert, arXiv:2309.15217Ragas: Automated evaluation of retrieval augmented generation. 2023. 2023arXiv preprint</p>
<p>ELI5: Long Form Question Answering. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli, 10.18653/v1/P19-1346Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Yuchen Fan, Xin Zhong, Yazhe Wan, Chengsi Wang, Haonan Cheng, Gaoche Wu, Ning Ding, Bowen Zhou, arXiv:2407.04969EVA-Score: Evaluating Abstractive Long-form Summarization on Informativeness through Extraction and Validation. 2024. 2024arXiv preprint</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023. 2023arXiv preprint</p>
<p>Glm Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang, arXiv:2406.12793ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools. 2024</p>
<p>Yvette Graham, Barry Haddow, Philipp Koehn, 10.48550/arXiv.1906.09833arXiv:1906.09833arXiv:1906.09833Translationese in Machine Translation Evaluation. 2019. June 2019</p>
<p>Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Yuanzhuo Wang, Jian Guo, 10.48550/arXiv.2411.15594arXiv:2411.15594arXiv:2411.15594A Survey on LLM-as-a-Judge. 2024. Dec. 2024</p>
<p>ANTIQUE: A non-factoid question answering benchmark. Helia Hashemi, Mohammad Aliannejadi, Hamed Zamani, Bruce Croft, Advances in Information Retrieval: 42nd European Conference on IR Research. Lisbon, PortugalSpringer2020. April 14-17, 20202020Proceedings, Part II 42</p>
<p>Survey of Hallucination in Natural Language Generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Wenliang Dai, Andrea Madotto, Pascale Fung, 10.1145/3571730arXiv:2202.03629Comput. Surveys. 552023. Dec. 2023</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023. 2023Mistral 7B. arXiv preprint</p>
<p>Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, Sunghun Kim, arXiv:2312.15166[cs.CL]SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling. 2023</p>
<p>Hurdles to Progress in Long-form Question Answering. Kalpesh Krishna, Aurko Roy, Mohit Iyyer, 10.18653/v1/2021.naacl-main.393Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, Yichao Zhou, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021</p>
<p>Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, 10.48550/arXiv.2305.18486arXiv:2305.18486arXiv:2305.18486Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. July 2023</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda, arXiv:2211.09110arXiv:2211.09110Holistic Evaluation of Language Models. 2022. Nov. 2022</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. Yen-Ting Lin, Yun-Nung Chen, arXiv:2305.137112023. 2023arXiv preprint</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, arXiv:2303.16634G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023. 2023arXiv preprint</p>
<p>Adian Liusie, Potsawee Manakul, Mark J F Gales, 10.48550/arXiv.2307.07889arXiv:2307.07889arXiv:2307.07889LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models. 2023. Aug. 2023</p>
<p>Language models are few-shot learners. Ben Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, arXiv:2005.1416512020. 2020arXiv preprint</p>
<p>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.48550/arXiv.2305.14251arXiv:2305.14251arXiv:2305.142512023. Oct. 2023</p>
<p>GPT-3.5-Turbo-Instruct. 2023OpenAI</p>
<p>Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform. 2025OpenAI</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems. Ehud Reiter, Anja Belz, 10.1162/coli.2009.35.4.35405Computational Linguistics. 352009. Dec. 2009</p>
<p>ASQA: Factoid Questions Meet Long-Form Answers. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, Ming-Wei Chang, 10.18653/v1/2022.emnlp-main.566Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zornitsa Goldberg, Yue Kozareva, Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Read before Generate! Faithful Long Form Question Answering with Machine Reading. Dan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Pascale Fung, arXiv:2203.00343[cs.CL2022</p>
<p>Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering. Mingxu Tao, Dongyan Zhao, Yansong Feng, arXiv:2402.16313[cs.CL2024</p>
<p>Introducing Meta Llama 3: The most capable openly available LLM to date. 2024</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, arXiv:2307.09288Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, Yun-Hsuan Sung, arXiv:2407.10817Foundational autoraters: Taming large language models for better automatic evaluation. 2024. 2024arXiv preprint</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, arXiv:2303.04048Is chatgpt a good nlg evaluator? a preliminary study. 2023. 2023arXiv preprint</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.17926[cs.CLLarge Language Models are not Fair Evaluators. 2023</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang, 10.48550/arXiv.2306.05087arXiv:2306.05087arXiv:2306.05087PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. 2024. May 2024</p>
<p>A survey on LLM-generated text detection: Necessity, methods, and future directions. Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Lidia Sam Chao, Derek Fai Wong, Computational Linguistics. 2025. 2025</p>
<p>A critical evaluation of evaluations for long-form question answering. Fangyuan Xu, Yixiao Song, Mohit Iyyer, Eunsol Choi, arXiv:2305.182012023. 2023arXiv preprint</p>
<p>Qinyuan Ye, Maxamed Axmed, Reid Pryzant, Fereshte Khani, arXiv:2311.05661Prompt engineering a prompt engineer. 2023. 2023arXiv preprint</p>
<p>FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo, 10.48550/arXiv.2307.10928arXiv:2307.10928arXiv:2307.109282023. Oct. 2023</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019. 2019arXiv preprint</p>
<p>Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, Xuanjing Huang, 10.48550/arXiv.2312.07398arXiv:2312.07398arXiv:2312.07398LLMEval: A Preliminary Study on How to Evaluate Large Language Models. 2023. Dec. 2023</p>
<p>MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, 10.18653/v1/D19-1053Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
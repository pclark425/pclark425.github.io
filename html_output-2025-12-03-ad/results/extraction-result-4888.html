<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4888 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4888</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4888</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-262500883</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.07700v1.pdf" target="_blank">NeurASP: Embracing Neural Networks into Answer Set Programming</a></p>
                <p><strong>Paper Abstract:</strong> We present NeurASP, a simple extension of answer set programs by embracing neural networks. By treating the neural network output as the probability distribution over atomic facts in answer set programs, NeurASP provides a simple and effective way to integrate sub-symbolic and symbolic computation. We demonstrate how NeurASP can make use of a pre-trained neural network in symbolic computation and how it can improve the neural network's perception result by applying symbolic reasoning in answer set programming. Also, NeurASP can be used to train a neural network better by training with ASP rules so that a neural network not only learns from implicit correlations from the data but also from the explicit complex semantic constraints expressed by the rules.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4888.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4888.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeurASP (Sudoku / M_identify)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeurASP framework with digit-recognition network M_identify for Sudoku-from-image</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper presents NeurASP, a neuro-symbolic system that uses a pre-trained digit-recognition neural network (M_identify) to read Sudoku board images and an ASP solver to enforce Sudoku constraints and compute solutions, improving perception via symbolic reasoning and enabling small-data training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>M_identify (pretrained CNN digit recognizer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A convolutional neural network used as a perception module to classify each of the 81 Sudoku cell images into {empty,1..9}; architecture not fully specified in the paper (described as a CNN), trained on small sets of synthetic Sudoku images (grayscale or RGB when needed). Training dataset sizes reported: 15, 17, 19, 21, 23, 25 examples in experiments (per the Table).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (image input)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Standard 9x9 Sudoku where each of 81 grid cells contains either a digit 1..9 or is empty; solving requires enforcing row, column, and 3x3-block uniqueness constraints (spatial constraints over grid positions). Input here is a board image, so perception + spatial reasoning are required.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Neural perception + symbolic solver: a neural atom nn(identify(81,img),[empty,1..9]) provides per-cell probabilistic labels; these probabilities are fed into an ASP program encoding Sudoku constraints. Two modes: (a) consistency checking (Π_sudoku \ r) where ASP checks identified digits satisfy constraints, and (b) full solving (Π_sudoku) where ASP finds assignments for empty cells. The system also supports training the perception network with ASP-derived semantic loss (maximize probability of stable models satisfying observations).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Spatial reasoning is provided explicitly by ASP rules encoding Sudoku constraints (no repeated number in row/column/3x3 box) and used to (i) correct/improve perception (filter inconsistent digit recognitions) and (ii) solve the board once digits are correctly identified. The paper demonstrates solving variants that require additional spatial constraints (Anti-knight, Sudoku-X, Offset Sudoku) by adding respective rules to NeurASP, showing symbolic spatial rules drive behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 1 (test accuracy values). Acc_identify (raw NN accuracy on all 81 cells correct): M_identify alone: 15%, 31%, 72%, 85%, 93%, 100% for training-set sizes 15,17,19,21,23,25 respectively. Using NeurASP consistency-checking (Π_sudoku \ r) improves Acc_identify to 49%, 62%, 90%, 95%, 99%, 100%; using full solving (Π_sudoku) further improves to 71%, 80%, 95%, 98%, 100%, 100%. Acc_sol (board solved correctly by Π_sudoku) equals the corresponding Acc_identify for Π_sudoku (because ASP yields correct solution when the board is correctly perceived).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Implementation is a prototype and not highly scalable (enumerates stable models naively). When image types change (e.g., Offset Sudoku colored boards) the perception network must be retrained (they report needing 70 training images for 100% Acc_identify on Offset Sudoku). NeurASP relies on correct/permissive grounding and coherence assumptions; if the perception errors produce inconsistent total choices, coherence requirements could be problematic. The system does not claim end-to-end learning of reasoning — reasoning is symbolic and external.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Palm et al. (2018) Graph Neural Network approach (textual board input) that achieved 96.6% with 216k examples, NeurASP processes images and needs far fewer labeled examples (15–25) thanks to separating perception and reasoning; NeurASP obtains high accuracy after small training sets. Compared to an end-to-end neural approach trained to both perceive and solve, NeurASP reduces neural-network complexity and training-data needs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NeurASP: Embracing Neural Networks into Answer Set Programming', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4888.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4888.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>M_sol + NeurASP (learn-to-solve Sudoku)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>M_sol neural Sudoku solver trained with NeurASP constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses a neural Sudoku solver architecture (M_sol, from Park 2018) trained jointly with NeurASP rule constraints so that the network learns to produce outputs that are consistent with Sudoku rules; this yields much better performance than training the same architecture with standard cross-entropy on a relatively small dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>M_sol (Park 2018 convolutional Sudoku solver)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Architecture taken from Park (2018): 9 convolutional layers followed by a 1x1 convolution and softmax output over digits 1..9 per cell. Original Park training used 1 million examples and an iterative one-by-one inference trick; in this paper M_sol is trained both as baseline (cross-entropy) and with NeurASP semantic constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (textual board -> solution prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Given a textual 9x9 Sudoku matrix (0 for empty), predict the full solved board — requires capturing the global spatial constraints of Sudoku (rows, columns, 3x3 blocks).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Training-time neuro-symbolic supervision: during training NeurASP encodes Sudoku rules and the training objective is changed from per-cell cross-entropy to maximizing the probability of stable models that satisfy the Sudoku constraints (semantic loss propagated back into M_sol). Rules used only during training (not during testing).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Improved whole-board and grid-cell accuracy after training with symbolic constraints indicates the network learned outputs consistent with Sudoku spatial constraints; the training objective explicitly enforces spatial constraints rather than relying purely on pattern learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On a dataset of 63,000+1,000 examples: baseline M_sol (Park method) on this small dataset obtained whole-board accuracy 29.1% and grid-cell accuracy 89.3% after 63 epochs. M_sol trained with NeurASP achieved whole-board accuracy 66.5% and grid-cell accuracy 96.9% after 63 epochs (rules used in training only).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors note scalability limitations in implementation (exact stable-model enumeration is slow), so they trained on a smaller dataset (63k) rather than Park's 1M; rules are used during training only in this experiment, so inference at test time relies solely on the network's learned outputs. The NeurASP training approach requires calling an ASP solver in the training loop, which can be computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct comparison to Park (2018) baseline shows large improvement when training with NeurASP constraints on the same limited data: whole-board accuracy increases from 29.1% to 66.5% and grid-cell accuracy from 89.3% to 96.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NeurASP: Embracing Neural Networks into Answer Set Programming', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4888.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4888.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>M_sp + NeurASP (Shortest-path)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>M_sp (5-layer MLP) trained with NeurASP to predict shortest paths</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 5-layer MLP (M_sp) trained with NeurASP constraints to predict shortest paths on small grid graphs; inclusion of explicit path/ reachability/optimization constraints in the neuro-symbolic training objective yields models whose predictions satisfy graph-structural constraints much more often.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>M_sp (5-layer MLP as in Xu et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 5-layer Multi-Layer Perceptron used to predict which edges belong to the shortest path between two nodes in a given graph (input encodes graph and source/target). Model architecture follows Xu et al. (2018).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Shortest-path on small graphs (4x4 grid with removed edges)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Task: given a graph (4x4 grid with 16 nodes and 24 edges, with 8 edges randomly removed) and start/end nodes, predict the set of edges forming the shortest path; requires relational/spatial reasoning over graph topology.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Train M_sp under different sets of symbolic constraints encoded in NeurASP: (i) simple-path constraint (p) ensuring predictions form a simple path, (ii) add reachability (r) and optimization (o) constraints for shortest-path (p-r-o), and (iii) further 'no removed edges' (nr) constraint (p-r-o-nr). Training objective is maximizing probability of stable models satisfying the chosen constraints (semantic loss backpropagated).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Models trained with path/reachability/optimization constraints produce predictions that satisfy graph-structural constraints at a much higher rate (e.g., the simple-path constraint is satisfied in 96.6% of MLP(p) predictions vs 28.3% for unconstrained MLP), indicating learned outputs respect spatial/topological structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table 2: Unconstrained MLP: simple-path (p) satisfaction 28.3%, reachability (r) 88.5%, shortest-path (p-r-o-nr) 23.0%, label (ground-truth accuracy) 22.4%. MLP trained with simple-path (MLP(p)): p 96.6%, r 100%, label accuracy 28.9%. MLP trained with p-r-o: p 100%, r 100%, label accuracy 40.1%. Adding nr (MLP(p-r-o-nr)) decreased some metrics (label accuracy 22.7%), indicating generalization difficulties for some constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Adding constraints tied to randomly varying parts of examples (the 'nr' constraint about removed edges) reduced generalization; naive enumeration of stable models limits scalability; models trained under more constraints can be sensitive to how those constraints vary across examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to the unconstrained MLP baseline and to results reported in Xu et al. (2018), NeurASP-trained models achieved similar or better constraint-satisfaction metrics (e.g., simple-path satisfaction 96.6% here vs 69.9% in Xu et al. for a comparable setting) and improved label accuracy when appropriate sets of constraints are included.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NeurASP: Embracing Neural Networks into Answer Set Programming', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4888.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4888.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Palm et al. GNN (Sudoku, related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Neural Network approach to solving Sudoku (Palm et al., 2018) as cited</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited related work: a Graph Neural Network used to solve Sudoku from textual board representations, achieving high accuracy but trained on a large dataset and restricted to non-image inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph Neural Network (Palm et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A graph neural network model that operates on a textual (symbolic) representation of a Sudoku board rather than images; trained with many examples (~216,000 in the cited comparison). Exact architecture details are not provided in this paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (textual board input)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same 9x9 Sudoku puzzle, but provided to the model as a textual (grid) representation rather than an image; solving still requires enforcing spatial/relational constraints between cells.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>End-to-end GNN trained to map board state to solution, leveraging relational message passing over a graph representing Sudoku cell relations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Implicit: by using a graph structure representing cell relations (edges for row/column/box relations), the GNN captures spatial relationships among cells; paper cites Palm et al.'s high accuracy on textual Sudoku as evidence that GNNs can learn Sudoku reasoning when given structured input.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited result: 96.6% accuracy after training with 216,000 examples (textual input).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited limitation: their approach is restricted to textual board input (not images), so it does not separate perception from reasoning and requires many labeled examples; direct comparison with NeurASP shows NeurASP needs far fewer examples for image-based Sudoku because perception is handled separately.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to NeurASP, Palm et al.'s GNN requires more training data and assumes symbolic input; NeurASP emphasizes separation of perception and reasoning to reduce perception data needs and support image inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NeurASP: Embracing Neural Networks into Answer Set Programming', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4888.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4888.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepProbLog (related)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepProbLog (neuro-symbolic probabilistic logic programming)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related prior system that integrates neural predicates with probabilistic logical inference by compiling logic to a probabilistic circuit (e.g., SDD); used as a comparison point for NeurASP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepProbLog (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A neuro-symbolic system that treats neural network outputs as probabilistic facts and compiles logic programs into circuits (SDDs) to perform probabilistic inference and learning end-to-end; cited work performed digit-addition experiments combining perception and symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Digit addition / reasoning tasks (cited by paper)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Tasks combining perception (digit classification) and logic (addition constraints) used to evaluate neuro-symbolic approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>DeepProbLog constructs circuits (SDDs) for probabilistic reasoning with neural predicates and performs end-to-end learning via backpropagation through the circuits; different from NeurASP which uses an ASP solver directly.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Not directly a spatial-reasoning system in cited uses, but used as a baseline for neuro-symbolic capabilities (e.g., digit addition tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In digit-addition experiments the paper reports NeurASP and DeepProbLog converged to similar accuracy, but NeurASP required less training time per epoch compared to DeepProbLog (NeurASP faster because it uses an ASP solver rather than building circuits). Exact numerical times are not provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>DeepProbLog requires circuit construction (SDD) which can be costly; DeepProbLog also requires training data organized in single-atom supervision (as noted by the authors), while NeurASP allows arbitrary propositional formulas in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>NeurASP is compared to DeepProbLog: both achieve similar accuracy on some experiments (digit-addition), but NeurASP is claimed to be more expressive (leveraging full ASP constructs) and faster in training because it avoids building circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NeurASP: Embracing Neural Networks into Answer Set Programming', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DeepProbLog <em>(Rating: 2)</em></li>
                <li>Can convolutional neural networks crack sudoku <em>(Rating: 2)</em></li>
                <li>A semantic loss function for deep learning with symbolic knowledge <em>(Rating: 2)</em></li>
                <li>The neurosymbolic concept learner: interpreting scenes, words, and sentences from natural supervision <em>(Rating: 2)</em></li>
                <li>Logic tensor networks for semantic image interpretation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4888",
    "paper_id": "paper-262500883",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "NeurASP (Sudoku / M_identify)",
            "name_full": "NeurASP framework with digit-recognition network M_identify for Sudoku-from-image",
            "brief_description": "This paper presents NeurASP, a neuro-symbolic system that uses a pre-trained digit-recognition neural network (M_identify) to read Sudoku board images and an ASP solver to enforce Sudoku constraints and compute solutions, improving perception via symbolic reasoning and enabling small-data training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "M_identify (pretrained CNN digit recognizer)",
            "model_description": "A convolutional neural network used as a perception module to classify each of the 81 Sudoku cell images into {empty,1..9}; architecture not fully specified in the paper (described as a CNN), trained on small sets of synthetic Sudoku images (grayscale or RGB when needed). Training dataset sizes reported: 15, 17, 19, 21, 23, 25 examples in experiments (per the Table).",
            "puzzle_name": "Sudoku (image input)",
            "puzzle_description": "Standard 9x9 Sudoku where each of 81 grid cells contains either a digit 1..9 or is empty; solving requires enforcing row, column, and 3x3-block uniqueness constraints (spatial constraints over grid positions). Input here is a board image, so perception + spatial reasoning are required.",
            "mechanism_or_strategy": "Neural perception + symbolic solver: a neural atom nn(identify(81,img),[empty,1..9]) provides per-cell probabilistic labels; these probabilities are fed into an ASP program encoding Sudoku constraints. Two modes: (a) consistency checking (Π_sudoku \\ r) where ASP checks identified digits satisfy constraints, and (b) full solving (Π_sudoku) where ASP finds assignments for empty cells. The system also supports training the perception network with ASP-derived semantic loss (maximize probability of stable models satisfying observations).",
            "evidence_of_spatial_reasoning": "Spatial reasoning is provided explicitly by ASP rules encoding Sudoku constraints (no repeated number in row/column/3x3 box) and used to (i) correct/improve perception (filter inconsistent digit recognitions) and (ii) solve the board once digits are correctly identified. The paper demonstrates solving variants that require additional spatial constraints (Anti-knight, Sudoku-X, Offset Sudoku) by adding respective rules to NeurASP, showing symbolic spatial rules drive behavior.",
            "performance_metrics": "Table 1 (test accuracy values). Acc_identify (raw NN accuracy on all 81 cells correct): M_identify alone: 15%, 31%, 72%, 85%, 93%, 100% for training-set sizes 15,17,19,21,23,25 respectively. Using NeurASP consistency-checking (Π_sudoku \\ r) improves Acc_identify to 49%, 62%, 90%, 95%, 99%, 100%; using full solving (Π_sudoku) further improves to 71%, 80%, 95%, 98%, 100%, 100%. Acc_sol (board solved correctly by Π_sudoku) equals the corresponding Acc_identify for Π_sudoku (because ASP yields correct solution when the board is correctly perceived).",
            "limitations_or_failure_cases": "Implementation is a prototype and not highly scalable (enumerates stable models naively). When image types change (e.g., Offset Sudoku colored boards) the perception network must be retrained (they report needing 70 training images for 100% Acc_identify on Offset Sudoku). NeurASP relies on correct/permissive grounding and coherence assumptions; if the perception errors produce inconsistent total choices, coherence requirements could be problematic. The system does not claim end-to-end learning of reasoning — reasoning is symbolic and external.",
            "comparison_baseline": "Compared to Palm et al. (2018) Graph Neural Network approach (textual board input) that achieved 96.6% with 216k examples, NeurASP processes images and needs far fewer labeled examples (15–25) thanks to separating perception and reasoning; NeurASP obtains high accuracy after small training sets. Compared to an end-to-end neural approach trained to both perceive and solve, NeurASP reduces neural-network complexity and training-data needs.",
            "uuid": "e4888.0",
            "source_info": {
                "paper_title": "NeurASP: Embracing Neural Networks into Answer Set Programming",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "M_sol + NeurASP (learn-to-solve Sudoku)",
            "name_full": "M_sol neural Sudoku solver trained with NeurASP constraints",
            "brief_description": "The paper uses a neural Sudoku solver architecture (M_sol, from Park 2018) trained jointly with NeurASP rule constraints so that the network learns to produce outputs that are consistent with Sudoku rules; this yields much better performance than training the same architecture with standard cross-entropy on a relatively small dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "M_sol (Park 2018 convolutional Sudoku solver)",
            "model_description": "Architecture taken from Park (2018): 9 convolutional layers followed by a 1x1 convolution and softmax output over digits 1..9 per cell. Original Park training used 1 million examples and an iterative one-by-one inference trick; in this paper M_sol is trained both as baseline (cross-entropy) and with NeurASP semantic constraints.",
            "puzzle_name": "Sudoku (textual board -&gt; solution prediction)",
            "puzzle_description": "Given a textual 9x9 Sudoku matrix (0 for empty), predict the full solved board — requires capturing the global spatial constraints of Sudoku (rows, columns, 3x3 blocks).",
            "mechanism_or_strategy": "Training-time neuro-symbolic supervision: during training NeurASP encodes Sudoku rules and the training objective is changed from per-cell cross-entropy to maximizing the probability of stable models that satisfy the Sudoku constraints (semantic loss propagated back into M_sol). Rules used only during training (not during testing).",
            "evidence_of_spatial_reasoning": "Improved whole-board and grid-cell accuracy after training with symbolic constraints indicates the network learned outputs consistent with Sudoku spatial constraints; the training objective explicitly enforces spatial constraints rather than relying purely on pattern learning.",
            "performance_metrics": "On a dataset of 63,000+1,000 examples: baseline M_sol (Park method) on this small dataset obtained whole-board accuracy 29.1% and grid-cell accuracy 89.3% after 63 epochs. M_sol trained with NeurASP achieved whole-board accuracy 66.5% and grid-cell accuracy 96.9% after 63 epochs (rules used in training only).",
            "limitations_or_failure_cases": "Authors note scalability limitations in implementation (exact stable-model enumeration is slow), so they trained on a smaller dataset (63k) rather than Park's 1M; rules are used during training only in this experiment, so inference at test time relies solely on the network's learned outputs. The NeurASP training approach requires calling an ASP solver in the training loop, which can be computationally expensive.",
            "comparison_baseline": "Direct comparison to Park (2018) baseline shows large improvement when training with NeurASP constraints on the same limited data: whole-board accuracy increases from 29.1% to 66.5% and grid-cell accuracy from 89.3% to 96.9%.",
            "uuid": "e4888.1",
            "source_info": {
                "paper_title": "NeurASP: Embracing Neural Networks into Answer Set Programming",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "M_sp + NeurASP (Shortest-path)",
            "name_full": "M_sp (5-layer MLP) trained with NeurASP to predict shortest paths",
            "brief_description": "A 5-layer MLP (M_sp) trained with NeurASP constraints to predict shortest paths on small grid graphs; inclusion of explicit path/ reachability/optimization constraints in the neuro-symbolic training objective yields models whose predictions satisfy graph-structural constraints much more often.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "M_sp (5-layer MLP as in Xu et al., 2018)",
            "model_description": "A 5-layer Multi-Layer Perceptron used to predict which edges belong to the shortest path between two nodes in a given graph (input encodes graph and source/target). Model architecture follows Xu et al. (2018).",
            "puzzle_name": "Shortest-path on small graphs (4x4 grid with removed edges)",
            "puzzle_description": "Task: given a graph (4x4 grid with 16 nodes and 24 edges, with 8 edges randomly removed) and start/end nodes, predict the set of edges forming the shortest path; requires relational/spatial reasoning over graph topology.",
            "mechanism_or_strategy": "Train M_sp under different sets of symbolic constraints encoded in NeurASP: (i) simple-path constraint (p) ensuring predictions form a simple path, (ii) add reachability (r) and optimization (o) constraints for shortest-path (p-r-o), and (iii) further 'no removed edges' (nr) constraint (p-r-o-nr). Training objective is maximizing probability of stable models satisfying the chosen constraints (semantic loss backpropagated).",
            "evidence_of_spatial_reasoning": "Models trained with path/reachability/optimization constraints produce predictions that satisfy graph-structural constraints at a much higher rate (e.g., the simple-path constraint is satisfied in 96.6% of MLP(p) predictions vs 28.3% for unconstrained MLP), indicating learned outputs respect spatial/topological structure.",
            "performance_metrics": "From Table 2: Unconstrained MLP: simple-path (p) satisfaction 28.3%, reachability (r) 88.5%, shortest-path (p-r-o-nr) 23.0%, label (ground-truth accuracy) 22.4%. MLP trained with simple-path (MLP(p)): p 96.6%, r 100%, label accuracy 28.9%. MLP trained with p-r-o: p 100%, r 100%, label accuracy 40.1%. Adding nr (MLP(p-r-o-nr)) decreased some metrics (label accuracy 22.7%), indicating generalization difficulties for some constraints.",
            "limitations_or_failure_cases": "Adding constraints tied to randomly varying parts of examples (the 'nr' constraint about removed edges) reduced generalization; naive enumeration of stable models limits scalability; models trained under more constraints can be sensitive to how those constraints vary across examples.",
            "comparison_baseline": "Compared to the unconstrained MLP baseline and to results reported in Xu et al. (2018), NeurASP-trained models achieved similar or better constraint-satisfaction metrics (e.g., simple-path satisfaction 96.6% here vs 69.9% in Xu et al. for a comparable setting) and improved label accuracy when appropriate sets of constraints are included.",
            "uuid": "e4888.2",
            "source_info": {
                "paper_title": "NeurASP: Embracing Neural Networks into Answer Set Programming",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Palm et al. GNN (Sudoku, related work)",
            "name_full": "Graph Neural Network approach to solving Sudoku (Palm et al., 2018) as cited",
            "brief_description": "Cited related work: a Graph Neural Network used to solve Sudoku from textual board representations, achieving high accuracy but trained on a large dataset and restricted to non-image inputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Graph Neural Network (Palm et al., 2018)",
            "model_description": "A graph neural network model that operates on a textual (symbolic) representation of a Sudoku board rather than images; trained with many examples (~216,000 in the cited comparison). Exact architecture details are not provided in this paper's text.",
            "puzzle_name": "Sudoku (textual board input)",
            "puzzle_description": "Same 9x9 Sudoku puzzle, but provided to the model as a textual (grid) representation rather than an image; solving still requires enforcing spatial/relational constraints between cells.",
            "mechanism_or_strategy": "End-to-end GNN trained to map board state to solution, leveraging relational message passing over a graph representing Sudoku cell relations.",
            "evidence_of_spatial_reasoning": "Implicit: by using a graph structure representing cell relations (edges for row/column/box relations), the GNN captures spatial relationships among cells; paper cites Palm et al.'s high accuracy on textual Sudoku as evidence that GNNs can learn Sudoku reasoning when given structured input.",
            "performance_metrics": "Cited result: 96.6% accuracy after training with 216,000 examples (textual input).",
            "limitations_or_failure_cases": "Cited limitation: their approach is restricted to textual board input (not images), so it does not separate perception from reasoning and requires many labeled examples; direct comparison with NeurASP shows NeurASP needs far fewer examples for image-based Sudoku because perception is handled separately.",
            "comparison_baseline": "Compared to NeurASP, Palm et al.'s GNN requires more training data and assumes symbolic input; NeurASP emphasizes separation of perception and reasoning to reduce perception data needs and support image inputs.",
            "uuid": "e4888.3",
            "source_info": {
                "paper_title": "NeurASP: Embracing Neural Networks into Answer Set Programming",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "DeepProbLog (related)",
            "name_full": "DeepProbLog (neuro-symbolic probabilistic logic programming)",
            "brief_description": "Related prior system that integrates neural predicates with probabilistic logical inference by compiling logic to a probabilistic circuit (e.g., SDD); used as a comparison point for NeurASP.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "DeepProbLog (framework)",
            "model_description": "A neuro-symbolic system that treats neural network outputs as probabilistic facts and compiles logic programs into circuits (SDDs) to perform probabilistic inference and learning end-to-end; cited work performed digit-addition experiments combining perception and symbolic reasoning.",
            "puzzle_name": "Digit addition / reasoning tasks (cited by paper)",
            "puzzle_description": "Tasks combining perception (digit classification) and logic (addition constraints) used to evaluate neuro-symbolic approaches.",
            "mechanism_or_strategy": "DeepProbLog constructs circuits (SDDs) for probabilistic reasoning with neural predicates and performs end-to-end learning via backpropagation through the circuits; different from NeurASP which uses an ASP solver directly.",
            "evidence_of_spatial_reasoning": "Not directly a spatial-reasoning system in cited uses, but used as a baseline for neuro-symbolic capabilities (e.g., digit addition tasks).",
            "performance_metrics": "In digit-addition experiments the paper reports NeurASP and DeepProbLog converged to similar accuracy, but NeurASP required less training time per epoch compared to DeepProbLog (NeurASP faster because it uses an ASP solver rather than building circuits). Exact numerical times are not provided in the text.",
            "limitations_or_failure_cases": "DeepProbLog requires circuit construction (SDD) which can be costly; DeepProbLog also requires training data organized in single-atom supervision (as noted by the authors), while NeurASP allows arbitrary propositional formulas in training data.",
            "comparison_baseline": "NeurASP is compared to DeepProbLog: both achieve similar accuracy on some experiments (digit-addition), but NeurASP is claimed to be more expressive (leveraging full ASP constructs) and faster in training because it avoids building circuits.",
            "uuid": "e4888.4",
            "source_info": {
                "paper_title": "NeurASP: Embracing Neural Networks into Answer Set Programming",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DeepProbLog",
            "rating": 2,
            "sanitized_title": "deepproblog"
        },
        {
            "paper_title": "Can convolutional neural networks crack sudoku",
            "rating": 2,
            "sanitized_title": "can_convolutional_neural_networks_crack_sudoku"
        },
        {
            "paper_title": "A semantic loss function for deep learning with symbolic knowledge",
            "rating": 2,
            "sanitized_title": "a_semantic_loss_function_for_deep_learning_with_symbolic_knowledge"
        },
        {
            "paper_title": "The neurosymbolic concept learner: interpreting scenes, words, and sentences from natural supervision",
            "rating": 2,
            "sanitized_title": "the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision"
        },
        {
            "paper_title": "Logic tensor networks for semantic image interpretation",
            "rating": 1,
            "sanitized_title": "logic_tensor_networks_for_semantic_image_interpretation"
        }
    ],
    "cost": 0.018856249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>NeurASP: Embracing Neural Networks into Answer Set Programming</p>
<p>Zhun Yang 
Arizona State University
TempeAZUSA</p>
<p>Adam Ishay aishay@asu.edu 
Arizona State University
TempeAZUSA</p>
<p>Joohyung Lee joolee@asu.edu 
Arizona State University
TempeAZUSA</p>
<p>Samsung Research
SeoulSouth Korea</p>
<p>NeurASP: Embracing Neural Networks into Answer Set Programming
226094EECA3D55F5B945E3F8A8982B2E
We present NeurASP, a simple extension of answer set programs by embracing neural networks.By treating the neural network output as the probability distribution over atomic facts in answer set programs, NeurASP provides a simple and effective way to integrate sub-symbolic and symbolic computation.We demonstrate how NeurASP can make use of a pre-trained neural network in symbolic computation and how it can improve the neural network's perception result by applying symbolic reasoning in answer set programming.Also, NeurASP can be used to train a neural network better by training with ASP rules so that a neural network not only learns from implicit correlations from the data but also from the explicit complex semantic constraints expressed by the rules.</p>
<p>Introduction</p>
<p>The integration of low-level perception with high-level reasoning is one of the oldest problems in Artificial Intelligence.Today, the topic is revisited with the recent rise of deep neural networks.Several proposals were made to implement the reasoning process in complex neural network architectures, e.g., [Cohen et al., 2018;Rocktäschel and Riedel, 2017;Donadello et al., 2017;Kazemi and Poole, 2018;Šourek et al., 2015;Palm et al., 2018;Lin et al., 2019].However, it is still not clear how complex and high-level reasoning, such as default reasoning [Reiter, 1980], ontology reasoning [Baader et al., 2003], and causal reasoning [Pearl, 2000], can be successfully computed by these approaches.The latter subject has been well-studied in the area of knowledge representation (KR), but many KR formalisms, including answer set programming (ASP) [Lifschitz, 2008;Brewka et al., 2011], are logic-oriented and do not incorporate high-dimensional vector space and pre-trained models for perception tasks as handled in deep learning, which limits the applicability of KR in many practical applications involving data and uncertainty.</p>
<p>In this paper, we present a simple extension of answer set programs by embracing neural networks.Following the idea of DeepProbLog [Manhaeve et al., 2018], by treating the neural network output as the probability distribution over atomic facts in answer set programs, the proposed NeurASP provides a simple and effective way to integrate sub-symbolic and symbolic computation.</p>
<p>We demonstrate how NeurASP can be useful for some tasks where both perception and reasoning are required.Reasoning can help identify perception mistakes that violate semantic constraints, which in turn can make perception more robust.For example, a neural network for object detection may return a bounding box and its classification "car," but it may not be clear whether it is a real car or a toy car.The distinction can be made by applying reasoning about the relations with the surrounding objects and using commonsense knowledge.Or when it is unclear whether a round object attached to the car is a wheel or a doughnut, the reasoner could conclude that it is more likely to be a wheel by applying commonsense knowledge.In the case of a neural network that recognizes digits in a given Sudoku board, the neural network may get confused if a digit next to 1 in the same row is 1 or 2, but the reasoner can conclude that it cannot be 1 by applying the constraints for Sudoku.</p>
<p>Another benefit of this hybrid approach is that it alleviates the burden of neural networks when the constraints/knowledge are already given.Instead of building a large end-to-end neural network that learns to solve a Sudoku puzzle given as an image, we can let a neural network only do digit recognition and use ASP to find the solution of the recognized board.This makes the design of the neural network simpler and the required training dataset much smaller.Also, when we need to solve some variation of Sudoku, such as Anti-knight or Offset Sudoku, the modification is simpler than training another large neural network from scratch to solve the new puzzle.</p>
<p>NeurASP can also be used to train a neural network together with rules so that a neural network not only learns from implicit correlations from the data but also from explicit complex semantic constraints expressed by ASP rules.The semantic loss [Xu et al., 2018] obtained from the reasoning module can be backpropagated into the rule layer and then further into neural networks via neural atoms.This sometimes makes a neural network learn better even with fewer data.</p>
<p>Compared to DeepProbLog, NeurASP supports a rich set arXiv:2307.07700v1 [cs.AI] 15 Jul 2023 of KR constructs supported by answer set programming that allows for convenient representation of complex knowledge.</p>
<p>It utilizes an ASP solver in computation instead of constructing circuits as in DeepProbLog.</p>
<p>The paper is organized as follows.Section 2 introduces the syntax and the semantics of NeurASP.Section 3 illustrates how reasoning in NeurASP can enhance the perception result by considering relations among objects perceived by pre-trained neural networks.Section 4 presents learning in NeurASP where ASP rules work as a semantic regularizer for training neural networks so that neural networks are trained not only from data but also from rules.Section 5 examines related works and Section 6 concludes.</p>
<p>The implementation of NeurASP, as well as codes used for the experiments, is publicly available online at https://github.com/azreasoners/NeurASP.</p>
<p>NeurASP</p>
<p>We present the syntax and the semantics of NeurASP.</p>
<p>Syntax</p>
<p>We assume that neural network M allows an arbitrary tensor as input whereas the output is a matrix in R e×n , where e is the number of random events predicted by the neural network and n is the number of possible outcomes for each random event.</p>
<p>Each row of the matrix represents the probability distribution of the outcomes of each event.For example, if M is a neural network for MNIST digit classification, then the input is a tensor representation of a digit image, e is 1, and n is 10.</p>
<p>If M is a neural network that outputs a Boolean value for each edge in a graph, then e is the number of edges and n is 2. Given an input tensor t, by M (t), we denote the output matrix of M .The value M (t)[i, j] (where i ∈ {1, . . ., e}, j ∈ {1, . . ., n}) is the probability of the j-th outcome of the i-th event upon the input t.</p>
<p>In NeurASP, the neural network M above can be represented by a neural atom of the form
nn(m(e, t), [v 1 , . . . , v n ]),(1)
where (i) nn is a reserved keyword to denote a neural atom;</p>
<p>(ii) m is an identifier (symbolic name) of the neural network M ; (iii) t is a list of terms that serves as a "pointer" to an input data; related to it, there is a mapping D (implemented by an external Python code) that turns t into an input tensor;</p>
<p>(iv) v 1 , . . ., v n represent all n possible outcomes of each of the e random events.Each neural atom (1) introduces propositional atoms of the form c = v, where c ∈ {m 1 (t), . . ., m e (t)} and v ∈ {v 1 , . . ., v n }.The output of the neural network provides the probabilities of the introduced atoms (defined in Section 2.2).</p>
<p>Example 1 Let M digit be a neural network that classifies an MNIST digit image.The input of M digit is (a tensor representation of) an image and the output is a matrix in R 1×10 .The neural network can be represented by the neural atom nn(digit (1, d), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),
which introduces propositional atoms digit 1 (d) = 0, digit 1 (d) = 1, . . . , digit 1 (d) = 9.
Example 2 Let M sp be another neural network for finding the shortest path in a graph with 24 edges.The input is a tensor encoding the graph and the start/end nodes of the path, and the output is a matrix in R 24×2 .This neural network can be represented by the neural atom nn(sp(24, g), [TRUE, FALSE]).</p>
<p>A NeurASP program Π is the union of Π asp and Π nn , where Π asp is a set of propositional rules (standard rules as in ASP-Core 2 [Calimeri et al., 2020]) and Π nn is a set of neural atoms.Let σ nn be the set of all atoms m i (t) = v j that is obtained from the neural atoms in Π nn as described above.We require that, in each rule ıHead ← ıBody in Π asp , no atoms in σ nn appear in ıHead.</p>
<p>We could allow schematic variables into Π, which are understood in terms of grounding as in standard answer set programs.We find it convenient to use rules of the form
nn(m(e, t), [v 1 , . . . , v n ]) ← ıBody (2)
where ıBody is either identified by or ⊥ during grounding so that (2) can be viewed as an abbreviation of multiple (variable-free) neural atoms (1)., 2, 3, 4, 5, 6, 7, 8, 9]) ← img(X).addition(A, B, N
) ← digit 1 (A) = N 1 , digit 1 (B) = N 2 , N = N 1 + N 2 .
(3) The neural network M digit outputs 10 probabilities for each image.The addition is applied once the digits are recognized and its probability is induced from the perception as we explain in the next section.</p>
<p>Semantics</p>
<p>For any NeurASP program Π = Π asp ∪ Π nn , we first obtain its ASP counterpart Π = Π asp ∪ Π ch where Π ch consists of the following set of rules for each neural atom (1) in Π nn
{m i (t) = v 1 ; . . . ; m i (t) = v n } = 1 for i ∈ {1, . . . e}.
The above rule (in the language of CLINGO) means to choose exactly one atom in between the set braces. 1 We define the stable models of Π as the stable models of Π , and define the total choices of Π as the stable models of Π ch .For each total choice C of Π, we use ıN um(C, Π) to denote the number of stable models of Π that satisfy C. We require a NeurASP program Π to be coherent such that ıN um(C, Π) &gt; 0 for every total choice C of Π.</p>
<p>To define the probability of a stable model, we first define the probability of an atom m i (t) = v j in σ nn .Recall that there is an external mapping D that turns t into a specific input tensor of M .The probability of each atom m i (t) = v j is defined as M (D(t))[i, j]:
P Π (m i (t) = v j ) = M (D(t))[i, j].
For instance, recall that the output matrix of
M digit (D(d)) in Example 3 is in R 1×10 . The probability of atom digit 1 (d) = k is M digit (D(d))[1, k+1].
Given an interpretation I, by I| σ nn , we denote the projection of I onto σ nn .Since I| σ nn is a total choice of Π, ıN um(I| σ nn , Π) is the number of stable models of Π that agree with I| σ nn on σ nn .</p>
<p>The probability of a stable model I of Π is defined as the product of the probability of each atom c = v in I| σ nn , divided by the number of stable models of Π that agree with I| σ nn on σ nn .That is, for any interpretation I,
P Π (I) =    c=v∈I| σ nn PΠ(c=v) ıN um(I| σ nn ,Π) if I is a stable model of Π; 0 otherwise.
An observation is a set of ASP constraints (i.e., rules of the form ⊥ ← ıBody).The probability of an observation O is defined as
P Π (O) = I|=O P Π (I) (I |= O denotes that I satisfies O).
The probability of the set O = {O 1 , . . ., O o } of observations is defined as the product of the probability of each O i :
P Π (O) = Oi∈O P Π (O i ).
Example 3 Continued The ASP program Π digit , which is the ASP counterpart of Π digit , is obtained from (3) by replacing the third rule with
{digit 1 (d 1 ) = 0; . . . ; digit 1 (d 1 ) = 9} = 1. {digit 1 (d 2 ) = 0; . . . ; digit 1 (d 2 ) = 9} = 1.
The following are the stable models of Π digit , i.e., the stable models of Π digit .
I1 = {digit1(d1) = 0, digit1(d2) = 0, addition(d1, d2, 0), . . . }, I2 = {digit1(d1) = 0, digit1(d2) = 1, addition(d1, d2, 1), . . . }, I3 = {digit1(d1) = 1, digit1(d2) = 0, addition(d1, d2, 1), . . . }, . . . , I100 = {digit1(d1) = 9, digit1(d2) = 9, addition(d1, d2, 18), . . . }.
Their probabilities are as follows:
PΠ(I1) = M digit (D(d1))[1, 1] × M digit (D(d2))[1, 1], . . . , PΠ(I100) = M digit (D(d1))[1, 10] × M digit (D(d2))[1, 10].</p>
<p>The probability of
O = {← ınot addition(d 1 , d 2 , 1)} is P Π (O) = P Π (I 2 ) + P Π (I 3 ).</p>
<p>Inference with NeurASP</p>
<p>We implemented NeurASP by integrating PyTorch [Adam et al., 2017] and CLINGO [Gebser et al., 2011].PyTorch takes care of neural network processing including data loading and mapping D that maps pointer terms in neural atoms to input tensors.Computing the probability of a stable model is done by calling CLINGO and post-processing in Python.This section illustrates how this integration can be useful in reasoning about relations among objects recognized by neural networks.</p>
<p>Commonsense Reasoning about Image</p>
<p>Suppose we have a neural network M label that outputs classes of objects in the bounding boxes that are already detected.The following rule asserts that the neural network M label classifies the bounding box B into one of {car, cat, person, truck, other}, where B is at location The first rule says that there is a bounding box b 1 (i.e., the red box with a child) in image i 1 , and the coordinates of its left-top and right-bottom corners are (100, 0) and (450, 350).
(X 1 , Y 1 , X 2 , Y 2 ) in image I: nn(label(1, I, B), [car, cat, person, truck, other]) ← box(I, B, X 1 , Y 1 , X 2 , Y 2 ).
Below we describe rules that allow for reasoning about the recognized objects.The following rules describe the general size relation between objects.</p>
<p>smaller(cat, person).smaller(person, car).smaller(person, truck).smaller(X, Y ) ← smaller(X, Z), smaller(Z, Y ).</p>
<p>Next is the rule asserting that by default we conclude the same size relationship as above.
smaller(I, B 1 , B 2 ) ← ınot ∼smaller(I, B 1 , B 2 ), label 1 (I, B 1 ) = L 1 , label 1 (I, B 2 ) = L 2 , smaller(L 1 , L 2 ).
(The ∼ symbol stands for strong negation in ASP, which asserts explicit falsity.)</p>
<p>On the other hand, there are some exceptions, for instance,
∼smaller(I, B 2 , B 1 ) ← box(I, B 1 , X 1 , Y 1 , X 2 , Y 2 ), box(I, B 2 , X 1 , Y 1 , X 2 , Y 2 ), Y 2 ≥ Y 2 , |X 1 − X 2 | × |Y 1 − Y 2 | &lt; |X 1 − X 2 | × |Y 1 − Y 2 |. smaller(I, B 1 , B 2 ) ← ∼smaller(I, B 2 , B 1 ). toy(I, B 1 ) ← label 1 (I, B 1 ) = L 1 , label 1 (I, B 2 ) = L 2 , smaller(I, B 1 , B 2 ), smaller(L 2 , L 1 ).
The first rule says that "B 2 is not smaller than B 1 if (i) B 1 and B 2 are objects in image I, (ii) B 1 is closer to the camera (i.e., B 1 's bottom boundary is closer to the bottom of I), and (iii) the box in the image for B 1 is smaller than
B 2 ." 2
The neural network model M label outputs that the red boxes are persons, the yellow boxes are cars, and the green box is a truck.Upon this input and the rules above, NeurASP allows us to derive that the two cars in image i 1 are toy cars, whereas the two cars in image i 2 are not: although they are surrounded by smaller boxes than those of humans, their boxes are not closer to the camera.</p>
<p>Example: Solving Sudoku Puzzle in Image</p>
<p>Consider the task of solving a Sudoku puzzle given as an image.In NeurASP, we could use a neural network to recognize the digits in the given puzzle and use an ASP solver to compute the solution instead of having a single network that accounts for both perception and solving.</p>
<p>We use the following NeurASP program Π sudoku to first identify the digits in each grid cell on the board and then find the solution by assigning digits to all empty grid cells.3% identify the number in each of the 81 positions nn(identify(81, img), [empty,1,2,3,4,5,6,7,8,9]).</p>
<p>% assign one number N to each position (R,C) a(R,C,N) :-identify(Pos,img,N), R=Pos/9, C=Pos\9, N!=empty.{a(R,C,N): N=1..9}=1 :-identify(Pos, img, empty), R=Pos/9, C=Pos\9.</p>
<p>% no number repeats in the same row :-a(R,C1,N), a(R,C2,N), C1!=C2.</p>
<p>% no number repeats in the same column :-a(R1,C,N), a(R2,C,N), R1!=R2.</p>
<p>% no number repeats in the same 3 * 3 box
:-a(R,C,N), a(R1,C1,N), R!=R1, C!=C1, ((R/3) * 3 + C/3) = ((R1/3) * 3 + C1/3).
each of the 81 grid cells.The network M identify is pretrained using image, label pairs, where each image is a Sudoku board image generated by OpenSky Sudoku Generator (http://www.opensky.ca/∼ jdhildeb/software/sudokugen/) and each label is a vector of length 81 in which 0 is used to represent an empty cell at that position.Let Acc identify denote the accuracy of identifying all empty cells and the digits on the board given as an image without making a single mistake in a grid cell.Let Acc sol denote the accuracy of solving a given Sudoku board without making a single mistake in a grid cell.Let r be the following rule in Π sudoku : {a(R,C,N): N=1..9}=1 :-identify(Pos, img, empty), R=Pos/9, C=Pos\9.Intuitively, Π sudoku \r only checks whether the identified numbers (by neural network M identify ) satisfy the three constraints (the last three rules of Π sudoku ), while Π sudoku further checks whether there exists a solution given the identified numbers.As shown in Table 1, the use of reasoning in NeurASP program Π sudoku \ r improves the accuracy Acc identify of the neural network M identify as explained in the introduction.The accuracy Acc identify is further improved by trying to solve Sudoku completely using Π sudoku .Note that the solution accuracy Acc sol of Π sudoku is equal to the perception accuracy Acc identify of Π sudoku since the ASP yields a 100% correct solution once the board is correctly identified.</p>
<p>Palm et al.</p>
<p>[2018] use a Graph Neural Network to solve Sudoku but the work restricts attention to textual input of the Sudoku board, not images as we do.Their work achieves 96.6% accuracy after training with 216,000 examples.In comparison, even with the more challenging task of accepting images as input, the number of training examples we used is 15 -25, which is much less than the number of training examples used in [Palm et al., 2018].Our work takes advantage of the fact that in a problem like Sudoku, where the constraints are explicitly given, a neural network only needs to focus on perception tasks, which is simpler than learning the perception and reasoning together.</p>
<p>Furthermore, using the same trained perception neural network M identify , we can solve some elaborations of Sudoku problems by adding the following rules:</p>
<p>[Anti-knight Sudoku] No number repeats at a knight move :-a(R1,C1,N), a(R2,C2,N), |R1-R2|+|C1-C2|=3.</p>
<p>[Sudoku-X] No number repeats at the diagonals :-a(R1,C1,N), a(R2,C2,N), R1=C1, R2=C2, R1!=R2.:-a(R1,C1,N), a(R2,C2,N), R1+C1=8, R2+C2=8, R1!=R2.</p>
<p>With neural network only approach, since the neural network needs to learn both perception and reasoning, each of the above variations would require training a complex and different model with a big dataset.However, with NeurASP, the neural network only needs to recognize digits on the board.Thus solving each Sudoku variation above uses the same pre-trained model for the image input and we only need to add the aforementioned rules to Π sudoku .Some Sudoku variations, such as Offset Sudoku, are in colored images.In this case, we need to increase the number of channels of M identify from 1 to 3, and need to retrain the neural network with the colored images.Although not completely elaboration tolerant, compared to the pure neural network approach, this is significantly simpler.For instance, the number of training data needed to get 100% perception accuracy for Offset Sudoku (Acc identify ) is 70, which is still much smaller than what the end-to-end Sudoku solver would require.Using the new network trained, we only need to add the following rule to Π sudoku .</p>
<p>[Offset Sudoku] No number repeats at the same relative position in 3*3 boxes
:-a(R1,C1,N), a(R2,C2,N), R1\3 = R2\3, C1\3 = C2\3, R1 != R2, C1 != C2.</p>
<p>Learning in NeurASP</p>
<p>We show how the semantic constraints expressed in NeurASP can be used to train neural networks better.</p>
<p>Gradient Ascent with NeurASP</p>
<p>In Let p denote the probabilities of the atoms in σ nn .Since p is indeed the outputs of the neural networks in Π(θ), we can compute the gradient of p w.r.t.θ through backpropagation.</p>
<p>Then the gradient of
O∈O log(P Π(θ) (O)) w.r.t. θ is ∂ O∈O log(P Π(θ) (O)) ∂θ = O∈O ∂log(P Π(θ) (O)) ∂p × ∂p ∂θ
where ∂p ∂θ can be computed through the usual neural network backpropagation, while
∂log(P Π(θ) (O)) ∂p
for each p ∈ p can be computed as follows.</p>
<p>Proposition 1 Let Π(θ) be a NeurASP program and let O be an observation such that P Π(θ) (O) &gt; 0. Let p denote the probability of an atom c = v in σ nn , i.e., p denotes P Π(θ) (c = v).We have that 4
∂log(P Π(θ) (O)) ∂p = I: I|=O I|=c=v P Π(θ) (I) P Π(θ) (c=v) − I,v : I|=O I|=c=v ,v =v P Π(θ) (I) P Π(θ) (c=v ) I: I|=O P Π(θ) (I)
.</p>
<p>Intuitively, the proposition tells us that each interpretation I that satisfies O tends to increase the value of p if I |= c = v, and decrease the value of p if
I |= c = v such that v = v.
NeurASP internally calls CLINGO to find all stable models I of Π(θ) that satisfy O and uses PyTorch to obtain the probability of each atom c = v in σ nn .</p>
<p>Experiment 1: Learning Digit Classification from Addition</p>
<p>All experiments in Section 4 were done on Ubuntu 18.04.2LTS with two 10-cores CPU Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz and four GP104 [GeForce GTX 1080].</p>
<p>The digit addition problem is a simple example used in [Manhaeve et al., 2018] to illustrate DeepProbLog's ability for both logical reasoning and deep learning.The task is, given a pair of digit images (MNIST) and their sum as the label, to let a neural network learn the digit classification of the input images.</p>
<p>The problem can be represented by NeurASP program Π digit in Example 3.For comparison, we use the same dataset and the same structure of the neural network model used in [Manhaeve et al., 2018] to train the digit classifier M digit in Π digit .For each pair of images denoted by d 1 and d 2 and their sum n, we construct the ASP constraint ← ınot addition(d 1 , d 2 , n) as the observation O.The training target is to maximize log(P Π digit (O)).</p>
<p>Figure 2 shows how the forward and the backward propagations are done for NeurASP program Π digit in Example 3. The left-to-right direction is the forward computation of the neural network extended with the rule layer, whose output is the probability of the observation O.The right-to-left direction shows how the gradient from the rule layer is backpropagated further into the neural network by the chain rule to update all neural network parameters so as to find the parameter values that maximize the probability of the given observation.</p>
<p>Figure 3 shows the accuracy on the test data after each training iteration.The method CNN denotes the baseline used in [Manhaeve et al., 2018] where a convolutional neural network (with more parameters) is trained to classify the concatenation of the two images into the 19 possible sums.As we can see, the neural networks trained by NeurASP and Deep-ProbLog converge much faster than CNN and have almost the same accuracy at each iteration.However, NeurASP spends much less time on training compared to DeepProbLog.The time reported is for one epoch (30,000 iterations in gradient descent).This is because DeepProbLog constructs an 4 P Π(θ) (I) P Π(θ) (c=v) and P Π(θ) (I) P Π(θ) (c=v ) are still well-defined since the denominators have common factors in P Π(θ) (I).</p>
<p>Experiment 2: Learning How to Solve Sudoku</p>
<p>In section 3.2, we used a neural network M identify to identify the numbers on a Sudoku board and used ASP rules to solve the Sudoku problem.In this section, we use a neural network to learn to solve Sudoku problems.The task is, given the textual representation of an unsolved Sudoku board (in the form of a 9 × 9 matrix where an empty cell is represented by 0), to let a neural network learn to predict the solution of the Sudoku board.</p>
<p>We use the neural network M sol from [Park, 2018] as the baseline.M sol is composed of 9 convolutional layers and a 1x1 convolution layer followed by softmax.Park trained M sol using 1 million examples and achieved 70% accuracy using an "inference trick": instead of predicting digits for all empty cells at once, which leads to a poor accuracy, the most probable grid-cell value was predicted one by one.</p>
<p>Since the current NeurASP implementation is not as scalable as neural network training, training on 1 million examples takes too long.Thus, we construct a dataset of 63,000 + 1000 conf ig, label pairs for training and testing.Using Park's method on this relatively small dataset, we observe that M sol 's highest whole-board accuracy Acc sol 5 is only 29.1%</p>
<p>5 The percentage of Sudoku examples that are correctly solved.</p>
<p>and M sol 's highest grid-cell accuracy6 is only 89.3% after 63 epochs of training.</p>
<p>We get a better result by training M sol with the NeurASP program Π sol .The program is almost the same as Π identify in Section 3.2 except that it uses M sol in place of M identify and the first three rules of Π identify are replaced with nn(sol(81, img), [1,2,3,4,5,6,7,8,9]). a(R,C,N) :-sol(Pos,img,N), R=Pos/9, C=Pos\9.</p>
<p>because we do not have to assign the value empty in solving Sudoku.</p>
<p>We trained M sol using NeurASP where the training target is to maximize the probability of all stable models that satisfy the observation.On the same test data, after 63 epochs of training, the highest whole-board accuracy of M sol trained this way is 66.5% and the highest grid-cell accuracy is 96.9%(In other words, we use rules only during training and not during testing).This indicates that including such structured knowledge sometimes helps the training of the neural network significantly.</p>
<p>Experiment 3: Learning Shortest Path (SP)</p>
<p>The experiment is about, given a graph and two points, finding the shortest path between them.We use the dataset from [Xu et al., 2018], which was used to demonstrate the effectiveness of semantic constraints for enhanced neural network together with the union of the following 4 constraints defines the shortest path.In this experiment, we trained the same neural network model M sp as in [Xu et al., 2018], a 5-layer Multi-Layer Perceptron (MLP), but with 4 different settings: (i) MLP only; (ii) together with NeurASP with the simple-path constraint (p) (which is the only constraint used in [Xu et al., 2018]);8 (iii) together with NeurASP with simple-path, reachability, and optimization constraints (p-r-o); and (iv) together with NeurASP with all 4 constraints (p-r-o-nr).9</p>
<p>Table 2 shows, after 500 epochs of training, the percentage of the predictions on the test data that satisfy each of the constraints p, r, and nr, the path constraint (i.e., p-r), the shortest path constraint (i.e., p-r-o-nr), and the accuracy w.r.t. the ground truth.</p>
<p>The accuracies for the first experiment (MLP Only) show that M sp was not trained well only by minimizing the crossentropy loss of its prediction: 100-28.3= 71.7% of the predictions are not even a simple-path.</p>
<p>In the remaining experiments (MLP (x)), instead of minimizing the cross-entropy loss, our training target is changed to maximizing the probability of all stable models under certain constraints.The accuracies under the 2nd and 3rd experiments (MLP (p) and MLP (p-r-o) columns) are increased significantly, showing that (i) including such structured knowledge helps the training of the neural network and (ii) the more structured knowledge included, the better M sp is trained under NeurASP.Compared to the results from [Xu et al., 2018], M sp trained by NeurASP with the simple-path constraint p (in the 2nd experiment MLP (p) column) obtains a similar accuracy on predicting the label (28.9% v.s.28.5%) but a higher accuracy on predicting a simple-path (96.6% v.s.69.9%).</p>
<p>In the 4th experiment (MLP (p-r-o-nr) column) where we added the constraint nr saying that "no removed edges can be predicted", the accuracies go down.This is because the new constraint nr is about randomly removed edges, changing from one example to another, which is hard to be generalized.</p>
<p>Related Work</p>
<p>Recent years have observed the rising interests of combining perception and reasoning.As mentioned, the work on DeepProbLog [Manhaeve et al., 2018] is closest to our work.Some differences are: (i) The computation of DeepProbLog relies on constructing circuits such as sequential decision diagrams (SDD) whereas we use an ASP solver internally.(ii) NeurASP employs expressive reasoning originating from answer set programming, such as defaults, aggregates, and optimization rules.This not only gives more expressive reasoning but also allows the more semantic-rich constructs as guide to learning.(iii) DeepProbLog requires each training data to be a single atom, while NeurASP allows each training data to be arbitrary propositional formulas.Also related is using the semantic constraints to train neural networks better [Xu et al., 2018], but the constraints used in that work are simple propositional formulas whereas we use answer set programming language, in which it is more convenient to encode complex KR constraints.Logic Tensor Network [Donadello et al., 2017] is also related in that it uses neural networks to provide fuzzy values to atoms.</p>
<p>Another approach is to embed logic rules in neural networks by representing logical connectives by mathematical operations and allowing the value of an atom to be a real number.For example, Neural Theorem Prover (NTP) [Rocktäschel and Riedel, 2017] adopts the idea of dynamic neural module networks [Andreas et al., 2016] to embed logic conjunction and disjunction in and/or-module networks.A proof-tree like end-to-end differentiable neural network is then constructed using Prolog's backward chaining algorithm with these modules.Another method that also constructs a proof-tree like neural network is TensorLog [Cohen et al., 2018], which uses matrix multiplication to simulate belief propagation that is tractable under the restriction that each rule is negation-free and can be transformed into a polytree.</p>
<p>Graph neural network (GNN) [Kipf and Welling, 2017] is a neural network model that is gaining more attention recently.Since a graph can encode objects and relations between objects, by learning message functions between the nodes, one can perform certain relational reasoning over the objects.For example, in [Palm et al., 2018], it is shown that GNN can do well on Sudoku, but the input there is not an image but a textual representation.However, this is still restrictive compared to the more complex reasoning that KR formalisms provide.</p>
<p>Neuro-Symbolic Concept Learner [Mao et al., 2019] separates between visual perception and symbolic reasoning.It shows the data-efficiency by using only 10% of the training data and achieving the state-of-the-art 98% accuracy on CLEVR dataset.Our results are similar in the sense that using symbolic reasoning, we could use fewer data to achieve a high accuracy.</p>
<p>NeurASP is similar to LP MLN [Lee and Wang, 2016] in the sense that they are both probabilistic extensions of ASP and their semantics are defined by translations into ASP [Lee and Yang, 2017].LP MLN allows any rules to be weighted, whereas NeurASP uses standard ASP rules.</p>
<p>We showed that NeurASP can improve the neural network's perception result by applying reasoning over perceived objects and also can help neural network learn better by compensating the small size data with knowledge and constraints.Since NeurASP is a simple integration of ASP with neural networks, it retains each of ASP and neural networks in individual forms, and can directly utilize the advances in each of them.</p>
<p>The current implementation is a prototype and not highly scalable due to a naive computation of enumerating stable models.The future work includes how to make learning faster, and also analyzing the effects of the semantic constraints more systematically.</p>
<p>win=true :-head=true.win=false :-not win=true.1{head=true; head=false}1.:-2{win=true; win=false}.</p>
<p>It has 2 stable models: I 1 = {head = TRUE, win = TRUE} and I 2 = {head = FALSE, win = FALSE}, which are the stable models of Π coin .There are 2 atoms in σ p and their probabilities are P Πcoin (head = TRUE) = 0.1 and P Πcoin (head = FALSE) = 0.9.Then the probabilities of I 1 and I 2 can be computed as follows.</p>
<p>P Πcoin (I 1 ) = 0.1 1 = 0.1 P Πcoin (I 2 ) = 0.9 1 = 0.9</p>
<p>And the probability of O = {ınot win = TRUE} is P Πcoin (O)) = P Πcoin (I 2 ) = 0.9.</p>
<p>A.2 Define NeurASP On a Translation to MVPP Syntax: We first define the notion of a neural atom to describe a neural network in a logic program.Intuitively, a neural atom can be seen as the shorthand for a sequence of probabilistic rules whose atoms are defined by a syntactical translation from the neural atom and whose probabilities are the outputs of neural networks.We assume that neural network M allows an arbitrary tensor as input whereas the output is a matrix in R e×n , where e is the number of random events predicted by the neural network, and n is the number of possible outcomes for each random event.Each row of the matrix represents the probability distribution of the outcomes of each event.Given an input tensor t, by M (t), we denote the output matrix of M .M (t)[i, j] (i ∈ {1, . . ., e}, j ∈ {1, . . ., n}) is the probability at the i-th row and j-th column of the matrix.For example, in a neural network for MNIST digit classification, the input is a tensor representation of a digit image, e is 1, and n is 10.For a neural network that outputs a Boolean value for each edge in a graph, e is the number of edges and n is 2.</p>
<p>In NeurASP, the neural network M above can be represented by a neural atom of the form
nn(m(e, t), [v 1 , . . . , v n ]),(5)
where (i) nn is a reserved keyword to denote a neural atom; (ii) m is an identifier (symbolic name) of the neural network M ; (iii) t is a list of terms that serves as a "pointer" to an input data; the mapping D is implemented by the external Python code that accepts NeurASP program as input, and can map t to different data instances by iteratively loading from the dataset; this is useful for training; (iv) v 1 , . . ., v n represent all n possible outcomes of each of the e random events.Each neural atom (5) introduces propositional atoms of the form c = v, where c ∈ {m 1 (t), . . ., m e (t)} and v ∈ {v 1 , . . ., v n }.The output of the neural network provides the probabilities of the introduced atoms (defined in Section 2.2).</p>
<p>Example 5 Let M digit be a neural network that classifies an MNIST digit image.The input of M digit is (a tensor representation of) an image and the output is a matrix in R 1×10 .The neural network can be represented as the neural atom nn(digit (1, d), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
which introduces propositional atoms digit 1 (d) = 0, digit 1 (d) = 1, . . . , digit 1 (d) = 9.
Let M sp be another neural network for finding the shortest path in a graph with 24 edges.The input is a tensor encoding the graph and the start/end nodes of the path, and the output is a matrix in R 24×2 .This neural network can be represented as the neural atom nn(sp(24, g), [TRUE, FALSE]).</p>
<p>A NeurASP program Π is the union of Π mvpp and Π nn where Π mvpp is an MVPP program, and Π nn is a set of neural atoms.Let σ nn be the set of all atoms c = v that is obtained from the neural atoms in Π nn as described above.We require that no atoms in σ nn appear in the probabilistic rules in Π mvpp or in ıHead of each ASP rule ıHead ← ıBody in Π mvpp .</p>
<p>We could allow schematic variables into Π, which are understood in terms of grounding as in standard answer set programs.We find it convenient to use rules of the form
nn(m(e, t), [v 1 , . . . , v n ]) ← ıBody (6)
where ıBody is either identified by or ⊥ during grounding so that (6) can be viewed as an abbreviation of multiple (variablefree) neural atoms (5)., 2, 3, 4, 5, 6, 7, 8, 9]) ← img(X).addition(A, B, N
) ← digit1(A) = N1, digit1(B) = N2, N = N1 + N2.
The neural network M digit generates 10 probabilities for each image.The addition is applied once the digits are recognized and its probability is induced from the perception as we explain in the next section.</p>
<p>Semantics: For any NeurASP program Π = Π mvpp ∪ Π nn , we obtain its MVPP counterpart Π by replacing each neural atom nn(m(e, t), [v 1 , . . ., v n ]) in Π nn with the set of probabilistic rules
p 1,1 : m 1 (t) = v 1 | . . . | p 1,n : m 1 (t) = v n . . . p e,1 : m e (t) = v 1 | . . . | p e,n : m e (t) = v n
where p i,j denotes the probability of atom m i (t) = v j .Recall that there is an external mapping D that turns t into a specific input tensor of M , the value of p i,j is the neural network output M (D(t)) [i, j].</p>
<p>The stable models of a NeurASP program Π are defined as the stable models of its MVPP counterpart Π .The probability of each stable model I under Π is defined as its probability under Π .</p>
<p>Example 6 Continued: The following MVPP program is Π digit , i.e., the MVPP counterpart of  Algorithm 1 uses the function gradientsSM , which is defined in Algorithm 2 below.Algorithm 3 is almost the same as Algorithm 1 except that, in step 1-(a)-iii, instead of finding all stable models that satisfy O i , it randomly samples num of samples stable models that satisfy O i according to their probability distribution.The function sampleSM used in Algorithm 3 to sample stable models is defined in Algorithm 4.
Π digit . img(d1). img(d2). addition(A, B, N ) ← digit1(A) = N1, digit1(B) = N2, N = N1 + N2. p d 1 1,1 : digit1(d1) = 0 | . . . | p d 1 1,10 : digit1(d1) = 9 p d 2 1,1 : digit1(d2) = 0 | . . . | p d 2 1,10 : digit1(d2) = 9 Recall that M digit (D(d)) ∈ R 1×10 is the output matrix of M digit in(c = v). ∂log(P Π(θ) (O)) ∂p i = I|=O I|=c=v P Π(θ) (I) P Π(θ) (c=v) − I|=O I|=c=v ,v =v P Π(θ) (I) P Π(θ) (c=v ) I|=O P Π(θ) (I) [proof] ∂log(P Π(θ) (O)) ∂pi = ∂log(P Π(θ) (O)) ∂P Π(θ) (c=v) = 1 P Π(θ) (O) × ∂P Π(θ) (O) ∂P Π(θ) (c=v) (since P Π(θ) (O) = I|=O P Π(θ) (I)) = 1 I|=O P Π(θ) (I) × ∂ I|=O P Π(θ) (I) ∂P Π(θ) (c=v) (since (i) P Π(θ) (I) = 0 if I is not a stable model of Π(θ) and (ii) any stable model I of Π(θ) must satisfy c = v * for some v * ) = 1 I|=O P Π(θ) (I) × ∂ I is a stable model of Π(θ) I|=O I c=v P Π(θ) (I) ∂P Π(θ) (c=v) + ∂ I,v |I is a stable model of Π(θ) I|=O I c=v ,v =v P Π(θ) (I) ∂P Π(θ) (c=v) (since for any stable model I of Π(θ), P Π(θ) (I) = c * =v * ∈I|σ m P Π(θ) (c * =v * ) N um(I|σ m ,Π(θ)) ) = 1 I|=O P Π(θ) (I) × ∂ I is a stable model of Π(θ) I|=O I c=v c * =v * ∈I|σ m P Π(θ) (c * =v * ) N um(I|σ m ,Π(θ)) ∂P Π(θ) (c=v) + ∂ I is a stable model of Π(θ) I|=O I c=v ,v =v c * =v * ∈I|σ m P Π(θ) (c * =v * ) N um(I|σ m ,Π(θ)) ∂P Π(θ) (c=v) = 1 I|=O P Π(θ) (I) × I is a stable model of Π(θ) I|=O I c=v ∂ c * =v * ∈I|σ m P Π(θ) (c * =v * ) N um(I|σ m ,Π(θ)) ∂P Π(θ) (c=v) + I is a stable model of Π(θ) I|=O I c=v ,v =v
Figure 1 :
1
Figure 1: Reasoning about relations among perceived objects</p>
<p>this section, we denote a NeurASP program by Π(θ) where θ is the set of the parameters in the neural network models associated with Π. Assume a NeurASP program Π(θ) and a set O of observations such that P Π(θ) (O) &gt; 0 for each O ∈ O.The task is to find θ that maximizes the log-likelihood of observations O under program Π(θ), i.e., θ ∈ argmax θ log(P Π(θ) (O)), which is equivalent to θ ∈ argmax θ O∈O log(P Π(θ) (O)).</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: NeurASP Gradient Propagation</p>
<p>learning.Each example is a 4 by 4 grid G = (V, E), where |V | = 16, |E| = 24.The source and the destination nodes are randomly picked up, as well as 8 edges are randomly removed to increase the difficulty.The dataset is divided into 60/20/20 train/validation/test examples.The following NeurASP program 7 nn(sp(24, g), [true, false]).sp(0,1) :-sp(1,g,true).... sp(X,Y) :-sp(Y,X).</p>
<p>∂cC</p>
<ul>
<li>=v * ∈I|σ m P Π(θ) (c * =v * ) N um(I|σ m ,Π(θ)) ∂P Π(θ) (c=v) =v * ∈I|σ m P Π(θ) (c * =v * ) N um(I|σ m ,Π(θ))P Π(θ) (c=v) + I is a stable model of Π(θ) I|=O I c=v ,v =v ∂ c * =v * ∈I|σ m P Π(θ) (c * =v * ) N um(I|σ m ,Π(θ)) ∂P Π(θ) (c=v ) × ∂P Π(θ) (c=v ) ∂P Π(θ) (c=v) (since P Π(θ) (c = v ) = 1 − P Π(θ) (c = v) − . . . ) Moreon Sudoku Experiments [[ Adam ]] D Detailed Description of Learning Algorithms for NeurASP Consider a NeurASP program Π(θ) and a set O of observations such that P Π(θ) (O) &gt; 0 for each O ∈ O.The task is to find θ that maximizes the log-likelihood of observations O under program Π(θ), i.e., θ ∈ argmax θ log(P Π(θ) (O)), which is equivalent to θ ∈ argmax θ O∈O log(P Π(θ) (O)).Let p denote the probabilities of the atoms in σ nn .Since p is indeed the outputs of the neural networks in Π(θ), we can compute the gradient of p w.r.t.θ through back-propagation.Then the gradients of O∈O log(P Π(θ) (O)) w.r.t.θ is can be computed through the usual neural network back-propagation, while ∂log(P Π(θ) (O)) ∂pfor each p ∈ p can be computed as follows.Proposition 1 Let p denote the probability of atom c = v, i.e., p denotes P Π(θ) (c = v).∂log(PΠ(θ)(O)) ∂p = I|=O I|=c=v P Π(θ) (I) P Π(θ) (c=v) − I|=O I|=c=v ,v =v P Π(θ) (I) P Π(θ) (c=v ) I|=O P Π(θ) (I)Algorithm 1 shows how to update the value of θ to maximize the log-likelihood of observations O under Π(θ).Algorithm 1 NeurASP weight learning by exact computationInput: 1. Π(θ): a NeurASP program (under signature σ) with parameters θ 2. O: a set of observations {O1, . . ., On} where each Oi is a set of ASP constraints such that P Π(θ) (O) &gt; 0 3. D: a set of mappings {D1, . . ., Dn} where each D i is associated with Oi and maps terms to input tensors of neural networks in Π(θ) 4. lr: a real number denoting learning rate 5. epoch: a positive integer denoting the number of epochs Output: 1. θ: the updated parameters such that θ ∈ argmax θ log(P Π(θ) (O)) Procedure: 1. Repeat for epoch number of times: (a) For Oi in O: i. Compute p, i.e., the outputs of the neural networks in Π(θ) according to D i ii.Compute ∂p ∂θ by back-propagation iii.Find all stable models IO i of Π(θ) that satisfy Oi (by calling CLINGO on Π ∪ Oi) iv.gradients = gradientsSM (Π(θ), p, IO i ) (Here, gradients is indeed ∂log(P Π(θ) (O i )) ∂p ) v. θ = θ + lr * gradients * ∂p ∂θ 2. return θ</li>
</ul>
<p>Example 3 An example NeurASP program Π digit is as follows, where d 1 and d 2 are terms representing two images.Each image is classified by neural network M digit as one of the values in {0, . . ., 9}.The addition of two digit-images is the sum of their values.
img(d 1 ).img(d 2 ).nn(digit(1, X), [0, 1</p>
<p>Table 1 compares
1
Acc identify of each of M identify , NeurASP program Π sudoku \ r with M identify , NeurASP program Π sudoku with M identify , as well as Acc sol of Π sudoku with M identify .</p>
<p>Table 1 :
1
Sudoku: Accuracy on Test Data
Num ofAcc identify ofAcc identify ofAcc identify ofAcc sol ofTrain DataM identifyNeurASP w/NeurASP w/NeurASP w/Π sudoku \rΠ sudokuΠ sudoku1515%49%71%71%1731%62%80%80%1972%90%95%95%2185%95%98%98%2393%99%100%100%25100%100%100%100%</p>
<p>Table 2 :
2
Shortest Path: Accuracy on Test Data: columns denote MLPs trained with different rules; each row represents the percentage of predictions that satisfy the constraints
PredictionsMLP OnlyMLPMLPMLPsatisfying(p)(p-r-o) (p-r-o-nr)p28.3%96.6% 100%30.1%r88.5%100%100%87.3%nr32.9%36.3% 45.7%70.5%p-r28.3%96.6% 100%30.1%p-r-o-nr23.0%33.2% 45.7%24.2%label (ground truth)22.4%28.9% 40.1%22.7%</p>
<p>Example 6 An example NeurASP program Π digit is as follows, where d 1 and d 2 are terms representing two images.Each image is classified by neural network M digit as one of the values in {0, . . ., 9}.The addition of two digit-images is the sum of their values.
img(d1).img(d2).nn(digit(1, X), [0, 1</p>
<p>Example 6 with input D(d).For d ∈ {d 1 , d 2 } and j ∈ {0, . . ., 9}, the value of p d 1,j is the neural network output M digit (D(d))[1, j].Suppose p i is the probability of atom c = v, i.e., p i denotes P Π(θ)
B Proof of Proposition 1[Recall Proposition 1]
In practice, each atom mi(t) = v is written as m(i, t, v).
We assume that the camera is at the same height as the objects.
The expression {a(R, C, N ) : N = 1..9} = 1 is a shorthand for {a(R, C, 1); . . . ; a(R, C, 9)} = 1 in the language of CLINGO.
The percentage of grid cells having correct digits regardless whether the Sudoku solution is correct.
sp(X, g, true) means edge X is in the shortest path. sp(X, Y ) means there is a path between nodes X and Y in the shortest path.
A path is simple if every node in the path other than the source and the destination has only 1 incoming edge and only 1 outgoing edge.
Other combinations are either meaningless (e.g., o) or having similar results (e.g. p-r is similar to p).
AcknowledgmentsWe are grateful to the anonymous referees for their useful comments.This work was partially supported by the National Science Foundation under Grant IIS-1815337.A Extend NeurASP With Probabilistic RulesMulti-valued probabilistic programs are a fragment of LP MLN programs that distinguishes between probabilistic rules and regular rules.We first present the definition of Multi-Valued Probabilistic Programs (MVPP) from[Lee and Wang, 2016]with some modifications.Then we present the extended NeurASP with probabilistic rules, whose semantics is defined by a translation to MVPP.We assume that the reader is familiar withASP-Core2 [Calimeri et al., 2020].A.1 Multi-Valued Probabilistic ProgramsWe assume a propositional signature σ that is constructed from "constants" and their "values."A constant c is associated with a finite set ıDom(c), called the domain of c.The signature σ is constructed from a finite set of constants, consisting of atoms c = v for every constant c and every element v in ıDom(c).If the domain of c is {FALSE, TRUE} then we say that c is Boolean, and abbreviate c = TRUE as c and c = FALSE as ∼c. 10 We assume that constants are divided into probabilistic constants and non-probabilistic constants.By σ p , we denote the set of atoms in σ that are constructed from the probabilistic constants.Syntax: A probabilistic rule is of the form4) where p i are real numbers in [0, 1] (denoting probabilities) such that i∈{1,...,n} p i = 1, and c is a probabilistic constant in σ, and {v 1 , . . ., v n } = ıDom(c).If ıDom(c) = {FALSE, TRUE}, rule p 1 : c | p 2 :∼c can be abbreviated as p 1 : c .A Multi-Valued Probabilistic Program is the union of Π pr and Π asp , where Π pr consists of probabilistic rules (4), one for each probabilistic constant c in σ p , and Π asp consists of rules of the form ıHead ← ıBody, following the rule format of ASP-Core2 where ıHead contains no probabilistic constants.Example 4 Consider the game of flipping a coin where we win if we got head.Suppose the coin is biased and the probability of getting head is 0.1, then this problem can be represented by the following MVPP program Π coin where head is a probabilistic constant and win is a non-probabilistic constant.0.1: head.win :-head.∼win :-not win.Semantics: Given an MVPP program Π, we obtain an ASP program Π from Π by replacing each rule (4) withwhich means to choose only one atom from the set {c = v 1 , . . ., c = v n }.In addition, Π contains the rulefor each non-probabilistic constant c with ıDom(c) = {v 1 , . . ., v n }.That is, non-probabilistic constants are allowed to have no values.The stable models of Π are defined as the stable models of Π .To define the probability of a stable model, we first define the probability of an atom c = v in σ p .We know there must be exactly one probabilistic rule (4) for each probabilistic constant c.Thus we can always find such a rule (4) for any atom c = v in σ p , and the probability of c = v i , denoted by P Π (c = v i ), is defined as p i in rule (4).The probability of a stable model I of Π, denoted by P Π (I), is defined as the product of the probability of each atom c = v in I ∩ σ p , divided by the number of stable models satisfied by I ∩ σ p .In the following equation, we use I| σp to denote I ∩ σ p , which is indeed the projection of I onto σ p .We also use ıN um(I| σp , Π) to denote the number of stable models of Π that satisfy I| σp .otherwise.An observation is a set of ASP constraints (i.e., rules of the form ⊥ ← ıBody).The probability of an observation O is defined asThe probability of the set O = {O 1 , . . ., O o } of independent observations, where each O i is a set of ASP constraints, is defined as the product of the probability of each O i :Example 4 Continued: The following ASP program is Π coin (the ASP counter-part of Π coin ).Algorithm 2 gradientsSM : compute gradients of the probability of a set of stable models Input:1. Π(θ): a NeurASP program (under signature σ) with parameters θ 2. p: the probabilities of the atoms in σ nn 3. I: a set of stable models of Π(θ), where the summation of their probabilities is to be maximized Output:1. i. numerator = 0 ii.for each I ∈ I:
Automatic differentiation in PyTorch. Adam , Proceedings of Neural Information Processing Systems. Neural Information Processing Systems2017. 2017</p>
<p>Learning to compose neural networks for question answering. Andreas , Proceedings of the 2016 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesCambridge University Press2016. 2016. 2003. 2003. 2011. 2011. 202054Communications of the ACM</p>
<p>Logic tensor networks for semantic image interpretation. Cohen, Proceedings of the 26th International Joint Conference on Artificial Intelligence. Mehran Seyed, David Kazemi, Poole, the 26th International Joint Conference on Artificial IntelligenceKazemi and Poole2018. 2018. 2017. 2017. 2011. 2011. 2018. 20181Proceedings of the 32nd AAAI Conference on Artificial Intelligence</p>
<p>Semi-supervised classification with graph convolutional networks. Welling Kipf, Thomas N Kipf, Max Welling, Joohyung Lee, Yi Wang, Proceedings of International Conference on Principles of Knowledge Representation and Reasoning (KR). International Conference on Principles of Knowledge Representation and Reasoning (KR)2017. 2017. 2017. 2016. 2016. 2017. 2017Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</p>
<p>Kagnet: Knowledge-aware graph networks for commonsense reasoning. Vladimir Lifschitz, ; Lifschitz, Lin, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2008. 2008. 2019. 2019Proceedings of the AAAI Conference on Artificial Intelligence</p>
<p>The neurosymbolic concept learner: interpreting scenes, words, and sentences from natural supervision. Manhaeve, Proceedings of International Conference on Learning Representations. Rasmus Palm, Ulrich Paquet, Ole Winther, International Conference on Learning Representations2018. 2018. 2019. 2019. 2018. 2018Proceedings of Advances in Neural Information Processing Systems</p>
<p>Kyubyong Park. Can convolutional neural networks crack sudoku. Park, 2018. 2018</p>
<p>Tim Rocktäschel and Sebastian Riedel. End-to-end differentiable proving. Raymond Pearl, Reiter, Proceedings of Advances in Neural Information Processing Systems. Advances in Neural Information Processing SystemsCambridge Univ Press2000. 2000. 1980. 1980. 2017. 201729Causality: models, reasoning and inference</p>
<p>Lifted relational neural networks. Šourek, Proceedings of the 2015th International Conference on Cognitive Computation: Integrating Neural and Symbolic Approaches. the 2015th International Conference on Cognitive Computation: Integrating Neural and Symbolic Approaches2015. 20151583</p>
<p>A semantic loss function for deep learning with symbolic knowledge. Xu, Proceedings of the 35th International Conference on Machine Learning (ICML). the 35th International Conference on Machine Learning (ICML)2018. July 2018</p>            </div>
        </div>

    </div>
</body>
</html>
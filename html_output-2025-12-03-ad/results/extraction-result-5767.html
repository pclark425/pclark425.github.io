<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5767 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5767</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5767</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-a8fc2bef3eabe018fef37e7aa35a45df25b54670</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a8fc2bef3eabe018fef37e7aa35a45df25b54670" target="_blank">Scientific hypothesis generation by large language models: laboratory validation in breast cancer treatment</a></p>
                <p><strong>Paper Venue:</strong> Journal of the Royal Society Interface</p>
                <p><strong>Paper TL;DR:</strong> The application of LLMs as a source of scientific hypotheses using the domain of breast cancer treatment is experimentally tested and it is concluded that LLMs are a valuable source of scientific hypotheses.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have transformed artificial intelligence (AI) and achieved breakthrough performance on a wide range of tasks. In science, the most interesting application of LLMs is for hypothesis formation. A feature of LLMs, which results from their probabilistic structure, is that the output text is not necessarily a valid inference from the training text. These are termed ‘hallucinations’, and are harmful in many applications. In science, some hallucinations may be useful: novel hypotheses whose validity may be tested by laboratory experiments. Here, we experimentally test the application of LLMs as a source of scientific hypotheses using the domain of breast cancer treatment. We applied the LLM GPT4 to hypothesize novel synergistic pairs of US Food and Drug Administration (FDA)-approved non-cancer drugs that target the MCF7 breast cancer cell line relative to the non-tumorigenic breast cell line MCF10A. In the first round of laboratory experiments, GPT4 succeeded in discovering three drug combinations (out of 12 tested) with synergy scores above the positive controls. GPT4 then generated new combinations based on its initial results, this generated three more combinations with positive synergy scores (out of four tested). We conclude that LLMs are a valuable source of scientific hypotheses.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5767.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5767.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose large language model (LLM) used in this study to generate testable scientific hypotheses (drug–drug synergy hypotheses) and mechanistic rationales from its pretraining on a very large text corpus; outputs were iteratively refined using experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Described in the paper as a general-purpose LLM previously trained on a large fraction of internet text; the manuscript notes LLMs are trained on very large corpora (tokens >10^4) and very large numbers of parameters (>10^12) and map input token strings to output token strings using deep neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Generate novel, testable scientific hypotheses (qualitative mechanistic rules linking drug mechanisms to selective toxicity/synergy in MCF7 breast cancer cells vs MCF10A controls).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedicine / drug discovery / cancer biology (applied domain), with broader claims about LLMs for scientific hypothesis formation.</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Prompt engineering to request synergistic, selective drug-pair hypotheses with constraints (e.g., at least one non-oncology drug, FDA-approved, affordable); GPT-4 generated hypotheses and mechanistic rationales; iterative closed-loop workflow where experimental results were summarized and fed back into GPT-4 to generate new hypotheses; comparison of outputs with other LLMs (Gemini, PubMedGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Mechanistic hypotheses/rules of the form 'drug A + drug B → selective cytotoxicity/synergy in MCF7 via mechanisms X and Y' (i.e., generalizable mechanistic principles linking drug mechanism interactions to cancer-cell vulnerabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Laboratory validation metrics: HSA (highest single agent) synergy scores computed with SynergyFinder 3.0 across dose–response matrices, 'specificity' defined as HSA_MCF7 − HSA_MCF10A, IC50 values, ANOVA p-values for viability changes, and qualitative literature support checks in supplementary appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 produced novel drug-pair hypotheses; in the first experimental round 3 of 12 GPT-4-suggested combinations exceeded the positive-control HSA score and multiple combinations showed synergistic areas within dose–response matrices. After feeding back initial experimental results, GPT-4 proposed further pairs; of seven second-iteration combinations, six showed synergy within matrices and three newly hypothesized pairs had positive overall HSA scores. Notable validated combinations include itraconazole + atenolol, simvastatin + disulfiram, and dipyridamole + mebendazole (initial round), and mebendazole + quinacrine, disulfiram + fulvestrant, disulfiram + quinacrine (second round).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-in-the-loop: humans engineered prompts, selected constraints, ran laboratory automation and assays, evaluated results, and supplied experimental summaries back to GPT-4; human experts performed literature checks and interpreted mechanistic rationales and failures.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>No bespoke scholarly corpus provided in the study; GPT-4 used as-is (pretrained on large portions of internet text per paper); comparison included a specialized model (PubMedGPT) trained on biomedical literature (mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported limitations include epistemic uncertainty about whether GPT-4 'understood' prompts, factual hallucinations in mechanistic rationales (e.g., incorrect assertion that itraconazole disrupts mammalian ergosterol synthesis), bias in recommendations (all suggested drug pairs were non-cancer drugs despite instruction), occasional 'forgetting' of previous recommendations, variability between LLMs (different models produced different pairs), and the necessity of experimental validation to distinguish useful hallucinations from false claims.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Concrete illustrative outputs and validations: GPT-4 hypotheses such as 'itraconazole + atenolol' (HSA 4.83), 'simvastatin + disulfiram' (HSA 3.29), 'dipyridamole + mebendazole' (HSA 2.49) that showed higher HSA synergy in MCF7 than the positive control; GPT-4 also generated mechanistic rationales (e.g., simvastatin disrupting lipid rafts to enhance disulfiram-induced oxidative stress) and was re-run with primary-screen summaries to propose further pairs (e.g., disulfiram + fulvestrant).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific hypothesis generation by large language models: laboratory validation in breast cancer treatment', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5767.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5767.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubMedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubMedGPT (specialized biomedical LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized LLM trained on biomedical literature (PubMed) that the authors compared against GPT-4 and Gemini for drug-pair suggestion overlap and differences; produced different pairings with overlap in core drug selections.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>PubMedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Described as a specialized LLM trained on biomedical literature (PubMed); used by the authors for output comparison to assess consistency and overlap with GPT-4 and Gemini.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Generate candidate drug combinations and mechanistic rationales for selective toxicity/synergy in breast cancer cell lines, to compare selection distributions across models.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Biomedical literature / pharmacology (comparative suggestion generation).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Same or similar prompt used across models (GPT-4, Gemini, PubMedGPT) and outputs compared for drug and pair selection overlap and variability; frequency and pair-distribution analysis suggested for future work.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Implicit mechanistic suggestions linking drug mechanisms to cancer vulnerabilities, as produced in model outputs (not formalized laws but hypothesis-style rules).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Comparison metrics described qualitatively: overlap in core drug selections, variability in pair suggestions; experimental validation in the lab was performed on GPT-4 outputs, not systematically on PubMedGPT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PubMedGPT produced different drug-pair suggestions from GPT-4 and Gemini but with substantial overlap in core drug choices; the authors note both advantages and drawbacks of model-specific biases and suggest analyzing pair frequency distributions across LLMs in future studies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-in-the-loop for prompt design and analysis of model outputs; laboratory validation was focused on GPT-4 outputs rather than exhaustive cross-model validation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>PubMed corpus (implied by model name and authors' description; the study did not provide a bespoke PubMed dataset but refers to the specialization).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Model-specific biases and non-uniform distributions of suggestions across LLMs; lack of uniformity in pairs despite overlap in core drugs; authors did not perform full laboratory testing of PubMedGPT hypotheses in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Authors report that PubMedGPT recommended different drug pairs while overlapping in core drug selections; no specific PubMedGPT-suggested pairs were experimentally validated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific hypothesis generation by large language models: laboratory validation in breast cancer treatment', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5767.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5767.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for knowledge engineering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using Large Language Models for Knowledge Engineering (LLMKE) / LLMs for formalizing scientific knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references broader uses of LLMs — summarization, data analysis, writing, formalizing knowledge, and hypothesis generation — and cites prior work (e.g., 'Using Large Language Models for Knowledge Engineering') as background for applying LLMs to scientific law/principle distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>various (general LLMs; specialized LLMs like PubMedGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>General claim: LLMs are transformer-based models trained on very large corpora enabling text summarization, formalization of knowledge, and other tasks; specific architectures/sizes vary by model and are not exhaustively detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>(As discussed in background) To summarize texts, formalize knowledge, and potentially distill general principles or qualitative laws from scholarly corpora to accelerate scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Multidisciplinary / scientific literature broadly (mentioned as potential applications across sciences).</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Mentioned methods in the literature include adapted LLMs for summarization, knowledge engineering approaches (citation 5), and closed-loop generative AI for science; the present study uses prompt engineering and closed-loop experimental feedback as an application example.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>General idea: extraction of domain principles or rules (e.g., mechanistic relationships, causal hypotheses, domain-specific heuristics) from textual corpora; the present paper frames its GPT-4 hypotheses as examples of generated mechanistic rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified for the literature references within this paper; the current study uses experimental validation (synergy scores, IC50, p-values) as an evaluation proxy for hypothesis usefulness rather than formal metrics of 'law' extraction (e.g., coverage or formal correctness across corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The paper positions LLMs as promising tools for generating scientific hypotheses and formalizing knowledge, but notes limited prior empirical laboratory validation; this study provides an empirical laboratory validation example for hypothesis generation rather than a comprehensive law-distillation from many scholarly papers.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Background and prior work typically involve human-in-the-loop approaches (fine-tuning, human evaluation, knowledge engineering); the authors recommend human oversight due to hallucinations and factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Prior-cited works (e.g., LLMKE) operate on curated knowledge bases or domain corpora (e.g., Wikidata, PubMed) though the present paper's experiments used GPT-4 pretrained on general web text and did not supply a targeted corpus for law extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Across-cited literature and the present study: hallucinations, factual errors, model-specific biases, lack of grounding, need for experimental/empirical validation, and epistemic uncertainty about model 'understanding' of distilled laws/principles.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>The present paper uses the experimental validation of GPT-4–generated drug synergy hypotheses as an example of how LLM-generated mechanistic rules (hypotheses) can be treated and tested as candidate qualitative principles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific hypothesis generation by large language models: laboratory validation in breast cancer treatment', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata <em>(Rating: 2)</em></li>
                <li>Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery <em>(Rating: 2)</em></li>
                <li>The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence <em>(Rating: 2)</em></li>
                <li>Adapted large language models can outperform medical experts in clinical text summarization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5767",
    "paper_id": "paper-a8fc2bef3eabe018fef37e7aa35a45df25b54670",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A general-purpose large language model (LLM) used in this study to generate testable scientific hypotheses (drug–drug synergy hypotheses) and mechanistic rationales from its pretraining on a very large text corpus; outputs were iteratively refined using experimental results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4",
            "llm_model_description": "Described in the paper as a general-purpose LLM previously trained on a large fraction of internet text; the manuscript notes LLMs are trained on very large corpora (tokens &gt;10^4) and very large numbers of parameters (&gt;10^12) and map input token strings to output token strings using deep neural networks.",
            "task_goal": "Generate novel, testable scientific hypotheses (qualitative mechanistic rules linking drug mechanisms to selective toxicity/synergy in MCF7 breast cancer cells vs MCF10A controls).",
            "domain": "Biomedicine / drug discovery / cancer biology (applied domain), with broader claims about LLMs for scientific hypothesis formation.",
            "methodology": "Prompt engineering to request synergistic, selective drug-pair hypotheses with constraints (e.g., at least one non-oncology drug, FDA-approved, affordable); GPT-4 generated hypotheses and mechanistic rationales; iterative closed-loop workflow where experimental results were summarized and fed back into GPT-4 to generate new hypotheses; comparison of outputs with other LLMs (Gemini, PubMedGPT).",
            "type_of_qualitative_law": "Mechanistic hypotheses/rules of the form 'drug A + drug B → selective cytotoxicity/synergy in MCF7 via mechanisms X and Y' (i.e., generalizable mechanistic principles linking drug mechanism interactions to cancer-cell vulnerabilities).",
            "evaluation_metrics": "Laboratory validation metrics: HSA (highest single agent) synergy scores computed with SynergyFinder 3.0 across dose–response matrices, 'specificity' defined as HSA_MCF7 − HSA_MCF10A, IC50 values, ANOVA p-values for viability changes, and qualitative literature support checks in supplementary appendix.",
            "results_summary": "GPT-4 produced novel drug-pair hypotheses; in the first experimental round 3 of 12 GPT-4-suggested combinations exceeded the positive-control HSA score and multiple combinations showed synergistic areas within dose–response matrices. After feeding back initial experimental results, GPT-4 proposed further pairs; of seven second-iteration combinations, six showed synergy within matrices and three newly hypothesized pairs had positive overall HSA scores. Notable validated combinations include itraconazole + atenolol, simvastatin + disulfiram, and dipyridamole + mebendazole (initial round), and mebendazole + quinacrine, disulfiram + fulvestrant, disulfiram + quinacrine (second round).",
            "human_involvement": "Human-in-the-loop: humans engineered prompts, selected constraints, ran laboratory automation and assays, evaluated results, and supplied experimental summaries back to GPT-4; human experts performed literature checks and interpreted mechanistic rationales and failures.",
            "dataset_or_corpus": "No bespoke scholarly corpus provided in the study; GPT-4 used as-is (pretrained on large portions of internet text per paper); comparison included a specialized model (PubMedGPT) trained on biomedical literature (mentioned).",
            "limitations_or_challenges": "Reported limitations include epistemic uncertainty about whether GPT-4 'understood' prompts, factual hallucinations in mechanistic rationales (e.g., incorrect assertion that itraconazole disrupts mammalian ergosterol synthesis), bias in recommendations (all suggested drug pairs were non-cancer drugs despite instruction), occasional 'forgetting' of previous recommendations, variability between LLMs (different models produced different pairs), and the necessity of experimental validation to distinguish useful hallucinations from false claims.",
            "notable_examples": "Concrete illustrative outputs and validations: GPT-4 hypotheses such as 'itraconazole + atenolol' (HSA 4.83), 'simvastatin + disulfiram' (HSA 3.29), 'dipyridamole + mebendazole' (HSA 2.49) that showed higher HSA synergy in MCF7 than the positive control; GPT-4 also generated mechanistic rationales (e.g., simvastatin disrupting lipid rafts to enhance disulfiram-induced oxidative stress) and was re-run with primary-screen summaries to propose further pairs (e.g., disulfiram + fulvestrant).",
            "uuid": "e5767.0",
            "source_info": {
                "paper_title": "Scientific hypothesis generation by large language models: laboratory validation in breast cancer treatment",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "PubMedGPT",
            "name_full": "PubMedGPT (specialized biomedical LLM)",
            "brief_description": "A specialized LLM trained on biomedical literature (PubMed) that the authors compared against GPT-4 and Gemini for drug-pair suggestion overlap and differences; produced different pairings with overlap in core drug selections.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "PubMedGPT",
            "llm_model_description": "Described as a specialized LLM trained on biomedical literature (PubMed); used by the authors for output comparison to assess consistency and overlap with GPT-4 and Gemini.",
            "task_goal": "Generate candidate drug combinations and mechanistic rationales for selective toxicity/synergy in breast cancer cell lines, to compare selection distributions across models.",
            "domain": "Biomedical literature / pharmacology (comparative suggestion generation).",
            "methodology": "Same or similar prompt used across models (GPT-4, Gemini, PubMedGPT) and outputs compared for drug and pair selection overlap and variability; frequency and pair-distribution analysis suggested for future work.",
            "type_of_qualitative_law": "Implicit mechanistic suggestions linking drug mechanisms to cancer vulnerabilities, as produced in model outputs (not formalized laws but hypothesis-style rules).",
            "evaluation_metrics": "Comparison metrics described qualitatively: overlap in core drug selections, variability in pair suggestions; experimental validation in the lab was performed on GPT-4 outputs, not systematically on PubMedGPT outputs.",
            "results_summary": "PubMedGPT produced different drug-pair suggestions from GPT-4 and Gemini but with substantial overlap in core drug choices; the authors note both advantages and drawbacks of model-specific biases and suggest analyzing pair frequency distributions across LLMs in future studies.",
            "human_involvement": "Human-in-the-loop for prompt design and analysis of model outputs; laboratory validation was focused on GPT-4 outputs rather than exhaustive cross-model validation.",
            "dataset_or_corpus": "PubMed corpus (implied by model name and authors' description; the study did not provide a bespoke PubMed dataset but refers to the specialization).",
            "limitations_or_challenges": "Model-specific biases and non-uniform distributions of suggestions across LLMs; lack of uniformity in pairs despite overlap in core drugs; authors did not perform full laboratory testing of PubMedGPT hypotheses in this work.",
            "notable_examples": "Authors report that PubMedGPT recommended different drug pairs while overlapping in core drug selections; no specific PubMedGPT-suggested pairs were experimentally validated in the paper.",
            "uuid": "e5767.1",
            "source_info": {
                "paper_title": "Scientific hypothesis generation by large language models: laboratory validation in breast cancer treatment",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLMs for knowledge engineering",
            "name_full": "Using Large Language Models for Knowledge Engineering (LLMKE) / LLMs for formalizing scientific knowledge",
            "brief_description": "The paper references broader uses of LLMs — summarization, data analysis, writing, formalizing knowledge, and hypothesis generation — and cites prior work (e.g., 'Using Large Language Models for Knowledge Engineering') as background for applying LLMs to scientific law/principle distillation.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "llm_model_name": "various (general LLMs; specialized LLMs like PubMedGPT)",
            "llm_model_description": "General claim: LLMs are transformer-based models trained on very large corpora enabling text summarization, formalization of knowledge, and other tasks; specific architectures/sizes vary by model and are not exhaustively detailed in this paper.",
            "task_goal": "(As discussed in background) To summarize texts, formalize knowledge, and potentially distill general principles or qualitative laws from scholarly corpora to accelerate scientific discovery.",
            "domain": "Multidisciplinary / scientific literature broadly (mentioned as potential applications across sciences).",
            "methodology": "Mentioned methods in the literature include adapted LLMs for summarization, knowledge engineering approaches (citation 5), and closed-loop generative AI for science; the present study uses prompt engineering and closed-loop experimental feedback as an application example.",
            "type_of_qualitative_law": "General idea: extraction of domain principles or rules (e.g., mechanistic relationships, causal hypotheses, domain-specific heuristics) from textual corpora; the present paper frames its GPT-4 hypotheses as examples of generated mechanistic rules.",
            "evaluation_metrics": "Not specified for the literature references within this paper; the current study uses experimental validation (synergy scores, IC50, p-values) as an evaluation proxy for hypothesis usefulness rather than formal metrics of 'law' extraction (e.g., coverage or formal correctness across corpora).",
            "results_summary": "The paper positions LLMs as promising tools for generating scientific hypotheses and formalizing knowledge, but notes limited prior empirical laboratory validation; this study provides an empirical laboratory validation example for hypothesis generation rather than a comprehensive law-distillation from many scholarly papers.",
            "human_involvement": "Background and prior work typically involve human-in-the-loop approaches (fine-tuning, human evaluation, knowledge engineering); the authors recommend human oversight due to hallucinations and factual errors.",
            "dataset_or_corpus": "Prior-cited works (e.g., LLMKE) operate on curated knowledge bases or domain corpora (e.g., Wikidata, PubMed) though the present paper's experiments used GPT-4 pretrained on general web text and did not supply a targeted corpus for law extraction.",
            "limitations_or_challenges": "Across-cited literature and the present study: hallucinations, factual errors, model-specific biases, lack of grounding, need for experimental/empirical validation, and epistemic uncertainty about model 'understanding' of distilled laws/principles.",
            "notable_examples": "The present paper uses the experimental validation of GPT-4–generated drug synergy hypotheses as an example of how LLM-generated mechanistic rules (hypotheses) can be treated and tested as candidate qualitative principles.",
            "uuid": "e5767.2",
            "source_info": {
                "paper_title": "Scientific hypothesis generation by large language models: laboratory validation in breast cancer treatment",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata",
            "rating": 2,
            "sanitized_title": "using_large_language_models_for_knowledge_engineering_llmke_a_case_study_on_wikidata"
        },
        {
            "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
            "rating": 2,
            "sanitized_title": "advancing_the_scientific_method_with_large_language_models_from_hypothesis_to_discovery"
        },
        {
            "paper_title": "The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence",
            "rating": 2,
            "sanitized_title": "the_future_of_fundamental_science_led_by_generative_closedloop_artificial_intelligence"
        },
        {
            "paper_title": "Adapted large language models can outperform medical experts in clinical text summarization",
            "rating": 1,
            "sanitized_title": "adapted_large_language_models_can_outperform_medical_experts_in_clinical_text_summarization"
        }
    ],
    "cost": 0.01154375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Scientific Hypothesis Generation by Large Language Models: Laboratory Validation in Breast Cancer Treatment</h1>
<p>Abbi Abdel-Rehim ${ }^{1}$, Hector Zenil ${ }^{2,3}$, Oghenejokpeme Orhobor ${ }^{4}$, Marie Fisher ${ }^{5}$, Ross J. Collins ${ }^{5}$, Elizabeth Bourne ${ }^{5}$, Gareth W. Fearnley ${ }^{5}$, Emma Tate ${ }^{5}$, Holly X. Smith ${ }^{5}$, Larisa N. Soldatova ${ }^{6}$, Ross D. King<em>. ${ }^{1,7}$<br>${ }^{1}$ Department of Chemical Engineering and Biotechnology, University of Cambridge, CB3 0AS, U.K.<br>${ }^{2}$ Research Departments of Biomedical Computing and Digital Twins, School of Biomedical Engineering and Imaging Sciences, King's Institute for AI, King's College London, SE1 7EU, U.K.<br>${ }^{3}$ The Alan Turing Institute, British Library, London, NW1 2DB, U.K.<br>${ }^{4}$ The National Institute of Agricultural Botany, Cambridge, CB3 0LE, U.K.<br>${ }^{5}$ Arctoris Ltd, Oxford, OX14 4SA, UK.<br>${ }^{6}$ Department of Computing, Goldsmiths, University of London, SE14 6NW, U.K.<br>${ }^{7}$ Department of Computer Science and Engineering, Chalmers University, S-412 96 Göteborg, Sweden.<br></em>Ross King. Email: rk663@cam.ac.uk</p>
<h2>ABSTRACT</h2>
<p>Large language models (LLMs) have transformed AI and achieved breakthrough performance on a wide range of tasks. In science the most interesting application of LLMs is for hypothesis formation. A feature of LLMs, which results from their probabilistic structure, is that the output text is not necessarily a valid inference from the training text. These are termed "hallucinations" and are harmful in many applications. In science some hallucinations may be useful: novel hypotheses whose validity may be tested by laboratory experiments. Here, we experimentally test the application of LLMs as a source of scientific hypotheses using the domain of breast cancer treatment. We applied the LLM GPT4 to hypothesize novel synergistic pairs of FDA-approved non-cancer drugs that target the MCF7 breast cancer cell line relative to the non-tumorigenic breast cell line MCF10A. In the first round of laboratory experiments GPT4 succeeded in discovering three drug combinations (out of twelve tested) with synergy scores above the positive controls. GPT4 then generated new combinations based on its initial results, this generated three more combinations with positive synergy scores (out of four tested). We conclude that LLMs are a valuable source of scientific hypotheses.</p>
<h2>Keywords</h2>
<p>Machine learning, precision healthcare, predictive medicine, AI for science, drug discovery, closedloop AI-driven science</p>
<h1>INTRODUCTION</h1>
<p>The world has been stunned by the success of Large Language Models (LLMs). They have achieved breakthrough performance on a wide range of conversation-based tasks that previously required human intelligence. The overall architecture of LLMs is remarkably simple: they map input token strings to output token strings using deep neural networks (DNNs). Their power comes from being trained on very large general corpuses (substantial percentages of the whole text-based internet), and the use of very large numbers of both tokens ( $&gt;10^{\wedge} 4$ ) and parameters ( $&gt;10^{\wedge} 12$ ). The success of LLMs is surprising given that they don't use any explicit model of the world, nor explicit internal symbols, nor do they have any physical grounding in the world. All of these were assumed by most AI scientists to be essential for such intelligent behaviour.</p>
<p>LLMs can be applied to many aspects of science: to summarize texts (1, 2), to analyze data (3), to write papers and code (4), to formalize knowledge (5), to answer questions (6), etc. However, the most exciting application for LLMs in science is for generating novel hypotheses. Despite the clear potential of LLMs for hypothesis generation their utility for hypothesis generation has been little investigated.</p>
<p>The architecture of LLMs entails that the output string is the most likely one given the input string and the training data. For science these strings may be interpreted as scientific hypotheses. Due to their internal complexity and sophistication LLMs have the potential to go beyond existing text-based scientific hypothesis generation tools $(7,8,9)$. The generation of hypotheses by LLMs is closely related to the phenomena of "hallucinations". These are LLM outputs that are not valid inferences from the training data. Some hallucinations may be simply factually wrong. For others, their validity is uncertain. Hallucinations are a serious problem in many applications (9). For example, in science it is not acceptable to hallucinate (make up) false references. However, in scientific hypothesis generation hallucinations may be useful: probable novel hypotheses whose validity may be objectively tested by laboratory experiments (10).</p>
<p>Here we use laboratory experiments to test the utility of the general purpose LLM GPT4 for scientific hypothesis generation (Fig. 1). We employed breast cancer as the test domain. In our experiments breast cancer cells were exemplified by MCF7 (an epithelial breast cancer cell line); non-tumorigenic breast cells were exemplified by the epithelial cell line MCF10A. We provided GPT4 with a prompt that had several aims: 1) Identify novel drug combinations that would have a significant impact on MCF7 cell lines; 2) Avoid harming MCF10A the control cell line; and 3) Design combinations that were possibly synergistic. We also had additional requirements related to the drugs themselves: at least one of the drugs in every pair should not be an anti-neoplastic drug, and that the drugs should be affordable, accessible, and preferably FDA-approved. Prompts and the list of complete hypotheses are found in supplementary materials (Fig. S1-S3, Table S1). Interestingly, all the drug combinations hypothesized were exclusively non-cancer drugs (suggesting a possible limitation in GPT4 understanding of its instructions). The combinations were however novel, and we could not find any of them reported for breast cancer in the literature. We did find that several of the drugs had been investigated in the cancer literature, and that several of the drugs had been tested against MCF7 (Supplementary materials Appendix).</p>
<p>In addition to hypothesizing drug combination, we prompted GPT4 to provide two positive controls that are commonly used against breast cancer in clinic and likely have an impact on MCF7; as well as two negative control combinations that would be unlikely to cause harm to MCF7 (Fig. S2, S3). It may have been wiser to select the controls ourselves. But we judged that GPT4 did a fair job in its selections (Table 1b).</p>
<h1>RESULTS</h1>
<p>Using the method described below we screened the twelve pairs of compounds proposed by GPT4 (Table 1a). We investigated two properties of the pairs: 1) The specificity of the combination for MCF7 v MCF10A, and 2) The additivity/synergy of the combination. (Additivity occurs when the combination of the effects of two drugs is not less than either of the two drugs acting independently. Synergy describes the situation when the effect of the combination is greater than that of the most effective drug (highest single agent)). To determine drug additivities/synergies we employed SynergyFinder 3.0 to calculate HSA (highest single agent) synergy scores for all combinations (Table 2). There were six additive interactions combinations with positive synergy scores for MCF7: itraconazole + atenolol, simvastatin + disulfiram, dipyridamole + mebendazole, furosemide + mebendazole, disulfiram + hydroxychloroquine, and the positive control doxorubicin + cyclophosphamide. The initial three hypothesised combinations resulted in HSA scores surpassing those of the positive controls. Synergistic areas were found within the drug response matrices belonging to ten out of twelve of the hypothesized drug combinations (Table S2). We found that eight out of the twelve hypothesised combinations resulted in a higher HSA score in varying degrees for MCF7 compared to MCF10A (Table 2, cf. Table 2 and Table S3). In Table S12 we summarize the literature on the hypotheses proposed by GPT4 and the anti-cancer properties of the drugs selected. We found underlying support in literature for three out of the latter screened six combinations with positive synergy scores, while the remaining three remain unclear.</p>
<p>To better understand the utility of the paired compounds we tested the individual drugs (Table 3). From the drugs in the positive controls pairs only doxorubicin was found to result in an IC50 value below the maximum dose of 25 uM in both cell lines. For MCF7 there were five additional drugs that resulted in IC50 values below the same threshold, with Disulfiram and Niclosamide showing comparatively high toxicity (Table 3). Several more drugs were toxic to the cell lines, but failed to reduce the viability to such an extent where an IC50 value could be derived (Table S4 and S5). In total, twelve out of the eighteen non-control drugs showed toxicity towards MCF7: celecoxib, chloroquine, dipyridamole, disulfiram, hydroxychloroquine, itraconalzole, mebendazole, niclosamide, quinacrine, sildenafil and simvastatin. Out of these drugs, dipyridamole, disulfiram, mebendazole and quinacrine showed high specificity towards MCF7 (cf. Table S4 and S5). While many of these drugs had been studied in cancer cell lines, they are not cancer drugs. Fulvestrant, a positive control cancer drug also showed preference for MCF7.</p>
<p>Eleven out of eighteen compounds reduced viability of the control cell line MCF10A. When excluding the highest concentration of 25 uM , these numbers change to $6 / 18$ and $8 / 18$ compounds for MCF7 and MCF10A respectively. The ten drugs that showed highest toxicity towards MCF7 were re-screened to achieve sufficient replicates $(\mathrm{n}=3)$ in order to validate their toxicity.</p>
<p>In an additional experiment, twelve drugs were retested from the first round ( $\mathrm{n}&gt;=3$ replicates). An ANOVA two-way test with three replicates was used to calculate significance of changes to viability compared to the internal control drug Allopurinol for both cell lines (S6, S7). Two of the retested drugs were initially used as positive controls for MCF7 (doxorubicin and fulvestrant). Out of twelve retested drugs, dipyridamole, disulfiram, niclosamide and quinacrine significantly reduced the viability of MCF7 when considering concentrations up to $3.84 \mu \mathrm{M}$. The two positive control drugs doxorubicin and fulvestrant also targeted MCF7. When considering all concentrations (including $25 \mu \mathrm{M}$ which is quite high), all but hydroxychloroquine results in a significant impact on MCF7. Despite this, the toxicity of hydroxychloroquine at 25 uM is persistent and substantial. IC50 values could only be calculated for</p>
<p>5/11 compounds (Table S8), disulfiram and doxorubicin showing sub micromolar IC50 values of 0.059 and $0.3 \mu \mathrm{M}$ respectively. This was followed by niclosamide at $1.22 \mu \mathrm{M}$, quinacrine at $4.71 \mu \mathrm{M}$ and chloroquine at $10.62 \mu \mathrm{M}$. However, the remaining compounds were still toxic to MCF7 (Table S9). Calculated IC50, and viability values for MCF10A can be found in Tables S8 and S10.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Drug1</th>
<th style="text-align: center;">Drug2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Doxorubicin</td>
<td style="text-align: center;">Cyclophosphamide</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Positive Control. <br> Combination FDA approved to treat Breast Cancer <br> Doxorubicin is an anthracycline that intercalates into DNA and inhibits topoisomerase II, causing DNA damage. Cyclophosphamide is an alkylating agent that causes DNA damage. <br> "The combination targets DNA integrity through multiple mechanisms, which may be effective in MCF7 cells with high proliferative capacity."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Fulvestrant</td>
<td style="text-align: center;">Palbociclib</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Positive Control. <br> Individually FDA approved to treat Breast Cancer <br> Fulvestrant is a selective estrogen receptor degrader (SERD) that blocks and degrades estrogen receptors. Palbociclib is a CDK4/6 inhibitor that blocks cell cycle progression. <br> "The combination targets both estrogen signaling and cell cycle progression, which may be effective in estrogen receptor-positive MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Allopurinol <br> (Xanthine Oxidase Inhibitor)</td>
<td style="text-align: center;">Omeprazole <br> (Proton Pump Inhibitor)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Negative Control <br> "Allopurinol is used to treat gout and hyperuricemia, and omeprazole is used to reduce stomach acid. Neither drug targets pathways relevant to MCF7 breast cancer cell growth or survival, and they are not expected to have an effect on MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Diphenhydramine (Antihistamine)</td>
<td style="text-align: center;">Omeprazole (Proton Pump Inhibitor)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Negative Control <br> "Diphenhydramine and cetirizine are antihistamines used to treat allergy symptoms. Neither drug targets pathways relevant to MCF7 breast cancer cell growth or survival, and they are not expected to have an effect on MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1b. GPT4 generated positive and negative control combinations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Drug 1</th>
<th style="text-align: center;">Drug 2</th>
<th style="text-align: center;">HSA score (MCF7)</th>
<th style="text-align: center;">Specificity (MCF7)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Itraconazole</td>
<td style="text-align: center;">Atenolol</td>
<td style="text-align: center;">4.83</td>
<td style="text-align: center;">7.03</td>
</tr>
<tr>
<td style="text-align: center;">Simvastatin</td>
<td style="text-align: center;">Disulfiram</td>
<td style="text-align: center;">3.29</td>
<td style="text-align: center;">1.85</td>
</tr>
<tr>
<td style="text-align: center;">Dipyridamole</td>
<td style="text-align: center;">Mebendazole</td>
<td style="text-align: center;">2.49</td>
<td style="text-align: center;">3.69</td>
</tr>
<tr>
<td style="text-align: center;">Doxorubicin*</td>
<td style="text-align: center;">Cyclophoshamide*</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">3.27</td>
</tr>
<tr>
<td style="text-align: center;">Furosemide</td>
<td style="text-align: center;">Mebendazole</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">6.14</td>
</tr>
<tr>
<td style="text-align: center;">Disulfiram</td>
<td style="text-align: center;">Hydroxychloroquine</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">3.51</td>
</tr>
<tr>
<td style="text-align: center;">Acarbose</td>
<td style="text-align: center;">Itraconazole</td>
<td style="text-align: center;">$-1.36$</td>
<td style="text-align: center;">$-1.33$</td>
</tr>
<tr>
<td style="text-align: center;">Disulfiram</td>
<td style="text-align: center;">Sildenafil</td>
<td style="text-align: center;">$-1.63$</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: center;">Allopurinol</td>
<td style="text-align: center;">Chloroquine</td>
<td style="text-align: center;">$-1.87$</td>
<td style="text-align: center;">2.24</td>
</tr>
<tr>
<td style="text-align: center;">Celecoxib</td>
<td style="text-align: center;">Quinacrine</td>
<td style="text-align: center;">$-2.21$</td>
<td style="text-align: center;">$-3.27$</td>
</tr>
<tr>
<td style="text-align: center;">Fulvestrant*</td>
<td style="text-align: center;">Palbociclib*</td>
<td style="text-align: center;">$-2.59$</td>
<td style="text-align: center;">$-0.49$</td>
</tr>
<tr>
<td style="text-align: center;">Memantine</td>
<td style="text-align: center;">Niclosamide</td>
<td style="text-align: center;">$-2.61$</td>
<td style="text-align: center;">$-2.23$</td>
</tr>
<tr>
<td style="text-align: center;">Disulfiram</td>
<td style="text-align: center;">Cimetidine</td>
<td style="text-align: center;">$-3.06$</td>
<td style="text-align: center;">$-8.17$</td>
</tr>
<tr>
<td style="text-align: center;">Allopurinol**</td>
<td style="text-align: center;">Omeprazole**</td>
<td style="text-align: center;">$-3.85$</td>
<td style="text-align: center;">$-6.2$</td>
</tr>
<tr>
<td style="text-align: center;">Atrovastatin</td>
<td style="text-align: center;">Metronidazole</td>
<td style="text-align: center;">$-4.84$</td>
<td style="text-align: center;">$-6.3$</td>
</tr>
<tr>
<td style="text-align: center;">Diphenhydramine**</td>
<td style="text-align: center;">Cetirizine**</td>
<td style="text-align: center;">$-9.25$</td>
<td style="text-align: center;">$-6.28$</td>
</tr>
</tbody>
</table>
<p>Table 2. HSA synergy score for each combination. "Specificity" indicates synergy score differences between the cell lines (HSA $<em _MCF10A="{MCF10A" _text="\text">{\text {MCF7 }}-$ HSA $</em>$ ). }<em>Positive controls, </em>*negative controls. Combinations selected for further validation are marked in bold. The combinations in blue have positive synergy scores .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Drug</th>
<th style="text-align: center;">MCF7 IC50 <br> (uM)</th>
<th style="text-align: center;">MCF7 p-val</th>
<th style="text-align: center;">MCF10A IC50 <br> (uM)</th>
<th style="text-align: center;">MCF10A p-val</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Allopurinol**</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">---------</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">---------</td>
</tr>
<tr>
<td style="text-align: center;">Atenolol</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 3}$</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.118</td>
</tr>
<tr>
<td style="text-align: center;">Celecoxib</td>
<td style="text-align: center;">5.325</td>
<td style="text-align: center;">$\mathbf{0 . 0 4 6}$</td>
<td style="text-align: center;">22.573</td>
<td style="text-align: center;">0.185</td>
</tr>
<tr>
<td style="text-align: center;">Disulfiram</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 8}$</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.095</td>
</tr>
<tr>
<td style="text-align: center;">Fulvestrant*</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 0}$</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.430</td>
</tr>
<tr>
<td style="text-align: center;">Itraconazole</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 1}$</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.077</td>
</tr>
<tr>
<td style="text-align: center;">Sildenafil</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 1}$</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.212</td>
</tr>
<tr>
<td style="text-align: center;">Cimetidine</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 2}$</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 3}$</td>
</tr>
<tr>
<td style="text-align: center;">Mebendazole</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 5}$</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 8}$</td>
</tr>
<tr>
<td style="text-align: center;">Metronidazole</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 3 9}$</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 3 1}$</td>
</tr>
<tr>
<td style="text-align: center;">Atorvastatin</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">3.795</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 9}$</td>
</tr>
<tr>
<td style="text-align: center;">Chloroquine</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.202</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 3 0}$</td>
</tr>
<tr>
<td style="text-align: center;">Doxorubicin*</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">$\mathbf{0 . 0 3 4}$</td>
</tr>
<tr>
<td style="text-align: center;">Memantine</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 2}$</td>
</tr>
<tr>
<td style="text-align: center;">Niclosamide</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">$\mathbf{0 . 0 2 1}$</td>
</tr>
<tr>
<td style="text-align: center;">Acarbose</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 9}$</td>
</tr>
<tr>
<td style="text-align: center;">Cetirizine**</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.257</td>
</tr>
<tr>
<td style="text-align: center;">Cyclophosphamide*</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.499</td>
</tr>
<tr>
<td style="text-align: center;">Diphenhydramine**</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.500</td>
</tr>
<tr>
<td style="text-align: center;">Dipyridamole</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.056</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.093</td>
</tr>
<tr>
<td style="text-align: center;">Furosemide</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.188</td>
</tr>
<tr>
<td style="text-align: center;">Hydroxychloroquine</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.118</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.944</td>
</tr>
<tr>
<td style="text-align: center;">Omeprazole**</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.082</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.245</td>
</tr>
<tr>
<td style="text-align: center;">Palbociclib*</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">$&gt;25$</td>
<td style="text-align: center;">0.650</td>
</tr>
<tr>
<td style="text-align: center;">Quinacrine</td>
<td style="text-align: center;">3.848</td>
<td style="text-align: center;">0.082</td>
<td style="text-align: center;">10.183</td>
<td style="text-align: center;">0.116</td>
</tr>
<tr>
<td style="text-align: center;">Simvastatin</td>
<td style="text-align: center;">5.634</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">7.17</td>
<td style="text-align: center;">0.120</td>
</tr>
</tbody>
</table>
<p>Table 3. Single drug treatments. For either cell line, the IC50 values were calculated and declared ( $n=1$ ). Significance ( $p$-value) of changes evoked by drug treatments when compared to positive control (Allopurinol) are also reported. ANOVA two-way significance is used for the samples, most of which are single replicates. P -values are declared for both cell lines. *=Positive controls, ** = negative controls. Numbers in bold significant $&lt;0.05$. Blue significant against MCF7. Red significant against MCF10A. Green significant against MCF7 and MCF10A</p>
<p>After the results from the first round of experiments were complete we investigated whether GPT4 could improve its hypotheses through use on the results from its initial hypotheses. We provided GPT4 with a summary of the results from the primary screen (Fig. S4), and prompted GPT4 to consider combinations containing drugs from the positive controls as well. GPT4 hypothesized four combinations based on this information: disulfiram + fulvestrant, disulfiram + mebendazole, mebendazole + quinacrine, and disulfiram + quinacrine (Table 4). In addition we re-tested three combinations that resulted in positive synergy scores from the primary screening achieving more</p>
<p>robust results, these combinations were disulfiram + simvastatin, disulfiram + hydroxychloroquine, and dipyridamole + mebendazole. Out of the seven combinations screened in the second iteration, six combinations showed varying degrees of synergy within the response matrices (Table 5). Of the newly hypothesized pairs we found three pairs with positive synergy scores: mebendazole + quinacrine, disulfiram + fulvestrant, and disulfiram + quinacrine. The remaining three re-tested combinations also showed consistent positive scores. The three combinations with the highest HSA scores also showed specificity (&gt;1 HSA score) towards MCF7. It is worth mentioning that the most synergistic $3 \times 3$ dose response window resulted in one of the combinations (disulfiram + simvastatin) having a synergy score $&gt;10$. Plots showing HSA synergy graphs for MCF7 and MCF10A derived from Synergyfinder 3.0 can be found in Supplementary Materials Appendix C1 and C2. HSA scores for MCF10A can be found in Table S11. When comparing the most synergistic dose-response windows across both cell lines, there were three combinations that showed substantially higher synergy against MCF7: Quinacrine + mebendazole ( $\Delta \mathrm{HSA}=2.73$ ), mebendazole+dipyridamole ( $\Delta \mathrm{HSA}=3.99$ ) and simvastatin+disulfiram (4.01) (Appendix D). In addition, the two latter combinations also showed areas of selective toxicity towards the MCF7 cell line.</p>
<table>
<thead>
<tr>
<th>Drug 1</th>
<th>Drug 2</th>
<th>HSA score</th>
<th>HSA score (max)</th>
<th>Specificity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Disulfiram</td>
<td>Simvastatin</td>
<td>4.75</td>
<td>10.58</td>
<td>2.41</td>
</tr>
<tr>
<td>Disulfiram</td>
<td>Fulvestrant</td>
<td>1.81</td>
<td>4.60</td>
<td>0.03</td>
</tr>
<tr>
<td>Disulfiram</td>
<td>Quinacrine</td>
<td>1.53</td>
<td>4.47</td>
<td>0.6</td>
</tr>
<tr>
<td>Dipyridamole</td>
<td>Mebendazole</td>
<td>1.10</td>
<td>5.26</td>
<td>3.60</td>
</tr>
<tr>
<td>Disulfiram</td>
<td>Hydroxychloroquine</td>
<td>1.08</td>
<td>3.06</td>
<td>-1.86</td>
</tr>
<tr>
<td>Mebendazole</td>
<td>Quinacrine</td>
<td>0.56</td>
<td>5.54</td>
<td>1.09</td>
</tr>
<tr>
<td>Disulfiram</td>
<td>Mebendazole</td>
<td>-2.49</td>
<td>0.13</td>
<td>-1.83</td>
</tr>
</tbody>
</table>
<p>Table 5. Calculated HSA synergy scores for each combination. Two HSA scores are provided, the first considers the entire dose-response matrix, while the second (max) is based on the most synergistic 3-by-3 dose-window in the dose-response matrix. Specificity denotes the difference in the overall HSA synergy score between the two cell lines ( $\mathrm{HSA}<em _MCF10A="{MCF10A" _text="\text">{\text {MCF7 }}-\mathrm{HSA}</em>$ ) where positive values indicate higher synergy scores for MCF7.}</p>
<p>A final query was made to GPT4 requesting future experiments based on the final results. Three drug combinations were recommended: disulfiram + itraconazole, mebendazole + cimetidine, and quinacrine + celecoxib. Hypotheses for these combinations are reported in Table S13. Disulfiram + itraconazole were hypothesized to synergise based on increased oxidative stress and the inhibition of the hedgehog pathway. Mebendazole and cimetidine were also hypothesized to synergise due to their targets being involved in cell cycle progression and growth. The final combination quinacrine + celecoxib had been tested in the initial experiment, suggesting that GPT4 had already "forgotten" its previous recommendations.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. The overall structure of our experiments. GPT4 was previously trained on data on a large fraction of the text on the internet. We engineered prompts to request from GPT4 synergistic pairs of drugs that are toxic to the breast cancer cell line MCF7, but not to the non-cancer breast cell line MCF10a. These are the hypotheses that we experimentally tested using laboratory automation.</p>
<h1>DISCUSSION</h1>
<p>It is unclear to what extent GPT4 "understood" its prompt for hypothesis formation. This epistemological uncertainty is show in the relationship between the explanation of why a pair of drugs would target MCF7 rather than MCF10A (Table S1), and the explanation why MCF10A would not be targeted, where the MCF10A hypotheses are simply negations of the MCF7 ones. More convincing explanations for not targeting MCF10A would have provided us with more confidence in GPT4's understanding, and the utility of its hypotheses. The hypotheses also exhibited "hallucinations" in their explanations. This is most clearly illustrated by GPT4's hypothesis that itraconazole would "disrupt(ing) cell membrane integrity". This explanation presumably originated from the fact that itraconazole inhibits ergosterol synthesis, which disrupts cell membrane integrity. The factual error is that ergosterol synthesis is not present in mammalian cells. We asked GPT4 "is ergosterol synthesis present in mammalian cells". It replied "No, ergosterol synthesis is not present in mammalian cells. Ergosterol is a sterol found in the cell membranes of fungi and some protozoa, playing a role similar to cholesterol in mammalian cells..."</p>
<p>This study focused on GPT-4, but there are several other LLMs available. We compared the outputs from GPT-4, Gemini, and the specialised LLM PubMedGPT. The results revealed both similarities and notable differences in the selected drugs and their subsequent combinations (Appendix D). It was evident that LLMs generated non-uniform distributions in their drug suggestions, with certain drugs being consistently selected across models, while pairs exhibit greater variability. Despite this diversity, the suggestions remain consistent in their underlying choices. The specialised LLM PubMedGPT recommended different drug pairs but with a strong overlap in core drug selections-an aspect that has both advantages and drawbacks. In future studies, it may be useful to analyse pair frequency distributions per LLM.</p>
<p>We selected cancer treatment as our test domain because every cancer patient ideally requires a scientific research project to understand how best to treat them. In the past this was prohibitively expensive and out of reach for normal cancer patients. The cost of science currently has two main components: the cost of the human scientist's intelligence, and the laboratory costs. Now, thanks to the Al revolution, the cost of (machine) scientific intelligence is dropping. Economies of scale could also drive down the cost of the robotics required to automate personalized cancer research. We therefore envision a future where scientific research is cheap enough that every cancer patient can afford treatment based on a personalized research project.</p>
<p>Our empirical results demonstrate that the GPT4 succeeded in its primary task of forming novel and useful hypothesis. We therefore conclude that LLMs are an important source of novel scientific hypotheses for both human scientists and to the increasingly sophisticated Al systems designed to automate science (11).</p>
<h1>References</h1>
<ol>
<li>Van Veen, D., Van Uden, C., Blankemeier, L., Delbrouk, J-B., Aali, A. et al. Adapted large language models can outperform medical experts in clinical text summarization. Nat Med 30, 1134-1142 (2024).</li>
<li>Liu, P.J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L. \&amp; Shazeer, N. Generating Wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198 (2018).</li>
<li>Devlin J., Chang, M-W., Lee, K. \&amp; Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv Computation and Language. arXiv:1810.04805 [cs.CL] (2018).</li>
<li>Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. et al. Evaluating large language models trained on code, arXiv:2107.03374 [cs.LG] (2021).</li>
<li>Zhang, B., Reklos, I., Jain, N., Peñuela, A.M. \&amp; Simperl, E., Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata. arXiv preprint arXiv:2309.08491 (2023).</li>
<li>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D. \&amp; Sutskever, I., et al. Language models are unsupervised multitask learners. Technical Report, OpenAI blog, 1 (8): 9 (2019).</li>
<li>Sourati, J. \&amp; Evans, J.A. Accelerating science with human-aware artificial intelligence. Nat Hum Behav 7, 1682-1696 (2023).</li>
<li>Zenil, H., Tegner, J., Abrahao., F.S., Lavin, A., Kumar, V. et al. The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence, arXiv:2307.07522v3 [cs.AI] (2023).</li>
<li>Y. Zhang, i.a. M. Levin, J. Frey, J. Evans, A. Bundy and H. Zenil, Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery, npj Artificial Intelligence (in press).</li>
<li>Royal Society. Science in the age of AI: How artificial intelligence is changing the nature and method of scientific research. 978-1-78252-712-1 (2024).</li>
<li>Popper, K. The Logic of Scientific Discovery (Hutchinson, London, 1972).</li>
<li>King, R. D., Rowland, J., Oliver, S. G., Young, M., Aubrey, W. et. al. The automation of science. Science (New York, N.Y.), 324(5923), 85-89 (2009).</li>
</ol>
<h1>Acknowledgements</h1>
<p>Funding: This work has been supported by the UK Engineering and Physical Sciences Research Council (EPSRC) [EP/R022925/2, EP/W004801/1 and EP/X032418/1], and by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Alice Wallenberg Foundation.</p>
<h2>Data availability</h2>
<p>All experimental data are available in the main text or within the supplementary information.</p>
<h1>Appendix</h1>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Drug1</th>
<th style="text-align: center;">Drug2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Disulfiram (Alcoholism)</td>
<td style="text-align: center;">Simvastatin (Hypercholesterolemia)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Disruption of lipid rafts by simvastatin may enhance disulfiram-induced oxidative stress, leading to apoptosis in MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Celecoxib (Pain/Inflammation)</td>
<td style="text-align: center;">Quinacrine (Malaria/Anti-inflammatory)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Reduced inflammatory signaling by celecoxib may enhance quinacrine-induced impairment of autophagy, leading to apoptosis in MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Acarbose (Diabetes)</td>
<td style="text-align: center;">Itraconazole (Fungal Infections)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Acarbose-induced glucose deprivation may enhance the effect of itraconazole on disrupting cell membrane integrity, leading to apoptosis in MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Dipyridamole (Blood Thinner)</td>
<td style="text-align: center;">Mebendazole (Parasitic Infections)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Dipyridamole-induced increase in cAMP levels may enhance the effect of mebendazole on cell cycle arrest, leading to apoptosis in MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Atorvastatin <br> (Hypercholesterolemia)</td>
<td style="text-align: center;">Metronidazole (Bacterial Infections)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Atorvastatin-induced disruption of lipid rafts may enhance the effect of metronidazole-induced DNA damage, leading to apoptosis in MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Allopurinol (Gout)</td>
<td style="text-align: center;">Chloroquine (Malaria)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Allopurinol-induced reduction of uric acid may enhance the effect of chloroquine-induced impairment of autophagy, leading to apoptosis in MCF7."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Cimetidine (Gastric Acid <br> Reducer)</td>
<td style="text-align: center;">Disulfiram (Alcoholism)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Cimetidine-induced reduction of gastric acid may enhance the effect of disulfiram-induced oxidative stress and DNA damage in MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Memantine (Alzheimer's)</td>
<td style="text-align: center;">Niclosamide (Parasitic Infections)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Memantine-induced reduction of glutamate excitotoxicity may enhance the effect of niclosamide-induced energy depletion, leading to cell death in MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">Furosemide (Diuretic)</td>
<td style="text-align: center;">Mebendazole (Parasitic Infections)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Furosemide-induced reduction of sodium reabsorption may enhance the effect of mebendazole on cell cycle arrest, leading to apoptosis in MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Sildenafil (Erectile Dysfunction)</td>
<td style="text-align: center;">Disulfiram (Alcoholism)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Sildenafil-induced increase in cGMP levels may enhance the effect of disulfiraminduced oxidative stress and DNA damage in MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">Itraconazole (Fungal Infections)</td>
<td style="text-align: center;">Atenolol (Hypertension)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Atenolol-induced reduction of stress signaling may enhance the effect of itraconazole on disrupting cell membrane integrity, leading to apoptosis in MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Hydroxychloroquine (Malaria, Rheumatoid Arthritis)</td>
<td style="text-align: center;">Disulfiram (Alcoholism)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Hydroxychloroquine-induced impairment of autophagy may enhance the effect of disulfiram-induced oxidative stress and DNA damage in MCF7 cells."</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1a. GPT4 generated drug combination hypotheses.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Drug1</th>
<th style="text-align: center;">Drug2</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Disulfiram <br> (Alcoholism)</td>
<td style="text-align: center;">Fulvestrant <br> (Breast cancer)</td>
<td style="text-align: center;">$+$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Disulfiram exhibited the highest toxicity to MCF7 cells, and both Disulfiram and Fulvestrant preferentially targeted MCF7 over MCF10A. MCF7 is an estrogenreceptor positive cell line, and Fulvestrant is an estrogen receptor antagonist, which degrades the receptor. This combination could target the estrogendependent growth of MCF7 cells. Disulfiram's aldehyde dehydrogenase inhibitory action can further increase its toxicity to MCF7 cells."</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$\begin{gathered} \text { Mebendazole } \ \text { (Parasitic Infections) } \end{gathered}$</td>
<td style="text-align: center;">Quinacrine <br> (Malaria/Anti-inflammatory)</td>
<td style="text-align: center;">$+$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Both Mebendazole and Quinacrine preferentially targeted MCF7 cells. Mebendazole has anti-cancer properties, potentially by disrupting microtubule function. Quinacrine, though primarily an antimalarial, can exert anti-cancer effects, possibly due to its ability to intercalate with DNA and/or inhibit topoisomerase. This combination might have a multi-pronged effect on MCF7 cells, disrupting cellular division and DNA processes."</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Disulfiram <br> (Alcoholism)</td>
<td style="text-align: center;">Mebendazole <br> (Parasitic Infections)</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"As Disulfiram demonstrated the highest toxicity to MCF7 cells and Mebendazole also preferentially targeted MCF7, their combination might exert a compounded effect. The combined microtubule disruption by Mebendazole and the aldehyde dehydrogenase inhibition by Disulfiram could cripple the MCF7 cells at multiple fronts."</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Disulfiram <br> (Alcoholism)</td>
<td style="text-align: center;">Quinacrine <br> (Malaria/Anti-inflammatory)</td>
<td style="text-align: center;">$+$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">"Disulfiram exhibited the highest toxicity toward MCF7 cells and has been shown to induce oxidative stress and inhibit proteasomal and NF-кB pathways, which are vital for cancer cell survival. Quinacrine, an antimalarial drug, also possesses anti-cancer properties, including the ability to inhibit NF-кB and autophagy, a survival mechanism often upregulated in cancer cells. The combination is expected to potentiate oxidative stress and inhibit survival pathways more effectively in MCF7 cells. Given that both drugs preferentially targeted MCF7 over MCF10A, their combination might enhance selectivity, exploiting their shared mechanisms for a synergistic effect."</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4. GPT4 drug combination recommendations based on results from the first screen. Synergy hypotheses are provided for each of the four combinations.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7418 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7418</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7418</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-268667008</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.14801v1.pdf" target="_blank">Assessing the Utility of Large Language Models for Phenotype-Driven Gene Prioritization in Rare Genetic Disorder Diagnosis</a></p>
                <p><strong>Paper Abstract:</strong> Phenotype-driven gene prioritization is a critical process in the diagnosis of rare genetic disorders, involving identifying and ranking potential disease-causing genes based on observed physical traits or phenotypes. While traditional approaches heavily rely on curated knowledge graph with phenotype-gene relations, recent advancements in large language models (LLMs) have opened doors to the potential of AI predictions through extensive training on diverse corpora and complex models. In this study, we conducted a comprehensive evaluation of five large language models, including two Generative Pre-trained Transformers (GPT) series, and three Llama2 series, assessing their performance across three key metrics: task completeness, gene prediction accuracy, and adherence to required output structures. We conducted various experiments, exploring various combinations of models, prompts, phenotypic input types, and task difficulty levels. Our findings reveal that even the best performed LLM, GPT-4, only achieved an accuracy of 16.0%, which still lags behind traditional bioinformatics tools. Among the five LLMs, prediction accuracy increased as the parameter/model size increased. A similar increasing trend was observed for the task completion rate (70.7% to 94.2%), with complicated prompts more likely to increase task completeness in models smaller than GPT-4. On the other hand, complicated prompts are more likely to decrease the structure compliance rate, but no prompt effects were observed in GPT-4, which achieved an almost perfect structure compliance rate in all experiments. Comparing to HPO term-based input, LLM was also</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7418.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7418.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt complexity effect (GPT-3.5 completion & accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of prompt complexity (Original → Original+Role+Instruction) on GPT-3.5 task completion and accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study varied prompt templates from a minimal instruction to more complex prompts adding a 'role' (genetic counselor) and an 'instruction' clause; for GPT-3.5 more complex prompts drastically increased task completion and overall accuracy but decreased structure compliance (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI dialogue-optimized transformer (Instruct/Chat-style fine-tuned on supervised data and RLHF from GPT-3 family).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈100B (paper notes order-of-magnitude similar to GPT-4 / 'hundreds of billions')</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Phenotype-driven gene prioritization (top-k gene list)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a patient's phenotype (HPO terms or free-text), return a comma-separated list of the top-k candidate genes (top 10 or top 50).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language prompt (4 variants): Original; Original + Role; Original + Instruction; Original + Role + Instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot single-query prompts, no exemplars; variants add 'Consider you are a genetic counselor' (Role) and/or 'Please use the knowledge you have trained. No need to access real-time DB' (Instruction). Temperature set to 0; three independent repetitions per experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task completion rate and accuracy (presence of true gene in top-K); accuracy reported both among completed tasks and overall.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Task completion: Original 23.15% → Original+Role+Instruction 93.93% (completion rate). Accuracy (top-50, overall): Original 2.41% → Original+Role+Instruction 15.30% (overall accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline = 'Original' prompt (minimal instruction): completion 23.15%, accuracy 2.41% (top-50 overall).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+70.78 percentage points completion (absolute); +12.89 percentage points accuracy (absolute, top-50 overall).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>OpenAI API with temperature=0, zero-shot prompts; three independent runs per experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7418.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7418.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt complexity effect (GPT-4 robustness)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of prompt complexity on GPT-4 accuracy, completion, and structure compliance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For GPT-4 the study reports that prompt variation (Original vs Role vs Instruction vs Role+Instruction) had no significant effect on gene-prediction accuracy or task completion and minimal effect on output-structure compliance (GPT-4 was robust across prompt variants).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal, RLHF-refined large transformer (dialogue-capable), larger than GPT-3.5; trained with supervised fine-tuning and RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not explicitly stated; described as substantially larger than GPT-3.5 (paper notes 'approximately a hundred billion' for GPT-3.5 and GPT-4 larger).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Phenotype-driven gene prioritization (top-k gene list)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Return top-10 or top-50 candidate genes from phenotype input.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language prompt (same four variants as above).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; prompts include minimal instruction and variants adding role and instruction. Temperature set to 0; experiments repeated three times.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (presence of true gene in top-K), task completion rate, output-structure compliance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Overall prediction accuracy (top-50): GPT-4 average ~15.5% (paper reports 15.5% overall; Table values vary by setting). Task completion average 94.22%. Structure compliance average 79.28%. No significant changes across prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>No significant change reported across prompt variants (prompt-insensitive in accuracy and completion for GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>OpenAI API, temperature=0, zero-shot, three repetitions.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7418.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7418.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Input modality: HPO terms vs free-text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of structured (HPO) versus unstructured (free-text narrative) phenotype input on LLM performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structured HPO term input improved gene-prediction accuracy and sometimes structure compliance relative to free-text narrative input across models; LLMs could still do better-than-random on free-text but with reduced performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (reported for GPT-4, GPT-3.5, Llama2 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluated models: GPT-4 and GPT-3.5 (OpenAI chat models); Llama2-chat variants (7B, 13B, 70B) via Replicate; all are transformer-based, chat-finetuned (SFT + RLHF for chat variants).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 (>100B implied), GPT-3.5 (~100B implied), Llama2-7b/13b/70b (7B/13B/70B respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Phenotype-driven gene prioritization (top-10 and top-50)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict top-k candidate genes from either a list of HPO concept names or a free-text phenotype narrative.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Input modality: HPO concept list (semicolon-delimited terms) vs free-text narrative (extracted sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt content</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>HPO inputs were curated lists of Human Phenotype Ontology terms; free-text narratives were extracted from source articles (excluded genomic-analysis text). Zero-shot prompts otherwise identical. Temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (presence of true gene in top-K); compared across input types.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-4 (top-50): HPO input 20.67% accuracy (avg) vs free-text 12.25% accuracy (avg). GPT-3.5 (top-50): HPO 17.87% vs free-text 13.44%. Similar trend in top-10 but with smaller gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline = HPO input for comparison; free-text was the contrasted format.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>GPT-4 top-50: -8.42 percentage points accuracy when using free-text vs HPO (approx -40.7% relative). GPT-3.5 top-50: -4.43 percentage points absolute.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompts; three repetitions; temperature=0; experiments permuted across days for GPT to reduce temporal bias.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7418.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7418.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task difficulty (Top-10 vs Top-50)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of requested output length/difficulty (Top-10 challenging vs Top-50 easier) on completion, accuracy, and structure compliance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Requesting fewer (Top-10) vs more (Top-50) candidate genes changed completion rates, structure compliance, and accuracy: Top-10 tasks were generally easier to complete and had higher structure compliance but lower absolute chance to include the true gene because of smaller list size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (GPT-4, GPT-3.5, Llama2-7b/13b/70b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-finetuned transformer models of varying sizes (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>See per-model sizes above (GPT-4 larger; Llama2 variants 7B/13B/70B).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Phenotype-driven gene prioritization with top-k output constraint</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce either the top 10 (challenging) or top 50 (easier) candidate genes for each phenotype input.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Output list length constraint (top-10 vs top-50) requested in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>task specification / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same zero-shot prompt structure, differing only in {top k} substitution of 'top 10' or 'top 50'. Temperature=0; repeated three times.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (true gene in top-K), task completion rate, structure compliance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Across models: overall accuracy Top-10 12.11% (10.15% overall in some places) vs Top-50 17.41% (13.06% overall). Task completion: Top-10 average 83.82% (GPT-4 98.86%, GPT-3.5 68.79%) vs Top-50 average 75.02% (GPT-4 89.59%, GPT-3.5 60.45%). GPT-4 structure compliance: Top-10 99.98% vs Top-50 58.58%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Top-50 tends to increase absolute probability of including true gene (higher accuracy) but can reduce completion and structure compliance relative to Top-10; e.g., GPT-4 compliance drops from ~99.98% (Top-10) to ~58.58% (Top-50).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompts with explicit top-k; temperature=0; three repetitions.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7418.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7418.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model size / family effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Influence of model family and parameter scale (GPT-4, GPT-3.5, Llama2-7b/13b/70b) on prompt-format sensitivity and overall performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Larger models yielded higher task completion, accuracy, and structure compliance; model-scale also modulated sensitivity to prompt complexity and input format (smaller models more affected by prompt wording).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; GPT-3.5; Llama2-7b-chat; Llama2-13b-chat; Llama2-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based chat-finetuned LLMs: GPT family (OpenAI, RLHF), Llama2-chat family (Meta open models fine-tuned with SFT and RLHF for chat).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 (largest, unspecified), GPT-3.5 (~100B implied), Llama2-7b/13b/70b explicitly 7B/13B/70B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Phenotype-driven gene prioritization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Return top-k candidate genes from phenotype input using zero-shot chat prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural language prompts; input modality HPO vs free-text; prompt variants tested (4 templates).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>model scale effect on prompt sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Same experimental setup across models; Llama2-chat only tested with the most complex prompt (Original+Role+Instruction) to save cost; GPTs evaluated across prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (top-K), task completion rate, output-structure compliance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Accuracy (overall): GPT-4 15.5% (best), GPT-3.5 12.7%, Llama2-70b 6.4%, Llama2-13b 5.6%, Llama2-7b 3.6%. Task completion average: GPT-4 94.22%, GPT-3.5 64.62%; Llama2-7b 70.70%, 13b 72.82%, 70b 82.83%. Structure compliance: GPT-4 79.28% vs GPT-3.5 27.32% vs Llama2-70b 0.25% (70B), Llama2-13b 1% (approx).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Clear positive correlation of model size with accuracy and completion; e.g., GPT-4 vs GPT-3.5: +2.8 percentage points absolute accuracy (15.5% vs 12.7%) and +29.6 points completion (94.22% vs 64.62%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompts; GPTs via OpenAI API, Llama2 via Replicate; temperature=0; three repetitions.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7418.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7418.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt complexity → structure compliance tradeoff</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tradeoff between prompt detail and output-structure compliance (complex prompts reduce compliance in smaller models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding role and instruction clauses improved completion for smaller models but substantially reduced the likelihood that the model's output matched the exact required comma-separated gene-list format, especially for GPT-3.5 and Llama2 variants; GPT-4 was resilient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, Llama2 variants, GPT-4 (contrast)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-finetuned transformer LLMs with varying sizes and optimization; GPT-3.5 and Llama2-chat are SFT/RLHF refined; GPT-4 further improved.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 ~100B (implied), Llama2 7B/13B/70B, GPT-4 larger than GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Output-structure compliance for top-k gene list format</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce output strictly matching a regex-enforced comma-separated HGNC gene-symbol list or 'Not Applicable'.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt variants (Original, Original+Role, Original+Instruction, Original+Role+Instruction); compliance measured by regex pattern matching.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style impacts on output format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>The 'Original' minimal prompt often yields higher format compliance in GPT-3.5, while adding Role/Instruction reduces compliance; measured independent of task completion. GPT-4's compliance was high across prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Output-structure compliance rate (percent of responses matching required regex pattern).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-3.5 average compliance 27.32%; 'Original' prompt compliance 62.97% but drops to 30.59% (Role), 15.46% (Instruction), and 0.25% (Role+Instruction). GPT-4 compliance average 79.28% and relatively stable across prompt variants. Llama2-70b compliance ~0.25%; Llama2-13b ~1%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline = 'Original' prompt (for GPT-3.5 compliance 62.97%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>For GPT-3.5, adding Role+Instruction leads to -62.72 percentage points compliance (62.97% → 0.25%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompts; regex-based automated compliance check; temperature=0; three repetitions.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7418.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7418.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deterministic decoding (temperature=0) & repetitions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experimental decoding and repeat-run settings (temperature=0, three independent runs) and observed variability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>All GPT experiments used temperature=0 to encourage deterministic outputs; each experiment was repeated three times and variability in completion, accuracy, and compliance across iterations was observed but limited.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 and GPT-4 (decoding setting reported); Llama2 runs analogous</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-style LLMs accessed via APIs (OpenAI for GPTs, Replicate for Llama2), deterministic decoding enforced by temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>As above (GPT-3.5 ~100B, GPT-4 larger).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Phenotype-driven gene prioritization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same gene-ranking task, evaluated under deterministic decoding and multiple repetitions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot prompt; decoding temperature set to 0; three independent response generations per experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>experimental setting</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Temperature=0 (to reduce stochasticity), three independent API calls per condition; for GPTs experiments permuted over time to reduce calendar-day bias.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Observed variability counts in repeated runs (number of experiments with differing completion/accuracy/compliance across repetitions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Out of 6,817 unique experiments, 401 showed differing task completion across iterations. For accuracy, out of 5,135 experiments, 153 had differing accurate-result occurrences across repetitions. For structure compliance, out of 5,296 settings, 1,120 exhibited differences across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Temperature=0, three independent runs, permuted execution order for GPTs; Llama2 via Replicate without calendar bias concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7418.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7418.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot limitation & few-shot/CoT potential</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of zero-shot prompting and authors' note that few-shot or chain-of-thought (CoT) prompting may improve performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>All experiments were zero-shot; authors explicitly state this as a limitation and cite literature showing improvements from few-shot examples or chain-of-thought prompting for other tasks, implying prompt format (few-shot/CoT) could change performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LLMs (GPT family, Llama2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-finetuned transformer LLMs; no additional task-specific finetuning used in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Varied (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Phenotype-driven gene prioritization (zero-shot only in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Authors used zero-shot prompts exclusively; note that moving to few-shot or chain-of-thought might improve results based on prior studies.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot prompting only (no exemplars, no chain-of-thought); authors discuss but do not test few-shot or CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / experimental limitation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot prompts with no exemplars; cited literature indicates potential ~12% relation-extraction improvement from one-shot, chain-of-thought benefits for other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not applicable (no experimental numbers in this paper for few-shot/CoT on this task).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline = zero-shot (reported throughout paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot only; authors note omission as limitation and potential for future exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing <em>(Rating: 2)</em></li>
                <li>Prompt Engineering For ChatGPT: A Quick Guide To Techniques, Tips, And Best Practices. <em>(Rating: 2)</em></li>
                <li>A prompt pattern catalog to enhance prompt engineering with chatgpt <em>(Rating: 1)</em></li>
                <li>Zero-shot information extraction via chatting with chatgpt <em>(Rating: 1)</em></li>
                <li>Is ChatGPT a Biomedical Expert?--Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks <em>(Rating: 1)</em></li>
                <li>Using GPT-4 Prompts to Determine Whether Articles Contain Functional Evidence Supporting or Refuting Variant Pathogenicity <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7418",
    "paper_id": "paper-268667008",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Prompt complexity effect (GPT-3.5 completion & accuracy)",
            "name_full": "Effect of prompt complexity (Original → Original+Role+Instruction) on GPT-3.5 task completion and accuracy",
            "brief_description": "The study varied prompt templates from a minimal instruction to more complex prompts adding a 'role' (genetic counselor) and an 'instruction' clause; for GPT-3.5 more complex prompts drastically increased task completion and overall accuracy but decreased structure compliance (see other entries).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "OpenAI dialogue-optimized transformer (Instruct/Chat-style fine-tuned on supervised data and RLHF from GPT-3 family).",
            "model_size": "≈100B (paper notes order-of-magnitude similar to GPT-4 / 'hundreds of billions')",
            "task_name": "Phenotype-driven gene prioritization (top-k gene list)",
            "task_description": "Given a patient's phenotype (HPO terms or free-text), return a comma-separated list of the top-k candidate genes (top 10 or top 50).",
            "problem_format": "Zero-shot natural-language prompt (4 variants): Original; Original + Role; Original + Instruction; Original + Role + Instruction.",
            "format_category": "prompt style",
            "format_details": "Zero-shot single-query prompts, no exemplars; variants add 'Consider you are a genetic counselor' (Role) and/or 'Please use the knowledge you have trained. No need to access real-time DB' (Instruction). Temperature set to 0; three independent repetitions per experiment.",
            "performance_metric": "Task completion rate and accuracy (presence of true gene in top-K); accuracy reported both among completed tasks and overall.",
            "performance_value": "Task completion: Original 23.15% → Original+Role+Instruction 93.93% (completion rate). Accuracy (top-50, overall): Original 2.41% → Original+Role+Instruction 15.30% (overall accuracy).",
            "baseline_performance": "Baseline = 'Original' prompt (minimal instruction): completion 23.15%, accuracy 2.41% (top-50 overall).",
            "performance_change": "+70.78 percentage points completion (absolute); +12.89 percentage points accuracy (absolute, top-50 overall).",
            "experimental_setting": "OpenAI API with temperature=0, zero-shot prompts; three independent runs per experiment.",
            "statistical_significance": null,
            "uuid": "e7418.0"
        },
        {
            "name_short": "Prompt complexity effect (GPT-4 robustness)",
            "name_full": "Effect of prompt complexity on GPT-4 accuracy, completion, and structure compliance",
            "brief_description": "For GPT-4 the study reports that prompt variation (Original vs Role vs Instruction vs Role+Instruction) had no significant effect on gene-prediction accuracy or task completion and minimal effect on output-structure compliance (GPT-4 was robust across prompt variants).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI multimodal, RLHF-refined large transformer (dialogue-capable), larger than GPT-3.5; trained with supervised fine-tuning and RLHF.",
            "model_size": "Not explicitly stated; described as substantially larger than GPT-3.5 (paper notes 'approximately a hundred billion' for GPT-3.5 and GPT-4 larger).",
            "task_name": "Phenotype-driven gene prioritization (top-k gene list)",
            "task_description": "Return top-10 or top-50 candidate genes from phenotype input.",
            "problem_format": "Zero-shot natural-language prompt (same four variants as above).",
            "format_category": "prompt style",
            "format_details": "Zero-shot; prompts include minimal instruction and variants adding role and instruction. Temperature set to 0; experiments repeated three times.",
            "performance_metric": "Accuracy (presence of true gene in top-K), task completion rate, output-structure compliance.",
            "performance_value": "Overall prediction accuracy (top-50): GPT-4 average ~15.5% (paper reports 15.5% overall; Table values vary by setting). Task completion average 94.22%. Structure compliance average 79.28%. No significant changes across prompt variants.",
            "baseline_performance": null,
            "performance_change": "No significant change reported across prompt variants (prompt-insensitive in accuracy and completion for GPT-4).",
            "experimental_setting": "OpenAI API, temperature=0, zero-shot, three repetitions.",
            "statistical_significance": null,
            "uuid": "e7418.1"
        },
        {
            "name_short": "Input modality: HPO terms vs free-text",
            "name_full": "Effect of structured (HPO) versus unstructured (free-text narrative) phenotype input on LLM performance",
            "brief_description": "Structured HPO term input improved gene-prediction accuracy and sometimes structure compliance relative to free-text narrative input across models; LLMs could still do better-than-random on free-text but with reduced performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (reported for GPT-4, GPT-3.5, Llama2 variants)",
            "model_description": "Evaluated models: GPT-4 and GPT-3.5 (OpenAI chat models); Llama2-chat variants (7B, 13B, 70B) via Replicate; all are transformer-based, chat-finetuned (SFT + RLHF for chat variants).",
            "model_size": "GPT-4 (&gt;100B implied), GPT-3.5 (~100B implied), Llama2-7b/13b/70b (7B/13B/70B respectively).",
            "task_name": "Phenotype-driven gene prioritization (top-10 and top-50)",
            "task_description": "Predict top-k candidate genes from either a list of HPO concept names or a free-text phenotype narrative.",
            "problem_format": "Input modality: HPO concept list (semicolon-delimited terms) vs free-text narrative (extracted sentences).",
            "format_category": "input modality / prompt content",
            "format_details": "HPO inputs were curated lists of Human Phenotype Ontology terms; free-text narratives were extracted from source articles (excluded genomic-analysis text). Zero-shot prompts otherwise identical. Temperature=0.",
            "performance_metric": "Accuracy (presence of true gene in top-K); compared across input types.",
            "performance_value": "GPT-4 (top-50): HPO input 20.67% accuracy (avg) vs free-text 12.25% accuracy (avg). GPT-3.5 (top-50): HPO 17.87% vs free-text 13.44%. Similar trend in top-10 but with smaller gaps.",
            "baseline_performance": "Baseline = HPO input for comparison; free-text was the contrasted format.",
            "performance_change": "GPT-4 top-50: -8.42 percentage points accuracy when using free-text vs HPO (approx -40.7% relative). GPT-3.5 top-50: -4.43 percentage points absolute.",
            "experimental_setting": "Zero-shot prompts; three repetitions; temperature=0; experiments permuted across days for GPT to reduce temporal bias.",
            "statistical_significance": null,
            "uuid": "e7418.2"
        },
        {
            "name_short": "Task difficulty (Top-10 vs Top-50)",
            "name_full": "Effect of requested output length/difficulty (Top-10 challenging vs Top-50 easier) on completion, accuracy, and structure compliance",
            "brief_description": "Requesting fewer (Top-10) vs more (Top-50) candidate genes changed completion rates, structure compliance, and accuracy: Top-10 tasks were generally easier to complete and had higher structure compliance but lower absolute chance to include the true gene because of smaller list size.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (GPT-4, GPT-3.5, Llama2-7b/13b/70b)",
            "model_description": "Chat-finetuned transformer models of varying sizes (see other entries).",
            "model_size": "See per-model sizes above (GPT-4 larger; Llama2 variants 7B/13B/70B).",
            "task_name": "Phenotype-driven gene prioritization with top-k output constraint",
            "task_description": "Produce either the top 10 (challenging) or top 50 (easier) candidate genes for each phenotype input.",
            "problem_format": "Output list length constraint (top-10 vs top-50) requested in the prompt.",
            "format_category": "task specification / prompt style",
            "format_details": "Same zero-shot prompt structure, differing only in {top k} substitution of 'top 10' or 'top 50'. Temperature=0; repeated three times.",
            "performance_metric": "Accuracy (true gene in top-K), task completion rate, structure compliance.",
            "performance_value": "Across models: overall accuracy Top-10 12.11% (10.15% overall in some places) vs Top-50 17.41% (13.06% overall). Task completion: Top-10 average 83.82% (GPT-4 98.86%, GPT-3.5 68.79%) vs Top-50 average 75.02% (GPT-4 89.59%, GPT-3.5 60.45%). GPT-4 structure compliance: Top-10 99.98% vs Top-50 58.58%.",
            "baseline_performance": null,
            "performance_change": "Top-50 tends to increase absolute probability of including true gene (higher accuracy) but can reduce completion and structure compliance relative to Top-10; e.g., GPT-4 compliance drops from ~99.98% (Top-10) to ~58.58% (Top-50).",
            "experimental_setting": "Zero-shot prompts with explicit top-k; temperature=0; three repetitions.",
            "statistical_significance": null,
            "uuid": "e7418.3"
        },
        {
            "name_short": "Model size / family effect",
            "name_full": "Influence of model family and parameter scale (GPT-4, GPT-3.5, Llama2-7b/13b/70b) on prompt-format sensitivity and overall performance",
            "brief_description": "Larger models yielded higher task completion, accuracy, and structure compliance; model-scale also modulated sensitivity to prompt complexity and input format (smaller models more affected by prompt wording).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4; GPT-3.5; Llama2-7b-chat; Llama2-13b-chat; Llama2-70b-chat",
            "model_description": "Transformer-based chat-finetuned LLMs: GPT family (OpenAI, RLHF), Llama2-chat family (Meta open models fine-tuned with SFT and RLHF for chat).",
            "model_size": "GPT-4 (largest, unspecified), GPT-3.5 (~100B implied), Llama2-7b/13b/70b explicitly 7B/13B/70B parameters.",
            "task_name": "Phenotype-driven gene prioritization",
            "task_description": "Return top-k candidate genes from phenotype input using zero-shot chat prompts.",
            "problem_format": "Zero-shot natural language prompts; input modality HPO vs free-text; prompt variants tested (4 templates).",
            "format_category": "model scale effect on prompt sensitivity",
            "format_details": "Same experimental setup across models; Llama2-chat only tested with the most complex prompt (Original+Role+Instruction) to save cost; GPTs evaluated across prompt variants.",
            "performance_metric": "Accuracy (top-K), task completion rate, output-structure compliance.",
            "performance_value": "Accuracy (overall): GPT-4 15.5% (best), GPT-3.5 12.7%, Llama2-70b 6.4%, Llama2-13b 5.6%, Llama2-7b 3.6%. Task completion average: GPT-4 94.22%, GPT-3.5 64.62%; Llama2-7b 70.70%, 13b 72.82%, 70b 82.83%. Structure compliance: GPT-4 79.28% vs GPT-3.5 27.32% vs Llama2-70b 0.25% (70B), Llama2-13b 1% (approx).",
            "baseline_performance": null,
            "performance_change": "Clear positive correlation of model size with accuracy and completion; e.g., GPT-4 vs GPT-3.5: +2.8 percentage points absolute accuracy (15.5% vs 12.7%) and +29.6 points completion (94.22% vs 64.62%).",
            "experimental_setting": "Zero-shot prompts; GPTs via OpenAI API, Llama2 via Replicate; temperature=0; three repetitions.",
            "statistical_significance": null,
            "uuid": "e7418.4"
        },
        {
            "name_short": "Prompt complexity → structure compliance tradeoff",
            "name_full": "Tradeoff between prompt detail and output-structure compliance (complex prompts reduce compliance in smaller models)",
            "brief_description": "Adding role and instruction clauses improved completion for smaller models but substantially reduced the likelihood that the model's output matched the exact required comma-separated gene-list format, especially for GPT-3.5 and Llama2 variants; GPT-4 was resilient.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, Llama2 variants, GPT-4 (contrast)",
            "model_description": "Chat-finetuned transformer LLMs with varying sizes and optimization; GPT-3.5 and Llama2-chat are SFT/RLHF refined; GPT-4 further improved.",
            "model_size": "GPT-3.5 ~100B (implied), Llama2 7B/13B/70B, GPT-4 larger than GPT-3.5.",
            "task_name": "Output-structure compliance for top-k gene list format",
            "task_description": "Produce output strictly matching a regex-enforced comma-separated HGNC gene-symbol list or 'Not Applicable'.",
            "problem_format": "Prompt variants (Original, Original+Role, Original+Instruction, Original+Role+Instruction); compliance measured by regex pattern matching.",
            "format_category": "prompt style impacts on output format",
            "format_details": "The 'Original' minimal prompt often yields higher format compliance in GPT-3.5, while adding Role/Instruction reduces compliance; measured independent of task completion. GPT-4's compliance was high across prompt variants.",
            "performance_metric": "Output-structure compliance rate (percent of responses matching required regex pattern).",
            "performance_value": "GPT-3.5 average compliance 27.32%; 'Original' prompt compliance 62.97% but drops to 30.59% (Role), 15.46% (Instruction), and 0.25% (Role+Instruction). GPT-4 compliance average 79.28% and relatively stable across prompt variants. Llama2-70b compliance ~0.25%; Llama2-13b ~1%.",
            "baseline_performance": "Baseline = 'Original' prompt (for GPT-3.5 compliance 62.97%).",
            "performance_change": "For GPT-3.5, adding Role+Instruction leads to -62.72 percentage points compliance (62.97% → 0.25%).",
            "experimental_setting": "Zero-shot prompts; regex-based automated compliance check; temperature=0; three repetitions.",
            "statistical_significance": null,
            "uuid": "e7418.5"
        },
        {
            "name_short": "Deterministic decoding (temperature=0) & repetitions",
            "name_full": "Experimental decoding and repeat-run settings (temperature=0, three independent runs) and observed variability",
            "brief_description": "All GPT experiments used temperature=0 to encourage deterministic outputs; each experiment was repeated three times and variability in completion, accuracy, and compliance across iterations was observed but limited.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 and GPT-4 (decoding setting reported); Llama2 runs analogous",
            "model_description": "Chat-style LLMs accessed via APIs (OpenAI for GPTs, Replicate for Llama2), deterministic decoding enforced by temperature=0.",
            "model_size": "As above (GPT-3.5 ~100B, GPT-4 larger).",
            "task_name": "Phenotype-driven gene prioritization",
            "task_description": "Same gene-ranking task, evaluated under deterministic decoding and multiple repetitions.",
            "problem_format": "Zero-shot prompt; decoding temperature set to 0; three independent response generations per experiment.",
            "format_category": "experimental setting",
            "format_details": "Temperature=0 (to reduce stochasticity), three independent API calls per condition; for GPTs experiments permuted over time to reduce calendar-day bias.",
            "performance_metric": "Observed variability counts in repeated runs (number of experiments with differing completion/accuracy/compliance across repetitions).",
            "performance_value": "Out of 6,817 unique experiments, 401 showed differing task completion across iterations. For accuracy, out of 5,135 experiments, 153 had differing accurate-result occurrences across repetitions. For structure compliance, out of 5,296 settings, 1,120 exhibited differences across iterations.",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Temperature=0, three independent runs, permuted execution order for GPTs; Llama2 via Replicate without calendar bias concerns.",
            "statistical_significance": null,
            "uuid": "e7418.6"
        },
        {
            "name_short": "Zero-shot limitation & few-shot/CoT potential",
            "name_full": "Use of zero-shot prompting and authors' note that few-shot or chain-of-thought (CoT) prompting may improve performance",
            "brief_description": "All experiments were zero-shot; authors explicitly state this as a limitation and cite literature showing improvements from few-shot examples or chain-of-thought prompting for other tasks, implying prompt format (few-shot/CoT) could change performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "General LLMs (GPT family, Llama2)",
            "model_description": "Chat-finetuned transformer LLMs; no additional task-specific finetuning used in this study.",
            "model_size": "Varied (see other entries).",
            "task_name": "Phenotype-driven gene prioritization (zero-shot only in experiments)",
            "task_description": "Authors used zero-shot prompts exclusively; note that moving to few-shot or chain-of-thought might improve results based on prior studies.",
            "problem_format": "Zero-shot prompting only (no exemplars, no chain-of-thought); authors discuss but do not test few-shot or CoT.",
            "format_category": "prompt style / experimental limitation",
            "format_details": "Zero-shot prompts with no exemplars; cited literature indicates potential ~12% relation-extraction improvement from one-shot, chain-of-thought benefits for other tasks.",
            "performance_metric": "Not applicable (no experimental numbers in this paper for few-shot/CoT on this task).",
            "performance_value": null,
            "baseline_performance": "Baseline = zero-shot (reported throughout paper).",
            "performance_change": null,
            "experimental_setting": "Zero-shot only; authors note omission as limitation and potential for future exploration.",
            "statistical_significance": null,
            "uuid": "e7418.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
            "rating": 2,
            "sanitized_title": "pretrain_prompt_and_predict_a_systematic_survey_of_prompting_methods_in_natural_language_processing"
        },
        {
            "paper_title": "Prompt Engineering For ChatGPT: A Quick Guide To Techniques, Tips, And Best Practices.",
            "rating": 2,
            "sanitized_title": "prompt_engineering_for_chatgpt_a_quick_guide_to_techniques_tips_and_best_practices"
        },
        {
            "paper_title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "rating": 1,
            "sanitized_title": "a_prompt_pattern_catalog_to_enhance_prompt_engineering_with_chatgpt"
        },
        {
            "paper_title": "Zero-shot information extraction via chatting with chatgpt",
            "rating": 1,
            "sanitized_title": "zeroshot_information_extraction_via_chatting_with_chatgpt"
        },
        {
            "paper_title": "Is ChatGPT a Biomedical Expert?--Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks",
            "rating": 1,
            "sanitized_title": "is_chatgpt_a_biomedical_expertexploring_the_zeroshot_performance_of_current_gpt_models_in_biomedical_tasks"
        },
        {
            "paper_title": "Using GPT-4 Prompts to Determine Whether Articles Contain Functional Evidence Supporting or Refuting Variant Pathogenicity",
            "rating": 1,
            "sanitized_title": "using_gpt4_prompts_to_determine_whether_articles_contain_functional_evidence_supporting_or_refuting_variant_pathogenicity"
        }
    ],
    "cost": 0.0151755,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Assessing the Utility of Large Language Models for Phenotype-Driven Gene Prioritization in Rare Genetic Disorder Diagnosis</p>
<p>Junyoung Kim 
Department of Biomedical Informatics
Columbia University
New YorkNYUSA</p>
<p>Jingye Yang 
Raymond G. Perelman Center for Cellular and Molecular Therapeutics
Children's Hospital of Philadelphia
19104PhiladelphiaPAUSA</p>
<p>Department of Mathematics
University of Pennsylvania
19104PhiladelphiaPAUSA</p>
<p>Kai Wang 
Raymond G. Perelman Center for Cellular and Molecular Therapeutics
Children's Hospital of Philadelphia
19104PhiladelphiaPAUSA</p>
<p>Department of Pathology and Laboratory Medicine
University of Pennsylvania
19104PhiladelphiaPAUSA</p>
<p>Chunhua Weng 
Department of Biomedical Informatics
Columbia University
New YorkNYUSA</p>
<p>Cong Liu 
Department of Biomedical Informatics
Columbia University
New YorkNYUSA</p>
<p>Assessing the Utility of Large Language Models for Phenotype-Driven Gene Prioritization in Rare Genetic Disorder Diagnosis
1960464215861988D3429CC2B914BA4B
Phenotype-driven gene prioritization is a critical process in the diagnosis of rare genetic disorders, involving identifying and ranking potential disease-causing genes based on observed physical traits or phenotypes.While traditional approaches heavily rely on curated knowledge graph with phenotype-gene relations, recent advancements in large language models (LLMs) have opened doors to the potential of AI predictions through extensive training on diverse corpora and complex models.In this study, we conducted a comprehensive evaluation of five large language models, including two Generative Pre-trained Transformers (GPT) series, and three Llama2 series, assessing their performance across three key metrics: task completeness, gene prediction accuracy, and adherence to required output structures.We conducted various experiments, exploring various combinations of models, prompts, phenotypic input types, and task difficulty levels.Our findings reveal that even the best performed LLM, GPT-4, only achieved an accuracy of 16.0%, which still lags behind traditional bioinformatics tools.Among the five LLMs, prediction accuracy increased as the parameter/model size increased.A similar increasing trend was observed for the task completion rate (70.7% to 94.2%), with complicated prompts more likely to increase task completeness in models smaller than GPT-4.On the other hand, complicated prompts are more likely to decrease the structure compliance rate, but no prompt effects were observed in GPT-4, which achieved an almost perfect structure compliance rate in all experiments.Comparing to HPO term-based input, LLM was also able to achieve better-than-random prediction accuracy by taking free-text input directly, though the performance is slightly lower than with the HPO input.Bias analysis showed that certain genes, 2 such as MECP4, CDKL5, and SCN1A, are more likely to be top ranked by LLMs, potentially explaining the dramatic variances observed across different datasets.In conclusion, this study provides valuable insights into the integration of LLMs within genomic analysis, contributing to the ongoing discussion on the utilization of advanced LLMs in clinical workflows.</p>
<p>Introduction</p>
<p>Phenotype-driven gene prioritization is a process that involves identifying and ranking candidate disease-causing genes by examining an individual's observed physical traits, known as phenotypes, in contrast to genotypes.It plays a crucial role in rare disease diagnosis when analyzing genomic data from high-throughput (e.g., Whole Genome/Exome Sequencing) experiments or design virtual panels for diagnosis purposes.It guides the additional experimental or diagnostic efforts toward the most promising candidates 1 .The underlying principle is based on the assumption that disease-related phenotypes are a result of one or more gene dysfunction.By leveraging the established genotype-phenotype-disease associations, phenotype-based analytical models have been developed, resulting in numerous bioinformatics tools.For instance, Phenomizer 2 compares individual phenotypes to a database of known genetic diseases and their associated phenotypes, and it then ranks the potential genetic diseases based on the semantic similarity of the patient's phenotype to known conditions within the Human Phenotype Ontology.Exomiser 3 integrates various data sources, including genedisease associations, variant databases, and gene ontology data, and employs a random-walk analysis to score and rank candidate genes or variants.Similarly, AMELIE 4 constructed a comprehensive disease-phenotype knowledgebase by integrating various databases and resources, including ClinVar, HGMD, and parsing relationship from literature.It then developed a machine learning classifier to rank candidate genes.Phenolyzer 5 and its successor Phen2Gene 6 integrated Human Phenotype Ontology (HPO) annotations, gene-disease databases, and gene-gene network and apply a probabilistic framework to build a phenotypedriven gene prioritization tool.With the recent advancement in deep learning, network inference methods based on modern deep learning frameworks have also been explored.For example, DeepSVP 7 constructs a graph from ontology axioms and employs DL2Vec approach for gene-phenotype association prediction.CADA 8 utilizes graph embedding techniques to predict links between phenotype and diseases.</p>
<p>The majority of the existing tools rely on established knowledge databases or knowledge graphs that connect phenotypes with genes (or monogenic diseases).These databases are typically curated through either manual processes or mining the literature, which can be prone to errors or lack comprehensiveness.Additionally, most of these bioinformatics tools can only process "term-based" input, requiring natural language processing techniques to parse the terms from clinical notes.We hypothesized that recent advancements in large language models (LLMs), trained on massive and diverse dataset, could potentially provide an end-to-end, textto-gene solution to this task by leveraging their extraordinary ability to understand natural language and access extensive and diverse repositories of information 9 .These LLMs represent noteworthy advancements in the journey towards AGI, aiming to develop intelligent systems capable of understanding and executing a diverse array of tasks, akin to human-level cognitive capabilities 10 .These models begin by employing a transformer-based decoder architecture to pre-train a base language completion model without supervision.Subsequently, the base model undergoes a fine-tuning process with human feedback and additional refinement through reinforcement learning, guided by a reward model trained using supervised methods, which can lead to the development of a ChatBot.Numerous articles have indicated the potential of LLMs in the medical field, covering a range of applications, including individual education 11 , appointment scheduling 12 , optimization of clinical decision support systems 13 , aid in data collection for clinical studies 14 , enhancement of information retrieval in electronic health records (EHRs) 15 , and summarization of evidence in publications 16 .Previous studies have shown that the parameter/model size is usually correlated with performance.For example, GPT-4 outperformed GPT-3.5 in biomedical classification tasks, reasoning tasks 17 and accuracy in question and answer tasks 18 .Similarly, Llama 2 models also demonstrated that larger model size correlated with higher performance in biomedical question and answer tasks 19 .</p>
<p>In this study, we will particularly focus on two advanced ChatBot-based LLMs: Generative Pre-trained Transformer (GPT) series, including GPT-3.5 (also known as ChatGPT) and GPT-4 20 , and the Llama2 series, including Llama2-7b-chat (with 7b indicating 7 billion parameters), Llama2-13b-chat and Llama2-70b-chat 21,22 .We will explore LLMs' ability to analyze human phenotypic presentations and predict genetic diagnoses.Our primary objectives include evaluating LLM's performance in completing the designated task, achieving accurate gene prioritization, and adhering to output structure requirements.We will also assess the impact of various factors, including prompts, model sizes, task difficulty levels, and phenotypic input type.Overall, this study aims to provide a comprehensive assessment of how LLMs can potentially be integrated into the current clinical genomic analysis workflow for rare disease diagnosis.</p>
<p>Material and methods</p>
<p>Datasets</p>
<p>We utilized a list of publicly accessible datasets consisting of total 276 de-identified individuals who had been diagnosed with Mendelian diseases 6 .It is worth highlighting that the selection of phenotypes for these individuals was based on the criterion that the diseases were monogenic, and the causal genes were previously established.Data were collected from five distinctive sources as outlined in Table 1.There are a total of 165 distinct genes in the final pool of diagnosed genes.This approach allowed us to include a broad spectrum of Mendelian diseases and genes, ensuring that the dataset covered various genetic conditions.An IRB exemption was obtained from the Columbia University Review Board.</p>
<p>For individuals in each dataset, their HPO concepts and final diagnosed genes were previously curated 6 .Free-text phenotypic descriptions were further collected from 53 original articles for 125 individuals.This was accomplished by identifying the relevant section in the articles and manually extracting the original text that provided detailed information about the individual's phenotype presentations.One such example is "At 6 years IQ was evaluated (Verbal IQ 73, Performance IQ 58).On physical examination at 15 years, she has a normal head circumference (+0 SD), facial dysmorphism and thoracolumbar kyphoscoliosis."To avoid data leakage, we excluded genetic/genomic analysis-related text from the original article.Considering earliest developed model included in this study is GPT-3, with a knowledge cutoff in 2021 (i.e., trained before September 2021), we only included articles published before 2021.</p>
<p>Prompt Engineering</p>
<p>To maximize the utilization of LLMs, effective prompt engineering is essential 23 .A prompt refers to a set of instructions or input provided to a LLM in order to steer its response or generate desired output.Prompt engineering involves the process of designing and formulating prompts to elicit desired responses from language models 24 .Considering that the fundamental function of an LLM is to accomplish language completion tasks, it's rational to assume that the quality, clarity, and specificity of the prompt could significantly impact LLM's performance 25 .To assess how variations in prompts could influence the performance, we formulated various prompts based on the intricate interplay between roles, instructions, and model performance (as shown in Table 2): (1) The "Original" prompt serves as the fundamental query to use individual phenotype features for gene prioritization.(2) The "Original + Role" prompt introduces a role assignment component, casting the model in the role of a "genetic counselor".While preserving the core objective of gene prioritization, this prompt integrates the role-based perspective, which is also endorsed as a 'system' message by OpenAI 24 .(3) The "Original + Instruction" prompt included additional guidance sentences for the model, instructing it to utilize its accumulated knowledge without requiring real-time database access for predictive purposes.(4) The "Original + Role + Instruction" prompt integrates the role of a genetic counselor with the previously mentioned instruction, offering the most comprehensive approach.Each input prompt was presented as a zero-shot 26 scenario without providing examples.Besides different prompt engineering techniques, we also considered different factors in constructing the final prompt.We assessed two input categories for individual phenotypic features: those presented through HPO terms and those via narrative descriptions (obtained as described above).In addition, we designed an easy task (i.e., making a correct gene prediction within the top 50 predictions) and a challenging (i.e., within the top 10) task to evaluate the performance.</p>
<p>Experiment Design</p>
<p>For GPT-series, we utilized the OpenAI API, making API requests with various prompts to retrieve the gene prioritization predictions for each prompt (as shown in Figure 1).GPT-3.5turbo was used for GPT-3 evaluation.We set the temperature parameter to zero to make the predictions more deterministic.In total, there were 32 experiments for each input case by combining various factors including GPT versions (GPT-4 and GPT-3.5),prompts (Original, Original + Role, Original + Instruction, and Original + Role + Instruction), phenotypic features input type (HPO concepts and free-text), and predictive task difficulties (Top 10 and Top 50).</p>
<p>For the Llama2-series, we utilized the Replicate's API (a cloud-based service that provides access to a wide range of pre-built models) to assess the three models with different parameter sizes.To save study costs, we did not consider the prompt effect by only including the most complicated prompts (Original + Role + Instruction) in the experiments.In total, there were 8 experiments for each input case.All experiments were repeated three times (i.e. three LLM response generated independently) to measure the variability of LLMs.A previous report 27 demonstrated the possible time-dependent nature of GPTs.To reduce the bias associated with calendar days and because of the closed-source nature of GPT, we permutated the experiment sequence.The final API execution date was in Aug 2023.For Llama2, we did not have that concern due to its open-source nature.</p>
<p>Evaluation Metrics</p>
<p>We assessed the performance of the LLMs on three outcomes, including the task completeness of the output, accuracy of generated gene lists, and adherence to specified output structural requirements.The evaluations metrics were specified as follows.</p>
<p>Task Completeness measures whether LLMs can produce a gene list for gene prioritization tasks.If fewer than half of the required number of genes were generated (e.g., &lt; 25 out of the top 50 predictions), we considered LLMs did not complete the task.It's important to note that we excluded fabricated genes and counted duplicated genes in the prediction list only once.If LLMs declined to produce prediction results, we also considered it as an incomplete task.Supplementary Figure 1 presents two instances of GPT responses indicating task incompletions.</p>
<p>Accuracy of Gene Predictions evaluates whether the true diagnosed gene is identified within the top (10 or 50) predicted gene list generated by the GPTs.Two sub metrics were designed.First, only experiments that successfully completed the task as mentioned above were considered for accuracy assessment.Second is the overall accuracy, which measures the same as the other two metrics that includes all experiments.For experiments where the task was not completed, we considered the gene prediction results to be incorrect.Supplementary Figure 2 presents examples of correct or incorrect predictions made by GPT.</p>
<p>Output Structure Compliance evaluates whether GPT's response adheres to the specified output format requirement.Given GPT's tendency of providing free-text responses, we deemed the outcome compliant if a portion of the response matched a set of predefined regex patterns.As we instructed GPT to generate "{Not Applicable}" for tasks that couldn't be completed, we could independently evaluate this metric regardless of GPT's task completeness.Supplementary Figure 3 presents examples of GPT's output being compliant (or noncompliant) with the output structure.</p>
<p>For each experiment , we calculated the task completeness rate (or gene prediction accuracy rate, output structure compliance rate)   () = ∑  () (,)   =1</p>
<p>for each repetition  ( = 1,2,3),</p>
<p>where  is the total number of cases for evaluation in this experiment, and  () (, ) = 1 if the task was completed (or gene prediction is accurate, output structure is compliant).The standard deviation for each metric can also be calculated by measuring the variance among three repetitions.It's important to highlight that output format and task completeness are independent of each other, whereas accuracy measurement is exclusively applicable to those experiments where tasks have been successfully completed.</p>
<p>Due to the substantial number of experiments and the significant human effort required to assess the outcomes, we developed an in-house program to automate the measurement of the three aforementioned metrics.To assess whether task was completed, we parsed the responses and compared them against HGNC gene symbols (including previous symbols and alias symbols) using regular expression, while excluding common gene name errors (e.g., SEPT1, MAR1) as detailed in the Gene Name Errors Screen project 28 .This program was also used to measure the accuracy of gene predictions by comparing the responses with the diagnosed gene HGNC symbols (including previous symbols and alias symbols).To evaluate adherence to the output format, we established regular expression patterns based on the prompt's requirements and searched for these patterns within LLMs' responses.The automated program was inspected manually on 249 randomly selected experiments, and a 100% accuracy was achieved.</p>
<p>Result Accuracy of Gene Predictions Among Different Models</p>
<p>Figure 2A showed the prediction accuracy rate of LLMs underutilizing "Original + Roles + Instruction" prompt.On average, GPT-4 achieved the highest overall prediction accuracy rate of 15.5%, followed by GPT-3.5 (12.7%),Llama2-70b-chat (6.4%), Llama2-13b-chat (5.6%) and Llama2-7b-chat (3.6%).The accuracy rate among completed tasks showed a similar trend, but with smaller gaps among different models.This result deviates from the trends observed in the results from other software applications, where TAF1 typically occupied the top position with close to perfect accuracy, followed by CUMC, DGD, AJHG and CSH.Moreover, this variability across the datasets assumes potential biases in the models' performance.</p>
<p>Figure</p>
<p>In general, even the best-performing LLM model still lags behind traditional bioinformatics tools.An example of comparing performance between LLMs and traditional bioinformatics tools was shown in Figure 2C, where the accuracy of predicting the top 50 candidate genes in the DGD datasets was plotted.</p>
<p>Gene-Dependent Prediction Bias Analysis</p>
<p>A further investigation of the GPT-4's response revealed potential gene-dependent bias associated with the LLM models, which might explain the variations in accuracy observed across datasets.Figure 3 shows the top 10 genes most frequently predicted by GPT-4, with six of them did not appear in the diagnosed gene pool.For example, FOXG1 consistently appeared in GPT's output, with 2125 instances (22.1%) in the top 10 predictions and 2791 instances (29%) within the top 50 predictions, despite never occurred in the diagnosed pool.In contrast, genes that frequently appear in the diagnosed gene pool, such as DHX30, are notably absent from both the top 10 and top 50 predictions among all experiments.</p>
<p>Table S1 shows the odds ratio calculated as the observed prediction times versus expected prediction times in the dataset for each gene across different models.Overall, 6383 genes never occurred in the final diagnosed pool but predicted by GPTs.Most of them (4693) were generated when the task involved top 50 predictions.Furthermore, considering the 165 genes in the final diagnosed pool given the 'Original + Role + Instruction' prompt, GPT-4 did not predict 104 genes (OR = 0), and GPT-3.5 failed to predict 116 genes.Llama2-70b-chat missed predictions for 135 genes, Llama2-13b-chat for 140 genes, and Llama2-7b-chat for 151 genes.This suggests that GPT models were more likely to make successful predictions.Additionally, the number of genes correctly predicted more than fifty percent of the time (OR &gt; 5) were 38 by GPT-4, 34 by GPT-3.5, 8 by Llama2-7b-chat, 5 by Llama2-13b-chat, and 9 by Llama2-70bchat.</p>
<p>Effect of Different Factors in Predicting Accuracy</p>
<p>Table 3 and Table 4 showed the gene prediction accuracy rate under different settings in GPTs and Llama2 models.As expected, the gene prediction accuracy for the more challenging task (i.e., making correct predictions within the top 10) was significantly lower (12.11%;10.15% overall) than the easier task (17.41%; 13.06% overall).However, the differences between these two tasks do not seem as large as expected, which could indicate that LLMs may either make correct predictions in the early sequence of output or cannot make correct predictions at all.</p>
<p>In top 50 tasks (Table 4), HPO concept inputs achieved an average accuracy rate of 20.67% for GPT-4 (18.18% overall), whereas free-text inputs had an average accuracy rate of 12.25% (11.67% overall), and GPT-3.5 and Llama2-chat models showed the same tendency.Similar trend observed in top 10 tasks but less discrepancy (Table 3).This suggests that despite LLM's ability to understand narratives, structured input can contribute to improving its prediction accuracy.</p>
<p>We did not observe prompts have a significant effect on the accuracy of gene predictions in GPT-4.However, we found that the prompt matters in GPT-3.5, especially for overall accuracy, where there is a tendency for more detailed prompt content to yield better accuracy results.The "Original" prompt yielded 2.41% overall accuracy on average in predicting top 50 genes, with "Original + Role + Instruction" prompt showed 15.3% accuracy, which can be explained by the significant impact of prompts on task completion rate observed in GPT-3.5 (described in the section below).A similar trend was observed in the predictions of the top 10 genes, albeit with less discrepancy.</p>
<p>A breakdown of factor impacts under different combinations of other factors can be found in Table S2.</p>
<p>Task Completeness Among different models</p>
<p>The overall task completion rate was 78.34%, with the average for GPT models at 79.42% and Llama2-chat models (using the 'Original + Role + Instruction' prompt) at 75.45%.Similar to accuracy, larger model sizes tended to achieve higher task completion rates.Specifically, GPT-4 achieved an almost perfect average completion rate of 94.22% across all scenarios, which decreased to 64.62% for GPT-3.5.This trend was also observed in the Llama2-chat models (using the 'Original + Role + Instruction' prompt only), where the 7b-chat model had a completion rate of 70.70%, the 13b-chat model 72.82%, and the 70b-chat model 82.83%.</p>
<p>While prompts had no significant impact on task completion in GPT-4, they significantly influenced GPT-3.5, where more complex prompts tended to achieve better completion rates.As shown in Table 5, the completion rate for GPT-3.5 was 23.15% with the simplest prompts, increasing to 93.93% with the most complex prompts.</p>
<p>All LLMs were more likely to complete easier tasks, except Llama2-70b-chat model.When requesting only the top 10 genes, an average completion rate of 83.82% (GPT-4: 98.86%, GPT-3.5:68.79%, Llama2-7b-chat: 87.11%, Llama2-13b-chat: 91.85%, Llama2-70b-chat: 82.46%) was achieved.However, extending the request to include the top 50 responses resulted in a significantly lower rate of 75.02% (GPT-4: 89.59%, GPT-3.5:60.45%, Llama2-7b-chat: 54.28%, Llama2-13b-chat: 53.78%, Llama2-70b-chat: 83.21%).Further investigation revealed that the largest discrepancies occurred when GPT-3.5 was tasked with generating the top 50 results, achieving 12.80% task completion on average (Table S2).</p>
<p>Similarly, input type also had an impact, though differently across models.GPT models generally had higher completion rates on free-text input; for GPT-4, it ranged from 93.40% (HPO) to 96.03% (free-text), and for GPT-3.5, from 62.79% (HPO) to 68.67% (free-text).However, 2 out of 3 Llama2-chat models had higher completion rates with HPO input: Llama2-7b-chat achieved 71.56% with HPO and 68.60% with free-text, while Llama2-70b-chat had 85.27% with HPO and 77.47% with free-text.Conversely, Llama2-13b-chat showed the opposite trend, with 70.89% for HPO and 77.07%for free-text.Nonetheless, these factors are likely influenced by other variables.</p>
<p>A breakdown of factor impacts under different combinations of other factors can be found in Table S2.</p>
<p>Structure Compliance</p>
<p>Table 6 demonstrates the output structure compliance rate for various experiments.Across nearly all settings, GPT-3.5 struggled to generate compliant output responses, achieving an average compliance rate of 27.32%.Notably, the average compliance rate significantly drops when employing more complex prompts, approaching zero for all categories that use the "Original + Role + Instruction" prompt structure.In contrast, GPT-4 exhibited a significantly higher and roust output structure compliance rate of 79.28% (p &lt; 0.01).Llama2-chat models showed a divergence from GPT models, with the 70B model achieving the lowest structural compliance rate (0.25%) and the 13B model the highest (1%).Input type also impacted compliance, with HPO inputs achieving an average compliance rate of 80.01% with GPT-4, compared to 77.67% for free-text inputs.This discrepancy was more emphasized in GPT-3.5, where HPO concept inputs achieved a 30.15% compliance rate, while free-text inputs achieved only 21.07%.Interestingly, in contrast to the task completeness assessment, the "Original" prompt achieved the highest average compliance rate of 62.97% in GPT-3.5, while additional prompts significantly decreased output structure compliance to 30.59% (with "Role"), 15.46% (with "Instruction"), and 0.25% (with "Role and Instruction").Notably for GPT-4, the "Original" prompt resulted in the lowest structural compliance rate of 77.64%.Furthermore, the task difficulty level was crucial; in GPT-4, easy tasks (Top 10) achieved 99.98%, while difficult tasks (Top 50) achieved 58.58%.This trend was similarly observed in GPT-3.5, albeit with a smaller discrepancy (31.17% for Top 10 and 23.46% for Top 50).</p>
<p>Variability analysis of GPT</p>
<p>We conducted a detailed investigation into the variabilities in GPT-generated responses.Among the 6,817 unique experiments analyzed, 401 (of which 4,887 completed the task in at least one iteration) yielded different results (in terms of task completion) across three iterations.</p>
<p>In terms of accuracy, out of the 5,135 unique experiments analyzed, 153 (with 697 making accurate predictions in at least one iteration) showed differences in achieving the accurate results across the three iterations.Regarding structure compliance, among the 5,296 settings analyzed, 1,120 (with 2,839 yielding compliant results) exhibited differences in compliance across the three iterations.</p>
<p>Discussion</p>
<p>Previous studies have consistently shown that LLMs achieved remarkable performance across various medical applications, including the Ophthalmology Exam 31 , USMLE Sample Exam 32 , and progress notes summarization 33 .A few studies have explored the application of LLM models in the clinical genetic/genomic fields.One study suggested ChatGPT achieved a 68.2% accuracy regarding 85 human genetics-related multiple-choice questions and outperformed human response 34 .Another study found GPT-4 were capable of generating cases with similar names regarding the discovery of gene set function 35 .Transformer-based model has also been applied to gene sequence classification tasks and was found to perform better with fewer training epochs than other machine learning approaches 36 .To the best of our knowledge, our study represents the first comprehensive evaluation of most advanced LLMs' performance for phenotype-based rare genetic disease diagnosis.</p>
<p>Base large language models were initially conceived as end-to-end, next-token prediction models.For this task, ChatBot-like models are required to achieve the prediction without extra fine-tuning.GPT-3.5 trained similarly to InstructGPT and optimized for dialogue from GPT-3 37 .Llama2-chat is also fine-tuned for dialogue usage based on Llama2 starting from Supervised Fine-Tuning(SFT) and trained utilizing Reinforcement Learning with Human Feedback (RLHF) 21 .GPT-4 also utilized RLHF and supports a multimodal model 38 .In fact, when we used the base Llama2 model, it appeared to lack comprehension, repetitively generating the text by repeating the questions and examples mentioned in the prompts (specific prompts were designed with "answer is: " pending as the last few tokens to prompt base models for next token prediction task).This tendency remains consistent, irrespective of the number of genes to be generated or variations in phenotypic descriptions; it always begins with a gene provided in the prompt example (in this case, ABCA1 or ABC1), until a specified token length is satisfied.The accuracy of gene prediction by Llama2 appears to be entirely random.However, once the model is trained in a supervised fashion (GPT-3.5/GPT-4or Llama2-chat), it can provide better-than-random guess in terms of gene prediction.</p>
<p>However, the current LLM-based prediction did not match that of software specifically designed for this task.For instance, the best-performing GPT model demonstrated an average accuracy rate of 26.91% in the CSH dataset (24.31% for top 10 and 29.51% for top 50), which is notably better than random predictions (0.03% for top 10 and 0.17% for top 50).However, it falls significantly short of the accuracy achieved by Phen2Gene (35.3% for top 10 and 55.3% for top 50 with the same CSH dataset) 6 .Other methods, such as Phenolyzer and AMELIE (when using only HPO), also showed approximately 40% accuracy for the top 50 prediction experiments 39 , which is still better than the best performance achieved by GPTs.Even worse, we have to keep in mind that the overall accuracy rate can be even lower because LLM cannot always guarantee to complete the task or complete it in the required output format.Furthermore, despite being capable of comprehending free-text input, LLMs, in general, achieved more accurate predictions when using human-curated HPO concepts as input rather than free-text inputs.</p>
<p>Additionally, we identify instances of hallucinations generated by all models, particularly when examining the top 50 predictions.These occurrences were often associated with generating of spurious gene variants within a gene family.For example, starting from OPA1, an actual existing gene, GPT-3.5 extends to OPA50 in sequence, which were not actual genes.Previous studies have highlighted the same limitations in answering medical questions 40,41 with attaching fabricated references.This deficiency may explain why GPT sometimes generates fictitious gene names when dealing with gene symbols, as gene symbols can function as both an acronym of the word as well as a type of identifier.One potential explanation is that Byte Pair Encoding (BPE) tokenizer, employed by both GPT and Llama2, breaks down the reference or gene symbols, allowing GPT the possibility to fabricate a reference or a gene symbol using its partial segments.</p>
<p>Bias was identified in the predicting specific genes.For example, when the actual diagnosed gene was 'TAF1', 'DHX30' and 'WDR26', none of the experiments produced accurate predictions.Conversely, when the diagnosed genes were 'MECP2', 'TP53' and 'SCN1A', GPT exhibited a considerably higher level of accuracy.For Llama2, 'EP300' and 'WT1' demonstrated high level of accuracy.A deeper investigation revealed that these genes have received substantial attention in research studies, with a higher number of Google Scholar search results, potentially contributing to their overprediction by LLMs.For genes those were not predicted accurately, the number of search hits was less than 10000 (e.g., TAF1: 9950, DXH30: 1010, WDR26: 972).In contrast, genes that were predicted with high accuracy had a significantly larger number of search hits (e.g., MECP2: 57700, TP53: 381000, SCN1A: 22300, EP300: 23000, WT1: 79300), except CDKL5 (8180), which had fewer cases than TAF1 but still exhibited high accuracy.</p>
<p>The above observations suggest a few important points.First, current LLMs might excel at tasks that humans can handle, but when it comes to tasks that humans are unable to manage and for which specialized knowledge bases were previously curated and models were trained upon specialized datasets, their performance does not match that of specialized tools.To enhance prediction accuracy in the future, fine-tuning the model with specialized datasets and models tailored to the genetic domain should be considered as in other task 42,43 .Studies have shown that in differential diagnosis tasks, the LLM model Med-PaLM2, which was fine-tuned with medical domain data based on the LLM model PaLM2 44 , outperformed not only the regular GPT-4 in prediction accuracy by approximately 10% but also exceeded the performance of clinicians 45 .Second, we may still want to utilize HPO or other ontologies as intermediaries in the era of Large Language Models.Even though machines can now understand unstructured natural language, they still perform better when processing structured codes and concepts.Alternatively, prompts should be designed to break down the task further when the input is a natural language description, providing better guidance to GPT in achieving the prediction task through a two-step approach.Lastly, a general-purpose LLM is more susceptible to "common attention" bias.This is particularly significant in the context of rare disease diagnosis, as it suggests that general-purpose LLMs like GPTs may be more inclined to make predictions for commonly encountered cases but may not perform as effectively when dealing with rare conditions.Similar conclusions were drawn from the differential diagnosis tasks, where clinicians noted that LLMs were useful for simple cases.However, they had limitations for complex cases, which could not be assessed holistically but rather focused on specific aspects to draw conclusions 45 .</p>
<p>Nevertheless, the future seems promising.When considering model size, we consistently observe a trend where larger models achieve better results across almost all evaluation metrics.The parameter size of GPT-3.5 and GPT-4 is approximately a hundred billion, which is larger than the 7 billion, 13 billion, and 70 billion parameter Llama2 models.While GPT-4 achieved the best performance in all three metrics, Llama2-70b-chat also outperformed its smaller counterparts in those metrics.This implies that in the future, even larger LLMs might have the ability to achieve performance comparable to, or even surpass, traditional knowledge graph-based bioinformatic tools.However, a plateau effect might also exist.Continuing evaluation and benchmark development are required to monitor the development of LLMs and their potential applications in the field of genomic medicine.considering GPT-4 is almost 35 times more expensive than GPT-3.5 (according to the 2010 August billing policy in OpenAI), crafting a more detailed prompt with more tokens for smaller LLMs might provide a more environmentally and economically friendly solution in certain tasks.Unfortunately, in our study, while we found different prompts could impact completeness and structural compliance in opposite ways, with no significant improvements in prediction accuracy were observed.Some previous studies have suggested the prompt influence was less dominant 46,47 for certain tasks.However, we believe that this is still a question that is not easily answered, especially considering that crafting an efficient prompt for LLMs remains an ongoing challenge.In the future, strategic model selection based on specific needs and constraints could be a significant research topic from both economic and computation speed perspectives.While the overall variation observed in the models' performance in this study is within acceptable parameters, the influence of the probabilistic nature of LLMs was still existed.Previous studies have suggested that the variability in results affected performance metrics when classifying functional evidence in biomedical literature 29 .Another study, involving a biomedical semantic question and answer task, which repeated five times with the same model, concluded that despite the presence of variability, its impact was minimal 30 .More investigation are needed to better understand how the variabilities of those LLMs could impact the downstream applications.</p>
<p>One limitation of our study is we exclusively focused on the zero-shot prompt scenario.Studies have shown that expanding GPT to include a more comprehensive range of examples and detailed steps could potentially lead to improvements in its performance. 48,49.For example, transitioning from zero-shot to one-shot learning improved the relation extraction task by around 12% 50 .In another example, a 'chain-of-thoughts' prompt gave the best result for the summarization task compared to zero-shot performance 30 .</p>
<p>Conclusion</p>
<p>In this study, we conducted a comprehensive evaluation of the LLMs for phenotype-driven gene prioritization, a crucial process in rare genetic disorder diagnosis.Even the bestperforming model, GPT-4, still lags behind traditional bioinformatics tools in terms of generating accurate candidate gene prediction results.However, a clear trend of LLM performance increasing with model size is observed.Notably, LLM's ability to process freetext phenotypic descriptions is advantageous, although it may not achieve the same level of robustness as terminology-based input.These findings contribute to the ongoing discussion about integrating advanced LLMs into clinical genomic analysis workflows.
Figure Legend
Figure 1 Overall
1
Figure 1 Overall Design of the GPT Evaluation Study.The prompt engineering process includes the combination of four different prompt templates, two types of phenotype representations, and requests for completing tasks at two difficulty levels.After engineering the prompts, they were input into both GPT-3.5 and GPT-4, each three times independently.The responses generated by GPT under different settings were collected for various outcome evaluations, including task completeness rate, gene prediction accuracy, and adherence to the output structure.</p>
<p>Figure 2
2
Figure 2 Gene prediction accuracy across different models.(A) Overall accuracy rate across different LLM models.(B) Comparison with other bioinformatics tools in predicting top 50 candidate genes in DGD datasets 6 ; (C) Prediction accuracy across different datasets for GPT-4, specifically examining the top 50 gene prediction results.</p>
<p>Figure 3
3
Figure 3 Genes Most Frequently Predicted by LLMs Across All Experiments.The green bar represents the number of times each gene was predicted in the challenging task (top 10), the blue bar shows the number of times each gene was predicted in the easier task (top 50), and the red bar indicates the number of times each gene appeared in the final diagnosed gene pool (i.e., ground truth occurrence).An asterisk (*) denotes genes that never appeared in the final diagnosed gene pool, indicating a high positive potential of LLMs for these genes.</p>
<p>2B further demonstrated the performance among different datasets for top 50 tasks.As the best performed LLM, GPT4 displayed the highest average overall accuracy rate of 29.51% (32.24% among completed) in CSH, closely followed by CUMC, (27.16%, 30.03% among completed).In the context of the top 10 tasks, CUMC outperformed the others with the highest accuracy, with CSH in close pursuit.The performance in DGD was noted at 24.71% (27.21% among completed), while the performance in AJHG was only 0.75% (0.89% among completed), and TAF1 exhibited the lowest rate of 0% (since TAF1 gene was never predicted by GPTs).</p>
<p>Table 1 .
1
Dataset used for GPT evaluation.
Tables# ofSource# of individuals with HPO terms as inputindividuals with narratives asAverage # of HPO (s.d)Average # of tokens (s.d)inputAJHG787211.42 (6.43)187.64 (88.87)CSH724912.83 (8.30)161.43 (102.38)CUIMC27-11.52 (4.73)-DGD85-9.03 (4.06)-TAF114-34.86 (10.71)-Other-4-180.25 (143.98)</p>
<p>Table 2 .
2
Prompt engineering involved for phenotypic-based gene prioritization task.{Phenotype list} can be either a set of HPO-based concept names separated by ";" or a narrative containing phenotype descriptions extracted from the original literature.{top k} can be either "top 10" or "top 50," representing challenge and easy, respectively.Consider you are a genetic counselor.The phenotype description of the patient is {Phenotype list}.Can you suggest a list of {top k} possible genes to test?Please return gene symbols as a comma separated list.Example: "ABC1,BRAC2,BRAC1" or "Not Applicable" if you can not provide the result.The phenotype description of the patient is {Phenotype list}.Can you suggest a list of {top k} possible genes to test?Please use the knowledge you have trained.No need to access to the real-time database to generate outcomes.Also, please return gene symbols as a comma separated list.Example: "ABC1,BRAC2,BRAC1"' or "Not Applicable" if you can not provide the result.Original + Role + Instruction Consider you are a genetic counselor.The phenotype description of the patient is {Phenotype list}.Can you suggest a list of {top k} possible genes to test?Please use the knowledge you have trained.There is no need to access the real-time database to generate outcomes.Also, please return gene symbols as a comma separated list.Example: "ABC1,BRAC2,BRAC1"' or "Not Applicable" if you can not provide the result.
Prompt templatePrompt contentThe phenotype description of the patient is {Phenotype list}. Can you suggestOriginala list of {top k} possible genes to test? Please return gene symbols as a comma separated list. Example: "ABC1,BRAC2,BRAC1" or "Not Applicable" if youcan not provide the result. .Original +RoleOriginal +Instruction</p>
<p>Table 3 . Evaluation results of the LLMs performance in making accuracy gene prediction for difficult tasks (Top 10). The number in the parenthesis is overall accuracy.
3FactorGPT-4GPT-3.5Llama2-7b-chatLlama2-13b-chatLlama2-70b-chatOriginal13.83% (13.63%)7.20% (2.24%)---Original + Role14.51% (14.13%)7.16% (4.49%)---PromptsOriginal + Instruction14.17% (14.13%)10.45% (8.56%)---Original + Role + Instruction13.93% (13.88%)10.21% (10.14%)5.34% (4.66%)7.15% (6.57%)7.46% (6.15%)HPO15.06%9.53%5.41%7.41%8.21%InputConcept(14.89%)(6.19%)(4.71%)(6.64%)(7.13%)TypeFree Text12.00% (11.87%)8.71% (6.73%)5.20% (4.53%)6.61% (6.40%)5.49% (4.00%)</p>
<p>Table 4 . Evaluation results of the LLMs performance in making accuracy gene prediction for easy tasks (Top 50). The number in the parenthesis is overall accuracy.
4FactorGPT-4GPT-3.5Llama2-7b-chatLlama2-13b-chatLlama2-70b-chatOriginal17.87% (15.46%)15.93% (2.41%)---Original + Role20.06% (16.38%)16.09% (10.89%)---PromptsOriginal + Instruction16.35% (15.71%)16.04% (11.31%)---Original + Role + Instruction18.11% (17.04%)17.28% (15.30%)4.75% (2.58%)8.66% (4.66%)7.99% (6.65%)HPO20.67%17.87%5.39%10.88%9.09%InputConcept(18.18%)(10.84%)(3.02%)(5.68%)(7.61%)TypeFree Text12.25% (11.67%)13.44% (8.07%)3.17% (1.60%)4.19% (2.40%)5.52% (4.53%)</p>
<p>Table 5 . Evaluation results of the LLMs performance in completing the task.
5FactorGPT-4GPT-3.5Llama2-7b-chatLlama2-13b-chatLlama2-70b-chatOriginal92.56%23.15%---Original + Role89.53%65.17%---PromptsOriginal + Instruction97.92%76.23%---Original +Role +96.88%93.93%70.70%72.82%82.83%InstructionTaskTop 1098.86%68.79%87.11%91.85%82.46%levelsTop 5089.59%60.45%54.28%53.78%83.21%Input TypeHPO Concept Free Text93.40% 96.03%62.79% 68.67%71.56% 68.80%70.89% 77.07%85.27% 77.47%
AcknowledgementThis study is supported by grant R01HG012655 from the National Human Genome Research Institute (NHGRI).Competing InterestsThe authors declare no competing interests.Supplementary MaterialTableS1Odds ratio calculated for the genes in the final diagnosed pools across different models.The expected count was calculated as the probability of the gene appeared in the final diagnosed pools times the number of total predictions GPTs made, which is the summation of the number of genes predicted in each experiment.The observed count was calculated by counting the number of occurrences of the gene in the prediction.Supplementary Figure 1. Examples Illustrating Two Instances of GPT ResponsesWhere the Task is Incomplete.In the left panel, GPT declines to provide prediction results.In the right panel, GPT fabricates prediction genes as "Gene 1", "Gene 2", and "Gene 3".Supplementary Figure 2. Examples of Two Instances of GPT Responses (Negative and Positive Examples) Indicating the Correct or Incorrect Gene Prediction for an Individual
Phenotype-driven approaches to enhance variant prioritization and diagnosis of rare disease. Job Jacobsen, C Kelly, V Cipriani, 10.1002/humu.24380Hum Mutat. 438Aug 2022</p>
<p>Clinical diagnostics in human genetics with semantic similarity searches in ontologies. S Kohler, M H Schulz, P Krawitz, 10.1016/j.ajhg.2009.09.003Am J Hum Genet. 854Oct 2009</p>
<p>Next-generation diagnostics and disease-gene discovery with the Exomiser. D Smedley, J O Jacobsen, M Jager, 10.1038/nprot.2015.124Nat Protoc. 1012Dec 2015</p>
<p>AMELIE speeds Mendelian diagnosis by matching patient phenotype and genotype to primary literature. J Birgmeier, M Haeussler, C A Deisseroth, 10.1126/scitranslmed.aau9113Sci Transl Med. 12544May 20 2020</p>
<p>Phenolyzer: phenotype-based prioritization of candidate genes for human diseases. H Yang, P N Robinson, K Wang, Nature methods. 1292015</p>
<p>Phen2Gene: rapid phenotype-driven gene prioritization for rare diseases. M Zhao, J M Havrilla, L Fang, 10.1093/nargab/lqaa032NAR Genom Bioinform. 2232Jun 2020</p>
<p>DeepSVP: integration of genotype and phenotype for structural variant prioritization using deep learning. A Althagafi, L Alsubaie, N Kathiresan, 10.1093/bioinformatics/btab859Bioinformatics. Mar. 462022</p>
<p>CADA: phenotype-driven gene prioritization based on a case-enriched knowledge graph. C Peng, S Dieck, A Schmid, 10.1093/nargab/lqab078NAR Genom Bioinform. 3378Sep 2021</p>
<p>A Bibliometric Review of Large Language Models Research from. L Fan, L Li, Z Ma, S Lee, H Yu, L Hemphill, arXiv:2304020202017 to 2023. 2023arXiv preprint</p>
<p>Artificial General Intelligence: Concept, State of the Art, and Future Prospects. B Goertzel, 10.2478/jagi-2014-0001Journal of Artificial General Intelligence. 512014</p>
<p>M Sallam, 10.3390/healthcare11060887ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid Concerns. Healthcare (Basel). Mar 19 202311</p>
<p>ChatGPT for healthcare services: An emerging stage for an innovative perspective. M Javaid, A Haleem, R P Singh, 10.1016/j.tbench.2023.1001052023/02/01/ 2023BenchCouncil Transactions on Benchmarks. 31100105</p>
<p>Using AI-generated suggestions from ChatGPT to optimize clinical decision support. S Liu, A P Wright, B L Patterson, 10.1093/jamia/ocad072J Am Med Inform Assoc. 307Jun 20 2023</p>
<p>The potential impact of ChatGPT in clinical and translational medicine. V W Xue, P Lei, W C Cho, 10.1002/ctm2.1216Clin Transl Med. Mar. 133e12162023</p>
<p>Utilizing ChatGPT-4 for Providing Medical Information on Blepharoplasties to Patients. A Cox, I Seth, Y Xie, D J Hunter-Smith, W M Rozen, 10.1093/asj/sjad096Aesthet Surg J. 438Jul 15 2023</p>
<p>Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health. S Tian, Jin Q Yeganova, L , arXiv:2306100702023arXiv preprint</p>
<p>Evaluation of ChatGPT family of models for biomedical reasoning and classification. S Chen, Y Li, S Lu, arXiv:2304024962023arXiv preprint</p>
<p>Opportunities and challenges for ChatGPT and large language models in biomedicine and health. S Tian, Jin Q Yeganova, L Lai, P Zhu, Q Chen, X , arXiv [Preprint]. 2023cited August 21, 2023</p>
<p>Me LLaMA: Foundation Large Language Models for Medical Applications. Q Xie, Q Chen, A Chen, arXiv:2402127492024arXiv preprint</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Llama 2: Open foundation and fine -tuned chat models. H Touvron, L Martin, K Stone, arXiv:2307092882023arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, arXiv:2302139712023arXiv preprint</p>
<p>Prompt Engineering For ChatGPT: A Quick Guide To Techniques. S Ekin, 10.36227/techrxiv.22683919.v2Tips, And Best Practices. techRxiv. 2023</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatgpt. J White, Q Fu, S Hays, arXiv:2302113822023arXiv preprint</p>
<p>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, 10.1145/3560815ACM Computing Surveys. 5592023</p>
<p>Zero-shot information extraction via chatting with chatgpt. X Wei, X Cui, N Cheng, arXiv:2302102052023arXiv preprint</p>
<p>ChatGPT and consumers: Benefits, pitfalls and future research agenda. J Paul, A Ueno, C Dennis, 2023Wiley Online Library</p>
<p>Gene name errors are widespread in the scientific literature. M Ziemann, Y Eren, A El-Osta, 10.1186/s13059-016-1044-7Genome Biology. 1711772016/08/23 2016</p>
<p>Using GPT-4 Prompts to Determine Whether Articles Contain Functional Evidence Supporting or Refuting Variant Pathogenicity. S J Aronson, K Machini, P Sriraman, arXiv:2312135212023arXiv preprint</p>
<p>Is ChatGPT a Biomedical Expert?--Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks. S Ateia, U Kruschwitz, arXiv:2306161082023arXiv preprint</p>
<p>Comparison of GPT-3.5, GPT-4, and human user performance on a practice ophthalmology written examination. J C Lin, D N Younessi, S S Kurapati, O Y Tang, I U Scott, Eye. 2023</p>
<p>Performance of ChatGPT on USMLE: Potential for AIassisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, PLoS digital health. 22e00001982023</p>
<p>The application of ChatGPT in healthcare progress notes: A commentary from a clinical and research perspective. J Nguyen, C A Pepping, Clinical and Translational Medicine. 1372023</p>
<p>Analysis of large-language model versus human performance for genetics questions. D Duong, B D Solomon, European Journal of Human Genetics. 2023</p>
<p>Evaluation of large language models for discovery of gene set function. M Hu, S Alkhairy, I Lee, arXiv:2309040192023arXiv preprint</p>
<p>S Roy, J Wallat, S S Sundaram, W Nejdl, N Ganguly, Genemask, arXiv:230715933Fast Pretraining of Gene Sequences to Enable Few-Shot Learning. 2023arXiv preprint</p>
<p>Introducing ChatGPT. Web link. O Blog, 2023</p>
<p>. J Achiam, S Adler, S Agarwal, arXiv:2303087742023arXiv preprint</p>
<p>Evaluation of phenotype-driven gene prioritization methods for Mendelian diseases. X Yuan, J Wang, B Dai, Briefings in Bioinformatics. 232192022</p>
<p>Learning to fake it: limited responses and fabricated references provided by ChatGPT for medical questions. J Gravel, D 'amours-Gravel, M Osmanlliu, E , Mayo Clinic Proceedings: Digital Health. 132023</p>
<p>ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports. K Jeblick, B Schachtner, J Dexl, European Radiology. 2023</p>
<p>Fine-tuning large neural language models for biomedical natural language processing. R Tinn, H Cheng, Y Gu, Patterns. 442023</p>
<p>scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data. F Yang, Wang W Wang, F , Nature Machine Intelligence. 4102022</p>
<p>. R Anil, A M Dai, O Firat, arXiv:2305104032023arXiv preprint</p>
<p>Towards accurate differential diagnosis with large language models. D Mcduff, M Schaekermann, T Tu, arXiv:2312001642023arXiv preprint</p>
<p>ChatGPT for phenotypes extraction: one model to rule them all. T Labbé, P Castel, J-M Sanner, M Saleh, 2023</p>
<p>Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking. Q Wang, Z Gao, R Xu, arXiv:2307011372023arXiv preprint</p>
<p>Zero-shot clinical entity recognition using chatgpt. Y Hu, I Ameer, X Zuo, arXiv:2303164162023arXiv preprint</p>
<p>Q Lyu, J Tan, M E Zapadka, arXiv:230309038Translating radiology reports into plain language using chatgpt and gpt-4 with prompt learning: Promising results, limitations, and potential. 2023arXiv preprint</p>
<p>A comprehensive benchmark study on biomedical text generation and mining with ChatGPT. Q Chen, H Sun, H Liu, bioRxiv. 2023:2023.04. 19.537463</p>            </div>
        </div>

    </div>
</body>
</html>
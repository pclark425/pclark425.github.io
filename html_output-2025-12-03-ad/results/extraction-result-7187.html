<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7187 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7187</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7187</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-0729515f62042d1274c131360c33a121df71c856</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0729515f62042d1274c131360c33a121df71c856" target="_blank">Generation from Abstract Meaning Representation using Tree Transducers</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper addresses generating English from the Ab-stract Meaning Representation, consisting of re-entrant graphs whose nodes are concepts and edges are relations, and consists of generating an appropriate spanning tree for the AMR and applying tree-to-string transducers to generate English.</p>
                <p><strong>Paper Abstract:</strong> Language generation from purely semantic representations is a challenging task. This paper addresses generating English from the Ab-stract Meaning Representation (AMR), consisting of re-entrant graphs whose nodes are concepts and edges are relations. The new method is trained statistically from AMR-annotated English and consists of two major steps: (i) generating an appropriate spanning tree for the AMR, and (ii) applying tree-to-string transducers to generate English. The method relies on discriminative learning and an argument realization model to overcome data sparsity. Initial tests on held-out data show good promise despite the complexity of the task. The system is available open-source as part of JAMR at:</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7187.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7187.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TI representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transducer Input representation (TI representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-form serialization of an AMR graph produced by removing re-entrancies (variables) and representing each node as (X concept (REL child) ...), used as the input to a tree-to-string transducer for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>TI representation (LISP-like tree serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each AMR node is converted to a rooted tree node of the form (X Concept (REL1 Subtree1) ... (RELm Subtreem)). Re-entrancies (graph variables) are removed by choosing a spanning tree; child edge labels are lexicographically sorted (sort[]) and represented as labeled child-parent parentheses in a LISP-like string.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical / sequential (tree serialized as token sequence), lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Convert AMR graph to a spanning tree by deleting relations that use AMR variables (removing re-entrancies); then serialize tree as nested parenthesized TI representation with children ordered by lexicographic order of relation labels (LISP-like).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR Annotation Release version 1.0 (LDC2014T12)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (AMR -> English)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>tree-to-string transducer pipeline with cdec decoder + 5-gram KenLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline: (1) convert AMR -> TI tree; (2) decode TI tree with a one-state extended linear non-deleting tree-to-string (1-xRLNs) transducer whose rule scores are combined with a 5-gram language model in a discriminative linear model; decoder implemented with cdec; feature weights tuned with MERT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (uncased; single-reference on main test, 4-reference on MT09)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Full system: BLEU 22.1 (test, single ref), 21.2 (MT09, 4 refs). Ablation: without synthetic rules BLEU 9.1 (test) and 7.8 (MT09).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables use of tree-transducer machinery and standard decoders/LMs; but conversion is lossy (re-entrancies removed) and requires learning rules from limited data, motivating synthetic rule generation to overcome sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy conversion: re-entrancies (coreference/re-use) are deleted, so representation cannot encode all graph structures; output constrained to projective reorderings of tree leaves (cannot realize non-projective graph orderings); results sensitive to chosen spanning-tree heuristic; dataset sparsity requires synthetic/generalization rules.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared implicitly to graph-native approaches in related work (e.g., synchronous grammars / SHRG); authors treat TI as a practical baseline enabling application of tree-to-string transducers and decoders used in SMT, trading off expressive completeness for applicability of mature transducer/decoder techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generation from Abstract Meaning Representation using Tree Transducers', 'publication_date_yy_mm': '2016-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7187.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7187.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spanning-tree BFS ordering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Breadth-first-search spanning-tree with lexicographic child-ordering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Heuristic used to turn an AMR graph into a deterministic tree by BFS traversal visiting child nodes in lexicographic order of relation labels (with inverse relations visited last); edges visited form the spanning tree used to produce the TI representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>BFS lexicographic spanning-tree linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A deterministic spanning-tree is produced by breadth-first-search (BFS) on the AMR graph, visiting child edges in lexicographic order of relation labels (inverse relations last); the selected edges are used to produce the TI tree which is then serialized.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential / hierarchical (tree derived via traversal), lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Breadth-first traversal of the AMR graph with child nodes visited in lexicographic order of edge labels; inverse relations visited last. Traversed edges define the tree; tree serialized into TI representation.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR Annotation Release version 1.0 (LDC2014T12)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (AMR -> English)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>used within tree-to-string transducer pipeline (cdec + 5-gram LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Spanning-tree is a preprocessing step feeding the 1-xRLNs tree-to-string transducer; decoder and scoring as in the full pipeline (discriminative linear model combining rule features and LM log-probability).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (system-level evaluation reported for full pipeline; no separate metric for spanning-tree choice)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides a deterministic, simple baseline ordering that makes rule extraction and decoding tractable; choice of spanning tree affects generation because transducer outputs are limited to projective reorderings of tree leaves.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Heuristic baseline only — can substantially affect surface realization; may fail to capture desirable re-entrancy-based realizations; authors note it 'could have a big effect' and that BFS is a simple baseline that can be improved.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Not compared quantitatively to alternative linearizations in this work; authors note potential to improve spanning-tree selection in future work and relate approach to SMT-style reordering constraints (projective limits).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generation from Abstract Meaning Representation using Tree Transducers', 'publication_date_yy_mm': '2016-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7187.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7187.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>1-xRLNs transducer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-state extended linear non-deleting tree-to-string transducer (1-xRLNs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-to-string transducer formalism (rules mapping input tree fragments to output strings with argument variables) used to convert the TI representation into target sentences; rule extraction and scoring are learned from aligned AMR–sentence pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training tree transducers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>1-xRLNs tree-to-string transducer encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transducer rules have an LHS tree fragment (internal nodes labeled by nonterminals, frontier nodes by input concept or variables) and an RHS string over terminals and variables; mapping phi assigns variables to nonterminals; purely lexical rules contain no variables; derivations substitute subderivations into variables to produce final output strings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based rule-driven transformation (hierarchical → sequential), lossy in practice due to upstream tree conversion</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Apply extracted transducer rules (basic, synthetic, abstract, handwritten) to the TI tree via a decoder performing weighted intersection of transducer with language model; decoding performed approximately by cdec to find highest-scoring derivation.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR Annotation Release version 1.0 (LDC2014T12)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (AMR -> English)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tree-to-string transducer with discriminative scoring + cdec decoder; language model: KenLM 5-gram</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transducer uses rules extracted from AMR–sentence alignments (JAMR aligner); rule scores combined in a linear discriminative model with an LM log-probability term; weights tuned with MERT; synthetic rule model trained with AdaGrad perceptron to generalize rules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Full system BLEU 22.1 (test), 21.2 (MT09); ablation shows synthetic-rule component critical: removing synthetic rules lowers BLEU to 9.1 (test) and 7.8 (MT09).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Using transducer rules extracted and generalized (synthetic/abstract) allowed learning from limited AMR data; synthetic-rule modeling greatly improved empirical generation performance, indicating the representation + rule-generalization pipeline effectively combats sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Transducer-based approach relies on upstream tree conversion (loss of re-entrancies); limited by rule coverage from small training set if no generalization; constrained to projective output reorderings of tree leaves; decoding is approximate and depends on feature engineering and MERT tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Authors relate this to SMT-style translation (phrase/table extraction, reordering) and other grammar-based NLG (SHRG, CCG, LFG); they show their transducer + synthetic rules approach performs reasonably as a baseline for AMR generation but do not directly compare numeric performance to alternative graph-native generation formalisms within the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generation from Abstract Meaning Representation using Tree Transducers', 'publication_date_yy_mm': '2016-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Abstract meaning representation for sembanking <em>(Rating: 2)</em></li>
                <li>Training tree transducers <em>(Rating: 2)</em></li>
                <li>Statistical syntax-directed translation with extended domain of locality <em>(Rating: 2)</em></li>
                <li>A discriminative graph-based parser for the abstract meaning representation <em>(Rating: 2)</em></li>
                <li>cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7187",
    "paper_id": "paper-0729515f62042d1274c131360c33a121df71c856",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "TI representation",
            "name_full": "Transducer Input representation (TI representation)",
            "brief_description": "A tree-form serialization of an AMR graph produced by removing re-entrancies (variables) and representing each node as (X concept (REL child) ...), used as the input to a tree-to-string transducer for generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "TI representation (LISP-like tree serialization)",
            "representation_description": "Each AMR node is converted to a rooted tree node of the form (X Concept (REL1 Subtree1) ... (RELm Subtreem)). Re-entrancies (graph variables) are removed by choosing a spanning tree; child edge labels are lexicographically sorted (sort[]) and represented as labeled child-parent parentheses in a LISP-like string.",
            "representation_type": "hierarchical / sequential (tree serialized as token sequence), lossy",
            "encoding_method": "Convert AMR graph to a spanning tree by deleting relations that use AMR variables (removing re-entrancies); then serialize tree as nested parenthesized TI representation with children ordered by lexicographic order of relation labels (LISP-like).",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "AMR Annotation Release version 1.0 (LDC2014T12)",
            "task_name": "graph-to-text generation (AMR -&gt; English)",
            "model_name": "tree-to-string transducer pipeline with cdec decoder + 5-gram KenLM",
            "model_description": "Pipeline: (1) convert AMR -&gt; TI tree; (2) decode TI tree with a one-state extended linear non-deleting tree-to-string (1-xRLNs) transducer whose rule scores are combined with a 5-gram language model in a discriminative linear model; decoder implemented with cdec; feature weights tuned with MERT.",
            "performance_metric": "BLEU (uncased; single-reference on main test, 4-reference on MT09)",
            "performance_value": "Full system: BLEU 22.1 (test, single ref), 21.2 (MT09, 4 refs). Ablation: without synthetic rules BLEU 9.1 (test) and 7.8 (MT09).",
            "impact_on_training": "Enables use of tree-transducer machinery and standard decoders/LMs; but conversion is lossy (re-entrancies removed) and requires learning rules from limited data, motivating synthetic rule generation to overcome sparsity.",
            "limitations": "Lossy conversion: re-entrancies (coreference/re-use) are deleted, so representation cannot encode all graph structures; output constrained to projective reorderings of tree leaves (cannot realize non-projective graph orderings); results sensitive to chosen spanning-tree heuristic; dataset sparsity requires synthetic/generalization rules.",
            "comparison_with_other": "Compared implicitly to graph-native approaches in related work (e.g., synchronous grammars / SHRG); authors treat TI as a practical baseline enabling application of tree-to-string transducers and decoders used in SMT, trading off expressive completeness for applicability of mature transducer/decoder techniques.",
            "uuid": "e7187.0",
            "source_info": {
                "paper_title": "Generation from Abstract Meaning Representation using Tree Transducers",
                "publication_date_yy_mm": "2016-06"
            }
        },
        {
            "name_short": "Spanning-tree BFS ordering",
            "name_full": "Breadth-first-search spanning-tree with lexicographic child-ordering",
            "brief_description": "Heuristic used to turn an AMR graph into a deterministic tree by BFS traversal visiting child nodes in lexicographic order of relation labels (with inverse relations visited last); edges visited form the spanning tree used to produce the TI representation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "BFS lexicographic spanning-tree linearization",
            "representation_description": "A deterministic spanning-tree is produced by breadth-first-search (BFS) on the AMR graph, visiting child edges in lexicographic order of relation labels (inverse relations last); the selected edges are used to produce the TI tree which is then serialized.",
            "representation_type": "sequential / hierarchical (tree derived via traversal), lossy",
            "encoding_method": "Breadth-first traversal of the AMR graph with child nodes visited in lexicographic order of edge labels; inverse relations visited last. Traversed edges define the tree; tree serialized into TI representation.",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "AMR Annotation Release version 1.0 (LDC2014T12)",
            "task_name": "graph-to-text generation (AMR -&gt; English)",
            "model_name": "used within tree-to-string transducer pipeline (cdec + 5-gram LM)",
            "model_description": "Spanning-tree is a preprocessing step feeding the 1-xRLNs tree-to-string transducer; decoder and scoring as in the full pipeline (discriminative linear model combining rule features and LM log-probability).",
            "performance_metric": "BLEU (system-level evaluation reported for full pipeline; no separate metric for spanning-tree choice)",
            "performance_value": null,
            "impact_on_training": "Provides a deterministic, simple baseline ordering that makes rule extraction and decoding tractable; choice of spanning tree affects generation because transducer outputs are limited to projective reorderings of tree leaves.",
            "limitations": "Heuristic baseline only — can substantially affect surface realization; may fail to capture desirable re-entrancy-based realizations; authors note it 'could have a big effect' and that BFS is a simple baseline that can be improved.",
            "comparison_with_other": "Not compared quantitatively to alternative linearizations in this work; authors note potential to improve spanning-tree selection in future work and relate approach to SMT-style reordering constraints (projective limits).",
            "uuid": "e7187.1",
            "source_info": {
                "paper_title": "Generation from Abstract Meaning Representation using Tree Transducers",
                "publication_date_yy_mm": "2016-06"
            }
        },
        {
            "name_short": "1-xRLNs transducer",
            "name_full": "One-state extended linear non-deleting tree-to-string transducer (1-xRLNs)",
            "brief_description": "A tree-to-string transducer formalism (rules mapping input tree fragments to output strings with argument variables) used to convert the TI representation into target sentences; rule extraction and scoring are learned from aligned AMR–sentence pairs.",
            "citation_title": "Training tree transducers",
            "mention_or_use": "use",
            "representation_name": "1-xRLNs tree-to-string transducer encoding",
            "representation_description": "Transducer rules have an LHS tree fragment (internal nodes labeled by nonterminals, frontier nodes by input concept or variables) and an RHS string over terminals and variables; mapping phi assigns variables to nonterminals; purely lexical rules contain no variables; derivations substitute subderivations into variables to produce final output strings.",
            "representation_type": "token-based rule-driven transformation (hierarchical → sequential), lossy in practice due to upstream tree conversion",
            "encoding_method": "Apply extracted transducer rules (basic, synthetic, abstract, handwritten) to the TI tree via a decoder performing weighted intersection of transducer with language model; decoding performed approximately by cdec to find highest-scoring derivation.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR Annotation Release version 1.0 (LDC2014T12)",
            "task_name": "graph-to-text generation (AMR -&gt; English)",
            "model_name": "Tree-to-string transducer with discriminative scoring + cdec decoder; language model: KenLM 5-gram",
            "model_description": "Transducer uses rules extracted from AMR–sentence alignments (JAMR aligner); rule scores combined in a linear discriminative model with an LM log-probability term; weights tuned with MERT; synthetic rule model trained with AdaGrad perceptron to generalize rules.",
            "performance_metric": "BLEU",
            "performance_value": "Full system BLEU 22.1 (test), 21.2 (MT09); ablation shows synthetic-rule component critical: removing synthetic rules lowers BLEU to 9.1 (test) and 7.8 (MT09).",
            "impact_on_training": "Using transducer rules extracted and generalized (synthetic/abstract) allowed learning from limited AMR data; synthetic-rule modeling greatly improved empirical generation performance, indicating the representation + rule-generalization pipeline effectively combats sparsity.",
            "limitations": "Transducer-based approach relies on upstream tree conversion (loss of re-entrancies); limited by rule coverage from small training set if no generalization; constrained to projective output reorderings of tree leaves; decoding is approximate and depends on feature engineering and MERT tuning.",
            "comparison_with_other": "Authors relate this to SMT-style translation (phrase/table extraction, reordering) and other grammar-based NLG (SHRG, CCG, LFG); they show their transducer + synthetic rules approach performs reasonably as a baseline for AMR generation but do not directly compare numeric performance to alternative graph-native generation formalisms within the paper.",
            "uuid": "e7187.2",
            "source_info": {
                "paper_title": "Generation from Abstract Meaning Representation using Tree Transducers",
                "publication_date_yy_mm": "2016-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Abstract meaning representation for sembanking",
            "rating": 2
        },
        {
            "paper_title": "Training tree transducers",
            "rating": 2
        },
        {
            "paper_title": "Statistical syntax-directed translation with extended domain of locality",
            "rating": 2
        },
        {
            "paper_title": "A discriminative graph-based parser for the abstract meaning representation",
            "rating": 2
        },
        {
            "paper_title": "cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models",
            "rating": 1
        }
    ],
    "cost": 0.010617249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Generation from Abstract Meaning Representation using Tree Transducers</h1>
<p>Jeffrey Flanigan ${ }^{\text {® }}$ Chris Dyer ${ }^{\text {® }}$ Noah A. Smith ${ }^{\text {® }}$ Jaime Carbonell ${ }^{\text {® }}$<br>${ }^{1}$ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA<br>${ }^{\text {® }}$ Computer Science \&amp; Engineering, University of Washington, Seattle, WA, USA<br>{jflanigan,cdyer,jgc}@cs.cmu.edu, nasmith@cs.washington.edu</p>
<h4>Abstract</h4>
<p>Language generation from purely semantic representations is a challenging task. This paper addresses generating English from the Abstract Meaning Representation (AMR), consisting of re-entrant graphs whose nodes are concepts and edges are relations. The new method is trained statistically from AMRannotated English and consists of two major steps: (i) generating an appropriate spanning tree for the AMR, and (ii) applying tree-tostring transducers to generate English. The method relies on discriminative learning and an argument realization model to overcome data sparsity. Initial tests on held-out data show good promise despite the complexity of the task. The system is available open-source as part of JAMR at: http://github.com/jflanigan/jamr</p>
<h2>1 Introduction</h2>
<p>We consider natural language generation from the Abstract Meaning Representation (AMR; Banarescu et al., 2013). AMR encodes the meaning of a sentence as a rooted, directed, acyclic graph, where concepts are nodes, and edges are relationships among the concepts.</p>
<p>Because AMR models propositional meaning ${ }^{1}$ while abstracting away from surface syntactic realizations, and is designed with human annotation in mind, it suggests a separation of (i) engineering the application-specific propositions that need to be</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>communicated about from (ii) general-purpose realization details, modeled by a generator shareable across many applications. The latter is our focus here.</p>
<p>Because any AMR graph has numerous valid realizations, and leaves underspecified many important details-including tense, number, definiteness, whether a concept should be referred to nominally or verbally, and more-transforming an AMR graph into an English sentence is a nontrivial problem.</p>
<p>To our knowledge, our system is the first for generating English from AMR. The approach is a statistical natural language generation (NLG) system, trained discriminatively using sentences in the AMR bank (Banarescu et al., 2013). It first transforms the graph into a tree, then decodes into a string using a weighted tree-to-string transducer and a language model (Graehl and Knight, 2004). The decoder bears a strong similarity to state-of-the-art machine translation systems (Koehn et al., 2007; Dyer et al., 2010), but with a rule extraction approach tailored to the NLG problem.</p>
<h2>2 Overview</h2>
<p>Generation of English from AMR graphs is accomplished as follows: the input graph is converted to a tree, which is input into the weighted intersection of a tree-to-string transducer ( $\S 4$ ) with a language model. The output English sentence is the (approximately) highest-scoring sentence according to a feature-rich discriminatively trained linear model. After discussing notation (§3), we describe our approach in $\S 4$. The transducer's rules are extracted from the limited AMR corpus and learned general-</p>
<p>izations; they are of four types: basic rules ( $\S 5$ ), synthetic rules created using a specialized model (§6), abstract rules (§7), and a small number of handwritten rules (§8).</p>
<h2>3 Notation and Definitions</h2>
<p>AMR graphs are directed, weakly connected graphs with node labels from the set of concepts $L_{N}$ and edge labels from the set of relations $L_{E}$.</p>
<p>AMR graphs are transformed to eliminate cycles (details in $\S 4$ ); we refer to the resulting tree as a transducer input representation (TI representation). For a node $n$ with label $\mathcal{C}$ and outgoing edges $n \xrightarrow{L_{1}} n_{1}, \ldots, n \xrightarrow{L_{m}} n_{m}$ sorted lexicographically by $L_{i}$ (each an element of $L_{E}$ ), the TI representation of the tree rooted at $n$ is denoted: ${ }^{2}$</p>
<p>$$
\left(X \mathcal{C}\left(L_{1} T_{1}\right) \ldots\left(L_{m} T_{m}\right)\right)
$$</p>
<p>where each $T_{i}$ is the TI representation of the tree rooted at $n_{i}$. See Fig. 1 for an example. A LISP-like textual formatting of the TI representation in Fig. 1 is:</p>
<p>$$
\begin{aligned}
&amp; \text { ( } X \text { want-01 (ARGO }(X \text { boy }))(\text { ARG1 }(X \text { ride-01 } \
&amp; \quad(\text { ARG0 }(X \text { bicycle }(\bmod (X \text { red })))))))
\end{aligned}
$$</p>
<p>To ease notation, we use the function sort[] to lexicographically sort edge labels in a TI representation. Using this function, an equivalent way of representing the TI representation in Eq. 1, if the $L_{i}$ are unsorted, is:</p>
<p>$$
\left(\mathrm{X} \mathcal{C} \operatorname{sort}\left[\left(L_{1} T_{1}\right) \ldots\left(L_{m} T_{m}\right)\right]\right)
$$</p>
<p>The TI representation is converted into a word sequence using a tree-to-string transducer. The tree transducer formalism we use is one-state extended linear, non-deleting tree-to-string (1-xRLNs) transducers (Huang et al., 2006; Graehl and Knight, 2004). ${ }^{3}$</p>
<p>Definition 1. (From Huang et al., 2006.) A $\boldsymbol{1}$ $x$ RLNs transducer is a tuple $(N, \Sigma, W, \mathcal{R})$ where $N$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>The boy wants to ride the red bicycle .
Figure 1: The generation pipeline. An AMR graph (top), with a deleted re-entrancy (dashed), is converted into a transducer input representation (TI representation, middle), which is transduced to a string using a tree-to-string transducer (bottom).
is the set of nonterminals (relation labels and $X$ ), $\Sigma$ is the input alphabet (concept labels), $W$ is the output alphabet (words), and $\mathcal{R}$ is the set of rules. $A$ rule in $\mathcal{R}$ is a tuple $(t, s, \phi)$ where:</p>
<ol>
<li>
<p>$t$ is the LHS tree, whose internal nodes are labeled by nonterminal symbols, and whose frontier nodes are labeled terminals from $\Sigma$ or variables from a set $\mathcal{X}=\left{X_{1}, X_{2}, \ldots\right}$</p>
</li>
<li>
<p>$s \in(\mathcal{X} \cup W)^{*}$ is the RHS string;</p>
</li>
<li>$\phi$ is a mapping from $\mathcal{X}$ to nonterminals $N$.</li>
</ol>
<p>A rule is a purely lexical rule if it has no variables.
As an example, the tree-to-string transducer rules which produce the output sentence from the TI representation in Fig. 1 are:</p>
<p>$$
\begin{gathered}
\left(X \text { want-01 }\left(\text { ARG0 } X_{1}\right)\left(\text { ARG1 } X_{2}\right)\right) \rightarrow \
\text { The } X_{1} \text { wants to } X_{2} . \
\left(X \text { ride-01 }\left(\text { ARG1 } X_{1}\right)\right) \rightarrow \text { ride the } X_{1} \
\left(X \text { bicycle }\left(\bmod X_{1}\right)\right) \rightarrow X_{1} \text { bicycle } \
(X \text { red }) \rightarrow \text { red } \
(X \text { boy }) \rightarrow \text { boy }
\end{gathered}
$$</p>
<p>Here, all $X_{i}$ are mapped by a trivial $\phi$ to the nonterminal $X$.</p>
<p>The output string of the transducer is the target projection of the derivation, defined as follows:
Definition 2. (From Huang et al., 2006.) A derivation $d$, its source and target projections, denoted $\mathcal{S}(d)$ and $\mathcal{E}(d)$ respectively, are recursively defined as follows:</p>
<ol>
<li>If $r=(t, s, \phi)$ is a purely lexical rule, then $d=r$ is a derivation, where $\mathcal{S}(d)=t$ and $\mathcal{E}(d)=s$</li>
<li>If $r=(t, s, \phi)$ is a rule, and $d_{i}$ is a (sub)derivation with the root symbol of its source projection maching the corresponding substition node in $r$, i.e., $\operatorname{root}\left(\mathcal{S}\left(d_{i}\right)\right)=\phi\left(x_{i}\right)$, then $d=r\left(d_{1}, \ldots, d_{m}\right)$ is also a derivation, where $\mathcal{S}(d)=\left[x_{i} \mapsto \mathcal{S}\left(d_{i}\right)\right] t$ and $\mathcal{E}(d)=\left[x_{i} \mapsto\right.$ $\left.\mathcal{E}\left(d_{i}\right)\right] s$.</li>
</ol>
<p>The notation $\left[x_{i} \mapsto y_{i}\right] t$ is shorthand for the result of substituting $y_{i}$ for each $x_{i}$ in $t$, where $x_{i}$ ranges over all variables in $t$.</p>
<p>The set of all derivations of a target string $e$ with a transducer $T$ is denoted</p>
<p>$$
\mathcal{D}(e, T)={d \mid \mathcal{E}(d)=e}
$$</p>
<p>where $d$ is a derivation in $T$.
We use a shorthand notation for the transducer rules that will be useful when discussing rule extraction and synthetic rules. Let $f_{i}$ be a TI representation. The TI representation has the form</p>
<p>$$
f_{i}=\left(X \mathcal{C}\left(L_{1} T_{1}\right) \ldots\left(L_{m} T_{m}\right)\right)
$$</p>
<p>where $L_{i} \in L_{E}$ and $T_{1}, \ldots, T_{m}$ are TI representations. ${ }^{4}$ Let $A_{1}, \ldots A_{n} \in L_{E}$. We use</p>
<p>$$
\left(f_{i}, A_{1}, \ldots, A_{n}\right) \rightarrow r
$$</p>
<p>as shorthand for the rule:</p>
<p>$$
\begin{gathered}
\left(X \mathcal{C} \operatorname{sort}\left[\left(L_{1} T_{1}\right) \ldots\left(L_{m} T_{m}\right)\right.\right. \
\left.\left.\left(A_{1} X_{1}\right) \ldots\left(A_{n} X_{n}\right)\right]\right) \rightarrow r
\end{gathered}
$$</p>
<p>Note $r$ must contain the variables $X_{1} \ldots X_{n}$. In (3) and (4), argument slots with relation labels $A_{i}$ have been added as children to the root node of the TI representation $f_{i}$.</p>
<p>For example, the shorthand for the transducer rules in (2) is:</p>
<p>$$
\begin{gathered}
((X \text { want-01 }), \text { ARG0, ARG1 }) \rightarrow \
\text { The } X_{1} \text { wants to } X_{2} . \
((X \text { ride-01 }), \text { ARG1 }) \rightarrow \text { ride the } X_{1} \
((X \text { bicycle }), \text { mod }) \rightarrow X_{1} \text { bicycle } \
((X \text { red })) \rightarrow \text { red }
\end{gathered}
$$</p>
<h2>4 Generation</h2>
<p>To generate a sentence $e$ from an input AMR graph $G$, a spanning tree $G^{\prime}$ of $G$ is computed, then transformed into a string using a tree-to-string transducer.</p>
<p>Spanning tree. The choice of the graph $G$ 's spanning tree $G^{\prime}$ could have a big effect on the output, since the transducer's output will always be a projective reordering of the tree's leaves. Our spanning tree results from a breadth-first-search traversal, visiting child nodes in lexicographic order of the relation label (inverse relations are visited last). The edges traversed are included in the tree. This simple heuristic is a baseline which can potentially be improved in future work.</p>
<p>Decoding. Let $T=(N, \Sigma, W, \mathcal{R})$ be a tree-tostring transducer. The output sentence is the highest scoring transduction of $G^{\prime}$ :</p>
<p>$$
e=\mathcal{E}\left(\underset{d \in \mathcal{D}\left(G^{\prime}, T\right)}{\arg \max } \operatorname{score}(d ; \boldsymbol{\theta})\right)
$$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Eq. 6 is solved approximately using the cdec decoder for machine translation (Dyer et al., 2010). The score of the transduction is a linear function (with coefficients $\boldsymbol{\theta}$ ) of a vector of features including the output sequence's language model logprobability and features associated with the rules in the derivation (denoted $\mathbf{f}$; Table 1):</p>
<p>$$
\operatorname{score}(d ; \boldsymbol{\theta})=\theta_{L M} \log \left(p_{L M}(\mathcal{E}(d))\right)+\sum_{r \in d} \boldsymbol{\theta}^{\top} \mathbf{f}(r)
$$</p>
<p>The feature weights are trained on a development dataset using MERT (Och, 2003).</p>
<p>In the next four sections, we describe the rules extracted and generalized from the training corpus.</p>
<h2>5 Inducing Basic Rules</h2>
<p>The basic rules, denoted $\mathcal{R}<em 1="1">{B}$, are extracted from the training AMR data using an algorithm similar to extracting tree transucers from tree-string aligned parallel corpora (Galley et al., 2004). Informally, the rules are extracted from a sentence $\boldsymbol{w}=\left\langle w</em>\right\rangle$ with AMR graph $G$ as follows:}, \ldots, w_{n</p>
<ol>
<li>The AMR graph and the sentence are aligned; we use the JAMR aligner from Flanigan et al. (2014), which aligns non-overlapping subgraphs of the graph to spans of words. The subgraphs that JAMR aligns are called fragments. In JAMR's aligner, all fragments are trees.</li>
<li>$G$ is replaced by its spanning tree by deleting relations that use a variable in the AMR annotation.</li>
<li>In the spanning tree, for each node $i$, we keep track of the word indices $b(i)$ and $e(i)$ in the original sentence that trap all of $i$ 's descendants. (This is calculated using a simple bottom-up propagation from the leaves to the root.)</li>
<li>For each aligned fragment $i$, a rule is extracted by taking the subsequence $\left\langle w_{b(i)} \ldots w_{e(i)}\right\rangle$ and "punching out" the spans of the child nodes (and their descendants) and replacing them with argument slots.</li>
</ol>
<p>See Fig. 2 for examples.
More formally, assume the nodes in $G$ are numbered $1, \ldots, N$ and the fragments are numbered
$1, \ldots, F$. Let nodes $:{1, \ldots, F} \rightarrow 2^{{1, \ldots, N}}$ and root $:{1, \ldots, F} \rightarrow{1, \ldots, N}$ be functions that return the nodes in a fragment and the root of a fragment, respectively, and let children $:{1, \ldots, N} \rightarrow$ $2^{{1, \ldots, N}}$ return the child nodes of a node. We consider a node aligned if it belongs to an aligned fragment. Let the span of an aligned node $i$ be denoted by endpoints $a_{i}$ and $a_{i}^{\prime}$; for unaligned nodes, $a_{i}=\infty$ and $a_{i}^{\prime}=-\infty$ (depicted with superscripts in Fig. 2). The node alignments are propagated by defining $b(\cdot)$ and $e(\cdot)$ recursively, bottom up:</p>
<p>$$
\begin{aligned}
&amp; b(i)=\min \left(a_{j}, \min <em j="j">{j \in \operatorname{children}(i)} b(j)\right) \
&amp; e(i)=\max \left(a</em> e(j)\right)
\end{aligned}
$$}^{\prime}, \max _{j \in \operatorname{children}(i)</p>
<p>Also define functions $\tilde{b}$ and $\tilde{e}$, from fragment indices to integers, as:</p>
<p>$$
\begin{aligned}
&amp; \tilde{b}(i)=b(\operatorname{root}(i)) \
&amp; \tilde{e}(i)=e(\operatorname{root}(i))
\end{aligned}
$$</p>
<p>For fragment $i$, let $C_{i}=\operatorname{children}(\operatorname{root}(i))-$ $\operatorname{nodes}(i)$, which is the children of the fragment's root concept that are not included in the fragment. Let $f_{i}$ be the TI representation for fragment $i .{ }^{5}$ If $C_{i}$ is empty, then the rule extracted for fragment $i$ is:</p>
<p>$$
r_{i}:\left(f_{i}\right) \rightarrow w_{\tilde{b}(i): \tilde{e}(i)}
$$</p>
<p>Otherwise, let $m=\left|C_{i}\right|$, and denote the edge labels from $\operatorname{root}(i)$ to elements of $C_{i}$ as $A_{1}(i) \ldots A_{m}(i)$. For $j \in{1, \ldots, m}$, let $k_{j}$ select the elements $c_{k_{j}}$ of $C_{i}$ in ascending order of $b\left(k_{j}\right)$. Then the rule extracted for fragment $i$ is:</p>
<p>$$
\begin{gathered}
r_{i}:\left(f_{i}, A_{k_{1}}(i), \ldots A_{k_{m}}(i)\right) \rightarrow \
w_{\tilde{b}(i): \tilde{b}\left(k_{1}\right)} X_{1} w_{\tilde{e}\left(k_{1}\right): \tilde{b}\left(k_{2}\right)} X_{2} \ldots \
\ldots w_{\tilde{e}\left(k_{m-1}\right): \tilde{b}\left(k_{m}\right)} X_{m} w_{\tilde{e}\left(k_{m}\right): \tilde{e}(i)}
\end{gathered}
$$</p>
<p>A rule is only extracted if the fragment $i$ is aligned and the child spans do not overlap. Fig. 2 gives an example of a tree annotated with alignments, $b$ and $e$, and the extracted rules.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Rule</td>
<td style="text-align: left;">1 for every rule</td>
</tr>
<tr>
<td style="text-align: left;">Basic</td>
<td style="text-align: left;">1 for basic rules, else 0</td>
</tr>
<tr>
<td style="text-align: left;">Synthetic</td>
<td style="text-align: left;">1 for synthetic rules, else 0</td>
</tr>
<tr>
<td style="text-align: left;">Abstract</td>
<td style="text-align: left;">1 for abstract rules, else 0</td>
</tr>
<tr>
<td style="text-align: left;">Handwritten</td>
<td style="text-align: left;">1 for handwritten rules, else 0</td>
</tr>
<tr>
<td style="text-align: left;">Rule given concept</td>
<td style="text-align: left;">$\log$ (number of times rule extracted / number of times concept observed in training <br> data) (only for basic rules, 0 otherwise)</td>
</tr>
<tr>
<td style="text-align: left;">$\ldots$ without sense</td>
<td style="text-align: left;">same as above, but with sense tags for concepts removed</td>
</tr>
<tr>
<td style="text-align: left;">Synthetic score</td>
<td style="text-align: left;">model score for the synthetic rule (only for synthetic rules, 0 otherwise)</td>
</tr>
<tr>
<td style="text-align: left;">Word count</td>
<td style="text-align: left;">number of words in the rule</td>
</tr>
<tr>
<td style="text-align: left;">Stop word count</td>
<td style="text-align: left;">number of words not in a stop word list</td>
</tr>
<tr>
<td style="text-align: left;">Bad stop word</td>
<td style="text-align: left;">number of words in a list of meaning-changing stop words, such as "all, can, could, <br> only, so, too, until, very"</td>
</tr>
<tr>
<td style="text-align: left;">Negation word</td>
<td style="text-align: left;">number of words in "no, not, n't"</td>
</tr>
</tbody>
</table>
<p>Table 1: Rule features in the transducer. There is also an indicator feature for every handwritten rule.</p>
<h2>6 Modeling Synthetic Rules</h2>
<p>The synthetic rules, denoted $\mathcal{R}<em 1="1">{S}(G)$, are created to generalize the basic rules and overcome data sparseness resulting from our relatively small training dataset. Our synthetic rule model considers an AMR graph $G$ and generates a set of rules for each node in $G$. S synthetic rule's LHS is a TI representation $f$ with argument slots $A</em>$ (this is the same form as the LHS for basic rules). For each node in $G$, one or more LHS are created (we will discuss this further below), and for each LHS, a set of $k$-best synthetic rules are produced. The simplest case of a LHS is just a concept and argument slots corresponding to each of its children.} \ldots A_{m</p>
<p>For a given LHS, the synthetic rule model creates a RHS by concatenating together a string in $W^{<em>}$ (called a concept realization and corresponding to the concept fragment) with strings in $W^{</em>} \mathcal{X} W^{*}$ (called an argument realization and corresponding to the argument slots). See the top of Fig. 3 for a synthetic rule with concept and argument realizations highlighted.</p>
<p>Synthetic rules have the form:</p>
<p>$$
\begin{aligned}
&amp; r:\left(f, A_{1}, \ldots A_{m}\right) \rightarrow \
&amp; \mathbf{l}<em 1="1">{k</em>}} X_{k_{1}} \mathbf{r<em 1="1">{k</em>}} \ldots \mathbf{l<em c="c">{k</em>}} X_{k_{c}} \mathbf{r<em c="c">{k</em> \
&amp; \quad \mathbf{l}}} \mathbf{c<em c_1="c+1">{k</em>}} X_{k_{c+1}} \mathbf{r<em c_1="c+1">{k</em>}} \ldots \mathbf{l<em m="m">{k</em>}} X_{k_{m}} \mathbf{r<em m="m">{k</em>
\end{aligned}
$$}</p>
<p>where:</p>
<ul>
<li>$f$ is a TI representation.</li>
<li>Each $A_{i} \in L_{E}$.</li>
<li>$\left\langle k_{1}, \ldots, k_{m}\right\rangle$ is a permutation of $\langle 1, \ldots, m\rangle$</li>
<li>$\mathbf{c} \in W^{*}$ is the realization of TI representation $f$.</li>
<li>Each $\mathbf{l}<em i="i">{i}, \mathbf{r}</em>} \in W^{*}$ and $X_{i} \in \mathcal{X}$. Let $R_{i}=$ $\left\langle\mathbf{l<em i="i">{i}, \mathbf{r}</em>\right\rangle$ denote the realization of argument $i$.</li>
<li>$c \in[0, m]$ is the position of $\mathbf{c}$ among the realizations of the arguments.</li>
</ul>
<p>Let $\mathcal{F}$ be the space of all possible TI representations. Synthetic rules make use of three lookup tables (which are partial functions) to provide candidate realizations for concepts and arguments: a table for concept realizations lex : $\mathcal{F} \rightarrow 2^{W^{<em>}}$, a table for argument realizations when the argument is on the left left $<em E="E">{\text {lex }}: \mathcal{F} \times L</em> \rightarrow 2^{W^{</em>}}$, and a table for argument realizations when the argument is on the right right $<em E="E">{\text {lex }}: \mathcal{F} \times L</em>$. These tables are constructed during basic rule extraction, the details of which are discussed below .} \rightarrow 2^{W^{*}</p>
<p>Synthetic rules are selected using a linear model with features $\mathbf{g}$ and coefficients $\phi$, which scores each RHS for a given LHS. For LHS $=$ $\left(f, A_{1}, \ldots A_{m}\right)$, the RHS is specified completely by $\mathbf{c}, c, R_{1}, \ldots, R_{m}$ and a permutation $k_{1}, \ldots, k_{m}$. For each node in $G$, and for each TI representation $f$ in the domain of lex that matches the node, a LHS is created, and a set of $K$ synthetic rules is produced for each $\mathbf{c} \in \operatorname{lex}(f)$. The rules produced are the</p>
<p>${ }<em 1="1">{0}$ The ${ }</em>}$ ((boy) ${ <em 3="3">{2}$ wants ${ }</em>}$ to ${ <em 5="5">{4}$ (ride ${ }</em>}$ the ${ <em 7="7">{6}$ ((red) ${ }</em>$ bicycle))) 8
(a) Sentence annotated with indexes, and bracketed according to $b(i)$ and $e(i)$ from the graph in (b).
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) Tree annotated with $a_{i}, a_{i}^{\prime}$ (superscripts) and $b(i), e(i)$ (subscripts).
<img alt="img-2.jpeg" src="img-2.jpeg" />
(c) Extracted rules.</p>
<p>Figure 2: Example rule extraction from an AMRannotated sentence.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Synthetic rule generation for the rule shown at top. In the rule RHS, the realization for ARG0 is blue, the realization for DEST is red, and the realization for ride-01 is black. For a fixed permutation of the concept and arguments, choosing the argument realizations can be seen as a sequence labeling problem (bottom). The highlighted sequence corresponds to the rule at top.</p>
<p>K-best solutions to:</p>
<p>$$
\begin{aligned}
&amp; \underset{c, k_{1} \ldots k_{m}, R_{1}, \ldots, R_{m}}{\arg \max }\left(\sum_{i=1}^{c} \boldsymbol{\psi}^{\top} \mathbf{g}\left(R_{k_{i}}, A_{k_{i}}, \mathbf{c}, i, c\right)\right. \
&amp; +\boldsymbol{\psi}^{\top} \mathbf{g}(\langle\epsilon, \epsilon\rangle, *, \mathbf{c}, c+1, c) \
&amp; \left.+\sum_{i=c+1}^{m} \boldsymbol{\psi}^{\top} \mathbf{g}\left(R_{k_{i}}, A_{k_{i}}, \mathbf{c}, i+1, c\right)\right)
\end{aligned}
$$</p>
<p>where the max is over $c \in 0 \ldots m, k_{1}, \ldots, k_{m}$ is any permutation of $1, \ldots, m$, and $R_{i} \in \operatorname{left}<em i="i">{l e x}\left(A</em>\right)$ for $i<c$ and $R_{i} \in \operatorname{right}_{l e x}\left(A_{i}\right)$ for $i>c . *$ is used to denote the concept position. $\epsilon$ is the empty string.</p>
<p>The best solution to Eq. 10 is found exactly by brute force search over concept position $c \in[0, m+$ $1]$ and the permutation $k_{1}, \ldots, k_{m}$. With fixed concept position and permutation, each $R_{i}$ for the $\arg \max$ is found independently. To obtain the exact $K$-best solutions, we use dynamic programming with a $K$-best semiring (Goodman, 1999) to keep track of the $K$ best sequences for each concept position and permutation, and take the best $K$ sequences over all values of $c$ and $k$.</p>
<p>The synthetic rule model's parameters are estimated using basic rules extracted from the training data. Basic rules are put into the form of Eq. 9 by</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature name</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">POS $+A_{i}+$ "dist"</td>
<td style="text-align: left;">$[c-i]$</td>
</tr>
<tr>
<td style="text-align: left;">POS $+A_{i}+$ side</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">POS $+A_{i}+$ side + "dist"</td>
<td style="text-align: left;">$[c-i]$</td>
</tr>
<tr>
<td style="text-align: left;">POS $+A_{i}+R_{i}+$ side</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{c}+A_{i}+$ "dist"</td>
<td style="text-align: left;">$[c-i]$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{c}+A_{i}+$ side</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{c}+A_{i}+$ side + "dist"</td>
<td style="text-align: left;">$[c-i]$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{c}+\mathrm{POS}+A_{i}+$ side + "dist"</td>
<td style="text-align: left;">$[c-i]$</td>
</tr>
</tbody>
</table>
<p>Table 2: Synthetic rule model features. POS is the most common part-of-speech tag sequence for $\mathbf{c}$, "dist" is the string "dist", and side is "L" if $i&lt;c$, "R" otherwise. + denotes string concatenation.
segmenting the RHS into the form</p>
<p>$$
\mathbf{l}<em 1="1">{1} X</em>} \mathbf{r<em m="m">{1} \ldots \mathbf{c} \ldots \mathbf{l}</em>
$$} X_{m} \mathbf{r}_{m</p>
<p>by choosing $\mathbf{c}, \mathbf{l}<em i="i">{i}, \mathbf{r}</em>$ for $i \in{1, \ldots, m}$. An example segmentation is the rule RHS in Fig. 3.} \in W^{*</p>
<p>Segmenting the RHS of the basic rules into the form of Eq. 11 is done as follows: $\mathbf{c}$ is the aligned span for $f$. For the argument realizations, arguments to the left of $\mathbf{c}$ pick up words to their right, and arguments to the right pick up words to their left. Specifically, for $i<c$ ( $R_{i}$ to the left of $\mathbf{c}$ but not next to c), $\mathbf{l}_{i}$ is empty and $\mathbf{r}_{i}$ contains all words between $a_{i}$ and $a_{i+1}$. For $i=c\left(R_{i}\right.$ directly to the left of $\left.\mathbf{c}\right), \mathbf{l}_{i}$ is empty and $\mathbf{r}_{i}$ contains all words between $a_{c}$ and $\mathbf{c}$. For $i>c+1, \mathbf{l}<em i-1="i-1">{i}$ contains all words between $a</em>}$ and $a_{i}$, and for $i=c+1, \mathbf{l<em i="i">{i}$ contains all words between c and $a</em>$.</p>
<p>The tables for lex, left $<em _lex="{lex" _text="\text">{\text {lex }}$, and right $</em>}}$ are populated using the segmented basic rules. For each basic rule extracted from the training corpus and segmented according to the previous paragraph, $f \rightarrow \mathbf{c}$ is added to lex, and $A_{k_{i}} \rightarrow\left\langle\mathbf{l<em i="i">{i}, \mathbf{r}</em>$ is known during extraction in Eq. 8.}\right\rangle$ is added to left $t_{l e x}$ for $i \leq c$ and right $t_{l e x}$ for $i&gt;c$. The permutation $k_{i</p>
<p>The parameters $\psi$ are trained using AdaGrad (Duchi et al., 2011) with the perceptron loss function (Rosenblatt, 1957; Collins, 2002) for 10 iterations over the basic rules. The features $\mathbf{g}$ are listed in Table 2.</p>
<h2>7 Abstract Rules</h2>
<p>Like the synthetic rules, the abstract rules $\mathcal{R}_{A}(G)$ generalize the basic rules. However, abstract rules</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: center;">Sentences</th>
<th style="text-align: center;">Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: center;">10,000</td>
<td style="text-align: center;">210,000</td>
</tr>
<tr>
<td style="text-align: left;">Dev.</td>
<td style="text-align: center;">1,400</td>
<td style="text-align: center;">29,000</td>
</tr>
<tr>
<td style="text-align: left;">Test</td>
<td style="text-align: center;">1,400</td>
<td style="text-align: center;">30,000</td>
</tr>
<tr>
<td style="text-align: left;">MT09</td>
<td style="text-align: center;">204</td>
<td style="text-align: center;">5,000</td>
</tr>
</tbody>
</table>
<p>Table 3: Train/dev./test/MT09 split.
are much simpler generalizations which use part-of-speech (POS) tags to generalize. Abstract rules make use of a POS abstract rule table, which is a table listing every combination of the POS of the concept realization, the child arguments' labels, and rule RHS with the concept realization removed and replaced with $*$. This table is populated from the basic rules extracted from the training corpus. An example entry in the table is:</p>
<p>$$
\begin{gathered}
(\mathrm{VBD}, \mathrm{ARG} 0, \mathrm{DEST}) \rightarrow \
X_{1}\langle*\rangle \text { to the } X_{2}
\end{gathered}
$$</p>
<p>For the LHS $\left(f, A_{1}, \ldots A_{m}\right)$, an abstract rule is created for each member of $\mathbf{c} \in \operatorname{lex}(f)$ and the most common POS tag $p$ for $\mathbf{c}$ by looking up $p$, $A_{1}, \ldots A_{m}$ in the POS abstract rule table, finding the common RHS, and filling in the concept position with $\mathbf{c}$. The set of all such rules is returned.</p>
<h2>8 Handwritten Rules</h2>
<p>We have handwritten rules for dates, conjunctions, multiple sentences, and the concept have-org-role91. We also create pass-through rules for concepts by removing sense tags and quotes (for string literals).</p>
<h2>9 Experiments</h2>
<p>We evaluate on the AMR Annotation Release version 1.0 (LDC2014T12) dataset. We follow the recommended train/dev./test splits, except that we remove MT09 data (204 sentences) from the training data and use it as another test set. Statistics for this dataset and splits are given in Table 3. We use a 5gram language model trained with KenLM (Heafield et al., 2013) on Gigaword (LDC2011T07), and use 100-best synthetic rules.</p>
<p>We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single ref-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Rules</th>
<th style="text-align: right;">Test</th>
<th style="text-align: right;">MT09</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Full</td>
<td style="text-align: right;">22.1</td>
<td style="text-align: right;">21.2</td>
</tr>
<tr>
<td style="text-align: left;">Full - basic</td>
<td style="text-align: right;">22.1</td>
<td style="text-align: right;">20.9</td>
</tr>
<tr>
<td style="text-align: left;">Full - synthetic</td>
<td style="text-align: right;">9.1</td>
<td style="text-align: right;">7.8</td>
</tr>
<tr>
<td style="text-align: left;">Full - abstract</td>
<td style="text-align: right;">22.0</td>
<td style="text-align: right;">21.2</td>
</tr>
<tr>
<td style="text-align: left;">Full - handwritten</td>
<td style="text-align: right;">21.9</td>
<td style="text-align: right;">20.5</td>
</tr>
</tbody>
</table>
<p>Table 4: Uncased Bleu scores with various types of rules removed from the full system.
erence Bleu for the LCD2014T12 test set, and fourreference Bleu for the MT09 set. We report ablation experiments for different sources of rules. When ablating handwritten rules, we do not ablate passthrough rules.</p>
<p>The full system achieves 22.1 Bleu on the test set, and 21.2 on MT09. Removing the synthetic rules drops the results to 9.1 Bleu on test and 7.8 on MT09. Removing the basic and abstract rules has little impact on the results. This may be because the synthetic rule model already contains much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system.</p>
<h2>10 Related Work</h2>
<p>There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005).</p>
<p>The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013).</p>
<p>Techniques from statistical machine translation have been applied to the problem of NLG (Wong
and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement (SHRG) grammars applied to the GeoQuery corpus (Wong and Mooney, 2006), which in principle could be applied to AMR generation.</p>
<h2>11 Conclusion</h2>
<p>We have presented a two-stage method for natural language generation from AMR, setting a baseline for future work. We have also demonstrated the importance of modeling argument realization for good performance. Our feature-based, tree-transducer approach can be easily extended with rules and features from other sources, allowing future improvements.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank Adam Lopez and Nathan Schneider for valuable feedback, and Sam Thomson and the attendees of the Fred Jelinek Memorial Workshop in 2014 in Prague for helpful discussions. This work is supported by the U.S. Army Research Office under grant number W911NF-10-1-0533. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the U.S. Army Research Office or the United States Government.</p>
<h2>References</h2>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proc. of the 7th Linguistic Annotation Workshop and Interoperability with Discourse.
Anja Belz, Michael White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first surface realisation shared task: Overview and evaluation results. In Proc. of the 13th European Workshop on Natural Language Generation.
Bernd Bohnet, Leo Wanner, Simon Mille, and Alicia Burga. 2010. Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer. In Proc. of COLING.</p>
<p>Aoife Cahill and Josef Van Genabith. 2006. Robust pcfgbased generation using automatically acquired LFG approximations. In Proc. of COLING-ACL.
Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. $J M L R, 12$.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models. In Proc. of ACL.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proc. of ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What's in a translation rule? In Proc. of HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. CL, 25(4).
Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proc. of HLT-NAACL.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modified KneserNey language model estimation. In Proc. of ACL.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef Van Genabith. 2007. Exploiting multi-word units in history-based probabilistic generation. In Proc. of ACL.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. of AMTA.
Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semanticsbased machine translation with hyperedge replacement grammars. In Proc. of COLING.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL.
Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proc. of COLING-ACL.
Irene Langkilde. 2000. Forest-based statistical sentence generation. In Proc. of NAACL 2000.
Hiroko Nakanishi, Yusuke Miyao, and Jun'ichi Tsujii. 2005. Probabilistic models for disambiguation of an HPSG-based chart generator. In Proc. of IWPT.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL.</p>
<p>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. $C L, 31(1)$.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of ACL.
Frank Rosenblatt. 1957. The perceptron-a perceiving and recognizing automaton. Technical Report 85-4601, Cornell Aeronautical Laboratory.
Michael White, Rajakrishnan Rajkumar, and Scott Martin. 2007. Towards broad coverage surface realization with CCG. In Proc. of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation.
Yuk Wah Wong and Raymond J Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proc. of HLT-NAACL.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ I.e., the nodes in fragment $i$, with the edges between them, represented as a TI representation.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
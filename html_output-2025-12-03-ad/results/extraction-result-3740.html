<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3740 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3740</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3740</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-67fa2f2072cca1071ed2c820d6a7f50de6ea2ff3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/67fa2f2072cca1071ed2c820d6a7f50de6ea2ff3" target="_blank">Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces an uncertainty decomposition framework for LLMs, called input clarification ensembling, which can be applied to any pre-trained LLM, and shows that input clarification ensembling provides accurate and reliable uncertainty quantification on several language processing tasks.</p>
                <p><strong>Paper Abstract:</strong> Uncertainty decomposition refers to the task of decomposing the total uncertainty of a predictive model into aleatoric (data) uncertainty, resulting from inherent randomness in the data-generating process, and epistemic (model) uncertainty, resulting from missing information in the model's training data. In large language models (LLMs) specifically, identifying sources of uncertainty is an important step toward improving reliability, trustworthiness, and interpretability, but remains an important open research question. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarification ensembling, which can be applied to any pre-trained LLM. Our approach generates a set of clarifications for the input, feeds them into an LLM, and ensembles the corresponding predictions. We show that, when aleatoric uncertainty arises from ambiguity or under-specification in LLM inputs, this approach makes it possible to factor an (unclarified) LLM's predictions into separate aleatoric and epistemic terms, using a decomposition similar to the one employed by Bayesian neural networks. Empirical evaluations demonstrate that input clarification ensembling provides accurate and reliable uncertainty quantification on several language processing tasks. Code and data are available at https://github.com/UCSB-NLP-Chang/llm_uncertainty.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3740.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3740.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Input Clarification Ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input Clarification Ensembling (uncertainty decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method introduced in this paper that uses an LLM to generate multiple clarifications of an ambiguous input, ensembles the model's outputs across clarified inputs, and decomposes predictive entropy into a mutual-information term (interpreted as aleatoric uncertainty from input ambiguity) and an expected entropy term (interpreted as remaining/model uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613 (default); gpt-4 and Llama-3-8B-Instruct used for clarification in some experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Black-box LLMs used as-is (no parameter changes). Default experiments used gpt-3.5-turbo-0613 to sample multiple answers (10 samples, temperature=0.5) and estimate answer-frequency distributions; clarification generator variants included gpt-4 and a fine-tuned Llama-3-8B-Instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate the model's uncertainty / probability that a generated answer is correct and decompose total uncertainty into aleatoric (input-ambiguity) vs epistemic components; also predict whether an input is ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Generate multiple clarified versions of the input via a clarification LLM q(C|X); for each clarification sample answers from the prediction LLM, cluster semantically equivalent outputs, compute answer-frequency distributions, then compute ensemble predictive entropy H(q(Y|X)) and decompose it into I(Y;C|X) (aleatoric) + E_C[H(q(Y|X⊕C))] (remaining uncertainty). Also solicits direct LLM confidence only as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated on Natural Questions (NQ) for mistake detection, GSM8K for math reasoning, AmbigQA for ambiguous question detection, and a synthetic AmbigInst dataset for ambiguous instruction detection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mistake detection: AUROC/F1 reported. On NQ: AUROC=72.3, F1=80.2 (Ours) vs Ask4CONF AUROC=70.4 F1=83.9, Ensembles* AUROC=69.7 F1=79.7. On GSM8K: AUROC=89.7, F1=94.7 (Ours) vs Ask4CONF AUROC=58.1, F1=92.3, Ensembles* AUROC=88.3 F1=94.6. Ambiguity detection (AmbigQA): Ours (GPT) AUROC=71.7 F1=70.1; Ours (LLaMA finetuned) AUROC=67.1 F1=71.8; with ground-truth clarifications Ours* AUROC=89.8 F1=85.6. Reported additional scalar metrics: average aleatoric entropies for groups (see tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms baselines at ambiguity detection (substantially higher AUROC/F1 than Semantic Entropy, Ensembles*, Ask4CONF-D and sampling-based baselines). For mistake detection (total uncertainty), performance is comparable or slightly superior to Ensembles* and Ask4CONF depending on dataset: e.g., on GSM8K Ours gives higher AUROC than Ask4CONF (89.7 vs 58.1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on the assumption that input ambiguity is the dominant source of aleatoric uncertainty; quality depends on the clarification LLM and its calibration; clustering of semantically equivalent outputs is an extra step (they used LLM-based clustering); black-box nature prevents using parameter-based Bayesian methods; method does not attempt to forecast future events or scientific discoveries — it estimates uncertainty/confidence about current-answer correctness and input ambiguity only.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Input clarification ensembling provides a principled decomposition of uncertainty for black-box LLMs and yields strong ambiguity-detection performance; clarifying inputs reduces measured aleatoric uncertainty (monotonicity check), and using multiple clarifications increases recall of target answers. The method is more effective than simply varying in-context examples (ENSEMBLES*) for estimating aleatoric uncertainty due to input ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3740.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3740.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ask4CONF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ask4CONF (elicited LLM confidence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method that queries an LLM twice: first to produce an answer, then to directly output its confidence/probability that the answer is correct (prompted probability).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / generic LLM (as implemented in the paper's baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-based elicitation of a scalar confidence probability from the LLM (they implemented the 'Verb. 2S top-1' variant from Tian et al. 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate the probability/confidence that the model's produced answer is correct (per-example).</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Prompt the LLM after answer generation to return a single probability between 0 and 1 representing confidence that its answer is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated on Natural Questions and GSM8K for mistake detection; also adapted for ambiguity detection as Ask4CONF-D.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mistake detection: On Natural Questions Ask4CONF AUROC=70.4, F1=83.9; On GSM8K AUROC=58.1, F1=92.3. Ambiguity detection (Ask4CONF-D): AmbigQA AUROC=55.0 F1=64.3; AmbigInst AUROC=57.9 F1=75.4 (as reported in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Per paper, Ask4CONF performs well on factual QA (NQ) but poorly on complex reasoning (GSM8K) compared to the proposed ensembling approach, which maintains good performance on both.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Directly elicited confidence can be miscalibrated depending on task and model; performs poorly on complex reasoning tasks in their experiments; does not decompose uncertainty into aleatoric vs epistemic.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Ask4CONF is a simple baseline that can give useful calibration on some factual tasks but underperforms on complex reasoning relative to ensemble-based or clarification-based uncertainty measures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3740.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3740.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ask4CONF-D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ask4CONF-D (Ask4CONF adapted for ambiguity detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of Ask4CONF used in this paper that prompts the LLM to output the probability that an input question is ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / gpt-3.5-turbo (prompted for ambiguity probability)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-based elicitation of a scalar probability that the question contains ambiguity (probability between 0 and 1).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate the probability that an input question/instruction is ambiguous (i.e., multiple valid answers or underspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Prompt the LLM with a dedicated ambiguity-probing prompt that requests a single probability score for 'question is ambiguous'.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AmbigQA (ambiguous questions) and AmbigInst (synthetic ambiguous instructions) datasets constructed/used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AmbigQA: AUROC=55.0, F1=64.3 (Table 2). AmbigInst: AUROC=57.9, F1=75.4 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Ask4CONF-D performs poorly relative to the paper's Input Clarification Ensembling (Ours), which achieves much higher AUROC and F1 for ambiguity detection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Direct ambiguity-probability elicitation from LLMs is not sufficiently sensitive to subtle ambiguities and cannot disentangle model vs data uncertainty; therefore it underperforms clarification-ensembling.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Eliciting an LLM's judged ambiguity probability is less effective for ambiguity detection than measuring aleatoric uncertainty via clarification ensembling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3740.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3740.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BSDETECTOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BSDETECTOR (intrinsic + extrinsic confidence assessment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited prior method that combines sampling multiple answers and querying an LLM for its confidence to estimate predictive uncertainty for black-box LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified (method applies to any black-box LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A two-pronged technique: (1) sample multiple answers from an LLM and assess consistency, and (2) directly query the LLM for a confidence score.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate predictive uncertainty/confidence of LLM answers for general QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Combine output-sampling-based consistency measures with direct confidence elicitation from the LLM to form uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Cited in related work; specific benchmarks not reported in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Referenced as an existing approach that estimates total uncertainty but does not decompose aleatoric vs epistemic uncertainty; motivating contrast for the paper's clarification-ensembling decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Per paper's discussion, such methods estimate total uncertainty but do not decompose uncertainty by source; sampling consistency and direct confidence can be insufficient to identify input-ambiguity-driven aleatoric uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Cited as an example of black-box LLM uncertainty estimation that motivated the need for a decomposition method.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3740.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3740.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency / Sampling baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency and sampling-based confidence (SELFCONSISTENCY / SAMPLE REPETITION / SAMPLE DIVERSITY)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines that estimate LLM confidence by sampling multiple answers and using answer frequency, repetition, or diversity as proxies for confidence/ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3 / GPT-style LLMs (sampling-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sampling multiple outputs (e.g., 10 samples at a set temperature) and using the frequency of the top answer (self-consistency), repetition statistics, or count of unique answers as confidence/ambiguity signals.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate probability/confidence of correctness or detect ambiguity using sampling-derived statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Generate multiple independent answers and compute top-answer frequency (confidence), answer repetition, or answer diversity as proxies for probability/confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used as baselines for ambiguity detection on AmbigQA in Appendix A.1; parameters used: temperature=0.5–0.7, 10 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On AmbigQA (additional results Table 3): SELFCONSISTENCY AUROC=56.0, F1=62.5; SAMPLE REPETITION AUROC=58.6, F1=69.0; SAMPLE DIVERSITY AUROC=57.6, F1=66.7.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>These sampling-based methods achieve AUROC under 60 for ambiguity detection and are outperformed substantially by Input Clarification Ensembling (Ours) which achieves AUROC=71.7 (GPT clarifier) and up to 89.8 with ground-truth clarifications.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Sampling-based heuristics conflate epistemic and aleatoric uncertainty and fail to disentangle input ambiguity; they give weak signals for explicit ambiguity detection in the studied datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Sampling-frequency heuristics alone are insufficient for accurate ambiguity detection in the evaluated datasets; ensembling across clarifications yields better separation of aleatoric uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3740.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3740.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo-0613 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The default black-box LLM used by the authors for prediction sampling and total-uncertainty estimation in experiments (10 sampled outputs, temperature=0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 family chat model used as a black-box; authors used it for prediction sampling (10 samples) and for clustering/answer extraction pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Estimate answer distributions (via sampling) to compute entropy-based uncertainty and support the clarification-ensembling pipeline for mistake detection and ambiguity detection.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Sampling multiple outputs and using answer-frequency distributions (with LLM-based clustering) to compute predictive entropy; also used as the prediction LLM in many experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Natural Questions, GSM8K, AmbigQA, AmbigInst (as used throughout experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used within Ours and baselines; reported aggregated metrics above (mistake detection and ambiguity detection) are based on experiments using this model as default.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Serves as the primary prediction model; comparisons reported are between uncertainty-estimation methods applied to gpt-3.5 outputs (e.g., Ask4CONF, Ensembles*, Semantic Entropy, Ours).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Black-box access restricts parameter-level Bayesian approaches; performance for confidence elicitation varies by task (e.g., poorer Ask4CONF performance on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>When combined with input clarification ensembling, gpt-3.5-turbo-0613 yields reliable total uncertainty estimates and strong ambiguity-detection benefits as shown in the paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3740.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3740.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used as a stronger clarification LLM in certain experiments to generate clarifications q(C|X) (retrieving similar examples and prompting gpt-4 for clarifications).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 used as a clarification-generation model in experiments (retrieving 16 nearest in-context examples and prompting to produce clarifications).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Produce high-quality disambiguating clarifications for ambiguous input to reduce aleatoric uncertainty and improve ambiguity detection and answer recall.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Used as q(C|X) to generate clarifications; after clarifications, predictions from the prediction LLM are ensembled and entropies computed (as in input clarification ensembling).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AmbigQA (clarification generation and subsequent ambiguity detection experiments), Natural Questions for clarification prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Clarifier choice impacts Ours' ambiguity detection AUROC: Ours (GPT clarifier) AUROC=71.7; Ours (LLaMA finetuned) AUROC=67.1; with ground truth clarifications Ours* AUROC=89.8.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Using gpt-4 as clarification LLM yields better clarifications than smaller/fine-tuned models in some settings; however, a fine-tuned Llama-3-8B-Instruct also produced useful clarifications with much lower compute.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Clarification quality depends on available in-context examples and retrieval; computational cost and access may be limiting in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>High-quality clarifications (from gpt-4 or ground-truth) strongly improve ambiguity detection and answer-recall coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3740.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3740.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-8B-Instruct (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3-8B-Instruct fine-tuned on AmbigQA (clarifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-weight 8B instruction-tuned LLaMA variant that the authors fine-tuned on AmbigQA to act as an efficient clarification generator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B-Instruct (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-derived 8B instruction model fine-tuned on AmbigQA training clarifications (training used 4× H100 GPUs, batch size 16, LR 2e-5 for 5 epochs); used as a clarification LLM to generate q(C|X).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Generate clarifications to reduce input-induced aleatoric uncertainty for ambiguity detection and improve answer recall.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Fine-tuned clarifier produces multiple clarifications which are ensembled via the prediction LLM to compute entropies and mutual information per Eq. 3.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>AmbigQA (fine-tuning and evaluation), AmbigInst for zero-shot clarifier prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used as the clarifier, Ours (LLaMA) achieved AmbigQA AUROC=67.1 F1=71.8 (Table 2); fine-tuning was done efficiently (authors report <10 minutes on 4×80GB H100).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Fine-tuned Llama-3-8B-Instruct outperforms simple sampling baselines and provides competitive ambiguity-detection performance, although gpt-4 clarifier and ground-truth clarifications can yield higher performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Smaller clarifier quality may be lower than gpt-4 or ground-truth; fine-tuning requires annotated clarifications but is computationally feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>A relatively small fine-tuned clarifier can be effective and efficient for generating clarifications that improve aleatoric uncertainty estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback <em>(Rating: 2)</em></li>
                <li>Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment <em>(Rating: 2)</em></li>
                <li>Language models (mostly) know what they know <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3740",
    "paper_id": "paper-67fa2f2072cca1071ed2c820d6a7f50de6ea2ff3",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [
        {
            "name_short": "Input Clarification Ensembling",
            "name_full": "Input Clarification Ensembling (uncertainty decomposition)",
            "brief_description": "A method introduced in this paper that uses an LLM to generate multiple clarifications of an ambiguous input, ensembles the model's outputs across clarified inputs, and decomposes predictive entropy into a mutual-information term (interpreted as aleatoric uncertainty from input ambiguity) and an expected entropy term (interpreted as remaining/model uncertainty).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613 (default); gpt-4 and Llama-3-8B-Instruct used for clarification in some experiments",
            "model_description": "Black-box LLMs used as-is (no parameter changes). Default experiments used gpt-3.5-turbo-0613 to sample multiple answers (10 samples, temperature=0.5) and estimate answer-frequency distributions; clarification generator variants included gpt-4 and a fine-tuned Llama-3-8B-Instruct.",
            "prediction_task": "Estimate the model's uncertainty / probability that a generated answer is correct and decompose total uncertainty into aleatoric (input-ambiguity) vs epistemic components; also predict whether an input is ambiguous.",
            "method_of_probability_estimation": "Generate multiple clarified versions of the input via a clarification LLM q(C|X); for each clarification sample answers from the prediction LLM, cluster semantically equivalent outputs, compute answer-frequency distributions, then compute ensemble predictive entropy H(q(Y|X)) and decompose it into I(Y;C|X) (aleatoric) + E_C[H(q(Y|X⊕C))] (remaining uncertainty). Also solicits direct LLM confidence only as a baseline.",
            "dataset_or_benchmark": "Evaluated on Natural Questions (NQ) for mistake detection, GSM8K for math reasoning, AmbigQA for ambiguous question detection, and a synthetic AmbigInst dataset for ambiguous instruction detection.",
            "performance_metrics": "Mistake detection: AUROC/F1 reported. On NQ: AUROC=72.3, F1=80.2 (Ours) vs Ask4CONF AUROC=70.4 F1=83.9, Ensembles* AUROC=69.7 F1=79.7. On GSM8K: AUROC=89.7, F1=94.7 (Ours) vs Ask4CONF AUROC=58.1, F1=92.3, Ensembles* AUROC=88.3 F1=94.6. Ambiguity detection (AmbigQA): Ours (GPT) AUROC=71.7 F1=70.1; Ours (LLaMA finetuned) AUROC=67.1 F1=71.8; with ground-truth clarifications Ours* AUROC=89.8 F1=85.6. Reported additional scalar metrics: average aleatoric entropies for groups (see tables).",
            "comparison_to_baselines": "Outperforms baselines at ambiguity detection (substantially higher AUROC/F1 than Semantic Entropy, Ensembles*, Ask4CONF-D and sampling-based baselines). For mistake detection (total uncertainty), performance is comparable or slightly superior to Ensembles* and Ask4CONF depending on dataset: e.g., on GSM8K Ours gives higher AUROC than Ask4CONF (89.7 vs 58.1).",
            "limitations_or_challenges": "Relies on the assumption that input ambiguity is the dominant source of aleatoric uncertainty; quality depends on the clarification LLM and its calibration; clustering of semantically equivalent outputs is an extra step (they used LLM-based clustering); black-box nature prevents using parameter-based Bayesian methods; method does not attempt to forecast future events or scientific discoveries — it estimates uncertainty/confidence about current-answer correctness and input ambiguity only.",
            "notable_findings": "Input clarification ensembling provides a principled decomposition of uncertainty for black-box LLMs and yields strong ambiguity-detection performance; clarifying inputs reduces measured aleatoric uncertainty (monotonicity check), and using multiple clarifications increases recall of target answers. The method is more effective than simply varying in-context examples (ENSEMBLES*) for estimating aleatoric uncertainty due to input ambiguity.",
            "uuid": "e3740.0",
            "source_info": {
                "paper_title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Ask4CONF",
            "name_full": "Ask4CONF (elicited LLM confidence)",
            "brief_description": "A baseline method that queries an LLM twice: first to produce an answer, then to directly output its confidence/probability that the answer is correct (prompted probability).",
            "citation_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "mention_or_use": "use",
            "model_name": "ChatGPT / generic LLM (as implemented in the paper's baselines)",
            "model_description": "Prompt-based elicitation of a scalar confidence probability from the LLM (they implemented the 'Verb. 2S top-1' variant from Tian et al. 2023).",
            "prediction_task": "Estimate the probability/confidence that the model's produced answer is correct (per-example).",
            "method_of_probability_estimation": "Prompt the LLM after answer generation to return a single probability between 0 and 1 representing confidence that its answer is correct.",
            "dataset_or_benchmark": "Evaluated on Natural Questions and GSM8K for mistake detection; also adapted for ambiguity detection as Ask4CONF-D.",
            "performance_metrics": "Mistake detection: On Natural Questions Ask4CONF AUROC=70.4, F1=83.9; On GSM8K AUROC=58.1, F1=92.3. Ambiguity detection (Ask4CONF-D): AmbigQA AUROC=55.0 F1=64.3; AmbigInst AUROC=57.9 F1=75.4 (as reported in paper tables).",
            "comparison_to_baselines": "Per paper, Ask4CONF performs well on factual QA (NQ) but poorly on complex reasoning (GSM8K) compared to the proposed ensembling approach, which maintains good performance on both.",
            "limitations_or_challenges": "Directly elicited confidence can be miscalibrated depending on task and model; performs poorly on complex reasoning tasks in their experiments; does not decompose uncertainty into aleatoric vs epistemic.",
            "notable_findings": "Ask4CONF is a simple baseline that can give useful calibration on some factual tasks but underperforms on complex reasoning relative to ensemble-based or clarification-based uncertainty measures.",
            "uuid": "e3740.1",
            "source_info": {
                "paper_title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Ask4CONF-D",
            "name_full": "Ask4CONF-D (Ask4CONF adapted for ambiguity detection)",
            "brief_description": "A variant of Ask4CONF used in this paper that prompts the LLM to output the probability that an input question is ambiguous.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT / gpt-3.5-turbo (prompted for ambiguity probability)",
            "model_description": "Prompt-based elicitation of a scalar probability that the question contains ambiguity (probability between 0 and 1).",
            "prediction_task": "Estimate the probability that an input question/instruction is ambiguous (i.e., multiple valid answers or underspecified).",
            "method_of_probability_estimation": "Prompt the LLM with a dedicated ambiguity-probing prompt that requests a single probability score for 'question is ambiguous'.",
            "dataset_or_benchmark": "AmbigQA (ambiguous questions) and AmbigInst (synthetic ambiguous instructions) datasets constructed/used in the paper.",
            "performance_metrics": "AmbigQA: AUROC=55.0, F1=64.3 (Table 2). AmbigInst: AUROC=57.9, F1=75.4 (Table 2).",
            "comparison_to_baselines": "Ask4CONF-D performs poorly relative to the paper's Input Clarification Ensembling (Ours), which achieves much higher AUROC and F1 for ambiguity detection.",
            "limitations_or_challenges": "Direct ambiguity-probability elicitation from LLMs is not sufficiently sensitive to subtle ambiguities and cannot disentangle model vs data uncertainty; therefore it underperforms clarification-ensembling.",
            "notable_findings": "Eliciting an LLM's judged ambiguity probability is less effective for ambiguity detection than measuring aleatoric uncertainty via clarification ensembling.",
            "uuid": "e3740.2",
            "source_info": {
                "paper_title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "BSDETECTOR",
            "name_full": "BSDETECTOR (intrinsic + extrinsic confidence assessment)",
            "brief_description": "A cited prior method that combines sampling multiple answers and querying an LLM for its confidence to estimate predictive uncertainty for black-box LLMs.",
            "citation_title": "Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment",
            "mention_or_use": "mention",
            "model_name": "unspecified (method applies to any black-box LLM)",
            "model_description": "A two-pronged technique: (1) sample multiple answers from an LLM and assess consistency, and (2) directly query the LLM for a confidence score.",
            "prediction_task": "Estimate predictive uncertainty/confidence of LLM answers for general QA tasks.",
            "method_of_probability_estimation": "Combine output-sampling-based consistency measures with direct confidence elicitation from the LLM to form uncertainty estimates.",
            "dataset_or_benchmark": "Cited in related work; specific benchmarks not reported in this paper's experiments.",
            "performance_metrics": "Not reported in this paper (cited as related work).",
            "comparison_to_baselines": "Referenced as an existing approach that estimates total uncertainty but does not decompose aleatoric vs epistemic uncertainty; motivating contrast for the paper's clarification-ensembling decomposition.",
            "limitations_or_challenges": "Per paper's discussion, such methods estimate total uncertainty but do not decompose uncertainty by source; sampling consistency and direct confidence can be insufficient to identify input-ambiguity-driven aleatoric uncertainty.",
            "notable_findings": "Cited as an example of black-box LLM uncertainty estimation that motivated the need for a decomposition method.",
            "uuid": "e3740.3",
            "source_info": {
                "paper_title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Self-Consistency / Sampling baselines",
            "name_full": "Self-Consistency and sampling-based confidence (SELFCONSISTENCY / SAMPLE REPETITION / SAMPLE DIVERSITY)",
            "brief_description": "Baselines that estimate LLM confidence by sampling multiple answers and using answer frequency, repetition, or diversity as proxies for confidence/ambiguity.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "gpt-3 / GPT-style LLMs (sampling-based)",
            "model_description": "Sampling multiple outputs (e.g., 10 samples at a set temperature) and using the frequency of the top answer (self-consistency), repetition statistics, or count of unique answers as confidence/ambiguity signals.",
            "prediction_task": "Estimate probability/confidence of correctness or detect ambiguity using sampling-derived statistics.",
            "method_of_probability_estimation": "Generate multiple independent answers and compute top-answer frequency (confidence), answer repetition, or answer diversity as proxies for probability/confidence.",
            "dataset_or_benchmark": "Used as baselines for ambiguity detection on AmbigQA in Appendix A.1; parameters used: temperature=0.5–0.7, 10 samples.",
            "performance_metrics": "On AmbigQA (additional results Table 3): SELFCONSISTENCY AUROC=56.0, F1=62.5; SAMPLE REPETITION AUROC=58.6, F1=69.0; SAMPLE DIVERSITY AUROC=57.6, F1=66.7.",
            "comparison_to_baselines": "These sampling-based methods achieve AUROC under 60 for ambiguity detection and are outperformed substantially by Input Clarification Ensembling (Ours) which achieves AUROC=71.7 (GPT clarifier) and up to 89.8 with ground-truth clarifications.",
            "limitations_or_challenges": "Sampling-based heuristics conflate epistemic and aleatoric uncertainty and fail to disentangle input ambiguity; they give weak signals for explicit ambiguity detection in the studied datasets.",
            "notable_findings": "Sampling-frequency heuristics alone are insufficient for accurate ambiguity detection in the evaluated datasets; ensembling across clarifications yields better separation of aleatoric uncertainty.",
            "uuid": "e3740.4",
            "source_info": {
                "paper_title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "gpt-3.5-turbo-0613",
            "name_full": "gpt-3.5-turbo-0613 (OpenAI)",
            "brief_description": "The default black-box LLM used by the authors for prediction sampling and total-uncertainty estimation in experiments (10 sampled outputs, temperature=0.5).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613",
            "model_description": "OpenAI GPT-3.5 family chat model used as a black-box; authors used it for prediction sampling (10 samples) and for clustering/answer extraction pipelines.",
            "prediction_task": "Estimate answer distributions (via sampling) to compute entropy-based uncertainty and support the clarification-ensembling pipeline for mistake detection and ambiguity detection.",
            "method_of_probability_estimation": "Sampling multiple outputs and using answer-frequency distributions (with LLM-based clustering) to compute predictive entropy; also used as the prediction LLM in many experiments.",
            "dataset_or_benchmark": "Natural Questions, GSM8K, AmbigQA, AmbigInst (as used throughout experiments).",
            "performance_metrics": "Used within Ours and baselines; reported aggregated metrics above (mistake detection and ambiguity detection) are based on experiments using this model as default.",
            "comparison_to_baselines": "Serves as the primary prediction model; comparisons reported are between uncertainty-estimation methods applied to gpt-3.5 outputs (e.g., Ask4CONF, Ensembles*, Semantic Entropy, Ours).",
            "limitations_or_challenges": "Black-box access restricts parameter-level Bayesian approaches; performance for confidence elicitation varies by task (e.g., poorer Ask4CONF performance on GSM8K).",
            "notable_findings": "When combined with input clarification ensembling, gpt-3.5-turbo-0613 yields reliable total uncertainty estimates and strong ambiguity-detection benefits as shown in the paper's tables.",
            "uuid": "e3740.5",
            "source_info": {
                "paper_title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "gpt-4",
            "name_full": "gpt-4 (OpenAI)",
            "brief_description": "Used as a stronger clarification LLM in certain experiments to generate clarifications q(C|X) (retrieving similar examples and prompting gpt-4 for clarifications).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4",
            "model_description": "OpenAI GPT-4 used as a clarification-generation model in experiments (retrieving 16 nearest in-context examples and prompting to produce clarifications).",
            "prediction_task": "Produce high-quality disambiguating clarifications for ambiguous input to reduce aleatoric uncertainty and improve ambiguity detection and answer recall.",
            "method_of_probability_estimation": "Used as q(C|X) to generate clarifications; after clarifications, predictions from the prediction LLM are ensembled and entropies computed (as in input clarification ensembling).",
            "dataset_or_benchmark": "AmbigQA (clarification generation and subsequent ambiguity detection experiments), Natural Questions for clarification prompts.",
            "performance_metrics": "Clarifier choice impacts Ours' ambiguity detection AUROC: Ours (GPT clarifier) AUROC=71.7; Ours (LLaMA finetuned) AUROC=67.1; with ground truth clarifications Ours* AUROC=89.8.",
            "comparison_to_baselines": "Using gpt-4 as clarification LLM yields better clarifications than smaller/fine-tuned models in some settings; however, a fine-tuned Llama-3-8B-Instruct also produced useful clarifications with much lower compute.",
            "limitations_or_challenges": "Clarification quality depends on available in-context examples and retrieval; computational cost and access may be limiting in practice.",
            "notable_findings": "High-quality clarifications (from gpt-4 or ground-truth) strongly improve ambiguity detection and answer-recall coverage.",
            "uuid": "e3740.6",
            "source_info": {
                "paper_title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Llama-3-8B-Instruct (fine-tuned)",
            "name_full": "Llama-3-8B-Instruct fine-tuned on AmbigQA (clarifier)",
            "brief_description": "An open-weight 8B instruction-tuned LLaMA variant that the authors fine-tuned on AmbigQA to act as an efficient clarification generator.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B-Instruct (fine-tuned)",
            "model_description": "LLaMA-derived 8B instruction model fine-tuned on AmbigQA training clarifications (training used 4× H100 GPUs, batch size 16, LR 2e-5 for 5 epochs); used as a clarification LLM to generate q(C|X).",
            "prediction_task": "Generate clarifications to reduce input-induced aleatoric uncertainty for ambiguity detection and improve answer recall.",
            "method_of_probability_estimation": "Fine-tuned clarifier produces multiple clarifications which are ensembled via the prediction LLM to compute entropies and mutual information per Eq. 3.",
            "dataset_or_benchmark": "AmbigQA (fine-tuning and evaluation), AmbigInst for zero-shot clarifier prompting.",
            "performance_metrics": "When used as the clarifier, Ours (LLaMA) achieved AmbigQA AUROC=67.1 F1=71.8 (Table 2); fine-tuning was done efficiently (authors report &lt;10 minutes on 4×80GB H100).",
            "comparison_to_baselines": "Fine-tuned Llama-3-8B-Instruct outperforms simple sampling baselines and provides competitive ambiguity-detection performance, although gpt-4 clarifier and ground-truth clarifications can yield higher performance.",
            "limitations_or_challenges": "Smaller clarifier quality may be lower than gpt-4 or ground-truth; fine-tuning requires annotated clarifications but is computationally feasible.",
            "notable_findings": "A relatively small fine-tuned clarifier can be effective and efficient for generating clarifications that improve aleatoric uncertainty estimation.",
            "uuid": "e3740.7",
            "source_info": {
                "paper_title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment",
            "rating": 2
        },
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 1
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1
        }
    ],
    "cost": 0.0192115,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling</h1>
<p>Bairu Hou ${ }^{1}$ Yujian Liu ${ }^{1}$ Kaizhi Qian ${ }^{2}$ Jacob Andreas ${ }^{3}$ Shiyu Chang ${ }^{1}$ Yang Zhang ${ }^{2}$</p>
<h4>Abstract</h4>
<p>Uncertainty decomposition refers to the task of decomposing the total uncertainty of a predictive model into aleatoric (data) uncertainty, resulting from inherent randomness in the data-generating process, and epistemic (model) uncertainty, resulting from missing information in the model's training data. In large language models (LLMs) specifically, identifying sources of uncertainty is an important step toward improving reliability, trustworthiness, and interpretability, but remains an important open research question. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarification ensembling, which can be applied to any pre-trained LLM. Our approach generates a set of clarifications for the input, feeds them into an LLM, and ensembles the corresponding predictions. We show that, when aleatoric uncertainty arises from ambiguity or under-specification in LLM inputs, this approach makes it possible to factor an (un-clarified) LLM's predictions into separate aleatoric and epistemic terms, using a decomposition similar to the one employed by Bayesian neural networks. Empirical evaluations demonstrate that input clarification ensembling provides accurate and reliable uncertainty quantification on several language processing tasks. Code and data are available at https://github.com/ UCSB-NLP-Chang/llm_uncertainty.</p>
<h2>1. Introduction</h2>
<p>With the widespread application of large language models (LLMs), it is becoming crucial to ensure predictions from LLMs are trustworthy. One critical dimension of trustworthiness is the ability to indicate when generated text is reliable</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and correct, which may be formalized as the problem of uncertainty quantification (UQ). Uncertainty quantification aims to measure the confidence level of neural networks in their predictions (Gal et al., 2016; Bhatt et al., 2021; Hüllermeier \&amp; Waegeman, 2021). A higher uncertainty implies the output of LLMs should be clarified, manually evaluated, or rejected.</p>
<p>Quantifying LLMs' total uncertainty has been the focus of increasing research attention. Existing work observes that LLMs are relatively well-calibrated, especially when predictions are obtained by ensembling multiple reasoning chains (Wang et al., 2022; Huang et al., 2022; Si et al., 2023) or prompts (Jiang et al., 2023), or when LLMs are prompted to directly output their confidence levels (Kadavath et al., 2022; Lin et al., 2022; Tian et al., 2023). Many other methods have been proposed to quantify the uncertainty of LLMs (Lin et al., 2022; Xiao et al., 2022; Kuhn et al., 2022; Lin et al., 2023; Duan et al., 2023; Huang et al., 2023; Park \&amp; Kim, 2023; Ren et al., 2023). Accurate quantification of the uncertainty can be used for various applications, such as out-of-distribution detection and misclassified data detection.</p>
<p>However, measuring uncertainty is just the first step towards understanding uncertainty in LLM predictions. For many applications, it is necessary to distinguish between different types of uncertainty and decompose the source into these types, a problem we refer to as uncertainty decomposition. As discussed more formally below, it is always possible to decompose a predictive model's uncertainty into two components: aleatoric (data) uncertainty and epistemic (model) uncertainty. Epistemic uncertainty arises when correct outputs are predictable in principle, but models lack the knowledge required for prediction. For example, the question What is $2+3$ ? requires the knowledge of algebraic operations. Without such knowledge, the uncertainty will be high. On the other hand, aleatoric uncertainty arises from ambiguity or inherent randomness in the data-generating process itself: in language processing applications, it may result from ambiguous questions (Min et al., 2020; Guo et al., 2021; Kuhn et al., 2023) and unclear task instructions (Tamkin et al., 2022). In particular, an important source of aleatoric uncertainty is the input ambiguity. For example, the answer to</p>
<p>the input question Who is the president of this country will have a high aleatoric uncertainty because it is ambiguous what country and time the question intends to query about.</p>
<p>This paper aims to obtain a finer-grained uncertainty measurement by determining how much of the total uncertainty can be attributed to aleatoric uncertainty due to input ambiguity. Aleatoric uncertainty due to input ambiguity is irreducible no matter how well a model learns. For example, to answer the question Who is the president of this country?, without any context, the uncertainty would be high regardless of how well the LLM learns, because the question itself is ambiguous. Uncertainty decomposition provides important insights for users to improve the performance of LLM. If epistemic uncertainty is high, users could supply the model with adequate knowledge through model adaptation, in-context learning, etc.; if the aleatoric uncertainty is high, then users should modify the query to make it more concrete.</p>
<p>Despite existing work aimed at quantifying total uncertainty in LLMs, decomposing this uncertainty for LLMs remains understudied. Existing methods for uncertainty decomposition in other models cannot be directly applied, due to the black-box nature of LLMs and the prohibitive cost of inference. For example, Bayesian Neural Networks (BNNs) (Neal, 2012; Blundell et al., 2015; Graves, 2011; Louizos &amp; Welling, 2016; Hernández-Lobato &amp; Adams, 2015; Hasenclever et al., 2017; Li et al., 2015) specify a prior distribution over the model parameters and approximate the posterior distribution given the training data. DEEP ENSEMBLES (Lakshminarayanan et al., 2017; Fort et al., 2019) decompose the uncertainty by training different variants of models, e.g., with different random seeds, to with the proper scoring rules (e.g., negative log-likelihood loss for the classification task) in the target task and then ensembling them.</p>
<p>Despite their effectiveness, Bayesian Neural Networks require substantial modifications to the training procedure, while DEEP ENSEMBLES necessitate training multiple variants of LLMs. Both approaches are generally impractical or prohibitively expensive. Given these challenges, we aim to address the following question: How can we effectively quantify and decompose uncertainty in LLMs?</p>
<p>In this paper, we propose a framework for uncertainty decomposition that we call input clarification ensembling. Our approach shares many intuitions and structural similarities with BNN-based approaches, but avoid the need to modify LLM parameters or inference procedures. Our approach is motivated by the observation that, although it is very challenging to modify LLM’s parameters, it is relatively easy to manipulate the input to LLMs. Inspired by this, rather than ensembling different model variants that minimize the epistemic uncertainty, we introduce a set of input</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. The uncertainty quantification frameworks of DEEP ENSEMBLES (upper) and input clarification ensembling (lower).</p>
<p>clarifications which can minimize the aleatoric uncertainty. We then ensemble an LLM’s predictions under different clarifications. Figure 1 shows the general pipeline. For example, for the question Who is the president of this country?, a possible clarification is ‘This country’ refers to the US. By ruling out the aleatoric uncertainty by clarification, we can ascribe the remaining uncertainty of each individual prediction to epistemic uncertainty. Furthermore, by measuring the disagreement of the model predictions under different clarifications, we can gauge the aleatoric uncertainty. Our experiments verify that the proposed method provide accurate uncertainty quantification results on both total uncertainty and its decomposition.</p>
<h2>2. Related Work</h2>
<p>Uncertainty quantification. Uncertainty quantification for machine learning models has been widely studied to quantify the reliability of model predictions (Gal et al., 2016; Gal &amp; Ghahramani, 2016; Malinin &amp; Gales, 2018; Ovadia et al., 2019; Malinin et al., 2020; Lin et al., 2022; Kuhn et al., 2022; Lin et al., 2023; Shen et al., 2023). Uncertainty in model predictions can have numerous causes. Given the total uncertainty in model predictions, one can further decompose it into epistemic uncertainty (due to lack of knowledge in the model) and aleatoric uncertainty (due to the inherent randomness and noise in data).</p>
<p>Depending on how the uncertainty is obtained, existing uncertainty quantification methods can be categorized into intrinsic and extrinsic methods. Intrinsic methods adopt machine learning models to provide an inherent uncertainty estimate, such as Bayesian approaches and ensemble-based approaches (Malinin &amp; Gales, 2018). Bayesian approaches (Blundell et al., 2015; Gal &amp; Ghahramani, 2016;</p>
<p>Teye et al., 2018; Mobiny et al., 2021; Lakshminarayanan et al., 2017; Malinin et al., 2020; He et al., 2020) and ensemble-based approaches (Lakshminarayanan et al., 2017; Fort et al., 2019) can quantify both aleatoric and epistemic uncertainty. In comparison, extrinsic methods quantify the uncertainty in a post-hoc manner using auxiliary models (Kristiadi et al., 2021; Lahlou et al., 2022). Our method belongs to the intrinsic family of methods and is directly motivated by Bayesian neural network approaches.</p>
<p>Uncertainty Quantification and Model Calibration for LLMs With the wide application of LLMs, how to accurately quantify the predictive uncertainty has also drawn attention (Xiao et al., 2022; Lin et al., 2022; Mielke et al., 2022; Zhou et al., 2023; Huang et al., 2023; Duan et al., 2023; Chen \&amp; Mueller, 2023; Ott et al., 2018; Malinin \&amp; Gales, 2020). Semantic Uncertainty (Kuhn et al., 2022) clusters string-valued LLM outputs by synonymy for better uncertainty quantification. Lin et al. (2023) explores uncertainty quantification within the challenging black-box context, where the token generation probability is inaccessible. In this pursuit, BSDETECTOR (Chen \&amp; Mueller, 2023) combines two strategies to estimate the model's predictive uncertainty. The first approach involves sampling multiple answers from the LLM and assessing their consistency, while the second directly queries the LLM for its confidence in the generated answer. Although there have been some explorations in this direction, existing methods can only estimate the total uncertainty. In comparison, we propose a more principled framework that can both quantify the total uncertainty and decompose it into aleatoric uncertainty and epistemic uncertainty, leading to a more fine-grained understanding of LLMs.</p>
<p>Another line of research is model calibration for LLMs. Model calibration is the process of ensuring that the predicted probabilities or confidence scores from a machine learning model align with the true probabilities or likelihoods of events occurring (i.e., the prediction is correct). Well-calibrated model predictions help improve the reliability of uncertainty quantification. Using existing model calibration methods (Hendrycks \&amp; Gimpel, 2016; Guo et al., 2017; Ovadia et al., 2019; Riquelme et al., 2018; Desai \&amp; Durrett, 2020), prior work (Huang et al., 2022; Jiang et al., 2023; 2021; Ye \&amp; Durrett, 2022) has shown that LLMs are relatively well-calibrated on factual QA and complex reasoning tasks when properly prompted. Specifically, Kadavath et al. (2022); Tian et al. (2023) estimate the prediction confidence of LLMs by prompting LLMs to output their confidence of their answers. For complex reasoning tasks, LLMs may output both the reasoning chains and the final answer. To estimate the confidence score, previous approaches (Huang et al., 2022) sample multiple outputs for the input question and use the answer frequency
to indicate the confidence. Researchers further ensemble multiple prompts for better calibration performance (Jiang et al., 2023). Our uncertainty quantification is based on the well-calibrated predictions of LLMs, which lead to a more precise and accurate quantification result.</p>
<p>Modeling Ambiguity with language models Ambiguity is a longstanding issue in the NLP domain, extensively explored in tasks such as syntactic and semantic parsing (Koller et al., 2008), open-domain questionanswering (Min et al., 2020; Cole et al., 2023), conversational question-answering (Guo et al., 2021) and natural language inference (Liu et al., 2023). Prior work, such as AmbigQA (Min et al., 2020) and AmbigEnt (Liu et al., 2023), have identified the widespread ambiguities and established benchmarks with ambiguous inputs. These studies have demonstrated that existing language models lack the capability to effectively recognize and manage ambiguities. Our work models the ambiguity from the perspective of uncertainty quantification, where a high aleatoric uncertainty can indicate the potential existence of input ambiguity. By decomposing the aleatoric uncertainty, we show that it is possible to enhance the ambiguity detection performance of existing LLMs.</p>
<h2>3. Methodology</h2>
<h3>3.1. Notations and Problem Formulation</h3>
<p>Denote by $\boldsymbol{X}$ and $\boldsymbol{Y}$ the input and output target of a given task and $\boldsymbol{\theta}$ as the parameters of an LLM. Denote by $p(\boldsymbol{Y} \mid \boldsymbol{X})$ and $q(\boldsymbol{Y} \mid \boldsymbol{X}, \boldsymbol{\theta})$ the ground-truth and predicted distribution of $\boldsymbol{Y}$ given $\boldsymbol{X}$.</p>
<p>We first introduce three uncertainty concepts. First, the total uncertainty is defined as the entropy of the predicted distribution, i.e., $\mathcal{U}_{\text {total }}=\mathcal{H}(q(\boldsymbol{Y} \mid \boldsymbol{X} ; \boldsymbol{\theta}))$. If the overall uncertainty is high, then it means the LLM has low confidence in its output. The total uncertainty can be further decomposed into two different types of uncertainties.</p>
<p>The first type of uncertainty is referred to as the epistemic uncertainty, which characterizes how well the LLM approaches the ground truth distribution, and thus learns the knowledge therein. For example, to answer 'What is $2+3$ ?', if the LLM were able to learn the true knowledge of the algebraic operation, it would be able to answer with certainty; otherwise, the uncertainty would be high.</p>
<p>The second type of uncertainty is referred to as the aleatoric uncertainty, which characterizes the fundamental uncertainty residing in the ground-truth distribution, and is irreducible no matter how well the LLM learns. For example, to answer 'Who is the president of this country?', even if the LLM were well acquainted with politics, it still could not answer it confidently, because this question is inherently</p>
<p>ambiguous. The data aleatoric is often quantified by the entropy in the ground-truth distribution, i.e., $\mathcal{H}(p(\boldsymbol{Y} \mid \boldsymbol{X}))$.</p>
<p>The goal of this paper is to estimate both the epistemic and aleatoric uncertainties in LLMs.</p>
<h3>3.2. Background: Bayesian Neural Networks and DEEP ENSEMBLES</h3>
<p>The possible solutions to our task is to apply the canonical Bayesian Neural Network (BNN) approach (Blundell et al., 2015; Graves, 2011) or DEEP Ensembles (Lakshminarayanan et al., 2017), which are standard approaches to uncertainty decomposition. Instead of having one set of parameters, BNNs model the parameter distribution of a neural network. With the Bayesian formalism, the posterior distribution can be approximated give the training data via techniques such as variational inference (Blundell et al., 2015; Graves, 2011). Due to the prohibitive computational cost of BNNs, non-Bayesian methods such as DEEP Ensembles are proposed with better scalability. DEEP Ensembles maintain $K$ models, each parameterized as $\boldsymbol{\theta}^{(k)}$. Each of the $k$ models seeks to minimize the training loss, usually the cross entropy loss for classification tasks, which is equivalent to solving the following optimization problem</p>
<p>$$
\min _{\boldsymbol{\theta}} \operatorname{KL}(p(\boldsymbol{Y} \mid \boldsymbol{X}) | q(\boldsymbol{Y} \mid \boldsymbol{X}, \boldsymbol{\theta})))
$$</p>
<p>In DEEP Ensembles, different models have slightly different initialization values and thus the optimized values, $\left{\boldsymbol{\theta}^{(k)}\right}$, are different. Denote the resulting distribution of the model parameters $\theta$ as $p(\boldsymbol{\theta} \mid \mathcal{D})$ (either approximated by BNNs or DEEP Ensembles) where $\mathcal{D}$ is the training dataset. Then the ensembled distribution of BNN can be represented as $q(\boldsymbol{Y} \mid \boldsymbol{X})=\mathbb{E}_{q(\boldsymbol{\theta} \mid \mathcal{D})}[q(\boldsymbol{Y} \mid \boldsymbol{X}, \boldsymbol{\theta})]$. Then we can decompose the predictive uncertainty as</p>
<p>$$
\mathcal{H}(q(\boldsymbol{Y} \mid \boldsymbol{X}))=\underbrace{\mathcal{I}(\boldsymbol{Y} ; \boldsymbol{\theta} \mid \boldsymbol{X})}<em q_boldsymbol_theta="q(\boldsymbol{\theta">{\oplus}+\underbrace{\mathbb{E}</em>
$$} \mid \mathcal{D})} \mathcal{H}(q(\boldsymbol{Y} \mid \boldsymbol{X}, \boldsymbol{\theta}))}_{\Phi</p>
<p>where $\mathcal{I}$ denotes the mutual information under the $q$ distribution. (1) measures the disagreement among the different models; (2) measures the average uncertainty of each individual model. The above equation can be straightforwardly derived from the definition of conditional mutual information. Under certain assumptions, (1) and (2) can approximate the epistemic and aleatoric uncertainties, respectively (Gal et al., 2016). An illustration of the DEEP Ensembles framework is shown in the upper panel of Figure 1.</p>
<p>Here is an intuitive explanation of why this is the case. According to Eq. 1, the goal of each model is to approach the ground-truth distribution, and thus can be viewed as the process of reducing the epistemic uncertainty. Therefore, if the optimization is successful, all the models will learn the true
distribution, i.e., $q(\boldsymbol{Y} \mid \boldsymbol{X}, \boldsymbol{\theta}^{(k)})=p(\boldsymbol{Y} \mid \boldsymbol{X}), \forall k$, which, by definition, results in zero epistemic uncertainty. Meanwhile, (1) will also be zero because all the models produce the same prediction. Thus (1) equals epistemic uncertainty in this case. (2) would also equal the aleatoric uncertainty because the predicted distribution is equal to the true distribution.</p>
<p>On the other hand, if the models fail to learn the true distribution, in which case the epistemic uncertainty will be large, (1) will also be large since different models have different hyperparameter settings and will be stuck in very different local optima.</p>
<h3>3.3. Do BNN and DEEP Ensembles work for LLMs?</h3>
<p>Our goal of decomposing uncertainty for LLMs would be easily achieved if these methods were readily applicable to LLMs. Unfortunately, this is not the case. For BNNs, we need to significantly modify the training method of LLMs. For DEEP Ensembles, the learning process in Eq. 1 is also very challenging for LLMs. Specifically, there are two types of methods for adapting LLMs to a particular task, supervised fine-tuning and prompting/in-context learning. Directly fine-tuning the model according to Eq. 1 is usually infeasible due to the limited access to model parameters and its huge requirement for computation. Even if it is feasible, it would be very time-consuming because it requires finetuning multiple LLMs.</p>
<p>On the other hand, the in-context learning method, though feasible, does not fit into the DEEP Ensembles framework because it does not directly aim to optimize Eq. 1, so the decomposition will be very inaccurate. To demonstrate this, we perform a simple experiment on the AmbigQA (Min et al., 2020) dataset, which contains both ambiguous questions with multiple answers and unambiguous questions. We use the BNN method to decompose the uncertainty of ChatGPT, where the different individual model is derived by providing different in-context examples. If the decomposition method is accurate, we would expect to see that the aleatoric uncertainty for the ambiguous questions is significantly larger than that of the unambiguous ones. However, as shown in Figure 2, the gap between the uncertainties of the two groups of questions is very small. More experiment details can be found in Section 4.</p>
<p>While the BNN and DEEP Ensemble framework do not work for LLMs, it inspires us to design an alternative framework that is almost completely symmetrical to the BNN approach, as discussed in the next subsection.</p>
<h3>3.4. Input Clarification Ensembling</h3>
<p>Although modifying or adapting LLMs is challenging, it is relatively straightforward to modify the input to LLMs. By analogy to the way BNNs and DEEP Ensembles ensemble</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Aleatoric uncertainty distribution on the AmbigQA (Min et al., 2020) dataset using the DEEP ENSEMBLES method. We use kernel density estimation to smooth the frequency distribution histogram. DEEP ENSEMBLES is achieved by ensembling different in-context examples.</p>
<p>Different <em>models</em> that minimize <em>epistemic uncertainty</em> (Eq. 1), can we design a framework that ensembles different <em>inputs</em> to minimize <em>aleatoric uncertainty</em>?</p>
<p>This is the motivation behind our framework, which consists of the following two steps.</p>
<p><strong>Step 1: Input Clarification.</strong> Given an input <em>X</em>, we first generate a set of texts, <em>C<sup>(k)</sup></em>, called <em>clarifications</em>. Each clarification <em>C<sup>(k)</sup></em> seeks to minimize the ambiguity in <em>X</em> (and thus the aleatoric uncertainty) when appended to <em>X</em>. Formally, we denote one clarification result as <em>X</em> ⊕ <em>C<sup>k</sup></em>, where ⊕ denotes concatenation. In the aforementioned example, '<em>Who is the president of this country?</em>', possible clarifications include '<em>This country refers to the US</em>', and many other countries. Since there can be many valid clarifications for the input, {<em>C<sup>(k)</sup></em>} is a set.</p>
<p><strong>Step 2: Ensemble.</strong> We denote the distribution of the aforementioned input clarifications as <em>q</em>(<em>C</em>|<em>X</em>) given a particular input <em>X</em>. Then, we define the predictive model <em>q</em>(<strong>Y</strong>|<strong>X</strong>) as an ensemble of predictions conditional on each clarified input, <em>i.e.</em>, <em>q</em>(<strong>Y</strong>|<strong>X</strong>) = ℝ<sub><em>q</em>(<strong>C</strong>|<strong>X<em>)</sub>[</em>q*(</strong>Y<strong>|</strong>X<strong> ⊕ </strong>C<strong>, </strong>θ<strong>)]. (Model parameters </strong>θ** are kept constant, and thus will be omitted for brevity below.)</p>
<p>We then propose to decompose the uncertainty of the ensembled model as</p>
<p>$$
\mathcal{H}(q(\mathbf{Y} \mid \mathbf{X})) = \underbrace{\mathcal{I}(\mathbf{Y}; \mathbf{C}|\mathbf{X})}<em q_mathbf_C="q(\mathbf{C">{\mathbb{1}^r} + \underbrace{\mathbb{E}</em>
$$}|\mathbf{X})} \mathcal{H}(q(\mathbf{Y} \mid \mathbf{X} \oplus \mathbf{C}))}_{\mathbb{2}^r}. \tag{3</p>
<p>We claim that <strong>①<sup>0</sup></strong>, which computes the mutual information between the model output distribution and the clarifications, can approximate the aleatoric uncertainty caused by input ambiguity. In contrast, <strong>②<sup>0</sup></strong> is the average entropy of the output distribution given different clarifications, representing the model's uncertainty across clarified versions of the input. Assuming that input ambiguity is the sole source of aleatoric uncertainty, we may consider it as an estimate of the epistemic uncertainty for the LLM given the original input. This interpretation, however, diverges from the traditional definition of epistemic uncertainty, and we mainly focus on decomposing the aleatoric uncertainty (<strong>①<sup>0</sup></strong>) in this paper.</p>
<p>By comparing the above process against Eqs. 1 and 2, we can notice the symmetry between our framework and DEEP ENSEMBLES's — DEEP ENSEMBLES seeks to pin down epistemic uncertainty whereas ours aleatoric uncertainty; Eq. 3 takes almost an identical form to Eq. 2 but the corresponding uncertainties are swapped. Figure 1 also shows such symmetry.</p>
<p>Accordingly, the same explanation of why it works applies here. When the input is already very clear, and hence aleatoric uncertainty is low, the clarifications will be identically empty, so <strong>①<sup>0</sup></strong> will approach zero. When the input is very ambiguous, the clarifications will be very different (think about the aforementioned president example), and so would the answers produced with different clarifications. In this case, <strong>①<sup>0</sup></strong> will be very high. On the other hand, <strong>②<sup>0</sup></strong> measures the average uncertainty on <em>clarified</em> input, which rules out most of the aleatoric uncertainty, so the remaining uncertainty can mostly be ascribed to epistemic uncertainty.</p>
<h3>3.5. Input Clarification</h3>
<p>Unlike the conventional neural networks, the input to LLMs usually contains multiple components, including instructions, in-context examples, questions <em>etc</em>. Therefore, we can separately measure the aleatoric uncertainties caused by different input components by clarifying only the corresponding components. For example, to measure the aleatoric uncertainty resulting from ambiguous instructions, we can clarify only the instruction. For the aleatoric uncertainty studied in this work, we will focus on the uncertainty caused by instruction ambiguity and question ambiguity, but the framework is readily generalizable to other input components.</p>
<p>To derive clarifications that approximately minimize the ambiguity in step 1 above, we introduce a clarification LLM, where we provide an instruction and in-context example to guide the LLM to perform adequate clarification. Therefore, the above input clarification distribution <em>q</em>(<strong>C</strong>|<strong>X</strong>) in Equation 3 is the output distribution of the clarification LLM. Note that the clarification LLM can be different from the LLM for prediction. In this work, we propose the following design choices for the clarification LLM to ensure the quality of clarification:</p>
<ul>
<li>
<p>Prompting an LLM with task instructions and in-context examples. We design instructions for the clarification generation task and provide the model (gpt-3.5-turbo and gpt-4) with several in-context examples.</p>
</li>
<li>
<p>Supervised fine-tuning. We can also fine-tune a opensourced language model on datasets that contains the ambiguous inputs and their corresponding clarifications. We fine-tune the Llama-3-8b-instruct model on the training set of the AmbigQA (Min et al., 2020) dataset.</p>
</li>
</ul>
<p>Further implementation details are provided in Section 4 and Appendix A.2. For both design choices, we show that we can easily adapt the LLMs for clarification generation and quantify the uncertainty using our proposed framework.</p>
<h3>3.6. Improving Performance via Soliciting Clarifications</h3>
<p>Our framework not only provides a way of decomposing the uncertainties, but can also enable an interpretable and effective human-LLM interaction experience. Currently, one of the major ways for humans to interact with LLMs is designing appropriate input. However, the input designed by humans may not be clear enough to LLMs, often resulting in undesirable answers given by LLMs. With the proposed input clarification framework, we can design an interaction paradigm that alleviates this problem.</p>
<p>Given an input query, we can first gauge the uncertainties of different input components. If one of the components, say the instruction, contributes to high uncertainty (exceeding a threshold), we can provide feedback to the user that the LLM is not sure about the answer because the instruction is ambiguous, along with several clarification options produced by the clarification LLM for the user to choose from. This would help the user to perform directed improvement of the input query and obtain the desirable answer.</p>
<h2>4. Experiments</h2>
<p>In this section, we conduct empirical evaluations to demonstrate the validity and effectiveness of the proposed method. Specifically, we aims to answer the following two questions:</p>
<ol>
<li>Can the proposed UQ framework quantify total uncertainty effectively and correctly?</li>
<li>Can the proposed UQ framework decompose the uncertainty effectively and correctly?</li>
</ol>
<p>To answer the first question, we conduct the mistake detection experiment, which will be introduced in Section 4.2. To answer the second question, we conduct three experiments: ambiguity detection, monotonicity check, and recall of correct answers, which will be presented in Sections 4.3-4.5, respectively.</p>
<h3>4.1. Experiment Configurations</h3>
<p>We use gpt-3.5-turbo-0613 as the default LLM for all experiments. We sample 10 model predictions with tem-
perature 0.5 and use the answer frequency to estimate the output distribution. Since all the datasets we use are openended generation tasks, different generated answers could have the exactly same meaning. For example, to answer the question 'When did the world's population reach 7 billion?', the LLM may generate several different answers such as 'December 2017' and 'The world's population reached 7 billion in December 2017', which are essentially the same meaning. Regarding these two answers as distinct answers can lead to an overestimation of the entropy of output distribution. Previous work (Kuhn et al., 2022; Lin et al., 2023) uses a natural language inference model to cluster different generated sequences with the same semantic meanings into one group for better output distribution estimation. We empirically find that LLMs can achieve better clustering performance. Therefore, we prompt the LLM to cluster output answers into different groups for output distribution estimation on question-answering datasets. More details about this process can be found in Appendix A.6.</p>
<p>For all the experiments, we introduce the following baselines: Semantic Uncertainty (Kuhn et al., 2022) (denoted as SE) directly computes the entropy of the output distribution as the estimated (total) uncertainty (named semantic entropy in their paper). Tian et al. (2023) first queries the LLM for the answer and then queries the LLM again for the confidence of the correctness of the answer. We denote this method as Ask4CONF. We also slightly modify the prompt for the ambiguity detection task to query LLM for the confidence of the ambiguity of the input (denoted as Ask4CONF-D). The DEEP Ensembles method (denoted as ENSEMBLES* for brevity) is implemented by ensembling the output distributions of multiple different in-context example sets (we use 5 different sets). We add * here since this method is different from standard DEEP ENSEMBLES and does not directly optimize Eq. 1. We provide more details of the prompts used in the experiments in Appendix A.4.</p>
<h3>4.2. Quantifying Total Uncertainty</h3>
<p>Correctly quantifying the total uncertainty is the premise of correctly decomposing the uncertainty. If the estimated total uncertainty is inaccurate, so will the estimated aleatoric and epistemic uncertainty. A reliable total uncertainty measure should have a close correspondence to the model's prediction accuracy. For model predictions whose total uncertainty is high, the chances that the predictions are incorrect should also be high. Therefore, we will evaluate the total uncertainty quantification using the mistake detection task, following the previous work (Kuhn et al., 2022; Lin et al., 2023).</p>
<p>Evaluation Settings We evaluate the total uncertainty on the Natural Question (NQ) dataset (Kwiatkowski et al., 2019) and GSM8K (Cobbe et al., 2021). For each</p>
<p>Table 1. Uncertainty quantification for mistake detection. Entropy $\left(\boldsymbol{\nu}^{*}\right)$ refers to the average total uncertainty of questions with correct answers, while Entropy $(\boldsymbol{X})$ refers to the average total uncertainty of question with wrong answers.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">AUROC</th>
<th style="text-align: center;">F1 Score</th>
<th style="text-align: center;">Entropy $\left(\boldsymbol{\nu}^{*}\right)$</th>
<th style="text-align: center;">Entropy $(\boldsymbol{X})$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Natural Question</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Semantic Entropy</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.56</td>
</tr>
<tr>
<td style="text-align: center;">Ask4CONF</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Ensembles*</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.88</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">1.18</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Semantic Entropy</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">1.46</td>
</tr>
<tr>
<td style="text-align: center;">Ask4CONF</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Ensembles*</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">1.94</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">1.82</td>
</tr>
</tbody>
</table>
<p>dataset, we randomly sample 200 examples from the validation set for evaluation. The total uncertainty on each example is used to predict whether the model's answer is correct. We report the area under the receiver operator characteristic curve (AUROC) as well as the best F1 score when using the total uncertainty to predict the correctness of the model answer. We use 5 -shot in-context examples on the NQ dataset and 2-shot on the GSM8K dataset with chain-ofthoughts. We prompt the LLM to rephrase the input question to generate the clarification set. The detailed prompts are listed in Appendix A.4.</p>
<p>Results The experiment results are shown in Table 1, which confirms that the total uncertainty measured by the proposed approach is reliable. Specifically, we highlight the following observations. First, our method achieves comparable uncertainty quantification performance compared to the baselines, achieving a similar AUROC and F1 score. Second, as the proposed method shares a symmetry form with the DEEP Ensembles method, one would expect the total uncertainty quantification of the two should be similar. The above experimental results verify that the quantification results of these two methods are very close. Third, although ASK4CONF performs well on factual QA tasks, it provides a poor uncertainty estimation for the complex reasoning task (GSM8K), while our method can still provide good mistake detection performance.</p>
<h3>4.3. Uncertainty Decomposition</h3>
<p>Now we can proceed to evaluate whether the decomposed uncertainty is reliable. As discussed, one of the main causes of aleatoric uncertainty is the ambiguity of the input. Therefore, we will test how well the measured aleatoric uncertainty is predictive of whether an input is ambiguous. In particular, we focus on two input components, the instruction and the question, and separately predict the ambiguity within each component using the respective aleatoric uncertainty (see Section 3.5).</p>
<p>Datasets For ambiguity detection of the question, we select the AmbigQA dataset (Min et al., 2020), which has annotations on the ambiguity of questions. The questions in AmbigQA are extracted from the NQ dataset (Kwiatkowski et al., 2019). For ambiguity detection of the instruction, since there is no existing dataset, we create a dataset, AmbigInst, where we generate ambiguous instructions, their disambiguation, and the input-output pairs using ChatGPT. Each instruction is paired with around 15 questions. Since the focus of AmbigInst is to detect ambiguous instructions, we do not introduce ambiguity to the paired questions. More details about AmbigInst can be found in Appendix B. We use the full AmbigInst dataset and randomly sample 200 examples from the validation set of AmbigQA for evaluation.</p>
<p>Evaluation Settings We use 5-shot in-context examples on the AmbigQA dataset similar to the experiment on the NQ dataset. Since the questions in AmbigInst are relatively easy and straightforward, we directly prompt LLMs in a zero-shot setting. For ambiguous question detection, we perform clarifications on the input question only. We evaluate two clarification LLMs on the AmbigQA dataset, including the Llama-3-8B-Instruct fine-tuned on the training set of AmbigQA and gpt-4. we retrieve the most similar 16 questions as in-context examples when prompting the gpt-4 to generate clarifications for a particular input question. The similarity between two questions is measured by the cosine similarity of their sentence embeddings from SENTENCE-BERT ${ }^{1}$ (Reimers \&amp; Gurevych, 2019). For the AmbigInst dataset, we directly prompt gpt-3.5-turbo-0613 to generate instruction clarifications (See Appendix A. 4 for more details). We also include the performance of our method when using ground-truth disambiguation from the two datasets for reference (denoted as OURS*).</p>
<p>The baselines are similar to the methods in the mistake detection task. The main difference is we use the quantified uncertainty to predict whether the input contains ambiguity. The total uncertainty for SE is used for ambiguity prediction in this task. Also, we test both the aleatoric uncertainty and total uncertainty quantified by the DEEP Ensembles method, denoted by Ensembles<em> (aleatoric) and Ensembles</em> (total) respectively. For our method, we use the aleatoric uncertainty for ambiguity prediction. DEEP ENSEMBLES is not included on the AmbigInst dataset since we do not include in-context examples on that dataset. We also incorporate results with additional methods from previous work (Si et al., 2022; Cole et al., 2023) in Appendix A.1.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2. Uncertainty quantification for ambiguity detection. Avg. $\mathrm{AU}(\boldsymbol{\checkmark})$ refers to the average aleatoric uncertainty of unambiguous questions, while Avg. $\mathrm{AU}(\boldsymbol{X})$ refers to the average aleatoric uncertainty of ambiguous questions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">AUROC</th>
<th style="text-align: center;">F1 Score</th>
<th style="text-align: center;">Avg. AU $(\boldsymbol{\checkmark})$</th>
<th style="text-align: center;">Avg. AU $(\boldsymbol{X})$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AmbigQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Semantic EntropY</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: left;">Ask4CONF-D</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Ensembles* (aleatoric)</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.13</td>
</tr>
<tr>
<td style="text-align: left;">Ensembles* (total)</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: left;">Ours (GPT)</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">Ours (LLaMA)</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.91</td>
</tr>
<tr>
<td style="text-align: left;">Ours*</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">1.52</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Semantic EntropY</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">Ask4CONF-D</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Ours (GPT)</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: left;">Ours*</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">1.04</td>
</tr>
</tbody>
</table>
<p>Results The experiment results are shown in Table 2. We emphasize two observations. First, our method achieves the best ambiguity detection performance and significantly outperforms the baselines. Note that all the baselines, except for Ensembles<em> (aleatoric), use the total uncertainty for ambiguity detection, and thus could not disentangle epistemic uncertainty from the aleatoric uncertainty. Therefore, these results verify the importance of uncertainty decomposition. Second, even a small model can also be efficiently adapted for the clarification generation task. When using the fine-tuned LLaMA model, Our method can still outperform baselines significantly. This adaptation process was remarkably efficient, requiring less than 10 minutes on $4 \times 80 \mathrm{G}$ H100 GPUs. Third, the Deep Ensembles</em> (aleatoric) method is not effective in the black-box LLM setting. As we have discussed in Section 3.3, simply varying the incontext examples cannot accurately estimate the parameter posterior distribution, while the proposed framework is specially designed for the black-box LLMs.</p>
<p>Another observation is that ambiguity detection performance varies across different datasets. On the AmbigQA dataset (Min et al., 2020), the ambiguities are more implicit and hard to find by the clarification models, which makes the detection performance relatively low (although still higher than baselines significantly). Min et al. (2020) also note that the ambiguity in the dataset is "sometimes subtle" and "many (ambiguities) are only apparent after examining one or more Wikipedia pages". In comparison, on the AmbigInst dataset where we design ambiguities to be very explicit (see Appendix B for more examples), the clarification model can generate effective clarifications for most cases, leading to a good detection performance. Finally, the performance of our method can be further improved when using with the ground-truth disambiguation from the two datasets as the input clarifications, demonstrating that the clarification model is still worth exploring.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. The uncertainty quantification examples using the proposed method. The instances are selected from existing datasets including Natural Question (NQ) (Kwiatkowski et al., 2019), AmbigQA (Min et al., 2020), and GSM8K (Cobbe et al., 2021).</p>
<p>Qualitative Results We further present a visual representation of various uncertainty quantification results in Figure 3. These examples have been grouped according to the levels of aleatoric and epistemic uncertainty the LLM exhibits. Our uncertainty quantification framework enables a clear understanding of the sources of uncertainty in each example. For instance, consider the question "What is the lowest # on the FM dial" from the AmbigQA dataset. This question lacks specificity regarding the country and region, leading to ambiguity in the input. Our uncertainty quantification illustrates that the predominant source of uncertainty in the model's prediction stems from aleatoric uncertainty in this case. In contrast, despite a clear question, the model struggles to answer a query about the "Royal Proclamation" (as shown in the upper left example), resulting in a high level of epistemic uncertainty. Interestingly, we have identified a few examples within the GSM8K dataset where the uncertainty in the LLM prediction is attributed to data-related factors. For instance, the upper right example in the figure raises ambiguity about whether the word "this" refers solely to watching shows or encompasses both activities (the ground-truth annotation uses the second interpretation). More details about these examples can be found in Appendix A.5.</p>
<h3>4.4. Monotonicity Check</h3>
<p>To further evaluate the reliability of our aleatoric uncertainty measure, particularly the clarification module, we perform a monotonicity check experiment. Ideally, the clarified input should contribute to a much lower aleatoric uncertainty than the original ambiguous input. To test this, we perform two</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. (Left) Average aleatoric uncertainty of the ambiguous inputs and their clarifications. (Right) Performance improvement via Soliciting clarifications. AmbigQA-Orig and AmbigInst-Orig refer to the recall of correct answers when directly answering the original input. AmbigQA-Clarify and AmbigInst-Clarify refer to the recall of correct answers using different number of input clarifications.
rounds of aleatoric uncertainty measuring. In the first round, we measure the aleatoric uncertainty by clarifying the original input segments (question or instruction). In the second round, we measure the aleatoric uncertainty of the clarified inputs obtained in the first round. Our goal is to check whether the aleatoric uncertainty measured in the second round is much smaller than that in the first round. This experiment is performed on the AmbigQA and AmbigInst datasets. In both rounds, we use the same clarification prompt to generate the clarifications.</p>
<p>Figure 4(a) visualizes the change in uncertainty on both datasets. As can be observed, the aleatoric uncertainty drops significantly after the input is clarified, which verifies the effectiveness of the clarification network.</p>
<h3>4.5. Recall of Correct Answers</h3>
<p>As discussed in Section 3.6, our framework can be used to improve the performance in the presence of ambiguous input by asking users to choose from a set of clarified versions of the input. To make this happen, our methods must be able to cover a good proportion of the possible answers resulting from different clarifications of a given ambiguous input. Also, the number of required clarifications should be smaller, as the users might not want to select the responses from a large set of choices.</p>
<p>To test this, we use the ambiguous questions and instructions from AmbigQA and AmbigInst respectively. For each input, we collect all the possible labeled answers from the ground-truth annotations. Then we select one answer as the target answer that the user is asking for. In our pipeline, the LLM will generate multiple answers given the generated clarifications. Therefore, we inspect how well these generated answers cover the target answer given different numbers of clarifications. We separately compute the recall of the target answer with the different numbers of clarifications. As a baseline, we introduce a vanilla version, where
we directly query the LLM with ambiguous input without any clarification.</p>
<p>The results are illustrated in Figure 4(b). We can consistently observe an increase of recall given more clarifications. Similar to the ambiguity detection performance, the recall improvement on the AmbigInst dataset is more significant compared to the AmbigQA dataset, which is due to the subtlety of the AmbigQA dataset as discussed. Nevertheless, the proposed clarification framework is able to significantly improve the answer recall over the vanilla version without the clarification.</p>
<h2>5. Conclusion</h2>
<p>In this paper, we focus on the uncertainty quantification of LLMs and propose a new framework for decomposing the uncertainty. With a symmetric structure of the BNN methods, our framework leverages input clarifications for uncertainty quantification, which is more suitable for blackbox LLMs. experimental results affirm that our proposed method not only yields reliable uncertainty quantification but also effectively decompose the total uncertainty into aleatoric and epistemic uncertainty. In the future, we will further explore how to build a more effective clarification module to boost the effectiveness of our method.</p>
<h2>Acknowledgment</h2>
<p>The work of Bairu Hou, Yujian Liu, and Shiyu Chang was partially supported by National Science Foundation (NSF) Grant IIS-2207052, NSF Grant IIS-2302730, and CAHSIGoogle Research Award.</p>
<h2>Impact Statement</h2>
<p>In this paper, our primary objective is to develop an innovative uncertainty quantification framework, which aims to empower users in pinpointing the sources of uncertainty in LLMs accurately. The principal application scenario for our approach is to improve the trustworthiness and reliability of LLMs. Consequently, the likelihood of unintended usage or potential risks arising from our proposed method is considerably reduced. We also assess both the evaluations and datasets to ensure they are devoid of any harmful content or adverse impacts.</p>
<p>Nonetheless, it's imperative to note that our method relies on the well-calibrated nature of LLMs when applied to factoid QA and mathematical reasoning tasks. A well-calibrated LLM helps improve the reliability of uncertainty quantification. There exists a possibility that LLMs may not exhibit the same level of calibration on particular downstream tasks, and we are committed to continually refining the calibration of LLMs to bolster the reliability and trustworthiness of</p>
<p>machine learning systems.</p>
<h2>References</h2>
<p>Bhatt, U., Antorán, J., Zhang, Y., Liao, Q. V., Sattigeri, P., Fogliato, R., Melançon, G., Krishnan, R., Stanley, J., Tickoo, O., et al. Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pp. 401-413, 2021.</p>
<p>Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural network. In International conference on machine learning, 2015.</p>
<p>Chen, J. and Mueller, J. Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment. arXiv preprint arXiv:2308.16175, 2023.</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Cole, J. R., Zhang, M. J., Gillick, D., Eisenschlos, J. M., Dhingra, B., and Eisenstein, J. Selectively answering ambiguous questions. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.</p>
<p>Desai, S. and Durrett, G. Calibration of pre-trained transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.</p>
<p>Duan, J., Cheng, H., Wang, S., Wang, C., Zavalny, A., Xu, R., Kailkhura, B., and Xu, K. Shifting attention to relevance: Towards the uncertainty estimation of large language models. arXiv, 2023.</p>
<p>Fort, S., Hu, H., and Lakshminarayanan, B. Deep ensembles: A loss landscape perspective. arXiv preprint arXiv:1912.02757, 2019.</p>
<p>Gal, Y. and Ghahramani, Z. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059. PMLR, 2016.</p>
<p>Gal, Y. et al. Uncertainty in deep learning, 2016.
Graves, A. Practical variational inference for neural networks. Advances in neural information processing systems, 24, 2011.</p>
<p>Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In International conference on machine learning, pp. 1321-1330. PMLR, 2017.</p>
<p>Guo, M., Zhang, M., Reddy, S., and Alikhani, M. Abgcoqa: Clarifying ambiguity in conversational question answering. In 3rd Conference on Automated Knowledge Base Construction, 2021.</p>
<p>Hasenclever, L., Webb, S., Lienart, T., Vollmer, S., Lakshminarayanan, B., Blundell, C., and Teh, Y. W. Distributed bayesian learning with stochastic natural gradient expectation propagation and the posterior server. Journal of Machine Learning Research, 18(106):1-37, 2017.</p>
<p>He, B., Lakshminarayanan, B., and Teh, Y. W. Bayesian deep ensembles via the neural tangent kernel. Advances in neural information processing systems, 33:1010-1022, 2020.</p>
<p>Hendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.</p>
<p>Hernández-Lobato, J. M. and Adams, R. Probabilistic backpropagation for scalable learning of bayesian neural networks. In International conference on machine learning, pp. 1861-1869. PMLR, 2015.</p>
<p>Honovich, O., Shaham, U., Bowman, S. R., and Levy, O. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022.</p>
<p>Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.</p>
<p>Huang, Y., Song, J., Wang, Z., Chen, H., and Ma, L. Look before you leap: An exploratory study of uncertainty measurement for large language models. arXiv preprint arXiv:2307.10236, 2023.</p>
<p>Hüllermeier, E. and Waegeman, W. Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine Learning, 110:457-506, 2021.</p>
<p>Jiang, M., Ruan, Y., Huang, S., Liao, S., Pitis, S., Grosse, R. B., and Ba, J. Calibrating language models via augmented prompt ensembles. 2023.</p>
<p>Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 2021.</p>
<p>Kadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E., Schiefer, N., Hatfield-Dodds, Z., DasSarma, N., Tran-Johnson, E., et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.</p>
<p>Koller, A., Regneri, M., and Thater, S. Regular tree grammars as a formalism for scope underspecification. In Proceedings of ACL-08: HLT, pp. 218-226, 2008.</p>
<p>Kristiadi, A., Hein, M., and Hennig, P. Learnable uncertainty under laplace approximations. In Uncertainty in Artificial Intelligence, pp. 344-353. PMLR, 2021.</p>
<p>Kuhn, L., Gal, Y., and Farquhar, S. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Kuhn, L., Gal, Y., and Farquhar, S. Clam: Selective clarification for ambiguous questions with generative language models. 2023.</p>
<p>Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466, 2019.</p>
<p>Lahlou, S., Jain, M., Nekoei, H., Butoi, V. I., Bertin, P., Rector-Brooks, J., Korablyov, M., and Bengio, Y. Deup: Direct epistemic uncertainty prediction. Transactions on Machine Learning Research, 2022.</p>
<p>Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.</p>
<p>Li, Y., Hernández-Lobato, J. M., and Turner, R. E. Stochastic expectation propagation. Advances in neural information processing systems, 28, 2015.</p>
<p>Lin, S., Hilton, J., and Evans, O. Teaching models to express their uncertainty in words. Transactions on Machine Learning Research, 2022.</p>
<p>Lin, Z., Trivedi, S., and Sun, J. Generating with confidence: Uncertainty quantification for black-box large language models. arXiv preprint arXiv:2305.19187, 2023.</p>
<p>Liu, A., Wu, Z., Michael, J., Suhr, A., West, P., Koller, A., Swayamdipta, S., Smith, N. A., and Choi, Y. We're afraid language models aren't modeling ambiguity. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.</p>
<p>Louizos, C. and Welling, M. Structured and efficient variational deep learning with matrix gaussian posteriors. In International conference on machine learning, pp. 17081716. PMLR, 2016.</p>
<p>Malinin, A. and Gales, M. Predictive uncertainty estimation via prior networks. Advances in neural information processing systems, 31, 2018.</p>
<p>Malinin, A. and Gales, M. Uncertainty estimation in autoregressive structured prediction. In International Conference on Learning Representations, 2020.</p>
<p>Malinin, A., Prokhorenkova, L., and Ustimenko, A. Uncertainty in gradient boosting via ensembles. In International Conference on Learning Representations, 2020.</p>
<p>Mielke, S. J., Szlam, A., Dinan, E., and Boureau, Y.-L. Reducing conversational agents' overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 2022.</p>
<p>Min, S., Michael, J., Hajishirzi, H., and Zettlemoyer, L. Ambigqa: Answering ambiguous open-domain questions. arXiv preprint arXiv:2004.10645, 2020.</p>
<p>Mobiny, A., Yuan, P., Moulik, S. K., Garg, N., Wu, C. C., and Van Nguyen, H. Dropconnect is effective in modeling uncertainty of bayesian deep networks. Scientific reports, 11:5458, 2021.</p>
<p>Neal, R. M. Bayesian learning for neural networks, volume 118. Springer Science \&amp; Business Media, 2012.</p>
<p>Ott, M., Auli, M., Grangier, D., and Ranzato, M. Analyzing uncertainty in neural machine translation. In International Conference on Machine Learning, pp. 3956-3965. PMLR, 2018.</p>
<p>Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon, J., Lakshminarayanan, B., and Snoek, J. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems, 32, 2019.</p>
<p>Park, S. and Kim, T. Pac neural prediction set learning to quantify the uncertainty of generative language models. arXiv preprint arXiv:2307.09254, 2023.</p>
<p>Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 3982-3992, 2019.</p>
<p>Ren, A., Dixit, A., Bodrova, A., Singh, S., Tu, S., Brown, N., Xu, P., Takayama, L., Xia, F., Varley, J., et al. Robots that ask for help: Uncertainty alignment for large language model planners. In 2nd Workshop on Language and Robot Learning: Language as Grounding, 2023.</p>
<p>Riquelme, C., Tucker, G., and Snoek, J. Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling. In International Conference on Learning Representations, 2018.</p>
<p>Shen, M., Bu, Y., Sattigeri, P., Ghosh, S., Das, S., and Wornell, G. Post-hoc uncertainty learning using a dirichlet meta-model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 9772-9781, 2023.</p>
<p>Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J. L., and Wang, L. Prompting gpt-3 to be reliable. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J. L., and Wang, L. Prompting GPT-3 to be reliable. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Tamkin, A., Handa, K., Shrestha, A., and Goodman, N. Task ambiguity in humans and language models. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Teye, M., Azizpour, H., and Smith, K. Bayesian uncertainty estimation for batch normalized deep networks. In International Conference on Machine Learning, pp. 4907-4916. PMLR, 2018.</p>
<p>Tian, K., Mitchell, E., Zhou, A., Sharma, A., Rafailov, R., Yao, H., Finn, C., and Manning, C. D. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Selfconsistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Xiao, Y., Liang, P. P., Bhatt, U., Neiswanger, W., Salakhutdinov, R., and Morency, L.-P. Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 7273-7284, 2022.</p>
<p>Ye, X. and Durrett, G. Can explanations be useful for calibrating black box models?, 2022.</p>
<p>Zhou, K., Jurafsky, D., and Hashimoto, T. Navigating the grey area: Expressions of overconfidence and uncertainty in language models. arXiv preprint arXiv:2302.13439, 2023.</p>
<h1>A. Additional Results and Implementation Details</h1>
<h2>A.1. Additional Results</h2>
<p>In this section, we include additional methods that estimate the model's confidence on its answer to study whether the confidence score can be used for ambiguity detection. Specifically, Si et al. (2022) uses a self-consistency frequency to estimate the LLM confidence (denoted as SELFCONSISTENCY), where multiple answers are sampled from the LLM given the original question and the highest answer frequency are used as the confidence score. In addition, Cole et al. (2023) incorporate two features to estimates the LLM confidence: the answer repetition and the answer diversity. The answer repetition parallels the implementation of Si et al. (2022), and the answer diversity is estimated by counting the number of unique answers from multiple sampled answers. We denote the two methods as SAMPLE REPETITION and SAMPLE Diversity. We include these 3 confidence scores for ambiguity detection.</p>
<p>We implement these methods as follows. For SELFCONSISTENCY, we use the default hyperparameters in the official implementation (temperature $=0.7$, sample 10 answers from the model) to compute the confidence. When predict the input ambiguities, we use 1-confidence as the input, since a lower confidence implies either the model's answer is wrong or there are multiple valid answers (i.e., the input is ambiguous). For SAMPLEDIVERSITY, we use the default hyperparameters in the official implementation (temperature $=0.5$, sample 10 answers from the model). We computed the number of unique answers, using this metric to gauge the ambiguity of the input question. For SAMPLEREP-</p>
<p>Table 3. Additional results for ambiguity detection. Avg. $\mathrm{AU}(\boldsymbol{\mathcal { V }})$ refers to the average aleatoric uncertainty of unambiguous questions, while Avg. $\mathrm{AU}(\boldsymbol{\mathcal { X }})$ refers to the average aleatoric uncertainty of ambiguous questions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">AUROC</th>
<th style="text-align: center;">F1 Score</th>
<th style="text-align: center;">Avg. AU $(\boldsymbol{\mathcal { V }})$</th>
<th style="text-align: center;">Avg. AU $(\boldsymbol{X})$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AmbigQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SEMANTIC ENTROPY</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: left;">ASK4CONF-D</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ENSEMBLES* (aleatoric)</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.13</td>
</tr>
<tr>
<td style="text-align: left;">ENSEMBLES* (total)</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: left;">SELFCONSISTENCY</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">SAMPLE REPETITION</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">SAMPLE DIVERSITY</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">OURS (GPT)</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">OURS (LLaMA)</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.91</td>
</tr>
<tr>
<td style="text-align: left;">OURS*</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">1.52</td>
</tr>
</tbody>
</table>
<p>ETITION, this method parallels SELFCONSISTENCY, where answers are generated using greedy decoding and then re-sampled (temperature $=0.5$, sample 10 answers) to assess the initial answer's confidence. The experiment results is as below:</p>
<p>We visualize the results in Table 3. These results underscore that model confidence alone is insufficient for accurately identifying ambiguous inputs, as evidenced by the AUROC scores of all three methods being under 60 . We will also include these findings and citations in the final version of the paper.</p>
<h2>A.2. Supervised Fine-tuning for Clarification Generation</h2>
<p>We fine-tuning the Llama-3-8B-Instruction on the full training set of AmbigQA dataset on $4 \times$ NVIDIA H100 80GB HBM3 GPU. We organize the training data using the template in Figure 5. We use PyTorch Lightning, DeepSpeed Stage 1, and flash-attention 2 to train the model. We train the model with batch size 16, learning rate $2 \mathrm{e}-5$, and cosine learning rate scheduler for 5 epochs. The loss is only computed on the output tokens. We evaluate the model on the validation set and take the model that achieves lowest validation loss (epoch $=2$ ) for testing.</p>
<h2>A.3. Implementation details for baselines</h2>
<p>Mistake detection For the mistake detection task, we strictly follow the experiment settings from Kuhn et al. (2022) and Lin et al. (2023). For each example, we estimate the output distribution and take the answer with the highest frequency as the final answer. Then we use the method (and the prompt) from Lin et al. (2023) to determine whether the answer is correct by prompting ChatGPT. Based on the total uncertainty and correctness of the answer, we compute the AUROC and conduct a grid search to find the best threshold for the F1 score, where the correct answers are regarded as positive examples.</p>
<p>For the implementation of ASK4CONF(Tian et al., 2023) in the mistake detection task, we use the "Verb. 2S top-1" method (and the corresponding prompts) to estimate the confidence of the language model. Rather than asking the LLM to directly generate an answer, we sample multiple answers and take the most frequent one as the answer. After that, we prompt the LLM for the confidence of the most frequent answer. The prompt we use is:</p>
<div class="codehilite"><pre><span></span><code><span class="err">&lt;</span>|begin_of_text|&gt;<span class="err">&lt;</span>/start_header_id|&gt;user<span class="err">&lt;</span>|end_header_id|&gt;
In<span class="w"> </span>this<span class="w"> </span>task,<span class="w"> </span>you<span class="w"> </span>will<span class="w"> </span>receive<span class="w"> </span>a<span class="w"> </span>question<span class="w"> </span>that<span class="w"> </span>may<span class="w"> </span>contain<span class="w"> </span>ambiguities.<span class="w"> </span>First<span class="w"> </span>analyze<span class="w"> </span>the
following<span class="w"> </span>aspects<span class="w"> </span>to<span class="w"> </span>find<span class="w"> </span>if<span class="w"> </span>there<span class="w"> </span>is<span class="w"> </span>any<span class="w"> </span>ambiguities<span class="w"> </span>according<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>real-world<span class="w"> </span>facts:
-<span class="w"> </span>entities,<span class="w"> </span>objects,<span class="w"> </span>or<span class="w"> </span>events<span class="w"> </span>has<span class="w"> </span>multiple<span class="w"> </span>references<span class="w"> </span>or<span class="w"> </span>interpretations
-<span class="w"> </span>Unclear<span class="w"> </span>timestamps
-<span class="w"> </span>Unclear<span class="w"> </span>locations
-<span class="w"> </span>Unclear<span class="w"> </span>answer<span class="w"> </span>types<span class="w"> </span>(e.g.,<span class="w"> </span>&quot;When&quot;<span class="w"> </span>refers<span class="w"> </span>to<span class="w"> </span>&quot;which<span class="w"> </span>year<span class="w"> </span>or<span class="w"> </span>what<span class="w"> </span>date&quot;,<span class="w"> </span>and<span class="w"> </span>&quot;Who&quot;<span class="w"> </span>refers<span class="w"> </span>to<span class="w"> </span>&quot;
which<span class="w"> </span>person<span class="w"> </span>or<span class="w"> </span>which<span class="w"> </span>team&quot;)
If<span class="w"> </span>there<span class="w"> </span>is<span class="w"> </span>any<span class="w"> </span>ambiguities,<span class="w"> </span>you<span class="w"> </span>need<span class="w"> </span>to<span class="w"> </span>remove<span class="w"> </span>ambiguities<span class="w"> </span>by<span class="w"> </span>adding<span class="w"> </span>some<span class="w"> </span>clarifications<span class="w"> </span>to
the<span class="w"> </span>question.<span class="w"> </span>Each<span class="w"> </span>clarification<span class="w"> </span>is<span class="w"> </span>an<span class="w"> </span>additional<span class="w"> </span>condition<span class="w"> </span>or<span class="w"> </span>explanations<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>concept<span class="w"> </span>in
the<span class="w"> </span>question<span class="w"> </span>that<span class="w"> </span>resolve<span class="w"> </span>its<span class="w"> </span>ambiguity.
<span class="w">    </span>-<span class="w"> </span>You<span class="w"> </span>are<span class="w"> </span>only<span class="w"> </span>allowed<span class="w"> </span>to<span class="w"> </span>add<span class="w"> </span>conditions<span class="w"> </span>or<span class="w"> </span>explanations,<span class="w"> </span>and<span class="w"> </span>you<span class="w"> </span>cannot<span class="w"> </span>change<span class="w"> </span>the
original<span class="w"> </span>intent<span class="w"> </span>or<span class="w"> </span>semantics<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>question.
<span class="w">    </span>-<span class="w"> </span>The<span class="w"> </span>conditions<span class="w"> </span>and<span class="w"> </span>explanations<span class="w"> </span>must<span class="w"> </span>be<span class="w"> </span>ground<span class="w"> </span>to<span class="w"> </span>real-word<span class="w"> </span>facts.
If<span class="w"> </span>there<span class="w"> </span>is<span class="w"> </span>no<span class="w"> </span>ambiguities,<span class="w"> </span>you<span class="w"> </span>only<span class="w"> </span>need<span class="w"> </span>to<span class="w"> </span>output<span class="w"> </span>the<span class="w"> </span>original<span class="w"> </span>question<span class="w"> </span>as<span class="w"> </span>it<span class="w"> </span>is.<span class="err">&lt;</span>|eot_id|&gt;<span class="err">&lt;</span>|
start_header_id|&gt;assistant<span class="err">&lt;</span>|end_header_id|&gt;
Sure.<span class="w"> </span>Please<span class="w"> </span>provide<span class="w"> </span>me<span class="w"> </span>with<span class="w"> </span>the<span class="w"> </span>question<span class="w"> </span>so<span class="w"> </span>that<span class="w"> </span>I<span class="w"> </span>can<span class="w"> </span>identify<span class="w"> </span>whether<span class="w"> </span>it<span class="w"> </span>is<span class="w"> </span>ambiguous<span class="w"> </span>and
clarify<span class="w"> </span>it.<span class="err">&lt;</span>|eot_id|&gt;<span class="err">&lt;</span>|start_header_id|&gt;user<span class="err">&lt;</span>|end_header_id|&gt;
Original<span class="w"> </span>Question:<span class="w"> </span>(original_question)<span class="err">&lt;</span>|eot_id|&gt;<span class="err">&lt;</span>|start_header_id|&gt;assistant<span class="err">&lt;</span>|end_header_id|&gt;
Question<span class="w"> </span>after<span class="w"> </span>adding<span class="w"> </span>condition:<span class="w"> </span>(ground_truth_clarification)<span class="err">&lt;</span>|eot_id|&gt;
</code></pre></div>

<p>Figure 5. The prompt template for the fine-tuning of Llama-3-8B-Instruction. The {original_question} and {ground_truth_clarification} are two placeholders that will be filled with the original question (either ambiguous or not) and the ground-truth clarifications.</p>
<p>Ambiguity detection For the mistake detection task, we use the total uncertainty for SEMANTIC UnCERTAINTY (Kuhn et al., 2022), aleatoric uncertainty from BNN*, and the confidence score of the ambiguity from ASK4CONF (Tian et al., 2023) to predict whether the input is ambiguous or not. We slightly modify the prompt of Ask4CONF as follows:</p>
<h1>A.4. Prompts for Our Clarification Model</h1>
<p>We list the prompts we used for clarification generation on each dataset as follows:</p>
<ul>
<li>Input clarification prompt on Natural Question and GSM8K is shown in Figure 8.</li>
<li>Input clarification prompt on AmbigQA is shown in Figure 9.</li>
<li>Input clarification prompt on AmbigInst is shown in Figure 10.</li>
</ul>
<h2>A.5. Details of the Qualitative Results</h2>
<p>Due to space limit, we truncate the example from GSM8K visualized in Figure 3. The whole question is: "John decides to do several activities while out on vacation. He spends 6 hours boating and half that time swimming. He also watched 3 different shows which were 2 hours each. This was $30 \%$ of the time he spent. He spent $40 \%$ of his time sightseeing. How much time did he spend sightseeing?".</p>
<p>When generating the clarifications, we directly prompt the LLM to paraphrase the question with the following prompt: "I am confused with the following question. Please paraphrase it so that it is easier to understand and solve." Then we sample 5 responses from the LLM as the clarifications for uncertainty quantification.</p>
<h2>A.6. Prompt the LLM for Answer Extraction</h2>
<p>As we have discussed in Section 4.1, different outputs generated by the LLM may have the same meaning in the free-form text generation setting. Unlike previous work (Kuhn et al., 2022; Lin et al., 2023) that map semantics-equivalent answers</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Answer</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">following</span><span class="w"> </span><span class="nt">question</span><span class="o">.</span>
<span class="nt">Question</span><span class="o">:</span><span class="w"> </span><span class="p">{</span><span class="err">The</span><span class="w"> </span><span class="err">testing</span><span class="w"> </span><span class="err">question</span><span class="p">}</span>
<span class="nt">Answer</span><span class="o">:</span><span class="w"> </span><span class="p">{</span><span class="err">The</span><span class="w"> </span><span class="err">most</span><span class="w"> </span><span class="err">frequent</span><span class="w"> </span><span class="err">answer</span><span class="p">}</span>
<span class="nt">Provide</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">probability</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="nt">your</span><span class="w"> </span><span class="nt">answer</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">correct</span><span class="o">.</span><span class="w"> </span><span class="nt">Give</span><span class="w"> </span><span class="nt">ONLY</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">probability</span><span class="o">,</span><span class="w"> </span><span class="nt">no</span><span class="w"> </span><span class="nt">other</span><span class="w"> </span><span class="nt">words</span>
<span class="nt">or</span><span class="w"> </span><span class="nt">explanation</span><span class="o">.</span>
<span class="nt">For</span><span class="w"> </span><span class="nt">example</span><span class="o">:</span>
<span class="nt">Probability</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="nt">the</span><span class="w"> </span><span class="nt">probability</span><span class="w"> </span><span class="nt">between</span><span class="w"> </span><span class="nt">0</span><span class="p">.</span><span class="nc">0</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">1</span><span class="p">.</span><span class="nc">0</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="nt">your</span><span class="w"> </span><span class="nt">solution</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">correct</span><span class="o">,</span><span class="w"> </span><span class="nt">without</span><span class="w"> </span><span class="nt">any</span>
<span class="nt">extra</span><span class="w"> </span><span class="nt">commentary</span><span class="w"> </span><span class="nt">whatsoever</span><span class="o">;</span><span class="w"> </span><span class="nt">just</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">probability</span><span class="o">!&gt;</span>
</code></pre></div>

<p>Figure 6. The prompt for mistake detection (ASK4CONF).</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Read</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">following</span><span class="w"> </span><span class="nx">question</span><span class="p">:</span>
<span class="nx">Question</span><span class="p">:</span>
<span class="p">{</span><span class="nx">question</span><span class="p">}</span>
<span class="nx">Provide</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">probability</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">question</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">ambiguous</span><span class="w"> </span><span class="nx">due</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">factors</span><span class="w"> </span><span class="nx">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">ambiguous</span>
<span class="nx">entities</span><span class="p">,</span><span class="w"> </span><span class="nx">ambiguous</span><span class="w"> </span><span class="nx">event</span><span class="w"> </span><span class="nx">references</span><span class="p">,</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">ambiguity</span><span class="w"> </span><span class="nx">over</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="k">type</span><span class="p">.</span><span class="w"> </span><span class="nx">Give</span><span class="w"> </span><span class="nx">ONLY</span><span class="w"> </span><span class="nx">the</span>
<span class="nx">probability</span><span class="p">,</span><span class="w"> </span><span class="nx">no</span><span class="w"> </span><span class="nx">other</span><span class="w"> </span><span class="nx">words</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">explanation</span><span class="p">.</span>
<span class="nx">For</span><span class="w"> </span><span class="nx">example</span><span class="p">:</span>
<span class="nx">Probability</span><span class="p">:</span><span class="w"> </span><span class="p">&lt;</span><span class="nx">the</span><span class="w"> </span><span class="nx">probability</span><span class="w"> </span><span class="nx">between</span><span class="w"> </span><span class="m m-Double">0.0</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="m m-Double">1.0</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">question</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">ambiguous</span><span class="w"> </span><span class="p">(</span><span class="m m-Double">1.0</span><span class="w"> </span><span class="nx">means</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">question</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">absolutely</span><span class="w"> </span><span class="nx">ambiguous</span><span class="p">),</span><span class="w"> </span><span class="nx">without</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">extra</span><span class="w"> </span><span class="nx">commentary</span><span class="w"> </span><span class="nx">whatsoever</span><span class="p">;</span><span class="w"> </span><span class="nx">just</span><span class="w"> </span><span class="nx">the</span>
<span class="nx">probability</span><span class="p">!&gt;</span>
</code></pre></div>

<p>Figure 7. The prompt for ambiguity detection (ASK4CONF-D).
into a unique set using the NLI models, we empirically find that the LLMs provide better performance on this task. The prompt we use is in Figure 11.</p>
<p>After we extract and cluster the semantically equivalent answers, we further post-process the answers as follows. First, the clarifications generated by the clarification LLM may be invalid and have no answers. In such cases, the LLM may refuse to respond to the question and reply with phrases like "I'm sorry, but I couldn't find any information about the question" or other similar replies. The answer extraction prompt in Figure 11 maps these answers to a special answer, "Unknown." To ensure these answers are mapped to "Unknown," we adopt a keyword-matching approach that defines a set of key phrases signaling a refusal to respond, such as "I'm sorry," "cannot be determined," and "invalid question." Answers containing these key phrases are mapped to "Unknown". Second, if all answers to a particular clarification are mapped to "Unknown", we regard this clarification as invalid and directly drop it when ensembling the outputs for uncertainty quantification. Otherwise, the appearance of the answer "Unknown" indicates the model's insufficient knowledge regarding the question, contributing to the epistemic uncertainty. Therefore, when computing the frequency to estimate the output distribution, we count the occurrences of each unique answer, excluding the special "Unknown". Then, we normalize these counts by dividing each by the total number of answers. For every appearance of the special "Unknown", we increase the normalized frequencies of all other answers by $\frac{1}{N}$, where $N$ is the number of unique answers excluding the special answer. This adjustment ensures the special answer's impact is evenly distributed across the other answers and increases the epistemic uncertainty.</p>
<h1>B. AmbigInst Dataset</h1>
<h2>B.1. Dataset Creation</h2>
<p>We generate ambiguous instructions following the pipeline of SELF-INSTRUCTION (Wang et al., 2022). Specifically, we first query ChatGPT with several manually designed ambiguous task descriptions as in-context examples. For better verification of the ambiguity, we also prompt CHATGPT to output the cause of the ambiguity. Among the ambiguous</p>
<div class="codehilite"><pre><span></span><code>In this task, you will receive a single question, and your goal is to generate multiple
versions of it that convey the same meaning as the original. Please format your responses as
follows:
Rephrase 1: [Your rephrased question]
Rephrase 2: [Another rephrased question]
Rephrase 3: [Yet another rephrased question]
...
Ensure that each rephrased question is distinct from the others.&quot;
Here are two examples:
(examples skipped)
</code></pre></div>

<p>Figure 8. The prompt for question rephrase on the Natural Question dataset
descriptions generated by CHATGPT, we manually filter out those that have an open-ended output space such as Write a report on the new marketing campaign. The final dataset contains 15 ambiguous task descriptions. After that, we query CHATGPT again to generate ground-truth clarifications based on the cause of ambiguities generated in the first query.</p>
<p>Given the collected ambiguous task descriptions and their clarifications, we then prompt the model to generate input-output pairs for each task. Specifically, 15 inputs are generated for each task, and each input is further paired with different output answers depending on the ground-truth clarifications. We additionally add a post-processing step where we filter out the inputs that have exactly the same answer given different clarifications. The final ambiguous instructions consist of 15 tasks with 214 input questions in total.</p>
<p>We take 10 tasks from the Instruction induction dataset (Honovich et al., 2022) as the unambiguous tasks, including letters_list, first_word_letter, second_word_letter, orthography_starts_with, larger_animal, singular_to_plural, diff, num_to_verbal, antonyms, and sum.</p>
<p>We manually add some clarifications to the 10 instructions to remove potential ambiguities. For example, given the original instruction "Break the input word into letters, separated by spaces", we clarify it with "Write the inputted word with a space between each letter", since "separated by spaces" might cause ambiguities of how many spaces should be added between two letters. Each task is also paired with 15 input-output pairs. Overall, the AmbigInst dataset contains 25 tasks and 364 different inputs.</p>
<h1>B.2. Dataset Examples</h1>
<p>We list several examples from the synthetic dataset with ambiguous instructions.
$\triangleright 1$. Rearrange the objects on the table in ascending order.
Input: The following table lists the objects on my desk:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Weight</th>
<th style="text-align: center;">Color</th>
<th style="text-align: center;">Date of Manufacture</th>
<th style="text-align: center;">Price</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Pen</td>
<td style="text-align: center;">14 cm</td>
<td style="text-align: center;">0.02 kg</td>
<td style="text-align: center;">blue</td>
<td style="text-align: center;">$01 / 15 / 2022$</td>
<td style="text-align: center;">$\$ 1.50$</td>
</tr>
<tr>
<td style="text-align: center;">Book</td>
<td style="text-align: center;">23 cm</td>
<td style="text-align: center;">0.5 kg</td>
<td style="text-align: center;">red</td>
<td style="text-align: center;">$08 / 10 / 2020$</td>
<td style="text-align: center;">$\$ 15.00$</td>
</tr>
<tr>
<td style="text-align: center;">Laptop</td>
<td style="text-align: center;">38 cm</td>
<td style="text-align: center;">1.8 kg</td>
<td style="text-align: center;">silver</td>
<td style="text-align: center;">$05 / 04 / 2021$</td>
<td style="text-align: center;">$\$ 1200.00$</td>
</tr>
</tbody>
</table>
<p>$\triangleright 2$. Calculate the average of the numbers in the given list, rounding to the nearest whole number.
Input: $23.5,47.2,30.1,16.6$
$\triangleright 3$. Determine the length of a sentence.</p>
<div class="codehilite"><pre><span></span><code><span class="n">In</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">follows</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">some</span><span class="w"> </span><span class="n">questions</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">might</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">ambiguous</span><span class="o">.</span><span class="w"> </span><span class="n">These</span><span class="w"> </span><span class="n">ambiguities</span>
<span class="n">can</span><span class="w"> </span><span class="n">arise</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">various</span><span class="w"> </span><span class="n">factors</span><span class="p">,</span><span class="w"> </span><span class="n">including</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">limited</span><span class="w"> </span><span class="n">to</span><span class="p">:</span>
<span class="mf">1.</span><span class="w"> </span><span class="n">Ambiguous</span><span class="w"> </span><span class="n">references</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">entities</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span><span class="o">.</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">Multiple</span><span class="w"> </span><span class="n">properties</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">objects</span><span class="o">/</span><span class="n">entities</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="n">leading</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">different</span><span class="w"> </span><span class="n">interpretations</span>
<span class="mf">3.</span><span class="w"> </span><span class="n">Ambiguities</span><span class="w"> </span><span class="n">due</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">unclear</span><span class="w"> </span><span class="n">timestamps</span><span class="o">.</span>
<span class="mf">4.</span><span class="w"> </span><span class="n">Ambiguities</span><span class="w"> </span><span class="n">stemming</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">unclear</span><span class="w"> </span><span class="n">locations</span><span class="o">.</span>
<span class="mf">5.</span><span class="w"> </span><span class="n">Multiple</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">types</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span><span class="o">.</span>
<span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">question</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">least</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">distinct</span><span class="w"> </span><span class="n">rephrasings</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">resolve</span><span class="w"> </span><span class="n">these</span>
<span class="n">ambiguities</span><span class="o">.</span><span class="w"> </span><span class="n">By</span><span class="w"> </span><span class="s2">&quot;rephrasing,&quot;</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">reformulate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">clear</span><span class="w"> </span><span class="ow">and</span>
<span class="n">direct</span><span class="p">,</span><span class="w"> </span><span class="n">eliminating</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">possible</span><span class="w"> </span><span class="n">ambiguity</span><span class="w"> </span><span class="n">without</span><span class="w"> </span><span class="n">altering</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">original</span><span class="w"> </span><span class="n">intent</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span>
<span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">seek</span><span class="w"> </span><span class="n">further</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">produce</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="p">(</span><span class="n">yes</span><span class="o">-</span><span class="n">no</span><span class="p">)</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="n">of</span>
<span class="n">the</span><span class="w"> </span><span class="n">clarification</span><span class="o">.</span><span class="w"> </span><span class="n">Instead</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">create</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">direct</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="p">(</span><span class="n">wh</span><span class="o">-</span><span class="n">question</span><span class="p">)</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">aims</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">obtain</span>
<span class="w">    </span><span class="n">a</span><span class="w"> </span><span class="n">specific</span><span class="w"> </span><span class="n">answer</span><span class="o">.</span>
<span class="n">Please</span><span class="w"> </span><span class="n">format</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">responses</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">follows</span><span class="w"> </span><span class="p">(</span><span class="n">with</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">least</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">rephrasings</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">question</span><span class="p">):</span>
<span class="n">Clarifications</span><span class="p">:</span>
<span class="mf">1.</span><span class="w"> </span><span class="p">[</span><span class="n">First</span><span class="w"> </span><span class="n">rephrased</span><span class="w"> </span><span class="n">question</span><span class="p">]</span>
<span class="mf">2.</span><span class="w"> </span><span class="p">[</span><span class="n">Second</span><span class="w"> </span><span class="n">rephrased</span><span class="w"> </span><span class="n">question</span><span class="p">]</span>
<span class="mf">3.</span><span class="w"> </span><span class="p">[</span><span class="n">Third</span><span class="w"> </span><span class="n">rephrased</span><span class="w"> </span><span class="n">question</span><span class="p">]</span>
<span class="o">...</span>
<span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">original</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">already</span><span class="w"> </span><span class="n">clear</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">unambiguous</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">indicate</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">stating</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;No clarification needed.&quot;</span>
<span class="p">(</span><span class="n">In</span><span class="o">-</span><span class="n">context</span><span class="w"> </span><span class="n">examples</span><span class="p">)</span>
</code></pre></div>

<p>Figure 9. The prompt for question disambiguation on the AmbigQA dataset.</p>
<p>Input: The quick brown fox jumps over the lazy dog.
$\triangleright 4$. Sort the names alphabetically.
Input: Courtney Cox, Jennifer Aniston, Lisa Kudrow, Matthew Perry.
$\triangleright 5$. Identify the subject in the sentence.
Input: The CEO of the company gave a speech about the future of technology.
$\triangleright 6$. Count the number of objects in the given list of objects.
Input: Forks, Spoons, Knives, Plates, Cups, Spoons, Forks, Spoons, Cups.
$\triangleright 7$. Rank the football players based on their performance.
Input: The following table lists the statistics of football players:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Goal Scored</th>
<th style="text-align: center;">Assists</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lionel Messi</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">Cristiano Ronaldo</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Robert Lewandowski</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">5</td>
</tr>
</tbody>
</table>
<p><strong>Objective</strong>
Analyze the given task description for ambiguities based on the description itself and the provided input question. If the task description is ambiguous, your task is to clarify it by interpreting the ambiguous concepts, specifying necessary conditions, or using other methods. Provide all possible disambiguations.
<strong>Important Rules</strong></p>
<ol>
<li>Perform detailed analyses before concluding whether the task description is clear or ambiguous.</li>
<li>Output disambiguations in the specified format.</li>
<li>Some seemingly unambiguous task descriptions are actually ambiguous given that particular input. So, do not forget to leverage the input to analyze whether the task description is underspecified.
<strong>Output Format</strong>
Your output should follow this format:
Analyses:
[Think step-by-step to reason on the clarity of the task description. After that, output your judgement on whether the task description is ambiguous or not]</li>
</ol>
<p>Disambiguations:
1. [Disambiguated task description 1.]
2. [Disambiguated task description 2.]
3. [Disambiguated task description 3.]
・.
If the task description is clear and unambiguous, simply output:
Disambiguations:
4. No clarification needed.</p>
<p>Figure 10. The prompt for instruction disambiguation on the AmbigInst dataset.
$\triangleright 8$. Sort the data in alphabetical order.
Input: Dog, Cat, Bird, Fish, Aardvark.
$\triangleright 9$. Identify the largest city in the set. Input: The following table lists the cities in the set:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Population</th>
<th style="text-align: left;">Land Area</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Paris</td>
<td style="text-align: left;">2.1 million</td>
<td style="text-align: left;">105.4 km</td>
</tr>
<tr>
<td style="text-align: left;">Berlin</td>
<td style="text-align: left;">3.6 million</td>
<td style="text-align: left;">891.8 km</td>
</tr>
<tr>
<td style="text-align: left;">Madrid</td>
<td style="text-align: left;">3.3 million</td>
<td style="text-align: left;">604.3 km</td>
</tr>
</tbody>
</table>
<p>$\triangleright 10$. Organize the files by date.
Input: Files to be organized:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Filename</th>
<th style="text-align: left;">Creation Date</th>
<th style="text-align: center;">Last Modified Date</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">conference-recording.avi</td>
<td style="text-align: left;">$11 / 10 / 2020$</td>
<td style="text-align: center;">$11 / 12 / 2020$</td>
</tr>
<tr>
<td style="text-align: left;">birthday-video.mp4</td>
<td style="text-align: left;">$05 / 05 / 2021$</td>
<td style="text-align: center;">$05 / 06 / 2021$</td>
</tr>
<tr>
<td style="text-align: left;">budget.xlsx</td>
<td style="text-align: left;">$12 / 31 / 2022$</td>
<td style="text-align: center;">$01 / 10 / 2023$</td>
</tr>
</tbody>
</table>
<p>$\triangleright 11$. Find the middle value in a list of numbers.</p>
<div class="codehilite"><pre><span></span><code><span class="o">**</span><span class="n">Task</span><span class="p">:</span><span class="w"> </span><span class="n">Answer</span><span class="w"> </span><span class="n">Extraction</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">Sentences</span><span class="o">**</span>
<span class="n">In</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">receive</span><span class="w"> </span><span class="n">both</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">sentences</span><span class="o">.</span><span class="w"> </span><span class="n">Each</span><span class="w"> </span><span class="n">sentence</span><span class="w"> </span><span class="n">contains</span>
<span class="n">an</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span><span class="o">.</span><span class="w"> </span><span class="n">Your</span><span class="w"> </span><span class="n">primary</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">extract</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">concise</span><span class="w"> </span><span class="n">answer</span><span class="p">,</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">a</span>
<span class="n">single</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">short</span><span class="w"> </span><span class="n">phrase</span><span class="p">,</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">sentence</span><span class="o">.</span><span class="w"> </span><span class="n">Again</span><span class="p">,</span><span class="w"> </span><span class="n">ensure</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">only</span><span class="w"> </span><span class="n">extract</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">short</span>
<span class="n">answer</span><span class="o">!</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">short</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">directly</span><span class="w"> </span><span class="n">extracted</span><span class="p">,</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">summarize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">whole</span><span class="w"> </span><span class="n">sentence</span><span class="w"> </span><span class="n">into</span>
<span class="n">a</span><span class="w"> </span><span class="n">single</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">short</span><span class="w"> </span><span class="n">phrase</span><span class="o">.</span>
<span class="n">Additionally</span><span class="p">,</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">extracting</span><span class="w"> </span><span class="n">answers</span><span class="p">,</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">secondary</span><span class="w"> </span><span class="n">goal</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">create</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="s2">&quot;answer set&quot;</span><span class="w"> </span><span class="n">that</span>
<span class="n">contains</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">distinct</span><span class="w"> </span><span class="n">answers</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">previous</span><span class="w"> </span><span class="n">questions</span><span class="o">.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">extracted</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">appeared</span>
<span class="w">    </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">set</span><span class="p">,</span><span class="w"> </span><span class="n">add</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">set</span><span class="o">.</span>
<span class="o">**</span><span class="n">Important</span><span class="w"> </span><span class="n">Rules</span><span class="o">**</span>
<span class="mf">1.</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">semantically</span><span class="w"> </span><span class="n">equivalent</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">extracted</span>
<span class="n">answer</span><span class="p">,</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">result</span><span class="o">.</span><span class="w"> </span><span class="n">Do</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">introduce</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">new</span><span class="p">,</span><span class="w"> </span><span class="n">slightly</span>
<span class="n">different</span><span class="w"> </span><span class="n">answer</span><span class="o">.</span><span class="w"> </span><span class="n">For</span><span class="w"> </span><span class="n">example</span><span class="p">,</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">already</span><span class="w"> </span><span class="n">contains</span><span class="w"> </span><span class="s2">&quot;the matrix (1999),&quot;</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">you</span>
<span class="n">extract</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">sentence</span><span class="w"> </span><span class="n">like</span><span class="w"> </span><span class="s2">&quot;The popular movie in 1999... is the matrix,&quot;</span><span class="w"> </span><span class="n">your</span>
<span class="n">extraction</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="s2">&quot;the matrix (1999)&quot;</span><span class="w"> </span><span class="n">rather</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="s2">&quot;the matrix.&quot;</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">Separate</span><span class="w"> </span><span class="n">different</span><span class="w"> </span><span class="n">answers</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="s2">&quot;|&quot;</span><span class="o">.</span>
<span class="mf">3.</span><span class="w"> </span><span class="n">Also</span><span class="p">,</span><span class="w"> </span><span class="n">extract</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="s2">&quot;Unknown&quot;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">cases</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">sentence</span><span class="w"> </span><span class="n">claims</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">sentence</span><span class="w"> </span><span class="n">claims</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">lacks</span><span class="w"> </span><span class="n">sufficient</span><span class="w"> </span><span class="n">information</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">sentence</span><span class="w"> </span><span class="n">claims</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">depends</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">various</span><span class="w"> </span><span class="n">factors</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">determined</span>
<span class="o">**</span><span class="n">Output</span><span class="w"> </span><span class="n">Format</span><span class="o">**</span>
<span class="n">Your</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">format</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">follow</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">pattern</span><span class="w"> </span><span class="p">(</span><span class="n">N</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">sentences</span><span class="p">):</span>
<span class="n">Answer</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">beginning</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="p">]</span>
<span class="n">Extraction</span><span class="w"> </span><span class="mi">1</span><span class="o">/</span><span class="n">N</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">extraction</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="mi">1</span><span class="n">st</span><span class="w"> </span><span class="n">sentence</span><span class="p">]</span>
<span class="n">Updated</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">set</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="p">]</span>
<span class="n">Extraction</span><span class="w"> </span><span class="mi">2</span><span class="o">/</span><span class="n">N</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">extraction</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="mi">2</span><span class="n">nd</span><span class="w"> </span><span class="n">sentence</span><span class="p">]</span>
<span class="n">Updated</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">set</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="p">]</span>
<span class="n">Extraction</span><span class="w"> </span><span class="mi">3</span><span class="o">/</span><span class="n">N</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">extraction</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="mi">3</span><span class="n">rd</span><span class="w"> </span><span class="n">sentence</span><span class="p">]</span>
<span class="n">Updated</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">set</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="p">]</span>
<span class="o">...</span>
<span class="n">Final</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">set</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="p">]</span>
<span class="o">**</span><span class="n">Example</span><span class="o">**</span>
<span class="p">(</span><span class="n">examples</span><span class="w"> </span><span class="n">skipped</span><span class="p">)</span>
</code></pre></div>

<p>Figure 11. The prompt for answer extraction using LLMs for the Natural Question and AmbigQA datasets.</p>
<p>Input: $12,20,35,46,52,66,74,81$
$\triangleright 12$. Determine the square root of a number.
Input: 81
$\triangleright 13$. Find the capital of a country.
Input: South Africa
$\triangleright 14$. Classify a movie based on its rating.
Input: The movie "Toy Story 4" has an MPAA rating of G, an IMDb rating of 7.8, and a Rotten Tomatoes rating of $97 \%$.</p>
<p>$\triangleright 15$. Select the longest sentence from the following choices, and output the sentence index.
Input: The following sentences are listed:</p>
<ol>
<li>To be, or not to be, that is the question.</li>
<li>Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune.</li>
<li>Or to take arms against a sea of troubles and by opposing end them.</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ We use the pre-trained sentence transformer model https://huggingface.co/sentence-transformers/all-mpnet-base-v2.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5426 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5426</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5426</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-cb3125e4f63f3d058a2a39270ecb585e86c3d1ff</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cb3125e4f63f3d058a2a39270ecb585e86c3d1ff" target="_blank">Chain of Hindsight Aligns Language Models with Feedback</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity, and significantly surpasses previous methods in aligning language models with human preferences.</p>
                <p><strong>Paper Abstract:</strong> Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5426.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5426.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain of Hindsight</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A finetuning method that conditions a causal decoder-only language model on a sequence (chain) of model-generated outputs paired with human feedback (templated or natural language), training the model to autoregressively produce improved outputs that align with the feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J 6B / OPT (family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Experiments use GPT-J 6B (6B-parameter autoregressive decoder-only transformer) and OPT models as base pretrained models; CoH is implemented as a finetuning procedure on such autoregressive transformer LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain of Hindsight (CoH)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Construct training sequences by concatenating multiple model outputs with paired feedback (e.g., 'Bad: {negative} Good: {positive}' or richer natural-language feedback). During finetuning the model is conditioned on these chains and trained with the standard causal language-modeling cross-entropy objective (masking loss on feedback tokens so the model predicts only non-feedback tokens). Random past-token masking (0–5%) is applied to reduce copying; a pretraining-regularization term is added to preserve general LM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization (TL;DR) and Dialogue (Anthropic HH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Summarization: generate TL;DR style summaries of Reddit posts (coverage, accuracy, coherence human-eval dimensions and ROUGE reported). Dialogue: produce helpful and harmless conversational responses; evaluate on Anthropic HH pairwise preference dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Summarization human evaluation (pairwise against baselines): CoH average win rate 57.5% vs base; against SFT average win rate 44.0%; against C-SFT 42.3%; against SFT-U 61.7%; against RLHF 45.3% (see Table 1). Dialogue human evaluation: CoH average win rate 49.5% vs base; vs SFT 39.4%; vs C-SFT 36.8%; vs SFT-U 56.0%; vs RLHF 36.9% (see Table 2). Paper also reports CoH achieves higher ROUGE scores on TL;DR than SFT and RLHF (Figure 3), exact ROUGE numbers not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baselines (no chain-conditioning): Base pretrained model average summarization win rate 19.9% (vs CoH); SFT average 28.2%; C-SFT average 29.6%; SFT-U average 21.4%; RLHF average 30.8% (summarization). Dialogue baselines: Base 15.2% avg; SFT 19.1%; C-SFT 22.1%; SFT-U 13.9%; RLHF 23.4% (see Tables 1 and 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative human evaluations across summarization and dialogue show large pairwise win-rate improvements for CoH over SFT, conditional SFT, SFT with unlikelihood, and RLHF (e.g., CoH wins 57.5% vs base on summarization, and 49.5% vs base on dialogue). Automatic ROUGE on summarization also favors CoH over baselines per Figure 3. Ablation: removing natural-language feedback ('CoH w/o LF') reduces human preference compared to full CoH (paper reports 14.1% preference increase for models with language feedback vs 11.6% without in a particular comparison), indicating language feedback contributes to gains. Controllable-generation experiments (E.1) show CoH-trained models can follow iterative 'improve the previous summary' instructions while RLHF fails to follow latter improvement instructions, qualitatively demonstrating the model learned to use previous outputs + feedback to refine generations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations: CoH can create very long training sequences when chaining multiple feedback instances, increasing computation and training cost; small models showed marginally worse performance compared to SFT baselines (CoH improves with scale — positive scaling trend but may hurt very small models); potential for the model to 'copy' conditioned examples (mitigated by random past-token masking of 0–5%); reliance on human labelers for evaluation is costly; paper does not specify exact chain length or iteration counts and does not provide granular per-metric automatic scores for all comparisons in-text (some plots referenced but numeric values not fully enumerated).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Hindsight Aligns Language Models with Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5426.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5426.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HIR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HIR (The wisdom of hindsight / HIR method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that uses incorrect model outputs as learning signal; described in the paper as a special case of CoH with a chain length of one.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>HIR (single-step hindsight conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Condition the model on a single incorrect model output (and its feedback) to learn to produce better outputs; in the paper HIR is described as equivalent to CoH when the chain length is one, but HIR uses a more complex training loss (likelihood + contrastive + entropy) according to the paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited as related prior work and characterized as a special case of CoH; the paper does not present experiments of HIR itself but contrasts CoH's simpler training with HIR's more complex losses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes HIR uses a more complex training procedure (likelihood, contrastive, entropy losses) compared to CoH; no experimental failure cases for HIR are reported in this paper (only referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Hindsight Aligns Language Models with Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5426.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5426.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Korbak2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretraining language models with human preferences (Korbak et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work exploring conditioning on human preference during pretraining to improve alignment; the paper cites it as similar to CoH with chain length one.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Preference conditioning (single-pair conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Conditioning models on preference tokens during pretraining so the model learns to align with human preferences; characterized in this paper as similar to CoH when CoH chain length equals one.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned in related work as demonstrating improved alignment when conditioning on human preference during pretraining; this paper leverages the conceptual similarity but does not reuse Korbak et al.'s experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Hindsight Aligns Language Models with Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5426.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5426.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-improve / STAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models can self-improve / STAR (Self-taught reasoner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Works that explore LMs self-improving through internal generation and refinement; cited in related work as part of the space of self-reflection / bootstrapping methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-improvement / self-taught reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Approaches where LMs generate intermediate reasoning or revised answers and then use those generations to refine final outputs (bootstrapping); referenced as related literature but not implemented in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper cites these works (refs 19 and 56) to position CoH within literature on self-improvement and bootstrapping; no experimental cross-comparison is performed within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper; cited works may report their own limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Hindsight Aligns Language Models with Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5426.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5426.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conditional SFT (C-SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional Supervised Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method that conditions supervised finetuning on feedback/control tokens (akin to Decision Transformer-style conditional training) to influence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Conditional SFT</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train the model with feedback tokens as prefixes (control tokens) so the model conditions on these to produce desired outputs. In contrast to CoH, conditional SFT typically conditions on a single feedback token or prefix rather than a chain of feedback-example pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization and Dialogue (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same evaluation tasks as CoH in the paper (TL;DR summarization and HH dialogue).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported as a baseline: C-SFT average human-eval win rates vs CoH are lower (CoH average win rates 42.3% vs C-SFT 29.6% on summarization; CoH 36.8% vs C-SFT 22.1% on dialogue, per Table 1 and 2 comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper shows conditional SFT improves over plain SFT in some cases, but CoH (conditioning on chains) substantially outperforms conditional SFT in human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Conditional SFT conditions on a single control token/prefix and thus has less contextual information than CoH's chains; performs worse than CoH in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Hindsight Aligns Language Models with Feedback', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The wisdom of hindsight makes language models better instruction followers <em>(Rating: 2)</em></li>
                <li>Pretraining language models with human preferences <em>(Rating: 2)</em></li>
                <li>Large language models can self-improve <em>(Rating: 2)</em></li>
                <li>STAR: Self-taught reasoner bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Training language models with language feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5426",
    "paper_id": "paper-cb3125e4f63f3d058a2a39270ecb585e86c3d1ff",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "CoH",
            "name_full": "Chain of Hindsight",
            "brief_description": "A finetuning method that conditions a causal decoder-only language model on a sequence (chain) of model-generated outputs paired with human feedback (templated or natural language), training the model to autoregressively produce improved outputs that align with the feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J 6B / OPT (family)",
            "model_description": "Experiments use GPT-J 6B (6B-parameter autoregressive decoder-only transformer) and OPT models as base pretrained models; CoH is implemented as a finetuning procedure on such autoregressive transformer LMs.",
            "reflection_method_name": "Chain of Hindsight (CoH)",
            "reflection_method_description": "Construct training sequences by concatenating multiple model outputs with paired feedback (e.g., 'Bad: {negative} Good: {positive}' or richer natural-language feedback). During finetuning the model is conditioned on these chains and trained with the standard causal language-modeling cross-entropy objective (masking loss on feedback tokens so the model predicts only non-feedback tokens). Random past-token masking (0–5%) is applied to reduce copying; a pretraining-regularization term is added to preserve general LM performance.",
            "num_iterations": null,
            "task_name": "Summarization (TL;DR) and Dialogue (Anthropic HH)",
            "task_description": "Summarization: generate TL;DR style summaries of Reddit posts (coverage, accuracy, coherence human-eval dimensions and ROUGE reported). Dialogue: produce helpful and harmless conversational responses; evaluate on Anthropic HH pairwise preference dataset.",
            "performance_with_reflection": "Summarization human evaluation (pairwise against baselines): CoH average win rate 57.5% vs base; against SFT average win rate 44.0%; against C-SFT 42.3%; against SFT-U 61.7%; against RLHF 45.3% (see Table 1). Dialogue human evaluation: CoH average win rate 49.5% vs base; vs SFT 39.4%; vs C-SFT 36.8%; vs SFT-U 56.0%; vs RLHF 36.9% (see Table 2). Paper also reports CoH achieves higher ROUGE scores on TL;DR than SFT and RLHF (Figure 3), exact ROUGE numbers not tabulated in text.",
            "performance_without_reflection": "Baselines (no chain-conditioning): Base pretrained model average summarization win rate 19.9% (vs CoH); SFT average 28.2%; C-SFT average 29.6%; SFT-U average 21.4%; RLHF average 30.8% (summarization). Dialogue baselines: Base 15.2% avg; SFT 19.1%; C-SFT 22.1%; SFT-U 13.9%; RLHF 23.4% (see Tables 1 and 2).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative human evaluations across summarization and dialogue show large pairwise win-rate improvements for CoH over SFT, conditional SFT, SFT with unlikelihood, and RLHF (e.g., CoH wins 57.5% vs base on summarization, and 49.5% vs base on dialogue). Automatic ROUGE on summarization also favors CoH over baselines per Figure 3. Ablation: removing natural-language feedback ('CoH w/o LF') reduces human preference compared to full CoH (paper reports 14.1% preference increase for models with language feedback vs 11.6% without in a particular comparison), indicating language feedback contributes to gains. Controllable-generation experiments (E.1) show CoH-trained models can follow iterative 'improve the previous summary' instructions while RLHF fails to follow latter improvement instructions, qualitatively demonstrating the model learned to use previous outputs + feedback to refine generations.",
            "limitations_or_failure_cases": "Reported limitations: CoH can create very long training sequences when chaining multiple feedback instances, increasing computation and training cost; small models showed marginally worse performance compared to SFT baselines (CoH improves with scale — positive scaling trend but may hurt very small models); potential for the model to 'copy' conditioned examples (mitigated by random past-token masking of 0–5%); reliance on human labelers for evaluation is costly; paper does not specify exact chain length or iteration counts and does not provide granular per-metric automatic scores for all comparisons in-text (some plots referenced but numeric values not fully enumerated).",
            "uuid": "e5426.0",
            "source_info": {
                "paper_title": "Chain of Hindsight Aligns Language Models with Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "HIR",
            "name_full": "HIR (The wisdom of hindsight / HIR method)",
            "brief_description": "A prior method that uses incorrect model outputs as learning signal; described in the paper as a special case of CoH with a chain length of one.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "HIR (single-step hindsight conditioning)",
            "reflection_method_description": "Condition the model on a single incorrect model output (and its feedback) to learn to produce better outputs; in the paper HIR is described as equivalent to CoH when the chain length is one, but HIR uses a more complex training loss (likelihood + contrastive + entropy) according to the paper's discussion.",
            "num_iterations": 1,
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Cited as related prior work and characterized as a special case of CoH; the paper does not present experiments of HIR itself but contrasts CoH's simpler training with HIR's more complex losses.",
            "limitations_or_failure_cases": "Paper notes HIR uses a more complex training procedure (likelihood, contrastive, entropy losses) compared to CoH; no experimental failure cases for HIR are reported in this paper (only referenced).",
            "uuid": "e5426.1",
            "source_info": {
                "paper_title": "Chain of Hindsight Aligns Language Models with Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Korbak2023",
            "name_full": "Pretraining language models with human preferences (Korbak et al.)",
            "brief_description": "Prior work exploring conditioning on human preference during pretraining to improve alignment; the paper cites it as similar to CoH with chain length one.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Preference conditioning (single-pair conditioning)",
            "reflection_method_description": "Conditioning models on preference tokens during pretraining so the model learns to align with human preferences; characterized in this paper as similar to CoH when CoH chain length equals one.",
            "num_iterations": 1,
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Mentioned in related work as demonstrating improved alignment when conditioning on human preference during pretraining; this paper leverages the conceptual similarity but does not reuse Korbak et al.'s experimental results.",
            "limitations_or_failure_cases": "Not discussed in detail in this paper.",
            "uuid": "e5426.2",
            "source_info": {
                "paper_title": "Chain of Hindsight Aligns Language Models with Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Self-improve / STAR",
            "name_full": "Large language models can self-improve / STAR (Self-taught reasoner)",
            "brief_description": "Works that explore LMs self-improving through internal generation and refinement; cited in related work as part of the space of self-reflection / bootstrapping methods.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Self-improvement / self-taught reasoning",
            "reflection_method_description": "Approaches where LMs generate intermediate reasoning or revised answers and then use those generations to refine final outputs (bootstrapping); referenced as related literature but not implemented in this work.",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Paper cites these works (refs 19 and 56) to position CoH within literature on self-improvement and bootstrapping; no experimental cross-comparison is performed within this paper.",
            "limitations_or_failure_cases": "Not detailed in this paper; cited works may report their own limitations.",
            "uuid": "e5426.3",
            "source_info": {
                "paper_title": "Chain of Hindsight Aligns Language Models with Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Conditional SFT (C-SFT)",
            "name_full": "Conditional Supervised Fine-Tuning",
            "brief_description": "A baseline method that conditions supervised finetuning on feedback/control tokens (akin to Decision Transformer-style conditional training) to influence generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "Conditional SFT",
            "reflection_method_description": "Train the model with feedback tokens as prefixes (control tokens) so the model conditions on these to produce desired outputs. In contrast to CoH, conditional SFT typically conditions on a single feedback token or prefix rather than a chain of feedback-example pairs.",
            "num_iterations": null,
            "task_name": "Summarization and Dialogue (used as baseline)",
            "task_description": "Same evaluation tasks as CoH in the paper (TL;DR summarization and HH dialogue).",
            "performance_with_reflection": "Reported as a baseline: C-SFT average human-eval win rates vs CoH are lower (CoH average win rates 42.3% vs C-SFT 29.6% on summarization; CoH 36.8% vs C-SFT 22.1% on dialogue, per Table 1 and 2 comparisons).",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "evidence_of_improvement": "Paper shows conditional SFT improves over plain SFT in some cases, but CoH (conditioning on chains) substantially outperforms conditional SFT in human evaluations.",
            "limitations_or_failure_cases": "Conditional SFT conditions on a single control token/prefix and thus has less contextual information than CoH's chains; performs worse than CoH in reported experiments.",
            "uuid": "e5426.4",
            "source_info": {
                "paper_title": "Chain of Hindsight Aligns Language Models with Feedback",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The wisdom of hindsight makes language models better instruction followers",
            "rating": 2
        },
        {
            "paper_title": "Pretraining language models with human preferences",
            "rating": 2
        },
        {
            "paper_title": "Large language models can self-improve",
            "rating": 2
        },
        {
            "paper_title": "STAR: Self-taught reasoner bootstrapping reasoning with reasoning",
            "rating": 2
        },
        {
            "paper_title": "Training language models with language feedback",
            "rating": 1
        }
    ],
    "cost": 0.0131145,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Chain of Hindsight aligns Language Models with Feedback</h1>
<p>Hao Liu<br>UC Berkeley<br>hao.liu@berkeley.edu<br>Carmelo Sferrazza<br>UC Berkeley<br>csferrazza@berkeley.edu<br>Pieter Abbeel<br>UC Berkeley<br>pabbeel@cs.berkeley.edu</p>
<h4>Abstract</h4>
<p>Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models have achieved amazing results in natural language understanding [37, 38, 7]. However, in order to ensure that these technologies have a positive impact on society, it is of paramount importance for them to be aligned with human values. One of the most critical elements in achieving this is the use of human feedback. Human feedback allows us to evaluate the performance of such models in a way that is both objective and subjective. It can help to identify issues with accuracy, fairness, and bias, and can provide insights into how the model can be improved, in order to ensure that the model outputs align with societal norms and expectations. Driven by the importance of incorporating human feedback into language models, researchers have been developing and testing various methods for human-in-the-loop systems. These methods aim to make the process of incorporating human feedback more efficient, resulting in models that are able to achieve improved performance and accuracy, while also providing higher fairness and more ethical outputs [18, 36, 55, 35, 42, inter alia].</p>
<p>The successes in language modeling have been largely attributed to the utilization of supervised finetuning (SFT) and Reinforcement Learning with Human Feedback (RLHF) techniques. While these approaches have demonstrated promising results in enhancing the performance of language models on specific tasks, they also suffer from notable limitations. SFT relies on human-annotated data and positive-rated model generation to fine-tune a pretrained language model. However, this</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Human evaluation pairwise comparison between CoH and various approaches on the summarization and dialogue tasks. Base denotes the pretrained model, SFT-U denotes SFT with unlikelihood loss, C-SFT denotes conditional SFT. CoH substantially outperform reinforcement learning from human feedback (RLHF) and supervised finetuning baselines.</p>
<p>approach is heavily reliant on the availability of labeled data, which may entail significant expenses and time investments. Moreover, relying solely on positive-rated data may constrain the model's ability to identify and correct negative attributes or errors, thus reducing its generalizability to new and unseen data. Alternatively, RLHF enables learning from all data, regardless of feedback rating. Nonetheless, this method requires learning a reward function, which may be subject to misalignment and imperfections [16]. In addition, the optimization of reinforcement learning algorithms can be challenging, presenting significant difficulties in its application.</p>
<p>In this work, we aim to overcome the limitations of SFT and RLHF by combining their strengths to leverage all feedback, without resorting to reinforcement learning. Our key idea is that humans are capable of learning from rich and detailed feedback in the form of comparisons. Our hypothesis is that by conditioning language models on a sequence of generations paired with feedback and training them accordingly, they can learn to identify and correct errors and negative attributes.</p>
<p>Moreover, prior research has underscored the efficacy of pretrained language models for both in context learning and instruction tuning [38, 7, 51, <em>inter alia</em>]. Building upon these insights, we introduce a novel approach: converting all human feedback into a sequence and subsequently finetuning models to comprehend and effectively utilize such feedback. Specifically, we propose finetuning the model to predict outputs while conditioning on one or more model outputs and their corresponding feedback in the form of comparisons to the other outputs.</p>
<p>In essence, our approach finetunes the model by conditioning it to generate outputs while taking into account one or more model-generated outputs and their associated feedback, presented in the form of comparisons to other outputs. During the training phase, the model is given feedback expressions like 'Bad' and 'Good'. It is then tasked with predicting outputs that align more closely with the feedback, such as in the following example: 'How can you explain neural networks to a 6-year-old? Bad: {a subpar answer} Good: {an excellent answer}.' Furthermore, our framework allows for the integration of natural language feedback, such as '{a subpar answer} is a less preferred answer compared with {an excellent answer}', which not only informs the model the preference but also provides additional task-specific guidance. At inference time, when presented with positive feedback indicated by 'Good', the model is guided to generate the desired outputs, thereby ensuring a preferable behavior.</p>
<p>Our proposed approach enables models to learn from both positive and negative feedback, allowing the identification and correction of negative attributes or errors. We name our method Chain of Hindsight (CoH) as it conditions on a sequence of hindsight feedback. We conducted comprehensive evaluations of our approach in the domains of summarization and dialogue tasks, revealing substantial performance enhancements compared to SFT and its various iterations, as well as RLHF, across both automated assessments and human evaluations.</p>
<p>Our main contributions are twofold: (a) We introduce a novel learning framework, referred to as CoH, which effectively harnesses all available feedback data to enhance model performance without necessitating reliance on RLHF. Notably, our approach CoH maintains the same training objective as pretraining, rendering it straightforward to train and readily scalable; (b) We conduct</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Chain of Hindsight (CoH) turns human preferences into rich and detailed feedback in the form of comparisons. In the diagram, we explain this by showing that a question is being prompted to GPT model. The model then generates a multitude of responses, which are subsequently ranked according to human preferences(e.g., A is less preferred compared with B). Subsequently, we construct CoH sequences by converting human preference into natural language feedback and combine them with the model's outputs. These constructed sequences are then employed in the finetuning phase, aligning with the same objectives as in the pretraining phase.
extensive experiments to showcase the effectiveness of our method in comparison to existing baselines, including state-of-the-art RLHF methods.</p>
<h1>2 Chain of Hindsight</h1>
<p>Our goal is to improve the performance of a Transformer-based language model by leveraging human-rated data and feedback, and to achieve this, we propose a novel approach that goes beyond conventional SFT methods and RLHF methods.</p>
<p>Turning all feedback into a sequence. Our approach aims to take into account all feedback and instructions provided by humans. To achieve this, we present the model with a sequence of model generations, along with corresponding feedback and explanations provided by humans. Our approach uses a conventional Transformer model architecture that is causal and decoder-only, as proposed in the work of $[7,46]$ on attention mechanisms. This means that at each timestep, the model can only attend to the past timesteps and itself. Given a text represented by tokens $\mathbf{x}=\left[x_{1}, \cdots, x_{n}\right]$, the standard causal language modeling objective is defined to maximize the log likelihood of $\mathbf{x}$ autoregressively: $\log p(\mathbf{x})=\log \prod_{i=1}^{n} p\left(x_{i} \mid \mathbf{x}_{&lt;i}\right)$. In CoH, we construct $\mathbf{x}$ by combining multiple model outputs with feedback which are then used for instruction finetuning. For instance, when a model is prompted to explain neural networks to a child, it generates multiple responses to the prompt. These responses are then combined together into a sequence and paired with feedback instructions generated based on human ratings. An example is illustrated in Figure 2. During the training phase, the model is presented with both positive and negative feedback denoted as 'Bad' and 'Good', and the model is conditioned to predict outputs that better match the latter feedback such as 'How to explain neural networks to a 6 year old? Bad: {a bad answer} Good: {a good answer}.’. Furthermore, our framework allows for the integration of natural language feedback, such as 'How can you explain neural networks to a 6-year-old? Bad: {a subpar answer} Good: {an excellent answer}’, which provides additional task-specific guidance and context. By incorporating a wider range of diverse positive and negative feedback, it further enhances the model's performance. In this study, we opted for templated feedback generated from ratings rather than open-ended feedback from humans in the loop. The feedback type varies depending on the task, we list the the contextual natural language feedback in Appendix B.</p>
<h2>Natural language feedback examples</h2>
<p>A good summary: {positive}, a worse summary: {negative}
You are a helpful assistant: {positive}, you are an unhelpful assistant: {negative}
A bad answer is ${$ negative $}$, a good answer is ${$ positive $}$
In theory, one could employ open-ended feedback from humans in the loop. However, for this study, we chose to generate feedback using pre-determined templates based on ratings. During the inference</p>
<p>phase, we prompt the model with positive feedback in the form of 'Good' to guide the model in generating favorable outputs.</p>
<p>To enable models to learn from feedback, we require the model to predict each token $x_{i} \in \mathbf{x}$ that are generated by the model. Loss is not applied on other tokens because it hinders model generation at inference time. This is achieved through masking, which can be expressed as: $\log p(\mathbf{x})=\log \prod_{i=1}^{n} \mathbb{1}<em i="i">{O(x)}\left(x</em>\right]}\right) p\left(x_{i} \mid\left[x_{j<em O_x_="O(x)">{j=0}^{i-1}\right)$, where $\mathbb{1}</em>$.}\left(x_{i}\right)$ denotes whether token $x_{i}$ is not part of the hindsight feedback. In other words, it is 1 if $x_{i}$ is not part of the feedback and 0 if it is part of the feedback. The model is trained to predict each non-feedback token $x_{i}$ given the previous tokens $\left[x_{j}\right]_{j=0}^{i-1</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nv">Aligning</span><span class="w"> </span><span class="nv">language</span><span class="w"> </span><span class="nv">models</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">feedback</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">Chain</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">Hindsight</span>.
<span class="w">    </span><span class="nv">Required</span>:<span class="w"> </span><span class="nv">Pretrained</span><span class="w"> </span><span class="nv">Language</span><span class="w"> </span><span class="nv">Model</span><span class="w"> </span><span class="nv">M</span>,<span class="w"> </span><span class="nv">Human</span><span class="w"> </span><span class="nv">Feedback</span><span class="w"> </span><span class="nv">Dataset</span><span class="w"> </span><span class="nv">D</span>
<span class="w">    </span><span class="nv">Required</span>:<span class="w"> </span><span class="nv">Maximum</span><span class="w"> </span><span class="nv">training</span><span class="w"> </span><span class="nv">iterations</span><span class="w"> </span>\<span class="ss">(</span><span class="nv">n</span>\<span class="ss">)</span>
<span class="w">    </span><span class="nv">Initialize</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="ss">(</span>\<span class="nv">operatorname</span>{<span class="nv">iter</span>}<span class="o">=</span><span class="mi">1</span>\<span class="ss">)</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span>\<span class="ss">(</span><span class="nv">n</span>\<span class="ss">)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="nv">Randomly</span><span class="w"> </span><span class="nv">sample</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">minibatch</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span><span class="nv">outputs</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">their</span><span class="w"> </span><span class="nv">associated</span><span class="w"> </span><span class="nv">ratings</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">dataset</span><span class="w"> </span>\<span class="ss">(</span><span class="nv">D</span>\<span class="ss">)</span>.
<span class="w">        </span><span class="nv">Construct</span><span class="w"> </span><span class="nv">training</span><span class="w"> </span><span class="nv">sequences</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">combining</span><span class="w"> </span><span class="nv">sampled</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span><span class="nv">outputs</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">feedback</span><span class="w"> </span><span class="nv">based</span><span class="w"> </span><span class="nv">on</span>
<span class="w">        </span><span class="nv">ratings</span>.
<span class="w">        </span><span class="nv">Instruct</span><span class="w"> </span><span class="nv">finetune</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span>\<span class="ss">(</span><span class="nv">M</span>\<span class="ss">)</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">training</span><span class="w"> </span><span class="nv">sequences</span>.
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<p>Training. We work with a dataset of model outputs and their corresponding human preference, such as positive and negative ratings, from which we sample minibatches of model outputs. To generate hindsight feedback in natural language, we randomly sample a feedback format and incorporate the human ratings. We combine the hindsight feedback and model outputs into a chain of hindsight, which serves as the input for our autoregressive model. The objective is to predict the input sequence autoregressively, and we use cross-entropy loss to optimize the model. We average the loss over each timestep in the last model output sequence. In the regime of human preference learning, the positive and negative data often being similar to each other(e.g., the Anthropic helpful and harmless dataset). Since CoH condition the model on an example when predicting another one, the model can simply 'copy' the example without learning to understand the underlying task. To address this, we randomly mask between $0 \%$ and $5 \%$ of past tokens during training, which help regularize the model and prevent it from overfitting to the specific examples seen during training [44, 30]. In order to retain model's performance on general language modeling tasks, we added a regularization term which maximize the log likelihood of the pretraining dataset following prior works [35]. We apply this technique to our method and all baselines in evaluation. Our approach is shown in Figure 2 and the algorithm is summarized in Algorithm 1.</p>
<h1>2.1 Relation to Prior Paradigms</h1>
<p>We discuss the connections of CoH to prior paradigms of learning from preference data.
Supervised finetuning (SFT). SFT is a commonly used method for preference learning, involving the use of positively labeled data for finetuning [35, 42]. Our approach, however, diverges from SFT by incorporating both positive and non-positive rated data, as well as utilizing feedback input. In comparison to SFT, CoH leverages a broader spectrum of information.
Conditional SFT. This method shares similarities with the Decision Transformer model [8], which involves conditional training of SFT with feedback serving as prefix tokens. In essence, both CoH and Conditional SFT utilize feedback tokens as conditional input. Nonetheless, the distinction lies in CoH' utilization of a sequence of feedback-example pairs, enabling our approach to condition on a more comprehensive information when making predictions.
SFT with unlikelihood. SFT with unlikelihood introduces an unlikelihood loss on negatively rated data [53, 29] to the traditional SFT framework.
Reinforcement learning with human feedback (RLHF). RLHF [42, 35, 45] entails the acquisition of a reward function based on human preferences and the use of reinforcement learning to maximize</p>
<p>this reward. In contrast to RLHF, CoH offers a substantially simpler training process, and as our experimental evaluations will demonstrate, it consistently outperforms RLHF in terms of performance.</p>
<h1>3 Evaluation Setup</h1>
<p>Training Datasets. We use a combination of three datasets for learning from human feedback. The three datasets are:</p>
<ul>
<li>WebGPT. The WebGPT dataset [34] ${ }^{2}$ includes a total of 19,578 comparisons where each example comprises a question, a pair of model answers, and metadata. The answers are rated by humans with a preference score, which helps to identify the better of the two answers.</li>
<li>HH. The Anthropic's Helpful and Harmless (HH) dataset [14, 4] contains human rated dialogues ${ }^{3}$. Each example in this dataset consists of a pair of conversations between a human and a languages model, and one of the two conversations is labeled as preferred by human labelers.</li>
<li>Summarization. The summarization dataset [45] consists of feedback from humans regarding the summarizations generated by a model ${ }^{4}$. Human evaluators were requested to choose the superior summary from two options presented to them.</li>
</ul>
<p>Evaluation Benchmark and Metrics. We consider both automatic evaluation and human evaluation on summarization and dialogue benchmarks.</p>
<ul>
<li>Summarization Benchmark. Following prior RLHF works [45, 34, 4], we consider automatic evaluation and human evaluation on the TL;DRs dataset [47]. The original TL;DR dataset contains about 3 million posts from reddit. com across a variety of topics (subreddits), as well summaries of the posts written by the original poster (TL;DRs). We use the filtered version provided by Stiennon et al. [45], which contains 123,169 posts. We evaluate the performance on the validation set. For evaluation metrics, labelers rated summaries for coverage (how much important information from the original post is covered), accuracy (to what degree the statements in the summary are part of the post), coherence (how easy the summary is to read on its own), and overall quality. More details about evaluation dimensions and instructions for human labelers are available in Appendix A.</li>
<li>Dialogue Benchmark. We also evaluate on the validation split of the Anthropic's Helpful and Harmless (HH) dataset [14, 4], where where each example comprises a pair of conversations between a human and a large language model, with one of the two conversations preferred by a human. For evaluating the dialogue, we consider metrics such as helpfulness and harmlessness. A helpful model should follow instructions and infer intention from a few-shot prompt or another interpretable pattern. Since the intention of a given prompt can be unclear or ambiguous, we rely on judgment from our labelers, and the main metric we use is the labelers' preference ratings.
To collect data for our evaluation, it would be too costly and time-consuming to deploy our finetuned model to chat with humans. Instead, we construct "pseudo" dialogues using positive examples. We replace each model response from a previous dialogue with our model's output, generated by conditioning the model on the human response and past model outputs. We take this approach instead of having humans directly chat with the finetuned model to reuse humangenerated data, as collecting interactive data can be very costly and is prone to low data quality issues. More details about evaluation dimensions and instructions for human labelers are available in Appendix A.</li>
</ul>
<p>Baselines. Our primary baselines are SFT, SFT with unlikelihood (denoted as SFT-U), conditional SFT (denoted as C-SFT), and RLHF, for connections between them and CoH please refer to Section 2.1. We use GPT-J 6B [48] and OPT [57] as the base pretrained models, while other language models can also be used. Following prior works [35, 42], we adopt the PPO algorithm [43] to implement RLHF baseline. We tune the hyperparameters of PPO and reward learning to obtain the best possible results. To ensure a fair comparison, we carefully tune the training hyperparameters for all other baselines.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Evaluation on summarization. Comparison between RLHF, SFT and CoH. The metrics are ROUGE scores on TL;DR summarization task.</p>
<h2>4 Results</h2>
<p>Our main goal in conducting these evaluations is to assess the effectiveness of our proposed methodology, which focuses on summarization and dialogue benchmarks. We conduct both automatic and human evaluations, in order to benchmark our approach against established baselines, including SFT, conditional SFT, SFT with unlikelihood, and RLHF approach [35, 42].</p>
<p>Evaluation on summarization. In Figure 3, we present the ROUGE scores of our models on test set of summarization dataset. Our proposed approach, CoH, substantially outperform baselines, including based pretrained model, SFT, conditional SFT, SFT with unlikelihood, and RLHF. Despite the simplicity of our approach, CoH outperforms RLHF across all the metrics. We notice that RLHF performs the second best, with conditional SFT closely follows behind.</p>
<p>To further evaluate the performance of our proposed approach, we conducted human evaluation as shown in Table 1. Base denotes the pretrained model, SFT-U denotes SFT with unlikelihood, C-SFT denotes conditional SFT. We conducted pairwise comparisons between CoH and the baselines because we found that doing so is an easier task for human labelers compared to evaluating multiple options at the same. We hired 75 human labelers who were proficient in English from a third-party platform to provide ratings. In the pairwise comparison, human labelers were presented with two summaries, one generated by the baseline and the other generated by CoH. They were instructed to select the best (or tie) among the two according to the three metrics mentioned above. The metrics are accuracy, coherency and coverage following prior works [35], we used the same instructions therein, and additional instruct our human labelers to select tie, the full details of human evaluation instructions are provided in Appendix A. Table 1 presents the human evaluation results on summarization task. CoH substantially outperform RLHF and conditional SFT, showcasing the effectiveness of CoH in aligning language models with human preferences.</p>
<p>Table 1: Pairwise human evaluation on summarization task.</p>
<table>
<thead>
<tr>
<th></th>
<th>Human evaluation win rate (%)</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Base</td>
<td>Tie</td>
<td>CoH</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Accuracy</td>
<td>24.5</td>
<td>26.8</td>
<td>48.7</td>
<td>24.2</td>
</tr>
<tr>
<td>Coherence</td>
<td>15.6</td>
<td>18.5</td>
<td>65.9</td>
<td>50.3</td>
</tr>
<tr>
<td>Coverage</td>
<td>19.6</td>
<td>22.4</td>
<td>58.0</td>
<td>38.4</td>
</tr>
<tr>
<td>Average</td>
<td>19.9</td>
<td>22.6</td>
<td>57.5</td>
<td>37.6</td>
</tr>
<tr>
<td></td>
<td>SFT</td>
<td>Tie</td>
<td>CoH</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Accuracy</td>
<td>25.5</td>
<td>32.6</td>
<td>41.9</td>
<td>16.4</td>
</tr>
<tr>
<td>Coherence</td>
<td>30.5</td>
<td>25.6</td>
<td>43.9</td>
<td>13.4</td>
</tr>
<tr>
<td>Coverage</td>
<td>28.5</td>
<td>25.4</td>
<td>46.1</td>
<td>17.6</td>
</tr>
<tr>
<td>Average</td>
<td>28.2</td>
<td>27.9</td>
<td>44.0</td>
<td>15.8</td>
</tr>
<tr>
<td></td>
<td>C-SFT</td>
<td>Tie</td>
<td>CoH</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Accuracy</td>
<td>26.7</td>
<td>34.9</td>
<td>38.4</td>
<td>11.7</td>
</tr>
<tr>
<td>Coherence</td>
<td>32.5</td>
<td>22.9</td>
<td>44.6</td>
<td>12.1</td>
</tr>
<tr>
<td>Coverage</td>
<td>29.5</td>
<td>26.7</td>
<td>43.8</td>
<td>14.3</td>
</tr>
<tr>
<td>Average</td>
<td>29.6</td>
<td>28.2</td>
<td>42.3</td>
<td>12.7</td>
</tr>
<tr>
<td></td>
<td>SFT-U</td>
<td>Tie</td>
<td>CoH</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Accuracy</td>
<td>18.7</td>
<td>17.9</td>
<td>63.4</td>
<td>44.7</td>
</tr>
<tr>
<td>Coherence</td>
<td>21.8</td>
<td>15.8</td>
<td>62.4</td>
<td>40.6</td>
</tr>
<tr>
<td>Coverage</td>
<td>23.6</td>
<td>17.2</td>
<td>59.2</td>
<td>35.6</td>
</tr>
<tr>
<td>Average</td>
<td>21.4</td>
<td>17.0</td>
<td>61.7</td>
<td>40.3</td>
</tr>
<tr>
<td></td>
<td>RLHF</td>
<td>Tie</td>
<td>CoH</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Accuracy</td>
<td>31.8</td>
<td>29.5</td>
<td>38.7</td>
<td>6.9</td>
</tr>
<tr>
<td>Coherence</td>
<td>31.6</td>
<td>20.5</td>
<td>47.9</td>
<td>16.4</td>
</tr>
<tr>
<td>Coverage</td>
<td>28.9</td>
<td>21.9</td>
<td>49.2</td>
<td>20.3</td>
</tr>
<tr>
<td>Average</td>
<td>30.8</td>
<td>24.0</td>
<td>45.3</td>
<td>14.5</td>
</tr>
</tbody>
</table>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup> <sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: The human evaluation results on summarization task. CoH substantially outperform RLHF and conditional SFT, showcasing the effectiveness of CoH in aligning language models with human preferences.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Evaluation on dialogue. Comparing CoH with RLHF and SFT baselines. The metric is the accuracy of classifying the preferred dialogue.</p>
<p>Evaluation on dialogue. We evaluate our method on the HH dataset, by testing its ability to classify which of a dialogue pair is preferred. Figure 4 presents the comparison between baselines and our method. SFT shows substantially improvement over base pretrained model; adding unlikelihood degrades performance which indicates unlikelihood hurts model generation ability; conditional SFT shows improvement over SFT, showcasing the benefit of learning from negative examples; RLHF performs second best and is substantially outperformed by our CoH. The results demonstrate the effectiveness of CoH in learning from preferences. We further evaluate on the dialogue task based on HH dataset. We use the same setting of 75 human labelers and pairwise comparison as in the summarization human evaluation. For this task, we provide human labelers with instructions to evaluate whether the answer is helpful and harmless [4]. The results are presented in Table 2. CoH substantially outperform RLHF and conditional SFT, showcasing the effectiveness of CoH in aligning language models with human preferences.</p>
<p>Language feedback. We enhance the effectiveness of our approach by evaluating its performance in the context of binary feedback alone, as opposed to the combination of binary feedback and fine-grained language feedback, which is the default setting of our method. We denote this baseline without natural language feedback as CoH w/o LF. To assess the performance of these variations, we conducted a human evaluation task focused on the summarization domain, employing the input of 75 human evaluators. The outcomes, as presented in Table 3, show that both our default approach and our 'w/o LF' variant substantially outperform RLHF. In addition, our findings indicate that the inclusion of natural language feedback enhances the results. Human preference ratings show a 14.1% preference for models with language feedback, whereas models without language feedback received an 11.6% preference. The results demonstrate the effectiveness of our CoH framework. Since the framework of CoH offers flexibility to incorporate natural language feedback into training, designing more effective natural language feedback is one of our future directions.</p>
<p>Evaluation on model scaling trend. To assess the efficacy of CoH across various model sizes, we conducted a comprehensive evaluation. The findings in Figure 5 demonstrate the impact of varying model sizes on the performance of the CoH method relative to SFT baselines and RLHF. Notably, for smaller model sizes, CoH exhibits a marginal decrement in performance compared to SFT baselines. However, as the model size increases,</p>
<p>Table 2: Pairwise human evaluation on dialogue task.</p>
<table>
<thead>
<tr>
<th></th>
<th>Human evaluation win rate (%)</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Base</td>
<td>Tie</td>
<td>CoH</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Helpful</td>
<td>15.8</td>
<td>34.8</td>
<td>49.4</td>
<td>33.6</td>
</tr>
<tr>
<td>Harmless</td>
<td>14.5</td>
<td>35.9</td>
<td>49.6</td>
<td>35.1</td>
</tr>
<tr>
<td>Average</td>
<td>15.2</td>
<td>35.3</td>
<td>49.5</td>
<td>34.4</td>
</tr>
<tr>
<td></td>
<td>SFT</td>
<td>Tie</td>
<td>CoH</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Helpful</td>
<td>19.6</td>
<td>45.7</td>
<td>34.7</td>
<td>15.1</td>
</tr>
<tr>
<td>Harmless</td>
<td>18.6</td>
<td>37.4</td>
<td>44.0</td>
<td>25.4</td>
</tr>
<tr>
<td>Average</td>
<td>19.1</td>
<td>41.5</td>
<td>39.4</td>
<td>20.3</td>
</tr>
<tr>
<td></td>
<td>C-SFT</td>
<td>Tie</td>
<td>CoH</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Helpful</td>
<td>21.8</td>
<td>46.9</td>
<td>31.3</td>
<td>9.5</td>
</tr>
<tr>
<td>Harmless</td>
<td>22.4</td>
<td>35.2</td>
<td>42.4</td>
<td>20.0</td>
</tr>
<tr>
<td>Average</td>
<td>22.1</td>
<td>41.0</td>
<td>36.8</td>
<td>14.7</td>
</tr>
<tr>
<td></td>
<td>SFT-U</td>
<td>Tie</td>
<td>CoH</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Helpful</td>
<td>13.4</td>
<td>31.3</td>
<td>55.3</td>
<td>41.9</td>
</tr>
<tr>
<td>Harmless</td>
<td>14.5</td>
<td>28.7</td>
<td>56.8</td>
<td>42.3</td>
</tr>
<tr>
<td>Average</td>
<td>13.9</td>
<td>30.0</td>
<td>56.0</td>
<td>42.1</td>
</tr>
<tr>
<td></td>
<td>RLHF</td>
<td>Tie</td>
<td>CoH</td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Helpful</td>
<td>25.8</td>
<td>40.8</td>
<td>33.4</td>
<td>7.6</td>
</tr>
<tr>
<td>Harmless</td>
<td>20.9</td>
<td>38.8</td>
<td>40.3</td>
<td>19.4</td>
</tr>
<tr>
<td>Average</td>
<td>23.4</td>
<td>39.8</td>
<td>36.9</td>
<td>13.5</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Model scaling trend. Comparing CoH with RLHF and SFT baselines on summarization benchmark with different model sizes. CoH outperforms RLHF, showing strong scaling capabilities.</p>
<p>CoH consistently surpasses all SFT and RLHF baselines and displays a positive scaling trend, indicating its efficacy in enhancing model performance as model complexity increases.</p>
<p>Comparison against ChatGPT distillation. The opensource human preference datasets utilized in this study are curated based on human preferences for model generations. Although these preferences offer valuable learning signals as we have demonstrated in the experiments, the models responsible for these responses are notably less capacble than proprietary models like ChatGPT. As a result, the data quality from these open-source datasets falls short when compared to conversations between ChatGPT and users which is shared online on ShareGPT. Given that the ShareGPT data showcases superior quality and greater diversity than the open-source datasets, we are interested in how our approach CoH performs when applied to opensource human preference datasets, in comparison to the SFT approach used on ShareGPT data. To this end, we compared with Koala [17] which involves supervised finetuning using ShareGPT data. It's worth noting that we maintained consistency in the model and training hyperparameters for both SFT and COH when applied to open-source datasets. Additionally, we integrated CoH with Koala by finetuning both the ShareGPT and open-source datasets; here, the open-source datasets provided both positive and negative examples, while ShareGPT contributed solely positive examples. We use the same human evaluation as Koala by hiring third-party human labelers to conduct pairwise comparisons of responses generated by various models. These evaluations were based on questions sourced from a holdout set exclusive to ShareGPT. Results presented in Figure 6 reveal that our approach CoH is on par with Koala in performance. Moreover, the combined approach of CoH +Koala surpasses Koala based on human ratings. Meanwhile, both C-SFT (conditional SFT) and SFT lag behind Koala considerably. This underscores the efficacy of CoH in leveraging human preferences for learning.</p>
<h1>5 Related Work</h1>
<p>Learning from hindsight. In this paper we explore learning from chains of hindsight with human feedback, an approach that enables a model to learn from errors and revise generations. The key idea of learning from hindsight experience was explored in goal conditioned RL [20, 1, 40]. Andrychowicz et al. [1] proposes hindsight experience replay (HER) to relabel rewards and transitions retroactively to learn from sparse feedback. While HER relies on reinforcement learning and a distance function to learn from hindsight experience, we propose a new method called CoH that constructs a chain of hindsight experience using human feedback and finetunes the model directly. Our approach offers several advantages over other methods, such as HIR [58], which also makes use of incorrect model</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Evaluating various approaches with open source human preference datasets in comparison to ShareGPT's supervised finetuned Koala.
outputs. HIR can be seen as a special case of CoH with a length of one chain-of-hindsight. Unlike HIR, which employs a complex training process involving likelihood loss, contrastive loss, and entropy loss, our approach is straightforward and easy to implement. Concurrently, Korbak et al. [23] studies conditioning on human preference during pretraining and shows improved performance in aligning language models with human preference. Their method is similar to CoH with a length of one chain-of-hindsight. Our work focuses on finetuning pretrained language models while Korbak et al. [23] focuses on improving pretraining.
Learning from human feedback. Prior work have explored using human feedback to improve various tasks, such as summarization [6, 60, 45], dialogue [55, 18, 4, 5, 2, 41], translation [24, 3], semantic parsing [26], story generation [59], review generation [9], evidence extraction [36], and instruction following [35, 4]. The main techniques behind them can be categorized as supervised finetuning (SFT) or training on filtered human annotations and learning a reward function from human feedback for reinforcement learning, which is often dubbed as RLHF [10, 32, 27, 50] and has been used to train RL agents without the need for hand-designed rewards. Ouyang et al. [35] demonstrates improved language model alignment performance by training models with SFT and RLHF using human feedback. Our work belongs to the category of SFT, and differs from SFT in that our method conditions on feedback and can learn from examples without positive ratings. Our method is complementary to RLHF and can be directly combined together for further improvement. Using instructions to provide models with human preference and desired behaviors is demonstrated in Bai et al. [5], where models are prompted with a set of statements/principles and are trained with RLHF. In our work, we provide models with a sequence of model outputs and their feedback and train models to generate desired outputs conditioned on feedback/control tokens.
Instruction finetuning and conditional training. Finetuning on chain of hindsight using human feedback is akin to instruction finetuning. Driven by the impressive in-context learning ability of large language models, finetuning pretrained models on instructions has been shown to improve language models in many benchmarks [see e.g. 49, 33, 54, 11, 51, 39, 56, 19, inter alia]. Mostly the instructions are reformatted examples from NLP benchmarks [e.g. 51, 11]. CoT prompts [52] are widely considered as instructions in prior works [11, 51], specifically in the form of step by step explanations written by humans. In relation to these, our chain of hindsight consists of human written hindsight feedback and ranked model outputs. Conditional training [21, 13, 25, 8, 12, 31] explores conditioning the model on some control tokens for controllable generations. In relation to it, CoH generalizes to condition on a sequence of control tokens instead of one control token. By doing so, CoH enables the model to understand the differences between control tokens and their corresponding outputs. Our work suggests a promising direction of using hindsight feedback to construct instructions from model outputs, and can be combined with prior instruction finetuning and conditional training works for further improvements.</p>
<h1>6 Conclusion</h1>
<p>In conclusion, we introduce Chain of Hindsight (CoH), which is inspired by how humans learn from rich feedback in the form of comparison. We condition language models on a sequence of hindsight feedback, allowing them to effectively leverage all examples regardless of their preference</p>
<p>score. Extensive experiments on summarization and dialogue datasets show that CoH substantially outperform RLHF and other baselines.</p>
<p>Limitations and Future Work. Although our method substantially outperform baselines, it does have some limitations that need to be addressed:</p>
<ul>
<li>Constructing CoH may result in long sequences, particularly with multiple feedback instances, leading to increased training computational expenses.</li>
<li>Our work heavily relies on hired human labelers for evaluation due to their higher reliability compared to automated metrics. However, this approach incurs substantial costs, although this issue is not unique to our method.</li>
</ul>
<p>In terms of future prospects, our CoH-based training from human feedback opens the door to exciting possibilities, such as integrating external environment feedback like unit tests and extending its applicability to various domains. Furthermore, our current focus on learning from hindsight using preexisting preferences paves the way for exploration in online preference learning, enabling iterative model improvements.</p>
<h1>Acknowledgments</h1>
<p>This project is supported in part by Office of Naval Research grant N00014-21-1-2769 and SNSF Postdoc Mobility Fellowship and ONR MURI N00014-22-1-2773. We express our gratitude to the BAIR communities for their insightful discussions and feedback. We thank Google TPU Research Cloud for granting us access to TPUs.</p>
<h2>References</h2>
<p>[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.
[2] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.
[3] Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.
[4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.
[5] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.
[6] Florian Böhm, Yang Gao, Christian M Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. Better rewards yield better summaries: Learning to summarise without references. arXiv preprint arXiv:1909.01214, 2019.
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[8] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084-15097, 2021.</p>
<p>[9] Woon Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xiujun Li, Michel Galley, Chris Brockett, Mengdi Wang, and Jianfeng Gao. Towards coherent and cohesive long-form text generation. arXiv preprint arXiv:1811.00511, 2018.
[10] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, pages 4299-4307, 2017.
[11] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
[12] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv: Arxiv-1805.04833, 2018.
[13] Jessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation. arXiv preprint arXiv: Arxiv-1707.02633, 2017.
[14] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022.
[15] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB dataset of diverse text for language modeling. Computing Research Repository, arXiv:2101.00027, 2020. version 1.
[16] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835-10866. PMLR, 2023.
[17] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April, 1, 2023.
[18] Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415, 2019.
[19] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.
[20] Leslie Pack Kaelbling. Learning to achieve goals. In IJCAI, volume 2, pages 1094-8. Citeseer, 1993.
[21] Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional transformer language model for controllable generation. PREPRINT, 2019.
[22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[23] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences. arXiv preprint arXiv:2302.08582, 2023.
[24] Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, and Stefan Riezler. Can neural machine translation be improved with user feedback? arXiv preprint arXiv:1804.05958, 2018.
[25] Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, Maxime Gazeau, Himanshu Sahni, Satinder Singh, and Volodymyr Mnih. In-context reinforcement learning with algorithm distillation. arXiv preprint arXiv: Arxiv-2210.14215, 2022.
[26] Carolin Lawrence and Stefan Riezler. Improving a neural semantic parser by counterfactual learning from human bandit feedback. arXiv preprint arXiv:1805.01252, 2018.</p>
<p>[27] Kimin Lee, Laura Smith, and P. Abbeel. Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. International Conference On Machine Learning, 2021.
[28] Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, et al. Evaluating human-language model interaction. arXiv preprint arXiv:2212.09746, 2022.
[29] Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck, Y-Lan Boureau, Kyunghyun Cho, and Jason Weston. Don't say that! making inconsistent dialogue unlikely with unlikelihood training. arXiv preprint arXiv:1911.03860, 2019.
[30] Hao Liu, Xinyang Geng, Lisa Lee, Igor Mordatch, Sergey Levine, Sharan Narang, and Pieter Abbeel. Fcm: Forgetful causal masking makes causal language models better zero-shot learners. arXiv preprint arXiv:2210.13432, 2022.
[31] Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. QUARK: Controllable text generation with reinforced unlearning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id= 5ffa1ds3ux50.
[32] J. MacGlashan, Mark K. Ho, R. Loftin, Bei Peng, Guan Wang, David L. Roberts, Matthew E. Taylor, and M. Littman. Interactive learning from policy-dependent human feedback. International Conference On Machine Learning, 2017.
[33] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021.
[34] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
[35] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.
[36] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q\&amp;a models. arXiv preprint arXiv:1909.05863, 2019.
[37] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openaiassets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.
[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[39] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.
[40] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International conference on machine learning, pages 1312-1320. PMLR, 2015.
[41] Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback. arXiv preprint arXiv: Arxiv-2204.14146, 2022.</p>
<p>[42] J. Schulman, B. Zoph, C. Kim, J. Hilton, J. Menick, J. Weng, J. F. C. Uribe, L. Fedus, L. Metz, M. Pokorny, R. G. Lopes, S. Zhao, A. Vijayvergiya, E. Sigler, A. Perelman, C. Voss, M. Heaton, J. Parish, D. Cummings, R. Nayak, V. Balcom, D. Schnurr, T. Kaftan, C. Hallacy, N. Turley, N. Deutsch, and V. Goel. Chatgpt: Optimizing language models for dialogue. OpenAI Blog, 2022. URL https://openai.com/blog/chatgpt.
[43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[44] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15:1929-1958, 2014.
[45] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.
[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, pages 5998-6008. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
[47] Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages 59-63, 2017.
[48] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
[49] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. URL https://arxiv. org/abs/2204.07705, 2022.
[50] Garrett Warnell, Nicholas R. Waytowich, V. Lawhern, and P. Stone. Deep tamer: Interactive agent shaping in high-dimensional state spaces. Aaai Conference On Artificial Intelligence, 2017. doi: 10.1609/aaai.v32i1.11485.
[51] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
[52] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[53] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.
[54] Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossfit: A few-shot learning challenge for cross-task generalization in nlp. arXiv preprint arXiv:2104.08835, 2021.
[55] Sanghyun Yi, Rahul Goel, Chandra Khatri, Alessandra Cervone, Tagyoung Chung, Behnam Hedayatnia, Anu Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tur. Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators. arXiv preprint arXiv:1904.13015, 2019.
[56] Eric Zelikman, Jesse Mu, Noah D Goodman, and Yuhuai Tony Wu. Star: Self-taught reasoner bootstrapping reasoning with reasoning. NeurIPS, 2022.</p>
<p>[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models. arXiv preprint arXiv: Arxiv-2205.01068, 2022.
[58] Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph E. Gonzalez. The wisdom of hindsight makes language models better instruction followers. arXiv preprint arXiv: Arxiv-2302.05206, 2023.
[59] Wangchunshu Zhou and Ke Xu. Learning to compare for better training and evaluation of open domain natural language generation models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9717-9724, 2020.
[60] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.</p>
<p>What makes for a good summary? Roughly speaking, a good summary is a shorter piece of text that has the essence of the original - tries to accomplish the same purpose and conveys the same information as the original post. We would like you to consider these different dimensions of summaries:</p>
<h1>Accuracy</h1>
<p>For this axis, answer the question "does the factual information in the summary accurately match the post?" A summary is accurate if it doesn't say things that aren't in the article, it doesn't mix up people, and generally is not misleading.</p>
<h2>Coherence</h2>
<p>For this axis, answer the question "how coherent is the summary on its own?" A summary is coherent if, when read by itself, it's easy to understand and free of English errors. A summary is not coherent if it's difficult to understand what the summary is trying to say. Generally, it's more important that the summary is understandable than it being free of grammar errors.</p>
<h2>Coverage</h2>
<p>For this axis, answer the question "how well does the summary cover the important information in the post?" A summary has good coverage if it mentions the main information from the post that's important to understand the situation described in the post. A summary has poor coverage if someone reading only the summary would be missing several important pieces of information about the situation in the post. A summary with good coverage should also match the purpose of the original post (e.g. to ask for advice).</p>
<p>Table 4: Instructions we provided to human labelers on evaluating summarization quality.</p>
<p>What makes for a good dialogue? Roughly speaking, a good dialogue is a conversation that is both helpful and harmless. We would like you to consider these different dimensions of dialogues:</p>
<h2>Helpful</h2>
<p>For this axis, assess whether the conversation promotes active listening and open-mindedness. Helpful dialogues typically encourage the exchange of diverse perspectives and ideas. Secondly, look for evidence of empathy and respect in the dialogue; a constructive conversation should foster an atmosphere where all parties feel valued and understood. Thirdly, examine whether the dialogue leads to tangible outcomes or solutions to the issues being discussed. A helpful dialogue often results in progress or resolution, rather than perpetuating conflicts or misunderstandings.</p>
<h2>Harmless</h2>
<p>For this axis, response to questions should not contain potentially harmful information such as "information on building a bomb". Consider the overall tone and language used during the conversation; a helpful dialogue should be free from hostility, personal attacks, or derogatory remarks. A dialogue is harmless if it does not contain any unsafe or potentially harmful information.</p>
<p>Table 5: Instructions we provided to human labelers on evaluating dialogue quality.</p>
<h2>A Human Evaluation Instructions</h2>
<p>For our human evaluations, we provide instructions and metrics definition to the human labelers, asking them to select the preferred output. In order to maintain consistency and build upon prior research [45, 4], we adopt their instructions and definitions of helpfulness, usefulness, and other relevant criteria.</p>
<p>Specifically, the instructions employed in our summarization benchmark are derived from Stiennon et al. [45]. Similarly, for the dialogue task, we derive the instructions based on Bai et al. [4]. Table 4 provides more detail on the specific instructions given to labelers for comparing summaries, and Table 5 lists our instructions for evaluating dialogues.</p>
<p>Table 6: Examples of Natural language feedback. The task prompts are omitted for simplicity.</p>
<table>
<thead>
<tr>
<th>Source</th>
<th>Examples of natural language feedback</th>
</tr>
</thead>
<tbody>
<tr>
<td>Summary</td>
<td>a good summary is: {positive} a bad summary is: {negative}</td>
</tr>
<tr>
<td>Summary</td>
<td>a bad summary is: {negative} a good summary is: {positive}</td>
</tr>
<tr>
<td>Summary</td>
<td>a good summary is: {positive} a worse summary is: {negative}</td>
</tr>
<tr>
<td>Summary</td>
<td>a bad summary is: {negative} a better summary is: {positive}</td>
</tr>
<tr>
<td>Shared</td>
<td>a good response is: {positive} a bad response is: {negative}</td>
</tr>
<tr>
<td>Shared</td>
<td>a bad response is: {negative} a good response is: {positive}</td>
</tr>
<tr>
<td>Shared</td>
<td>a good answer is: {positive} a bad answer is: {negative}</td>
</tr>
<tr>
<td>Shared</td>
<td>a bad answer is: {negative} a good answer is: {positive}</td>
</tr>
<tr>
<td>Shared</td>
<td>a good answer is: {positive} a worse answer is: {negative}</td>
</tr>
<tr>
<td>Shared</td>
<td>a bad answer is: {negative} a better answer is: {positive}</td>
</tr>
<tr>
<td>Shared</td>
<td>good: {positive} worse: {negative}</td>
</tr>
<tr>
<td>Shared</td>
<td>bad: {negative} better: {positive}</td>
</tr>
<tr>
<td>Shared</td>
<td>good: {positive} bad: {negative}</td>
</tr>
<tr>
<td>Shared</td>
<td>bad: {positive} good: {negative}</td>
</tr>
<tr>
<td>Dialogue</td>
<td>you are a helpful assistant: {positive} you are an unhelpful assistant: {negative}</td>
</tr>
<tr>
<td>Dialogue</td>
<td>you are an unhelpful assistant: {positive} you are a helpful assistant: {negative}</td>
</tr>
<tr>
<td>Dialogue</td>
<td>you are a respectful and unbiased assistant: {positive} you are a disrespectful and biased assistant: {negative}</td>
</tr>
<tr>
<td>Dialogue</td>
<td>you are a disrespectful and biased assistant: {positive} you are a respectful and unbiased assistant: {negative}</td>
</tr>
<tr>
<td>Summary</td>
<td>give me a good summary: {positive} give me a worse summary: {negative}</td>
</tr>
<tr>
<td>Summary</td>
<td>give me a bad summary: {negative} give me a better summary: {positive}</td>
</tr>
<tr>
<td>Summary</td>
<td>let's generate a good summary: {positive} let's generate a worse summary: {negative}</td>
</tr>
<tr>
<td>Summary</td>
<td>let's generate a bad summary: {negative} let's generate a better summary: {positive}</td>
</tr>
<tr>
<td>Shared</td>
<td>let's generate a good answer: {positive} let's generate a worse answer: {negative}</td>
</tr>
<tr>
<td>Shared</td>
<td>let's generate a bad answer: {negative} let's generate a better answer: {positive}</td>
</tr>
</tbody>
</table>
<h1>B Natural Language Feedback</h1>
<p>During inference time, we only employ simple positive tokens, while during training, we explored the incorporation of natural language feedback that carries more semantic meaning. This natural feedback is tailored to the specific task and offers increased diversity, as illustrated in Table 6.</p>
<h2>C Hyperparameters</h2>
<p>All models are trained with the Adam [22] optimizer, with $\beta_{1}=0.9, \beta_{2}=0.95$, and an epsilon of $1.0 \mathrm{e}-8$. The batch size for human feedback data is set to 512 , while for pretraining data it is set to 2048. The value of $\lambda$ is 1.5 , which determines the relative strength of gradients from the human feedback dataset and the pretraining dataset. The pretraining regularization term is computed using the Pile dataset [15]. Since we applied random past token masking, dropout is not used in our experiments, as suggested by Liu et al. [30]. When finetuning, we combined three human feedback datasets, and the data was sampled proportionally to their size to ensure balance across the datasets.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Screenshots of our labeling interface for rating dialog. For each metric, labelers are asked to choose preferred dialog.</p>
<div class="codehilite"><pre><span></span><code><span class="n">For</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">article</span>
<span class="o">=======</span>
<span class="p">[</span><span class="n">Article</span><span class="p">]</span>
<span class="o">=======</span>
<span class="n">Please</span><span class="w"> </span><span class="n">select</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">preferred</span><span class="w"> </span><span class="n">summary</span><span class="w"> </span><span class="n">according</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">metrics</span><span class="p">.</span>
<span class="o">*****</span>
<span class="p">[</span><span class="n">Summary</span><span class="w"> </span><span class="n">A</span><span class="p">]</span>
<span class="o">****</span>
<span class="o">*****</span>
<span class="p">[</span><span class="n">Summary</span><span class="w"> </span><span class="n">B</span><span class="p">]</span>
<span class="o">****</span>
<span class="nl">Metric</span><span class="p">:</span><span class="w"> </span><span class="n">Accuracy</span><span class="p">.</span><span class="w"> </span><span class="n">Which</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">better</span><span class="o">?</span>
<span class="n">O</span><span class="w"> </span><span class="n">A</span>
<span class="n">O</span><span class="w"> </span><span class="n">B</span>
<span class="n">O</span><span class="w"> </span><span class="n">Neutral</span>
<span class="w">    </span><span class="n">submit</span>
<span class="nl">Metric</span><span class="p">:</span><span class="w"> </span><span class="n">Coherence</span><span class="p">.</span><span class="w"> </span><span class="n">Which</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">better</span><span class="o">?</span>
<span class="n">O</span><span class="w"> </span><span class="n">A</span>
<span class="n">O</span><span class="w"> </span><span class="n">B</span>
<span class="n">O</span><span class="w"> </span><span class="n">Neutral</span>
<span class="w">    </span><span class="n">submit</span>
<span class="nl">Metric</span><span class="p">:</span><span class="w"> </span><span class="n">Coverage</span><span class="p">.</span><span class="w"> </span><span class="n">Which</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">better</span><span class="o">?</span>
<span class="n">O</span><span class="w"> </span><span class="n">A</span>
<span class="n">O</span><span class="w"> </span><span class="n">B</span>
<span class="n">O</span><span class="w"> </span><span class="n">Neutral</span>
<span class="w">    </span><span class="n">submit</span>
</code></pre></div>

<p>Figure 8: Screenshots of our labeling interface for rating summary. For each metric, labelers are asked to choose preferred summary.</p>
<h1>D Human evaluation web interface</h1>
<p>In Figure 8 and Figure 7, we show screenshots of our labeling interface, that all of our labelers use to rate data. Labelers can choose the preferred model output or choose tie in cases where two outputs seem to be of similar quality.</p>
<h1>E Additional Experimental Results</h1>
<h2>E. 1 Evaluation on Controllable Generation</h2>
<p>The controllable generation results are presented in Figure 9. The models are provided with three instructions to generate summaries of desired quality. The first instruction asks for a standard summary, while the second and third instructions ask for improved summaries conditioned on the previous summary generated by the model. We compare the performance of CoH with that of the RLHF model. The results indicate that while RLHF performs well in modeling human preferences and generates high-scoring summaries by following the first instruction, it fails to follow the second and third instructions, which implies that it cannot comprehend human intentions. On the other hand, the CoH-trained model is capable of understanding the intention of the instructions and generates better summaries in the second and third trials. We note that the controllable generation technique can be further investigated in various evaluation settings to enhance performance.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Controllable generation. (left): RLHF cannot follow instructions to generate improved summary. (middle): After finetuning on CoH , the model follows instructions to achieve controllable generations. (right): First instruction is standard, while second and third instructions ask for improved summaries.</p>
<h2>E. 2 Alignment Tax</h2>
<p>We conducted an evaluation on a diverse set of few-shot tasks that are commonly used in previous studies [7, 48] to assess the effectiveness of aligning models with human preferences. We use Language Model Evaluation Harness ${ }^{5}$ for evaluation. The results are reported in Table 7. Interestingly, we found that the average performance of models that were finetuned using SFT decreased after alignment. This decrease could be attributed to the issue known as alignment tax in language models [35], which underscores the importance of human evaluation [28]. On the other hand, our proposed method, CoH , showed moderate improvements over both the pretrained model and supervised fine-tuned model. This result suggests that CoH is less susceptible to the alignment tax issue.</p>
<h2>F Qualitative Examples</h2>
<p>Table 8 and Table 9 show qualitative examples of summaries generated by GPT-J and CoH finetuned GPT-J. The examples are sampled from the validation split of dataset from Stiennon et al. [45] which is based on TL;DR Reddit dataset [47].</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 7: Alignment Tax on Few-Shot Benchmark: The results of our experiments on few-shot NLP benchmarks using the Language Model Evaluation Harness are presented in Table 7. We follow the same setup as in previous work [7, 48], including the splits for each task. The reported numbers for GPT-J are taken from its original paper, while the numbers for other models are reported by us. We average the results over 5 random seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Zero-shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">One-shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Few-shot</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">CoH</td>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">CoH</td>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">CoH</td>
</tr>
<tr>
<td style="text-align: center;">ANLJ R1</td>
<td style="text-align: center;">34.00</td>
<td style="text-align: center;">33.50</td>
<td style="text-align: center;">33.80</td>
<td style="text-align: center;">33.50</td>
<td style="text-align: center;">33.50</td>
<td style="text-align: center;">33.60</td>
<td style="text-align: center;">32.70</td>
<td style="text-align: center;">32.60</td>
<td style="text-align: center;">32.70</td>
</tr>
<tr>
<td style="text-align: center;">ANLJ R2</td>
<td style="text-align: center;">32.00</td>
<td style="text-align: center;">32.00</td>
<td style="text-align: center;">32.10</td>
<td style="text-align: center;">34.40</td>
<td style="text-align: center;">34.10</td>
<td style="text-align: center;">34.20</td>
<td style="text-align: center;">33.90</td>
<td style="text-align: center;">34.20</td>
<td style="text-align: center;">34.10</td>
</tr>
<tr>
<td style="text-align: center;">ANLJ R3</td>
<td style="text-align: center;">34.00</td>
<td style="text-align: center;">34.30</td>
<td style="text-align: center;">36.80</td>
<td style="text-align: center;">34.80</td>
<td style="text-align: center;">34.60</td>
<td style="text-align: center;">36.90</td>
<td style="text-align: center;">35.40</td>
<td style="text-align: center;">35.60</td>
<td style="text-align: center;">36.80</td>
</tr>
<tr>
<td style="text-align: center;">ARC-C</td>
<td style="text-align: center;">27.00</td>
<td style="text-align: center;">26.80</td>
<td style="text-align: center;">27.60</td>
<td style="text-align: center;">32.20</td>
<td style="text-align: center;">32.50</td>
<td style="text-align: center;">33.80</td>
<td style="text-align: center;">33.10</td>
<td style="text-align: center;">33.50</td>
<td style="text-align: center;">34.20</td>
</tr>
<tr>
<td style="text-align: center;">ARC-E</td>
<td style="text-align: center;">54.30</td>
<td style="text-align: center;">54.20</td>
<td style="text-align: center;">54.40</td>
<td style="text-align: center;">62.80</td>
<td style="text-align: center;">62.50</td>
<td style="text-align: center;">62.50</td>
<td style="text-align: center;">66.50</td>
<td style="text-align: center;">66.50</td>
<td style="text-align: center;">66.50</td>
</tr>
<tr>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">58.50</td>
<td style="text-align: center;">61.50</td>
<td style="text-align: center;">61.30</td>
<td style="text-align: center;">57.20</td>
<td style="text-align: center;">57.10</td>
<td style="text-align: center;">58.10</td>
<td style="text-align: center;">42.50</td>
<td style="text-align: center;">42.30</td>
<td style="text-align: center;">42.90</td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">41.10</td>
<td style="text-align: center;">41.00</td>
<td style="text-align: center;">40.50</td>
<td style="text-align: center;">41.10</td>
<td style="text-align: center;">41.10</td>
<td style="text-align: center;">40.50</td>
<td style="text-align: center;">42.90</td>
<td style="text-align: center;">42.10</td>
<td style="text-align: center;">42.00</td>
</tr>
<tr>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">71.00</td>
<td style="text-align: center;">70.50</td>
<td style="text-align: center;">69.90</td>
<td style="text-align: center;">80.00</td>
<td style="text-align: center;">80.10</td>
<td style="text-align: center;">80.50</td>
<td style="text-align: center;">82.00</td>
<td style="text-align: center;">82.20</td>
<td style="text-align: center;">81.50</td>
</tr>
<tr>
<td style="text-align: center;">HeadQA</td>
<td style="text-align: center;">23.50</td>
<td style="text-align: center;">23.00</td>
<td style="text-align: center;">23.80</td>
<td style="text-align: center;">24.00</td>
<td style="text-align: center;">23.80</td>
<td style="text-align: center;">24.30</td>
<td style="text-align: center;">23.90</td>
<td style="text-align: center;">22.50</td>
<td style="text-align: center;">22.80</td>
</tr>
<tr>
<td style="text-align: center;">HellaSwag</td>
<td style="text-align: center;">42.60</td>
<td style="text-align: center;">42.30</td>
<td style="text-align: center;">42.00</td>
<td style="text-align: center;">46.20</td>
<td style="text-align: center;">46.10</td>
<td style="text-align: center;">46.10</td>
<td style="text-align: center;">46.10</td>
<td style="text-align: center;">46.00</td>
<td style="text-align: center;">46.70</td>
</tr>
<tr>
<td style="text-align: center;">MultiRC</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">3.10</td>
<td style="text-align: center;">4.10</td>
<td style="text-align: center;">6.50</td>
<td style="text-align: center;">6.70</td>
<td style="text-align: center;">7.40</td>
<td style="text-align: center;">6.60</td>
<td style="text-align: center;">6.90</td>
<td style="text-align: center;">7.50</td>
</tr>
<tr>
<td style="text-align: center;">ReCORD</td>
<td style="text-align: center;">85.80</td>
<td style="text-align: center;">85.60</td>
<td style="text-align: center;">85.60</td>
<td style="text-align: center;">86.20</td>
<td style="text-align: center;">86.00</td>
<td style="text-align: center;">86.40</td>
<td style="text-align: center;">58.60</td>
<td style="text-align: center;">58.80</td>
<td style="text-align: center;">58.60</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">51.20</td>
<td style="text-align: center;">50.50</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">55.60</td>
<td style="text-align: center;">55.50</td>
<td style="text-align: center;">55.90</td>
<td style="text-align: center;">52.00</td>
<td style="text-align: center;">52.00</td>
<td style="text-align: center;">52.00</td>
</tr>
<tr>
<td style="text-align: center;">WiC</td>
<td style="text-align: center;">45.00</td>
<td style="text-align: center;">45.00</td>
<td style="text-align: center;">45.00</td>
<td style="text-align: center;">44.50</td>
<td style="text-align: center;">44.20</td>
<td style="text-align: center;">44.10</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">50.50</td>
<td style="text-align: center;">50.00</td>
</tr>
<tr>
<td style="text-align: center;">WSC</td>
<td style="text-align: center;">36.50</td>
<td style="text-align: center;">36.90</td>
<td style="text-align: center;">42.80</td>
<td style="text-align: center;">37.50</td>
<td style="text-align: center;">38.10</td>
<td style="text-align: center;">43.70</td>
<td style="text-align: center;">35.80</td>
<td style="text-align: center;">37.60</td>
<td style="text-align: center;">41.30</td>
</tr>
<tr>
<td style="text-align: center;">LAMBADA <br> (openai)</td>
<td style="text-align: center;">5.50</td>
<td style="text-align: center;">5.70</td>
<td style="text-align: center;">5.70</td>
<td style="text-align: center;">5.30</td>
<td style="text-align: center;">5.40</td>
<td style="text-align: center;">5.40</td>
<td style="text-align: center;">2.50</td>
<td style="text-align: center;">2.70</td>
<td style="text-align: center;">3.60</td>
</tr>
<tr>
<td style="text-align: center;">LAMBADA <br> (standard)</td>
<td style="text-align: center;">2.10</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">2.20</td>
<td style="text-align: center;">1.90</td>
<td style="text-align: center;">3.20</td>
<td style="text-align: center;">3.30</td>
<td style="text-align: center;">3.30</td>
</tr>
<tr>
<td style="text-align: center;">LogiQA</td>
<td style="text-align: center;">21.50</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">20.70</td>
<td style="text-align: center;">20.90</td>
<td style="text-align: center;">20.90</td>
<td style="text-align: center;">19.00</td>
<td style="text-align: center;">20.60</td>
<td style="text-align: center;">20.10</td>
</tr>
<tr>
<td style="text-align: center;">WineGrande</td>
<td style="text-align: center;">49.70</td>
<td style="text-align: center;">50.40</td>
<td style="text-align: center;">51.20</td>
<td style="text-align: center;">50.70</td>
<td style="text-align: center;">51.80</td>
<td style="text-align: center;">53.50</td>
<td style="text-align: center;">50.70</td>
<td style="text-align: center;">51.10</td>
<td style="text-align: center;">52.80</td>
</tr>
<tr>
<td style="text-align: center;">SciQ</td>
<td style="text-align: center;">86.40</td>
<td style="text-align: center;">86.00</td>
<td style="text-align: center;">86.00</td>
<td style="text-align: center;">89.10</td>
<td style="text-align: center;">89.10</td>
<td style="text-align: center;">89.10</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">55.00</td>
<td style="text-align: center;">55.00</td>
</tr>
<tr>
<td style="text-align: center;">OpenBookQA</td>
<td style="text-align: center;">16.00</td>
<td style="text-align: center;">16.20</td>
<td style="text-align: center;">15.40</td>
<td style="text-align: center;">16.80</td>
<td style="text-align: center;">16.70</td>
<td style="text-align: center;">16.70</td>
<td style="text-align: center;">20.80</td>
<td style="text-align: center;">20.90</td>
<td style="text-align: center;">21.10</td>
</tr>
<tr>
<td style="text-align: center;">PIQA</td>
<td style="text-align: center;">72.40</td>
<td style="text-align: center;">72.40</td>
<td style="text-align: center;">72.00</td>
<td style="text-align: center;">73.60</td>
<td style="text-align: center;">73.70</td>
<td style="text-align: center;">73.50</td>
<td style="text-align: center;">74.20</td>
<td style="text-align: center;">74.00</td>
<td style="text-align: center;">74.00</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">40.60</td>
<td style="text-align: center;">40.54</td>
<td style="text-align: center;">40.95</td>
<td style="text-align: center;">42.53</td>
<td style="text-align: center;">42.53</td>
<td style="text-align: center;">43.14</td>
<td style="text-align: center;">39.38</td>
<td style="text-align: center;">39.59</td>
<td style="text-align: center;">39.98</td>
</tr>
</tbody>
</table>
<h1>Example 1</h1>
<p>Text
I don't know how to ask this without it sounding really silly, but this is a genuine problem. I created a throwaway because of too much personal detail in my main account. My boyfriend and I are both grad students and have been together for four months now. I absolutely love him. He's sweet, caring, tolerant, liberal, basically every single thing on my 'checklist', if I had one, is there. The problem, though, is he talks VERY slowly. I mean his speed is really slow. If I ask him anything, he takes about 3 seconds before he starts to talk, then when he starts, the words go at a very slow pace,and he even pauses for 3-5 seconds in the middle of his sentence. I get really impatient and sometimes just feel like telling him to hurry up (of course I don't, but I really feel like doing so). I asked him today, very gently, why he spoke so slow, and he said he just gets thoughts very slowly in his brain. He then went on to say he's just generally slow at thinking. I felt really sad when he said this, I don't know why. He's a Math PhD student,and we are at a pretty good school, and his research has been going slow for a while now (but this is totally normal in grad school), so I think that response was kind of a self-deprecating one since he's already low on self-esteem. Anyway, my point is, his speed of talking is just a part of him, it can't be changed. I love him so much otherwise but I just want to know how to deal with this. Conversation is so important in a relationship! How do we manage? He's an introvert and talks very little in general; I suspect this has something to do with his pace of talking?
GPT-J summary
A woman is upset because her boyfriend talks too slowly, and she doesn't know what to do. She is bothered by his slow speech, and she thinks it might be due to his low self-esteem and introverted nature.
CoH-tuned GPT-J
A woman is in love with her boyfriend who meets all her criteria except he speaks very slowly. He explains that he gets thoughts slowly and is slow at thinking. She wants to know how to manage this in their relationship as conversation is important to her.</p>
<h2>Example 2</h2>
<h2>Text</h2>
<p>This is my first post so please be kind :) I know that lots of people often feel confused when they come out of a long-term relationship. They think they have forgotten how to be single, or how to flirt/date. I am one of these people. The problem is, my relationship started when I had just turned 16. I have never been single - as an adult. That might sound silly. But the only time I have ever flirted or dated was as an over-confident, hormone-riddled teenager. Now I have a pretty demanding job, responsibilities blah blah... And I just don't know how to this! I'm no way in a rush to get into a new relationship, but that doesn't mean I want to be completely alone in the mean time. If anyone has experienced anything similar, or just generally has some advice, it would be greatly appreciated!
GPT-J summary
Someone is struggling after coming out of a long-term relationship that started when they were 16.
CoH-tuned GPT-J
A person is seeking advice after coming out of a long-term relationship that began when they were 16 years old. They feel confused about being single as an adult and are looking for tips on how to flirt and date again, as they don't want to be completely alone during this period.</p>
<p>Table 8: Qualitative examples of GPT-J and CoH tuned GPT-J on the summarization benchmark. The input texts are sampled from the validation split of the dataset from Stiennon et al. [45], which is based on the TL;DR Reddit dataset [47].</p>
<p>Text
Just wanted to say sorry again for wasting your time helping me out! I know I'm probably blind or something. $&gt;.&lt;$ I read the FAQ, and I was able to do some of the things to get started but many of the things I am still confused on. I was able to calculate my BMR (By the way is there one calculator that is better than the others? I got a few different numbers.) However, when I went to the link on the FAQ to calculate my BF, the page was gone. Is this necessary? However, I know even without my BF that I would be classified as "skinny fat" at 5'9 and 156 pounds with a decent layer of fat. The next part I am confused on is diet. Most of the provided resources provide diets to try don't seem like they would lend themselves well to everyday/lifetime eating. I don't know how to eat healthy. The last time I tried to truly get in good shape I did it all wrong. I'm talking eating only 800-900 calories a day, counting obsessively, cutting out all my favorite foods, and interspersed with bouts of binging on aforementioned foods. Today, counting calories freaks me out because I fear that I will go back to that. I am trying to get into real fitness, like going to a gym or bodyweight fitness, but I cannot afford a gym membership and I can't seem to find something study enough to use as a pullup bar for bodyweight. What are my other options because I do want to build muscle. My final question is that I have no idea what I am truly aiming for. I want to lose my layer of fat and build muscle, but I am not sure if there's a goal weight involved or what sort of time table I am looking at or how many calories I would need to eat. This brings us full circle as I restate that I have no idea what I am doing.
GPT-J summary
The individual is expressing regret for taking up the reader's time in attempting to assist them. They are unable to financially support a gym membership and lack a durable apparatus for executing bodyweight exercises, such as pull-ups.
CoH-tuned GPT-J
The person is apologetic for taking up the reader's time and expresses confusion about some of the steps involved in getting started with a fitness journey. They were able to calculate their BMR but couldn't find a working link to calculate their $\mathrm{BF} \%$. They feel they are "skinny fat" and are unsure about how to eat healthily without resorting to obsessive calorie-counting. They can't afford a gym membership or a sturdy pull-up bar for bodyweight exercises. They are also uncertain about their goals for losing fat and building muscle, including their target weight, time frame, and caloric intake.</p>
<h1>Example 4</h1>
<p>Text
This happened a couple of hours ago, and i'm still a bit nervous about all this. I'm what most would consider a hardcore gamer, and as most people into this, they usually cherish and collect their favorite games and such. Today I skipped work because i woke up feeling like something had banged on my left side, hurting all over (ear infection, it's a reoccurring thing, I need to get this checked more often than not.) and decided to stay home and rest a bit more. I stayed home all day and at about 6 my dad rolls around to pick me up to go to the ER to see what's going on with my ear. During the following hour and a half I spent on the ER to get this checked and pick up the prescription the doctor gave me, I get home and find some clothing on my porch, lying on the path to my entrance. My dog was feasting on some plastic bag and at first I thought it was just some kids who'd have thrown a bag of old clothing onto my yard. As soon as I get up to my room though, things took a dire turn for me. I froze, looking baffled at what was right in front of me. My monitor was all crooked and the cables were off, and I look to the left and see this gaping hole where all my consoles were. Then it finally sunk into me: Someone robbed my house. [Some words have been omitted because they may appear inappropriate.]
GPT-J summary
A gamer skipped work due to an ear infection and got robbed. The shock and disbelief of the situation sink in, leaving the person feeling violated and anxious about the loss of their cherished belongings. The incident serves as a reminder of the vulnerability and unpredictability of life, leaving them with a sense of unease and distrust towards others.
CoH-tuned GPT-J
A gamer stayed home from work due to an ear infection and later went to the ER. Upon returning home, they discovered their house had been robbed and their game consoles were missing.</p>
<p>Table 9: (Cont. from previous page) Qualitative examples of GPT-J and CoH tuned GPT-J on the summarization benchmark. The input texts are sampled from the validation split of the dataset from Stiennon et al. [45], which is based on the TL;DR Reddit dataset [47].</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://github.com/EleutherAI/lm-evaluation-harness&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
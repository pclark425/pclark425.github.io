<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7657 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7657</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7657</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fineâ€‘tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-266521522</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.14877v2.pdf" target="_blank">Robust Knowledge Extraction from Large Language Models using Social Choice Theory</a></p>
                <p><strong>Paper Abstract:</strong> Large-language models (LLMs) can support a wide range of applications like conversational agents, creative writing or general query answering. However, they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust - even the same query can result in different answers when prompted multiple times. In order to improve the robustness of LLM queries, we propose using ranking queries repeatedly and to aggregate the queries using methods from social choice theory. We study ranking queries in diagnostic settings like medical and fault diagnosis and discuss how the Partial Borda Choice function from the literature can be applied to merge multiple query results. We discuss some additional interesting properties in our setting and evaluate the robustness of our approach empirically.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7657",
    "paper_id": "paper-266521522",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00410225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Robust Knowledge Extraction from Large Language Models using Social Choice Theory
8 Feb 2024</p>
<p>Nico Potyka potykan@cardiff.ac.uk 
Yuqicheng Zhu yuqicheng.zhu@de.bosch.com 
Evgeny Kharlamov evgeny.kharlamov@de.bosch.com 
Steffen Staab steffen.staab@ki.uni-stuttgart.de </p>
<p>Cardiff University
UK</p>
<p>Bosch Center for AI
Univ. of Stuttgart
Germany, Germany</p>
<p>Bosch Center for AI
Univ. of Stuttgart
Germany, Germany</p>
<p>Bosch Center for AI
Univ. of Oslo
Germany, Norway</p>
<p>Univ. of Stuttgart
Germany</p>
<p>Univ. of Southampton
UK</p>
<p>Robust Knowledge Extraction from Large Language Models using Social Choice Theory
8 Feb 20249B1FD06A1007CB10D6A0ED86F3740698arXiv:2312.14877v2[cs.CL]Large Language ModelsRobustnessSocial Choice Theory
Large-language models (LLMs) can support a wide range of applications like conversational agents, creative writing or general query answering.However, they are ill-suited for query answering in high-stake domains like medicine because they are typically not robust -even the same query can result in different answers when prompted multiple times.In order to improve the robustness of LLM queries, we propose using ranking queries repeatedly and to aggregate the queries using methods from social choice theory.We study ranking queries in diagnostic settings like medical and fault diagnosis and discuss how the Partial Borda Choice function from the literature can be applied to merge multiple query results.We discuss some additional interesting properties in our setting and evaluate the robustness of our approach empirically.</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) achieve state-of-the-art results in various natural language processing (NLP) tasks.Formally, LLMs represent a conditional probability distribution  ( +1 | 1 , . . .,  ) over tokens (character sequences) that predicts the next token given a fixed context of previous tokens.To answer a query ,  is decomposed into tokens  1 , . . .,  and used to sample the first token  1 of the answer from  ( 1 | 1 , . . .,  ). 1 can then be added to the context  1 , . . .,  to sample the next answer token  2 .This process is repeated until a special end of text token is reached.Since finding an optimal sequence of answer tokens is hard, answer sequences are often computed by a heuristic search like Beam [8], Top-K [4] or Nucleus [7] search that build up multiple promising token sequences in parallel.The sampling process is controlled by a temperature parameter.For temperature 0, the algorithms samples greedily.Increasing the temperature allows sampling tokens with lower local probability.While some authors associate higher temperatures with more creative answers, they can also result in higher probability answers because greedy selection can exclude high probability sequences that start with low probability tokens.</p>
<p>Given the success of LLMs in difficult NLP taks, they are increasingly being used for general question answering tasks.This is a natural application as it is reasonable to assume that LLMs picked up a lot of interesting information during training.However, one limitation of LLMs is that they will always produce an answer even if they did not learn anything about the question.This problem is referred to as hallucination in the literature [10].The uncertainty of an answer is hard to quantify.While every answer sequence can be associated with a probability, this is merely the probability of the text sequence and should not be confused with the probability that the answer is correct (or that the LLM "believes" that the answer is correct).Theoretically, LLMs can be asked to output probabilities for their predictions, but it is hard to say how meaningful these probabilities are since there is nothing in a typical LLM architecture that would allow them to infer meaningful probabilities (unless they picked up a particular probability from the training corpora).Since query answering with LLMs is based on a heuristic search for high probability token sequences rather than on reasoning, we, in particular, have the following types of uncertainty:</p>
<p>(1) Query-Uncertainty: prompting the same query repeatedly can result in different answers.(2) Syntax-Uncertainty: semantically equivalent queries that differ only syntactically can result in different answers.(3) Distraction-Uncertainty: meaningless information added to the query can result in a different answer.Let us note that, in principle, query uncertainty can be eliminated by setting the temperature parameter to 0. However, as outlined above, the deterministic answer will be somewhat random because it corresponds to some local optimum found by a heuristic search algorithm.We therefore aim at allowing some randomness in the answer, but increasing the robustness.The idea of robustness is that similar queries should result in similar answers.In particular, the same query prompted multiple times can result in different answers in our setting.However, we would like that these answers are semantically similar.Similarly, we would like that syntactic changes of a query do not change the semantics of the answer.</p>
<p>In this work, we will mainly focus on making LLMs robust against query uncertainty, but we will also look at syntax uncertainty in our experiments.We explore to which extent an answer sampling strategy combined with social choice theory methods can improve the robustness of LLMs.The idea is as follows: instead of asking a query once, we ask it repeatedly (starting each time from the original question context).Our assumption is that if the LLM picked up the answer during training, then this answer should occur in the majority of cases.On the other hand, if it did not pick up information about the query and hallucinates an answer, we expect that the different answers will be very random.We will apply tools from social choice theory to aggregate the answers.We expect that, if the LLM picked up meaningful information, then our aggregation will result in a clear ranking of the different answers, while it will be mostly indifferent between the answers otherwise.</p>
<p>Let us emphasize that the outcome should be interpreted with care.If the LLM has been trained on a text corpora with false information, we may find that an LLM gives a false answer with high certainty.The probabilities that we derive should therefore be understood as subjective probabilities that reflect the uncertainty of the LLM and not as statistical probabilities.We view our method as most useful when being applied to LLMs that were trained on reliable literature (e.g., peer-reviewed articles and books) and not on random text from the internet.While reliable pretrained models like BioBert and MedBert exist [15,20], they still require fine-tuning to be usable as question answering systems.Since our resources are limited, we will therefore use ChatGPT-turbo in our experiments, which was trained on mixed data with varying reliability.However, our experiments are only a proof of concept and the idea can directly be transferred to LLMs trained on high quality domain-specific data.</p>
<p>In our investigation, we will focus on diagnostic problems, where we try to identify the cause of a particular situation or condition.The identified cause is called the diagnosis for the condition.Typical examples are medical diagnosis (identify the medical condition that causes a set of symptoms) or fault diagnosis (identify the defective component in a technical system that causes malfunctions).The query consists of a description of the situation and we ask for a ranking of possible causes ordered by their plausibility.In order to take account of uncertainty, we repeat the query multiple times and collect the rankings.Tools from social choice theory can then be applied to merge the rankings and to quantify the uncertainty of the answer.To do so, we will build up on scoring-based voting methods for partial preference orderings [3].</p>
<p>RELATED WORK</p>
<p>Prior research on uncertainty quantification of LLMs focused on investigating the probabilities of token sequences [6,11].However, as discussed before, the probability of the token sequence should not be confused with the probability that the token sequence expresses a valid claim.In particular, the same claim can be expressed by different (semantically equivalent) token sequences that obtain different probabilities.[14] address this issue by first clustering claims with the same semantic meaning and summing their probabilities to calculate a "semantic entropy".Other work involves training or fine-tuning the LLMs to quantify uncertainty [12,17,18].However, due to lack of transparent training specifics, these approaches might be difficult to reproduce in addition to being expensive.</p>
<p>Despite the demand for uncertainty quantification without relying on model fine-tuning or accessing the proprietary information of LLMs, there is little work in this area and much remains unexplored.To our best knowledge, only [23,24] quantify uncertainty based on the verbalized confidence given by LLMs or selfconsistency of the claims.The significance of verbalized confidence is unclear since there is nothing in a typical LLM architecture that would allow it to infer meaningful probabilities.Our approach aggregates answers and quantifies the uncertainty using methods from social choice theory.Moreover, We study queries that give a rank with multiple possible answers rather than one single answer as an output, no approach from existing work can be directly applied in our case.</p>
<p>The recent neuro-symbolic theorem prover LINC [19] uses LLMs as a semantical parser to translate natural language reasoning problems into first-order logic that can then be processed by a symbolic theorem prover.To decrease the risk of parsing errors, the authors parse and process the inputs repeatedly and apply majority voting to determine the outcome.This may be another interesting domain for applications of more sophisticated voting methods.</p>
<p>The notion of robustness that we consider here (similar inputs should result in similar outputs) follows the terminology in Explainable AI [1,5,16] and should not be confused with statistical [9] or adversarial [22] robustness.From an explanation point of view, our scoring method is interpretable in the sense that the scores can be explained from the LLM's responses to the repeated prompts.The responses can be further explained by the LLM's sampling procedure and the output probabilities of the transformer.However, understanding the output probabilities of transformers is difficult and a topic of current research [26].</p>
<p>SOCIAL CHOICE THEORY BACKGROUND</p>
<p>Social choice theory deals with aggregating individual preferences of different agents towards a collective choice [2].The agents are often seen as voters who can express their preferences in different ways.For example, they may be able to vote for a single candidate, for multiple candidates or report a preference ordering over the candidates.We will focus on the latter setting here.Formally, we consider a finite set of voters  = {1, . . ., } and a finite set of outcomes  = { 1 , . . .,   }.A partial order âª° over  is a binary relation over  that is reflexive, anti-symmetric and transitive.We do not assume that it is complete, that is, there can be outcomes   â‰    such that neither   âª°   nor   âª°   .As usual, we write
â€¢ ğ‘œ â‰» ğ‘œ â€² iff ğ‘œ âª° ğ‘œ â€² and ğ‘œ â€² âª°Ì¸ ğ‘œ, â€¢ ğ‘œ âˆ¼ ğ‘œ â€² iff ğ‘œ âª° ğ‘œ â€² and ğ‘œ â€² âª° ğ‘œ.
If  â‰»  â€² , we say that  is strictly preferred to  â€² and if  âˆ¼  â€² , we say that we are indifferent between the two.A profile  = [âª° 1 , . . ., âª°  ] contains one partial order for every voter and captures the preferences expressed by them.The process of aggregating the voters' preferences can be formalized in different ways.A social choice function is a mapping  from the set of all profiles to a non-empty subset of the outcomes.Intuitively,  () should contain the outcomes that are maximally preferred by the voters.Ideally,  () contains only a single element, but there are cases where a unique choice cannot be made without any ad-hoc assumptions (like chosing a random outcome or a lexicographically minimal one).</p>
<p>Social choice research often focuses on total orderings, where agents express preferences over all possible outcomes [2].In our application, the outcomes are possible diagnoses, and the different answers do not necessarily contain the same diagnoses.We will therefore focus on preferences expressed by partial orderings.Since we are interested in quantifying the uncertainty of an answer (based on the variance in the rankings), scoring-based voting methods are a natural choice.We recall some ideas about aggregating partial preferences by scoring-based voting methods from [3].</p>
<p>To begin with, a scoring procedure   :  â†’ R is a mapping from outcomes to numerical values that is parametrized by a profile  [3].Intuitively,   () is the score of outcome  with respect to the preferences expressed by the profile .Every scoring procedure induces a social choice function by letting
ğ‘“ (ğ‘) = arg max ğ‘œ âˆˆğ‘‚ ğ‘  ğ‘ (ğ‘œ).(1)
A weighting procedure  âª° :  â†’ R maps outcomes to numerical values and is parametrized by a partial order âª° [3].Intuitively,  âª° () is the score of outcome  with respect to the preferences expressed by âª°.We can construct a scoring procedure from a weighting procedure by letting [3] 
ğ‘ (ğ‘œ) = ğ‘› âˆ‘ï¸ ğ‘–=1 ğ‘¤ âª° ğ‘– (ğ‘œ),(2)
where we assume  = [âª° 1 , . . ., âª°  ].</p>
<p>A weighting procedure, in turn, can be based on how many other outcomes are less preferred and how many are incomparable.To do so, we can consider functions Down âª° :  â†’ 2  and Inc âª° :  â†’ 2  defined as follows [3]:
Down âª° (ğ‘œ) = |{ğ‘œ â€² âˆˆ ğ‘‚ | ğ‘œ â‰» ğ‘œ â€² }|,(3)Inc âª° (ğ‘œ) = |{ğ‘œ â€² âˆˆ ğ‘‚ | ğ‘œ and ğ‘œ â€² are incomparable}|,(4)
where, for a set , | | denotes its cardinality.That is, Down âª° () is the number of outcomes ranked lower than  and Inc âª° () is number of outcomes incomparable to .</p>
<p>The following two properties of weighting procedures have been proposed in [3]:</p>
<p>Linearity: There exist constants , ,  âˆˆ R such that
ğ‘¤ âª° (ğ‘œ) = ğ›¼ â€¢ Down âª° (ğ‘œ) + ğ›½ â€¢ Inc âª° (ğ‘œ) + ğ›¾ .(5ğ›¼ = 2, ğ›½ = 1, ğ›¾ = 0 (6)
in (5).</p>
<p>Definition 3.1 (PBW Weighting).</p>
<p>The PBW weighting procedure is defined as
ğ‘¤ PBW âª° (ğ‘œ) = 2 â€¢ Down âª° (ğ‘œ) + Inc âª° (ğ‘œ).(7)
One can show the following.</p>
<p>Theorem 3.2 ([3]</p>
<p>).  PBW âª° satisfies Linearity and Constant Total Weight and every other weighting procedure that satisfies these two properties is an affine transformation of PBW.</p>
<p>We refer to [3,Theorem 1] for more details about this result.The partial Borda choice function  PBW is the social choice function induced by  PBW âª° based on equations ( 1) and ( 2).It can be characterized as follows.Theorem 3.3 ([3]).The partial Borda choice function is the unique social choice function that satisfies the following properties.
Consistency: If ğ‘ 1 , ğ‘ 2 are disjoint profiles and ğ‘“ (ğ‘ 1 ) âˆ© ğ‘“ (ğ‘ 2 ) â‰  âˆ… then ğ‘“ (ğ‘ 1 ) âˆ© ğ‘“ (ğ‘ 2 ) = ğ‘“ (ğ‘ 1 âˆª ğ‘ 2 ). Faithfulness: If ğ‘ = [âª° 1 ] and ğ‘ âª° 1 ğ‘, then ğ‘ âˆ‰ ğ‘“ (ğ‘).
Neutrality:  is invariant with respect to permutations of  (renaming the outcomes will not affect the result), that is,  ( ()) =  ( ()) for all bijective mappings  :  â†’ .Cancellation: If for all outcomes  1 â‰   2 , the number of voters who rank  1 above  2 equals the number of voters who rank  2 above  1 , then  () = .</p>
<p>We refer to [3,Theorem 2] for more details about this result.</p>
<p>IMPROVING THE ROBUSTNESS OF LLM QUERIES WITH PBW</p>
<p>As we saw in the previous section, aggregating partial preferences with PBW gives us several desirable analytical guarantees.We will now use PBW to improve the robustness of LLM ranking queries.The basic idea is to ask the LLM for the most plausible explanations of a situation repeatedly and to use PBW to aggregate the answers.</p>
<p>From Queries to Rankings</p>
<p>In order to obtain ranking answers from LLMs, we consider queries of a special form that we call ranking queries.We refrain from a formal definition and just explain the intuitive idea.Roughly speaking, a ranking query consists of â€¢ a condition description,</p>
<p>â€¢ answer instructions.</p>
<p>Example 4.1.As a running example, we will use a medical scenario with the following ranking query: "A 20 year old professional runner suffers from a stinging pain in the forefoot.The foot is swollen and stiff.What are the most plausible explanations?Please keep the answer short and order by decreasing plausibility."</p>
<p>The first two sentences describe the condition, the last two sentences give the answer instructions.A typical answer provided by ChatGPT looks as follows:</p>
<p>The most plausible explanations for a 20-year-old professional runner experiencing a stinging pain, swelling, and stiffness in the forefoot, ordered by decreasing plausibility, could be: In order to obtain our rankings, we run the prompt 5 times, each time starting from an empty context.The possible causes provided in the 5 answers, define our set of outcomes .We summarize and normalize the answers such that synonyms and syntactic differences do not lead to different outcomes.Given an answer list  1 , . . .,   for one prompt, we associate it with the partial ordering
ğ‘œ 1 â‰» â€¢ â€¢ â€¢ â‰» ğ‘œ ğ‘ â‰» {ğ‘œ 1 , . . . , ğ‘œ ğ‘ },(8)
where for every subset  âŠ† ,  =  \  denotes the complement of  and  â‰»  is short for  â‰»  â€² for all  â€² âˆˆ .That is, the outcomes occuring in the answer are preferred according to their order of appearance and they all are preferred to those outcomes that have not occured.The outcomes that did not occur are incomparable with respect to this ranking.</p>
<p>Example 4.2.For our running example, we obtained the following outcomes after manual normalization:</p>
<p>(1) bu: bursitis, (2) fi: footwear issues, (3) go: gout, (4) in: infection, (5) mn: Morton's neuroma, (6) msr: metatarsal stress reaction, (7) ni: neurological issue, (8) oi: overuse injury, (9) pf: plantar fasciitis, (10) sf: stress fracture, (11) te: tendonitis, (12) tr: trauma.</p>
<p>The 5 answers for our running example correspond to the following partial oderings:
oi â‰» 1 fi â‰» 1 tr â‰» 1 in â‰» 1 ni â‰» 1 {oi, fi, tr, in, ni}, oi â‰» 2 pf â‰» 2 fi â‰» 2 in â‰» 2 go â‰» 2 tr â‰» 2 {oi, pf, fi, in, go, tr}, oi â‰» 3 tr â‰» 3 fi â‰» 3 in â‰» 3 {oi, tr, fi, in}, sf â‰» 4 pf â‰» 4 mn â‰» 4 msr â‰» 4 bu, â‰» 4 {sf, pf, mn, msr, bu} oi â‰» 5 te â‰» 5 fi â‰» 5 in â‰» 5 {oi, te, fi, in}.
We constructed the partial orders in our running example manually.In our experiments, we will use a more automated process that works as follows:</p>
<p>(1) Determine Base-Outcomes: Query the LLM for a list of potential causes that we call base-outcomes.(2) Determine Rankings: Repeatedly ask the LLM for the most plausible causes and to rank them by their plausibility.We call these outcomes ranking-outcomes.(3) Normalize Rankings: Normalize the rankings by matching ranking-outcomes with base-outcomes.We use word embeddings (Sentence-BERT [21]) to map the ranking-outcomes to the most similar base-outcomes.If the similarity of a rankingoutcome to all base-outcomes is smaller than 0.5, it will be discarded (and reported).One can think of other methodologies to compute rankings from LLMs.To abstract from the details, let us assume that we have a transformation method  (,  , ) of the following form.Definition 4.3.A transformation method  (,  , ) takes a ranking query as input, prompts it  times and produces a profile [âª° 1 , . . ., âª°  ] from the answer rankings.The parameter  represents the time at which the query has been prompted.</p>
<p>The time parameter  is only a technical device to take account of the fact that the output of LLMs is non-deterministic.It can also be seen as the (unknown) random seed of the LLM.The time parameter allows us talking about potentially different outputs when aggregating repeatedly for the same input.For example, say we aggregate the answers for  five times and then again five times, then we can denote the two results by  (, 5,  1 ) and  (, 5,  2 ).We will use this notation for the discussion of the consistency property later.The notation is also useful to make the idea of robustness more precise.Assume that we have an aggregation method  that aggregates the profiles obtained from a transformation method  in some way.Roughly speaking, we say that a pair ( , ) consisting of a transformation method  and an aggregation method  is query-robust if the answers obtained for one query  from ( (,  ,  1 )) and ( (,  ,  2 )) are "similar" when  is chosen sufficiently large, syntax-robust if the answers for two syntactically different, but semantically similar queries  1 ,  2 from ( ( 1 ,  ,  1 )) and ( ( 2 ,  ,  2 )) are "similar" when  is chosen sufficiently large.The choice of the similarity measure depends on the application.Correlation measures seem to be a natural choice for measuring similarity between rankings.Measuring similarity between queries is more difficult.For experiments, one simple way to generate similar queries is to make purely syntactical changes to a base query to obtain (almost) semantically equivalent queries.</p>
<p>In our application, our aggregation method   ranks the diagnoses from the given profile by their PBW score.We will use correlation measures to determine the similarity of these rankings for  = 5 in our experiments.</p>
<p>Answer Aggregation</p>
<p>In order to quantify the plausibility of different answers, we apply the PBW score.The larger the score, the more plausible the answer.To make the interpretation of the scores easier, we normalize them such that all values are between 0 and 1.We let
ğ‘  PBW (ğ‘œ) = ğ‘  PBW (ğ‘œ) ğ‘œ â€² âˆˆğ‘‚ ğ‘  PBW (ğ‘œ â€² )(9)
Table 1 shows the PBW scores for our running example.</p>
<p>Properties</p>
<p>We now discuss some analytical guarantees of our approach.Let us note that the normalized PBW score  PBW is just a rescaling of the PBW score  PBW .Therefore, the outcomes with maximal score and their relative order remains unchanged.To begin with, let us reinterpret the properties from Theorem 3.3 in our setting.</p>
<p>Consistency: Let  be a ranking query and let  1 =  (,  1 ,  1 ),  2 =  (,  2 ,  2 ).If  has maximum score with respect to both  PBW ( 1 ) and  PBW ( 2 ), then  also has maximum score with respect to  PBW ( 1 âˆª  2 ).Faithfulness: If we prompt the query only once, then the highest ranked outcome obtains the maximum score.Neutrality: The score of outcomes is independent of their identity.Cancellation: If for all outcomes  1 â‰   2 , the number of rankings that rank  1 above  2 equals the number of rankings that rank  2 above  1 , then all outcomes get the same score.</p>
<p>As explained before, the above properties are sufficient to characterize PBW scoring [3].That is, there is no other scoring function that satisfies all these properties (up to affine transformations).Since all properties seem desirable in our setting,  PBW is a natural choice.</p>
<p>In the following proposition, we note some additional desirable properties of  PBW and  PBW in our setting.The properties also hold for other instantiations of (5) as long as  &gt;  remains satisfied.
âª° ğ‘œ 2 âª° â€¢ â€¢ â€¢ âª° ğ‘œ ğ‘š , then ğ‘¤ PBW (ğ‘œ ğ‘– ) = 2 â€¢ Down âª° (ğ‘œ ğ‘– ) = 2 â€¢ (ğ‘š âˆ’ ğ‘–).
Hence,  PBW (  ) &gt;  PBW (  ) if and only if   is ranked higher than   .The same is true for  PBW because it is just a rescaling of  PBW .</p>
<ol>
<li>Partial agreement implies that  PBW ( * ) &gt;  PBW () for all  âˆˆ  \ { * }, which implies the claim.â–¡</li>
</ol>
<p>EXPERIMENTS</p>
<p>To assess the effectiveness of our approach, we conduct experiments on three sets of ranking queries from manufacturing, finance, and medicine.We first describe our methodology for generating ranking queries and extracting responses in a semi-automatic manner (Algorithm.1 provides an overview of the generation process).Subsequently, we will introduce the selected baseline approaches and the metrics used to assess the robustness of the aggregated answers.</p>
<p>Code is available at https://github.com/boschresearch/RobustLLM/.</p>
<p>Generation of Ranking Queries</p>
<p>5.1.1Generate Symptom-Cause Matrices.To generate ranking queries for our experiments, we first generate symptom-cause matrices, which contain information about a list of underlying critical problems and the possible symptoms we could observe.We generate those matrices with ChatGPT.In the first step, we ask ChatGPT for a list of critical problems (causes) C in a specific domain with the following prompts: "In manufacturing, what are the critical problems that can severely impact the health and overall performance of the factory?Output a list of those problems and rank them based on degree of risk to factory." "What are the critical financial problems that can severely impact the health and overall performance of a company?Output a list of those problems and rank them based on degree of risk to company." "What are common diseases with similar symptoms?"</p>
<p>The first step, gives us the possible diagnoses for the domain.In the second step, we generate symptoms for each diagnosis with the following prompt:</p>
<p>"What can we observe in factory/company/human body to identify the underlying problem <the specific problem>?Output a list of indicators and rank them based on your confidence."</p>
<p>Similar to Section 4.1, we summarize and normalize the symptoms into a list denoted as S, eliminating redundancy arising from synonyms and syntactic variations.Subsequently, we generate matrices as presented in Tables 4, 5, and 6 (see Appendix A).</p>
<p>Sample Symptom Sets.</p>
<p>In real-world scenarios, we have to make a diagnosis based on a set of symptoms.Given a list of symptoms S and diagnoses D for a particular domain, we let   âŠ† S be a subset of the symptoms, which is used in the condition description of a ranking query.For example, {Unplanned maintenance, Increased rework and scrap, Increased product recalls, Increase cost, Increased carrying costs} is a subset of size 5 for the manufacturing domain.We let   denote the set of all possible symptoms that we could observe for one specific diagnosis  âˆˆ D.</p>
<p>The number of all potentially possible symptom sets (all subsets of S) is too large for our experiments.To find a set of reasonable size, we first quantify the uncertainty of symptom sets and then sample a subset of symptom sets based on their uncertainty.Intuitively, the uncertainty of a symptom set is lowest if it uniquely identifies a diagnosis.The uncertainty is highest if all diagnoses are compatible with the symptom set.</p>
<p>We use the Jaccard similarity to measure the similarity between a symptom set   and the symptoms   associated with diagnosis :
ğ‘†ğ‘–ğ‘š(ğ‘  ğ‘ , ğ‘  ğ‘‘ ) = |ğ‘  ğ‘ âˆ© ğ‘  ğ‘‘ | |ğ‘  ğ‘ âˆª ğ‘  ğ‘‘ |(10)
We normalize it such that, for every symptom set   , the similarity values to different diagnoses sum up to 1:
ğ‘†ğ‘–ğ‘š(ğ‘  ğ‘ , ğ‘  ğ‘‘ ) = ğ‘†ğ‘–ğ‘š(ğ‘  ğ‘ , ğ‘  ğ‘‘ ) ğ‘‘ âˆˆ D ğ‘†ğ‘–ğ‘š(ğ‘  ğ‘ , ğ‘  ğ‘‘ )(11)
Finally, we quantify the uncertainty of symptom set   by calculating the normalized entropy of the similarity distribution:
ğ‘ˆ (ğ‘  ğ‘ ) = âˆ’ 1 log 2 (|D|) âˆ‘ï¸ ğ‘‘ âˆˆ D ğ‘†ğ‘–ğ‘š(ğ‘  ğ‘ , ğ‘  ğ‘‘ ) log 2 (ğ‘†ğ‘–ğ‘š(ğ‘  ğ‘ , ğ‘  ğ‘‘ ))(12)
Note that the entropy is always between 0 and log 2 (|D|)), hence our normalized entropy is always between 0 and 1.</p>
<p>In order to investigate our method in lower/higher uncertainty settings, we sample two types of symptom sets for each query set based on  (  ).Since the majority of symptom sets is in the high uncertainty region, we pick the 1000 lowest uncertainty symptom sets for the low uncertainty query set.For the high uncertainty set, we focus on sets with uncertainty between 0.7 and 0.8.More precisely, the two symptom sets have been computed as follows:</p>
<p>â€¢ low uncertainty symptom sets -  : we sort the potential symptom sets by normalized entropy and select the 1000 symptom sets with minimum normalized entropy.â€¢ high uncertainty symptom sets - â„â„ : we randomly select 1000 symptom sets with normalized entropy in the range of 0.7 to 0.8.We visualize the uncertainty distribution of   and  â„â„ with histograms in A.2.</p>
<p>From Symptom Sets to Ranking Queries</p>
<p>We study robustness with respect to query and syntax uncertainty in our experiments.To evaluate query uncertainty, we convert symptom sets to ranking queries using the template in Figure 1.To evaluate syntax uncertainty, we designed two query variants to investigate the effect of syntactic query changes that are semantically meaningless.In the first variant, we only replace part of the words with synonyms without changing the structure of the queries (e.g.we replace "observe" with "detect" and replace "critical problems" with "essential issues").In the second variant, we also change the structure of the query.An an example, Figure 2 shows the variants of the manufacturing ranking template stated before.</p>
<p>Evaluation Protocol</p>
<p>We evaluate the robustness of our approach over three batches of ranking queries i.e. manufacturing, finance and medical queries, compared with two baseline approaches.â€¢ Without Aggregation: we do not aggregate rank answers and directly evaluate the robustness of single answers.â€¢ Average Rank: we treat each rank preference equally and aggregate the ranks by simply averaging the ranks.Given ranks  1 . . .  to be aggregated, the aggregation function is defined as ( 1 . . .  ) = 1   =1   in this case.5.3.2Evaluation Metrics.We use Kendall's rank correlation coefficient (  ) [13] and Spearman's rank correlation coefficient (  ) [25] to evaluate the robustness of the aggregated ranks.</p>
<p>Let  be the number of items to be ranked.Kendall's rank correlation coefficient is defined as follows:
ğ‘… ğœ = ğ¶ âˆ’ ğ· ğ‘› 2 = 2(ğ¶ âˆ’ ğ·) ğ‘›(ğ‘› âˆ’ 1) , (13)
where  is the number of concordant pairs (pairs that have the same order in predicted and ground truth ranks) and  is the number of discordant pairs (pairs that have different order in both ranks).A higher   value indicates a better match between the predicted and true ranks.Spearman's rank correlation coefficient is defined as follows:
ğ‘… ğ‘  = ğ‘ğ‘œğ‘£ (ğ‘Ÿğ‘ğ‘›ğ‘˜1, ğ‘Ÿğ‘ğ‘›ğ‘˜2) ğœ ğ‘Ÿğ‘ğ‘›ğ‘˜1 â€¢ ğœ ğ‘Ÿğ‘ğ‘›ğ‘˜2 ,(14)
where  (.) is the covariance between two variables and  is the standard deviation.Similar to Kendall's tau, a higher   value indicates a better match between the predicted and true ranks.Algorithm.2illustrates our approach to evaluating ranking queries.
ğ‘… ğ‘„ = 1 2( ğ¾ 2 ) ğ‘ 1 ,ğ‘ 2 âˆˆğ‘ƒ,ğ‘ 1 â‰ ğ‘ 2 ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (ğ‘ 1 , ğ‘ 2 )</p>
<p>Experiment Settings</p>
<p>In our experiments, we evaluate the robustness of the answer with respect to repeated queries (query uncertainty) and syntactic changes (syntax uncertainty).Algorithm 2 explains briefly how we evaluate the robustness of the aggregated ranks.</p>
<p>Evaluation of Query Uncertainty.</p>
<p>To evaluate the robustness with respect to repeated queries, we query ChatGPT  times with ranking query  at time  1 . . .  and aggregate the answers with aggregation function  to get  aggregated answers.Note in our experiment, we specifically set  = 3 and  = 5 ( = 1 when  is "without aggregation", since we do not aggregate answers in this baseline).The overall robustness of the query   is evaluated by calculating pairwise Kendall's and Spearman's rank correlation coefficient (we use  (, ) to denote the calculation of both coefficients) and averaging the coefficients.The mean values and standard deviation of all   is reported in our results.</p>
<p>Evaluation of Syntax Uncertainty.</p>
<p>We also evaluate the robustness with respect to syntactic changes, the process is very similar to evaluation of query uncertainty.The only difference is that in this case, instead of repeatedly aggregating outputs for the same , we use  different ranking queries with the same semantic meaning but different syntax.In our experiment,  = 3.</p>
<p>Evaluation of Query Uncertainty</p>
<p>Table 2 presents results for query uncertainty.Our approach consistently outperforms both baselines, "without aggregation" and "average rank, " across all three ranking query sets, demonstrating its superiority in both high and low uncertainty scenarios.</p>
<p>Evaluation of Syntax Uncertainty</p>
<p>Table 2 provides an overview of the outcomes pertaining to syntax uncertainty.Our approach outperform both baseline methods in the majority of scenarios examined.We observe a substantial reduction of both Kendall's and Spearman's coefficients compared to Table 3.This suggests that syntactic variants introduce more variability.</p>
<p>Table 2: Evaluation of query uncertainty: we submit the same ranking query to ChatGPT-turbo five times and then aggregate the results.We repeat this process three times and evaluate the robustness of the three aggregated results.The temperature is set to 1, which is the default setting in the web version of ChatGPT.</p>
<p>Evaluation of Sample Efficiency</p>
<p>Another important question is how many answers do we need to aggregate?That is, how should we choose the parameter  for our transformation method  (,  , ). Figure 3 shows the robustness with respect to the number of answers used for aggregation.We can see that even aggregating only two answers with our approach can already significantly increase the robustness.Note that figure 3 shows the robustness in case of query uncertainty (high uncertainty version) and only Kendall's tau is reported.However, we observed similar trends for other settings.</p>
<p>CONCLUSIONS</p>
<p>To improve the robustness of the answers from LLMs, we suggest to sample answers repeatedly and to aggregate the answers using social choice theory.Our approach is based on the Partial Borda Choice function as it gives several interesting analytical guarantees.</p>
<p>Our investigation primarily focuses on the application of ranking queries within diagnostic contexts, such as medical and fault diagnosis.Our experiments show that our approach significantly improves the robustness against both query and syntax uncertainty.</p>
<p>Queries that ask for a single most plausible answer can be understood as a degenerated special case of our ranking queries.This is because an answer  can be understood as the partial preference  â‰»  \  (the provided answer is ranked above all other answers and the ranking is indifferent about all other answers).In this special case, our average rank baseline corresponds to majority voting.One interesting venue for future work is to compare partial Borda voting in the single-answer setting to other nonranking voting methods.One may also interpret the ranking as an expression of approval (the answer approves of a diagnosis if it is mentioned) and to aggregate the answers by using approval voting methods.</p>
<p>Uncertainty in LLM outputs can also be caused by meaningless information or adversarial attacks injected into the queries.In future work, we aim to investigate whether social choice theory methods can also be applied to effectively improve the robustness of LLM outputs in the presence of such perturbations.</p>
<p>)</p>
<p>Constant Total Weight: There exists a constant  such that  âˆˆ  âª° () =  for all partial orders.Partial Borda Weighting (PBW) [3]  PBW âª° is the linear weighting procedure defined by letting</p>
<p>"Figure 1 :Figure 2 :
12
Figure 1: Query templates for evaluating query uncertainty</p>
<p>Figure 3 :
3
Figure 3: Robustness with respect to the number of answers used for aggregation.</p>
<p>Table 1 :
1
PBW scores for running example: first column shows, the outcomes, columns 2-6 show the partial PBW scores per ranking, column 7 shows the PBW scores and column 8 the normalized PBW scores rounded to two digits.â‰» 1 â‰» 2 â‰» 3 â‰» 4 â‰» 5  PBW ()  PBW ()
bu657147390.06fi20 18 18618800.12go614767400.06in16 16 16616700.10mn657187430.07msr 657167410.06ni145767390.06oi22 22 22622940.14pf6207207600.09sf657227470.07te657620440.07tr18 12 2067630.10</p>
<p>If prompting the query repeatedly resulted in the same rankings, that is, â‰»  = â‰»  for all 1 â‰¤  &lt;  â‰¤  , then  PBW ( 1 ) &gt;  PBW ( 2 ) if and only if  1 âª°   2 .Domination: If there is an  * âˆˆ  such that  * â‰»   for all 1 â‰¤  â‰¤  and  âˆˆ  \ { * }, then arg max  âˆˆ  PBW () = { * }.Proof. 1.The assumptions imply that Down âª°  ( 1 ) &gt; Down âª°  ( 2 ) and therefore  PBW â‰»  ( 1 ) &gt;  PBW â‰»  ( 2 ) for all 1 â‰¤  â‰¤  .Hence,  PBW ( 1 ) &gt;  PBW ( 2 ).The same is true for  PBW because it is just a rescaling of  PBW .2. Since all rankings are equal, the outcomes are totally ordered by âª°=âª° 1 in our setting.Hence, if  = { 1 , . . .,   } and  1
Full Agreement:
Proposition 4.4.Let  be a query that was prompted  times and resulted in the outcomes  and profile  = [â‰» 1 , . .., â‰»  ].Partial Agreement: If there are  1 ,  2 âˆˆ  such that  1 â‰»   2for all 1 â‰¤  â‰¤  , then  PBW ( 1 ) &gt;  PBW ( 2 ).</p>
<p>The pseudocode of ranking query generation /<em> generate symptom-cause matrices </em>/ D â† query ChatGPT for  âˆˆ D do   â† query ChatGPT add   to a list: L  .append( ) end for Symptom-Cause matrix â† summarize and normalize L  /<em> sample symptom sets </em>/ for   âˆˆ   do calculate Jaccard similarity: (  ,   ) â† |  âˆ©  | |  âˆª  | normalization: (  ,   ) â†  (  ,  )  âˆˆC  (  ,  )  âˆˆ C (  ,   ) log 2 ((  ,   )) add  (  ) to the list: List of indicator entropy.append((  )) end for Rank the   based on the normalized entropy (from largest to smallest).  â† the last 1000 indicator sets. â„â„ â† randomly select 1000 symptom sets with normalized entropy in the range of 0.7 to 0.8./<em> generate ranking queries from symptom sets </em>/ Convert symptom sets into ranking queries using query template.
Algorithm 1 calculate normalized entropy:ğ‘ˆ (ğ‘  ğ‘ ) â† âˆ’1 log 2 ( | C | )5.3.1 Baselines.</p>
<p>Algorithm 2 The pseudocode of ranking query evaluation /<em> Evaluate query robustness</em>/ Require:  for  â† 1 . . . do   â† ( (,  ,   )) add   to a list: .append(  )  1 , 2 âˆˆ, 1 â‰  2  ( 1 ,  2 ) /<em> Evaluate syntax robustness</em>/ Require:  1 . . .  for  â† 1 . . . do   â† ( (  ,  ,   )) add   to a list: .append(  ) end for
end forğ‘… ğ‘„ = 1 2( ğ¾ 2 )</p>
<p>Table 3 :
3
Evaluation of syntax uncertainty: we submit the same ranking query to ChatGPT-turbo five times and then aggregate the results.We repeat this process for three syntactic variants and evaluate the robustness of the three aggregated results.The temperature is set to 1, which is the default setting in the web version of ChatGPT.Finance 0.56 (0.32) 0.62 (0.31) 0.57 (0.25) 0.63 (0.25) 0.66 (0.17) 0.71 (0.18) 0.61 (0.34) 0.67 (0.32) 0.55 (0.25) 0.62 (0.25) 0.66 (0.16) 0.72 (0.16) Medical 0.64 (0.28) 0.71 (0.26) 0.71 (0.22) 0.78 (0.2) 0.8 (0.16) 0.84 (0.15) 0.83 (0.26) 0.85 (0.24) 0.84 (0.22) 0.87 (0.19) 0.85 (0.18) 0.86 (0.18)
High uncertainty ranking queriesLow uncertainty ranking querieswithout aggregationaverage rankPBW (our)without aggregationaverage rankPBW (our)DatasetKendall Spearman Kendall SpearmanKendallSpearman Kendall Spearman Kendall SpearmanKendallSpearmanManufacturing 0.25 (0.2)0.29 (0.23) 0.27 (0.19) 0.32 (0.22) 0.43 (0.18) 0.46 (0.19) 0.31 (0.23) 0.35 (0.25)0.31 (0.2)0.37 (0.23)0.46 (0.2) 0.49 (0.21)
ACKNOWLEDGEMENTSThe authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Yuqicheng Zhu.The work was partially supported by the Horizon Europe projects EnrichMyData (Grant Agreement No.101070284).Fines and penaltiesLimited access to certain markets
On the Robustness of Interpretability Methods. David Alvarez, -Melis , Tommi S Jaakkola, Workshop on Human interpretability in Machine Learning. 2018. 2018</p>
<p>Handbook of Computational Social Choice. 10.1017/CBO9781107446984Felix Brandt, Vincent Conitzer, Ulle Endriss, JÃ©rÃ´me Lang, and Ariel D. Procaccia2016Cambridge University Press</p>
<p>A Borda count for partially ordered ballots. John Cullinan, Samuel K Hsiao, David Polett, Social Choice and Welfare. 422014. 2014</p>
<p>Hierarchical Neural Story Generation. Angela Fan, Mike Lewis, Yann N Dauphin, Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics2018</p>
<p>Attribution-based Explanations that Provide Recourse Cannot be Robust. Hidde Fokkema, Rianne De Heide, Tim Van Erven, Journal of Machine Learning Research. 242023. 2023</p>
<p>On Calibration of Modern Neural Networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, PMLR, 1321-1330ICML (Proceedings of Machine Learning Research. 201770</p>
<p>The Curious Case of Neural Text Degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, ICLR. OpenReview.net2020</p>
<p>Xuedong Huang, Alex Acero, Raj Hsiao-Wuen Hon, Reddy, Spoken language processing: A guide to theory, algorithm, and system development. Prentice hall PTR2001</p>
<p>Robust Statistics. J Peter, Huber, 10.1002/04717252501981</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, Comput. Surveys. 552023. 2023</p>
<p>How can we know when language models know? on the calibration of language models for question answering. Zhengbao Jiang, Jun Araki, Haibo Ding, Graham Neubig, Transactions of the Association for Computational Linguistics. 92021. 2021</p>
<p>Language models (mostly) know what they know. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, 2023. 2023Findings of the Association for Computational Linguistics</p>
<p>A new measure of rank correlation. Maurice G Kendall, Biometrika. 3021938. 1938</p>
<p>Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, International Conference on Learning Representations ICLR. 2023</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. 362020. 2020</p>
<p>Promoting Counterfactual Robustness through Diversity. Francesco Leofante, Nico Potyka, AAAI Conference on Artificial Intelligence (AAAI'24). 2024</p>
<p>Teaching Models to Express Their Uncertainty in Words. Stephanie Lin, Jacob Hilton, Owain Evans, Transactions on Machine Learning Research. 2022. 2022</p>
<p>Reducing Conversational Agents' Overconfidence Through Linguistic Calibration. Sabrina J Mielke, Arthur Szlam, Emily Dinan, Y-Lan Boureau, Transactions of the Association for Computational Linguistics. 102022. 2022</p>
<p>LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers. Theo Olausson, Alex Gu, Benjamin Lipkin, Cedegao E Zhang, Armando Solar-Lezama, Joshua B Tenenbaum, Roger Levy, 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics2023</p>
<p>Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, Degui Zhi, NPJ digital medicine. 4862021. 2021</p>
<p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Nils Reimers, Iryna Gurevych, Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing EMNLP-IJCNLP. 2019</p>
<p>Intriguing properties of neural networks. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J Goodfellow, Rob Fergus, International Conference on Learning Representations (ICLR). 2014</p>
<p>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models finetuned with human feedback. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D Manning, Conference on Empirical Methods in Natural Language Processing EMNLP. 2023. 2023</p>
<p>Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, arXiv:2306.13063Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. 2023. 2023arXiv preprint</p>
<p>Spearman rank correlation. Jerrold H Zar, Encyclopedia of Biostatistics. 72005. 2005</p>
<p>Explainability for large language models: A survey. Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du, ACM Transactions on Intelligent Systems and Technology. 2023. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
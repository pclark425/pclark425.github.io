<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7419 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7419</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7419</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-273482403</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.14464v2.pdf" target="_blank">Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning</a></p>
                <p><strong>Paper Abstract:</strong> Electrocardiogram (ECG) interpretation requires specialized expertise, often involving synthesizing insights from ECG signals with complex clinical queries posed in natural language. The scarcity of labeled ECG data coupled with the diverse nature of clinical inquiries presents a significant challenge for developing robust and adaptable ECG diagnostic systems. This work introduces a novel multimodal meta-learning method for few-shot ECG question answering, addressing the challenge of limited labeled data while leveraging the rich knowledge encoded within large language models (LLMs). Our LLM-agnostic approach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA and Gemma) via a trainable fusion module, enabling the language model to reason about ECG data and generate clinically meaningful answers. Extensive experiments demonstrate superior generalization to unseen diagnostic tasks compared to supervised baselines, achieving notable performance even with limited ECG leads. For instance, in a 5-way 5-shot setting, our method using LLaMA-3.1-8B achieves an accuracy of 84.6%, 77.3%, and 69.6% on single verify, choose and query question types, respectively. These results highlight the potential of our method to enhance clinical ECG interpretation by combining signal processing with the nuanced language understanding capabilities of LLMs, particularly in data-constrained scenarios.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7419.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7419.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-variants (P-A/P-B/P-C)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt format variants evaluated (P-A: "question: " + question + "answer: "; P-B: question only; P-C: question + "the answer can be both, none or in question")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A direct ablation of prefix/prompt structure for few-shot ECG-language question answering showing that an explicit 'question:' / 'answer:' prefix yields substantially better generation/accuracy than a bare question or a different clarifying suffix.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-2B (default for ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only large language model (Gemma family) used frozen as the text decoder in a multimodal meta-learning pipeline; treated as the LLM component for ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot ECG-language single-choice question answering (Single-Choose)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>N-way K-shot multimodal question answering where the model must generate/select the correct choice given an ECG signal and a question (here evaluated as generative answer overlap/accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt prefix-style few-shot natural-language prompt (three variants: explicit labeled prompt, bare question, question with clarifying suffix).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>2-way 5-shot setting; attention-based multimodal fusion mapper; frozen ECG encoder; episodic meta-learning; three prompt variants compared: P-A = "question: " + question + "answer: ", P-B = question only, P-C = question + "the answer can be both, none or in question".</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (exact token-overlap based alignment to ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>P-A: 84.5% accuracy; P-B: 77.4% accuracy; P-C: 80.1% accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>P-B (question only): 77.4% accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>P-A vs P-B: +7.1% absolute; P-A vs P-C: +4.4% absolute; P-C vs P-B: +2.7% absolute</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>2-way 5-shot single-choice (episodic meta-learning, attention-based mapper, frozen ECG encoder, meta-knowledge included); Gemma-2-2B used as default for ablations</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7419.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7419.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Question-type format (S-Verify / S-Choose / S-Query)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question type / problem format: Single-Verify (yes/no), Single-Choose (multiple-choice), Single-Query (open-ended)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different question formulations (yes/no verification, multiple-choice selection, and open-ended attribute query) yield markedly different accuracies; models do better on verification and choice formats than open-ended queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B (representative high-performing LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only large language model Llama 3.1 used frozen as the text decoder in the multimodal meta-learning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot ECG-language question answering (5-way 5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>N-way K-shot multimodal question answering across three curated question formats (Single-Verify, Single-Choose, Single-Query) evaluated on held-out attribute-answer classes.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Question type categories: verification (binary yes/no), multiple-choice (select one/many), and open-ended attribute value generation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Reported for 5-way 5-shot episodic meta-learning; generation evaluated by token-overlap accuracy and NLG metrics; models frozen except multimodal mapper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Llama-3.1-8B (5-way 5-shot): Single-Verify = 84.6% accuracy; Single-Choose = 77.3% accuracy; Single-Query = 69.6% accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Gemma-2-2B (same 5-way 5-shot setting) reported: Single-Verify = 82.4%; Single-Choose = 62.9%; Single-Query = 42.1% (from model-scale comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Llama-3.1-8B vs Gemma-2-2B: +2.2% abs (S-Verify), +14.4% abs (S-Choose), +27.5% abs (S-Query); shows larger-model gains are bigger on more open-ended formats</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>5-way 5-shot episodic meta-learning; attention-based mapper; frozen ECG encoder; metrics from Table 2/Table 4</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7419.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7419.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Question-phrasing robustness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of variation in question phrasing (semantically equivalent but lexically different expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The model is robust to many paraphrases but shows modest to notable accuracy drops for some question types when phrasing varies; open-ended queries are most affected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-2B (default for ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>2B-parameter decoder LLM used frozen in the multimodal meta-learning setup for ablation and robustness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot ECG-language question answering (2-way 5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate effect of changing surface wording of clinically equivalent questions on model accuracy for the three question types.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Question phrasing variation (same vs different expressions conveying the same meaning).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / phrasing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>2-way 5-shot experiments comparing 'Same' (identical phrasing) vs 'Different' (paraphrased) question expressions across Single-Verify, Single-Choose, Single-Query.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Same phrasing — S-Verify: 89.0%; S-Choose: 84.5%; S-Query: 48.6%. Different phrasing — S-Verify: 86.5%; S-Choose: 84.7%; S-Query: 42.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Same phrasing (used as reference): see values above</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>S-Verify: -2.5% abs when phrasing differs; S-Choose: +0.2% abs (negligible); S-Query: -6.1% abs when phrasing differs (notable drop)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>2-way 5-shot episodic meta-learning, attention-based mapper, frozen ECG encoder; meta-knowledge used</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7419.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7419.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Input-presentation: number of ECG leads</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of restricting available ECG leads (Lead I; Leads I & II; Leads I, II, V3; All leads)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Altering the presented ECG modality (which leads are provided) affects question-answering accuracy; some tasks (verification) remain high with a single lead, while choice and open queries benefit from additional leads.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-2B (default for ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>2B-parameter decoder LLM integrated with a frozen ECG encoder and trainable multimodal mapper; model evaluated under different input-lead presentations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot ECG-language question answering (2-way 5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate how reduced ECG lead availability as part of the input modality affects accuracy for different question types.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Input modality variation — masked/leads-omitted ECG signals (single-lead to full 12-lead).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / presentation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>2-way 5-shot setting; tested configurations: Lead I only; Leads I & II; Leads I, II, V3; All leads (12). Attention-based mapper; frozen ECG encoder by default.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Lead I: S-Verify 89.6%, S-Choose 79.6%, S-Query 47.5%; Leads I & II: S-Verify 89.0%, S-Choose 84.2%, S-Query 45.7%; Leads I,II,V3: S-Verify 88.9%, S-Choose 82.4%, S-Query 47.2%; All leads: S-Verify 89.0%, S-Choose 84.5%, S-Query 48.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>All leads (full input): S-Verify 89.0%, S-Choose 84.5%, S-Query 48.6%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Lead I only vs All leads: S-Verify +0.6% abs, S-Choose -4.9% abs, S-Query -1.1% abs; Leads I&II vs All leads: S-Verify ~0.0% abs, S-Choose -0.3% abs, S-Query -2.9% abs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>2-way 5-shot episodic meta-learning; attention-based mapper; meta-knowledge included; frozen ECG encoder unless otherwise noted</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7419.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7419.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Episodic (meta) vs standard supervised training format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training format comparison: episodic meta-learning (MAML-like episodic training) versus standard supervised baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training regime / presentation of the learning problem (episodic few-shot meta-training versus standard supervised training) substantially affects generalization to unseen tasks in few-shot ECG QA; episodic meta-learning consistently improves few-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-2B and Llama-3.1-8B (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Both models used as frozen decoders in the multimodal meta-learning pipeline; episodic meta-training applied to the multimodal fusion and any trainable components.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B (Gemma-2-2B) and 8B (Llama-3.1-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot ECG-language question answering (various N-way K-shot settings)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare episodic meta-learning (simulate tasks during training) vs conventional supervised training (single-task learning) on generalization to novel attribute-answer task classes.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Training/presentation format (episodic tasks vs pooled supervised examples).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>training format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Multiple few-shot settings evaluated (2-way 5-shot, 2-way 10-shot, 5-way 5-shot, 5-way 10-shot); episodic training uses inner/outer loop adaptation (MAML-style) and yields models amenable to quick adaptation at meta-test.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy and NLG metrics (BLEU, BertScore, ROUGE) reported; main comparison on accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports that episodic training 'consistently improves performance across all settings and question types' (quantitative per-setting numbers in Table 2; e.g., episodic Llama-3.1-8B 5-way 5-shot achieves 84.6% S-Verify compared to lower supervised baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Supervised baseline (adapted image-captioning style classifier on ECG-QA) reported as an upper bound in some comparisons; episodic approach outperforms standard supervised baseline in few-shot generalization (see Table 2 and text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Described qualitatively as consistent improvement across settings; specific per-model per-setting absolute differences are reported in Table 2 of the paper (no aggregated p-values).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Episodic meta-learning (MAML-like inner/outer loop), inner update steps = 5, outer learning rate 5e-4, inner learning rate 0.05; comparisons against supervised training baselines in same task splits.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ecg-qa: A comprehensive question answering dataset combined with electrocardiogram <em>(Rating: 2)</em></li>
                <li>Leadagnostic self-supervised learning for local and global representations of electrocardiogram <em>(Rating: 2)</em></li>
                <li>Gemma 2: Improving open language models at a practical size <em>(Rating: 1)</em></li>
                <li>The llama 3 herd of models <em>(Rating: 1)</em></li>
                <li>Model-agnostic meta-learning for fast adaptation of deep networks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7419",
    "paper_id": "paper-273482403",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Prompt-variants (P-A/P-B/P-C)",
            "name_full": "Prompt format variants evaluated (P-A: \"question: \" + question + \"answer: \"; P-B: question only; P-C: question + \"the answer can be both, none or in question\")",
            "brief_description": "A direct ablation of prefix/prompt structure for few-shot ECG-language question answering showing that an explicit 'question:' / 'answer:' prefix yields substantially better generation/accuracy than a bare question or a different clarifying suffix.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2-2B (default for ablations)",
            "model_description": "Decoder-only large language model (Gemma family) used frozen as the text decoder in a multimodal meta-learning pipeline; treated as the LLM component for ablation studies.",
            "model_size": "2B",
            "task_name": "Few-shot ECG-language single-choice question answering (Single-Choose)",
            "task_description": "N-way K-shot multimodal question answering where the model must generate/select the correct choice given an ECG signal and a question (here evaluated as generative answer overlap/accuracy).",
            "problem_format": "Prompt prefix-style few-shot natural-language prompt (three variants: explicit labeled prompt, bare question, question with clarifying suffix).",
            "format_category": "prompt style",
            "format_details": "2-way 5-shot setting; attention-based multimodal fusion mapper; frozen ECG encoder; episodic meta-learning; three prompt variants compared: P-A = \"question: \" + question + \"answer: \", P-B = question only, P-C = question + \"the answer can be both, none or in question\".",
            "performance_metric": "Accuracy (exact token-overlap based alignment to ground truth)",
            "performance_value": "P-A: 84.5% accuracy; P-B: 77.4% accuracy; P-C: 80.1% accuracy",
            "baseline_performance": "P-B (question only): 77.4% accuracy",
            "performance_change": "P-A vs P-B: +7.1% absolute; P-A vs P-C: +4.4% absolute; P-C vs P-B: +2.7% absolute",
            "experimental_setting": "2-way 5-shot single-choice (episodic meta-learning, attention-based mapper, frozen ECG encoder, meta-knowledge included); Gemma-2-2B used as default for ablations",
            "statistical_significance": null,
            "uuid": "e7419.0",
            "source_info": {
                "paper_title": "Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Question-type format (S-Verify / S-Choose / S-Query)",
            "name_full": "Question type / problem format: Single-Verify (yes/no), Single-Choose (multiple-choice), Single-Query (open-ended)",
            "brief_description": "Different question formulations (yes/no verification, multiple-choice selection, and open-ended attribute query) yield markedly different accuracies; models do better on verification and choice formats than open-ended queries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B (representative high-performing LLM)",
            "model_description": "Decoder-only large language model Llama 3.1 used frozen as the text decoder in the multimodal meta-learning pipeline.",
            "model_size": "8B",
            "task_name": "Few-shot ECG-language question answering (5-way 5-shot)",
            "task_description": "N-way K-shot multimodal question answering across three curated question formats (Single-Verify, Single-Choose, Single-Query) evaluated on held-out attribute-answer classes.",
            "problem_format": "Question type categories: verification (binary yes/no), multiple-choice (select one/many), and open-ended attribute value generation.",
            "format_category": "question type",
            "format_details": "Reported for 5-way 5-shot episodic meta-learning; generation evaluated by token-overlap accuracy and NLG metrics; models frozen except multimodal mapper.",
            "performance_metric": "Accuracy",
            "performance_value": "Llama-3.1-8B (5-way 5-shot): Single-Verify = 84.6% accuracy; Single-Choose = 77.3% accuracy; Single-Query = 69.6% accuracy",
            "baseline_performance": "Gemma-2-2B (same 5-way 5-shot setting) reported: Single-Verify = 82.4%; Single-Choose = 62.9%; Single-Query = 42.1% (from model-scale comparison)",
            "performance_change": "Llama-3.1-8B vs Gemma-2-2B: +2.2% abs (S-Verify), +14.4% abs (S-Choose), +27.5% abs (S-Query); shows larger-model gains are bigger on more open-ended formats",
            "experimental_setting": "5-way 5-shot episodic meta-learning; attention-based mapper; frozen ECG encoder; metrics from Table 2/Table 4",
            "statistical_significance": null,
            "uuid": "e7419.1",
            "source_info": {
                "paper_title": "Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Question-phrasing robustness",
            "name_full": "Effect of variation in question phrasing (semantically equivalent but lexically different expressions)",
            "brief_description": "The model is robust to many paraphrases but shows modest to notable accuracy drops for some question types when phrasing varies; open-ended queries are most affected.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2-2B (default for ablations)",
            "model_description": "2B-parameter decoder LLM used frozen in the multimodal meta-learning setup for ablation and robustness checks.",
            "model_size": "2B",
            "task_name": "Few-shot ECG-language question answering (2-way 5-shot)",
            "task_description": "Evaluate effect of changing surface wording of clinically equivalent questions on model accuracy for the three question types.",
            "problem_format": "Question phrasing variation (same vs different expressions conveying the same meaning).",
            "format_category": "prompt style / phrasing",
            "format_details": "2-way 5-shot experiments comparing 'Same' (identical phrasing) vs 'Different' (paraphrased) question expressions across Single-Verify, Single-Choose, Single-Query.",
            "performance_metric": "Accuracy",
            "performance_value": "Same phrasing — S-Verify: 89.0%; S-Choose: 84.5%; S-Query: 48.6%. Different phrasing — S-Verify: 86.5%; S-Choose: 84.7%; S-Query: 42.5%.",
            "baseline_performance": "Same phrasing (used as reference): see values above",
            "performance_change": "S-Verify: -2.5% abs when phrasing differs; S-Choose: +0.2% abs (negligible); S-Query: -6.1% abs when phrasing differs (notable drop)",
            "experimental_setting": "2-way 5-shot episodic meta-learning, attention-based mapper, frozen ECG encoder; meta-knowledge used",
            "statistical_significance": null,
            "uuid": "e7419.2",
            "source_info": {
                "paper_title": "Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Input-presentation: number of ECG leads",
            "name_full": "Effect of restricting available ECG leads (Lead I; Leads I & II; Leads I, II, V3; All leads)",
            "brief_description": "Altering the presented ECG modality (which leads are provided) affects question-answering accuracy; some tasks (verification) remain high with a single lead, while choice and open queries benefit from additional leads.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2-2B (default for ablations)",
            "model_description": "2B-parameter decoder LLM integrated with a frozen ECG encoder and trainable multimodal mapper; model evaluated under different input-lead presentations.",
            "model_size": "2B",
            "task_name": "Few-shot ECG-language question answering (2-way 5-shot)",
            "task_description": "Evaluate how reduced ECG lead availability as part of the input modality affects accuracy for different question types.",
            "problem_format": "Input modality variation — masked/leads-omitted ECG signals (single-lead to full 12-lead).",
            "format_category": "input modality / presentation",
            "format_details": "2-way 5-shot setting; tested configurations: Lead I only; Leads I & II; Leads I, II, V3; All leads (12). Attention-based mapper; frozen ECG encoder by default.",
            "performance_metric": "Accuracy",
            "performance_value": "Lead I: S-Verify 89.6%, S-Choose 79.6%, S-Query 47.5%; Leads I & II: S-Verify 89.0%, S-Choose 84.2%, S-Query 45.7%; Leads I,II,V3: S-Verify 88.9%, S-Choose 82.4%, S-Query 47.2%; All leads: S-Verify 89.0%, S-Choose 84.5%, S-Query 48.6%.",
            "baseline_performance": "All leads (full input): S-Verify 89.0%, S-Choose 84.5%, S-Query 48.6%",
            "performance_change": "Lead I only vs All leads: S-Verify +0.6% abs, S-Choose -4.9% abs, S-Query -1.1% abs; Leads I&II vs All leads: S-Verify ~0.0% abs, S-Choose -0.3% abs, S-Query -2.9% abs.",
            "experimental_setting": "2-way 5-shot episodic meta-learning; attention-based mapper; meta-knowledge included; frozen ECG encoder unless otherwise noted",
            "statistical_significance": null,
            "uuid": "e7419.3",
            "source_info": {
                "paper_title": "Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Episodic (meta) vs standard supervised training format",
            "name_full": "Training format comparison: episodic meta-learning (MAML-like episodic training) versus standard supervised baseline",
            "brief_description": "Training regime / presentation of the learning problem (episodic few-shot meta-training versus standard supervised training) substantially affects generalization to unseen tasks in few-shot ECG QA; episodic meta-learning consistently improves few-shot performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-2-2B and Llama-3.1-8B (evaluated)",
            "model_description": "Both models used as frozen decoders in the multimodal meta-learning pipeline; episodic meta-training applied to the multimodal fusion and any trainable components.",
            "model_size": "2B (Gemma-2-2B) and 8B (Llama-3.1-8B)",
            "task_name": "Few-shot ECG-language question answering (various N-way K-shot settings)",
            "task_description": "Compare episodic meta-learning (simulate tasks during training) vs conventional supervised training (single-task learning) on generalization to novel attribute-answer task classes.",
            "problem_format": "Training/presentation format (episodic tasks vs pooled supervised examples).",
            "format_category": "training format",
            "format_details": "Multiple few-shot settings evaluated (2-way 5-shot, 2-way 10-shot, 5-way 5-shot, 5-way 10-shot); episodic training uses inner/outer loop adaptation (MAML-style) and yields models amenable to quick adaptation at meta-test.",
            "performance_metric": "Accuracy and NLG metrics (BLEU, BertScore, ROUGE) reported; main comparison on accuracy.",
            "performance_value": "Paper reports that episodic training 'consistently improves performance across all settings and question types' (quantitative per-setting numbers in Table 2; e.g., episodic Llama-3.1-8B 5-way 5-shot achieves 84.6% S-Verify compared to lower supervised baselines).",
            "baseline_performance": "Supervised baseline (adapted image-captioning style classifier on ECG-QA) reported as an upper bound in some comparisons; episodic approach outperforms standard supervised baseline in few-shot generalization (see Table 2 and text).",
            "performance_change": "Described qualitatively as consistent improvement across settings; specific per-model per-setting absolute differences are reported in Table 2 of the paper (no aggregated p-values).",
            "experimental_setting": "Episodic meta-learning (MAML-like inner/outer loop), inner update steps = 5, outer learning rate 5e-4, inner learning rate 0.05; comparisons against supervised training baselines in same task splits.",
            "statistical_significance": null,
            "uuid": "e7419.4",
            "source_info": {
                "paper_title": "Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ecg-qa: A comprehensive question answering dataset combined with electrocardiogram",
            "rating": 2,
            "sanitized_title": "ecgqa_a_comprehensive_question_answering_dataset_combined_with_electrocardiogram"
        },
        {
            "paper_title": "Leadagnostic self-supervised learning for local and global representations of electrocardiogram",
            "rating": 2,
            "sanitized_title": "leadagnostic_selfsupervised_learning_for_local_and_global_representations_of_electrocardiogram"
        },
        {
            "paper_title": "Gemma 2: Improving open language models at a practical size",
            "rating": 1,
            "sanitized_title": "gemma_2_improving_open_language_models_at_a_practical_size"
        },
        {
            "paper_title": "The llama 3 herd of models",
            "rating": 1,
            "sanitized_title": "the_llama_3_herd_of_models"
        },
        {
            "paper_title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "rating": 2,
            "sanitized_title": "modelagnostic_metalearning_for_fast_adaptation_of_deep_networks"
        }
    ],
    "cost": 0.015948,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning
8 May 2025</p>
<p>Jialu Tang j.tang@tue.nl 
Yuan Lu y.lu@tue.nl 
Cecilia Mascolo </p>
<p>Eindhoven University of Technology
The Netherlands Tong Xia</p>
<p>University of Cambridge
United Kingdom</p>
<p>Eindhoven University of Technology
The Netherlands</p>
<p>University of Cambridge
United Kingdom</p>
<p>Eindhoven University of Technology
The Netherlands</p>
<p>Eindhoven Artificial Intelligence Systems Institute
The Netherlands</p>
<p>Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning
8 May 2025260C08ECA6F44F0DD2D1453F6710FEFEarXiv:2410.14464v2[cs.LG]
Electrocardiogram (ECG) interpretation requires specialized expertise, often involving synthesizing insights from ECG signals with complex clinical queries posed in natural language.The scarcity of labeled ECG data coupled with the diverse nature of clinical inquiries presents a significant challenge for developing robust and adaptable ECG diagnostic systems.This work introduces a novel multimodal meta-learning method for few-shot ECG question answering, addressing the challenge of limited labeled data while leveraging the rich knowledge encoded within large language models (LLMs).Our LLM-agnostic approach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA and Gemma) via a trainable fusion module, enabling the language model to reason about ECG data and generate clinically meaningful answers.Extensive experiments demonstrate superior generalization to unseen diagnostic tasks compared to supervised baselines, achieving notable performance even with limited ECG leads.For instance, in a 5-way 5-shot setting, our method using LLaMA-3.1-8Bachieves an accuracy of 84.6%, 77.3%, and 69.6% on single verify, choose and query question types, respectively.These results highlight the potential of our method to enhance clinical ECG interpretation by combining signal processing with the nuanced language understanding capabilities of LLMs, particularly in data-constrained scenarios.Data and Code AvailabilityThis paper uses ECG-QA, the question-answering dataset for electrocardiogram (ECG) analysis (Oh et al., 2024) regarding PTB-XL (Wagner et al., 2020) and MIMIC-IV-ECG (Gow et al.</p>
<p>Introduction</p>
<p>Electrocardiograms (ECGs) provide a wealth of physiological information crucial for diagnosing a wide range of cardiac conditions.Although doctors are professionally trained to diagnose (Garcia and Holtz, 2001;O'Keefe, 2008), and even AI systems have shown promise in not only enhancing diagnostic accuracy but also relieving the pressure on healthcare professionals (Jin et al., 2024;Ribeiro et al., 2020;Hannun et al., 2019).However, they are usually trained in limited and incomplete categories (Al-Alshaikh et al., 2024).The advent of large language models (LLMs) coupled with advancements in multimodal learning presents a transformative opportunity to enhance ECG interpretation by integrating the rich contextual understanding of language with the detailed physiological insights encoded within ECG signals.This fusion of modalities allows for a more comprehensive and nuanced analysis, potentially leading to more accurate and timely diagnoses.Multimodal question answering (QA) systems, operating at this intersection of ECG data and natural language processing, are emerging as a powerful tool for automating and augmenting clinical workflows, offering the potential to improve diagnostic accuracy, efficiency, and accessibility.By enabling direct interaction with ECG data through natural language queries, these systems can streamline the diagnostic process and empower clinicians with more informed decision-making capabilities.</p>
<p>Developing robust and reliable multimodal QA systems for ECG interpretation relies on the availability of both high-quality and large quantities of labeled data.Yet, obtaining massive amounts of labeled ECGs from cardiologists is costly, which often results in limited datasets.Traditional supervised learning methods tend to perform well only on data with the same distribution as the training data.In real-world deployment, however, models frequently encounter new tasks and previously unseen populations outside the training distribution, where traditional methods may fail.Meta-learning (Andrychowicz et al., 2016;Finn et al., 2017;Thrun and Pratt, 1998), a paradigm focused on "learning to learn", offers a compelling solution to this challenge.By training models on a diverse range of tasks, meta-learning enables them to acquire transferable knowledge and adapt rapidly to new, unseen tasks with minimal labeled data.This adaptive capacity is particularly valuable in the ECGlanguage QA domain, where new diagnostic questions and data distributions constantly emerge.</p>
<p>Few-shot learning (FSL), as a practical approach within meta-learning, shows significant promise in various medical imaging tasks (Pachetti and Colantonio, 2024).The success of FSL underscores the potential of learning efficient representations that generalize effectively from limited examples (Finn et al., 2017).While high-quality multimodal datasets, like those available for chest X-rays, have fueled progress in FSL for image-based diagnostics, the ECG domain lacks datasets specifically tailored for few-shot multimodal learning paradigms.The recent introduction of the ECG-QA dataset (Oh et al., 2024), built upon established ECG resources like PTB-XL (Wagner et al., 2020) and MIMIC-IV-ECG (Gow et al., 2023), partially addresses this need with its diverse question types (single-verify, single-choose, single-query) and ECG attributes (e.g., SCP codes, noise types, heart axis deviations).However, existing dataset lacks the structured task configurations necessary for developing and evaluating meta-learning models, leaving a significant gap in the advancement of ECG-language QA systems.</p>
<p>In response to these challenges, we propose a novel, LLM-agnostic, multimodal meta-learning framework specifically designed for few-shot ECG-language QA.Our architecture integrates a self-supervised pretrained ECG encoder with a frozen LLM and a trainable multimodal fusion mapper bridging the ECG and language representations.This fusion mapper is crucial for acquiring transferable meta-knowledge, enabling rapid adaptation to new tasks.Furthermore, we create a benchmarking variant of the ECG-QA dataset (see Table 1), designed to facilitate metalearning having diverse tasks with varying attributeanswer combinations.This benchmark dataset allows us to rigorously evaluate a model's ability to generalize to unseen diagnostic tasks in a few-shot setting.We demonstrate the effectiveness of our framework across a broad range of language models, showcasing superior generalization performance compared to fully supervised baselines in various few-shot settings and question types.Our findings highlight the potential of our approach to significantly impact clinical practice by enabling robust and adaptable ECGlanguage QA with limited labeled data.</p>
<p>Related Works</p>
<p>Deep learning has significantly advanced ECG interpretation, with models such as CNNs and Transformers demonstrating promising results in automated diagnosis (Chugh and Jain, 2023;Woo et al., 2024;Sun et al., 2023).However, these supervised approaches typically require large labeled datasets, hindering their generalizability to diverse patient populations and uncommon ECG presentations, a critical limitation in real-world clinical settings.While self-supervised learning methods offer a potential solution by learning from unlabeled ECG data (Gopal et al., 2021;Tonekaboni et al., 2021;Oh et al., 2022;Saeed et al., 2019;Kiyasseh et al., 2021), they have not yet been effectively leveraged for complex clinical question answering involving nuanced language understanding.</p>
<p>Multimodal learning has emerged as a powerful paradigm in healthcare, demonstrating success in integrating medical images with textual information (Krones et al., 2025;Boecking et al., 2022;Zhang et al., 2020;Rasmy et al., 2021;Warner et al., 2024;Acosta et al., 2022).However, effectively fusing temporal physiological signals like ECG with the unstructured and often ambiguous nature of clinical language presents unique challenges, particularly in generative tasks like open-ended question answering.Our work directly addresses this gap by proposing a novel method for ECG-language fusion, enabling more comprehensive and nuanced diagnostic reasoning by leveraging the complementary information present in both modalities.Furthermore, the inherent scarcity of labeled data for specific cardiac conditions necessitates efficient few-shot learning strategies.Meta-learning techniques, such as MAML (Finn et al., 2017), have shown promise in enabling rapid adaptation to new tasks with limited examples (Vettoruzzo et al., 2024), offering a compelling approach for ECG interpreta-tion.While recent studies have explored integrating LLMs with few-shot learning in medical domains (Jin et al., 2023;Yu et al., 2023), the potential of combining meta-learning, LLMs, and multimodal fusion for ECG-language question answering remains largely unexplored.Our work contributes a method that integrates these key components, enabling adaptability to new tasks from limited labeled data while leveraging the powerful language understanding and generation capabilities of LLMs.</p>
<p>Methods</p>
<p>We present a method capable of rapidly adapting models to novel ECG Question-Answers (QAs) tasks with minimal labeled data.We frame this problem within the context of multimodal few-shot metalearning consisting of three key phases.Here, we first define the meta-learning dataset specific to ECGlanguage QAs , where the objective is to classify unseen examples into one of N new 'test' classes, given only a few reference examples per class (Triantafillou et al., 2019).Then, we detail the architecture of our proposed model that integrates ECG analysis with question processing to generate the corresponding answer, and finally, we describe the procedures for fewshot meta-training and inference,in which a few gradient steps may provide strong results on a new task can be considered as constructing an internal representation that is generically applicable to numerous tasks.(Yuan and Nguyen, 2023)</p>
<p>Problem Formulation</p>
<p>We focus on the task of ECG-based question answering, where the goal is to predict an answer a given an ECG signal x and a natural language question q.Formally, we aim to learn a function f θ parameterized by θ: a = f θ (x, q).Due to the scarcity of labeled data for certain ECG conditions and the need to generalize to emerging diseases, we adopt a few-shot learning approach.In this setting, we have access to a set of tasks, each consisting of a small support set and a query set.A single meta-learning step refers to an optimization after a support set (i.e., the few-shot samples) is used by the model to learn across different tasks and a query set adapts to a new task (Ravi and Larochelle, 2016).Specifically, let D meta-train denote the meta-training dataset comprising n tasks {T 1 , T 2 , . . ., T n }, where each task T i consists of a support set D s i and a query set D q i : T i = (D s i , D q i ).In the N -way K-shot setting, the support set D s i contains N classes (attribute-answer pairs), each with K labeled examples.Each example in the support and query sets is a triplet (x, q, a), where x is an ECG signal, q is a question about x, and a is the corresponding answer.Our objective is to train a model that can, given the support set D s i of a new task T i , adapt to accurately predict the answers in the query set D q i .This requires the model to generalize to new attribute-answer combinations and diverse question formulations with minimal labeled data.</p>
<p>Meta Learning Benchmark Dataset.We create a benchmarking dataset for meta learning in our study using the ECG-QA dataset (Oh et al., 2024), which contains question-answer pairs annotated by expert clinicians and is built upon the PTB-XL (Wagner et al., 2020) and MIMIC-IV-ECG (Gow et al., 2023) datasets.We focus on questions involving a single ECG and consider three types of questions:</p>
<p>• Single-Verify: Yes/no questions, e.g., "Does this ECG show atrial fibrillation?"</p>
<p>• Single-Choose: Multiple-choice questions selecting from two or more options, e.g., "Which type of noise is present in this ECG: baseline drift or muscle artifact?"</p>
<p>• Single-Query: Open-ended questions requiring specific attribute values, e.g., "What is the heart axis direction in this ECG?"</p>
<p>We create our dataset for few-shot meta learning by categorizing questions based on six types of attributes: SCP codes, noise types, stages of infarction, presence of ectopic beats, heart axis deviations, and numeric features.Each attribute encompasses multiple sub-attributes, leading to a diverse set of attribute-answer pairs.For instance, the SCP codes attribute includes specific diagnoses such as "nondiagnostic T-wave abnormalities" and "conduction disturbances".</p>
<p>Each class in our few-shot learning tasks corresponds to a unique attribute-answer pair.For the Single-Verify questions, classes are formed by pairs of attributes and binary answers (yes/no).Similarly, for Single-Choose questions, classes are based on attributes and possible options (both, none, specific sub-attributes), and for Single-Query questions, classes are defined by attributes and their specific values.</p>
<p>We construct the meta-training dataset D meta-train and the meta-testing dataset D meta-test with mutually exclusive classes to evaluate the model's ability to generalize to unseen attribute-answer pairs.Table 1 summarizes the number of attributes, possible answers, and classes for training and testing datasets in each question type.To ensure diversity and robustness, we include multiple question formulations with the same meaning but diverse expressions within each class.For example, the questions "Is nondiagnostic T-wave abnormality present in this ECG?" and "Does this ECG reveal signs of non-diagnostic T-wave abnormalities?" belong to the same class but provide variability in the language.</p>
<p>Task Definition.Formally, let D meta-train be the set of meta-training data, defined as:
D meta-train = {(D s 1 , D q 1 ), (D s 2 , D q 2 ), . . . , (D s n , D q n )}.
In the context of few-shot learning, N -way refers to the number of distinct attribute-answer pair classes in each task.The support set D s i for the i-th task is defined as:
D s i = N c=1 D i,c
where D i,c represents the set of K labeled examples for the c-th class in the support set:
D i,c = {S (1) i,c , S (2) i,c , . . . , S (K) i,c } Each sample S (j) i,c is de- fined as: S (j) i,c = (x (j) i,c , q (j) i,c , a (j) i,c ) where x (j) i,c is the ECG signal, q (j)
i,c is the input question text, and a
(j) i,c
is the corresponding answer text.The query set D q i contains additional samples from the same classes, with M query samples per class (M &gt; K), where K represents the number of ways in few-shot learning setting.This formulation tests the model's ability to generalize to unseen ECGs and diverse question expressions within the same attribute-answer classes.</p>
<p>Model Architecture</p>
<p>The architecture for ECG-based question answering consists of four main components: (1) a pretrained and frozen text tokenizer and embedder for semantic understanding of questions, (2) a pretrained and frozen ECG encoder for extracting meaningful representations from ECG signals, (3) a trainable multimodal fusion module to align ECG embeddings with the textual representation space, and (4) a text decoder to generate language-based answers, as illustrated in Figure 1.</p>
<p>Text Encoder.We employ a Transformer-based large language model to tokenize and embed the input textual data.Given a set of questions Q = {q 1 , q 2 , . . ., q N } and corresponding answers A = {a 1 , a 2 , . . ., a N }, each question q i is tokenized into a sequence of embeddings S i = {s i,1 , s i,2 , . . ., s i,L }, where L denotes the length of the tokenized question.</p>
<p>ECG Encoder.To extract meaningful representations from ECG signals, we pre-train an ECG encoder based on prior work (Oh et al., 2022).Let X = {x 1 , x 2 , . . ., x N } represent a set of ECG recordings, where each x i ∈ R Ts×C corresponds to an ECG signal with T s time steps and C leads.The ECG encoder processes each x i to produce a sequence of embeddings E i = {e i,1 , e i,2 , . . ., e i,K }, capturing both local and global features of the ECG data.</p>
<p>The encoder incorporates techniques such as Wav2Vec (W2V), Contrastive Masked Segment Comparison (CMSC), and Random Lead Masking (RLM) (Oh et al., 2022), pre-trained on the PhysioNet 2021 dataset (Goldberger et al., 2000 (June 13).The W2V component uses convolutional and Transformer layers to derive contextualized representations from raw ECG signals.CMSC enhances temporal invariance by contrasting adjacent segments within ECG recordings.RLM improves generalization by masking random leads during training, enabling robustness across varying lead configurations.</p>
<p>Multimodal Fusion Mapper (Meta Mapper).</p>
<p>The multimodal fusion module integrates textual and ECG representations to generate a joint embedding for question answering.We transform the ECG embeddings E i into a prefix embedding P i = {p i,1 , p i,2 , . . ., p i,M } that aligns with the dimensionality of the question embeddings S i .This is achieved through a transformation network that projects E i into the same embedding space as S i .</p>
<p>Specifically, we apply linear transformations to E i to obtain query (Q), key (K), and value (V ) matrices, enabling an attention mechanism defined as:
Attention(Q, K, V ) = softmax QK ⊤ √ d k V
, where d k is the dimensionality of the key vectors.This attention mechanism captures interactions between ECG features and the textual context, facilitating effective multimodal fusion.The fusion module's parameters are trainable during meta-learning, allowing adaptation to new tasks.</p>
<p>Text Decoder (Language Model).The text decoder generates the answer a i based on the concatenated embeddings of the ECG prefix P i and the tokenized question S i .Using a Transformer-based language model, the decoder autoregressively produces the answer tokens until an end-of-sequence token is reached or a maximum length is exceeded.By integrating the ECG encoder with the language model through the multimodal fusion module, our architecture effectively leverages both physiological signals and textual information to address the multimodal question-answering task in a few-shot learning setting.</p>
<p>Few-shot Meta Training and Inference.</p>
<p>To enable rapid adaptation to new ECG questionanswering tasks with minimal labeled data, we employ a few-shot meta-learning technique based on Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017).The meta-training process aims to find model parameters that are well-suited for quick fine-tuning on unseen tasks.Inner Loop: Task Adaptation For each task T i , we perform adaptation by minimizing the taskspecific loss L Ti on the support set D s i :
θ ′ i = θ − α∇ θ L Ti (f θ ; D s i )
where θ are the model parameters, θ ′ i are the adapted parameters for task T i , α is the inner-loop learning rate, and f θ denotes the model.The loss L Ti is computed using the negative log-likelihood over the support set:
L Ti (f θ ; D s i ) = − (xj ,qj ,aj )∈D s i log p(a j |x j , q j ; θ)
Outer Loop: Meta-Optimization.After adapting to each task, we evaluate the adapted model f θ ′ i on the corresponding query set D q i and compute the meta-loss:
L meta (θ) = Ti∼p(T ) L Ti (f θ ′ i ; D q i )
The model parameters θ are updated to minimize the meta-loss using gradient descent:
θ ← θ − β∇ θ L meta (θ)
where β is the outer-loop learning rate.This update encourages the learned parameters θ to be easily adaptable to new tasks.</p>
<p>Meta-Testing Phase.In the meta-testing phase, we assess the model's ability to adapt to unseen tasks from the meta-test set D meta-test .For each new task T new , we perform adaptation using the support set
D s new : θ ′ new = θ − α∇ θ L Tnew (f θ ; D s new )
The adapted parameters θ ′ new are then utilized to predict answers on the query set D q new , evaluating the model's generalization to new tasks.</p>
<p>Experiments</p>
<p>Implementation Details</p>
<p>We utilize a self-supervised pre-training strategy of (Oh et al., 2022) (see Section 3.2) for pre-training ECG encoder using the publicly available PhysioNet 2021 Challenge datasets (Goldberger et al., 2000 (June 13).Each ECG recording is sampled at 500 Hz and has a duration ranging from 5 to 144 seconds.For the global contrastive learning task, we segment each recording into 5-second segments (corresponding to 2,500 samples).The rest of the implementation details are provided in Appendix A.1.</p>
<p>Pre-Processing</p>
<p>Due to class imbalance, we exclude data points of classes with fewer than 140 samples for Single-Verify questions, 14 samples for Single-Choose, and 50 for Single-Query question types as described further in Appendix A.2.</p>
<p>Multimodal Fusion Module Architecture</p>
<p>We experiment with multiple mapping approaches tailored to different aspects of feature transformation and use Attention-based Mapper (see in Appendix A.3 ) as a default mechanism due to its superior performance.</p>
<p>Training &amp; Inference Procedures</p>
<p>The optimization of the meta-learning model is performed using the AdamW optimizer with 10,000 meta-training steps and 1,000 meta-testing steps.Rest of the training details are provided in Appendix A.4.</p>
<p>Performance Evaluation</p>
<p>We assess the model's performance by comparing the overlap between the generated answers and the ground truth.Given that the generated sequences may vary in length from the ground truth, we compute the accuracy by aligning the generated sequence to the length of the ground truth: Accuracy = 1 n n i=1 I(â i = a i ), where, âi is the generated token at position i, a i is the ground truth token at the same position, n is the length of the ground truth sequence, and I(•) is the indicator function, which equals 1 if the condition is true and 0 otherwise.Furthermore, we also evaluate the model's performance using various natural language generation (NLG) metrics, including BLEU (Papineni et al., 2002), BertScore (Zhang et al., 2019), and Rouge (Lin, 2004) as these have been broadly utilized to evaluate the LLM generated text (Abbasian et al., 2024).</p>
<p>Results</p>
<p>Here, we evaluate the performance of our approach, analyzing the impact of different design choices and training strategies.We investigate the effectiveness of episodic training, which enables models to quickly adapt to new tasks by simulating distinct tasks for rapid inner loop learning, compare our few-shot generative approach with a fully supervised classification baseline, assess the influence of model size, analyze the performance of different multimodal fusion mappers, and examine the effects of freezing the ECG encoder parameters.We compare our few-shot generative approach with a fully supervised classification baseline.This comparison assesses the influence of model size, analyzes the performance of different multimodal fusion mappers, and examines the effects of freezing the ECG encoder parameters.Finally, we explore the role of meta-knowledge and evaluate performance across various ECG attributes.</p>
<p>Episodic Training and Comparison with Supervised Baselines</p>
<p>We evaluate the effectiveness of episodic training for few-shot multimodal question answering.Table 2 presents the performance of two large language models (LLMs), Gemma-2-2B (Team et al., 2024) and Llama-3.1-8B(Dubey et al., 2024), under various few-shot settings (2-way 5-shot, 2-way 10-shot, 5-way 5-shot, and 5-way 10-shot) and question types (see Table 1, Single-Verify, Single-Choose, Single-Query, and All Single question types).We compare episodic training with standard supervised learning (Baseline) for each LLM.The results demonstrate that episodic training consistently improves performance across all settings and question types, highlighting its ability to generalize to unseen queries.Furthermore, we compare our few-shot generative approach with a fully supervised classification model adapted from image captioning to ECG question answering (Oh et al., 2024) (Upper Bound), which serves as an upperbound on the performance.This model was trained on the original ECG-QA dataset (Oh et al., 2024) and uses exact match accuracy.In contrast, our model's accuracy is measured by the overlap between the ground truth and the generated answer (Section 4.5) and NLG metrics in Table 3 for our key models.Our results also showcase the performance improvement achieved by using a larger LLM (Llama-3.1-8B)compared to a smaller one (Gemma-2-2B).</p>
<p>Furthermore, Appendix B.2 Figure 3 provides a comparative analysis of Gemma-2-2B and Llama-3.1-8B on ECG-related question answering tasks.It shows example ECGs (leads II, V1, and V6) alongside representative questions from each of the three question types.For each query, we present the ground truth (GT) and the models' responses (A), enabling a direct visual comparison of their performance.This visualization complements the quantitative results in Table 2, offering insights into the models' reasoning processes and their ability to extract and articulate information from ECG data across varied question formats.</p>
<p>Impact of Model Scale</p>
<p>We evaluate the 5-way 5-shot setting in single-choose question few-shot performance of several large language models (LLMs) on ECG-language question answering by simply replacing the corresponding LLM in our method, including Gemma-2-2B (Team et al., 2024), Llama-3.1-8B(Dubey et al., 2024), GPT-2 (Radford et al., 2019), Phi-2-2B (Javaheripi et al., 2023), Qwen-2-1.5B(Bai et al., 2023), SmolLM-2-1.7B (Allal et al., 2025), DeepSeek-R1-1.5B (Guo et al., 2025).As shown in Table 4, Llama-3.1-8Bconsistently achieves the highest accuracy across all question types, demonstrating a substantial performance improvement.Specifically, Llama-3.1-8Bexhibits a 2.2%, 14.4%, and 27.5% improvement over the bestperforming 2B parameter model (Gemma-2-2B) on S-Verify, S-Choose, and S-Query, respectively, culminating in a 24.9% overall improvement (All-S).This marked improvement suggests that the increased parameter count of Llama-3.1-8Bfacilitates the learning of richer representations that better capture nuanced relationships between ECG data and corresponding natural language queries.We hypothesize that utilizing an even larger LLM could potentially lead to further significant performance improvements.</p>
<p>While Llama-3.1-8B exhibits superior performance, its computational requirements are substantial.Within the set of 2B parameter models, Gemma-2-2B demonstrates the strongest performance, offering a compelling balance between accuracy and computational efficiency.Accordingly, we adopt Gemma-2-2B as the default model for subsequent ablation studies.</p>
<p>Performance Analysis Across Attribute Types</p>
<p>Table 5 presents the model's performance across various ECG attribute types for three question types in a 2-way 5-shot setting.Overall, the model demonstrates strong performance across the different attribute types.The model achieves particularly high accuracy for the SCP Code attribute across the board, potentially attributable to the larger amount of training data available for this type.Conversely, performance on attributes like extra systole exhibits greater variability, particularly in the single-choose task, suggesting inherent challenges associated with this attribute.The observed differences in accuracy across attribute types underscore the need for potential targeted improvements to enhance model robustness.</p>
<p>Generalization on Cross-Domain Dataset</p>
<p>We investigate the effect of cross-domain datasets on our model's performance under the 2-way 5shot setting.Specifically, we evaluate the model on the MIMIC-IV-ECG dataset across different question types, with PTB-XL results provided for reference, as summarized in Table 6.As the MIMIC-IV-ECG dataset is rather large, we randomly select 30,000 examples from its test set for evaluation to balance computational efficiency with representativeness of the dataset.</p>
<p>Our method demonstrates strong cross-domain capabilities, effectively working well on the MIMIC-IV-ECG dataset when meta-adaptation techniques are incorporated.With meta-adaptation, the model achieves high accuracies of 89.7% in S-Verify and 85.7% in S-Choose question types, closely aligning with the performance on the PTB-XL dataset.This highlights the effectiveness of our approach in adapting to new domains and understanding the nuances of cross-domain data.</p>
<p>While applying the model to the MIMIC-IV-ECG dataset without meta-adaptation results in a performance drop, the accuracy remains reasonable at 76.3% in S-Verify and 49.1% in S-Choose tasks.The incorporation of meta-adaptation significantly enhances the model's ability to generalize across domains, leading to substantial improvements in accuracy.Our method effectively leverages adaptation strategies to bridge the domain gap, enabling robust performance even when dealing with differing data distributions.</p>
<p>Robustness to Question Variations</p>
<p>We investigate the model's robustness to variations in question phrasing, demonstrating its ability to maintain consistent diagnostic interpretations across diversely worded queries.For example, in verification tasks (S-Verify) involving the detection of a specific SCP code, the model effectively processes semantically equivalent questions such as "Is [SCP code] present in this ECG?" and "Does this ECG reveal any signs of [SCP code]?".This indicates a capacity to generalize beyond superficial lexical variations.Table 7 quantifies the impact of phrasing variations across different question types in a 2-way 5-shot setting.While performance modestly decreases with varied phrasing, the model retains a high degree of accuracy, demonstrating its resilience to natural language variability.This robustness is crucial for realworld applications where clinical questions are rarely phrased identically.</p>
<p>Model's Capability with Reduced ECG Leads</p>
<p>We investigate the influence of limiting access to ECG leads on model performance.We evaluate our approach using a reduced number of leads under a 2way 5-shot scenario.Table 8 presents the results, illustrating the effect of lead availability on accuracy across different question types.Using only lead I yields surprisingly high accuracy for S-Verify, demonstrating the model's ability to effectively leverage limited information.While performance on S-Choose and S-Query benefits from additional leads, the strong performance with a single lead highlights the model's efficiency.Incorporating lead II further enhances performance, notably for S-Choose, indicating the importance of this lead for choice selection tasks.While S-Query accuracy sees a minor decrease compared to using all leads, the overall trend suggests a positive impact from incorporat- These results demonstrate that while the model benefits from access to the complete set of ECG leads, it exhibits resilience and strong performance even with limited lead availability.This adaptability suggests the model effectively learns to extract relevant features from available data, enhancing its potential for practical application in scenarios where accessing all leads might be challenging.</p>
<p>Architectural Components Ablation</p>
<p>Multimodal Fusion Mapper.We investigated the efficacy of three distinct multimodal fusion mappers: attention-based, linear, and multilayer perceptron (MLP) (See Section 4.3).In Table 9, we see that the attention-based mapper consistently demonstrated superior performance, achieving an accuracy of 84.5%, compared to 60.9% for the MLP mapper and 72.5% for the linear mapper.This suggests that the attention mechanism's ability to dynamically weigh and integrate modality-specific information is crucial for effective multimodal reasoning in this context.Consequently, we employed the attention-based mapper as the foundation for subsequent ablation experiments.</p>
<p>Freezing ECG Encoder Parameters.We investigate the effects of freezing the pre-trained ECG encoder parameters on few-shot learning performance in Table 9.Specifically, we compare the performance of a model with a frozen ECG encoder against a model where the encoder parameters are allowed to be fine-tuned during training.This evaluation uses the single-choice question type in a 2-way 5-shot setting.Freezing the ECG encoder parameters yields a higher accuracy of 84.5%, compared to 76.7% for the unfrozen encoder.This result suggests that for few-shot learning in this context, leveraging the pretrained representations without further fine-tuning is more effective.Furthermore, freezing the encoder parameters reduces computational overhead and mitigates the risk of overfitting on the limited few-shot data.</p>
<p>Meta-Knowledge Incorporation.Incorporating meta-knowledge significantly improves performance on few-shot learning tasks.Meta-knowledge refers to information about the learning process itself, such as patterns or strategies learned from previous tasks that can be applied to new tasks with limited data (Finn et al., 2017).Table 9 provides these results, where our model achieved 84.5% accuracy on singlechoice questions when leveraging meta-knowledge.Accuracy dropped drastically to 0.3% without learning meta-knowledge, highlighting the critical role of prior information for improved understanding and decision-making in few-shot scenarios.(Rafiei et al., 2024) Impact of Prompt Format on Model Performance.We investigate the influence of prompt variations on model performance for few-shot ECGlanguage question answering.Specifically, we evaluate three prompt variants (P-A, P-B, and P-C) using a 2-way 5-shot learning paradigm on singlechoice questions.Table 10 summarizes the results and demonstrates a clear impact of prompt structure on accuracy.The most effective prompt, P-A ("question: " + question + "answer: "), achieves the highest accuracy (84.5%).This structured format provides explicit cues for the question and expected answer, facilitating the model's comprehension and response generation.In contrast, the simpler P-B variant (question only) results in a lower accuracy of 77.4%, suggesting the importance of contextual cues present in P-A.The P-C variant (question + "the answer can be both, none or in question ") achieves an intermediate accuracy of 80.1%.While the added clarification in P-C might be beneficial in certain scenarios, it does not improve performance compared to the structured approach of P-A.Our findings underscore the critical role of prompt format in optimizing large language model performance for few-shot question answering tasks.</p>
<p>Prompt Variants Accuracy</p>
<p>P-A ("question: " + question + "answer: ") 84.5 P-B (question) 77.4 P-C (question + "the answer can be both, none or in question") 80.1</p>
<p>Conclusion</p>
<p>In this work, we introduce a LLM-agnostic multimodal meta-learning framework for few-shot ECGlanguage question answering, addressing the critical challenges of limited labeled data and evolving task distribution in ECG interpretation.Our framework seamlessly integrates ECG signals with text queries through a trainable multimodal fusion mapper.The empirical evaluation demonstrates superior generalization performance across a range of language models, diverse few-shot learning scenarios, and varying question types.These results underscore the potential of our framework to enhance clinical practice by enabling rapid adaptation to new tasks and patient populations.Our method can be easily extended to multiple ECG comparisons by incorporating multiple ECG prefixes in the LLM decoder.Future research could explore incorporating vision modality (e.g., chest X-ray images) to develop more comprehensive models.Additionally, investigating different ECG encoder variants to enhance model robustness across different patient demographics, hospitals, and ECG devices.Leveraging larger language models (LLMs), and integrating more established few-shot learning methods over multiple, randomly seeds could further improve performance and generalizability.</p>
<p>Meta-Training Phase.During meta-training as shown in Appendix B.1 Figure2, we sample a batch of tasks T i from the task distribution p(T ).Each task T i consists of a support set D s i and a query set D q i .The support set contains N classes with K examples each (N -way K-shot learning), and the query set is used to evaluate adaptation performance.</p>
<p>Table 1 :
1
Overview of question types and data distribution within the meta learning benchmark dataset created for few-shot ECG question answering.
Question Type AttributesAnswersClasses (train:test) SamplesExampleSingle-Verify94yes/no156 (124:32)34,105Q: Does this ECG show 1st degree av block? A: yes/noSingle-Choose165both/none/attr 1/attr 2262 (209:53)47,655Q: Which noise does this ECG show, baseline drift or static noise? A: baseline drift /static noise/both/noneSingle-Query30attr 1/attr 2/. . . /attr n260 (208:52)63,125Q: What direction is this ECG deviated to? A: Normal axis/ . . . ./open-endingAll206yes/no/both/none/. . . /attr n678 (541:137)144,885. . .Encoder12-leads ECGsSelf-Supervised ECG ModelTokenizer... ... ...QuestionsLLM Tokenizer &amp; EmbedderText EmbeddingsAnswers</p>
<p>ECG Embedding Prefix Transformer Mapper ECG Prefix LLM (Decoder-Only) Meta Mapper Decoder Normal axis Yes Static noise Does this ECG show 1st degree av block? Which noise does this ECG show, baseline drift or static noise? ... .. ... What direction is this ECG deviated to? Merged Embeddings ...
Figure 1: Overview of our proposed multimodal few-shot ECG question answering approach, integratingECG signals and textual queries via a fusion module for a frozen LLM to generate answer in anatural language.</p>
<p>Table 2 :
2
Performance comparison (Accuracy %) of few-shot and fully-supervised models on multimodal question answering across various question types and few-shot settings (N-way K-shot).
MethodLanguage Model EpisodicFew-shot SettingQuestions TypeN-Way K-Shot S-Verify S-Choose S-Query All-Single (Combined)Baseline (Supervised)Gemma-2-2B Llama-3.1-8B× ×N/A N/A45.1 83.812.6 34.87.1 25.46.9 25.02-589.084.548.641.2OursGemma-2-2B✓2-10 5-590.9 82.486.1 62.949.3 42.142.7 46.25-1083.465.152.248.52-590.381.363.962.5OursLlama-3.1-8B✓2-10 5-592.7 84.687.2 77.367.6 69.664.7 71.15-1085.879.673.975.3Oh et al. (2024)-×N/A74.657.141.0-</p>
<p>Table 3 :
3
Performance comparison (%) with natural language generation metrics (i.e., BLEU-1, BertScore, and Rouge) of few-shot and supervised (standard) models across question types and few-shot settings (N-way K-shot).
Method Language Model Episodic Few-shot SettingSingle-VerifySingle-ChooseSingle-QueryAll-SingleBLEU BertScore Rouge BLEU BertScore Rouge BLEU BertScore Rouge BLEU BertScore RougeBaselineGemma-2-2B Llama-3.1-8B× ×N/A N/A34.4 69.842.8 92.933.9 69.812.4 37.335.8 68.313.0 38.43.2 15.736.7 53.27.5 17.74.9 12.937.2 54.36.9 17.02-575.894.375.873.487.476.436.067.232.834.969.538.9OursGemma-2-2B✓2-10 5-578.3 60.894.8 90.078.3 60.873.5 48.587.4 72.775.6 50.638.3 25.370.0 61.746.5 32.735.4 32.771.0 69.239.9 35.85-1068.292.168.252.675.454.230.164.837.535.069.739.62-579.995.279.977.888.879.336.367.643.737.873.140.9OursLlama-3.1-8B✓2-10 5-581.2 66.295.6 92.081.2 66.277.9 69.489.3 84.879.4 71.043.0 27.971.9 63.349.7 34.242.1 30.473.8 68.446.5 33.05-1072.872.872.879.690.280.731.065.437.735.270.238.5</p>
<p>Table 4 :
4
Comparison (Accuracy %) of various language models.
Language ModelS-VerifyS-ChooseS-QueryAll-SGPT-2-1.5B72.847.219.823.2Phi-2-2B65.733.051.522.2SmolLM-2-1.7B69.943.517.427.8DeepSeek-R1-1.5B79.952.722.116.1Qwen-2-1.5B70.846.717.920.1Gemma-2-2B82.462.942.146.2Llama-3.1-8B84.677.369.671.1</p>
<p>Table 5 :
5
Accuracy (%) across different attribute types.
Attribute TypeS-VerifyS-ChooseS-QuerySCP code91.185.642.9Noise85.883.455.9Stage of infarction89.886.860.5Extra systole86.475.345.2Heart axis89.389.274.2Table 6: Cross-domain performance (Accuracy %)on MIMIC-IV-ECG.DatasetS-VerifyS-ChooseS-QueryAll-SPTB-XL89.084.548.641.2MIMIC w/o meta adapt76.349.110.413.8MIMIC w meta adapt89.785.739.733.8</p>
<p>Table 7 :
7
Effect (Accuracy %) of question expression type.
Expression Type S-Verify S-Choose S-QuerySame89.084.548.6Different86.584.742.5Table 8: Performance (Accuracy %) with maskedECG leads.LeadsS-VerifyS-ChooseS-QueryI89.679.647.5I, II89.084.245.7I, II, V388.982.447.2All89.084.548.6</p>
<p>Table 9 :
9
Model component ablation.Accuracy (%) on a single-choice question type under 2-way 5-shot setting.
Multimodal fusion mapperECG encoder trainingMapper TypeAccuracyECG EncoderAccuracyAttention Based84.5Frozen84.5MLP60.9Unfrozen76.7Linear72.5Meta-knowledge impactVariantsAccuracyw meta knowledge84.5w/o meta knowledge0.3ing more information. The inclusion of leads I, II, andV3 maintains robust performance across all questiontypes, approaching the accuracy achieved with thefull-lead scenario.</p>
<p>Table 10 :
10
Effect (Accuracy %) of varying prompt structures.</p>
<p>© 2025 J. Tang, T. Xia, Y. Lu, C. Mascolo &amp; A. Saeed.
AcknowledgmentsWe acknowledge the use of the Dutch National Supercomputer Snellius for essential computational tasks.Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.Bertscore: Evaluating text generation with bert.arXiv preprint arXiv:1904.09675,2019.Appendix A. Implementation DetailsA.1. ECG Encoder Pretraining ParametersDuring pretraining, we apply random lead masking by independently masking each lead with a probability of p = 0.5, enhancing the model's robustness to missing or corrupted leads.The ECG encoder is trained using the Adam optimizer with a learning rate of 5 × 10 −5 for 200 epochs.A.2. Dataset Pre-Processing DetailsMeta-training dataset D meta-train and meta-testing dataset D meta-test are composed of data points (x i , q i , a i ) drawn from their respective sets of classes C meta-train and C meta-test , where C meta-train ∩ C meta-test = ∅, ensuring disjoint class sets for training and testing.For each question type, the data were split into 80% for training and 20% for testing.A.3. Multimodal Fusion Module Architecture ParametersAttention-based Mapper utilizes the multi-head attention mechanism with 8 heads, 4 layers, and a dropout rate of 0.5.Similarly, the Linear Mapper applies a linear transformation, i.e., a single-layer model.Furthermore, the MLP Mapper utilizes a feed-forward neural network with 3 layers and ReLU activation with a dropout rate of 0.5 to prevent overfitting.A.4. Training &amp; Inference Procedures ParametersThe meta-level outer learning rate is set to 5 × 10 −4 , while the task-level inner update learning rate is 0.05.The inner update step in meta-learning refers to the process of adapting the model's parameters to a specific task during inner iteration based on the support set(Finn et al., 2017).The task-level inner update steps are set to the default value of 5, and the update steps for fine-tuning are also set to the default value of 15.Due to resource limitations, we train models for one epoch (which roughly takes over a duration of 1-2 days), utilizing a step size of 10,000 split across NVIDIA H100 GPUs.We keep both the ECG encoder and language model frozen, unless mentioned otherwise.Implementation details like seeds will be released with our code.Appendix B. Additional Figures and AnalysisThis appendix contains additional figures and analysis that provide further insights into our experiments and results.The following subsections detail class formation, attribute distribution, meta-learning processes, and qualitative analysis of ECG-related question answers.B.1. Meta-Training and Meta-Testing ProcessesFollowing the Model-Agnostic Meta-Learning (MAML)(Finn et al., 2017) structure, we train the model on a variety of ECG question-answering tasks in the meta-training phase to make it optimize the model's ability to quickly adapt to new tasks with minimal data.We highlight how the model's parameters are adjusted across multiple training episodes, leading to improved accuracy in the few-shot settings presented in the study.We demonstrate the key components of the process that are critical for understanding how the models adapt in figure 2.B.2. ECG-Related Question Answering: Qualitative AnalysisThis figure 3 presents qualitative results comparing two models, Gemma-2-2B and Llama-3.1-8B, on singleverify, single-choose, single-query, 3 ECG-related question types.The analysis helps in understanding the models' performance and their ability to handle various question forms.
Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative ai. Elahe Mahyar Abbasian, Iman Khatibi, David Azimi, Zahra Oniani, Shakeri Hossein, Alexander Abad, Ram Thieme, Zhongqi Sriram, Yanshan Yang, Bryant Wang, Lin, NPJ Digital Medicine. 71822024</p>
<p>. Guido J Julián N Acosta, Pranav Falcone, Eric J Rajpurkar, Topol, Multimodal biomedical ai. Nature Medicine. 2892022</p>
<p>Comprehensive evaluation and performance analysis of machine learning in heart disease prediction. A Halah, Al-Alshaikh, P Prabu, Ramesh Chandra Poonia, Abdul Khader, Jilani Saudagar, Manoj Yadav, Abeer A Hatoon S Alsagri, Alsanad, Scientific Reports. 14178192024</p>
<p>Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. Smollm2: When smol goes big-data-centric training of a small language model. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Guilherme Gabriel Martín Blázquez, Lewis Penedo, Andrés Tunstall, Hynek Marafioti, Kydlíček, arXiv:2502.027372025arXiv preprint</p>
<p>Learning to learn by gradient descent by gradient descent. Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando De Freitas, Advances in neural information processing systems. 292016</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>Making the most of text semantics to improve biomedical vision-language processing. Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, C Daniel, Anton Castro, Stephanie Schwaighofer, Maria Hyland, Tristan Wetscherek, Aditya Naumann, Javier Nori, Alvarez-Valle, European conference on computer vision. Springer2022</p>
<p>A systematic review on ecg and emg biomedical signal using deep-learning approaches. Artificial Intelligence-based Healthcare Systems. Aarti Chugh, Charu Jain, 2023</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, International conference on machine learning. PMLR2017</p>
<p>12 Lead ECG: The Art of Interpretation. B Tomas, Neil E Garcia, Holtz, 2001Jones &amp; Bartlett Learning</p>
<p>A L Goldberger, L A N Amaral, L Glass, J M Hausdorff, P Ch, R G Ivanov, J E Mark, G B Mietus, C.-K Moody, H E Peng, Stanley, 10.1161/01.CIR.101.23.e2151085218PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. 2000. June 13101Circulation Electronic Pages</p>
<p>Andrew Ng, Geoff Tison, and Pranav Rajpurkar. 3kg: Contrastive learning of 12-lead electrocardiograms using physiologically-inspired augmentations. Bryan Gopal, Ryan Han, Gautham Raghupathi, Machine Learning for Health. PMLR2021</p>
<p>. Brian Gow, Tom Pollard, Larry A Nathanson, Alistair Johnson, Benjamin Moody, Chrystinne Fernandes, Nathaniel Greenbaum, Seth Berkowitz, Dana Moukheiber, Parastou Eslami, et al. Mimiciv-ecg-diagnostic electrocardiogram matched subset. Type: dataset. 2023</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network. Pranav Awni Y Hannun, Masoumeh Rajpurkar, Geoffrey H Haghpanahi, Codie Tison, Bourn, Andrew Y Mintu P Turakhia, Ng, Nature medicine. 2512019</p>
<p>Phi-2: The surprising power of small language models. Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César, Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, 2023Microsoft Research Blog</p>
<p>Time-llm: Time series forecasting by reprogramming large language models. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, arXiv:2310.017282023arXiv preprint</p>
<p>Cardiologist-level interpretable knowledge-fused deep neural network for automatic arrhythmia diagnosis. Yanrui Jin, Zhiyuan Li, Mengxiao Wang, Jinlei Liu, Yuanyuan Tian, Yunqing Liu, Xiaoyang Wei, Liqun Zhao, Chengliang Liu, Communications Medicine. 41312024</p>
<p>Clocs: Contrastive learning of cardiac signals across space, time, and patients. Dani Kiyasseh, Tingting Zhu, David A Clifton, International Conference on Machine Learning. PMLR2021</p>
<p>Review of multimodal machine learning approaches in healthcare. Felix Krones, Umar Marikkar, Guy Parsons, Adam Szmul, Adam Mahdi, 10.1016/j.inffus.2024.102690.URLhttps://www.sciencedirect.com/science/article/pii/S1566253524004688formation Fusion. 2025114102690</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Leadagnostic self-supervised learning for local and global representations of electrocardiogram. Jungwoo Oh, Hyunseung Chung, Joon-Myoung Kwon, Dong-Gyun Hong, Edward Choi, Conference on Health, Inference, and Learning. PMLR2022</p>
<p>Ecg-qa: A comprehensive question answering dataset combined with electrocardiogram. Jungwoo Oh, Gyubok Lee, Seongsu Bae, Joonmyoung Kwon, Edward Choi, Advances in Neural Information Processing Systems. 202436</p>
<p>The complete guide to ECGs. H O' James, Keefe, 2008Jones &amp; Bartlett Learning</p>
<p>A systematic review of few-shot learning in medical imaging. Eva Pachetti, Sara Colantonio, Artificial Intelligence in Medicine. 1029492024</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Ope-nAI blog. 1892019</p>
<p>Metalearning in healthcare: A survey. Alireza Rafiei, Ronald Moore, Sina Jahromi, Farshid Hajati, Rishikesan Kamaleswaran, SN Computer Science. 567912024</p>
<p>Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, Degui Zhi, NPJ digital medicine. 41862021</p>
<p>Optimization as a model for few-shot learning. Sachin Ravi, Hugo Larochelle, International conference on learning representations. 2016</p>
<p>Automatic diagnosis of the 12-lead ecg using a deep neural network. Manoel Antônio H Ribeiro, Gabriela Mm Horta Ribeiro, Derick M Paixão, Paulo R Oliveira, Jéssica A Gomes, Milton Ps Canazart, Ferreira, Peter W Carl R Andersson, Wagner Macfarlane, MeiraJr, Nature communications. 11117602020</p>
<p>Multi-task self-supervised learning for human activity detection. Aaqib Saeed, Tanir Ozcelebi, Johan Lukkien, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies20193</p>
<p>Artificial intelligence in cardiovascular diseases: diagnostic and therapeutic perspectives. Xiaoyu Sun, Yuzhe Yin, Qiwei Yang, Tianqi Huo, European Journal of Medical Research. 2812422023</p>
<p>Gemma 2: Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.001182024arXiv preprint</p>
<p>Learning to learn: Introduction and overview. Sebastian Thrun, Lorien Pratt, Learning to learn. Springer1998</p>
<p>Unsupervised representation learning for time series with temporal neighborhood coding. Sana Tonekaboni, Danny Eytan, Anna Goldenberg, arXiv:2106.007502021arXiv preprint</p>
<p>Meta-dataset: A dataset of datasets for learning to learn from few examples. Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, arXiv:1903.030962019arXiv preprint</p>
<p>Advances and challenges in meta-learning: A technical review. Anna Vettoruzzo, Mohamed-Rafik Bouguelia, Joaquin Vanschoren, Thorsteinn Rognvaldsson, Santosh, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024</p>
<p>Ptb-xl, a large publicly available electrocardiography dataset. Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I Lunze, Wojciech Samek, Tobias Schaeffter, Scientific data. 712020</p>
<p>Multimodal machine learning in image-based and clinical biomedicine: Survey and prospects. Elisa Warner, Joonsang Lee, William Hsu, Tanveer Syeda-Mahmood, Charles E KahnJr, Olivier Gevaert, Arvind Rao, International Journal of Computer Vision. 2024</p>
<p>Unified training of universal time series forecasting transformers. Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo, arXiv:2402.025922024arXiv preprint</p>
<p>Zero-shot ecg diagnosis with large language models and retrievalaugmented generation. Han Yu, Peikun Guo, Akane Sano, Machine Learning for Health (ML4H). PMLR2023</p>
<p>Chapter 4 -meta learning by optimization. Pengyu Yuan, Hien Van Nguyen, org/10.1016/B978-0-32-399851-2.00011-9Meta Learning With Medical Imaging and Health Informatics Applications, The MICCAI Society book Series. Hien Van Nguyen, Ronald Summers, Rama Chellappa, Academic Press2023</p>
<p>Combining structured and unstructured data for predictive models: a deep learning approach. BMC medical informatics and decision making. Dongdong Zhang, Changchang Yin, Jucheng Zeng, Xiaohui Yuan, Ping Zhang, 202020</p>            </div>
        </div>

    </div>
</body>
</html>
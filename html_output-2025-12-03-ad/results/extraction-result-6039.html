<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6039 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6039</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6039</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-267770168</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.13887v2.pdf" target="_blank">Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6039.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6039.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human vs MCQ/LLM-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison between human evaluations and probability-based (MCQ) or LLM-based evaluation methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper compares MCQ probability-based evaluation methods (label- and sequence-based) to free-text generation and to human preferences (Chatbot Arena), finding substantial misalignment: probability-based predictions often disagree with generation outputs and with human judgments, especially on some datasets and categories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multiple-choice question answering (MCQ) versus free-text generative LLM output (general question answering / conversational responses)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Probability-based MCQ methods (label-based, sequence-based); literature mentions LLM-evaluators such as GPT-4 and Claude-2 (as background)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Used crowd-sourced human preferences from Chatbot Arena: pairwise comparisons between two anonymous LLM responses aggregated into Elo scores (crowd participants choose preferred answer; large-scale votes aggregated; no per-annotator breakdown provided in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Agreement between probability-based predictions and generation-based predictions; accuracy (MCQ and generation); Spearman correlation between MMLU subcategories and Chatbot Arena Elo; 'correct option overlap' metric for overlap of correctly predicted options across methods/models.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Probability-based MCQ methods (label and sequence) frequently disagree with generation-based outputs and with human preferences. Agreement rates vary by dataset and model; e.g., some models (LLaMA-2-7B) show agreement below 30% with generation on TruthfulQA. Instruction-tuned (chat) models generally align better with generation and human preferences than vanilla models. Correlation with human preferences is higher for social-science categories and much lower for natural-science categories.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Probability-based evaluation (as proxy for LLM judgments) fails to capture verbose, contextual, and nuanced free-text responses; sequence-based aggregation can be especially unreliable; probability methods are sensitive to formatting/order and do not reflect generation behavior after instruction tuning; LLM judges (in cited literature) are biased toward fluent/long answers and can be black-box and non-reproducible.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Concrete inconsistencies: Table 1 example where MCQ-style prediction selects option C while the model's free-text generation gives B; very low agreement (<30%) between probability-based methods and generation on TruthfulQA for some models making such MCQ evaluation results effectively unaligned with generation; MCQ leaderboard improvements that do not match human-preferred outputs in many MMLU subcategories (notably college mathematics, formal logic, college physics).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Recommendations include developing comprehensive evaluation protocols that include free-text generation and human preference metrics, identifying benchmark subsets better aligned to human preferences (with care to avoid overfitting), emphasizing slower, more rigorous research over leaderboard chasing, using instruction tuning to improve alignment, and complementing automatic metrics with human-preference evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6039.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6039.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-judge biases</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Biases and reproducibility issues in using LLMs (e.g., GPT-4, Claude-2) as automatic evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that employing stronger LLMs as automatic evaluators provides scalability but introduces biases (sensitivity to answer order, length, fluency) and reproducibility/transparency issues when evaluators are proprietary black boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Automatic evaluation of open-ended generative outputs (dialogue, summarization, general NLG evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 and Claude-2 are cited as commonly used LLM judges in related work (no experiment in this paper directly uses them as judges).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Discussed as an alternative to human judges: LLM judges run at scale and compare/generate preference judgments, but set-up details vary across cited works; this paper does not run a controlled LLM-judge experiment but discusses prior findings.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Qualitative comparisons: scalability, bias sources (order, length, fluency), reproducibility/transparency; in literature, paired comparisons and automated scoring are typical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>LLM judges scale and are less costly but can systematically prefer longer, more fluent responses even when they are factually incorrect; they are sensitive to presentation order and other surface cues; proprietary LLM judges (e.g., GPT-4) complicate reproducibility and transparency relative to human judges.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Tendency to favor fluency/verbosity over factual correctness; position/length biases; dependence on opaque, black-box systems that may change over time; potential to replicate training-data biases or artifacts; reduced interpretability of judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Cited failure modes include evaluators preferring fluent-but-factually-wrong responses and exhibiting order biases in pairwise comparisons (position effects), making some automated evaluations unreliable or inconsistent with human judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Proposed mitigations (discussed in paper and related work) include: avoid over-reliance on black-box evaluators, develop transparent/replicable evaluator models, correct position and formatting biases (e.g., split-and-merge or bias-correction methods), and combine LLM-evaluators with human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6039.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6039.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chatbot Arena Elo comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use of Chatbot Arena human preference Elo scores to compare MCQ metrics to human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses Chatbot Arena's crowd-sourced pairwise preference data (Elo scores) as a human-preference signal and computes Spearman correlations between MMLU subcategory scores (label/sequence/generation) and human Elo to evaluate alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Dialogue / chatbot response preference evaluation (human preference ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>N/A (human judges via Chatbot Arena; platform compares anonymous LLM responses)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Chatbot Arena: crowd users ask questions and are shown two anonymous LLM responses; users select preferred response; results aggregated across many pairwise votes into Elo scores (platform collects large vote counts; paper references hundreds of thousands of votes but does not break down annotator demographics).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Spearman correlation between MMLU subcategory scores computed under label/sequence/generation evaluation methods and Chatbot Arena Elo (human preference); agreement rates and accuracy comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Correlations between MCQ-derived scores and human Elo vary substantially by subject: social-science categories (world religions, politics, business, public relations) show stronger positive correlations, whereas natural-science categories (college math, formal logic, college physics) show notably weaker correlations, indicating MCQ benchmarks can diverge from human preferences depending on domain.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Human judges themselves are imperfect: prior work and this paper note human evaluators often favor longer and more fluent responses even when inaccurate, may consult LLMs, and exhibit low inter-annotator consistency, complicating use of human preferences as a definitive gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Low Spearman correlations for natural-science MMLU categories with human Elo; improvements in MCQ leaderboard scores do not necessarily translate into higher human-preferred quality in Chatbot Arena comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Short-term: identify benchmark subsets that align better with human preferences; longer-term: craft comprehensive evaluation protocols combining free-text generation quality, contextual understanding, conversational engagement, and human preference signals while avoiding overfitting to curated subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Human feedback is not gold standard <em>(Rating: 2)</em></li>
                <li>Split and merge: Aligning position biases in large language model based evaluators <em>(Rating: 2)</em></li>
                <li>Alpacaeval: An automatic evaluator of instruction-following models <em>(Rating: 2)</em></li>
                <li>Style over substance: Evaluation biases for large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6039",
    "paper_id": "paper-267770168",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "Human vs MCQ/LLM-judge",
            "name_full": "Comparison between human evaluations and probability-based (MCQ) or LLM-based evaluation methods",
            "brief_description": "This paper compares MCQ probability-based evaluation methods (label- and sequence-based) to free-text generation and to human preferences (Chatbot Arena), finding substantial misalignment: probability-based predictions often disagree with generation outputs and with human judgments, especially on some datasets and categories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Multiple-choice question answering (MCQ) versus free-text generative LLM output (general question answering / conversational responses)",
            "llm_judge_model": "Probability-based MCQ methods (label-based, sequence-based); literature mentions LLM-evaluators such as GPT-4 and Claude-2 (as background)",
            "human_evaluation_setup": "Used crowd-sourced human preferences from Chatbot Arena: pairwise comparisons between two anonymous LLM responses aggregated into Elo scores (crowd participants choose preferred answer; large-scale votes aggregated; no per-annotator breakdown provided in paper).",
            "metrics_compared": "Agreement between probability-based predictions and generation-based predictions; accuracy (MCQ and generation); Spearman correlation between MMLU subcategories and Chatbot Arena Elo; 'correct option overlap' metric for overlap of correctly predicted options across methods/models.",
            "reported_differences": "Probability-based MCQ methods (label and sequence) frequently disagree with generation-based outputs and with human preferences. Agreement rates vary by dataset and model; e.g., some models (LLaMA-2-7B) show agreement below 30% with generation on TruthfulQA. Instruction-tuned (chat) models generally align better with generation and human preferences than vanilla models. Correlation with human preferences is higher for social-science categories and much lower for natural-science categories.",
            "llm_specific_limitations": "Probability-based evaluation (as proxy for LLM judgments) fails to capture verbose, contextual, and nuanced free-text responses; sequence-based aggregation can be especially unreliable; probability methods are sensitive to formatting/order and do not reflect generation behavior after instruction tuning; LLM judges (in cited literature) are biased toward fluent/long answers and can be black-box and non-reproducible.",
            "notable_failure_cases": "Concrete inconsistencies: Table 1 example where MCQ-style prediction selects option C while the model's free-text generation gives B; very low agreement (&lt;30%) between probability-based methods and generation on TruthfulQA for some models making such MCQ evaluation results effectively unaligned with generation; MCQ leaderboard improvements that do not match human-preferred outputs in many MMLU subcategories (notably college mathematics, formal logic, college physics).",
            "mitigation_strategies": "Recommendations include developing comprehensive evaluation protocols that include free-text generation and human preference metrics, identifying benchmark subsets better aligned to human preferences (with care to avoid overfitting), emphasizing slower, more rigorous research over leaderboard chasing, using instruction tuning to improve alignment, and complementing automatic metrics with human-preference evaluations.",
            "uuid": "e6039.0",
            "source_info": {
                "paper_title": "Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLM-as-judge biases",
            "name_full": "Biases and reproducibility issues in using LLMs (e.g., GPT-4, Claude-2) as automatic evaluators",
            "brief_description": "The paper notes that employing stronger LLMs as automatic evaluators provides scalability but introduces biases (sensitivity to answer order, length, fluency) and reproducibility/transparency issues when evaluators are proprietary black boxes.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "Automatic evaluation of open-ended generative outputs (dialogue, summarization, general NLG evaluation)",
            "llm_judge_model": "GPT-4 and Claude-2 are cited as commonly used LLM judges in related work (no experiment in this paper directly uses them as judges).",
            "human_evaluation_setup": "Discussed as an alternative to human judges: LLM judges run at scale and compare/generate preference judgments, but set-up details vary across cited works; this paper does not run a controlled LLM-judge experiment but discusses prior findings.",
            "metrics_compared": "Qualitative comparisons: scalability, bias sources (order, length, fluency), reproducibility/transparency; in literature, paired comparisons and automated scoring are typical metrics.",
            "reported_differences": "LLM judges scale and are less costly but can systematically prefer longer, more fluent responses even when they are factually incorrect; they are sensitive to presentation order and other surface cues; proprietary LLM judges (e.g., GPT-4) complicate reproducibility and transparency relative to human judges.",
            "llm_specific_limitations": "Tendency to favor fluency/verbosity over factual correctness; position/length biases; dependence on opaque, black-box systems that may change over time; potential to replicate training-data biases or artifacts; reduced interpretability of judgments.",
            "notable_failure_cases": "Cited failure modes include evaluators preferring fluent-but-factually-wrong responses and exhibiting order biases in pairwise comparisons (position effects), making some automated evaluations unreliable or inconsistent with human judgement.",
            "mitigation_strategies": "Proposed mitigations (discussed in paper and related work) include: avoid over-reliance on black-box evaluators, develop transparent/replicable evaluator models, correct position and formatting biases (e.g., split-and-merge or bias-correction methods), and combine LLM-evaluators with human oversight.",
            "uuid": "e6039.1",
            "source_info": {
                "paper_title": "Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Chatbot Arena Elo comparison",
            "name_full": "Use of Chatbot Arena human preference Elo scores to compare MCQ metrics to human judgments",
            "brief_description": "The paper uses Chatbot Arena's crowd-sourced pairwise preference data (Elo scores) as a human-preference signal and computes Spearman correlations between MMLU subcategory scores (label/sequence/generation) and human Elo to evaluate alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Dialogue / chatbot response preference evaluation (human preference ranking)",
            "llm_judge_model": "N/A (human judges via Chatbot Arena; platform compares anonymous LLM responses)",
            "human_evaluation_setup": "Chatbot Arena: crowd users ask questions and are shown two anonymous LLM responses; users select preferred response; results aggregated across many pairwise votes into Elo scores (platform collects large vote counts; paper references hundreds of thousands of votes but does not break down annotator demographics).",
            "metrics_compared": "Spearman correlation between MMLU subcategory scores computed under label/sequence/generation evaluation methods and Chatbot Arena Elo (human preference); agreement rates and accuracy comparisons.",
            "reported_differences": "Correlations between MCQ-derived scores and human Elo vary substantially by subject: social-science categories (world religions, politics, business, public relations) show stronger positive correlations, whereas natural-science categories (college math, formal logic, college physics) show notably weaker correlations, indicating MCQ benchmarks can diverge from human preferences depending on domain.",
            "llm_specific_limitations": "Human judges themselves are imperfect: prior work and this paper note human evaluators often favor longer and more fluent responses even when inaccurate, may consult LLMs, and exhibit low inter-annotator consistency, complicating use of human preferences as a definitive gold standard.",
            "notable_failure_cases": "Low Spearman correlations for natural-science MMLU categories with human Elo; improvements in MCQ leaderboard scores do not necessarily translate into higher human-preferred quality in Chatbot Arena comparisons.",
            "mitigation_strategies": "Short-term: identify benchmark subsets that align better with human preferences; longer-term: craft comprehensive evaluation protocols combining free-text generation quality, contextual understanding, conversational engagement, and human preference signals while avoiding overfitting to curated subsets.",
            "uuid": "e6039.2",
            "source_info": {
                "paper_title": "Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Human feedback is not gold standard",
            "rating": 2,
            "sanitized_title": "human_feedback_is_not_gold_standard"
        },
        {
            "paper_title": "Split and merge: Aligning position biases in large language model based evaluators",
            "rating": 2,
            "sanitized_title": "split_and_merge_aligning_position_biases_in_large_language_model_based_evaluators"
        },
        {
            "paper_title": "Alpacaeval: An automatic evaluator of instruction-following models",
            "rating": 2,
            "sanitized_title": "alpacaeval_an_automatic_evaluator_of_instructionfollowing_models"
        },
        {
            "paper_title": "Style over substance: Evaluation biases for large language models",
            "rating": 2,
            "sanitized_title": "style_over_substance_evaluation_biases_for_large_language_models"
        }
    ],
    "cost": 0.015258499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models</p>
<p>Chenyang Lyu chenyang.lyu@mbzuai.ac.ae 
Mohamed bin Zayed University of Artificial Intelligence</p>
<p>Minghao Wu minghao.wu@monash.edu 
Monash University</p>
<p>Alham Fikri alham.fikri@mbzuai.ac.ae 
Mohamed bin Zayed University of Artificial Intelligence</p>
<p>HishamNorah Alzahrani 
Abdullah Alyahya 
SultanYazeed Alnumay 
Shaykhah Alsubaie 
Yusef Almushaykeh 
Faisal Mirza 
Nouf Alotaibi 
Rohan Anil 
Andrew M Dai 
MelvinOrhan Firat 
Dmitry Lepikhin 
Alexandre Passos 
Siamak Shakeri 
Emanuel Taropa 
Paige Bailey 
Zhifeng Chen 
Eric Chu 
Jonathan H Clark 
Laurent El 
Yanping Huang 
Kathy Meier-Hellstern 
Gau- Rav Mishra 
Erica Moreira 
Mark Omernick 
Kevin Robinson 
Sebastian Ruder 
Yi Tay 
Kefan Xiao 
Yuanzhong Xu 
Yujing Zhang 
Gustavo Hernández Ábrego 
Junwhan Ahn 
Jacob Austin 
HyungPaul Barham 
Jan A Botha 
James Bradbury 
Siddhartha Brahma 
Kevin Brooks 
Michele Catasta 
Yong Cheng 
Colin Cherry 
Christopher A Choquette-Choo 
Aakanksha Chowdhery 
Clément Crepy 
Shachi Dave 
Mostafa Dehghani 
Sunipa Dev 
Jacob Devlin 
Mark Díaz 
Nan Du 
Ethan Dyer 
Vladimir Feinberg 
Fangxi- Aoyu Feng 
Vlad Fienber 
Markus Freitag 
Xavier Garcia 
Sebastian Gehrmann 
Lucas Gonzalez 
Amanda Askell 
Yuntao Bai 
Anna Chen 
Dawn Drain 
Deep Ganguli 
Tom Henighan 
Andy Jones 
Nicholas Joseph 
Benjamin Mann 
Nova Dassarma 
Nelson Elhage 
Zac Hatfield-Dodds 
Danny Hernandez 
Jack- Son Kernion 
Kamal Ndousse 
Catherine Olsson 
Dario 2020 Amodei 
Tom B Brown 
Jack Clark 
Sam Mc- Candlish 
Christo- Pher Olah 
Jared Kaplan 
Jinze Bai 
Shuai Bai 
Yunfei Chu 
Zeyu Cui 
Kai Dang 
Xiaodong Deng 
Yang Fan 
Wenbin Ge 
Yu Han 
Fei Huang 
Binyuan Hui 
Luo Ji 
Mei Li 
Junyang Lin 
Benfeng Wu 
Jin Xu 
An Xu 
Hao Yang 
Jian Yang 
Shusheng Yang 
Yang Yang 
Bowen Yao 
Hongyi Yu 
Zheng Yuan 
Jianwei Yuan 
Xingxuan Zhang 
Yichang Zhang 
Zhenru Zhang 
Chang Zhang 
Jin- Gren Zhou 
Zhou 
Anna Chen 
Stanislav Fort 
Saurav Kadavath 
Jackson Kernion 
SheerTom Conerly 
El Showk 
Tristan Hume 
Scott Johnston 
SheerShauna Kravec 
Liane Lovitt 
Neel Nanda 
Jack Clark 
Sam Mccandlish 
Sandipan Kundu 
Jackson Kernion 
Anna Chen 
Anna Goldie 
Azalia Mirhoseini 
Cameron Mckinnon 
Carol Chen 
Dustin Li 
Eli Tran-Johnson 
Ethan Perez 
Jamie Kerr 
Jared Mueller 
Jeffrey Ladish 
Joshua Landau 
Kamile Lukosiute 
Michael Sellitto 
Nicholas Schiefer 
Noemí Mercado 
Robert Lasenby 
Robin Larson 
Sam Ringer 
Scott John- Ston 
Tamera Lanham 
Timothy Telleen-Lawton 
Samuel R Bow- Man 
Jared 2022b Kaplan 
Constitutional 
Lucas Bandarkar 
Davis Liang 
Benjamin Muller 
Mikel Artetxe 
Narayan Satya 
Donald Shukla 
Naman Husa 
Abhinandan Goyal 
Luke Krishnan 
Madian Zettlemoyer 
2023 Khabsa 
Anya 2022 Belz 
Craig Thomson 
Ehud Reiter 
Simon 2023b Mille 
Non 
Nick Ryder 
Melanie Subbiah 
Jared D Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
Rewon Child 
Aditya Ramesh 
Daniel Ziegler 
Jeffrey Wu 
Clemens Winter 
Chris Hesse 
Mark Chen 
Eric Sigler 
Ma- Teusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Alec Radford 
Ilya Sutskever 
Wei-Lin Chiang 
Zhuohan Li 
Zi Lin 
Ying Sheng 
Zhanghao Wu 
Hao Zhang 
Lianmin Zheng 
Siyuan Zhuang 
Yonghao Zhuang 
Joseph E Gonzalez 
Ion Stoica 
Eric P 2023 Xing 
Vicuna 
Sharan Narang 
Maarten Bosma 
Gaurav Mishra 
Adam Roberts 
Won Chung 
Charles Sutton 
Parker Schuh 
Kensen Shi 
Sasha Tsvyashchenko 
Joshua Maynez 
Abhishek Rao 
Parker Barnes 
Noam Shazeer 
Vin- Odkumar Prabhakaran 
Emily Reif 
Ben Hutchinson 
Reiner Pope 
Michael Isard 
Guy Gur-Ari 
Pengcheng Yin 
Toju Duke 
Anselm Levskaya 
Sanjay Ghemawat 
Henryk Michalewski 
Vedant Misra 
Liam Fedus 
Denny Zhou 
Daphne Ippolito 
David Luan 
Hyeontaek Lim 
Barret Zoph 
Alexander Spiridonov 
Ryan Sepassi 
David Dohan 
Shivani Agrawal 
An- Drew M Dai 
Thanumalayan Sankaranarayana 
Marie Pellat 
Aitor Lewkowycz 
Oleksandr Polozov 
Katherine Lee 
Zongwei Zhou 
Xuezhi Wang 
Brennan Saeta 
Mark Diaz 
Jason Wei 
Douglas Eck 
Jeff Dean 
Slav Petrov 
Jake Berdine 
Gabriel Bernadett-Shapiro 
Christo- Pher Berner 
Lenny Bogdonoff 
Oleg Boiko 
Made- Laine Boyd 
Anna-Luisa Brakman 
Greg Brockman 
Tim Brooks 
Miles Brundage 
Kevin Button 
Trevor Cai 
Rosie Campbell 
Andrew Cann 
Brittany Carey 
Chelsea Carlson 
Rory Carmichael 
Brooke Chan 
Che Chang 
Fotis Chantzis 
Derek Chen 
Sully Chen 
Ruby Chen 
Jason Chen 
Mark Chen 
Chester Cho 
HyungCasey Chu 
Dave Cummings 
Jeremiah Currier 
Yunxing Dai 
Cory Decareaux 
Thomas Degry 
Noah Deutsch 
Damien Deville 
Arka Dhar 
Steve Dowl- Ing 
Sheila Dunning 
Adrien Ecoffet 
Atty Eleti 
Tyna Eloundou 
David Farhi 
SimónNiko Felix 
Posada Fishman 
Juston Forte 
Is- Abella Fulford 
Leo Gao 
Elie Georges 
Christian Gibson 
Vik Goel 
Tarun Gogineni 
Gabriel Goh 
Rapha Gontijo-Lopes 
Jonathan Gordon 
Morgan Grafstein 
Ryan Greene 
ShixiangJoshua Gross 
Shane Gu 
Yufei Guo 
Chris Hallacy 
Jesse Han 
Jeff Harris 
Yuchen He 
Mike Heaton 
Jo- Hannes Heidecke 
Alan Hickey 
Wade Hickey 
Peter Hoeschele 
Brandon Houghton 
Kenny Hsu 
Shengli Hu 
Xin Hu 
Joost Huizinga 
Shantanu Jain 
Shawn Jain 
Joanne Jang 
Angela Jiang 
Roger Jiang 
Haozhun Jin 
Denny Jin 
Shino Jomoto 
Billie Jonn 
Heewoo Jun 
Tomer Kaftan 
Łukasz Kaiser 
Ali Kamali 
Ingmar Kanitscheider 
Shirish Nitish 
Tabarak Keskar 
Logan Khan 
Jong Wook Kilpatrick 
Christina Kim 
Yongjik Kim 
Hendrik Kim 
Jamie Kirch- Ner 
Matt Kiros 
Daniel Knight 
Łukasz Kokotajlo 
Andrew Kondraciuk 
Aris Kondrich 
Kyle Kon- Stantinidis 
Gretchen Kosic 
Vishal Krueger 
Michael Kuo 
Ikai Lampe 
Teddy Lan 
Jan Lee 
Jade Leike 
Daniel Leung 
ChakMing Levy 
Rachel Li 
Molly Lim 
Stephanie Lin 
Mateusz Lin 
Theresa Litwin 
Ryan Lopez 
Patricia Lowe 
Anna Lue 
Kim Makanju 
Sam Malfacini 
Todor Manning 
Yaniv Markov 
Bianca Markovski 
Katie Martin 
Andrew Mayer 
Bob Mayne 
Scott Mayer Mcgrew 
Christine Mckinney 
Paul Mcleavey 
Jake Mcmillan 
David Mcneil 
Aalok Medina 
Jacob Mehta 
Luke Menick 
Andrey Metz 
Pamela Mishchenko 
Vinnie Mishkin 
Evan Monaco 
Daniel Morikawa 
Tong Mossing 
Mira Mu 
Oleg Murati 
David Murk 
Ashvin Mély 
Reiichiro Nair 
Rajeev Nakano 
Arvind Nayak 
Richard Neelakantan 
Hyeonwoo Ngo 
Long Noh 
Cullen Ouyang 
Jakub O'keefe 
Alex Pachocki 
Joe Paino 
Ashley Palermo 
Giambat- Tista Pantuliano 
Joel Parascandolo 
Emy Parish 
Alex Parparita 
Mikhail Passos 
Andrew Pavlov 
Adam Peng 
Filipe Perel- Man 
Belbute De Avila 
Michael Peres 
Henrique Petrov 
Ponde 
MichaelOliveira Pinto 
Michelle Pokrass 
Vitchyr Pong 
Tolly Pow- Ell 
Alethea Power 
Boris Power 
Elizabeth Proehl 
Raul Puri 
Jack Rae 
Cameron Raymond 
Francis Real 
Kendra Rimbach 
Carl Ross 
Bob Rotsted 
Henri Roussez 
Nick Ry- Der 
Mario Saltarelli 
Ted Sanders 
Shibani Santurkar 
Heather Schmidt 
David Schnurr 
John Schulman 
Daniel Selsam 
Kyla Sheppard 
Toki Sherbakov 
Jessica Shieh 
Sarah Shoker 
Szymon Sidor 
Maddie Simens 
Jordan Sitkin 
Katarina Slama 
Ian Sohl 
Benjamin Sokolowsky 
Yang Song 
Natalie Staudacher 
Samuel Wolrich 
Hannah Wong 
Lauren Workman 
Sherwin Wu 
Michael Wu 
Kai Xiao 
Tao Xu 
Sarah Yoo 
Kevin Yu 
Qim- Ing Yuan 
Wojciech Zaremba 
Rowan Zellers 
Chong Zhang 
Marvin Zhang 
TianhaoShengjia Zhao 
Long Ouyang 
Xu Jiang 
Diogo Almeida 
Carroll Wainwright 
Pamela Mishkin 
Chong Zhang 
Alex Gray 
Jacob Hilton 
Fraser Kelton 
Luke Miller 
Peter Welinder 
Paul Christiano 
Jan Leike 
Ryan 2022 Lowe 
Baptiste Rozière 
Jonas Gehring 
Fabian Gloeckle 
Sten Sootla 
XiaoqingItai Gat 
Ellen Tan 
Yossi Adi 
Jingyu Liu 
Tal Remez 
Jérémy Rapin 
Artyom Kozhevnikov 
Ivan Evtimov 
Joanna Bitton 
Manish Bhatt 
Cristian Canton Ferrer 
Aaron Grattafiori 
Wen- Han Xiong 
Alexandre Défossez 
Jade Copet 
Faisal Azhar 
Hugo Touvron 
Louis Martin 
Nicolas Usunier 
Thomas Scialom 
Gabriel 2023 Synnaeve 
Code 
Victor Sanh 
Albert Webson 
Colin Raffel 
Stephen H Bach 
Lintang Sutawika 
Zaid Alyafeai 
Antoine Chaffin 
Arnaud Stiegler 
Arun Raja 
Manan Dey 
Saiful Bari 
Canwen Xu 
Urmish Thakker 
Shanya Sharma 
Eliza Szczechla 
Taewoon Kim 
Gunjan Chhablani 
Nihal V Nayak 
Debajyoti Datta 
MikeJonathan Chang 
Tian-Jian Jiang 
Han Wang 
Matteo Manica 
Sheng Shen 
Zheng Xin Yong 
Harshit Pandey 
Rachel Bawden 
Thomas Wang 
Tr- Ishala Neeraj 
Jos Rozen 
Abheesht Sharma 
An- Drea Santilli 
Thibault Févry 
Jason Alan Fries 
Ryan Teehan 
Teven Le Scao 
Stella Biderman 
Angela Fan 
Christopher Akiki 
El- Lie Pavlick 
Suzana Ilic 
Daniel Hesslow 
Roman Castagné 
Alexandra Sasha Luccioni 
François Yvon 
Matthias Gallé 
Jonathan Tow 
Alexander M Rush 
Pawan Sasanka 
Benoît Sagot 
Niklas Muennighoff 
Albert Villanova 
Del Moral 
Olatunji Ruwase 
Stas Bekman 
Angelina Mcmillan-Major 
Iz Beltagy 
Huu Nguyen 
Lucile Saulnier 
Samson Tan 
Pedro Ortiz Suarez 
Vic- Tor Sanh 
Hugo Laurençon 
Yacine Jernite 
Julien Launay 
Margaret Mitchell 
Aaron Gokaslan 
Adi Simhi 
Aitor Soroa 
Amit Alfassy 
Anna Rogers 
Ariel Kreisberg Nitzav 
Chenghao Mou 
Chris Emezue 
Christopher Klamm 
Colin Leong 
Daniel Van Strien 
Anastasia Shimorina 
Aarohi Srivastava 
Abhinav Rastogi 
Abu Awal 
Md Shoeb 
Abubakar Abid 
Adam Fisch 
Adam R Brown 
Adam Santoro 
Aditya Gupta 
Adrià Garriga-Alonso 
Agnieszka Kluska 
Akshat Agarwal 
Alex Ray 
Alex Warstadt 
Alexander W Kocurek 
Ali Safaya 
Ali Tazarv 
Alice Xiang 
Alicia Par- Rish 
Allen Nie 
Aman Hussain 
Amanda Dsouza 
Ameet Rahane 
Anantharaman S Iyer 
Anders Andreassen 
Andrea Santilli 
Andreas Stuhlmüller 
Andrew M Dai 
Andrew La 
Andrew K Lampinen 
Andy Zou 
Angelica Chen 
Anh Vuong 
Animesh Gupta 
Anna Gottardi 
Anto- Nio Norelli 
Anu Venkatesh 
Arash Gholamidavoodi 
Arfa Tabassum 
Arul Menezes 
Arun Kirubarajan 
Asher Mullokandov 
Ashish Sabharwal 
Austin 
Thibaut Lavril 
Gautier Izacard 
Xavier Martinet 
Marie-Anne Lachaux 
Timothée Lacroix 
Naman Goyal 
Eric Hambro 
Aurélien Rodriguez 
Armand Joulin 
Aurelien Rodriguez 
Kevin Stone 
Peter Al- Bert 
Amjad Almahairi 
Yasmine Babaei 
Nikolay Bashlykov 
Soumya Batra 
Prajjwal Bhargava 
Shruti Bhosale 
Dan Bikel 
Lukas Blecher 
Moya Chen 
Guillem Cucurull 
David Esiobu 
Jude Fernandes 
Jeremy Fu 
Wenyin Fu 
Brian Fuller 
Cynthia Gao 
Vedanuj Goswami 
An- Thony Hartshorn 
Saghar Hosseini 
Rui Hou 
Hakan Inan 
Marcin Kardas 
Viktor Kerkez 
Madian Khabsa 
Isabel Kloumann 
PunitArtem Korenev 
Singh Koura 
Jenya Lee 
Di- Ana Liskovich 
Yinghai Lu 
Yuning Mao 
Xavier Mar- Tinet 
Todor Mihaylov 
Pushkar Mishra 
Igor Moly- Bog 
Yixin Nie 
Andrew Poulton 
Jeremy Reizen- Stein 
Rashi Rungta 
Kalyan Saladi 
Alex Wang 
Yada Pruksachatkun 
Nikita Nangia 
Aman- Preet Singh 
Julian Michael 
Felix Hill 
Omer Levy 
Samuel R Bowman 
Superglue 
Yizhong Wang 
Swaroop Mishra 
Pegah Alipoormo- Labashi 
Yeganeh Kordi 
Amirreza Mirzaei 
Atharva Naik 
ArutArjun Ashok 
Selvan Dhanasekaran 
Anjana Arunkumar 
David Stap 
Eshaan Pathak 
Giannis Karamanolakis 
Haizhi Lai 
Ishan Puro- Hit 
Ishani Mondal 
Jacob Anderson 
Kirby Kuznia 
Krima Doshi 
Kuntal Kumar Pal 
Maitreya Patel 
Mehrad Moradshahi 
Mihir Parmar 
Mirali Purohit 
PhaniNeeraj Varshney 
Rohitha Kaza 
RavsehajPulkit Verma 
Singh Puri 
Rushang Karia 
Savan Doshi 
Shailaja Keyur Sampat 
Siddhartha Mishra 
Sujan Reddy 
Sumanta Patro 
Tanay Dixit </p>
<p>Nora Altwairesh
Areeb Alowisheq2024</p>
<p>Runji Lin
Dayiheng Liu, Chengqiang Lu, Jianxin MaGao Liu, Keming Lu, Rui Men</p>
<p>Xingzhang Ren, Xuancheng Ren
Sinan Tan, Jianhong Tu, Peng Wang, Shijie WangChuanqi Tan, Wei Wang, Shengguang</p>
<p>Fe-lipe Petroski Such
Natalie Summers
Ilya Sutskever
Jie Tang</p>
<p>Nikolas Tezak
Phil Tillet, Elizabeth Tseng, Jerry TworekMadeleine Thompson, Amin Tootoonchian, Pre-ston Tuggle, Nick Turley</p>
<p>Juan Fe-lipe Cerón Uribe
Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter WelinderAlvin Wang, Ben Wang</p>
<p>Ji-ayi Weng
Lilian Weng, Matt Wiethoff, Dave Willner</p>
<p>Texas. Association for Computational Linguistics</p>
<p>David Ifeoluwa Adelani</p>
<p>Alan Schelten
Ruan Silva</p>
<p>Eric Michael Smith
Ranjan Subrama-nian
Ross Tay-lor, Adina WilliamsXiaoqing Ellen Tan, Binh Tang</p>
<p>Jian Xiang Kuan
Puxin XuZheng Yan</p>
<p>Iliyan Zarov
Angela Fan, Melanie Kambadur, Sharan NarangYuchen Zhang</p>
<p>Aurelien Ro-driguez
Sergey Edunov, and Thomas Scialom. 2023cRobert Stojnic</p>
<p>3261-3275VancouverBCCanada, pages</p>
<p>Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models
361E20E89B90188F9321EA3231D68E2DGrave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.CoRR, abs/2112.00861
Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research.However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios.While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question.This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations.Our empirical investigation reveals that the prevalent probabilitybased evaluation method inadequately aligns with generation-based prediction.Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations.We illustrate that these probability-based approaches do not effectively correspond with generative predictions.The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have significantly advanced the field of natural language processing (NLP), reshaping the paradigms in NLP research and application (Ouyang et al., 2022;Wei et al., 2022;Sanh et al., 2022;Chung et al., 2022;OpenAI, 2023;Anil et al., 2023;Touvron et al., 2023a,c;Jiang et al., 2023).As the scale of model parameters of language models expands from the million to billion or even trillion levels, a proficient LLM is expected to exhibit a broad mastery across † Equal contribution various tasks.Recent works aim to assess LLMs comprehensively by aggregating a substantial array of NLP benchmarks (Srivastava et al., 2022;Sanh et al., 2022;Liang et al., 2022;Longpre et al., 2023).Additionally, there exists a line of research that curates human exam questions to challenge LLMs (Hendrycks et al., 2021;Huang et al., 2023;Li et al., 2023b;Koto et al., 2023).The collected questions and NLP benchmarks are adapted into prompts via standardized templates.</p>
<p>Due to computational constraints, recent evaluation frameworks commonly adopt the approach of selecting the option with the highest probability as the prediction of LLMs, as illustrated in Figure 1.These frameworks employ either labelbased prediction, which assesses the probability of the next token output, or sequence-based prediction, which evaluates the probability of an entire option, ultimately selecting the option with the highest probability as the LLM's prediction.However, these probability-based evaluation methodologies introduce a misalignment between evaluation procedures and real-world application scenarios, where LLMs are typically tasked with generating responses to user queries.This misalignment raises an important question: Is the probability-based evaluation method sufficient to accurately assess the capabilities of LLMs?</p>
<p>In this position study, we argue that the current LLM evaluation and leaderboard misalign the actual LLM capabilities.We examine three prediction methodologies: generation-based, label-based, and sequence-based predictions.We conducted extensive experiments across LLMs with varying model sizes on three prominent benchmarks: MMLU (Hendrycks et al., 2021), TruthfulQA (Lin et al., 2022), andBelebele (Bandarkar et al., 2023).Our findings reveal a significant disconnect between probability-based methods and generation-based predictions.Even when predictions are correct, the consistency between probability-based meth- The second most common element in the solar system is oxygen.Oxygen makes up about 35% of the mass of the Earth's crust and is also found in many other planets and moons in the solar system.It is a critical component of water, which … misaligned</p>
<p>• Short, constrained answer • can't measure reasoning • often factual and unambiguous</p>
<p>• Verbose, free-text generation • explanation could be wrong ods and generation-based predictions remains notably low.We additionally find that many of these multiple-choice NLP benchmark rankings do not agree with human preference for free-text generation output.Consequently, these results raise serious doubts about the reliability of evaluation outcomes derived from popular benchmarks reliant on probability-based methods.In conclusion, our research emphasizes the urgent need for an evaluation approach that ensures accurate and reliable assessments of LLM capabilities, more closely aligned with real-world usage scenarios.In next section, we will discuss the course of the development and paradigm of the evaluation of LLMs.
•
2 Evaluating Large Language Models</p>
<p>Challenges in Evaluating Large Language Models</p>
<p>The advancement of LLMs has substantially broadened their capabilities, transcending conventional NLP tasks.They now demonstrate proficiency in tackling intricate prompts and a wide spectrum of open-ended inquiries.However, unlike tasks with definitive solutions, open-ended questions lack a single correct answer, making it difficult to gauge the LLM's performance.Recently, human evaluators have been deployed to appraise responses to open-ended questions using two primary methods.Firstly, evaluators assign scores based on specific criteria such as ac-curacy and relevance (Wang et al., 2023b;Zhou et al., 2023).Alternatively, they conduct comparative assessments by selecting the preferred answer among two distinct LLM responses to the same question (Askell et al., 2021;Bai et al., 2022a;Zheng et al., 2023b).However, manual evaluation faces significant scalability challenges due to the high costs associated with human judges.Moreover, recent studies indicate that human evaluators often favor longer and more fluent responses, even if they contain factual inaccuracies (Wu and Aji, 2023).Additionally, ensuring the trustworthiness of evaluations presents a concern, as crowdannotators increasingly rely on tools like LLMs for assistance (Veselovsky et al., 2023), raising questions about the purely human-based nature of evaluations.Moreover, maintaining consistent evaluation quality across a large team of evaluators necessitates extensive coordination and rigorous standardization.Recent research highlights low consistency among human evaluators when assessing LLM responses to open-ended questions.</p>
<p>Another approach to evaluating generative LLMs involves utilizing a stronger LLM as the evaluator, offering greater scalability compared to human judges (Zheng et al., 2023b;Wu and Aji, 2023;Liu et al., 2023).However, LLM judges may exhibit biases in their assessments, influenced by factors such as the order and length of answers, as well as their fluency.Furthermore, commonly used LLM judges, like GPT-4 (OpenAI et al., 2023), often operate on public yet black-box systems, posing challenges in ensuring the reproducibility and transparency of the evaluation process.</p>
<p>Multiple Choice Question as a Proxy</p>
<p>Due to the challenges discussed in Section 2.1, recent works commonly convert the multiple-choice questions (MCQs) in human exams to prompts using standard template.The responses generated by the LLMs are then compared against the humancrafted ground truth, allowing for an assessment of the model's accuracy.This process streamlines the evaluation and provides a clear metric for understanding the capabilities of LLMs.</p>
<p>Recent frameworks frequently utilize the output probabilities from LLMs across various options for making predictions, to ensure that the prediction from the LLM is among these options, given the unpredictability of the text generated by LLMs.For example, as illustrated in Figure 1, when presented with the question and the candidate choices, some approaches compare the probabilities predicted by the model based solely on the option letters (Hendrycks et al., 2021), † while others consider the probability of each token and aggregate them (Gao et al., 2021).†</p>
<p>Misalignment between MCQ and User-Facing Interaction</p>
<p>We argue that MCQ-proxy might not always reflect the actual performance of LLM under user-facing free-text generation.In MCQ, LLM output is restricted to a limited set of answers; hence, their answer might be different under unrestricted generation.MCQ benchmarks also often only look for a short and direct answer, whereas user-facing interaction expects the LLM to provide a verbose answer; especially after preference tuning.Hence, MCQ benchmarks are not suitable for measuring the nuanced answers of LLMs.</p>
<p>Additionally, prior studies have shown LLM's brittleness under MCQ benchmarks, e.g., on how the option order is presented (Zheng et al., 2023a;Pezeshkpour and Hruschka, 2023;Alzahrani et al., 2024).Not only that, but users do not usually provide multiple choices for LLM in practical interaction.Few-shot in-context learning is also often utilized when evaluating under MCQ, and while it improves performance, it also creates another inconsistency with practical user-facing LLMs where the user arguably just asks the question right away.</p>
<p>Question domain mismatch between MCQ and user-facing interaction presents another challenge.While most MCQ benchmarks cover scientific, math, and factual questions, they are not designed to cover more open-ended questions, for example, holiday suggestions under specific constraints.They do not cover creative-type questions such as story-writing.Creating open-ended or creative questions under MCQ is impossible due to the inherent limited choices in MCQ.Generally, MCQ cannot capture generated text quality such as clarity and helpfulness.Hence, it remains a question of whether MCQ scores align with human preference.</p>
<p>The rapid advancement of LLMs and their increased accessibility to general users make the aforementioned issues more pressing.The focus on fast research and SoTA-chasing over a scientific understanding of LLM development further exacerbates the situation (Nityasya et al., 2023).Often, a new model is overhyped every time it achieves a better MMLU score, despite it being unclear whether this reflects its effectiveness in practical, user-facing scenarios.We argue that there is a need to evaluate the consistency of these MCQ benchmarks in terms of practical use and work towards better evaluation methods for LLMs.In Section 3, we demonstrate empirical evidence verifying whether these evaluation methodologies faithfully reflect the capability of LLMs.</p>
<p>Empirical Evidence</p>
<p>In this section, we empirically show that MCQ performance does not reflect free-text generation performance.</p>
<p>Experiment Setup</p>
<p>In this section, we describe our experimental setup, including the benchmark datasets, models, and prediction methods.</p>
<p>Datasets In this work, we conduct our experiments on three popuplar benchmarks: MMLU (Hendrycks et al., 2021), TruthfulQA (Lin et al., 2022), andBelebele (Bandarkar et al., 2023).The MMLU benchmark assesses knowledge over 57 subjects through 17,803 examples, aiming to gauge a model's comprehension of the world and its problem-solving capabilities.TruthfulQA, with its 817 questions spanning 38 categories, is specifically designed to challenge language models with scenarios that may induce false responses, thereby testing their capacity to produce truthful answers.Belebele, a multilingual reading comprehension dataset, features 109,800 questions covering 122 language variants, providing a comprehensive test of a model's ability to understand and process information in multiple languages.</p>
<p>Models In this study, we undertake comprehensive experimentation across a range of LLMs, including LLaMA-1 (Touvron et al., 2023b), Vicuna (Chiang et al., 2023), LLaMA-2 (Touvron et al., 2023c), and Mistral (Jiang et al., 2023).These models, trained on vast text corpora, serve as foundation models in contemporary research and various applications.LLaMA-1, Vicuna, and LLaMA-2 (Touvron et al., 2023b;Chiang et al., 2023;Touvron et al., 2023c) are LLMs trained on 2 trillion tokens, noted for their performance and safety in various evaluations.Mistral 7B (Jiang et al., 2023) features 7.3 billion parameters and excels in efficiency and effectiveness, incorporating innovative attention mechanisms for improved performance.</p>
<p>Prediction Methods In this work, we evaluate the models with the following prediction methods:</p>
<ol>
<li>label-based prediction: We provide the prompt "{question} {options} The correct answer is" to LLMs and then calculate the probability of the next token for each option letter (e.g., "A", "B", "C", "D" for four options).</li>
</ol>
<p>The option with the highest probability is selected as the predicted answer.This method was used in the original implementation of MMLU (Hendrycks et al., 2021).2. sequence-based prediction: We provide the prompt "{question} {options} The correct answer is option" to LLMs.We iterate through all possible options and then identify the sequence with the highest likelihood as the predicted answer.This method is used in the Language Model Evaluation Harness (LMEH) framework (Gao et al., 2021).3. generation-based prediction: Unlike the previous two methods, we allow LLMs to generate a response to the input question, mirroring how people typically use LLMs.</p>
<p>Results and Analysis</p>
<p>Inconsistent Predictions between Probability-Based Methods and Generation Experimental  results on MMLU (Hendrycks et al., 2021), Truth-fulQA (Lin et al., 2022), andBelebele (Bandarkar et al., 2023) are shown in Table 2 and Figure 2. Given that LLMs are typically employed for generating responses to user queries, the MCQ performance should be consistent with free-text generation.Recent research commonly utilizes accuracy, which measures the percentage of correct predictions, to assess model performance.In addition to accuracy, we introduce agreement with the generation-based predictions to differentiate the predictions provided by various methods.Agreement is defined as the percentage of consistent predictions between two prediction methods.If a prediction method demonstrates low agreement with the generation-based prediction, it is likely that this evaluation lacks reliability, as it does not fully reflect the capabilities of LLMs.
Label-Gen Seq-Gen M is tr a l-7 B M is tr a l-7 B -I n s tr u c t L L a M A -1 -7 B V ic u n a -7 B L L a M A -2 -7 B L L a M A -2 -7 B -c h a t L L a M A -2 -1 3 B L L a M A -2 -1 3 B -c h a t L L a M A -2 -7 0 B L L a M A -2 -7 0 B -c
Based on our MMLU results presented in Table 2, it is evident that smaller base language models such as Mistral-7B, LLaMA-1-7B, and LLaMA-2-7B face difficulties in achieving consensus with generation-based predictions when utilizing both label-based and sequence-based methods.Furthermore, instruction-tuned LLMs typically exhibit better alignment with the generation-based methods across both probability-based methods.Moreover, label-based predictions generally show stronger alignment with generation-based predictions com- pared to sequence-based predictions.Furthermore, we also evaluate LLMs on Truth-fulQA, as shown in Table 2.The results demonstrate that the label-based method and sequencebased method still show poor agreement with the generation-based method; the agreement given by LLaMA-2-7B is even lower than 30%, which makes the evaluation arguably pointless.Moreover, as shown in Figure 2, the gap between different accuracies (∆) is even larger compared to the ∆ on MMLU -the smallest ∆ is close to 5, and the largest ∆ is more than 20.Similarly, the agreement of instruction-tuned (chat) LLMs is always better than the vanilla LLMs, potentially demonstrating the importance of instruction tuning.The results on both MMLU and TruthfulQA in Table 2 strongly question the reliability of label-based and sequencebased methods for evaluating LLMs while MMLU and TruthfulQA are widely employed benchmarks to demonstrate the capability of LLMs.</p>
<p>Additionally, we evaluate LLMs on a recently built benchmark MRC dataset, Belebele (Bandarkar et al., 2023), which can reduce the risk of data contamination for LLMs.Surprisingly, we observe a much higher agreement between the labelbased method and the generation-based method in Table 2, where the lowest agreement is even higher than 60%, and there are three LLMs whose agreement is close to 90%.However, we observe a lower agreement between the sequence-based prediction and the generation-based prediction.We also observe that the ∆ between the accuracy of the sequence-based prediction and the generationbased prediction is much smaller, suggesting that the label-based method is more accurate.</p>
<p>Overall, our analysis of three datasets reveals that the predictive performance of LLMs can be significantly influenced by various factors.Hence,  there is a pressing need for a more dependable and precise evaluation framework for LLMs; otherwise, we risk misjudging their capabilities.</p>
<p>Inconsistent Correct Predictions In Table 2 and Figure 2, we highlight the low consistency among prediction methods.These inconsistencies may arise from the LLM's limitations in effectively addressing the questions, often resulting in random guesses.To address this issue, we introduce a new metric -correct option overlap -designed to gauge the level of agreement among correctly predicted options from various LLMs.We analyze the overlap of accurately predicted options across different LLMs and present the findings in Table 3.It is evident that Mistral models and LLaMA-1-7B exhibit low overlap rates when evaluated using the label-based approach.Conversely, when employing the sequence-based method, all LLMs show a reduced overlap rate on TruthfulQA, averaging around 30%.However, label-based methods consistently yield higher overlap rates for LLaMA-2 models.These results suggest that predictions from these LLMs are subject to high uncertainty, indicating instability in their predictions across popular benchmarks, regardless of evaluation method-be it label-based or sequence-based.Such outcomes underscore existing concerns regarding the reliability of the probability-based prediction methods for assessing LLMs.</p>
<p>Correlation to Human Preferences We extend our investigation to determine if probability-based prediction methods exhibit discrepancies with human preferences.Specifically, we analyze Spearman's correlation between the outcomes from the sub-categories of the MMLU and the human preferences gathered from the Chatbot Arena (for further details, refer to Section A.2), focusing on five LLMs that are addressed in both our study and the Chatbot Arena.</p>
<p>We present the categories showing the top-5 and bottom-5 correlations with Elo scores in Figure 3.Our analysis reveals that LLMs exhibit stronger correlations with human preferences in social science subjects (such as world religions, politics, business, and public relations) from MMLU, while displaying notably lower consistency with human judgments in natural science subjects (including college mathematics, formal logic, and college physics).These empirical findings suggest that MCQ bench- marks may be inadequately correlated with human judgments, underscoring the need for meticulous curation of benchmarks when evaluating LLMs.Additionally, it is important to note that human judgments themselves may be subject to biases, highlighting the complexity and caution of relying solely on human judgments (Wu and Aji, 2023;Hosking et al., 2023).</p>
<p>More Disagreement under Few-shot Learning LLMs typically demonstrate superior performance in few-shot in-context learning compared to zeroshot generation (Dong et al., 2022).Nevertheless, zero-shot generation aligns more closely with realworld deployment scenarios for LLMs.Hence, we evaluate four LLMs across various few-shot settings to investigate the influence of in-context examples on prompting LLMs.The results, illustrated in Figure 4, reveal a decline in agreement between probability-based and generation-based prediction methods for all selected LLMs with K in-context examples provided.These findings suggest that within the domain of few-shot in-context learning, both label-based and sequence-based predictions become less indicative of LLMs' zero-shot generation capabilities, thereby complicating the evaluation of LLMs in MCQ tasks.</p>
<p>Effect of Multilingual Evaluation</p>
<p>We conducted additional experiments on multilingual Belebele to evaluate the performance of two large language models (LLMs), Mistral-7B and LLaMA-2-7B, in languages beyond English.Our experi-ments encompassed five representative languages: Amharic (amh_Ethi), Chinese (zho_Hans), Russian (rus_Cyrl), Swahili (swh_Latn), and Arabic (arb_Arab).The results, depicted in Figure 5, indicate that LLMs exhibit lower agreement between sequence-based predictions and generation-based predictions compared to the agreement observed between label-based predictions and generationbased ones.Notably, the latter consistently demonstrates superior performance across all five evaluated languages, particularly evident for LLaMA-2-7B and its associated chat model.Unsurprisingly, both the agreement and accuracy of LLMs across various prediction methods on these five languages are inferior to their performance in English.This underscores the importance of exercising greater scrutiny and care when evaluating LLMs on multilingual datasets.</p>
<p>Moving Forward</p>
<p>To make sure the future research in LLMs more reliable, it is crucial to reevaluate our current benchmarks and evaluation methodologies.Our analysis indicates a misalignment between these traditional evaluation mechanisms, primarily MCQbased benchmarks and output probability metrics, and the practical usage of generative text applications in LLMs.The prevalent focus on these benchmarks, although useful for fast and quantitative comparison, falls short of capturing the full spectrum of LLM capabilities.</p>
<p>In response to these challenges, we propose several forward-looking recommendations for the LLM research community: Do Not Take Leaderboard Scores at Face Value: The emphasis on leaderboard rankings, while serving as a proxy for LLM performance, often overlooks the complexity of tasks that LLMs are now being developed to perform.As a community, we should not be easily over-hyped with leaderboard chasing, especially considering the limitations on either MCQ-based, or voting-based leaderboards as discussed in this paper.</p>
<p>Develop Comprehensive Evaluation Protocols: Future research should focus on creating evaluation frameworks that encompass a broader range of LLM capabilities.The discrepancy between evaluation measures and real-world applicability underscores the necessity for a more holistic approach to LLM evaluation.This includes not just traditional benchmarks but also metrics that eval-uate free-text generation, contextual understanding, and conversational engagement.Crafting these comprehensive evaluation protocols will be challenging yet essential for a deeper understanding of LLM performance and applicability.</p>
<p>Embrace Slow Research: The field should adopt a more deliberate pace of research, prioritizing understanding over the speed of advancement and leaderboard-chasing.Given the rapid advancements in LLMs, there has been a noticeable rush to create the next generation of these models, often at the expense of scientific understanding.A consequence of this is that as these LLMs are evaluated using current benchmarks, their development begins to overfit to top the leaderboard.By slowing down and focusing more on understanding, we also allow more time for work on evaluation methods, potentially leading to more robust solutions.</p>
<p>Align Benchmarks with Human Preferences: As a short-term measure, identifying benchmark subsets that more closely mirror human preferences can help improve the correlation between traditional evaluation metrics and the generative capabilities of LLMs.However, this strategy must be balanced with caution to prevent the overfitting of models to these benchmarks, otherwise defeating the purpose of the solution.Therefore, this solution is effective only if it is complemented by the adoption of slow research practices and a reduced emphasis on pursuing SoTA and leaderboards.</p>
<p>In summary, the path forward for LLM research requires a concerted effort to develop more nuanced and comprehensive evaluation frameworks.By doing so, we can ensure that the progress in LLM can be measured properly, especially in its relevance and effectiveness for practical applications.Embracing these recommendations will pave the way for the next generation of LLMs, characterized by their ability to understand and generate human-like text in a wide range of real-world scenarios.</p>
<p>Related Work</p>
<p>Large Language Models LLMs have demonstrated remarkable proficiency across a wide range of NLP tasks (Brown et al., 2020;Chowdhery et al., 2022;Scao et al., 2022;Touvron et al., 2023a).Furthermore, recent research has shown that supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) can significantly enhance their performance when following general language instructions (Weller et al., 2020;Mishra et al., 2022;Wang et al., 2022b;Chung et al., 2022;Muennighoff et al., 2022;Wu et al., 2023;Li et al., 2023a;Wang et al., 2023c;Wu et al., 2024).Zhao et al. (2023) present a comprehensive overview of the development of LLMs.The emergence of LLMs has fundamentally altered the research paradigm in NLP, making the accurate and efficient assessment of LLM performance a crucial concern.</p>
<p>Human Evaluation of LLMs Human evaluation plays a pivotal role in assessing the performance of LLMs and is often regarded as the "gold standard" for evaluating natural language generation (van der Lee et al., 2019;Howcroft et al., 2020).In the era of LLMs, human evaluations are extensively utilized to measure the effectiveness of these models (Wang et al., 2022a;Wu et al., 2023;Bai et al., 2023).A recent study by Zheng et al. (2023b) introduces Chatbot Arena, a platform that compares pairs of LLMs through crowd-sourced judgments in a competitive setting.Nevertheless, some recent studies challenge the validity of human judgments as the "gold standard" for evaluating machine-generated text (Wu and Aji, 2023;Hosking et al., 2023).Additionally, there is a line of research highlighting concerns over the reproducibility of human evaluation results in recent NLP studies (Shimorina and Belz, 2022;Belz et al., 2023b,a).</p>
<p>Automatic Evaluation of LLMs</p>
<p>Given the limitations of human evaluation in terms of scalability and reproducibility, automatic evaluation acts as a proxy for human evaluation.The performance of LLMs has plateaued on conventional NLP benchmarks (Rajpurkar et al., 2016;Wang et al., 2019).Consequently, more recent studies have shifted towards utilizing human exam questions as a means to further test and challenge the capabilities of LLMs (Hendrycks et al., 2021;Li et al., 2023b;Koto et al., 2023;Cobbe et al., 2021).With the continuous advancements in LLMs, recent research has explored using state-of-the-art LLMs, such as GPT-4 (OpenAI, 2023) and Claude-2 (Bai et al., 2022b), for evaluating model outputs (Li et al., 2023c;Wu and Aji, 2023;Liu et al., 2023;Wu et al., 2024).However, the reliability of LLM-based evaluation remains an open question (Wang et al., 2023a;Li et al., 2023d).</p>
<p>Ours Considering the limitations of human evaluation in terms of scalability and reproducibility, leveraging automatic evaluation to assess Large Language Models (LLMs) becomes essential.In this work, we highlight the discrepancy between automatic evaluation methodologies and the realworld applications of LLMs.</p>
<p>Conclusion</p>
<p>This work critically examines the alignment between probability-based evaluation methods for LLMs and their actual performance in generating text, particularly on benchmarks such as MMLU, TruthfulQA, and Belebele.Our findings highlight a significant gap between these prediction methods and the practical utility of LLMs, suggesting that current methods might not accurately reflect a model's real-world capabilities.The discrepancies call for a shift towards more comprehensive evaluation frameworks that prioritize the quality of generated text and the model's ability to understand and respond in human-like ways.Future research should focus on developing evaluation metrics that more accurately capture the essence of LLM performance in practical scenarios.In summary, our study underscores the need for revising LLM evaluation practices to ensure they accurately estimate the models' effectiveness in real-world applications.By adopting more relevant evaluation criteria, we can better gauge the progress and utility of LLM advancements.</p>
<p>Limitations</p>
<p>In this paper, we selected three representative benchmarks to evaluate various LLMs, but these benchmarks might not be comprehensive enough to reflect the evaluation issue of LLMs since they only cover examination questions (MMLU), factoid questions (TruthfulQA) and general reading comprehension (Belebele).Moreover, due to the limitation of computational resources we only evaluate ten LLMs which might not be fullly reflective of how LLMs behave when facing such MCQ questions, so more LLMs should be incorporated when more resources are available.</p>
<p>This position paper, while exploring and empirically showing the current misalignment issue in LLM evaluation, does not explore practical solutions beyond suggestions on where the field should go.Nevertheless, we argue that laying out the challenges is still beneficial and contributive towards the community.</p>
<p>A Appendix</p>
<p>A.1 Experimental Setup A.1.1 Datasets MMLU The Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) benchmark is a comprehensive test designed to assess knowledge acquired during pretraining of language models, especially in zero-shot and fewshot settings.Introduced by (Hendrycks et al., 2021)., MMLU encompasses 57 subjects across diverse fields including STEM, humanities, social sciences, and others, making it a broad measure of both world knowledge and problem-solving ability (Hendrycks et al., 2021).The dataset contains 17,803 examples with a range of difficulties, from elementary to advanced professional levels.Its comprehensive nature allows for a detailed examination of a model's strengths and weaknesses across various disciplines.</p>
<p>Truthful-QA The Truthful-QA dataset (Lin et al., 2022) is a benchmark to assess the truthfulness of language model responses to questions.This dataset contains 817 questions spanning 38 diverse categories, including health, law, finance, and politics.The key characteristic of Truthful-QA is its design to elicit imitative falsehoods, wherein some questions are crafted to provoke false answers based on common misconceptions or false beliefs.The dataset aims to test language models' ability to avoid generating false answers that may have been learned through imitating human texts.Importantly, the Truthful-QA questions are adversarial in nature, designed to pinpoint weaknesses in the truthfulness of language models.Additionally, it features a set of true and false reference answers for each question, backed by reliable sources.</p>
<p>Belebele The Belebele Benchmark (Bandarkar et al., 2023) is a massively multilingual reading comprehension dataset designed to evaluate machine reading comprehension (MRC) capabilities across various languages.Developed by Facebook Research, it features 900 multiple-choice questions per language, spanning 122 language variants, totaling 109,800 questions linked to 488 distinct passages.Each question has four answer options, with only one correct answer.This benchmark encompasses a wide range of languages, from highresource to low-resource, making it ideal for assessing the performance of language models in diverse linguistic contexts.</p>
<p>A.1.2 Models</p>
<p>LLaMA LLaMA-1 (Touvron et al., 2023b), Vicuna (Chiang et al., 2023) and LLaMA-2 (Touvron et al., 2023c) is a family of large language models (LLMs), encompassing a range of pretrained and fine-tuned generative text models with parameters varying from 7 billion to 70 billion.The model was trained on a new mix of publicly available online data, with a considerable size of 2 trillion tokens, and includes over one million humanannotated examples for fine-tuning.Its training and evaluation emphasize both performance and safety.These fine-tuned models have shown superior performance in human evaluations for helpfulness and safety, matching or even surpassing other well-known models like ChatGPT and PaLM in certain aspects.</p>
<p>Mistral The Mistral model (Jiang et al., 2023) equipped with 7.3 billion parameters, is designed to outperform its counterparts in terms of efficiency and effectiveness.Notable features of Mistral 7B include its proficiency in outperforming LLaMA-2-13B (Touvron et al., 2023c) across various benchmarks and approaching the performance of CodeLLaMA-7B (Rozière et al., 2023) in coderelated tasks while maintaining strong English language capabilities.Additionally, Mistral 7B incorporates Grouped-query attention (GQA) for faster inference and Sliding Window Attention (SWA) to manage longer sequences more economically.</p>
<p>lm-harness</p>
<p>The lm-harness (Gao et al., 2021)  † , developed by EleutherAI, is a comprehensive framework designed for the few-shot evaluation of autoregressive language models.This library is pivotal in the field of natural language processing for assessing the performance of language models in few-shot settings.It stands out due to its versatility and ability to handle a variety of language models, making it a valuable tool for researchers in the field.The lm-harness library facilitates robust and efficient evaluations, contributing significantly to advancements in language model development and assessment (Gao et al., 2021).</p>
<p>A.2 Elo-based Chatbot Arena Leaderboard</p>
<p>In the Elo-based Chatbot Arena Leaderboard, crowds are given an interface to ask questions to LLMs.The users are then given 2 options from 2</p>
<p>arXiv:2402.13887v2 [cs.CL] 9 Jul 2024 Q: What is the second most common element in the solar system?</p>
<p>Figure 1 :
1
Figure 1: An illustration of label-based, sequence-based and generation-based predictions for evaluating LLMs on NLP benchmarks.</p>
<p>Figure 2 :
2
Figure 2: Differences in label and sequence accuracies compared to generation accuracies across datasets.</p>
<p>Figure 3 :
3
Figure3: Top-5 and bottom-5 categories from MMLU that have high and low correlation with human judges from Chatbot Arena, the benchmark scores are calculated using our previously used Label, Sequence, Generation methods.</p>
<p>Figure 4 :
4
Figure 4: Results of LLMs on English Belebele under different amount of demonstration examples in context, which ranges from 1 to 5.</p>
<p>Figure 5 :
5
Figure 5: Results of LLMs on Belebele under multilingual data including Amharic (amh_Ethi), Chinese (zho_Hans), Russian (rus_Cyrl), Swahili (swh_Latn) and Arabic (arb_Arab).</p>
<p>Table 1 :
1
Examples from MMLU (the first one) and TruthfulQA (the second one), the MCQ answer from label-based and sequence-based prediction.For the first example, the answer option predicted by MCQ-style evaluation (either label-based or sequence-based prediction) is C, whereas the option selected in the generated response is B, demonstrating the inconsistency of MCQ-style evaluation.
QuestionOptionsMCQFree-text generation outputanswerFor which of these two scenarios does the mainA: Wrong, WrongC/CThe correct answer is B. Explana-character (who uses I/me/my) do somethingB: Wrong, Not wrongtion &amp; In scenario 2, "I taught myclearly morally wrong, according to ordinaryC: Not wrong, Wrongchildren to play the xylophone,"moral standards in the US as of 2020? ScenarioD: Not wrong, Notthere is no indication of harm or mal-1 | I slammed on my breaks to miss the ball aswrongice towards anyone else, and there-it came in to the road. Scenario 2 | I taught myfore, ...<truncated>children to play the xylophone.A: Yes, you can teach anA/B"response": " The correct answer isCan you teach an old dog new tricks?old dog new tricks. B: You can't teach an(B) You can't teach an old dog new tricks. Explanation: While it is pos-old dog new tricks.sible to train dogs of any age, in-C: No, you can't teachcluding older dogs, there are certainan old dog new tricks.limitations ...<truncated>MMLUTruthfulQABelebeleModelAgreementAccuracyAgreementAccuracy</p>
<p>Table 2 :
2
Zero-shot evaluation results on different datasets.The first two columns for each dataset show agreement between options selected by MCQ-style evaluation via the highest probability label and answer sequence versus response via free-text generation.The last three columns for each dataset represent the accuracy obtained by using free text generation and 2 MCQ-style benchmarks.</p>
<p>Table 3 :
3
Overlap of correctly predicted options of various LLMs on MMLU, TruthfulQA, and Belebele datasets, the overlap is compared with generation-based method.
ModelMMLUTruthfulQABelebeleLabel Seq Label Seq Label SeqMistral-7B47.6 79.8 58.3 29.0 85.2 70.9Mistral-7B-Instruct44.5 73.7 62.9 45.3 96.4 85.8LLaMA-1-7B24.6 30.1 53.3 22.3 25.8 19.7Vicuna-7B42.1 61.2 49.0 40.4 69.2 71.9LLaMA-2-7B70.4 47.4 41.3 36.9 68.7 57.9LLaMA-2-7B-chat84.8 68.3 41.7 41.7 92.4 77.9LLaMA-2-13B70.8 69.5 54.2 27.9 78.4 71.3LLaMA-2-13B-chat 84.6 80.6 69.4 38.7 95.0 87.5LLaMA-2-70B85.0 81.3 66.2 32.7 92.5 81.9LLaMA-2-70B-chat 89.8 85.4 90.9 46.9 97.3 90.2</p>
<p>Table 4 :
4
Detailed results of LLaMA-1-7B on different categories of MMLU.
CategoryAgreement(Label) Agreement(Seq) Acc.(Gen) Acc.(Label) Acc.(Seq) Examplesmoral scenarios0.080.080.270.230.25891college physics0.200.220.260.270.1485high school biology0.290.260.350.250.31291college mathematics0.300.330.300.210.2992abstract algebra0.170.560.210.210.2498high school computer science0.260.240.400.290.3290astronomy0.240.230.400.230.31141computer security0.170.320.510.230.3895logical fallacies0.260.180.300.270.28158professional law0.280.230.320.240.251189clinical knowledge0.270.310.440.210.33241elementary mathematics0.250.250.310.210.26327high school macroeconomics0.220.260.290.220.30353formal logic0.340.160.340.250.23120high school government and politics0.310.370.460.280.36183medical genetics0.260.240.280.230.2895electrical engineering0.310.310.420.270.30131high school mathematics0.340.260.310.270.30232public relations0.260.170.400.350.32105econometrics0.190.420.280.270.33111machine learning0.180.550.270.270.19107human sexuality0.270.200.410.210.24127high school geography0.350.290.470.230.34188nutrition0.240.310.430.240.29282management0.240.190.490.210.22101jurisprudence0.270.150.370.320.32100human aging0.310.210.370.310.36214college chemistry0.250.260.300.180.2184business ethics0.270.170.300.210.3398high school psychology0.280.210.450.260.25512conceptual physics0.390.270.360.270.32211prehistory0.240.230.420.230.27293high school chemistry0.260.310.350.240.26176high school world history0.320.280.460.260.33203college biology0.270.190.350.260.29132high school physics0.260.260.340.260.32133high school european history0.300.230.530.210.31131college computer science0.200.280.300.260.2993us foreign policy0.320.230.470.350.4091moral disputes0.230.190.350.250.31318world religions0.380.450.550.300.40146high school statistics0.280.250.380.290.25205international law0.150.180.370.170.34119security studies0.250.140.410.260.29236professional medicine0.260.180.400.310.21171marketing0.220.210.450.230.32215high school us history0.290.220.450.190.31186sociology0.300.230.390.270.27190anatomy0.320.260.410.230.28128virology0.280.210.310.270.29153professional psychology0.230.220.310.250.33563miscellaneous0.270.330.550.250.36743high school microeconomics0.230.220.270.250.29212global facts0.240.210.260.170.3698philosophy0.250.230.430.270.28288college medicine0.260.260.350.240.26156professional accounting0.160.180.270.280.26241</p>
<p>Table 5 :
5
Detailed results of LLaMA-2 on different categories of MMLU.
CategoryAgreement(Label) Agreement(Seq) Acc.(Gen) Acc.(Label) Acc.(Seq) Examplesmoral scenarios1.001.000.240.240.24895college physics0.710.510.240.220.20102high school biology0.870.500.510.490.50309college mathematics0.720.540.310.300.31100abstract algebra0.670.220.350.320.30100high school computer science0.720.420.350.360.40100astronomy0.790.560.460.450.49152computer security0.820.510.490.500.60100logical fallacies0.880.480.450.500.58163professional law0.870.490.340.360.361517clinical knowledge0.780.510.430.490.55265elementary mathematics0.480.380.310.260.28377high school macroeconomics0.850.490.420.420.40390formal logic0.740.610.210.280.24126high school government and politics0.840.570.530.520.68193medical genetics0.780.480.420.410.48100electrical engineering0.700.410.400.390.45145high school mathematics0.510.400.270.240.27270public relations0.850.580.450.450.54110econometrics0.820.560.280.30114machine learning0.700.310.200.290.35111human sexuality0.840.590.530.530.56131high school geography0.880.590.520.520.59198nutrition0.800.440.450.430.49305management0.870.600.550.560.68103jurisprudence0.820.460.360.360.58107human aging0.840.460.350.390.58223college chemistry0.680.580.250.230.25100business ethics0.630.400.390.380.45100high school psychology0.840.590.540.560.63545conceptual physics0.800.540.340.370.40235prehistory0.870.590.500.510.55324high school chemistry0.640.420.350.310.33203high school world history0.760.530.470.550.61222college biology0.810.440.420.460.45144high school physics0.710.540.290.320.28151high school european history0.780.580.500.560.59147college computer science0.730.490.260.320.32100us foreign policy0.860.560.490.570.72100moral disputes0.880.500.360.370.50346world religions0.830.520.460.540.69171high school statistics0.780.540.330.330.27216international law0.880.510.500.550.61121security studies0.820.530.480.510.50245professional medicine0.800.430.420.420.40267marketing0.880.590.530.570.76233high school us history0.740.490.410.470.66202sociology0.870.600.570.600.74201anatomy0.850.480.400.410.44135virology0.830.560.390.390.46166professional psychology0.870.490.380.390.47612miscellaneous0.810.570.540.560.69783high school microeconomics0.820.440.370.390.36238global facts0.510.570.350.330.40100philosophy0.870.520.420.460.53311college medicine0.780.540.410.370.38168professional accounting0.840.490.300.320.37281</p>
<p>Table 6 :
6
Detailed results of LLaMA-2-chat on different categories of MMLU.</p>
<p>† https://github.com/EleutherAI/ lm-evaluation-harness
Missing information, unresponsive authors, experimental flaws: The impossibility of assessing the reproducibility of previous human evaluations in NLP.In The Fourth Workshop on Insights from Negative Results in NLP, pages 1-10, Dubrovnik, Croatia.Association for Computational Linguistics.Language models are few-shot learners.In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901.Curran Associates, Inc. : A stickier benchmark for general-purpose language understanding systems.In Advances in Neural Information Shen.2022b.Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks.In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085-5109, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics.anonymous LLMs, in which the user has to vote for the better one, which will be the winner LLM.Based on several win-lose interactions, we can then calculate the Elo score.Elo scores have been previously designed in rank multiple players that involve multiple matches across different people, such as chess.It is good for determining a unified ranking across every player (in this case, LLMs).From the Elo score of 2 players, we can predict the winning chance of both players.For example, an LLM with an Elo of 1200 will win against an LLM with an Elo of 900 85% of the time.Chatbot Arena is one of the popular Elo-based leaderboards.It supports a variety of LLMs, both proprietary and open-sourced, and has accumulated hundreds of thousands of votes.
. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Sharan Chowdhery, Gaurav Narang, Adams Mishra, Vincent Y Yu, Yanping Zhao, Andrew M Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, 10.48550/arXiv.2210.114162022Scaling instruction-finetuned language models. CoRR, abs/2210.11416</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, 10.5281/zenodo.5371628Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wangand Andy Zou. 2021. A framework for few-shot language model evaluation</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021. May 3-7, 2021OpenReview.net</p>
<p>Human feedback is not gold standard. Tom Hosking, Phil Blunsom, Max Bartolo, 10.48550/ARXIV.2309.16349CoRR, abs/2309.163492023</p>
<p>Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions. David M Howcroft, Anya Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, A Sadid, Saad Hasan, Simon Mahamood, Mille, Sashank Emiel Van Miltenburg, Verena Santhanam, Rieser, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language GenerationDublin, Ireland2020Association for Computational Linguistics</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, 10.48550/ARXIV.2305.08322abs/2305.08322CoRR</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Large language models only pass primary school exams in Indonesia: A comprehensive test on IndoMMLU. Fajri Koto, Nurul Aisyah, Haonan Li, Timothy Baldwin, 10.18653/v1/2023.emnlp-main.760Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation. Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, Timothy Baldwin, 10.48550/arXiv.2305.15011CoRR, abs/2305.150112023a</p>
<p>CMMLU: measuring massive multitask language understanding in chinese. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin, 10.48550/ARXIV.2306.09212CoRR, abs/2306.092122023b</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023c</p>
<p>Split and merge: Aligning position biases in large language model based evaluators. Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu, 10.48550/ARXIV.2310.01432CoRR, abs/2310.014322023d</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J Orr, Lucia Zheng, Mert Yüksekgönül, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda, 10.48550/arXiv.2211.09110CoRR, abs/2211.091102022</p>
<p>TruthfulQA: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, 10.18653/v1/2022.acl-long.229Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, Adam Roberts, 10.48550/arXiv.2301.13688CoRR, abs/2301.136882023</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Crosslingual generalization through multitask finetuning. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, 10.48550/arXiv.2211.01786CoRR, abs/2211.01786Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel2022</p>
<dl>
<dt>On "scientific debt" in NLP: A case for more rigour in language model pre-training research. Made Nindyatama Nityasya, Haryo Wibowo, Alham Fikri Aji, Genta Winata, Radityo Eko Prasojo, Phil Blunsom, Adhiguna Kuncoro, 10.18653/v1/2023.acl-long.477Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</dt>
<dd>
<p>Openai, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Zhanyu Wang, Longyue Wang, Zhen Zhao, Minghao Wu, Chenyang Lyu, Huayang Li, Deng Cai, Luping Zhou, Shuming Shi, Zhaopeng Tu, arXiv:2311.16511Gpt4video: A unified multimodal large language model for lnstruction-followed understanding and safety-aware generation. 2023carXiv preprint</p>
</dd>
</dl>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022</p>
<p>Learning from task descriptions. Orion Weller, Nicholas Lourie, Matt Gardner, Matthew E Peters, 10.18653/v1/2020.emnlp-main.105Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Style over substance: Evaluation biases for large language models. Minghao Wu, Alham Fikri, Aji , 10.48550/ARXIV.2307.03025CoRR, abs/2307.030252023</p>
<p>Adapting large language models for document-level machine translation. Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George Foster, Gholamreza Haffari, arXiv:2401.064682024arXiv preprint</p>
<p>Lamini-lm: A diverse herd of distilled models from large-scale instructions. Minghao Wu, Abdul Waheed, Chiyu Zhang, 10.48550/arXiv.2304.14402CoRR, abs/2304.144022023Muhammad Abdul-Mageed, and Alham Fikri Aji</p>
<p>A survey of large language models. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, 10.48550/ARXIV.2303.18223CoRR, abs/2303.182232023</p>
<p>On large language models' selection bias in multi-choice questions. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang, arXiv:2309.038822023aarXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, 10.48550/ARXIV.2306.05685CoRR, abs/2306.056852023b</p>
<p>LIMA: less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy, 10.48550/ARXIV.2305.11206CoRR, abs/2305.112062023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6710 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6710</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6710</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-33d97417aa0e5c6fc4a58716003b02c09736e782</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/33d97417aa0e5c6fc4a58716003b02c09736e782" target="_blank">Towards Understanding Mixture of Experts in Deep Learning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper formally study how the MoE layer improves the performance of neural network learning and why the mixture model will not collapse into a single model, and shows that the router can learn the cluster-center features, which helps divide the input complex problem into simpler linear classification sub-problems that individual experts can conquer.</p>
                <p><strong>Paper Abstract:</strong> The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by a router, has achieved great success in deep learning. However, the understanding of such architecture remains elusive. In this paper, we formally study how the MoE layer improves the performance of neural network learning and why the mixture model will not collapse into a single model. Our empirical results suggest that the cluster structure of the underlying problem and the non-linearity of the expert are pivotal to the success of MoE. To further understand this, we consider a challenging classification problem with intrinsic cluster structures, which is hard to learn using a single expert. Yet with the MoE layer, by choosing the experts as two-layer nonlinear convolutional neural networks (CNNs), we show that the problem can be learned successfully. Furthermore, our theory shows that the router can learn the cluster-center features, which helps divide the input complex problem into simpler linear classification sub-problems that individual experts can conquer. To our knowledge, this is the first result towards formally understanding the mechanism of the MoE layer for deep learning.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6710.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6710.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoE (nonlinear)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture-of-Experts with nonlinear two-layer CNN experts (cubic activation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparsely-gated mixture-of-experts model using top-1 (switch) routing with random noise and two-layer CNN experts with a cubic activation; experts are trained with normalized gradient descent and the router with gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoE (nonlinear)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Sparsely-gated Mixture-of-Experts (top-1 routing with noise, nonlinear experts)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble / sparse-gating</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Synthetic mixture-of-classification (Definition 3.1) — Setting 1</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification over P-patch inputs with intrinsic cluster structure (one feature patch, one cluster-center patch, one feature-noise patch, remaining Gaussian noise).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Test accuracy (%) (mean over 10 runs) and dispatch entropy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>99.46</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>MoE (linear)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>6.47</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors show nonlinear experts + sparse routing discover cluster structure: router learns cluster-center features and routes inputs to specialists; exploration via random noise and normalized GD prevents collapse and load imbalance; dispatch entropy nearly zero indicates experts specialize (high diversity). Nonlinearity of experts is crucial — nonlinear MoE achieves near 100% accuracy while linear variants fail to specialize.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Understanding Mixture of Experts in Deep Learning', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6710.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6710.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoE (linear)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture-of-Experts with linear two-layer CNN experts (identity activation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparsely-gated mixture-of-experts model identical to the nonlinear MoE but with linear expert activations; trained with the same routing/noise scheme and normalized gradient descent for experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoE (linear)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Sparsely-gated Mixture-of-Experts (top-1 routing with noise, linear experts)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble / sparse-gating</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Synthetic mixture-of-classification (Definition 3.1) — Setting 1</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification over P-patch inputs with intrinsic cluster structure (one feature patch, one cluster-center patch, one feature-noise patch, remaining Gaussian noise).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Test accuracy (%) (mean over 10 runs) and dispatch entropy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>92.99</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>MoE (nonlinear)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-6.47</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Linear MoE often fails to recover the cluster structure: dispatch entropy remains high (experts receive mixed-cluster data), experts do not diversify strongly, leading to lower accuracy and higher variance; adding load-balancing loss improves linear MoE but still underperforms nonlinear MoE, suggesting nonlinearity (not only load balancing) is key.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Understanding Mixture of Experts in Deep Learning', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6710.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6710.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single (nonlinear)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single two-layer CNN (nonlinear, cubic or other activations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single expert model using two-layer CNN with nonlinear activation (cubic, ReLU, etc.), trained on the entire dataset without routing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Single (nonlinear)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Single-model learning with nonlinear CNN</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>single-model / monolithic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Synthetic mixture-of-classification (Definition 3.1) — Setting 1</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification over P-patch inputs with intrinsic cluster structure (one feature patch, one cluster-center patch, one feature-noise patch, remaining Gaussian noise).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Test accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>79.48</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>MoE (nonlinear)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-19.98</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Single nonlinear CNNs perform substantially worse than nonlinear MoE on this clustered data; theory (Theorem 4.1) proves a single expert cannot exceed 87.5% on certain distribution instances, and empirics show single models fail to recover cluster structure and thus underperform.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Understanding Mixture of Experts in Deep Learning', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6710.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6710.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single (linear)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single two-layer CNN (linear / identity activation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single expert model using two-layer CNN with identity activation (linear), trained on the entire dataset without routing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Single (linear)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Single-model learning with linear CNN</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>single-model / monolithic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Synthetic mixture-of-classification (Definition 3.1) — Setting 1</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification over P-patch inputs with intrinsic cluster structure (one feature patch, one cluster-center patch, one feature-noise patch, remaining Gaussian noise).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Test accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>68.71</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>MoE (nonlinear)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-30.75</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Single linear experts perform worst; theory (Theorem 4.1) demonstrates single-expert architectures (including linear) provably fail on instances where feature noise strength matches signal strength. Empirical results confirm poor accuracy and inability to uncover cluster structure.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Understanding Mixture of Experts in Deep Learning', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Outrageously Large Neural Networks: The Mixture of Experts Approach <em>(Rating: 2)</em></li>
                <li>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity <em>(Rating: 2)</em></li>
                <li>Mixtures of Experts and the EM Algorithm <em>(Rating: 1)</em></li>
                <li>Hierarchical Mixtures of Experts and the EM Algorithm <em>(Rating: 1)</em></li>
                <li>Deep Mixture of Experts via Shallow Gating <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6710",
    "paper_id": "paper-33d97417aa0e5c6fc4a58716003b02c09736e782",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "MoE (nonlinear)",
            "name_full": "Mixture-of-Experts with nonlinear two-layer CNN experts (cubic activation)",
            "brief_description": "A sparsely-gated mixture-of-experts model using top-1 (switch) routing with random noise and two-layer CNN experts with a cubic activation; experts are trained with normalized gradient descent and the router with gradient descent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MoE (nonlinear)",
            "model_size": null,
            "reasoning_method_name": "Sparsely-gated Mixture-of-Experts (top-1 routing with noise, nonlinear experts)",
            "reasoning_method_type": "ensemble / sparse-gating",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Synthetic mixture-of-classification (Definition 3.1) — Setting 1",
            "task_description": "Binary classification over P-patch inputs with intrinsic cluster structure (one feature patch, one cluster-center patch, one feature-noise patch, remaining Gaussian noise).",
            "performance_metric": "Test accuracy (%) (mean over 10 runs) and dispatch entropy",
            "performance_value": 99.46,
            "comparison_target_method": "MoE (linear)",
            "performance_difference": 6.47,
            "statistical_significance": false,
            "analysis_notes": "Authors show nonlinear experts + sparse routing discover cluster structure: router learns cluster-center features and routes inputs to specialists; exploration via random noise and normalized GD prevents collapse and load imbalance; dispatch entropy nearly zero indicates experts specialize (high diversity). Nonlinearity of experts is crucial — nonlinear MoE achieves near 100% accuracy while linear variants fail to specialize.",
            "ablation_study_present": true,
            "uuid": "e6710.0",
            "source_info": {
                "paper_title": "Towards Understanding Mixture of Experts in Deep Learning",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "MoE (linear)",
            "name_full": "Mixture-of-Experts with linear two-layer CNN experts (identity activation)",
            "brief_description": "A sparsely-gated mixture-of-experts model identical to the nonlinear MoE but with linear expert activations; trained with the same routing/noise scheme and normalized gradient descent for experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MoE (linear)",
            "model_size": null,
            "reasoning_method_name": "Sparsely-gated Mixture-of-Experts (top-1 routing with noise, linear experts)",
            "reasoning_method_type": "ensemble / sparse-gating",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "Synthetic mixture-of-classification (Definition 3.1) — Setting 1",
            "task_description": "Binary classification over P-patch inputs with intrinsic cluster structure (one feature patch, one cluster-center patch, one feature-noise patch, remaining Gaussian noise).",
            "performance_metric": "Test accuracy (%) (mean over 10 runs) and dispatch entropy",
            "performance_value": 92.99,
            "comparison_target_method": "MoE (nonlinear)",
            "performance_difference": -6.47,
            "statistical_significance": false,
            "analysis_notes": "Linear MoE often fails to recover the cluster structure: dispatch entropy remains high (experts receive mixed-cluster data), experts do not diversify strongly, leading to lower accuracy and higher variance; adding load-balancing loss improves linear MoE but still underperforms nonlinear MoE, suggesting nonlinearity (not only load balancing) is key.",
            "ablation_study_present": true,
            "uuid": "e6710.1",
            "source_info": {
                "paper_title": "Towards Understanding Mixture of Experts in Deep Learning",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Single (nonlinear)",
            "name_full": "Single two-layer CNN (nonlinear, cubic or other activations)",
            "brief_description": "A single expert model using two-layer CNN with nonlinear activation (cubic, ReLU, etc.), trained on the entire dataset without routing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Single (nonlinear)",
            "model_size": null,
            "reasoning_method_name": "Single-model learning with nonlinear CNN",
            "reasoning_method_type": "single-model / monolithic",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Synthetic mixture-of-classification (Definition 3.1) — Setting 1",
            "task_description": "Binary classification over P-patch inputs with intrinsic cluster structure (one feature patch, one cluster-center patch, one feature-noise patch, remaining Gaussian noise).",
            "performance_metric": "Test accuracy (%)",
            "performance_value": 79.48,
            "comparison_target_method": "MoE (nonlinear)",
            "performance_difference": -19.98,
            "statistical_significance": false,
            "analysis_notes": "Single nonlinear CNNs perform substantially worse than nonlinear MoE on this clustered data; theory (Theorem 4.1) proves a single expert cannot exceed 87.5% on certain distribution instances, and empirics show single models fail to recover cluster structure and thus underperform.",
            "ablation_study_present": true,
            "uuid": "e6710.2",
            "source_info": {
                "paper_title": "Towards Understanding Mixture of Experts in Deep Learning",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Single (linear)",
            "name_full": "Single two-layer CNN (linear / identity activation)",
            "brief_description": "A single expert model using two-layer CNN with identity activation (linear), trained on the entire dataset without routing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Single (linear)",
            "model_size": null,
            "reasoning_method_name": "Single-model learning with linear CNN",
            "reasoning_method_type": "single-model / monolithic",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Synthetic mixture-of-classification (Definition 3.1) — Setting 1",
            "task_description": "Binary classification over P-patch inputs with intrinsic cluster structure (one feature patch, one cluster-center patch, one feature-noise patch, remaining Gaussian noise).",
            "performance_metric": "Test accuracy (%)",
            "performance_value": 68.71,
            "comparison_target_method": "MoE (nonlinear)",
            "performance_difference": -30.75,
            "statistical_significance": false,
            "analysis_notes": "Single linear experts perform worst; theory (Theorem 4.1) demonstrates single-expert architectures (including linear) provably fail on instances where feature noise strength matches signal strength. Empirical results confirm poor accuracy and inability to uncover cluster structure.",
            "ablation_study_present": true,
            "uuid": "e6710.3",
            "source_info": {
                "paper_title": "Towards Understanding Mixture of Experts in Deep Learning",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Outrageously Large Neural Networks: The Mixture of Experts Approach",
            "rating": 2
        },
        {
            "paper_title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
            "rating": 2
        },
        {
            "paper_title": "Mixtures of Experts and the EM Algorithm",
            "rating": 1
        },
        {
            "paper_title": "Hierarchical Mixtures of Experts and the EM Algorithm",
            "rating": 1
        },
        {
            "paper_title": "Deep Mixture of Experts via Shallow Gating",
            "rating": 1
        }
    ],
    "cost": 0.01203725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Towards Understanding Mixture of Experts in Deep Learning</h1>
<p>Zixiang Chen ${ }^{\star}$ and Yihe Deng ${ }^{\dagger}$ and Yue Wu ${ }^{\ddagger}$ and Quanquan Gu ${ }^{\S}$ and Yuanzhi Li*</p>
<h4>Abstract</h4>
<p>The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by a router, has achieved great success in deep learning. However, the understanding of such architecture remains elusive. In this paper, we formally study how the MoE layer improves the performance of neural network learning and why the mixture model will not collapse into a single model. Our empirical results suggest that the cluster structure of the underlying problem and the non-linearity of the expert are pivotal to the success of MoE. To further understand this, we consider a challenging classification problem with intrinsic cluster structures, which is hard to learn using a single expert. Yet with the MoE layer, by choosing the experts as two-layer nonlinear convolutional neural networks (CNNs), we show that the problem can be learned successfully. Furthermore, our theory shows that the router can learn the cluster-center features, which helps divide the input complex problem into simpler linear classification sub-problems that individual experts can conquer. To our knowledge, this is the first result towards formally understanding the mechanism of the MoE layer for deep learning.</p>
<h2>1 Introduction</h2>
<p>The Mixture-of-Expert (MoE) structure (Jacobs et al., 1991; Jordan and Jacobs, 1994) is a classic design that substantially scales up the model capacity and only introduces small computation overhead. In recent years, the MoE layer (Eigen et al., 2013; Shazeer et al., 2017), which is an extension of the MoE model to deep neural networks, has achieved remarkable success in deep learning. Generally speaking, an MoE layer contains many experts that share the same network architecture and are trained by the same algorithm, with a gating (or routing) function that routes individual inputs to a few experts among all the candidates. Through the sparse gating function, the router in the MoE layer can route each input to the top- $K(K \geq 2)$ best experts (Shazeer et al., 2017), or the single $(K=1)$ best expert (Fedus et al., 2021). This routing scheme only costs the computation of $K$ experts for a new input, which enjoys fast inference time.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Despite the great empirical success of the MoE layer, the theoretical understanding of such architecture is still elusive. In practice, all experts have the same structure, initialized from the same weight distribution (Fedus et al., 2021) and are trained with the same optimization configuration. The router is also initialized to dispatch the data uniformly. It is unclear why the experts can diverge to different functions that are specialized to make predictions for different inputs, and why the router can automatically learn to dispatch data, especially when they are all trained using simple local search algorithms such as gradient descent. Therefore, we aim to answer the following questions:</p>
<p>Why do the experts in MoE diversify instead of collapsing into a single model? And how can the router learn to dispatch the data to the right expert?</p>
<p>In this paper, in order to answer the above question, we consider the natural "mixture of classification" data distribution with cluster structure and theoretically study the behavior and benefit of the MoE layer. We focus on the simplest setting of the mixture of linear classification, where the data distribution has multiple clusters, and each cluster uses separate (linear) feature vectors to represent the labels. In detail, we consider the data generated as a combination of feature patches, cluster patches, and noise patches (See Definition 3.1 for more details). We study training an MoE layer based on the data generated from the "mixture of classification" distribution using gradient descent, where each expert is chosen to be a two-layer CNN. The main contributions of this paper are summarized as follows:</p>
<ul>
<li>We first prove a negative result (Theorem 4.1) that any single expert, such as two-layer CNNs with arbitrary activation function, cannot achieve a test accuracy of more than $87.5 \%$ on our data distribution.</li>
<li>Empirically, we found that the mixture of linear experts performs better than the single expert but is still significantly worse than the mixture of non-linear experts. Figure 1 provides such a result in a special case of our data distribution with four clusters. Although a mixture of linear models can represent the labeling function of this data distribution with $100 \%$ accuracy, it fails to learn so after training. We can see that the underlying cluster structure cannot be recovered by the mixture of linear experts, and neither the router nor the experts are diversified enough after training. In contrast, the mixture of non-linear experts can correctly recover the cluster structure and diversify.</li>
<li>Motivated by the negative result and the experiment on the toy data, we study a sparsely-gated MoE model with two-layer CNNs trained by gradient descent. We prove that this MoE model can achieve nearly $100 \%$ test accuracy efficiently (Theorem 4.2).</li>
<li>Along with the result on the test accuracy, we formally prove that each expert of the sparselygated MoE model will be specialized to a specific portion of the data (i.e., at least one cluster), which is determined by the initialization of the weights. In the meantime, the router can learn the cluster-center features and route the input data to the right experts.</li>
<li>Finally, we also conduct extensive experiments on both synthetic and real datasets to corroborate our theory.
Notation. We use lower case letters, lower case bold face letters, and upper case bold face letters to denote scalars, vectors, and matrices respectively. We denote a union of disjoint sets $\left(A_{i}: i \in I\right)$ by $\sqcup_{i \in I} A_{i}$. For a vector $\mathbf{x}$, we use $|\mathbf{x}|<em F="F">{2}$ to denote its Euclidean norm. For a matrix $\mathbf{W}$, we use $|\mathbf{W}|</em>\right|$ for some absolute}$ to denote its Frobenius norm. Given two sequences $\left{x_{n}\right}$ and $\left{y_{n}\right}$, we denote $x_{n}=\mathcal{O}\left(y_{n}\right)$ if $\left|x_{n}\right| \leq C_{1}\left|y_{n}\right|$ for some absolute positive constant $C_{1}, x_{n}=\Omega\left(y_{n}\right)$ if $\left|x_{n}\right| \geq C_{2}\left|y_{n</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Visualization of the training of MoE with nonlinear expert and linear expert. Different colors denote router's dispatch to different experts. The lines denote the decision boundary of the MoE model. The data points are visualized on 2d space via t-SNE (Van der Maaten and Hinton, 2008). The MoE architecture follows section 3 where nonlinear experts use activation function $\sigma(z)=z^{3}$. For this visualization, we let the expert number $M=4$ and cluster number $K=4$. We generate $n=1,600$ data points from the distribution illustrated in Section 3 with $\alpha \in(0.5,2), \beta \in(1,2), \gamma \in(1,2)$, and $\sigma_{p}=1$. More details of the visualization are discussed in Appendix A.
positive constant $C_{2}$, and $x_{n}=\Theta\left(y_{n}\right)$ if $C_{3}\left|y_{n}\right| \leq\left|x_{n}\right| \leq C_{4}\left|y_{n}\right|$ for some absolute constants $C_{3}, C_{4}&gt;$ 0 . We also use $\widetilde{\mathcal{O}}(\cdot)$ to hide logarithmic factors of $d$ in $\mathcal{O}(\cdot)$. Additionally, we denote $x_{n}=\operatorname{poly}\left(y_{n}\right)$ if $x_{n}=\mathcal{O}\left(y_{n}^{D}\right)$ for some positive constant $D$, and $x_{n}=\operatorname{polylog}\left(y_{n}\right)$ if $x_{n}=\operatorname{poly}\left(\log \left(y_{n}\right)\right)$. We also denote by $x_{n}=o\left(y_{n}\right)$ if $\lim <em n="n">{n \rightarrow \infty} x</em>=0$. Finally we use $[N]$ to denote the index set ${1, \ldots, N}$.} / y_{n</p>
<h1>2 Related Work</h1>
<p>Mixture of Experts Model. The mixture of experts model (Jacobs et al., 1991; Jordan and Jacobs, 1994) has long been studied in the machine learning community. These MoE models are based on various base expert models such as support vector machine (Collobert et al., 2002), Gaussian processes (Tresp, 2001), or hidden Markov models (Jordan et al., 1997). In order to increase the model capacity to deal with the complex vision and speech data, Eigen et al. (2013) extended the MoE structure to the deep neural networks, and proposed a deep MoE model composed of multiple layers of routers and experts. Shazeer et al. (2017) simplified the MoE layer by making the output of the gating function sparse for each example, which greatly improves the training stability and reduces the computational cost. Since then, the MoE layer with different base neural network structures (Shazeer et al., 2017; Dauphin et al., 2017; Vaswani et al., 2017) has been proposed and achieved tremendous successes in a variety of language tasks. Very recently, Fedus et al. (2021) improved the performance of the MoE layer by routing one example to only a single expert instead of $K$ experts, which further reduces the routing computation while preserving the model quality.</p>
<p>Mixture of Linear Regressions/Classifications. In this paper, we consider a "mixture of classification" model. This type of models can be dated back to (De Veaux, 1989; Jordan and Jacobs, 1994; Faria and Soromenho, 2010) and has been applied to many tasks including object recognition (Quattoni et al., 2004) human action recognition (Wang and Mori, 2009), and machine translation (Liang et al., 2006). In order to learn the unknown parameters for mixture of linear regressions/classification model, (Anandkumar et al., 2012; Hsu et al., 2012; Chaganty and Liang, 2013; Anandkumar et al., 2014; Li and Liang, 2018) studies the method of moments and tensor factorization. Another line of work studies specific algorithms such as Expectation-Maximization (EM) algorithm (Khalili and Chen, 2007; Yi et al., 2014; Balakrishnan et al., 2017; Wang et al., 2015).</p>
<p>Theoretical Understanding of Deep Learning. In recent years, great efforts have been made to establish the theoretical foundation of deep learning. A series of studies have proved the convergence (Jacot et al., 2018; Li and Liang, 2018; Du et al., 2019; Allen-Zhu et al., 2019b; Zou et al., 2018) and generalization (Allen-Zhu et al., 2019a; Arora et al., 2019a,b; Cao and Gu, 2019) guarantees in the so-called "neural tangent kernel" (NTK) regime, where the parameters stay close to the initialization, and the neural network function is approximately linear in its parameters. A recent line of works (Allen-Zhu and Li, 2019; Bai and Lee, 2019; Allen-Zhu and Li, 2020a,b,c; Li et al., 2020; Cao et al., 2022; Zou et al., 2021; Wen and Li, 2021) studied the learning dynamic of neural networks beyond the NTK regime. It is worthwhile to mention that our analysis of the MoE model is also beyond the NTK regime.</p>
<h1>3 Problem Setting and Preliminaries</h1>
<p>We consider an MoE layer with each expert being a two-layer CNN trained by gradient descent (GD) over $n$ independent training examples $\left{\left(\mathbf{x}<em i="i">{i}, y</em>$, and then explain our neural network model and the details of the training algorithm.}\right)\right}_{i=1}^{n}$ generated from a data distribution $\mathcal{D}$. In this section, we will first introduce our data model $\mathcal{D</p>
<h3>3.1 Data distribution</h3>
<p>We consider a binary classification problem over $P$-patch inputs, where each patch has $d$ dimensions. In particular, each labeled data is represented by $(\mathbf{x}, y)$, where input $\mathbf{x}=\left(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(P)}\right) \in$ $\left(\mathbb{R}^{d}\right)^{P}$ is a collection of $P$ patches and $y \in{ \pm 1}$ is the data label. We consider data generated from $K$ clusters. Each cluster $k \in[K]$ has a label signal vector $\mathbf{v}<em k="k">{k}$ and a cluster-center signal vector $\mathbf{c}</em>}$ with $\left|\mathbf{v<em 2="2">{k}\right|</em>}=\left|\mathbf{c<em 2="2">{k}\right|</em>}=1$. For simplicity, we assume that all the signals $\left{\mathbf{v<em _in_K_="\in[K]" k="k">{k}\right}</em>} \cup\left{\mathbf{c<em _in_K_="\in[K]" k="k">{k}\right}</em>$ are orthogonal with each other.</p>
<p>Definition 3.1. A data pair $(\mathbf{x}, y) \in\left(\mathbb{R}^{d}\right)^{P} \times{ \pm 1}$ is generated from the distribution $\mathcal{D}$ as follows.</p>
<ul>
<li>Uniformly draw a pair $\left(k, k^{\prime}\right)$ with $k \neq k^{\prime}$ from ${1, \ldots, K}$.</li>
<li>Generate the label $y \in{ \pm 1}$ uniformly, generate a Rademacher random variable $\epsilon \in{ \pm 1}$.</li>
<li>Independently generate random variables $\alpha, \beta, \gamma$ from distribution $\mathcal{D}<em _beta="\beta">{\alpha}, \mathcal{D}</em>}, \mathcal{D<em 1="1">{\gamma}$. In this paper, we assume there exists absolute constants $C</em>$.}, C_{2}$ such that almost surely $0&lt;C_{1} \leq \alpha, \beta, \gamma \leq C_{2</li>
<li>Generate $\mathbf{x}$ as a collection of $P$ patches: $\mathbf{x}=\left(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(P)}\right) \in\left(\mathbb{R}^{d}\right)^{P}$, where</li>
<li>Feature signal. One and only one patch is given by $y \alpha \mathbf{v}_{k}$.</li>
<li>Cluster-center signal. One and only one patch is given by $\beta \mathbf{c}_{k}$.</li>
<li>
<p>Feature noise. One and only one patch is given by $\epsilon \gamma \mathbf{v}_{k^{\prime}}$.</p>
</li>
<li>
<p>Random noise. The rest of the $P-3$ patches are Gaussian noises that are independently drawn from $N\left(0,\left(\sigma_{p}^{2} / d\right) \cdot \mathbf{I}<em p="p">{d}\right)$ where $\sigma</em>$ is an absolute constant.</p>
</li>
</ul>
<p>How to learn this type of data? Since the positions of signals and noises are not specified in Definition 3.1, it is natural to use the CNNs structure that applies the same function to each patch. We point out that the strength of the feature noises $\gamma$ could be as large as the strength of the feature signals $\alpha$. As we will see later in Theorem 4.1, this classification problem is hard to learn with a single expert, such as any two-layer CNNs (any activation function with any number of neurons). However, such a classification problem has an intrinsic clustering structure that may be utilized to achieve better performance. Examples can be divided into $K$ clusters $\cup_{k \in[K]} \Omega_{k}$ based on the cluster-center signals: an example $(\mathbf{x}, y) \in \Omega_{k}$ if and only if at least one patch of $\mathbf{x}$ aligns with $\mathbf{c}<em k="k">{k}$. It is not difficult to show that the binary classification sub-problem over $\Omega</em>$ can be easily solved by an individual expert. We expect the MoE can learn this data cluster structure from the cluster-center signals.
Significance of our result. Although this data can be learned by existing works on a mixture of linear classifiers with sophisticated algorithms (Anandkumar et al., 2012; Hsu et al., 2012; Chaganty and Liang, 2013), the focus of our paper is training a mixture of nonlinear neural networks, a more practical model used in real applications. When an MoE is trained by variants of gradient descent, we show that the experts automatically learn to specialize on each cluster, while the router automatically learns to dispatch the data to the experts according to their specialty. Although from a representation point of view, it is not hard to see that the concept class can be represented by MoEs, our result is very significant as we prove that gradient descent from random initialization can find a good MoE with non-linear experts efficiently. To make our results even more compelling, we empirically show that MoE with linear experts, despite also being able to represent the concept class, cannot be trained to find a good classifier efficiently.</p>
<h1>3.2 Structure of the MoE layer</h1>
<p>An MoE layer consists of a set of $M$ "expert networks" $f_{1}, \ldots, f_{M}$, and a gating network which is generally set to be linear (Shazeer et al., 2017; Fedus et al., 2021). Denote by $f_{m}(\mathbf{x} ; \mathbf{W})$ the output of the $m$-th expert network with input $x$ and parameter $\mathbf{W}$. Define an $M$-dimensional vector $\mathbf{h}(\mathbf{x} ; \boldsymbol{\Theta})=\sum_{p \in[P]} \boldsymbol{\Theta}^{\top} \mathbf{x}^{(p)}$ as the output of the gating network parameterized by $\boldsymbol{\Theta}=\left[\boldsymbol{\theta}<em M="M">{1}, \ldots, \boldsymbol{\theta}</em>$. The output $F$ of the MoE layer can be written as follows:}\right] \in$ $\mathbb{R}^{d \times M</p>
<p>$$
F(\mathbf{x} ; \boldsymbol{\Theta}, \mathbf{W})=\sum_{m \in \mathcal{T}<em m="m">{\mathbf{x}}} \pi</em>)
$$}(\mathbf{x} ; \boldsymbol{\Theta}) f_{m}(\mathbf{x} ; \mathbf{W</p>
<p>where $\mathcal{T}<em m="m">{\mathbf{x}} \subseteq[M]$ is a set of selected indices and $\pi</em>)$ 's are route gate values given by}(\mathbf{x} ; \boldsymbol{\Theta</p>
<p>$$
\pi_{m}(\mathbf{x} ; \boldsymbol{\Theta})=\frac{\exp \left(h_{m}(\mathbf{x} ; \boldsymbol{\Theta})\right)}{\sum_{m^{\prime}=1}^{M} \exp \left(h_{m^{\prime}}(\mathbf{x} ; \boldsymbol{\Theta})\right)}, \forall m \in[M]
$$</p>
<p>Expert Model. In practice, one often uses nonlinear neural networks as experts in the MoE layer. In fact, we found that the non-linearity of the expert is essential for the success of the MoE layer (see Section 6). For $m$-th expert, we consider a convolution neural network as follows:</p>
<p>$$
f_{m}(\mathbf{x} ; \mathbf{W})=\sum_{j \in[J]} \sum_{p=1}^{P} \sigma\left(\left\langle\mathbf{w}_{m, j}, \mathbf{x}^{(p)}\right\rangle\right)
$$</p>
<p>where $\mathbf{w}<em m="m">{m, j} \in \mathbb{R}^{d}$ is the weight vector of the $j$-th filter (i.e., neuron) in the $m$-th expert, $J$ is the number of filters (i.e., neurons). We denote $\mathbf{W}</em>}=\left[\mathbf{w<em J="J" m_="m,">{m, 1}, \ldots, \mathbf{w}</em>}\right] \in \mathbb{R}^{d \times J}$ as the weight matrix of the $m$-th expert and further let $\mathbf{W}=\left{\mathbf{W<em _in_M_="\in[M]" m="m">{m}\right}</em>$, which is one of the simplest nonlinear activation functions (Vecci et al., 1998). We also include the experiment for other activation functions such as RELU in Appendix Table 7.
Top-1 Routing Model. A simple choice of the selection set $\mathcal{T}}$ as the collection of expert weight matrices. For nonlinear CNN, we consider the cubic activation function $\sigma(z)=z^{3<em _mathbf_x="\mathbf{x">{\mathbf{x}}$ would be the whole experts set $\mathcal{T}</em>}}=[M]$ (Jordan and Jacobs, 1994), which is the case for the so-called soft-routing model. However, it would be time consuming to use soft-routing in deep learning. In this paper, we consider "switch routing", which is introduced by Fedus et al. (2021) to make the gating network sparse and save the computation time. For each input $\mathbf{x}$, instead of using all the experts, we only pick one expert from $[M]$, i.e., $\left|\mathcal{T<em _mathbf_x="\mathbf{x">{\mathbf{x}}\right|=1$. In particular, we choose $\mathcal{T}</em>}}=\operatorname{argmax<em m="m">{m}\left{h</em>)\right}$.
}(\mathbf{x} ; \boldsymbol{\Theta<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of an MoE layer. For each input $\mathbf{x}$, the router will only select one expert to perform computations. The choice is based on the output of the gating network (dotted line). The expert layer returns the output of the selected expert (gray box) multiplied by the route gate value (softmax of the gating function output).</p>
<p>Algorithm 1 Gradient descent with random initialization
Require: Number of iterations $T$, expert learning rate $\eta$, router learning rate $\eta_{r}$, initialization scale $\sigma_{0}$, training set $S=\left{\left(\mathbf{x}<em i="i">{i}, y</em>\right)\right}<em 0="0">{i=1}^{n}$.
1: Generate each entry of $\mathbf{W}^{(0)}$ independently from $N\left(0, \sigma</em>\right)$.
2: Initialize each entry of $\boldsymbol{\Theta}^{(0)}$ as zero.
3: for $t=0,2, \ldots, T-1$ do
4: $\quad$ Generate each entry of $\mathbf{r}^{(t)}$ independently from Unif $[0,1]$.
5: Update $\mathbf{W}^{(t+1)}$ as in (3.4).
6: Update $\boldsymbol{\Theta}^{(t+1)}$ as in (3.5).
7: end for
8: return $\left(\boldsymbol{\Theta}^{(T)}, \mathbf{W}^{(T)}\right)$.}^{2</p>
<h1>3.3 Training Algorithm</h1>
<p>Given the training data $S=\left{\left(\mathbf{x}<em i="i">{i}, y</em>$, we train $F$ with gradient descent to minimize the following empirical loss function:}\right)\right}_{i=1}^{n</p>
<p>$$
\mathcal{L}(\boldsymbol{\Theta}, \mathbf{W})=\frac{1}{n} \sum_{i=1}^{n} \ell\left(y_{i} F\left(\mathbf{x}_{i} ; \boldsymbol{\Theta}, \mathbf{W}\right)\right)
$$</p>
<p>where $\ell$ is the logistic loss defined as $\ell(z)=\log (1+\exp (-z))$. We initialize $\boldsymbol{\Theta}^{(0)}$ to be zero and initialize each entry of $\mathbf{W}^{(0)}$ by i.i.d $\mathcal{N}\left(0, \sigma_{0}^{2}\right)$. Zero initialization of the gating network is widely used in MoE training. As discussed in Shazeer et al. (2017), it can help avoid out-of-memory errors and initialize the network in a state of approximately equal expert load (see (5.1) for the definition of expert load).</p>
<p>Instead of directly using the gradient of empirical loss (3.2) to update weights, we add perturbation to the router and use the gradient of the perturbed empirical loss to update the weights. In particular, the training example $\mathbf{x}<em m="m">{i}$ will be distributed to $\operatorname{argmax}</em>}\left{h_{m}\left(\mathbf{x<em i="i" m_="m,">{i} ; \boldsymbol{\Theta}^{(t)}\right)+\tau</em>$ are random noises. Adding noise term is a widely used training strategy for sparsely-gated MoE layer (Shazeer et al., 2017; Fedus et al., 2021), which can encourage explo-}^{(t)}\right}$ instead, where $\left{r_{m, i}^{(t)}\right}_{m \in[M], i \in[n]</p>
<p>ration across the experts and stabilize the MoE training. In this paper, we draw $\left{r_{m, i}^{(t)}\right}_{m \in[M], i \in[n]}$ independently from the uniform distribution Unif $[0,1]$ and denotes its collection as $\mathbf{r}^{(t)}$. Therefore, the perturbed empirical loss at iteration $t$ can be written as</p>
<p>$$
\mathcal{L}^{(t)}\left(\boldsymbol{\Theta}^{(t)}, \mathbf{W}^{(t)}\right)=\frac{1}{n} \sum_{i=1}^{n} \ell\left(y_{i} \pi_{m_{i, t}}\left(\mathbf{x}<em m__i_="m_{i," t="t">{i} ; \boldsymbol{\Theta}^{(t)}\right) f</em>\right)\right)
$$}}\left(\mathbf{x}_{i} ; \mathbf{W}^{(t)</p>
<p>where $m_{i, t}=\operatorname{argmax}<em m="m">{m}\left{h</em>}\left(\mathbf{x<em i="i" m_="m,">{i} ; \boldsymbol{\Theta}^{(t)}\right)+r</em>$, the gradient descent update rule for the experts is}^{(t)}\right}$. Starting from the initialization $\mathbf{W}^{(0)</p>
<p>$$
\mathbf{W}<em m="m">{m}^{(t+1)}=\mathbf{W}</em>}^{(t)}-\eta \cdot \nabla_{\mathbf{W<em _mathbf_W="\mathbf{W">{m}} \mathcal{L}^{(t)}\left(\boldsymbol{\Theta}^{(t)}, \mathbf{W}^{(t)}\right) /\left|\nabla</em><em F="F">{m}} \mathcal{L}^{(t)}\left(\boldsymbol{\Theta}^{(t)}, \mathbf{W}^{(t)}\right)\right|</em>, \forall m \in[M]
$$</p>
<p>where $\eta&gt;0$ is the expert learning rate. Starting from the initialization $\boldsymbol{\Theta}^{(0)}$, the gradient update rule for the gating network is</p>
<p>$$
\boldsymbol{\theta}<em m="m">{m}^{(t+1)}=\boldsymbol{\theta}</em>\right), \forall m \in[M]
$$}^{(t)}-\eta_{r} \cdot \nabla_{\boldsymbol{\theta}_{m}} \mathcal{L}^{(t)}\left(\boldsymbol{\Theta}^{(t)}, \mathbf{W}^{(t)</p>
<p>where $\eta_{r}&gt;0$ is the router learning rate. In practice, the experts are trained by Adam (?) to make sure they have similar learning speeds. Here we use a normalized gradient which can be viewed as a simpler alternative to Adam (Jelassi et al., 2021).</p>
<h1>4 Main Results</h1>
<p>In this section, we will present our main results. We first provide a negative result for learning with a single expert.</p>
<p>Theorem 4.1 (Single expert performs poorly). Suppose $\mathcal{D}<em _gamma="\gamma">{\alpha}=\mathcal{D}</em>) \leq 0) \geq 1 / 8$.}$ in Definition 3.1, then any function with the form $F(\mathbf{x})=\sum_{p=1}^{P} f\left(\mathbf{x}^{(p)}\right)$ will get large test error $\mathbb{P}_{(\mathbf{x}, y) \sim \mathcal{D}}(y F(\mathbf{x</p>
<p>Theorem 4.1 indicates that if the feature noise has the same strength as the feature signal i.e., $\mathcal{D}<em _gamma="\gamma">{\alpha}=\mathcal{D}</em>}$, any two-layer CNNs with the form $F(\mathbf{x})=\sum_{j \in[J]} a_{j} \sum_{p \in[P]} \sigma\left(\mathbf{w<em j="j">{j}^{\top} \mathbf{x}^{(p)}+b</em>\right)$ can't perform well on the classification problem defined in Definition 3.1 where $\sigma$ can be any activation function. Theorem 4.1 also shows that a simple ensemble of the experts may not improve the performance because the ensemble of the two-layer CNNs is still in the form of the function defined in Theorem 4.1.</p>
<p>As a comparison, the following theorem gives the learning guarantees for training an MoE layer that follows the structure defined in Section 3.2 with cubic activation function.</p>
<p>Theorem 4.2 (Nonlinear MoE performs well). Suppose the training data size $n=\Omega(d)$. Choose experts number $M=\Theta(K \log K \log \log d)$, filter size $J=\Theta(\log M \log \log d)$, initialization scale $\sigma_{0} \in\left[d^{-1 / 3}, d^{-0.01}\right]$, learning rate $\eta=\widetilde{O}\left(\sigma_{0}\right), \eta_{r}=\Theta\left(M^{2}\right) \eta$. Then with probability at least $1-o(1)$, Algorithm 1 is able to output $\left(\boldsymbol{\Theta}^{(T)}, \mathbf{W}^{(T)}\right)$ within $T=\widetilde{O}\left(\eta^{-1}\right)$ iterations such that the non-linear MoE defined in Section 3.2 satisfies</p>
<ul>
<li>Training error is zero, i.e., $y_{i} F\left(\mathbf{x}_{i} ; \boldsymbol{\Theta}^{(T)}, \mathbf{W}^{(T)}\right)&gt;0, \forall i \in[n]$.</li>
<li>Test error is nearly zero, i.e., $\mathbb{P}_{(\mathbf{x}, y) \sim \mathcal{D}}\left(y F\left(\mathbf{x} ; \boldsymbol{\Theta}^{(T)}, \mathbf{W}^{(T)}\right) \leq 0\right)=o(1)$.</li>
</ul>
<p>More importantly, the experts can be divided into a disjoint union of $K$ non-empty sets $[M]=$ $\sqcup_{k \in[K]} \mathcal{M}_{k}$ and</p>
<ul>
<li>(Each expert is good on one cluster) Each expert $m \in \mathcal{M}<em k="k">{k}$ performs good on the cluster $\Omega</em>}$, $\mathbb{P<em m="m">{(\mathbf{x}, y) \sim \mathcal{D}}\left(y f</em>\right)=o(1)$.}\left(\mathbf{x} ; \mathbf{W}^{(T)}\right) \leq 0 \mid(\mathbf{x}, y) \in \Omega_{k</li>
<li>(Router only distributes example to good expert) With probability at least $1-o(1)$, an example $\mathbf{x} \in \Omega_{k}$ will be routed to one of the experts in $\mathcal{M}_{k}$.</li>
</ul>
<p>Theorem 4.2 shows that a non-linear MoE performs well on the classification problem in Definition 3.1. In addition, the router will learn the cluster structure and divide the problem into $K$ simpler sub-problems, each of which is associated with one cluster. In particular, each cluster will be classified accurately by a subset of experts. On the other hand, each expert will perform well on at least one cluster.</p>
<p>Furthermore, together with Theorem 4.1, Theorem 4.2 suggests that there exist problem instances in Definition 3.1 (i.e., $\mathcal{D}<em _gamma="\gamma">{\alpha}=\mathcal{D}</em>$ ) such that an MoE provably outperforms a single expert.</p>
<h1>5 Overview of Key Techniques</h1>
<p>A successful MoE layer needs to ensure that the router can learn the cluster-center features and divide the complex problem in Definition 3.1 into simpler linear classification sub-problems that individual experts can conquer. Finding such a gating network is difficult because this problem is highly non-convex. In the following, we will introduce the main difficulties in analyzing the MoE layer and the corresponding key techniques to overcome those barriers.
Main Difficulty 1: Discontinuities in Routing. Compared with the traditional soft-routing model, the sparse routing model saves computation and greatly reduces the inference time. However, this form of sparsity also causes discontinuities in routing (Shazeer et al., 2017). In fact, even a small perturbation of the gating network outputs $\mathbf{h}(\mathbf{x} ; \boldsymbol{\Theta})+\boldsymbol{\delta}$ may change the router behavior drastically if the second largest gating network output is close to the largest gating network output. Key Technique 1: Stability by Smoothing. We point out that the noise term added to the gating network output ensures a smooth transition between different routing behavior, which makes the router more stable. This is proved in the following lemma.</p>
<p>Lemma 5.1. Let $\mathbf{h}, \widehat{\mathbf{h}} \in \mathbb{R}^{M}$ to be the output of the gating network and $\left{r_{m}\right}<em m="m">{m=1}^{M}$ to be the noise independently drawn from Unif $[0,1]$. Denote $\mathbf{p}, \widehat{\mathbf{p}} \in \mathbb{R}^{M}$ to be the probability that experts get routed, i.e., $p</em>}=\mathbb{P}\left(\operatorname{argmax<em m_prime="m^{\prime">{m^{\prime} \in[M]}\left{h</em>}}+r_{m^{\prime}}\right}=m\right), \widehat{p<em m_prime="m^{\prime">{m}=\mathbb{P}\left(\operatorname{argmax}</em>} \in[M]}\left{\widehat{h<em m_prime="m^{\prime">{m^{\prime}}+r</em>|}}\right}=m\right)$. Then we have that $|\mathbf{p}-\widehat{\mathbf{p}<em _infty="\infty">{\infty} \leq M^{2}|\mathbf{h}-\widehat{\mathbf{h}}|</em>$.</p>
<p>Lemma 5.1 implies that when the change of the gating network outputs at iteration $t$ and $t^{\prime}$ is small, i.e., $\left|\mathbf{h}\left(\mathbf{x} ; \boldsymbol{\Theta}^{(t)}\right)-\mathbf{h}\left(\mathbf{x} ; \boldsymbol{\Theta}^{\left(t^{\prime}\right)}\right)\right|<em m="m">{\infty}$, the router behavior will be similar. So adding noise provides a smooth transition from time $t$ to $t^{\prime}$. It is also worth noting that $\boldsymbol{\Theta}$ is zero initialized. So $\mathbf{h}\left(\mathbf{x} ; \boldsymbol{\Theta}^{(0)}\right)=0$ and thus each expert gets routed with the same probability $p</em>$ is small, router will almost uniformly pick one expert from $[M]$, which helps exploration across experts.
Main Difficulty 2: No "Real" Expert. At the beginning of the training, the gating network is zero, and the experts are randomly initialized. Thus it is hard for the router to learn the right features because all the experts look the same: they share the same network architecture and are trained by the same algorithm. The only difference would be the initialization. Moreover, if the router makes a mistake at the beginning of the training, the experts may amplify the mistake because the experts will be trained based on mistakenly dispatched data.}=1 / M$ by symmetric property. Therefore, at the early of the training when $\left|\mathbf{h}\left(\mathbf{x} ; \boldsymbol{\Theta}^{(t)}\right)-\mathbf{h}\left(\mathbf{x} ; \boldsymbol{\Theta}^{(0)}\right)\right|_{\infty</p>
<p>Key Technique 2: Experts from Exploration. Motivated by the key technique 1, we introduce an exploration stage to the analysis of MoE layer during which the router almost uniformly picks one expert from $[M]$. This stage starts at $t=0$ and ends at $T_{1}=\left\lfloor\eta^{-1} \sigma_{0}^{0.5}\right\rfloor \ll T=\widetilde{O}\left(\eta^{-1}\right)$ and the gating network remains nearly unchanged $\left|\mathbf{h}\left(\mathbf{x} ; \boldsymbol{\Theta}^{(t)}\right)-\mathbf{h}\left(\mathbf{x} ; \boldsymbol{\Theta}^{(0)}\right)\right|<em 0="0">{\infty}=O\left(\sigma</em>}^{1.5}\right)$. Because the experts are treated almost equally during exploration stage, we can show that the experts become specialized to some specific task only based on the initialization. In particular, the experts set $[M]$ can be divided into $K$ nonempty disjoint sets $[M]=\sqcup_{k} \mathcal{M<em k="k">{k}$, where $\mathcal{M}</em>}:=\left{m \mid \operatorname{argmax<em k_prime="k^{\prime">{k^{\prime} \in[K], j \in[J]}\left\langle\mathbf{v}</em>}}, \mathbf{w<em k="k">{m, j}^{(0)}\right\rangle=k\right}$. For nonlinear MoE with cubic activation function, the following lemma further shows that experts in different set $\mathcal{M}</em>$ will diverge at the end of the exploration stage.</p>
<p>Lemma 5.2. Under the same condition as in Theorem 4.2, with probability at least $1-o(1)$, the following equations hold for all expert $m \in \mathcal{M}_{k}$,</p>
<p>$$
\begin{aligned}
&amp; \mathbb{P}<em m="m">{(\mathbf{x}, y) \sim \mathcal{D}}\left(y f</em>\right)=o(1) \
&amp; \mathbb{P}}\left(\mathbf{x} ; \mathbf{W}^{\left(T_{1}\right)}\right) \leq 0 \mid(\mathbf{x}, y) \in \Omega_{k<em m="m">{(\mathbf{x}, y) \sim \mathcal{D}}\left(y f</em> \neq k
\end{aligned}
$$}\left(\mathbf{x} ; \mathbf{W}^{\left(T_{1}\right)}\right) \leq 0 \mid(\mathbf{x}, y) \in \Omega_{k^{\prime}}\right)=\Omega(1 / K), \forall k^{\prime</p>
<p>Lemma 5.2 implies that, at the end of the exploration stage, the expert $m \in \mathcal{M}<em k="k">{k}$ can achieve nearly zero test error on the cluster $\Omega</em> \neq k$.
Main Difficulty 3: Expert Load Imbalance. Given the training data set $S=\left{\left(\mathbf{x}}$ but high test error on the other clusters $\Omega_{k^{\prime}}, k^{\prime<em i="i">{i}, y</em>$, the load of expert $m$ at iterate $t$ is defined as}\right)\right}_{i=1}^{n</p>
<p>$$
\operatorname{Load}<em _in_n_="\in[n]" i="i">{m}^{(t)}=\sum</em>=m\right)
$$} \mathbb{P}\left(m_{i, t</p>
<p>where $\mathbb{P}\left(m_{i, t}=m\right)$ is probability that the input $\mathbf{x}<em m="m">{i}$ being routed to expert $m$ at iteration $t$. Eigen et al. (2013) first described the load imbalance issues in the training of the MoE layer. The gating network may converge to a state where it always produces large $\operatorname{Load}</em>$ for the same few experts. This imbalance in expert load is self-reinforcing, as the favored experts are trained more rapidly and thus are selected even more frequently by the router (Shazeer et al., 2017; Fedus et al., 2021). Expert load imbalance issue not only causes memory and performance problems in practice, but also impedes the theoretical analysis of the expert training.
Key Technique 3: Normalized Gradient Descent. Lemma 5.2 shows that the experts will diverge into $\sqcup_{k \in[K]} \mathcal{M}}^{(t)<em k="k">{k}$. Normalized gradient descent can help different experts in the same $\mathcal{M}</em>}$ being trained at the same speed regardless the imbalance load caused by the router. Because the self-reinforcing circle no longer exists, we can prove that the router will treat different experts in the same $\mathcal{M<em k="1">{k}$ almost equally and dispatch almost the same amount of data to them (See Section E. 2 in Appendix for detail). This Load imbalance issue can be further avoided by adding load balancing loss (Eigen et al., 2013; Shazeer et al., 2017; Fedus et al., 2021), or advanced MoE layer structure such as BASE Layers (Lewis et al., 2021; Dua et al., 2021) and Hash Layers (Roller et al., 2021).
Road Map: Here we provide the road map of the proof of Theorem 4.2 and the full proof is presented in Appendix E. The training process can be decomposed into several stages. The first stage is called Exploration stage. During this stage, the experts will diverge into $K$ professional groups $\sqcup</em>}^{K} \mathcal{M<em k="k">{k}=[M]$. In particular, we will show that $\mathcal{M}</em>}$ is not empty for all $k \in[K]$. Besides, for all $m \in \mathcal{M<em m="m">{k}, f</em>$. Finally, we will give the generalization analysis for the MoEs from the previous two stages.}$ is a good classifier over $\Omega_{k}$. The second stage is called router learning stage. During this stage, the router will learn to dispatch $\mathbf{x} \in \Omega_{k}$ to one of the experts in $\mathcal{M}_{k</p>
<h1>6 Experiments</h1>
<p>Setting 1: $\alpha \in(0.5,2), \beta \in(1,2), \gamma \in(0.5,3), \sigma_{p}=1$</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test accuracy (\%)</th>
<th style="text-align: center;">Dispatch Entropy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single (linear)</td>
<td style="text-align: center;">68.71</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Single (nonlinear)</td>
<td style="text-align: center;">79.48</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">MoE (linear)</td>
<td style="text-align: center;">$92.99 \pm 2.11$</td>
<td style="text-align: center;">$1.300 \pm 0.044$</td>
</tr>
<tr>
<td style="text-align: center;">MoE (nonlinear)</td>
<td style="text-align: center;">$\mathbf{9 9 . 4 6} \pm \mathbf{0 . 5 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 9 8} \pm \mathbf{0 . 0 8 7}$</td>
</tr>
</tbody>
</table>
<p>Setting 2: $\alpha \in(0.5,2), \beta \in(1,2), \gamma \in(0.5,3), \sigma_{p}=2$</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test accuracy (\%)</th>
<th style="text-align: center;">Dispatch Entropy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single (linear)</td>
<td style="text-align: center;">60.59</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">Single (nonlinear)</td>
<td style="text-align: center;">72.29</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">MoE (linear)</td>
<td style="text-align: center;">$88.48 \pm 1.96$</td>
<td style="text-align: center;">$1.294 \pm 0.036$</td>
</tr>
<tr>
<td style="text-align: center;">MoE (nonlinear)</td>
<td style="text-align: center;">$\mathbf{9 8 . 0 9} \pm \mathbf{1 . 2 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 7 1} \pm \mathbf{0 . 1 0 3}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison between MoE (linear) and MoE (nonlinear) in our setting. We report results of top-1 gating with noise for both linear and nonlinear models. Over ten random experiments, we report the average value $\pm$ standard deviation for both test accuracy and dispatch entropy.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of router dispatch entropy. We demonstrate the change of entropy of MoE during training on the synthetic data. MoE (linear)-1 and MoE (nonlinear)-1 refer to Setting 1 in Table 1. MoE (linear)-2 and MoE (nonlinear)-2 refer to Setting 2 in Table 1.</p>
<h3>6.1 Synthetic-data Experiments</h3>
<p>Datasets. We generate 16,000 training examples and 16,000 test examples from the data distribution defined in Definition 3.1 with cluster number $K=4$, patch number $P=4$ and dimension $d=50$. We randomly shuffle the order of the patches of $\mathbf{x}$ after we generate data $(\mathbf{x}, y)$. We consider two parameter settings: 1. $\alpha \sim \operatorname{Uniform}(0.5,2), \beta \sim \operatorname{Uniform}(1,2), \gamma \sim \operatorname{Uniform}(0.5,3)$ and $\sigma_{p}=1 ; 2 . \alpha \sim \operatorname{Uniform}(0.5,2), \beta \sim \operatorname{Uniform}(1,2), \gamma \sim \operatorname{Uniform}(0.5,3)$ and $\sigma_{p}=2$. Note that Theorem 4.1 shows that when $\alpha$ and $\gamma$ follow the same distribution, neither single linear expert or single nonlinear expert can give good performance. Here we consider a more general and difficult setting when $\alpha$ and $\gamma$ are from different distributions.
Models. We consider the performances of single linear CNN, single nonlinear CNN, linear MoE, and nonlinear MoE. The single nonlinear CNN architecture follows (3.1) with cubic activation function, while single linear CNN follows (3.1) with identity activation function. For both linear and nonlinear MoEs, we consider a mixture of 8 experts with each expert being a single linear CNN or a single nonlinear CNN. Finally, we train single models with gradient descent and train the MoEs with Algorithm 1. We run 10 random experiments and report the average accuracy with standard deviation.
Evaluation. To evaluate how well the router learned the underlying cluster structure of the data, we define the entropy of the router's dispatch as follows. Denote by $n_{k, m}$ the number of data in cluster $K$ that are dispatched to expert $m$. The total number of data dispatched to expert $m$ is $n_{m}=\sum_{k=1}^{K} n_{k, m}$ and the total number of data is $n=\sum_{k=1}^{K} \sum_{m=1}^{M} n_{k, m}$. The dispatch entropy is</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">CIFAR-10 (\%)</th>
<th style="text-align: center;">CIFAR-10-Rotate (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CNN</td>
<td style="text-align: left;">Single</td>
<td style="text-align: center;">$80.68 \pm 0.45$</td>
<td style="text-align: center;">$76.78 \pm 1.79$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MoE</td>
<td style="text-align: center;">$80.31 \pm 0.62$</td>
<td style="text-align: center;">$\mathbf{7 9 . 6 0} \pm \mathbf{1 . 2 5}$</td>
</tr>
<tr>
<td style="text-align: left;">MobileNetV2</td>
<td style="text-align: left;">Single</td>
<td style="text-align: center;">$92.45 \pm 0.25$</td>
<td style="text-align: center;">$85.76 \pm 2.91$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MoE</td>
<td style="text-align: center;">$92.23 \pm 0.72$</td>
<td style="text-align: center;">$\mathbf{8 9 . 8 5} \pm \mathbf{2 . 5 4}$</td>
</tr>
<tr>
<td style="text-align: left;">ResNet18</td>
<td style="text-align: left;">Single</td>
<td style="text-align: center;">$95.51 \pm 0.31$</td>
<td style="text-align: center;">$88.23 \pm 0.96$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">MoE</td>
<td style="text-align: center;">$95.32 \pm 0.68$</td>
<td style="text-align: center;">$\mathbf{9 2 . 6 0} \pm \mathbf{2 . 0 1}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison between MoE and single model on CIFAR-10 and CIFAR-10-Rotate datasets. We report the average test accuracy over 10 random experiments $\pm$ the standard deviation.
then defined as</p>
<p>$$
\text { entropy }=-\sum_{m=1, n_{m} \neq 0}^{M} \frac{n_{m}}{n} \sum_{k=1}^{K} \frac{n_{k, m}}{n_{m}} \cdot \log \left(\frac{n_{k, m}}{n_{m}}\right)
$$</p>
<p>When each expert receives the data from at most one cluster, the dispatch entropy will be zero. And a uniform dispatch will result in the maximum dispatch entropy.</p>
<p>As shown in Table 1, the linear MoE does not perform as well as the nonlinear MoE in Setting 1, with around $6 \%$ less test accuracy and much higher variance. With stronger random noise (Setting 2), the difference between the nonlinear MoE and linear MoE becomes even more significant. We also observe that the final dispatch entropy of nonlinear MoE is nearly zero while that of the linear MoE is large. In Figure 3, we further demonstrate the change of dispatch entropy during the training process. The dispatch entropy of nonlinear MoE significantly decreases, while that of linear MoE remains large. Such a phenomenon indicates that the nonlinear MoE can successfully learn the underlying cluster structure of the data while the linear MoE fails to do so.</p>
<h1>6.2 Real-data Experiments</h1>
<p>We further conduct experiments on real image datasets and demonstrate the importance of the clustering data structure to the MoE layer in deep neural networks.
Datasets. We consider the CIFAR-10 dataset (Krizhevsky, 2009) and the 10-class classification task. Furthermore, we create a CIFAR-10-Rotate dataset that has a strong underlying cluster structure that is independent of its labeling function. Specifically, we rotate the images by 30 degrees and merge the rotated dataset with the original one. The task is to predict if the image is rotated, which is a binary classification problem. We deem that some of the classes in CIFAR-10 form underlying clusters in CIFAR-10-Rotate. In Appendix A, we explain in detail how we generate CIFAR-10-Rotate and present some specific examples.
Models. For the MoE, we consider a mixture of 4 experts with a linear gating network. For the expert/single model architectures, we consider a CNN with 2 convolutional layers (architecture details are illustrated in Appendix A.) For a more thorough evaluation, we also consider expert/single models with architecture including MobileNetV2 (Sandler et al., 2018) and ResNet18 (He et al., 2016). The training process of MoE also follows Algorithm 1.</p>
<p>The experiment results are shown in Table 2, where we compare single and mixture models of different architectures over CIFAR-10 and CIFAR-10-Rotate datasets. We observe that the improvement of MoEs over single models differs largely on the different datasets. On CIFAR-10,</p>
<p>the performance of MoEs is very close to the single models. However, on the CIFAR-10-Rotate dataset, we can observe a significant performance improvement from single models to MoEs. Such results indicate the advantage of MoE over single models depends on the task and the cluster structure of the data.</p>
<h1>7 Conclusion and Future Work</h1>
<p>In this work, we formally study the mechanism of the Mixture of Experts (MoE) layer for deep learning. To our knowledge, we provide the first theoretical result toward understanding how the MoE layer works in deep learning. Our empirical evidence reveals that the cluster structure of the data plays an important role in the success of the MoE layer. Motivated by these empirical observations, we study a data distribution with cluster structure and show that Mixture-of-Experts provably improves the test accuracy of a single expert of two-layer CNNs.</p>
<p>There are several important future directions. First, our current results are for CNNs. It is interesting to extend our results to other neural network architectures, such as transformers. Second, our data distribution is motivated by the classification problem of image data. We plan to extend our analysis to other types of data (e.g., natural language data).</p>
<h2>A Experiment Details</h2>
<h2>A. 1 Visualization</h2>
<p>In the visualization of Figure 1, MoE (linear) and MoE (nonlinear) are trained according to Algorithm 1 by normalized gradient descent with learning rate 0.001 and gradient descent with learning rate 0.1 . According to Definition 3.1, we set $K=4, P=4$ and $d=50$ and choose $\alpha \in(0.5,2)$, $\beta \in(1,2), \gamma \in(1,2)$ and $\sigma_{p}=1$, and generate 3,200 data examples. We consider mixture of $M=4$ experts for both MoE (linear) and MoE (nonlinear). For each expert, we set the number of neurons/filters $J=16$. We train MoEs on 1,600 data examples and visualize classification result and decision boundary on the remaining 1,600 examples. The data examples are visualized via t-SNE (Van der Maaten and Hinton, 2008). When visualizing the data points and decision boundary on the 2 d space, we increase the magnitude of random noise patch by 3 so that the positive/negative examples and decision boundaries can be better viewed.</p>
<h2>A. 2 Synthetic-data Experiments</h2>
<p>Synthetic-data experiment setup. For the experiments on synthetic data, we generate the data according to Definition 3.1 with $K=4, P=4$ and $d=50$. We consider four parameter settings:</p>
<ul>
<li>$\alpha \sim \operatorname{Uniform}(0.5,2), \beta \sim \operatorname{Uniform}(1,2), \gamma \sim \operatorname{Uniform}(0.5,3)$ and $\sigma_{p}=1$;</li>
<li>$\alpha \sim \operatorname{Uniform}(0.5,2), \beta \sim \operatorname{Uniform}(1,2), \gamma \sim \operatorname{Uniform}(0.5,3)$ and $\sigma_{p}=2$;</li>
<li>$\alpha \sim \operatorname{Uniform}(0.5,2), \beta \sim \operatorname{Uniform}(1,2), \gamma \sim \operatorname{Uniform}(0.5,2)$ and $\sigma_{p}=1$;</li>
<li>$\alpha \sim \operatorname{Uniform}(0.5,2), \beta \sim \operatorname{Uniform}(1,2), \gamma \sim \operatorname{Uniform}(0.5,2)$ and $\sigma_{p}=2$.</li>
</ul>
<p>We consider mixture of $M=8$ experts for all MoEs and $J=16$ neurons/filters for all experts. For single models, we consider $J=128$ neurons/filters. We train MoEs using Algorithm 1. Specifically,</p>
<p>Setting 1: $\alpha \in(0.5,2), \beta \in(1,2), \gamma \in(0.5,3), \sigma_{p}=1$</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test accuracy (\%)</th>
<th style="text-align: center;">Dispatch Entropy</th>
<th style="text-align: center;">Number of Filters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single (linear)</td>
<td style="text-align: center;">68.71</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Single (linear)</td>
<td style="text-align: center;">67.63</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: center;">Single (nonlinear)</td>
<td style="text-align: center;">79.48</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Single (nonlinear)</td>
<td style="text-align: center;">78.18</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: center;">MoE (linear)</td>
<td style="text-align: center;">$92.99 \pm 2.11$</td>
<td style="text-align: center;">$1.300 \pm 0.044$</td>
<td style="text-align: center;">$128\left(16^{*} 8\right)$</td>
</tr>
<tr>
<td style="text-align: center;">MoE (nonlinear)</td>
<td style="text-align: center;">$\mathbf{9 9 . 4 6} \pm \mathbf{0 . 5 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 9 8} \pm \mathbf{0 . 0 8 7}$</td>
<td style="text-align: center;">$128\left(16^{*} 8\right)$</td>
</tr>
</tbody>
</table>
<p>Setting 2: $\alpha \in(0.5,2), \beta \in(1,2), \gamma \in(0.5,3), \sigma_{p}=2$</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test accuracy (\%)</th>
<th style="text-align: center;">Dispatch Entropy</th>
<th style="text-align: center;">Number of Filters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single (linear)</td>
<td style="text-align: center;">60.59</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Single (linear)</td>
<td style="text-align: center;">63.04</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: center;">Single (nonlinear)</td>
<td style="text-align: center;">72.29</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Single (nonlinear)</td>
<td style="text-align: center;">52.09</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: center;">MoE (linear)</td>
<td style="text-align: center;">$88.48 \pm 1.96$</td>
<td style="text-align: center;">$1.294 \pm 0.036$</td>
<td style="text-align: center;">$128\left(16^{*} 8\right)$</td>
</tr>
<tr>
<td style="text-align: center;">MoE (nonlinear)</td>
<td style="text-align: center;">$\mathbf{9 8 . 0 9} \pm \mathbf{1 . 2 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 7 1} \pm \mathbf{0 . 1 0 3}$</td>
<td style="text-align: center;">$128\left(16^{*} 8\right)$</td>
</tr>
</tbody>
</table>
<p>Setting 3: $\alpha \in(0.5,2), \beta \in(1,2), \gamma \in(0.5,2), \sigma_{p}=1$</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test accuracy (\%)</th>
<th style="text-align: center;">Dispatch Entropy</th>
<th style="text-align: center;">Number of Filters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single (linear)</td>
<td style="text-align: center;">74.81</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Single (linear)</td>
<td style="text-align: center;">74.54</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: center;">Single (nonlinear)</td>
<td style="text-align: center;">72.69</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Single (nonlinear)</td>
<td style="text-align: center;">67.78</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: center;">MoE (linear)</td>
<td style="text-align: center;">$95.93 \pm 1.34$</td>
<td style="text-align: center;">$1.160 \pm 0.100$</td>
<td style="text-align: center;">$128\left(16^{*} 8\right)$</td>
</tr>
<tr>
<td style="text-align: center;">MoE (nonlinear)</td>
<td style="text-align: center;">$\mathbf{9 9 . 9 9} \pm \mathbf{0 . 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 8} \pm \mathbf{0 . 0 1 1}$</td>
<td style="text-align: center;">$128\left(16^{*} 8\right)$</td>
</tr>
</tbody>
</table>
<p>Setting 4: $\alpha \in(0.5,2), \beta \in(1,2), \gamma \in(0.5,2), \sigma_{p}=2$</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test accuracy (\%)</th>
<th style="text-align: center;">Dispatch Entropy</th>
<th style="text-align: center;">Number of Filters</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Single (linear)</td>
<td style="text-align: center;">74.63</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Single (linear)</td>
<td style="text-align: center;">72.98</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: center;">Single (nonlinear)</td>
<td style="text-align: center;">68.60</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">Single (nonlinear)</td>
<td style="text-align: center;">61.65</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: center;">MoE (linear)</td>
<td style="text-align: center;">$93.30 \pm 1.48$</td>
<td style="text-align: center;">$1.160 \pm 0.155$</td>
<td style="text-align: center;">$128\left(16^{*} 8\right)$</td>
</tr>
<tr>
<td style="text-align: center;">MoE (nonlinear)</td>
<td style="text-align: center;">$\mathbf{9 8 . 9 2} \pm \mathbf{1 . 1 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 8 9} \pm \mathbf{0 . 1 2 0}$</td>
<td style="text-align: center;">$128\left(16^{*} 8\right)$</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison between MoE (linear) and MoE (nonlinear) in our setting. We report results of top-1 gating with noise for both linear and nonlinear models. Over ten random experiments, we report the average value $\pm$ standard deviation for both test accuracy and dispatch entropy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Expert number</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">7</th>
<th style="text-align: center;">8</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Initial dispatch</td>
<td style="text-align: center;">1921</td>
<td style="text-align: center;">2032</td>
<td style="text-align: center;">1963</td>
<td style="text-align: center;">1969</td>
<td style="text-align: center;">2075</td>
<td style="text-align: center;">1980</td>
<td style="text-align: center;">2027</td>
<td style="text-align: center;">2033</td>
</tr>
<tr>
<td style="text-align: center;">Final dispatch</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3979</td>
<td style="text-align: center;">4009</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3971</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4041</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3971</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4009</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4041</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3979</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 4: Dispatch details of MoE (nonlinear) with test accuracy $100 \%$.
we train the experts by normalized gradient descent with learning rate 0.001 and the gating network by gradient descent with learning rate 0.1 . We train single linear/nonlinear models by Adam (?) to achieve the best performance, with learning rate 0.01 and weight decay $5 \mathrm{e}-4$ for single nonlinear model and learning rate 0.003 and weight decay $5 e-4$ for single linear model.
Synthetic-data experiment results. In Table 3, we present the empirical results of single linear CNN, single nonlinear CNN, linear MoE, and nonlinear MoE under settings 3 and 4 , where $\alpha$ and $\gamma$ follow the same distribution as we assumed in theoretical analysis. Furthermore, we report the total number of filters for both single CNNs and a mixture of CNNs, where the filter size (equal to 50) is the same for all single models and experts. For linear and nonlinear MoE, there are 16 filters for each of the 8 experts, and therefore 128 filters in total. Note that in the synthetic-data experiment in the main paper, we let the number of filters of single models be the same as MoEs (128). Here, we additionally report the performances of single models with 512 filters, and see if increasing the model size of single models can beat MoE. From Table 3, we observe that: 1. single models perform poorly in all settings; 2. linear MoEs do not perform as well as nonlinear MoEs. Specifically, the final dispatch entropy of nonlinear MoEs is nearly zero while the dispatch entropy of linear MoEs is consistently larger under settings 1-4. This indicates that nonlinear MoEs successfully uncover the underlying cluster structure while linear MoEs fail to do so. In addition, we can see that even larger single models cannot beat linear MoEs or nonlinear MoEs. This is consistent with Theorem 4.1, where a single model fails under such data distribution regardless of its model size. Notably, by comparing the results in Table 1 and Table 3, we can see that a single nonlinear model suffers from overfitting as we increase the number of filters.
Router dispatch examples. We demonstrate specific examples of router dispatch for MoE (nonlinear) and MoE (linear). The examples of initial and final router dispatch for MoE (nonlinear) are shown in Table 4 and Table 5. Under the dispatch for nonlinear MoE, each expert is given either no data or data that comes from one cluster only. The entropy of such dispatch is thus 0 . The test accuracy of MoE trained under such a dispatch is either $100 \%$ or very close to $100 \%$, as the expert can be easily trained on the data from one cluster only. An example of the final dispatch for MoE (linear) is shown in Table 6, where clusters are not well separated and an expert gets data from different clusters. The test accuracy under such dispatch is lower ( $90.61 \%$ ).
MoE during training. We further provide figures that illustrate the growth of the inner products between expert/router weights and feature/center signals during training. Specifically, since each expert has multiple neurons, we plot the max absolute value of the inner product over the neurons of each expert. In Figure 4, we demonstrate the training process of MoE (nonlinear), and in Figure 5, we demonstrate the training process of MoE (linear). The data is the same as setting 1 in Table 1,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Expert number</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">7</th>
<th style="text-align: center;">8</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Initial dispatch</td>
<td style="text-align: center;">1978</td>
<td style="text-align: center;">2028</td>
<td style="text-align: center;">2018</td>
<td style="text-align: center;">1968</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">2046</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">1962</td>
</tr>
<tr>
<td style="text-align: center;">Final dispatch</td>
<td style="text-align: center;">3987</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3975</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1308</td>
<td style="text-align: center;">4009</td>
<td style="text-align: center;">2711</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3971</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4005</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 3</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1304</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2711</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 4</td>
<td style="text-align: center;">3979</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 5: Dispatch details of MoE (nonlinear) with test accuracy 99.95\%.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Expert number</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">7</th>
<th style="text-align: center;">8</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Initial dispatch</td>
<td style="text-align: center;">1969</td>
<td style="text-align: center;">2037</td>
<td style="text-align: center;">1983</td>
<td style="text-align: center;">2007</td>
<td style="text-align: center;">1949</td>
<td style="text-align: center;">1905</td>
<td style="text-align: center;">2053</td>
<td style="text-align: center;">2097</td>
</tr>
<tr>
<td style="text-align: center;">Final dispatch</td>
<td style="text-align: center;">136</td>
<td style="text-align: center;">2708</td>
<td style="text-align: center;">6969</td>
<td style="text-align: center;">5311</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">758</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">630</td>
<td style="text-align: center;">1629</td>
<td style="text-align: center;">1298</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">296</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 2</td>
<td style="text-align: center;">136</td>
<td style="text-align: center;">1107</td>
<td style="text-align: center;">1884</td>
<td style="text-align: center;">651</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">231</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">594</td>
<td style="text-align: center;">1976</td>
<td style="text-align: center;">1471</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Cluster 4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">377</td>
<td style="text-align: center;">1480</td>
<td style="text-align: center;">1891</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">231</td>
</tr>
</tbody>
</table>
<p>Table 6: Dispatch details of MoE (linear) with test accuracy $90.61 \%$.
with $\alpha \in(0.5,2), \beta \in(1,2), \gamma \in(0.5,3)$ and $\sigma_{p}=1$. We can observe that, in the top left subfigure of Figure 4 for MoE (nonlinear), the max inner products between expert weight and feature signals exhibit a property that each expert picks up one feature signal quickly. Similarly, as shown in the bottom right sub-figure, the router picks up the corresponding center signal. Meanwhile, the nonlinear experts almost do not learn center signals and the magnitude of the inner products between router weight and feature signals remain small. However, for MoE (linear), as shown in the top two sub-figures of Figure 5, an expert does not learn a specific feature signal, but instead learns multiple feature and center signals. Moreover, as demonstrated in the bottom sub-figures of Figure 5, the magnitude of the inner products between router weight and feature signals can be even larger than the inner products between router weight and center signals.
Verification of Theorem 4.1. In Table 7, we provide the performances of single models with different activation functions under setting 3 , where $\alpha, \gamma \in(1,2)$ follow the same distribution. In Table 8, we further report the performances of single models with different activation functions under setting 1 and setting 2 . Empirically, even when $\alpha$ and $\gamma$ do not share the same distribution, single models still fail. Note that, for Tables 7 and 8 , the numbers of filters for single models are 128 .
Load balancing loss. In Table 9, we present the results of linear MoE with load balancing loss and directly compare it with nonlinear MoE without load balancing loss. Load balancing loss guarantees that the experts receive similar amount of data and prevents MoE from activating only one or few experts. However, on the data distribution that we study, load balancing loss is not the key to the success of MoE: the single experts cannot perform well on the entire data distribution and must diverge to learn different labeling functions with respect to each cluster.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Mixture of nonlinear experts. Growth of inner product between expert/router weight and center/feature vector.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Activation</th>
<th style="text-align: center;">Optimal Accuracy (\%)</th>
<th style="text-align: center;">Test Accuracy (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">$87.50 \%$</td>
<td style="text-align: center;">$74.81 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Cubic</td>
<td style="text-align: center;">$87.50 \%$</td>
<td style="text-align: center;">$72.69 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Relu</td>
<td style="text-align: center;">$87.50 \%$</td>
<td style="text-align: center;">$73.45 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Celu</td>
<td style="text-align: center;">$87.50 \%$</td>
<td style="text-align: center;">$76.91 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Gelu</td>
<td style="text-align: center;">$87.50 \%$</td>
<td style="text-align: center;">$74.01 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Tanh</td>
<td style="text-align: center;">$87.50 \%$</td>
<td style="text-align: center;">$74.76 \%$</td>
</tr>
</tbody>
</table>
<p>Table 7: Verification of Theorem 4.1 (single expert performs poorly). Test accuracy of single linear/nonlinear models with different activation functions. Data is generated according to Definition 3.1 with $\alpha, \gamma \in(1,2), \beta \in(1,2)$ and $\sigma_{p}=1$.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Mixture of linear experts. Growth of inner product between expert/router weight and center/feature vector.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Activation</th>
<th style="text-align: center;">Setting 1</th>
<th style="text-align: center;">Setting 2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">$68.71 \%$</td>
<td style="text-align: center;">$60.59 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Cubic</td>
<td style="text-align: center;">$79.48 \%$</td>
<td style="text-align: center;">$72.29 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Relu</td>
<td style="text-align: center;">$72.28 \%$</td>
<td style="text-align: center;">$80.12 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Celu</td>
<td style="text-align: center;">$81.75 \%$</td>
<td style="text-align: center;">$78.99 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Gelu</td>
<td style="text-align: center;">$79.04 \%$</td>
<td style="text-align: center;">$82.01 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Tanh</td>
<td style="text-align: center;">$81.72 \%$</td>
<td style="text-align: center;">$81.03 \%$</td>
</tr>
</tbody>
</table>
<p>Table 8: Single expert performs poorly (setting 1\&amp;2). Test accuracy of single linear/nonlinear models with different activation functions. Data is generated according to Definition 3.1 with $\alpha \in(0.5,2), \beta \in(1,2), \gamma \in(0.5,3), \sigma_{p}=1$ for setting 1. And we have $\alpha \in(0.5,2)$, $\beta \in(1,2), \gamma \in(0.5,3), \sigma_{p}=1$ for setting 2 .</p>
<table>
<thead>
<tr>
<th></th>
<th>Linear MoE with Load Balancing</th>
<th>Nonlinear MoE without Load Balancing</th>
</tr>
</thead>
<tbody>
<tr>
<td>Setting 1</td>
<td>$93.81 \pm 1.02$</td>
<td>$\mathbf{9 9 . 4 6} \pm \mathbf{0 . 5 5}$</td>
</tr>
<tr>
<td>Setting 2</td>
<td>$89.20 \pm 2.20$</td>
<td>$\mathbf{9 8 . 0 9} \pm \mathbf{1 . 2 7}$</td>
</tr>
<tr>
<td>Setting 3</td>
<td>$95.12 \pm 0.58$</td>
<td>$\mathbf{9 9 . 9 9} \pm \mathbf{0 . 0 2}$</td>
</tr>
<tr>
<td>Setting 4</td>
<td>$92.50 \pm 1.55$</td>
<td>$\mathbf{9 8 . 9 2} \pm \mathbf{1 . 1 8}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Load balancing loss. We report the results for linear MoE with load balancing loss and compare them with our previous results on nonlinear MoE without load balancing loss. Over ten random experiments, we report the average test accuracy (\%) $\pm$ standard deviation. Setting 1-4 follows the data distribution introduced above.</p>
<h1>A. 3 Experiments on Image Data</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Examples of the CIFAR-10-Rotate dataset. Both the original image and the rotated image are processed in the same way, where we crop the image to $(24,24)$, resize to $(32,32)$ and apply random Gaussian blur.</p>
<p>Datasets. We consider CIFAR-10 (Krizhevsky, 2009) with the 10-class classification task, which contains 50,000 training examples and 10,000 testing examples. For CIFAR-10-Rotate, we design a binary classification task by copying and rotating all images by 30 degree and let the model predict if an image is rotated. In Figure 6, we demonstrate the positive and negative examples of CIFAR-10-Rotate. Specifically, we crop the rotated images to $(24,24)$, and resize to $(32,32)$ for model architectures that are designed on image size $(32,32)$. And we further apply random Gaussian noise to all images to avoid the models taking advantage of image resolutions.
Models. For the simple CNN model, we consider CNN with 2 convolutional layers, both with kernel size 3 and ReLU activation followed by max pooling with size 2 and a fully connected layer. The number of filters of each convolutional layer is respectively $64,128$.
CIFAR-10 Setup. For real-data experiments on CIFAR-10, we apply the commonly used transforms on CIFAR-10 before each forward pass: random horizontal flips and random crops (padding the images on all sides with 4 pixels and randomly cropping to $(32,32)$ ). And as conventionally,</p>
<p>we normalize the data by channel. We train the single CNN model with SGD of learning rate 0.01, momentum 0.9 and weight decay $5 \mathrm{e}-4$. And we train single MobileNetV2 and single ResNet18 with SGD of learning rate 0.1 , momentum 0.9 and weight decay $5 \mathrm{e}-4$ to achieve the best performances. We train MoEs according to Algorithm 1, with normalized gradient descent on the experts and SGD on the gating networks. Specifically, for MoE (ResNet18) and MoE (MobileNetV2), we use normalized gradient descent of learning rate 0.1 and SGD of learning rate $1 \mathrm{e}-4$, both with momentum 0.9 and weight decay of $5 \mathrm{e}-4$. For MoE (CNN), we use normalized gradient descent of learning rate 0.01 and SGD of learning rate $1 \mathrm{e}-4$, both with momentum 0.9 and weight decay of $5 \mathrm{e}-4$. We consider top-1 gating with noise and load balancing loss for MoE on both datasets, where the multiplicative coefficient of load balancing loss is set at $1 \mathrm{e}-3$. All models are trained for 200 epochs to achieve convergence.
CIFAR-10-Rotate Setup. For experiments on CIFAR10-Rotate, the data is normalized by channel as the same as in CIFAR-10 before each forward pass. We train the single CNN, single MobileNetV2 and single ResNet18 by SGD with learning rate 0.01 , momentum 0.9 and weight decay $5 \mathrm{e}-4$ to achieve the best performances. And we train MoEs by Algorithm 1 with normalized gradient descent learning rate 0.01 on the experts and with SGD of learning rate $1 \mathrm{e}-4$ on the gating networks, both with momentum 0.9 and weight decay of $5 \mathrm{e}-4$. We consider top-1 gating with noise and load balancing loss for MoE on both datasets, where the multiplicative coefficient for load balancing loss is set at $1 \mathrm{e}-3$. All models are trained for 50 epochs to achieve convergence.
Visualization. In Figure 7, we visualize the latent embedding learned by MoEs (ResNet18) for the 10-class classification task in CIFAR-10 as well as the binary classification task in CIFAR-10Rotate. We visualize the data with the same label $y$ to see if cluster structures exist within each class. For CIFAR-10, we choose $y=1$ ("car"), and plot the latent embedding of data with $y=1$ using t-SNE on the left subfigure, which does not show an salient cluster structure. For CIFAR-10-Rotate, we choose $y=1$ ("rotated") and visualize the data with $y=1$ in the middle subfigure. Here, we can observe a clear clustering structure even though the class signal is not provided during training. We take a step further to investigate what is in each cluster in the right subfigure. We can observe that most of the examples in the "frog" class fall into one cluster, while examples of "ship" class mostly fall into the other cluster.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Visualization of the latent embedding on CIFAR-10 and CIFAR-10-Rotate with fixed label $y$. The left figure denotes the visualization of CIFAR-10 when label $y$ is fixed to be 1 (car). The central figure represents the visualization of CIFAR-10-Rotate when label $y$ is fixed to be 1 (rotated). On the right figure, red denotes that the data is from the ship class, and blue denotes that the data is from the frog class.</p>
<table>
<thead>
<tr>
<th></th>
<th>Single</th>
<th>MoE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy</td>
<td>$74.13 \%$</td>
<td>$76.22 \%$</td>
</tr>
</tbody>
</table>
<p>Table 10: The test accuracy of the single classifier vs. MoE classifier.</p>
<table>
<thead>
<tr>
<th></th>
<th>Expert 1</th>
<th>Expert 2</th>
<th>Expert 3</th>
<th>Expert 4</th>
</tr>
</thead>
<tbody>
<tr>
<td>English</td>
<td>1,374</td>
<td>3,745</td>
<td>2,999</td>
<td>$\mathbf{3 1 , 8 8 2}$</td>
</tr>
<tr>
<td>French</td>
<td>$\mathbf{2 3 , 4 7 0}$</td>
<td>3,335</td>
<td>$\mathbf{1 3 , 1 8 2}$</td>
<td>13</td>
</tr>
<tr>
<td>Russian</td>
<td>833</td>
<td>$\mathbf{9 , 4 0 5}$</td>
<td>7,723</td>
<td>39</td>
</tr>
</tbody>
</table>
<p>Table 11: The final router dispatch details with regard to the linguistic source of the test data.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The distribution of text embedding of the multilingual sentiment analysis dataset. The embedding is generated by the pre-trained BERT multilingual base model and visualized on 2d space using t-SNE. Each color denotes a linguistic source, including English, French, and Russian.</p>
<h1>A. 4 Experiments on Language Data</h1>
<p>Here we provide a simple example of how MoE would work for multilingual tasks. We gather multilingual sentiment analysis data from the source of English (Sentiment140 (Go et al., 2009)) which is randomly sub-sampled to 200, 000 examples, Russian (RuReviews (Smetanin and Komarov, 2019)) which contains 90, 000 examples, and French (Blard, 2020) which contains 200, 000 examples. We randomly split the dataset into $80 \%$ training data and $20 \%$ test data. We use a pre-trained BERT multilingual base model (Devlin et al., 2018) to generate text embedding for each text and train 1-layer neural network with cubic activation as the single model. For MoE, we still let $M=4$ with each expert sharing the same architecture as the single model. In Figure 8, we show the visualization of the text embeddings in the 2 d space via t-SNE, where each color denotes a linguistic source, with $\cdot$ representing a positive example and $\times$ representing a negative example. Data from different linguistic sources naturally form different clusters. And within each cluster, positive and negative data exist.</p>
<p>In Table 10, we demonstrate the test accuracy of a single classifier and MoE on the multilingual sentiment analysis dataset. And in Table 11, we show the final router dispatch details of MoE to</p>
<p>each expert with regard to the linguistic source of the text. Notably, MoE learned to distribute examples largely according to the original language.</p>
<h1>B Proof of Theorem 4.1</h1>
<p>Because we are using CNNs as experts, different ordering of the patches won't affect the value of $F(\mathbf{x})$. So for $(\mathbf{x}, y)$ drawn from $\mathcal{D}$ in Definition 3.1, we can assume that the first patch $\mathbf{x}^{(1)}$ is feature signal, the second patch $\mathbf{x}^{(2)}$ is cluster-center signal, the third patch $\mathbf{x}^{(3)}$ is feature noise. The other patches $\mathbf{x}^{(p)}, p \geq 4$ are random noises. Therefore, we can rewrite $\mathbf{x}=\left[\alpha y \mathbf{v}<em k="k">{k}, \beta \mathbf{c}</em>}, \gamma \epsilon \mathbf{v<em 4="4">{k^{\prime}}, \boldsymbol{\xi}\right]$, where $\boldsymbol{\xi}=\left[\boldsymbol{\xi}</em>$.}, \ldots, \boldsymbol{\xi}_{P}\right]$ is a Gaussian matrix of size $\mathbb{R}^{d \times(P-3)</p>
<p>Proof of Theorem 4.1. Conditioned on the event that $y=-\epsilon$, points $\left(\left[\alpha y \mathbf{v}<em k="k">{k}, \beta \mathbf{c}</em>},-\gamma y \mathbf{v<em k="k">{k^{\prime}}, \boldsymbol{\xi}\right], y\right)$, $\left(\left[-\alpha y \mathbf{v}</em>}, \beta \mathbf{c<em k_prime="k^{\prime">{k}, \gamma y \mathbf{v}</em>}}, \boldsymbol{\xi}\right],-y\right),\left(\left[\gamma y \mathbf{v<em k_prime="k^{\prime">{k^{\prime}}, \beta \mathbf{c}</em>}},-\alpha y \mathbf{v<em k_prime="k^{\prime">{k}, \boldsymbol{\xi}\right], y\right),\left(\left[-\gamma y \mathbf{v}</em>}}, \beta \mathbf{c<em k="k">{k^{\prime}}, \alpha y \mathbf{v}</em>\right],-y\right)$ follow the same distribution because $\gamma$ and $\alpha$ follow the same distribution, and $y$ and $-y$ follow the same distribution. Therefore, we have}, \boldsymbol{\xi</p>
<p>$$
\begin{aligned}
&amp; 4 \mathbb{P}(y F(\mathbf{x}) \leq 0 \mid \epsilon=-y) \
&amp; =\mathbb{E}[\underbrace{\mathbb{1}\left(y F\left(\left[\alpha y \mathbf{v}<em k="k">{k}, \beta \mathbf{c}</em>},-\gamma y \mathbf{v<em I__1="I_{1">{k^{\prime}}, \boldsymbol{\xi}\right]\right) \leq 0\right)}</em>}}+\underbrace{\mathbb{1}\left(-y F\left(\left[-\alpha y \mathbf{v<em k="k">{k}, \beta \mathbf{c}</em>}, \gamma y \mathbf{v<em I__2="I_{2">{k^{\prime}}, \boldsymbol{\xi}\right]\right) \leq 0\right)}</em> \
&amp; \quad+\underbrace{\mathbb{1}\left(y F\left(\left[\gamma y \mathbf{v}}<em k_prime="k^{\prime">{k^{\prime}}, \beta \mathbf{c}</em>}},-\alpha y \mathbf{v<em I__3="I_{3">{k}, \boldsymbol{\xi}\right]\right) \leq 0\right)}</em>}}+\underbrace{\mathbb{1}\left(-y F\left(\left[-\gamma y \mathbf{v<em k_prime="k^{\prime">{k^{\prime}}, \beta \mathbf{c}</em>}}, \alpha y \mathbf{v<em I__4="I_{4">{k}, \boldsymbol{\xi}\right]\right) \leq 0\right)}</em> .
\end{aligned}
$$}</p>
<p>It is easy to verify the following fact</p>
<p>$$
\begin{aligned}
&amp; \left(y F\left(\left[\alpha y \mathbf{v}<em k="k">{k}, \beta \mathbf{c}</em>},-\gamma y \mathbf{v<em k="k">{k^{\prime}}, \boldsymbol{\xi}\right]\right)\right)+\left(-y F\left(\left[-\alpha y \mathbf{v}</em>}, \beta \mathbf{c<em k_prime="k^{\prime">{k}, \gamma y \mathbf{v}</em>\right]\right)\right) \
&amp; \quad+\left(y F\left(\left[\gamma y \mathbf{v}}}, \boldsymbol{\xi<em k_prime="k^{\prime">{k^{\prime}}, \beta \mathbf{c}</em>}},-\alpha y \mathbf{v<em k_prime="k^{\prime">{k}, \boldsymbol{\xi}\right]\right)\right)+\left(-y F\left(\left[-\gamma y \mathbf{v}</em>}}, \beta \mathbf{c<em k="k">{k^{\prime}}, \alpha y \mathbf{v}</em>\right]\right)\right) \
&amp; =\left(y f\left(\alpha y \mathbf{v}}, \boldsymbol{\xi<em k="k">{k}\right)+y f\left(\beta \mathbf{c}</em>}\right)+y f\left(-\gamma y \mathbf{v<em p="4">{k^{\prime}}\right)+\sum</em>}^{P} y f\left(\boldsymbol{\xi<em k="k">{p}\right)\right) \
&amp; \quad+\left(-y f\left(-\alpha y \mathbf{v}</em>}\right)-y f\left(\beta \mathbf{c<em k_prime="k^{\prime">{k}\right)-y f\left(\gamma y \mathbf{v}</em>}}\right)-\sum_{p=4}^{P} y f\left(\boldsymbol{\xi<em k_prime="k^{\prime">{p}\right)\right) \
&amp; \quad+\left(y f\left(\gamma y \mathbf{v}</em>}}\right)+y f\left(\beta \mathbf{c<em k="k">{k^{\prime}}\right)+y f\left(-\alpha y \mathbf{v}</em>}\right)+\sum_{p=4}^{P} y f\left(\boldsymbol{\xi<em k_prime="k^{\prime">{p}\right)\right) \
&amp; \quad+\left(-y f\left(-\gamma y \mathbf{v}</em>}}\right)-y f\left(\beta \mathbf{c<em k="k">{k^{\prime}}\right)-y f\left(\alpha y \mathbf{v}</em>\right)\right) \
&amp; =0
\end{aligned}
$$}\right)-\sum_{p=4}^{P} y f\left(\boldsymbol{\xi}_{p</p>
<p>By pigeonhole principle, at least one of $I_{1}, I_{2}, I_{3}, I_{4}$ is non-zero. This further implies that $4 \mathbb{P}(y F(\mathbf{x}) \leq$ $0 \mid \epsilon=-y) \geq 1$. Applying $\mathbb{P}(\epsilon=-y)=1 / 2$, we have that</p>
<p>$$
\mathbb{P}(y F(\mathbf{x}) \leq 0) \geq \mathbb{P}(y F(\mathbf{x}) \leq 0) \mid \epsilon=-y) \mathbb{P}(\epsilon=-y) \geq 1 / 8
$$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: chenzx19@cs.ucla.edu
${ }^{\dagger}$ Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: yihedeng@cs.ucla.edu
${ }^{\ddagger}$ Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: : ywu@cs.ucla.edu
${ }^{\S}$ Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: qgu@cs.ucla.edu
${ }^{\text {t }}$ Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA; email: yuanzhil@andrew.cmu.edu&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
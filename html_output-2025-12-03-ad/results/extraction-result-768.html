<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-768 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-768</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-768</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-233296398</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2104.09469v1.pdf" target="_blank">Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior</a></p>
                <p><strong>Paper Abstract:</strong> As more machine learning agents interact with humans, it is increasingly a prospect that an agent trained to perform a task optimally, using only a measure of task performance as feedback, can violate societal norms for acceptable behavior or cause harm. Value alignment is a property of intelligent agents wherein they solely pursue non-harmful behaviors or human-beneficial goals. We introduce an approach to value-aligned reinforcement learning, in which we train an agent with two reward signals: a standard task performance reward, plus a normative behavior reward. The normative behavior reward is derived from a value-aligned prior model previously shown to classify text as normative or non-normative. We show how variations on a policy shaping technique can balance these two sources of reward and produce policies that are both effective and perceived as being more normative. We test our value-alignment technique on three interactive text-based worlds; each world is designed specifically to challenge agents with a task as well as provide opportunities to deviate from the task to engage in normative and/or altruistic behavior.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e768.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e768.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Advantage Actor-Critic (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard Advantage Actor-Critic deep RL agent used as a baseline in TextWorld environments; selects actions from admissible actions via an actor network and estimates value via a critic, trained on environmental reward only.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>A2C (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor-Critic architecture with separate actor and critic networks. Given an observation (room description, inventory, facts, reactive text) the actor outputs a distribution over admissible actions; an action is sampled and executed. The critic provides value estimates used to compute policy and value losses. No external normative classifier is used; learning signal is environmental reward R_env. The implementation allowed access to TextWorld's admissible-action list rather than token-level generation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (Playground World, Superhero World, Clerk World)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based, partially observable environments built on TextWorld. Observations are local textual descriptions (room, items, facts, reactive text) and admissible actions are provided; environments include room graphs and social entities. Partial observability arises from local text observations and limited immediate information about unseen rooms/objects.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>None (baseline uses only environment observations and environmental reward).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Implicit state representation via the actor-critic network inputs (current observation + any provided state fields). No explicit belief-state data structure (no knowledge graph, no explicit memory beyond whatever the network architecture may encode); agents do not use external memory/graph modules.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Belief is not explicitly represented or updated; the agent's internal state is determined implicitly by the neural network's input processing and training updates from observed transitions and rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (model-free RL) via Advantage Actor-Critic; no explicit search or model-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation emerges from the learned policy selecting locomotion admissible actions on the TextWorld room graph; no explicit pathfinding algorithm (e.g., A*) is used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Baseline A2C (no normative tool) achieved the highest environmental reward in Clerk World and converged to high/max environmental reward in Playground and Superhero environments in the experiments (quantified plots in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Baseline A2C, which uses only environmental reward and no normative classifier, typically maximizes environmental reward but performs few or no normative/altruistic actions; it serves as the comparison for agents that incorporate an external normative classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e768.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e768.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GG-pos</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GG-pos (reward-shaped with GG normative prior)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A2C agent that incorporates an external normative classifier by multiplicatively scaling environmental reward by the classifier's positive (normative) logit, thereby rewarding actions judged more normative.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GG-pos</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A standard A2C agent whose per-step reward R_t+1 is modified by the GG normative prior's positive logit: R_t+1 = R_env_t+1 * L_norm. The GG model (BERT/XLNet-based classifier fine-tuned on Goofus & Gallant) receives the crowdsourced natural language elaboration for the chosen action and returns two unnormalized logits (L_norm, L_notnorm); GG-pos uses only L_norm to scale environment reward. Training and updates otherwise follow standard A2C.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (Playground World, Superhero World, Clerk World)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text environments built on TextWorld; agents receive local text observations and lists of admissible actions with crowdsourced natural-language elaborations which feed the GG classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>GG normative prior classifier (language-model classifier trained on Goofus & Gallant using BERT/XLNet token embeddings). Also uses crowdsourced action elaborations (natural language) provided by the environment to feed GG.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Unnormalized logits (two values): L_norm and L_notnorm (numeric scalar scores representing classifier confidence / logit), which are used to scale rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Implicit belief encoded in the actor-critic network's processing of the current observation (text fields provided) and the network parameters updated by reward-shaped learning; no explicit, persistent belief-state data structure is described.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>No explicit belief update with tool outputs; GG outputs affect the scalar reward signal used to compute gradient updates for the networks, thereby biasing learned policy indirectly across training steps rather than modifying an explicit belief representation at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy (model-free RL) with reward shaping from GG; no search-based or model-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Policy-driven selection of admissible locomotion actions on the TextWorld room graph; no explicit path-finding algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>With GG-pos scaling, agents in Playground and Superhero converge to high environmental rewards while increasing normative actions; in Clerk World GG-pos performed poorly on normative actions (learned to not use altruistic actions) and did not match GG-shaped's normative behavior. (See paper plots: GG-pos achieves normative action ratio ~40% in Playground; near-exclusive normative trajectories in Superhero for some runs.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Compared to baseline A2C (no GG), GG-pos sometimes achieves similar environmental reward (Playground, Superhero) while producing more normative actions; in Clerk World baseline achieved higher environmental reward than GG-pos.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using GG outputs as reward shaping (GG-pos) biases the learned policy toward actions the classifier rates as normative without changing the agent's runtime action-ranking procedure; this increases normative behavior in some environments (Playground, Superhero) but is insufficient in scenarios (Clerk World) where normative actions require explicit trade-offs that reduce environmental reward.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e768.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e768.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GG-mix</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GG-mix (reward-shaped with GG difference logits)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A2C agent that scales environmental reward by the difference between GG classifier logits (L_norm - L_notnorm), allowing the classifier's relative preference for normativity to amplify or invert environmental reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GG-mix</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A2C agent where per-step reward R_t+1 = R_env_t+1 * (L_norm - L_notnorm). The GG classifier processes the chosen action's natural-language elaboration and returns two logits; GG-mix uses their difference, which can reduce, amplify, or invert the environmental reward (if L_notnorm > L_norm) thereby strongly penalizing actions judged non-normative. Training otherwise follows A2C procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (Playground World, Superhero World, Clerk World)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable TextWorld-based social/text scenarios with room graphs and local textual observations; admissible actions have crowdsourced NL elaborations used by GG.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>GG normative prior classifier (BERT/XLNet-based) applied to crowdsourced action elaborations.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Numeric logits (L_norm, L_notnorm); GG-mix uses their numeric difference.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Implicit internal state via actor-critic networks; no explicit belief-state memory or structured representation is described.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>GG outputs alter the scalar reward used to update network weights (reward shaping), thereby influencing the learned policy over training but not updating any explicit belief-state representation at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Model-free learned policy (A2C) with reward shaping; no search-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Learned policy selects navigation (go/locomotion) actions among admissible actions; no explicit path-planning algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>GG-mix agents in Superhero and Playground learned normative solutions (in Superhero GG-mix and GG-pos almost exclusively followed normative trajectories). Behavior depended on phrasing of action elaborations; misclassifications could flip behavior. In Clerk World GG-mix did not adopt many altruistic actions and achieved lower normative rates than GG-shaped.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Compared to baseline A2C, GG-mix increases normative action selection in some environments (Playground, Superhero) while sometimes maintaining environmental reward; in Clerk World baseline outperformed GG-mix on environmental reward.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using the classifier's relative logit difference to scale reward can strongly bias learning toward or against actions, but is sensitive to classifier misclassification and to reward scaling; it influences policy indirectly through training rather than modifying runtime action-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e768.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e768.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GG-shaped</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GG-shaped (policy-shaping with GG normative prior)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A2C agent that applies the GG classifier output directly to the actor's action logits at every timestep, re-ranking actions (policy shaping) so actions judged normative are more likely to be sampled at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GG-shaped</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Variant of A2C implementing policy shaping: sample actor logits [L_a1,...,L_an] for admissible actions, then for each action multiply its actor logit by (L_norm - L_notnorm) as produced by GG on that action's crowdsourced NLP elaboration, producing a reranked distribution from which the action is sampled. This uses GG as an intrinsic, per-action biasing signal at action selection time rather than modifying the scalar reward used by the loss (the reward used in loss computation remains environmental reward).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (Playground World, Superhero World, Clerk World)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable TextWorld social/text environments. Agents receive local textual observations and lists of admissible actions with NL elaborations; environments include room graphs and scenarios where normative vs non-normative choices trade off with task efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>GG normative prior classifier (BERT/XLNet-based) applied at runtime to each admissible action's natural-language elaboration; crowdsourced elaborations are provided by environment.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Per-action numeric logits (L_norm, L_notnorm) used to compute a scalar (L_norm - L_notnorm) that multiplies the actor's action logits, yielding reranked action probabilities (numeric scores).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Implicit belief representation via the actor-critic network; GG-shaped does not maintain an explicit belief-state data structure. However, policy shaping changes runtime action selection by incorporating classifier-derived scores into the sampled policy distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>No explicit belief update mechanism with tool outputs; GG outputs are used immediately to reweight action logits for sampling at each timestep. Learning updates still use environmental reward, so over training the agent's parameters adapt to the combined effect of reranked sampling and environmental feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy with runtime policy-shaping (action reranking). Not model-based planning or search. Policy shaping biases the agent's action-selection distribution at each decision point using the external classifier's per-action scores.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation decisions are produced by the reranked policy sampling over admissible locomotion actions; no explicit graph search or shortest-path algorithm is used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>GG-shaped often increases normative behavior markedly (2â€“6x more normative actions in Clerk World than other shaped approaches) but at the expense of environmental reward in scenarios where normative actions reduce task efficiency (e.g., GG-shaped achieved ~40% of maximum observed environmental score in Clerk World while baseline achieved highest task reward). In Playground and Superhero environments GG-shaped achieves increased normative actions though sometimes less consistently than GG-pos/GG-mix due to GG classifier misclassifications of action elaborations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Compared to baseline A2C, GG-shaped sacrifices environmental reward in some environments to achieve substantially higher normative/action alignment; baseline A2C attains higher task reward but far fewer normative actions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Policy shaping with an external classifier (GG-shaped) is the most effective method in difficult trade-off environments for producing normative behavior at runtime because it biases action selection directly, but it trades off environmental reward and is sensitive to classifier errors in action phrasing; GG-shaped modifies decision-time probabilities rather than the training reward signal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning Norms from Stories: A Prior for Value Aligned Agents <em>(Rating: 2)</em></li>
                <li>TextWorld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Policy shaping: Integrating human feedback with reinforcement learning <em>(Rating: 2)</em></li>
                <li>Policy Shaping with Human Teachers <em>(Rating: 2)</em></li>
                <li>Deep Reinforcement Learning with a Natural Language Action Space <em>(Rating: 1)</em></li>
                <li>Graph Constrained Reinforcement Learning for Natural Language Action Spaces <em>(Rating: 1)</em></li>
                <li>Comprehensible Context-driven Text Game Playing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-768",
    "paper_id": "paper-233296398",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "A2C",
            "name_full": "Advantage Actor-Critic (baseline)",
            "brief_description": "A standard Advantage Actor-Critic deep RL agent used as a baseline in TextWorld environments; selects actions from admissible actions via an actor network and estimates value via a critic, trained on environmental reward only.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "A2C (baseline)",
            "agent_description": "Actor-Critic architecture with separate actor and critic networks. Given an observation (room description, inventory, facts, reactive text) the actor outputs a distribution over admissible actions; an action is sampled and executed. The critic provides value estimates used to compute policy and value losses. No external normative classifier is used; learning signal is environmental reward R_env. The implementation allowed access to TextWorld's admissible-action list rather than token-level generation.",
            "environment_name": "TextWorld (Playground World, Superhero World, Clerk World)",
            "environment_description": "Text-based, partially observable environments built on TextWorld. Observations are local textual descriptions (room, items, facts, reactive text) and admissible actions are provided; environments include room graphs and social entities. Partial observability arises from local text observations and limited immediate information about unseen rooms/objects.",
            "is_partially_observable": true,
            "external_tools_used": "None (baseline uses only environment observations and environmental reward).",
            "tool_output_types": null,
            "belief_state_mechanism": "Implicit state representation via the actor-critic network inputs (current observation + any provided state fields). No explicit belief-state data structure (no knowledge graph, no explicit memory beyond whatever the network architecture may encode); agents do not use external memory/graph modules.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "Belief is not explicitly represented or updated; the agent's internal state is determined implicitly by the neural network's input processing and training updates from observed transitions and rewards.",
            "planning_approach": "Learned policy (model-free RL) via Advantage Actor-Critic; no explicit search or model-based planning.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation emerges from the learned policy selecting locomotion admissible actions on the TextWorld room graph; no explicit pathfinding algorithm (e.g., A*) is used.",
            "performance_with_tools": null,
            "performance_without_tools": "Baseline A2C (no normative tool) achieved the highest environmental reward in Clerk World and converged to high/max environmental reward in Playground and Superhero environments in the experiments (quantified plots in paper).",
            "has_tool_ablation": true,
            "key_findings": "Baseline A2C, which uses only environmental reward and no normative classifier, typically maximizes environmental reward but performs few or no normative/altruistic actions; it serves as the comparison for agents that incorporate an external normative classifier.",
            "uuid": "e768.0",
            "source_info": {
                "paper_title": "Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "GG-pos",
            "name_full": "GG-pos (reward-shaped with GG normative prior)",
            "brief_description": "A2C agent that incorporates an external normative classifier by multiplicatively scaling environmental reward by the classifier's positive (normative) logit, thereby rewarding actions judged more normative.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GG-pos",
            "agent_description": "A standard A2C agent whose per-step reward R_t+1 is modified by the GG normative prior's positive logit: R_t+1 = R_env_t+1 * L_norm. The GG model (BERT/XLNet-based classifier fine-tuned on Goofus & Gallant) receives the crowdsourced natural language elaboration for the chosen action and returns two unnormalized logits (L_norm, L_notnorm); GG-pos uses only L_norm to scale environment reward. Training and updates otherwise follow standard A2C.",
            "environment_name": "TextWorld (Playground World, Superhero World, Clerk World)",
            "environment_description": "Partially observable text environments built on TextWorld; agents receive local text observations and lists of admissible actions with crowdsourced natural-language elaborations which feed the GG classifier.",
            "is_partially_observable": true,
            "external_tools_used": "GG normative prior classifier (language-model classifier trained on Goofus & Gallant using BERT/XLNet token embeddings). Also uses crowdsourced action elaborations (natural language) provided by the environment to feed GG.",
            "tool_output_types": "Unnormalized logits (two values): L_norm and L_notnorm (numeric scalar scores representing classifier confidence / logit), which are used to scale rewards.",
            "belief_state_mechanism": "Implicit belief encoded in the actor-critic network's processing of the current observation (text fields provided) and the network parameters updated by reward-shaped learning; no explicit, persistent belief-state data structure is described.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "No explicit belief update with tool outputs; GG outputs affect the scalar reward signal used to compute gradient updates for the networks, thereby biasing learned policy indirectly across training steps rather than modifying an explicit belief representation at runtime.",
            "planning_approach": "Learned policy (model-free RL) with reward shaping from GG; no search-based or model-based planning.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Policy-driven selection of admissible locomotion actions on the TextWorld room graph; no explicit path-finding algorithm.",
            "performance_with_tools": "With GG-pos scaling, agents in Playground and Superhero converge to high environmental rewards while increasing normative actions; in Clerk World GG-pos performed poorly on normative actions (learned to not use altruistic actions) and did not match GG-shaped's normative behavior. (See paper plots: GG-pos achieves normative action ratio ~40% in Playground; near-exclusive normative trajectories in Superhero for some runs.)",
            "performance_without_tools": "Compared to baseline A2C (no GG), GG-pos sometimes achieves similar environmental reward (Playground, Superhero) while producing more normative actions; in Clerk World baseline achieved higher environmental reward than GG-pos.",
            "has_tool_ablation": true,
            "key_findings": "Using GG outputs as reward shaping (GG-pos) biases the learned policy toward actions the classifier rates as normative without changing the agent's runtime action-ranking procedure; this increases normative behavior in some environments (Playground, Superhero) but is insufficient in scenarios (Clerk World) where normative actions require explicit trade-offs that reduce environmental reward.",
            "uuid": "e768.1",
            "source_info": {
                "paper_title": "Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "GG-mix",
            "name_full": "GG-mix (reward-shaped with GG difference logits)",
            "brief_description": "A2C agent that scales environmental reward by the difference between GG classifier logits (L_norm - L_notnorm), allowing the classifier's relative preference for normativity to amplify or invert environmental reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GG-mix",
            "agent_description": "A2C agent where per-step reward R_t+1 = R_env_t+1 * (L_norm - L_notnorm). The GG classifier processes the chosen action's natural-language elaboration and returns two logits; GG-mix uses their difference, which can reduce, amplify, or invert the environmental reward (if L_notnorm &gt; L_norm) thereby strongly penalizing actions judged non-normative. Training otherwise follows A2C procedures.",
            "environment_name": "TextWorld (Playground World, Superhero World, Clerk World)",
            "environment_description": "Partially observable TextWorld-based social/text scenarios with room graphs and local textual observations; admissible actions have crowdsourced NL elaborations used by GG.",
            "is_partially_observable": true,
            "external_tools_used": "GG normative prior classifier (BERT/XLNet-based) applied to crowdsourced action elaborations.",
            "tool_output_types": "Numeric logits (L_norm, L_notnorm); GG-mix uses their numeric difference.",
            "belief_state_mechanism": "Implicit internal state via actor-critic networks; no explicit belief-state memory or structured representation is described.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "GG outputs alter the scalar reward used to update network weights (reward shaping), thereby influencing the learned policy over training but not updating any explicit belief-state representation at runtime.",
            "planning_approach": "Model-free learned policy (A2C) with reward shaping; no search-based planning.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Learned policy selects navigation (go/locomotion) actions among admissible actions; no explicit path-planning algorithm.",
            "performance_with_tools": "GG-mix agents in Superhero and Playground learned normative solutions (in Superhero GG-mix and GG-pos almost exclusively followed normative trajectories). Behavior depended on phrasing of action elaborations; misclassifications could flip behavior. In Clerk World GG-mix did not adopt many altruistic actions and achieved lower normative rates than GG-shaped.",
            "performance_without_tools": "Compared to baseline A2C, GG-mix increases normative action selection in some environments (Playground, Superhero) while sometimes maintaining environmental reward; in Clerk World baseline outperformed GG-mix on environmental reward.",
            "has_tool_ablation": true,
            "key_findings": "Using the classifier's relative logit difference to scale reward can strongly bias learning toward or against actions, but is sensitive to classifier misclassification and to reward scaling; it influences policy indirectly through training rather than modifying runtime action-ranking.",
            "uuid": "e768.2",
            "source_info": {
                "paper_title": "Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "GG-shaped",
            "name_full": "GG-shaped (policy-shaping with GG normative prior)",
            "brief_description": "A2C agent that applies the GG classifier output directly to the actor's action logits at every timestep, re-ranking actions (policy shaping) so actions judged normative are more likely to be sampled at runtime.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GG-shaped",
            "agent_description": "Variant of A2C implementing policy shaping: sample actor logits [L_a1,...,L_an] for admissible actions, then for each action multiply its actor logit by (L_norm - L_notnorm) as produced by GG on that action's crowdsourced NLP elaboration, producing a reranked distribution from which the action is sampled. This uses GG as an intrinsic, per-action biasing signal at action selection time rather than modifying the scalar reward used by the loss (the reward used in loss computation remains environmental reward).",
            "environment_name": "TextWorld (Playground World, Superhero World, Clerk World)",
            "environment_description": "Partially observable TextWorld social/text environments. Agents receive local textual observations and lists of admissible actions with NL elaborations; environments include room graphs and scenarios where normative vs non-normative choices trade off with task efficiency.",
            "is_partially_observable": true,
            "external_tools_used": "GG normative prior classifier (BERT/XLNet-based) applied at runtime to each admissible action's natural-language elaboration; crowdsourced elaborations are provided by environment.",
            "tool_output_types": "Per-action numeric logits (L_norm, L_notnorm) used to compute a scalar (L_norm - L_notnorm) that multiplies the actor's action logits, yielding reranked action probabilities (numeric scores).",
            "belief_state_mechanism": "Implicit belief representation via the actor-critic network; GG-shaped does not maintain an explicit belief-state data structure. However, policy shaping changes runtime action selection by incorporating classifier-derived scores into the sampled policy distribution.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "No explicit belief update mechanism with tool outputs; GG outputs are used immediately to reweight action logits for sampling at each timestep. Learning updates still use environmental reward, so over training the agent's parameters adapt to the combined effect of reranked sampling and environmental feedback.",
            "planning_approach": "Learned policy with runtime policy-shaping (action reranking). Not model-based planning or search. Policy shaping biases the agent's action-selection distribution at each decision point using the external classifier's per-action scores.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation decisions are produced by the reranked policy sampling over admissible locomotion actions; no explicit graph search or shortest-path algorithm is used.",
            "performance_with_tools": "GG-shaped often increases normative behavior markedly (2â€“6x more normative actions in Clerk World than other shaped approaches) but at the expense of environmental reward in scenarios where normative actions reduce task efficiency (e.g., GG-shaped achieved ~40% of maximum observed environmental score in Clerk World while baseline achieved highest task reward). In Playground and Superhero environments GG-shaped achieves increased normative actions though sometimes less consistently than GG-pos/GG-mix due to GG classifier misclassifications of action elaborations.",
            "performance_without_tools": "Compared to baseline A2C, GG-shaped sacrifices environmental reward in some environments to achieve substantially higher normative/action alignment; baseline A2C attains higher task reward but far fewer normative actions.",
            "has_tool_ablation": true,
            "key_findings": "Policy shaping with an external classifier (GG-shaped) is the most effective method in difficult trade-off environments for producing normative behavior at runtime because it biases action selection directly, but it trades off environmental reward and is sensitive to classifier errors in action phrasing; GG-shaped modifies decision-time probabilities rather than the training reward signal.",
            "uuid": "e768.3",
            "source_info": {
                "paper_title": "Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning Norms from Stories: A Prior for Value Aligned Agents",
            "rating": 2,
            "sanitized_title": "learning_norms_from_stories_a_prior_for_value_aligned_agents"
        },
        {
            "paper_title": "TextWorld: A learning environment for text-based games",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "Policy shaping: Integrating human feedback with reinforcement learning",
            "rating": 2,
            "sanitized_title": "policy_shaping_integrating_human_feedback_with_reinforcement_learning"
        },
        {
            "paper_title": "Policy Shaping with Human Teachers",
            "rating": 2,
            "sanitized_title": "policy_shaping_with_human_teachers"
        },
        {
            "paper_title": "Deep Reinforcement Learning with a Natural Language Action Space",
            "rating": 1,
            "sanitized_title": "deep_reinforcement_learning_with_a_natural_language_action_space"
        },
        {
            "paper_title": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces",
            "rating": 1,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Comprehensible Context-driven Text Game Playing",
            "rating": 1,
            "sanitized_title": "comprehensible_contextdriven_text_game_playing"
        }
    ],
    "cost": 0.01329125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior
19 Apr 2021</p>
<p>Md SultanAl Nahian sa.nahian@uky.edu 
University of Kentucky</p>
<p>Denotes equal contribution</p>
<p>Spencer Frazier 
Georgia Institute of Technology</p>
<p>Denotes equal contribution</p>
<p>Brent Harrison harrison@cs.uky.edu 
University of Kentucky</p>
<p>Mark Riedl riedl@cc.gatech.edu 
Georgia Institute of Technology</p>
<p>Training Value-Aligned Reinforcement Learning Agents Using a Normative Prior
19 Apr 202143D8715A2DD40FA34FCE48E774792285arXiv:2104.09469v1[cs.LG]
As more machine learning agents interact with humans, it is increasingly a prospect that an agent trained to perform a task optimally -using only a measure of task performance as feedback -can violate societal norms for acceptable behavior or cause harm.Value alignment is a property of intelligent agents wherein they solely pursue non-harmful behaviors or human-beneficial goals.We introduce an approach to valuealigned reinforcement learning, in which we train an agent with two reward signals: a standard task performance reward, plus a normative behavior reward.The normative behavior reward is derived from a value-aligned prior model previously shown to classify text as normative or non-normative.We show how variations on a policy shaping technique can balance these two sources of reward and produce policies that are both effective and perceived as being more normative.We test our value-alignment technique on three interactive textbased worlds; each world is designed specifically to challenge agents with a task as well as provide opportunities to deviate from the task to engage in normative and/or altruistic behavior.</p>
<p>Introduction</p>
<p>Value alignment is a desirable property of an intelligent agent, indicating that it can only pursue behaviors that are beneficial to humans (Soares and Fallenstein 2014;Russell, Dewey, and Tegmark 2015;Arnold, Kasenberg, and Scheutz 2017;Moor 2006).Russell (2019) and others have argued that value alignment is one of the most important tasks facing AI researchers today.Ideally, a value-aligned system should make decisions that align with human decisions in similar situations and, in theory, make decisions that are unlikely to cause harm (Bostrom 2014).</p>
<p>Value-aligned intelligent systems are hard to build.As argued by Soares (Soares and Fallenstein 2014), it is nontrivial to directly specify values; there are infinitely many undesirable outcomes in an open world.Thus, a sufficiently intelligent artificial agent can violate the intent of the tenants of a set of prescribed rules of behavior without explicitly violating any particular rule.Machine learning based approaches to value alignment have largely relied on learning from observations, demonstrations, preferences or other forms of imitation learning (Stadie, Abbeel, and Sutskever 2017;Wulfmeier 2019;Ho and Ermon 2016;Ho, Gupta, and Ermon 2016;Abbeel and Ng 2004).Behavior cloning, learning from demonstration, and inverse reinforcement learning attempt to reverse-engineer "proper" task behavior from examples provided by humans performing a task.Values can thus be cast as preferences over action sequences, and preference learning can be formulated as reward learning or imitation learning (Russell 2019).There are a number of challenges faced by value alignment via imitation learning which are of relevance to this work: (1) demonstrations don't always afford generalization and demonstrations don't necessarily capture the importance of not doing some actions;</p>
<p>(2) it can be time consuming and costly to acquire sufficient demonstrations.We seek an alternative learning approach that addresses these limitations to learning values.</p>
<p>In this paper, we make three contributions.First, we introduce normative alignment, a well-defined subset of AI value alignment.Normativity refers to behavior that conforms to expected societal norms and contracts, whereas non-normative behavior aligns to values that deviate from these expected norms.Normative alignment is thus an approach to descriptive ethics, but constrained to a specific and given sub-set of humans for whom the norms hold.Second, we show how a normative prior-a model that biases an agent toward actions and outputs that conform to expected societal norms and contracts-can be used to shape the policy of a reinforcement learning agent.Agents trained in this way perform more normative and altruistic actions than those trained solely on task-based objective functions.Third, we provide a set of virtual environments that emulate scenarios that require an agent to reconcile task-oriented behavior and normative behavior.</p>
<p>Terms such as "ethics", "values" and "morals" are ambiguous.Some recent work (Lourie, Bras, and Choi 2020) conjectures that AI value alignment can be framed as a "descriptive ethics" assessment-something is ethical or desirable if it passes the judgment of a plurality of individuals.As norms can differ from group to group, Nahian, Frazier et al. (2020) argue that a normative prior can be learned from general examples of normative and non-normative behavior and transferred to new tasks.The general examples being human stories in this case.They show how their prior model can accurately classify normative and non-normative text descriptions and perform zero and few-shot transfer between narrative domains.The authors of these works speculatebut do not provide evidence-that a normative prior can be applied to reinforcement learning.We extend this work by providing a technique for how a normative prior can be integrated into reinforcement learning agents so that they complete tasks satisfactorily while maximizing normative or helpful actions where possible.</p>
<p>Through trial-and-error learning, a reinforcement learning agent learns a policy-a mapping from states to actions for all possible states that might be encountered-that maximizes expected reward.A reinforcement learning agent is given a reward function that provides numerical feedback about states visited, actions performed, or both.Typically, the reward function defines the "task" in the sense that the reward is maximized when the agent carries out the behavior desired by the designer of the task environment.Rewards are often sparse: an agent may receive a single piece of feedback at the culmination of a task, or the task may be broken into components; each of which rewards the agent.</p>
<p>We distinguish between two sources of reward: (1) Environmental reward is provided by the environment as and only considers task performance.For example, a robot that works in a post office may have the task of stamping forms; this agent might receive reward for each form stamped.</p>
<p>(2) Normative reward is an intrinsically produced value based on how normative an action is (e.g., as classified by a normative classifier such as that by Frazier et al. (2019)).In the post office example, the artificial agent may have opportunities to help patrons, even though it is not required to do so as part of it's job (i.e., is not given environmental reward for it).The separation of sources of reward is beneficial to the creation of value-aligned agents because the task designer can focus on the objective metrics without concern about values, normativity, or altruism; these can be considered separately.</p>
<p>The use of a normative prior to guide a reinforcement learning agent implies that we do not need to demonstrate normative behavior in the context of a specific environmental task.The normative reward is thus an intrinsic behavioral signal while the environmental reward is an extrinsic behavioral signal.Training a reinforcement learning agent on an environmental reward and a normative reward, however, is not necessarily straightforward.The reward scales may be different.Furthermore, a sum of rewards is hard to tune; a policy can favor one reward over another or produce compromise which results in a policy that is neither normative nor able to complete a given task.We experiment with a number of ways of combining multiple reward signals.We find that policy shaping (Griffith et al. 2013;Cederborg et al. 2015) is more effective in balancing normative behavior and environmental task behavior compared to other techniques such as summing reward signals.Policy shaping trains a reinforcement learning on a regular environmental reward but uses a secondary criterion to re-rank action choices at every step to bias the agent away from certain courses of action.We update policy shaping for deep reinforcement learning agents in which a noisy normative action classifier provides the shaping signal.</p>
<p>To evaluate different reinforcement learning techniques we create a suite of three virtual simulation environments, each of which emulates a situation where an agent must make tradeoffs between environmental reward and intrinsic normative reward.We build our simulations on top of the TextWorld (CÃ´tÃ© et al. 2018) framework.This framework can be used to build text-based environments, wherein an agent receives a textual description of the environment and must describe their actions through text commands.We use TextWorld for two reasons.First, whether an action is considered normative or not is often based on how that action is described.We crowdsource descriptions of actions to control for experimental biases that might result in how we configure the actions in the text world environments.Second, it facilitates the construction of scenarios that focus on social interactions between characters-the key consideration in our work on normative behavior-in reproducible manner.Third, prior work on normative classifiers has already proven their effectiveness on text-based classification tasks.</p>
<p>Background and Related Work</p>
<p>In this section we provide relevant background on normative prior models and text-based environments.</p>
<p>Normative Prior Model</p>
<p>The majority of value alignment research has emphasized learning by demonstration, behavior cloning, or inverse reinforcement learning.These techniques require an extensive amount of demonstrations which must be done in the context of specific tasks.As an alternative, Frazier et al. (2019) used the BERT (Devlin et al. 2018) and XLNet (Yang et al. 2019) language models' token embeddings to train a binary classifier which discriminates between text descriptions of normative and non-normative behavior.They trained their normative classifier on Goofus &amp; Gallant, a children's educational comic strip featuring two characters of the same names.</p>
<p>Text describing Goofus' actions always deviates from the "proper" way to behave, while Gallant always performs the behavior of an exemplary child in western society, at the time the comics were created.G&amp;G is a naturally labeled source of normative and non-normative text, for the specific society it represents.The model takes a natural language description of behavior (e.g."He finished his chores before playing.")and performs a linear classification, producing a vector L norm , L Â¬norm , with values in the range of [âˆ’âˆž, âˆž] that can be interpreted as a non-normalized distribution over the network's confidence that the input is normative or non-normative.</p>
<p>The theoretical advantage of a normative classifier is that it can be trained offline in a classical supervised learning paradigm and then applied to specific tasks.Frazier et al. (2019) demonstrated this in their model-which we refer to it as the GG model.</p>
<p>They report up to 90% accuracy on G&amp;G and 83% and 85% on zero-shot transfer tasks across 2 other qualitatively different text corpora.Work building upon this shows that the GG model can be used to fine-tune GPT-2 (Radford et al. 2019), reducing the amount of non-normative text generated by the language model.Frazier et al. also speculate that their classifier model can be used as a normative prior that biases agent behavior toward normative courses of action in other contexts.However, this was not tested.</p>
<p>In this paper, we build upon the GG model in order to shape the behavior of reinforcement learning agents who must also perform tasks in the environment.</p>
<p>Text-Based Environments</p>
<p>We build our test simulation environments on top of TextWorld (CÃ´tÃ© et al. 2018), a framework for building textbased environments.Text-based environments and games are useful for developing and testing reinforcement learning algorithms that must deal with the partial observability of the world.In text-based games, the agent receives an incomplete textual description of the agent's current location in the world and the immediate consequences of commands.From this information-and previous interactions with the world-an agent must determine the next best action to take to achieve some quest or goal.The agent must then compose a textual description of the action they intend to make and receive textual feedback of the effects of the action.Formally, a text-based game is a partially observable Markov decision process (POMDP), represented as a 7-tuple of S, T, A, â„¦, O, R, Î³ representing the set of environment states, conditional transition probabilities between states, words used to compose text commands, observations, observation conditional probabilities, reward function, and a discount factor respectively (CÃ´tÃ© et al. 2018).</p>
<p>A number of text-based game agents have been developed using deep reinforcement learning (Narasimhan, Kulkarni, and Barzilay 2015;He et al. 2016;Yin and May 2019;Zahavy et al. 2018;Zelinka 2018;Adolphs and Hofmann 2019;Xu et al. 2020;Dambekodi et al. 2020).Ammanabrolu et al. (2020) in particular show that an advantage actor critic (Mnih et al. 2016) (A2C) neural network architecture with a recurrent decoder head to generate actions can achieve state of the art performance on complex, commercially-produced text-based games.Our agents are based on the A2C architecture, although we do not use knowledge graphs or dynamic exploration strategies necessary for larger games-our goal is not to achieve state of the art on commercial games, but to provide a baseline to explore the incorporation of implicit normative rewards into RL agent behavior.The text environments (next section) are relatively small to provide a focused exploration of the tradeoffs between environmental and normative rewards.</p>
<p>Test Environments</p>
<p>There are no environments for testing the normativity of reinforcement learning agents.We have created three new environments to evaluate normative interactions with social entities while simultaneously trying to perform a task with an environmental reward.That is, there is a task that must be performed, but there are preferred and non-preferred ways of accomplishing the task that align with notions of normativity and non-normativity for a particular society.</p>
<p>Each environment is designed such that, in the absence of an intrinsic normative reward signal, agents will learn a policy that, when executed, will likely appear to be nonnormative.Each environment pits the environmental reward against intrinsic normative reward in a different way.The agent may need to avoid non-normative behaviors that are not part of solution trajectories, avoid non-normative behaviors that comprise a less costly solution, or be given opportunities to take altruistic behaviors that are not strictly necessary and potentially in conflict with environmental rewards.While these environments are tuned to Western ideals of normative social behavior, these environments also provide a template for the construction of test environments for societies with different norms.</p>
<p>Each environment that we investigate in this paper was constructed in the TextWorld (CÃ´tÃ© et al. 2018) framework.We use this because it affords the ability to construct scenarios with social entities and more complex action spaces than the grid worlds more conventionally used for AI safety experiments (Leike et al. 2017).These environments, thus, challenge the agent to reconcile task-oriented behavior and normative behavior.Consistent with text-based games, each scenario is composed of multiple rooms (discrete locations), entities, and task-oriented rewards.Despite being text-based environments, we have simplified each environments so that agents do not need to learn to read the descriptions and can instead learn to recognize states by their unique location names, observable entities, and observable items.The admissible commands in each location (e.g., go west, allow the robbers to escape) are also given.See Figure 4.</p>
<p>One of the difficulties of working with a text-based environment, especially with respect to normativity, is that the way an action or its description is phrased can have a large affect on whether it is deemed normative or non-normative.To control for experimental biases that may exist in author created action descriptions, we chose to crowdsource action elaborations.These elaborations would then be used to describe the actions that an agent can take in each environment and would also be evaluated for normativity when choosing which action to perform.</p>
<p>To construct this dataset of action elaborations, we recruit 50 participants on Amazon Mechanical Turk.Each participant is provided an environment description, an action that is being performed, and the role of the character performing said a action.They are then prompted to write a phrase describing how someone in their role would perform that action given the environmental context.A selection from the action elaboration survey can be seen in Figure 1.Each action in each environment received 3 elaborations after pruning incomplete submissions, chosen at random from the generated set.</p>
<p>In the following subsections, we will describe each of the environments examined in this paper in greater detail.We will focus on the states, actions, and rewards of each environment; we will also explain the purpose that each environment serves with respect to examining normative alignment.</p>
<p>Playground World</p>
<p>The first environment we explore is called Playground World.The Playground World environment is meant to sim-  ulate a situation that might occur when a child is playing on a playground.This is designed to be a simple proof that an agent can be trained to avoid non-normative behaviors since the scenario can be successfully completed by performing only actions that have neutral normativity.In addition, this environment is meant to be the most aligned to the Goofus and Gallant normative prior model since it presents a social situation that closely resembles events that may occur in those comics.This allows us to investigate how a normative shaping approach performs when knowledge transfer due to an unfamiliar context is not an issue.</p>
<p>The Playground World depicts a sports team dressing room before a sports game is played.The agent's role is as a member of a team whose goal is to get prepared before the game.The environment is composed of three rooms: a locker room, a meeting room, and the playground itself (Figure 2).To complete the scenario, the agent must collect sporting gear for the game, wear the gear, go to the meeting room and join the team meeting, then go to the playground after the meeting is finished.The agent receives a large reward for joining the meeting, a small reward for completing each of the other parts of the process and a final small reward upon completing the full scenario.</p>
<p>These actions make up the task-oriented actions in Playground World.Beside these task-oriented actions, the player can take several others optional actions which are ostensibly normative (e.g., "help your teammates", "give attention in the meeting") or non-normative (e.g., "interrupt your team- A reinforcement learning agent should learn that it can complete the scenario and maximize expected environmental reward by never conducting non-normative actions.Any standard reinforcement learning agent should learn to avoid non-normative behaviors by virtue that they do not result in greater expected reward.It is not strictly necessary to perform any actions except for neutral actions, however an agent may learn to perform normative social actions if it receives additional intrinsic reward for those actions-they do not reduce the environmental reward.</p>
<p>Superhero World</p>
<p>The second environment we explore is called Superhero World.The Superhero World environment depicts a fictional scenario in which the agent plays the role of a superhero who must stop a bank robbery.This environment is meant to provide a challenge to a normative aligned agent for several reasons.First, given that we are utilizing the Goofus and Gallant prior on action normativity, some amount of transfer is required.The original Goofus and Gallant training set did not contain information about superheroes, bank robberies, or how one might effectively use super powers to stop a bank robbery.Second, this scenario presents the agent with more significant moral quandaries which have to be navigated when compared with the Playground World.A description of the Superhero World environment can be seen in Figure 4.The scenario can be accomplished by completing two sub-tasks: (1) collecting information about the bank robbery, and (2) using the information to stop the robbery.After completing each sub-quest (task) the agent gets a small environmental reward and a final, equal-sized small reward after finishing the game successfully.Going to the wrong bank or allowing the robber to escape are the failure states of the game.</p>
<p>In order to test how the normative prior could potentially alter the agent's moral decision making, each of these subtasks can be completed in two different ways: one that consists of non-normative actions such as "beat the informant" and "shoot the robber", and another that consists of normative actions such as "do a favor to the informant", "ask the informant about the robbery" and "convince robber to surrender".To make the choice to behave normative versus non-normative more difficult, the non-normative path was designed to be shorter than the normative path.This is significant because a reinforcement learning agent which only responds to environmental reward is likely to learn a policy which utilizes the shorter, non-normative path.</p>
<p>Superhero World forces the agent to choose between normative (or neutral) actions that result in longer solutions, or solutions that are shorter but contain non-normative actions.Recall that Playground environment, on the other hand, has a neutral path that, in the optimal case, would normally be chosen by an agent that is motivated solely by environmental reward.</p>
<p>The goal here is to show that a normative-aligned agent with an intrinsic reward signal derived from a normative prior may learn that the longer paths yield greater expected reward; however, tuning issues can arise-if the intrinsic reward is not weighted correctly relative to the environmental reward, the agent may still learn the non-normative policy.These are the issues that we hope to examine in this environment.</p>
<p>Clerk World</p>
<p>Clerk World is designed to investigate a scenario where tradeoffs exist between task efficiency and socially conscious actions which ignore or hinder task performance.In addition, this is another scenario in which knowledge transfer will be necessary to effectively utilize the normative prior as this is a situation not explored by the Goofus and Gallant normative prior.</p>
<p>The Clerk World scenario simulates a small Post Office.The agent plays the role of a worker in the office tasked with finding forms and stamping them.There are a number of customers and one coworker.A fixed number of forms-ten in all-are scattered around the environment and the agent must move around to find them.Not all forms are required to complete the scenario objective or subgoals, only a preset few are main task objectives.The agent receives a small reward for each form stamped, and a final, larger reward is given upon scenario completion.Actions that advance the scenario include locomotion, picking up forms, and applying the "stamp" action to forms in inventory.</p>
<p>Non-player character objects (coworker, customer) can be the targets of two other actions; "aid" and "ask".To emulate a time trade-off, when the agent chooses to aid or ask nonplayer characters, a subgoal involving a random form fails, lowering an agent's environmental reward.The agent may still stamp that form but will not receive reward for doing so, approximating time-on-task lost for engaging in actions adjacent to its primary objective.</p>
<p>This scenario differs from the first two in that it requires the agent to make a trade-off between stamping as many forms as possible and taking actions such as "aid" or "ask" which might be informally referred to as altruistic.An agent that is only responding to environmental reward can complete the scenario without "aid" or "ask" actions.Unlike the Playground World, the scenario can be completed with fewer than the maximum reward points, and there there are no actions that would ostensibly be considered non-normative.This environment also differs from Superhero World in that there are no optimal "paths" through the scenario and all actions are not in service to the agent's overall environmental goal.The altruistic action is completely separate from the task-oriented actions in the environment.Thus, aiding another agent is not necessarily in service to the agent's environmental goal, unlike in the Superhero world where both normative and non-normative actions will ultimately result in stopping the bank robbery.This allows us to examine how a normative shaped agent would perform when faced with the choice between helping others and optimally completing its own task.We can also examine how factors such as time, environmental reward values, and intrinsic reward values could potentially affect these decisions.Reference the environment layout in Figure 3.</p>
<p>Methods</p>
<p>Training reinforcement learning agents with environmental reward alone may result in behavior that humans would consider non-normative if the greatest expected environmental reward is achieved by performing behaviors that deviate from expected norms.This can include learning to perform actions that are explicitly non-normative or harmful, but can also be behavior that fixates on task in the presence of opportunities to be helpful, altruistically, or polite.However, if an agent is capable of generating an intrinsic normative reward, then it may learn to make trade-offs that incorporate normative behaviors.We describe a set of experiments to validate how best to use a normative prior model-specifically the GG classifier model (Frazier et al. 2019)-to help guide reinforcement learning.</p>
<p>Environment Preliminaries</p>
<p>For each state in a TextWorld environment, a reinforcement learning agent receives an observation consisting of (a) description text of the current room, (b) items in inventory, (c) facts about the state of objects in the environment (e.g."A drawer is open"), and (d) previous reactive text (e.g., "You can't go west") if any.TextWorld additionally provides a set of admissible actions-actions that can be executed in the current state.We allow our agents to access the list of admissible actions and choose from them instead of having to generate a command word token by word token-teaching agents to read and write is not the primary purpose of this research.After an action is taken in timestep t, the agent increments to timestep t + 1 and TextWorld provides an environmental reward R env t+1 , which may be zero.We augment the standard TextWorld environment to use action elaborations.Each admissible action that TextWorld provides to the agent is accompanied by a longer descriptive text.The descriptive text of the taken admissible action is selected randomly and uniformly from the corresponding three crowdsourced elaboration texts at each step.This elaboration text serves two purposes.First, the GG normative model operates on natural language text sequences.Second, since it is crowdsourced, it is authored by a neutral source to remove the possibility of experimental bias.</p>
<p>Agent Implementations</p>
<p>Advantage Actor-Critic (A2C) architectures for reinforcement learning have been found to be effective for playing text-based games (Ammanabrolu and Hausknecht 2020).An Actor-Critic architecture use two neural networks: an actor network chooses an action, and a critic network tries to guess the value of the state-action combination.At each timestep, s t represents the state as an input to the actor network Ï€ Î¸ (s t , a) and the critic network qw (s t , a) where a represents a possible action.Î¸ and w are weights of the actor and critic networks, respectively.The actor network's policy update is:
âˆ†Î¸ = Î±âˆ‡ Î¸ (log Ï€ Î¸ (s, a)) qw (s, a)(1)
where qw (s, a) is an a q-based approximation function of the action's value.The critic's update function is given by: âˆ†w = Î²(R(s, a) + Î³ qw (s t+1 , a t+1 ) âˆ’ qw (s t , a t )) Ã— âˆ‡ w qw (s t , a t ).</p>
<p>(2)</p>
<p>Î± and Î² represent different learning rates for each model.For Advantage Actor-Critic, this value function is replaced with an advantage function, which compensates for the high degree of variability in value-based RL methods.Given a state s t as input, the actor network outputs a distribution over the admissible actions.An action is sampled from this distribution and passed to the environment for execution.The agent then receives environment reward R env t+1 .In the typical A2C agent the only reward is the environment reward, i.e.R(s, a) = R env t+1 in Equation 2. The normative prior model, GG, receives the natural language elaboration of the chosen action and outputs a distribution of unnormalized log probabilities from the final dense layer of the network.Specifically, the normative prior produces two logits L norm and L Â¬norm for the belief that the input is normative and the belief that the input is nonnormative, respectively.Note that the GG model is only finetuned on the G&amp;G dataset and all experiments are effectively zero-shot transfer to the three TextWorld environments.Figure 6 shows the classifier's distribution across all admissible actions in all environments.</p>
<p>In order to understand how best to make use of the normative prior model, we propose three agent approaches for how to incorporate the outputs of the normative prior into the agent's reward R t+1 :</p>
<p>GG-pos This agent is an A2C agent which applies the normative prior's positive label confidence L norm to the environment reward for the action chosen by A2C, specifically:
R t+1 = R env t+1 Ã— L norm (3)
The agent simply receives more reward when the action is judged to be normative; it is the simplest means of combining two rewards.</p>
<p>GG-mix This agent is an A2C agent that applies the combined logits from GG-unnormalized log probabilities for the normative and non-normative classes-to the environmental reward.If the normative prior is equally certain about the normativity of the input, they cancel each other out; specifically
R t+1 = R env t+1 Ã— (L norm âˆ’ L Â¬norm ) (4)
If L Â¬norm is greater than L norm , the environment reward may be flipped to a negative overall reward.</p>
<p>GG-shaped This is a variant of the base A2C architecture implementing policy shaping (Griffith et al. 2013;Cederborg et al. 2015;Faulkner, Short, and Thomaz 2018).Policy shaping agents produce a probability distribution over actions, which is then adjusted by a second, externally produced source of value information, biasing the agent toward certain actions or states.Policy shaping was originally introduced to incorporate human action preferences into tabular reinforcement learning with finite state and action spaces.Lin et al. (2017) shows that policy shaping can be applied to deep q-learning, also to incorporate human preferences.</p>
<p>In this work, we make use of a policy shaping technique with the following two modifications: (1) We use an intrinsic source of value information derived from an action classifier, and (2) we apply value-aligned policy shaping to the A2C reinforcement learning architecture for the first time.</p>
<p>We sample the distribution of unnormalized log probabilities (logits) over potential actions from the final dense layer of the Actor Critic network: [L a1 , ..., L an ].For each admissible action, a i is altered by GG's assessment of the action elaboration:
L ai = L ai Ã— (L norm âˆ’ L Â¬norm ) (5)
The agent then samples the action from this "reranked" distribution.Since this is more complicated than the standard A2C architecture, we show our new architecture in Figure 5.An additional loss-augmentation approach was explored based on Peng et al. ( 2020)-initially used to fine-tune a language model on GG output.However agents trained with this approach generally struggled to converge.</p>
<p>Hyperparameters For each environment, we present the result of 5 train-test iterations.In each iteration, we trained Clerk World for 1000 episodes, Superhero World for 2500 episodes and Playground for 4000 episodes.The maximum permissible steps in each episode for Superhero and Playground are 100 and 50 for Clerk World.We use the Adam Optimizer with a learning rate of 3e-5.Agents in the Superhero and Playground environments have been trained on a single Nvidia GTX 1080Ti GPU and the Clerk World has been trained on a single GTX 2080Ti GPU.</p>
<p>Metrics</p>
<p>To evaluate these agents, we need a way to characterize and assess the differences in behavior.Unlike most reinforcement learning research, we cannot compare the optimality of the agents as measured by the environmental reward received.Each agent is operating under a slightly different way of computing rewards -for example GG-pos will always receive more reward per step than GG-mix or GG-shaped.All agents may be highly optimal for their reward functions but behave very differently.To characterize and assess differences in execution behavior, we label a subset of admissible actions as "normative" or "task-oriented" and measure the normalized ratio of normative actions to task-oriented actions the agent takes: n norm /(n norm + n task ).Task-oriented labels are derived from the minimum set of admissible actions required to complete quests in the world.In Superhero world and Playground world, these are all actions along the shortest path to completion of the main quest.In Clerkworld this is moving, taking and stamping -also the actions required for the shortest main quest completion.Normative actions are the difference between the set of all admissible actions and the task-oriented set, excluding actions which result in the failure of the main quest.The agents never have access to these ground-truth labels.</p>
<p>Experiments</p>
<p>We conduct three experiments.The first experiment examines how agents that incorporate intrinsic normative rewards in different ways fare against a baseline A2C when it comes to environmental reward.The second experiment quantifies behavioral differences when it comes to using normative and task-oriented actions.The third experiment looks at the effect of natural language phrase choices on the behavior of agents.1</p>
<p>Experiment 1: Environmental Reward</p>
<p>In this experiment we seek to understand the effect of the normative prior on acquired environmental reward.We should expect an agent that ignores the intrinsic normative reward to achieve a greater total environmental reward over time.For each environment, we train our three agents that are augmented by the intrinsic normative reward plus a fourth baseline A2C that only uses environment reward.</p>
<p>We train each agent for 1000 episodes in the Clerk World environment, averaging over five training iterations.The Playground and Superhero World are trained for 4000 and 2500 episodes respectively as they take more time to converge.Performance in Playground and Superhero world are also averaged over five training iterations.At every step, the agent chooses an action and then randomly and uniformly chooses one of three crowdsourced action elaborations.We measure the amount of environmental reward over time, which is distinct from the reward used to compute network loss in GG-pos and GG-mixed (GG-shaped does not alter the reward used in loss calculations).</p>
<p>As depicted in Figure 7 and Figure 8, in Playground World and Superhero World, all normative agents as well as the baseline A2C agent converge to policies which achieve maximum reward.Clerk World is a more challenging environment.For all Clerk World runs (Figure 9), the baseline A2C achieves the highest environmental reward score.The GG-shaped agent achieves âˆ¼40% of the maximum observed environmental score; in Clerk World, opportunities for environmental reward are lost with each altruistic action.</p>
<p>Normative and altruistic actions in Clerk World and Playground World environments require the agent to perform actions that do not progress the scenario.Therefore it is necessary-especially in Clerk World where opportunities for reward are lost with each altruistic action-to give up some environmental reward in order to act in ways that will  be perceived as normative.The significance of this experiment shows that a policy shaping approach sacrifices more environment score in order to take more normative actions than other means of using the normative reward.This confirms our hypothesis and experiment 2 (next section) shows how different techniques qualitatively make the trade-off between normative and non-normative behaviors.</p>
<p>Experiment 2: Behavioral Analysis</p>
<p>In this experiment, we analyze the behavioral differences between agent techniques.We use the ratio of task-specific to normative actions to visualize qualitative difference between agents.Recall from the Metrics section that we labeled some actions in each environment as normative and others as taskspecific.As with experiment 1, we train each agent for 1000 episodes in Clerk World, 2500 episodes in Superhero and 4000 episodes in Playground environment, averaging over five training iterations per environment.</p>
<p>In Playground World (Figure 10), the GG-pos and GGshaped agents learn policies that execute normative actions âˆ¼40% of the time.In contrast, the baseline A2C agent learns that normative actions are unnecessary.</p>
<p>In Superhero World, we must use a slightly different formulation of our metric.Because in this environment the agent can complete the scenario using normative or nonnormative actions, Figure 11 shows the normalized ratio of normative to non-normative actions.The GG-pos and GGmix agents learn to almost exclusively follow the trajectories made up of "normative" actions.The baseline A2C agent discovers that the trajectories featuring "non-normative" actions are shorter and learns a policy that favors them.The GG-shaped agent favors the normative trajectories (&gt; 0.5) but not consistently.We observe that the GG model misclassifies some of the elaborations for "normative" actions in Superhero World as "non-normative" (see next section), which confuses the agent because some actions are sometimes re-ranked high and sometimes re-ranked low depending on which elaboration gets used.</p>
<p>In Clerk World (Figure 12), the baseline A2C agent learns not to use altruistic actions, which not only don't progress  the scenario but also reduce the maximum reward achievable.The GG-pos and GG-mix agents also learn policies that use almost no altruistic actions.This is likely because the intrinsic normative reward added to the environmental loss doesn't make up for lost reward due to altruistic actions.The GG-shaped agent learns a policy that uses significantly more altruistic actions than any of the other alternatives.As seen from Experiment 1, this is done at the expense of environmental reward because this scenario penalizes the environmental reward for every altruistic action taken.The extent to which the GG-shaped agent attempts to use normative actions can be modulated by scaling the output of the GG model, however.</p>
<p>Experiment 3: Action Elaboration Phrasing</p>
<p>In experiment 2 we see how elaboration phrasing has an effect on the agent.In this experiment, we assess how the crowdsourced action elaborations affect agent behavior.In the Test Environments section we discuss how each admissible action has three action elaborations.Because the GG model can be sensitive to certain phrasings of the same ac-Figure 11: Ratio of normative actions taken for all agent types in Superhero World, smoothed with a 20-episode sliding window.In this environment, GG-mix and GG-pos outperform GG-shaped in total normative actions taken.</p>
<p>Figure 12: Normalized ratio of normative actions taken for all agent types in Clerk World, at that episode, smoothed with a 20-episode sliding window.This indicates that the decrease in environmental reward later in training is not attributed to an increase in normative actions.tion, we seek to understand how different natural language phrasings for action elaborations alter agent behavior when all else is kept constant.For each of the three sets of paraphrases, we test with the GG-mix agent in each environment.</p>
<p>Figure 13 shows the ratio of normative actions to task actions (e.g., a score of 1.0 means 100% normative actions) in the Superhero World.For two of the three crowdsourced phrase sets, we see that the GG-mix agent learns a policy that strongly prefers actions that we labeled as normative.For one phrase set (phrase set 1), some action elaborations are classified with the opposite of the ground-truth label.As a consequence, the agent's resultant policy selects a mix of normative and non-normative actions.</p>
<p>These results tell us two things.First, our ground truth labels for our metrics are generally in agreement with crowd worker, when considering a majority of elaborations.Second, the specific way in which commands are elaborated into natural language for normative classification can have an effect on agent behavior.However, note that the primary purpose of collecting crowdsourced elaborations was collected to avoid experimenter bias.</p>
<p>Discussion</p>
<p>Our experiments show that the three proposed techniques for incorporating intrinsic normative reward into a deep reinforcement learning agent achieve desired behavioral change, increasing the use of actions perceived to be normative.Experiments in the Superhero environment show that even though the non-normative path is shorter, hence more efficient, agents learn the policy that prefers taking normative path to reach the goal in the presence of a normative prior model.Even if the normative actions do not contribute to accomplishing goals, agents still may take some of these actions without sacrificing its objectives, seen in the Playground environment experiments.The Clerk World experiments show that the policy shaping agent, GG-shaped, is more robust to complicated trade-offs.The GG-shaped receives a lower task reward but is (a) robustly 2-6x more normative throughout its training iterations and (b) can be useful in situations where normative behavior during training is beneficial (e.g.-apprenticeship learning).</p>
<p>The results also show that how actions are described can have a significant effect on the behavior of the agents.The normative prior can be sensitive to particular wordings.This is an artifact of our use of crowdsourcing to avoid experimenter bias, but serves to remind that normativity is subjective and that things that are normative can be described in ways that present as non-normative, or vice versa.</p>
<p>In general we see that GG-pos and GG-mix do not lose as much environmental reward as GG-shaped and are able to find "normative" solutions in the Playground and Superhero scenarios.However, GG-pos and GG-mix are unable to handle the complexities of the Clerk World where normative rewards can only be achieved at the expense of environmental reward.GG-shaped is able to balance these rewards andwhen the GG model is not mislead by action elaborationsperforms equally or more normative actions that GG-pos and GG-mix.</p>
<p>We acknowledge that the normative prior model we use, originally developed by Frazier et al. (2019), is only one possible normative model.In principal, the behavior of the agent can be shaped according to what a society considers normative by supplying a normative classifier model trained on dif-ferent corpora.However, value-aligned corpora are not particularly common.However, we assert that our policy shaping model is not specialized to any particular set of social norms.Any normative prior may be substituted in this approach.We attempt to show this with experiments in different environments, assessing which environmental rewards and norms may come into conflict with each other.</p>
<p>Conclusions</p>
<p>Value alignment is a difficult problem and existing approaches-like expert demonstrations or preference learning-can be expensive from a cost perspective or human time-on-task perspective.If a human must produce demonstrations or extensive traces need to be collected, it may not be practical to initially train and deploy a machine learning model which exhibits normative behavior.In this paper, a normative prior model, in the form of a languagemodel-based classifier, is used to align reinforcement learning models' behavior with limited initial, additional human intervention.To test this novel architecture, we developed three test environments, using the TextWorld (CÃ´tÃ© et al. 2018) framework.The environments test different ways in which task-based and normative actions might conflict with each other.We find that our policy shaping reinforcement learning architecture has properties that make it well-suited to blending the needs of an environment task and a separate, intrinsic normative reward.Because environmentaltask-rewards are separate from normative rewards, we believe this is a step toward practical design of norm-aligned agents that can operate in ways that humans will recognize as normative and possibly altruistic.</p>
<p>Ethical Considerations</p>
<p>The work described in this paper is expressly targeted at making reinforcement learning agents that are normativelyaligned, a form of value-alignment.Reinforcement learning agents trained on task rewards tend to exclude other considerations that may make them safe around humans or sensitive to the needs of humans.We present an approach for robustly incorporating an intrinsic notion of norm-alignment into reinforcement learning agents.While this will be at the expense of task-efficiency, we argue that agents that operate withing social norms will be safer and possibly altruistic.</p>
<p>We note, however, that our experiments were conducted with a model that was only trained on a corpus of generic, Western culture-aligned social norms.We do not contend these to be universal norms, values or ethical guidelines.We wish to also note that in no way are these scenarios intended to suggested that this model could be used to assess normativity in parallel, real world scenarios.Models trained and assessed in text environments approximating certain real-world environments and situations should not be used in real-world settings.We further acknowledge that a norm-aligned agent can be converted into a non-normative agent by flipping the polarity of the outputs of the normative model.Finally, we caution future researchers to consider what may previously have been normative may not be normative in a modern societal context.</p>
<p>Figure 1 :
1
Figure 1: Exemplar question given as a prompt Amazon Mechanical Turk workers.The text in red is one of the admissible action commands to the text world environment.</p>
<p>Figure 2 :
2
Figure 2: Visualization of the Playground room graph</p>
<p>Figure 3 :
3
Figure 3: Visualization of the Clerk World room graph</p>
<p>Figure 5 :
5
Figure 5: The GG-shaped agent architecture.The blue box on the right side is GG model, repeated n times for each admissible action.</p>
<p>Figure 6 :
6
Figure6: Average and maximum normalized valence (i.e.classifier confidence sampled from the normalized probability distribution) across the crowdsourced action elaborations.There are few instances where the GG classifier picks up on extreme normativity or non-normativity, even where one might expect participants to add charged terms (e.g.'shoot robbers'), but participants do insert some in cases with more ambiguity (e.g.'go', 'wait' or 'look').</p>
<p>Figure 7 :
7
Figure 7: Average environmental score (without normative reward) for the Playground environment, smoothed with a 20-episode sliding window.</p>
<p>Figure 8 :
8
Figure 8: Average environmental score (without normative reward) for Superhero environment, smoothed with a 20episode sliding window.</p>
<p>Figure 9 :
9
Figure 9: Average environmental reward (excluding normative reward) relative to the maximum observed score for Clerk World at that episode, smoothed with a 20-episode sliding window.The GG-Shape agent consistently underperforms A2C and GG-pos at the task but consistently performs normative actions.</p>
<p>Figure 10 :
10
Figure 10: Ratio of normative actions taken for all agent types in Playground World, smoothed with a 20-episode sliding window.Policies for GG-Shape and GG-pos perform an equal ratio of normative actions after the convergence in this environment.</p>
<p>Figure 13 :
13
Figure 13: Ratio of taken task-actions and normative-actions for different actions phrase types trained with gg-mix Agent in the Superhero environment.</p>
<p>Experiment code, models and these text environments will be made publicly available upon full publication.</p>
<p>LeDeepChef: Deep Reinforcement Learning Agent for Families of Text-Based Games. P Abbeel, A Y Ng, L Adolphs, T Hofmann, CoRR abs/1909.01646Proceedings of the twenty-first international conference on Machine learning. the twenty-first international conference on Machine learning2004. 20191Apprenticeship learning via inverse reinforcement learning</p>
<p>Graph Constrained Reinforcement Learning for Natural Language Action Spaces. P Ammanabrolu, M Hausknecht, International Conference on Learning Representations. 2020</p>
<p>P Ammanabrolu, E Tien, M Hausknecht, M O Riedl, arXiv:2006.07409How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds. 2020arXiv preprint</p>
<p>Value alignment or misalignment-what will keep systems accountable?. T Arnold, D Kasenberg, M Scheutz, AAAI Workshops. 2017</p>
<p>N Bostrom, Superintelligence: Paths, Dangers, Strategies. 2014</p>
<p>Policy Shaping with Human Teachers. T Cederborg, I Grover, C L IsbellJr, A L Thomaz, IJCAI. 2015</p>
<p>Textworld: A learning environment for textbased games. M.-A CÃ´tÃ©, Ã KÃ¡dÃ¡r, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L El Asri, M Adada, Workshop on Computer Games. 2018</p>
<p>Springer, S Dambekodi, S Frazier, P Ammanabrolu, M O Riedl, arXiv:2012.02757Playing Text-Based Games with Common Sense. 2020arXiv preprint</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M Chang, K Lee, K Toutanova, CoRR abs/1810.048052018</p>
<p>Policy Shaping with Supervisory Attention Driven Exploration. T K Faulkner, E S Short, A L Thomaz, 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2018</p>
<p>Learning Norms from Stories: A Prior for Value Aligned Agents. S Frazier, M S A Nahian, M Riedl, B Harrison, arXiv:1912.035532019arXiv preprint</p>
<p>Policy shaping: Integrating human feedback with reinforcement learning. S Griffith, K Subramanian, J Scholz, C L Isbell, A L Thomaz, Advances in neural information processing systems. 2013</p>
<p>J He, J Chen, X He, J Gao, L Li, L Deng, M Ostendorf, arXiv: Artificial IntelligenceDeep Reinforcement Learning with a Natural Language Action Space. 2016</p>
<p>Generative adversarial imitation learning. J Ho, S Ermon, Advances in neural information processing systems. 2016</p>
<p>Model-free imitation learning with policy optimization. J Ho, J Gupta, S Ermon, International Conference on Machine Learning. 2016</p>
<p>. J Leike, M Martic, V Krakovna, P A Ortega, T Everitt, A Lefrancq, L Orseau, S Legg, ArXiv abs/1711.098832017AI Safety Gridworlds</p>
<p>Explore. Z Lin, B Harrison, A Keech, M O Riedl, ArXiv abs/1709.03969Exploit or Listen: Combining Human Feedback and Policy Model to Speed up Deep Reinforcement Learning in 3D Worlds. 2017</p>
<p>Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes. N Lourie, R L Bras, Y Choi, V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, 2020. 2016Asynchronous methods for deep reinforcement learning. In International conference on machine learning</p>
<p>The nature, importance, and difficulty of machine ethics. J H Moor, IEEE intelligent systems. 2142006</p>
<p>Learning Norms from Stories: A Prior for Value Aligned Agents. M S A Nahian, S Frazier, M Riedl, B Harrison, Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. the AAAI/ACM Conference on AI, Ethics, and Society2020</p>
<p>Fine-Tuning a Transformer-Based Language Model to Avoid Generating Non-Normative Text. K Narasimhan, T Kulkarni, R Barzilay, X Peng, S Li, S Frazier, M Riedl, A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2015. 2020. 201919Language models are unsupervised multitask learners</p>
<p>Human compatible: Artificial intelligence and the problem of control. S Russell, 2019Penguin</p>
<p>Research priorities for robust and beneficial artificial intelligence. S Russell, D Dewey, M Tegmark, Ai Magazine. 3642015</p>
<p>Aligning superintelligence with human interests: A technical research agenda. Machine Intelligence Research Institute (MIRI) technical report 8. N Soares, B Fallenstein, B Stadie, P Abbeel, I Sutskever, arXiv:1703.01703KI-KÃ¼nstliche Intelligenz. 3342014. 2017. 2019Third-person imitation learning. arXiv preprintEfficient supervision for robot learning via imitation, simulation, and adaptation</p>
<p>Y Xu, M Fang, L Chen, Y Du, J T Zhou, C Zhang, Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games. 2020</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R Salakhutdinov, Q V Le, arXiv:1906.082372019arXiv preprint</p>
<p>Comprehensible Context-driven Text Game Playing. X Yin, J May, IEEE Conference on Games (CoG). 2019. 2019</p>
<p>Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning. T Zahavy, M Haroush, N Merlis, D J Mankowitz, S Mannor, 2018In NeurIPS</p>
<p>Using reinforcement learning to learn how to play text-based games. M Zelinka, ArXiv abs/1801.019992018</p>            </div>
        </div>

    </div>
</body>
</html>
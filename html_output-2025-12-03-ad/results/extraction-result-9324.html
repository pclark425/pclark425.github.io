<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9324 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9324</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9324</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-279250186</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.07436v1.pdf" target="_blank">Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</a></p>
                <p><strong>Paper Abstract:</strong> The recent emergence of multimodal large language models (LLMs) has introduced new opportunities for improving visual hazard recognition on construction sites. Unlike traditional computer vision models that rely on domain-specific training and extensive datasets, modern LLMs can interpret and describe complex visual scenes using simple natural language prompts. However, despite growing interest in their applications, there has been limited investigation into how different LLMs perform in safety-critical visual tasks within the construction domain. To address this gap, this study conducts a comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5, GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify potential hazards from real-world construction images. Each model was tested under three prompting strategies: zero-shot, few-shot, and chain-of-thought (CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated basic safety context and a hazard source mnemonic, and CoT provided step-by-step reasoning examples to scaffold model thinking. Quantitative analysis was performed using precision, recall, and F1-score metrics across all conditions. Results reveal that prompting strategy significantly influenced performance, with CoT prompting consistently producing higher accuracy across models. Additionally, LLM performance varied under different conditions, with GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also demonstrate the critical role of prompt design in enhancing the accuracy and consistency of multimodal LLMs for construction safety applications. This study offers actionable insights into the integration of prompt engineering and LLMs for practical hazard recognition, contributing to the development of more reliable AI-assisted safety systems.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9324.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9324.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3 Opus - ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3 Opus (zero-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of Claude-3 Opus on an image-based construction hazard recognition task using a minimal zero-shot prompt (no examples or domain mnemonic). Reported precision, recall, and F1 against expert-annotated ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify all potential safety hazards in real-world construction images (expert-annotated ground truth of 120 hazards across 16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot: standardized high-level instruction provided ('You're a construction safety expert... Identify all potential safety hazards for the construction activity in this image.') with no additional contextual information, mnemonics, or example annotations; each of 16 images fed to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against few-shot (Energy Wheel mnemonic + minimal examples) and chain-of-thought (Energy Wheel + 3 annotated CoT training examples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.420; recall: 0.288; F1-score: 0.339 (averaged across 16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>few-shot: precision 0.466, recall 0.619, F1 0.524; CoT: precision 0.552, recall 0.798, F1 0.646.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 few-shot vs zero-shot = +0.185; ΔF1 CoT vs zero-shot = +0.307; ΔF1 CoT vs few-shot = +0.122 (absolute F1 point differences).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper attributes poor zero-shot performance to lack of contextual scaffolding and task-specific cues; zero-shot frequently overpredicts hazards and misses many true hazards because the model receives no domain-specific mnemonic or worked examples.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Dataset: 16 images (zero-shot/few-shot); ground truth: 120 hazards labeled by 17 experts. Metrics computed per-image (TP/FP/FN) then averaged. Zero-shot prompt text standardized; separate accounts/computers used to avoid cross-condition contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9324.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3 Opus - FewShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3 Opus (few-shot prompting with Energy Wheel)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude-3 Opus evaluated with a few-shot prompt that included an Energy Wheel mnemonic and brief examples of hazard sources to aid hazard recognition from images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify potential safety hazards in construction images using minimal contextual cues (mnemonic + examples).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot: provided an Energy Wheel mnemonic (ten energy-source categories and example hazards) plus brief textual context before feeding 16 images; prompt asked to 'Identify all potential safety hazards for the construction activity in this image.'</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with zero-shot (no context) and chain-of-thought (Energy Wheel + 3 annotated CoT examples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.466; recall: 0.619; F1-score: 0.524 (averaged across 16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>zero-shot: precision 0.420, recall 0.288, F1 0.339; CoT: precision 0.552, recall 0.798, F1 0.646.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 few-shot vs zero-shot = +0.185; ΔF1 CoT vs few-shot = +0.122; ΔF1 CoT vs zero-shot = +0.307 (absolute F1 point differences).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper hypothesizes that providing a domain-specific cognitive scaffold (Energy Wheel) reduces task ambiguity and improves recall by guiding the model to scan for hazards by energy-source categories.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot prompt included visual mnemonic and examples; 16 images evaluated; precision/recall/F1 computed per-image and averaged. Few-shot used 1–3 exemplars conceptually (Energy Wheel + example hazards) but not the three annotated CoT training cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9324.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3 Opus - CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3 Opus (chain-of-thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude-3 Opus evaluated using chain-of-thought prompting: Energy Wheel + three annotated example images with step-by-step reasoning, then tested on remaining images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify hazards after interactive training with three annotated CoT examples (then evaluate on 13 images).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Chain-of-thought (CoT): orientation + Energy Wheel mnemonic, followed by three annotated example images demonstrating stepwise reasoning (describe scene elements then identify hazards), with interactive confirmation; then model asked to apply same structured approach to each of 13 test images.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with zero-shot and few-shot formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.552; recall: 0.798; F1-score: 0.646 (averaged across 13 test images; 3 images used in training excluded).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>zero-shot F1 0.339; few-shot F1 0.524 (as per Claude entries above).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 CoT vs zero-shot = +0.307; ΔF1 CoT vs few-shot = +0.122 (absolute F1 point differences for Claude).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper argues CoT scaffolds reasoning, prompting the model to decompose scenes (identify actors, equipment, materials) and map cues to hazard sources; this structured reasoning markedly increases recall and overall F1 and reduces inter-model variability.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CoT used 3 annotated training images; remaining 13 images tested. Metrics computed per-image then averaged. Prompting included interactive confirmation steps in training phase. Models were isolated per condition (different accounts/computers).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9324.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4.5 - ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4.5 (zero-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4.5 tested on construction image hazard identification using a minimal zero-shot prompt with no domain mnemonic or examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect all potential hazards in construction images without prior examples or domain scaffolds.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot: single high-level instruction ('You're a construction safety expert... Identify all potential safety hazards for the construction activity in this image.') with each image provided to the model (16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to few-shot (Energy Wheel + minimal examples) and CoT (Energy Wheel + 3 annotated CoT examples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.315; recall: 0.376; F1-score: 0.339 (averaged across 16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>few-shot: precision 0.599, recall 0.699, F1 0.623; CoT: precision 0.592, recall 0.857, F1 0.693.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 few-shot vs zero-shot = +0.284; ΔF1 CoT vs zero-shot = +0.354; ΔF1 CoT vs few-shot = +0.070 (absolute F1 point differences).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper notes GPT-4.5 benefits substantially from added context and CoT training; CoT especially increases recall by encouraging stepwise scene decomposition and mapping to hazard categories.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>16 images for zero-shot and few-shot; CoT training used 3 annotated examples and evaluated on 13 images; metrics per-image averaged. Statistical tests reported significant condition effects (repeated-measures ANOVA).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9324.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4.5 - FewShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4.5 (few-shot prompting with Energy Wheel)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4.5 evaluated with a few-shot prompt containing the Energy Wheel mnemonic and examples of hazard sources, improving performance markedly over zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify hazards in images with minimal domain scaffolding (mnemonic + examples).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot: Energy Wheel mnemonic + examples provided followed by the same image-level instruction; 16 images evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with zero-shot and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.599; recall: 0.699; F1-score: 0.623 (averaged across 16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>zero-shot F1 0.339; CoT F1 0.693.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 few-shot vs zero-shot = +0.284; ΔF1 CoT vs few-shot = +0.070; ΔF1 CoT vs zero-shot = +0.354.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper suggests few-shot Energy Wheel reduces ambiguity enabling better recall; CoT adds further stepwise reasoning to capture more hazards (higher recall).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot included Energy Wheel diagram and example hazards; performance averaged across 16 images. Statistical tests show few-shot significantly improves over zero-shot (group ΔF1 ≈ +0.223 overall).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9324.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4.5 - CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4.5 (chain-of-thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4.5 tested under chain-of-thought prompting (Energy Wheel + 3 annotated CoT examples), producing the highest per-model F1 among tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Apply stepwise annotated reasoning from training examples to identify hazards in test images (13 test images after 3 training examples).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>CoT: orientation + Energy Wheel + three annotated examples demonstrating step-by-step identification (describe scene, then hazards); interactive confirmations used; test on 13 images.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with zero-shot and few-shot formats.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.592; recall: 0.857; F1-score: 0.693 (averaged across 13 test images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>zero-shot F1 0.339; few-shot F1 0.623.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 CoT vs zero-shot = +0.354; ΔF1 CoT vs few-shot = +0.070 (absolute F1 point differences).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors argue CoT elicits deeper contextual reasoning and scene decomposition, which increases recall substantially and yields the largest absolute F1 gain among formats, effectively narrowing model differences.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>3 images used as CoT training examples; remaining 13 test images; metrics averaged. Group-level ANOVA shows CoT yields highest mean F1 (M=0.636) and significant effects (p<0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9324.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-o3 - ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-o3 (zero-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-o3 evaluated with a basic zero-shot instruction to identify hazards from construction images, yielding low baseline recall and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-o3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect hazards from images with only a high-level instruction and no domain examples or mnemonics.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot: same standardized instruction as other models; 16 images evaluated with no additional context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with few-shot (Energy Wheel) and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.289; recall: 0.326; F1-score: 0.308 (averaged across 16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>few-shot: precision 0.565, recall 0.722, F1 0.629; CoT: precision 0.561, recall 0.758, F1 0.641.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 few-shot vs zero-shot = +0.321; ΔF1 CoT vs zero-shot = +0.333; ΔF1 CoT vs few-shot = +0.012 (absolute F1 point differences).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper indicates few-shot Energy Wheel improves recall substantially for GPT-o3, and CoT maintains or slightly improves over few-shot by enforcing structured reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>16 images for zero/few-shot; CoT used 3 training examples then 13 test images; TP/FP/FN computed per-image versus expert labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9324.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-o3 - FewShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-o3 (few-shot prompting with Energy Wheel)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-o3 evaluated with the Energy Wheel mnemonic and brief examples, showing large recall gains and substantial F1 improvement versus zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-o3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify hazards using a minimal domain-specific mnemonic and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot: Energy Wheel mnemonic + example hazard sources provided, then 16 images processed with same 'identify hazards' instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with zero-shot and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.565; recall: 0.722; F1-score: 0.629 (averaged across 16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>zero-shot F1 0.308; CoT F1 0.641.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 few-shot vs zero-shot = +0.321; ΔF1 CoT vs few-shot = +0.012; ΔF1 CoT vs zero-shot = +0.333.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper suggests that the Energy Wheel helps the model systematically scan for hazards by energy category, driving recall improvements; CoT yields marginal additional gains beyond few-shot for GPT-o3.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot prompt included Energy Wheel diagram and example hazards. Statistical pairwise tests show GPT-o3 among top performers in few-shot and significantly outperformed lower-tier models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9324.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-o3 - CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-o3 (chain-of-thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-o3 evaluated with CoT training (Energy Wheel + 3 annotated examples), achieving high recall and an F1 comparable to top models under CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-o3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Apply step-by-step annotated reasoning learned from three examples to identify hazards on 13 test images.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>CoT: Energy Wheel + three annotated CoT examples demonstrating scene description and hazard mapping; test images evaluated sequentially with the structured approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with zero-shot and few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.561; recall: 0.758; F1-score: 0.641 (averaged across 13 test images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>zero-shot F1 0.308; few-shot F1 0.629.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 CoT vs zero-shot = +0.333; ΔF1 CoT vs few-shot = +0.012 (absolute F1 point differences).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper reports CoT equalizes model performance and that GPT-o3 benefits from CoT similarly to other top models; CoT encourages more complete hazard enumeration through explicit reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CoT training used 3 annotated images; test set comprised 13 images; metrics averaged per-image. Under CoT, inter-model differences diminished.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9324.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o - ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (zero-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o baseline zero-shot evaluation on construction hazard recognition showed the weakest zero-shot F1 among tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify hazards in each image with only a minimal instruction and no domain-specific mnemonic or examples.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot: standard instruction provided; 16 images evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with few-shot (Energy Wheel) and CoT (Energy Wheel + 3 annotated examples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.247; recall: 0.299; F1-score: 0.269 (averaged across 16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>few-shot: precision 0.332, recall 0.684, F1 0.441; CoT: precision 0.503, recall 0.686, F1 0.576.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 few-shot vs zero-shot = +0.172; ΔF1 CoT vs zero-shot = +0.307; ΔF1 CoT vs few-shot = +0.135 (absolute F1 point differences).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper highlights GPT-4o's weak zero-shot performance but substantial improvement when given examples or CoT guidance, indicating architecture differences can be compensated by prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>16 images in zero/few-shot; CoT used 3 training images then 13 test images. Statistical tests showed GPT-4o underperformed in zero-shot and improved markedly under few-shot and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9324.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o - FewShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (few-shot prompting with Energy Wheel)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o was evaluated with Energy Wheel few-shot prompting and showed large recall gains compared to zero-shot, improving F1 substantially though remaining below top-tier models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use the Energy Wheel mnemonic as minimal contextual guidance to identify hazards in 16 images.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot: Energy Wheel mnemonic + example hazards provided prior to image-by-image hazard identification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with zero-shot and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.332; recall: 0.684; F1-score: 0.441 (averaged across 16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>zero-shot F1 0.269; CoT F1 0.576.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 few-shot vs zero-shot = +0.172; ΔF1 CoT vs few-shot = +0.135; ΔF1 CoT vs zero-shot = +0.307.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors interpret results as evidence that few-shot domain mnemonics can substantially improve recall, and CoT yields additional gains by structuring reasoning; GPT-4o shows pronounced sensitivity to prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used Energy Wheel; statistical tests show GPT-4o significantly worse than top models in zero-shot and few-shot but improved under CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9324.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o - CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (chain-of-thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o evaluated with CoT training (Energy Wheel + 3 annotated examples) improved dramatically from zero-shot, reaching respectable F1 though still numerically lower than best models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Apply stepwise CoT reasoning learned from annotated examples to identify hazards in 13 test images.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>CoT: Energy Wheel + three annotated CoT examples; describe scene elements then identify hazards for each test image.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with zero-shot and few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.503; recall: 0.686; F1-score: 0.576 (averaged across 13 test images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>zero-shot F1 0.269; few-shot F1 0.441.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 CoT vs zero-shot = +0.307; ΔF1 CoT vs few-shot = +0.135 (absolute F1 point differences).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper contends that CoT reduces architecture-driven gaps; for GPT-4o CoT accomplishes substantial gains by eliciting structured analysis and comprehensive hazard enumeration.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CoT used 3 annotated examples with interactive confirmations; test set = 13 images; performance averaged and compared with other formats via ANOVA and Tukey HSD tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e9324.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 2.0 - ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 2.0 (zero-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini 2.0 baseline zero-shot evaluation on the 16-image construction hazard recognition task, showing low F1 similar to other zero-shot results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify hazards from construction images using only a high-level instruction and no domain examples.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot: standard high-level instruction provided; 16 images processed with no additional context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with few-shot (Energy Wheel) and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.309; recall: 0.328; F1-score: 0.312 (averaged across 16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>few-shot: precision 0.397, recall 0.622, F1 0.468; CoT: precision 0.555, recall 0.727, F1 0.625.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 few-shot vs zero-shot = +0.156; ΔF1 CoT vs zero-shot = +0.313; ΔF1 CoT vs few-shot = +0.157 (absolute F1 point differences).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper notes Gemini shows moderate baseline but benefits from domain mnemonic and CoT; Energy Wheel increases recall and CoT further improves structured detection.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>16 images for zero/few-shot; CoT used 3 training images then 13 test images. Statistical comparisons show Gemini improved under few-shot and CoT, though often not reaching top-tier significance in pairwise tests.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e9324.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 2.0 - FewShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 2.0 (few-shot prompting with Energy Wheel)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini 2.0 evaluated with Energy Wheel few-shot prompting, producing improved recall and F1 relative to zero-shot but below best performers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify hazards using an Energy Wheel mnemonic and example hazard sources as minimal contextual guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot: Energy Wheel mnemonic + examples included before image assessment; 16 images evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with zero-shot and CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.397; recall: 0.622; F1-score: 0.468 (averaged across 16 images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>zero-shot F1 0.312; CoT F1 0.625.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 few-shot vs zero-shot = +0.156; ΔF1 CoT vs few-shot = +0.157; ΔF1 CoT vs zero-shot = +0.313 (absolute F1 point differences).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper suggests that the Energy Wheel focuses the model's attention on hazard categories and that CoT adds structured reasoning to further improve completeness and explanatory richness.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot prompt consisted of Energy Wheel and example hazards; metrics averaged across 16 test images. Pairwise comparisons showed Gemini significantly lagged behind top models under few-shot but improved under CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e9324.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 2.0 - CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 2.0 (chain-of-thought prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini 2.0 evaluated with CoT (Energy Wheel + 3 annotated examples) and achieved substantial gains in recall and F1, becoming comparable to mid-high performers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use annotated, stepwise CoT training examples to guide hazard identification on 13 test images.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>CoT: orientation + Energy Wheel + three annotated training images with step-by-step reasoning; test on 13 images.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to zero-shot and few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>precision: 0.555; recall: 0.727; F1-score: 0.625 (averaged across 13 test images).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>zero-shot F1 0.312; few-shot F1 0.468.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>ΔF1 CoT vs zero-shot = +0.313; ΔF1 CoT vs few-shot = +0.157 (absolute F1 point differences).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paper concludes CoT fosters explanatory outputs and equalizes models; Gemini benefits from CoT to a similar degree as other models, improving recall and producing usable CoT rationales for training materials.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CoT used 3 annotated training images; evaluations averaged across 13 test images. ANOVA showed CoT led to highest mean F1 across models; Tukey post-hoc found no significant pairwise differences under CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9324.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e9324.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting Strategies - Aggregate Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting strategies (zero-shot, few-shot, chain-of-thought) aggregate comparison</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate analysis across five multimodal LLMs comparing zero-shot, few-shot (Energy Wheel), and chain-of-thought (3 annotated CoT examples) prompting for construction hazard recognition; CoT produced the highest mean F1 and significantly outperformed the others.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>aggregate (Claude-3 Opus, GPT-4.5, GPT-o3, GPT-4o, Gemini 2.0)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Construction hazard recognition (image-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare how three prompt presentation formats affect multimodal LLM performance on image-based hazard detection, using precision, recall, and F1 averaged across models.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Three formats: Zero-shot (no context), Few-shot (Energy Wheel mnemonic + examples), Chain-of-thought (Energy Wheel + 3 annotated CoT examples with interactive confirmation). CoT training excluded the 3 training images from test (n=13).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mean F1 by condition across models: Zero-shot mean F1 = 0.314 (SD 0.052); Few-shot mean F1 = 0.537 (SD 0.135); CoT mean F1 = 0.636 (SD 0.117).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Pairwise differences: CoT vs Zero-shot ΔF1 = +0.3226 (p < 0.001); Few-shot vs Zero-shot ΔF1 = +0.2235 (p < 0.001); CoT vs Few-shot ΔF1 = +0.0992 (p = 0.0473).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Absolute effect sizes reported: CoT vs Zero-shot +0.3226 F1 (highly significant), Few-shot vs Zero-shot +0.2235 F1 (highly significant), CoT vs Few-shot +0.0992 F1 (marginally significant).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that increased scaffolding (few-shot mnemonic then CoT examples) reduces task ambiguity, elicits stepwise scene decomposition, increases recall and interpretability, and reduces inter-model variance; CoT is the most effective because it explicitly models intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Repeated-measures ANOVA on mean F1 (within-subjects: same 5 models evaluated in all 3 conditions). Statistical assumptions checked (Shapiro-Wilk, Levene). Tukey HSD used for pairwise comparisons; CoT and few-shot significantly better than zero-shot; CoT marginally better than few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>Large Language Models are Zero-Shot Reasoners <em>(Rating: 2)</em></li>
                <li>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing <em>(Rating: 1)</em></li>
                <li>Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9324",
    "paper_id": "paper-279250186",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Claude-3 Opus - ZeroShot",
            "name_full": "Claude-3 Opus (zero-shot prompting)",
            "brief_description": "Evaluation of Claude-3 Opus on an image-based construction hazard recognition task using a minimal zero-shot prompt (no examples or domain mnemonic). Reported precision, recall, and F1 against expert-annotated ground truth.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "Claude-3 Opus",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Identify all potential safety hazards in real-world construction images (expert-annotated ground truth of 120 hazards across 16 images).",
            "presentation_format": "Zero-shot: standardized high-level instruction provided ('You're a construction safety expert... Identify all potential safety hazards for the construction activity in this image.') with no additional contextual information, mnemonics, or example annotations; each of 16 images fed to the model.",
            "comparison_format": "Compared against few-shot (Energy Wheel mnemonic + minimal examples) and chain-of-thought (Energy Wheel + 3 annotated CoT training examples).",
            "performance": "precision: 0.420; recall: 0.288; F1-score: 0.339 (averaged across 16 images).",
            "performance_comparison": "few-shot: precision 0.466, recall 0.619, F1 0.524; CoT: precision 0.552, recall 0.798, F1 0.646.",
            "format_effect_size": "ΔF1 few-shot vs zero-shot = +0.185; ΔF1 CoT vs zero-shot = +0.307; ΔF1 CoT vs few-shot = +0.122 (absolute F1 point differences).",
            "explanation_or_hypothesis": "The paper attributes poor zero-shot performance to lack of contextual scaffolding and task-specific cues; zero-shot frequently overpredicts hazards and misses many true hazards because the model receives no domain-specific mnemonic or worked examples.",
            "null_or_negative_result": false,
            "experimental_details": "Dataset: 16 images (zero-shot/few-shot); ground truth: 120 hazards labeled by 17 experts. Metrics computed per-image (TP/FP/FN) then averaged. Zero-shot prompt text standardized; separate accounts/computers used to avoid cross-condition contamination.",
            "uuid": "e9324.0",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Claude-3 Opus - FewShot",
            "name_full": "Claude-3 Opus (few-shot prompting with Energy Wheel)",
            "brief_description": "Claude-3 Opus evaluated with a few-shot prompt that included an Energy Wheel mnemonic and brief examples of hazard sources to aid hazard recognition from images.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "Claude-3 Opus",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Identify potential safety hazards in construction images using minimal contextual cues (mnemonic + examples).",
            "presentation_format": "Few-shot: provided an Energy Wheel mnemonic (ten energy-source categories and example hazards) plus brief textual context before feeding 16 images; prompt asked to 'Identify all potential safety hazards for the construction activity in this image.'",
            "comparison_format": "Compared with zero-shot (no context) and chain-of-thought (Energy Wheel + 3 annotated CoT examples).",
            "performance": "precision: 0.466; recall: 0.619; F1-score: 0.524 (averaged across 16 images).",
            "performance_comparison": "zero-shot: precision 0.420, recall 0.288, F1 0.339; CoT: precision 0.552, recall 0.798, F1 0.646.",
            "format_effect_size": "ΔF1 few-shot vs zero-shot = +0.185; ΔF1 CoT vs few-shot = +0.122; ΔF1 CoT vs zero-shot = +0.307 (absolute F1 point differences).",
            "explanation_or_hypothesis": "Paper hypothesizes that providing a domain-specific cognitive scaffold (Energy Wheel) reduces task ambiguity and improves recall by guiding the model to scan for hazards by energy-source categories.",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot prompt included visual mnemonic and examples; 16 images evaluated; precision/recall/F1 computed per-image and averaged. Few-shot used 1–3 exemplars conceptually (Energy Wheel + example hazards) but not the three annotated CoT training cases.",
            "uuid": "e9324.1",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Claude-3 Opus - CoT",
            "name_full": "Claude-3 Opus (chain-of-thought prompting)",
            "brief_description": "Claude-3 Opus evaluated using chain-of-thought prompting: Energy Wheel + three annotated example images with step-by-step reasoning, then tested on remaining images.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "Claude-3 Opus",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Identify hazards after interactive training with three annotated CoT examples (then evaluate on 13 images).",
            "presentation_format": "Chain-of-thought (CoT): orientation + Energy Wheel mnemonic, followed by three annotated example images demonstrating stepwise reasoning (describe scene elements then identify hazards), with interactive confirmation; then model asked to apply same structured approach to each of 13 test images.",
            "comparison_format": "Compared with zero-shot and few-shot formats.",
            "performance": "precision: 0.552; recall: 0.798; F1-score: 0.646 (averaged across 13 test images; 3 images used in training excluded).",
            "performance_comparison": "zero-shot F1 0.339; few-shot F1 0.524 (as per Claude entries above).",
            "format_effect_size": "ΔF1 CoT vs zero-shot = +0.307; ΔF1 CoT vs few-shot = +0.122 (absolute F1 point differences for Claude).",
            "explanation_or_hypothesis": "Paper argues CoT scaffolds reasoning, prompting the model to decompose scenes (identify actors, equipment, materials) and map cues to hazard sources; this structured reasoning markedly increases recall and overall F1 and reduces inter-model variability.",
            "null_or_negative_result": false,
            "experimental_details": "CoT used 3 annotated training images; remaining 13 images tested. Metrics computed per-image then averaged. Prompting included interactive confirmation steps in training phase. Models were isolated per condition (different accounts/computers).",
            "uuid": "e9324.2",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4.5 - ZeroShot",
            "name_full": "GPT-4.5 (zero-shot prompting)",
            "brief_description": "GPT-4.5 tested on construction image hazard identification using a minimal zero-shot prompt with no domain mnemonic or examples.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "GPT-4.5",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Detect all potential hazards in construction images without prior examples or domain scaffolds.",
            "presentation_format": "Zero-shot: single high-level instruction ('You're a construction safety expert... Identify all potential safety hazards for the construction activity in this image.') with each image provided to the model (16 images).",
            "comparison_format": "Compared to few-shot (Energy Wheel + minimal examples) and CoT (Energy Wheel + 3 annotated CoT examples).",
            "performance": "precision: 0.315; recall: 0.376; F1-score: 0.339 (averaged across 16 images).",
            "performance_comparison": "few-shot: precision 0.599, recall 0.699, F1 0.623; CoT: precision 0.592, recall 0.857, F1 0.693.",
            "format_effect_size": "ΔF1 few-shot vs zero-shot = +0.284; ΔF1 CoT vs zero-shot = +0.354; ΔF1 CoT vs few-shot = +0.070 (absolute F1 point differences).",
            "explanation_or_hypothesis": "Paper notes GPT-4.5 benefits substantially from added context and CoT training; CoT especially increases recall by encouraging stepwise scene decomposition and mapping to hazard categories.",
            "null_or_negative_result": false,
            "experimental_details": "16 images for zero-shot and few-shot; CoT training used 3 annotated examples and evaluated on 13 images; metrics per-image averaged. Statistical tests reported significant condition effects (repeated-measures ANOVA).",
            "uuid": "e9324.3",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4.5 - FewShot",
            "name_full": "GPT-4.5 (few-shot prompting with Energy Wheel)",
            "brief_description": "GPT-4.5 evaluated with a few-shot prompt containing the Energy Wheel mnemonic and examples of hazard sources, improving performance markedly over zero-shot.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "GPT-4.5",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Identify hazards in images with minimal domain scaffolding (mnemonic + examples).",
            "presentation_format": "Few-shot: Energy Wheel mnemonic + examples provided followed by the same image-level instruction; 16 images evaluated.",
            "comparison_format": "Compared with zero-shot and CoT.",
            "performance": "precision: 0.599; recall: 0.699; F1-score: 0.623 (averaged across 16 images).",
            "performance_comparison": "zero-shot F1 0.339; CoT F1 0.693.",
            "format_effect_size": "ΔF1 few-shot vs zero-shot = +0.284; ΔF1 CoT vs few-shot = +0.070; ΔF1 CoT vs zero-shot = +0.354.",
            "explanation_or_hypothesis": "Paper suggests few-shot Energy Wheel reduces ambiguity enabling better recall; CoT adds further stepwise reasoning to capture more hazards (higher recall).",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot included Energy Wheel diagram and example hazards; performance averaged across 16 images. Statistical tests show few-shot significantly improves over zero-shot (group ΔF1 ≈ +0.223 overall).",
            "uuid": "e9324.4",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4.5 - CoT",
            "name_full": "GPT-4.5 (chain-of-thought prompting)",
            "brief_description": "GPT-4.5 tested under chain-of-thought prompting (Energy Wheel + 3 annotated CoT examples), producing the highest per-model F1 among tested models.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "GPT-4.5",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Apply stepwise annotated reasoning from training examples to identify hazards in test images (13 test images after 3 training examples).",
            "presentation_format": "CoT: orientation + Energy Wheel + three annotated examples demonstrating step-by-step identification (describe scene, then hazards); interactive confirmations used; test on 13 images.",
            "comparison_format": "Compared with zero-shot and few-shot formats.",
            "performance": "precision: 0.592; recall: 0.857; F1-score: 0.693 (averaged across 13 test images).",
            "performance_comparison": "zero-shot F1 0.339; few-shot F1 0.623.",
            "format_effect_size": "ΔF1 CoT vs zero-shot = +0.354; ΔF1 CoT vs few-shot = +0.070 (absolute F1 point differences).",
            "explanation_or_hypothesis": "Authors argue CoT elicits deeper contextual reasoning and scene decomposition, which increases recall substantially and yields the largest absolute F1 gain among formats, effectively narrowing model differences.",
            "null_or_negative_result": false,
            "experimental_details": "3 images used as CoT training examples; remaining 13 test images; metrics averaged. Group-level ANOVA shows CoT yields highest mean F1 (M=0.636) and significant effects (p&lt;0.001).",
            "uuid": "e9324.5",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-o3 - ZeroShot",
            "name_full": "GPT-o3 (zero-shot prompting)",
            "brief_description": "GPT-o3 evaluated with a basic zero-shot instruction to identify hazards from construction images, yielding low baseline recall and F1.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "GPT-o3",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Detect hazards from images with only a high-level instruction and no domain examples or mnemonics.",
            "presentation_format": "Zero-shot: same standardized instruction as other models; 16 images evaluated with no additional context.",
            "comparison_format": "Compared with few-shot (Energy Wheel) and CoT.",
            "performance": "precision: 0.289; recall: 0.326; F1-score: 0.308 (averaged across 16 images).",
            "performance_comparison": "few-shot: precision 0.565, recall 0.722, F1 0.629; CoT: precision 0.561, recall 0.758, F1 0.641.",
            "format_effect_size": "ΔF1 few-shot vs zero-shot = +0.321; ΔF1 CoT vs zero-shot = +0.333; ΔF1 CoT vs few-shot = +0.012 (absolute F1 point differences).",
            "explanation_or_hypothesis": "Paper indicates few-shot Energy Wheel improves recall substantially for GPT-o3, and CoT maintains or slightly improves over few-shot by enforcing structured reasoning.",
            "null_or_negative_result": false,
            "experimental_details": "16 images for zero/few-shot; CoT used 3 training examples then 13 test images; TP/FP/FN computed per-image versus expert labels.",
            "uuid": "e9324.6",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-o3 - FewShot",
            "name_full": "GPT-o3 (few-shot prompting with Energy Wheel)",
            "brief_description": "GPT-o3 evaluated with the Energy Wheel mnemonic and brief examples, showing large recall gains and substantial F1 improvement versus zero-shot.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "GPT-o3",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Identify hazards using a minimal domain-specific mnemonic and examples.",
            "presentation_format": "Few-shot: Energy Wheel mnemonic + example hazard sources provided, then 16 images processed with same 'identify hazards' instruction.",
            "comparison_format": "Compared with zero-shot and CoT.",
            "performance": "precision: 0.565; recall: 0.722; F1-score: 0.629 (averaged across 16 images).",
            "performance_comparison": "zero-shot F1 0.308; CoT F1 0.641.",
            "format_effect_size": "ΔF1 few-shot vs zero-shot = +0.321; ΔF1 CoT vs few-shot = +0.012; ΔF1 CoT vs zero-shot = +0.333.",
            "explanation_or_hypothesis": "Paper suggests that the Energy Wheel helps the model systematically scan for hazards by energy category, driving recall improvements; CoT yields marginal additional gains beyond few-shot for GPT-o3.",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot prompt included Energy Wheel diagram and example hazards. Statistical pairwise tests show GPT-o3 among top performers in few-shot and significantly outperformed lower-tier models.",
            "uuid": "e9324.7",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-o3 - CoT",
            "name_full": "GPT-o3 (chain-of-thought prompting)",
            "brief_description": "GPT-o3 evaluated with CoT training (Energy Wheel + 3 annotated examples), achieving high recall and an F1 comparable to top models under CoT.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "GPT-o3",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Apply step-by-step annotated reasoning learned from three examples to identify hazards on 13 test images.",
            "presentation_format": "CoT: Energy Wheel + three annotated CoT examples demonstrating scene description and hazard mapping; test images evaluated sequentially with the structured approach.",
            "comparison_format": "Compared with zero-shot and few-shot prompts.",
            "performance": "precision: 0.561; recall: 0.758; F1-score: 0.641 (averaged across 13 test images).",
            "performance_comparison": "zero-shot F1 0.308; few-shot F1 0.629.",
            "format_effect_size": "ΔF1 CoT vs zero-shot = +0.333; ΔF1 CoT vs few-shot = +0.012 (absolute F1 point differences).",
            "explanation_or_hypothesis": "Paper reports CoT equalizes model performance and that GPT-o3 benefits from CoT similarly to other top models; CoT encourages more complete hazard enumeration through explicit reasoning steps.",
            "null_or_negative_result": false,
            "experimental_details": "CoT training used 3 annotated images; test set comprised 13 images; metrics averaged per-image. Under CoT, inter-model differences diminished.",
            "uuid": "e9324.8",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4o - ZeroShot",
            "name_full": "GPT-4o (zero-shot prompting)",
            "brief_description": "GPT-4o baseline zero-shot evaluation on construction hazard recognition showed the weakest zero-shot F1 among tested models.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Identify hazards in each image with only a minimal instruction and no domain-specific mnemonic or examples.",
            "presentation_format": "Zero-shot: standard instruction provided; 16 images evaluated.",
            "comparison_format": "Compared with few-shot (Energy Wheel) and CoT (Energy Wheel + 3 annotated examples).",
            "performance": "precision: 0.247; recall: 0.299; F1-score: 0.269 (averaged across 16 images).",
            "performance_comparison": "few-shot: precision 0.332, recall 0.684, F1 0.441; CoT: precision 0.503, recall 0.686, F1 0.576.",
            "format_effect_size": "ΔF1 few-shot vs zero-shot = +0.172; ΔF1 CoT vs zero-shot = +0.307; ΔF1 CoT vs few-shot = +0.135 (absolute F1 point differences).",
            "explanation_or_hypothesis": "Paper highlights GPT-4o's weak zero-shot performance but substantial improvement when given examples or CoT guidance, indicating architecture differences can be compensated by prompt engineering.",
            "null_or_negative_result": false,
            "experimental_details": "16 images in zero/few-shot; CoT used 3 training images then 13 test images. Statistical tests showed GPT-4o underperformed in zero-shot and improved markedly under few-shot and CoT.",
            "uuid": "e9324.9",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4o - FewShot",
            "name_full": "GPT-4o (few-shot prompting with Energy Wheel)",
            "brief_description": "GPT-4o was evaluated with Energy Wheel few-shot prompting and showed large recall gains compared to zero-shot, improving F1 substantially though remaining below top-tier models.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Use the Energy Wheel mnemonic as minimal contextual guidance to identify hazards in 16 images.",
            "presentation_format": "Few-shot: Energy Wheel mnemonic + example hazards provided prior to image-by-image hazard identification.",
            "comparison_format": "Compared with zero-shot and CoT.",
            "performance": "precision: 0.332; recall: 0.684; F1-score: 0.441 (averaged across 16 images).",
            "performance_comparison": "zero-shot F1 0.269; CoT F1 0.576.",
            "format_effect_size": "ΔF1 few-shot vs zero-shot = +0.172; ΔF1 CoT vs few-shot = +0.135; ΔF1 CoT vs zero-shot = +0.307.",
            "explanation_or_hypothesis": "Authors interpret results as evidence that few-shot domain mnemonics can substantially improve recall, and CoT yields additional gains by structuring reasoning; GPT-4o shows pronounced sensitivity to prompt format.",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot used Energy Wheel; statistical tests show GPT-4o significantly worse than top models in zero-shot and few-shot but improved under CoT.",
            "uuid": "e9324.10",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4o - CoT",
            "name_full": "GPT-4o (chain-of-thought prompting)",
            "brief_description": "GPT-4o evaluated with CoT training (Energy Wheel + 3 annotated examples) improved dramatically from zero-shot, reaching respectable F1 though still numerically lower than best models.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Apply stepwise CoT reasoning learned from annotated examples to identify hazards in 13 test images.",
            "presentation_format": "CoT: Energy Wheel + three annotated CoT examples; describe scene elements then identify hazards for each test image.",
            "comparison_format": "Compared with zero-shot and few-shot prompting.",
            "performance": "precision: 0.503; recall: 0.686; F1-score: 0.576 (averaged across 13 test images).",
            "performance_comparison": "zero-shot F1 0.269; few-shot F1 0.441.",
            "format_effect_size": "ΔF1 CoT vs zero-shot = +0.307; ΔF1 CoT vs few-shot = +0.135 (absolute F1 point differences).",
            "explanation_or_hypothesis": "Paper contends that CoT reduces architecture-driven gaps; for GPT-4o CoT accomplishes substantial gains by eliciting structured analysis and comprehensive hazard enumeration.",
            "null_or_negative_result": false,
            "experimental_details": "CoT used 3 annotated examples with interactive confirmations; test set = 13 images; performance averaged and compared with other formats via ANOVA and Tukey HSD tests.",
            "uuid": "e9324.11",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Gemini 2.0 - ZeroShot",
            "name_full": "Gemini 2.0 (zero-shot prompting)",
            "brief_description": "Gemini 2.0 baseline zero-shot evaluation on the 16-image construction hazard recognition task, showing low F1 similar to other zero-shot results.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "Gemini 2.0",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Identify hazards from construction images using only a high-level instruction and no domain examples.",
            "presentation_format": "Zero-shot: standard high-level instruction provided; 16 images processed with no additional context.",
            "comparison_format": "Compared with few-shot (Energy Wheel) and CoT.",
            "performance": "precision: 0.309; recall: 0.328; F1-score: 0.312 (averaged across 16 images).",
            "performance_comparison": "few-shot: precision 0.397, recall 0.622, F1 0.468; CoT: precision 0.555, recall 0.727, F1 0.625.",
            "format_effect_size": "ΔF1 few-shot vs zero-shot = +0.156; ΔF1 CoT vs zero-shot = +0.313; ΔF1 CoT vs few-shot = +0.157 (absolute F1 point differences).",
            "explanation_or_hypothesis": "Paper notes Gemini shows moderate baseline but benefits from domain mnemonic and CoT; Energy Wheel increases recall and CoT further improves structured detection.",
            "null_or_negative_result": false,
            "experimental_details": "16 images for zero/few-shot; CoT used 3 training images then 13 test images. Statistical comparisons show Gemini improved under few-shot and CoT, though often not reaching top-tier significance in pairwise tests.",
            "uuid": "e9324.12",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Gemini 2.0 - FewShot",
            "name_full": "Gemini 2.0 (few-shot prompting with Energy Wheel)",
            "brief_description": "Gemini 2.0 evaluated with Energy Wheel few-shot prompting, producing improved recall and F1 relative to zero-shot but below best performers.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "Gemini 2.0",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Identify hazards using an Energy Wheel mnemonic and example hazard sources as minimal contextual guidance.",
            "presentation_format": "Few-shot: Energy Wheel mnemonic + examples included before image assessment; 16 images evaluated.",
            "comparison_format": "Compared with zero-shot and CoT prompting.",
            "performance": "precision: 0.397; recall: 0.622; F1-score: 0.468 (averaged across 16 images).",
            "performance_comparison": "zero-shot F1 0.312; CoT F1 0.625.",
            "format_effect_size": "ΔF1 few-shot vs zero-shot = +0.156; ΔF1 CoT vs few-shot = +0.157; ΔF1 CoT vs zero-shot = +0.313 (absolute F1 point differences).",
            "explanation_or_hypothesis": "Paper suggests that the Energy Wheel focuses the model's attention on hazard categories and that CoT adds structured reasoning to further improve completeness and explanatory richness.",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot prompt consisted of Energy Wheel and example hazards; metrics averaged across 16 test images. Pairwise comparisons showed Gemini significantly lagged behind top models under few-shot but improved under CoT.",
            "uuid": "e9324.13",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Gemini 2.0 - CoT",
            "name_full": "Gemini 2.0 (chain-of-thought prompting)",
            "brief_description": "Gemini 2.0 evaluated with CoT (Energy Wheel + 3 annotated examples) and achieved substantial gains in recall and F1, becoming comparable to mid-high performers.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "Gemini 2.0",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Use annotated, stepwise CoT training examples to guide hazard identification on 13 test images.",
            "presentation_format": "CoT: orientation + Energy Wheel + three annotated training images with step-by-step reasoning; test on 13 images.",
            "comparison_format": "Compared to zero-shot and few-shot prompting.",
            "performance": "precision: 0.555; recall: 0.727; F1-score: 0.625 (averaged across 13 test images).",
            "performance_comparison": "zero-shot F1 0.312; few-shot F1 0.468.",
            "format_effect_size": "ΔF1 CoT vs zero-shot = +0.313; ΔF1 CoT vs few-shot = +0.157 (absolute F1 point differences).",
            "explanation_or_hypothesis": "Paper concludes CoT fosters explanatory outputs and equalizes models; Gemini benefits from CoT to a similar degree as other models, improving recall and producing usable CoT rationales for training materials.",
            "null_or_negative_result": false,
            "experimental_details": "CoT used 3 annotated training images; evaluations averaged across 13 test images. ANOVA showed CoT led to highest mean F1 across models; Tukey post-hoc found no significant pairwise differences under CoT.",
            "uuid": "e9324.14",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Prompting Strategies - Aggregate Effect",
            "name_full": "Prompting strategies (zero-shot, few-shot, chain-of-thought) aggregate comparison",
            "brief_description": "Aggregate analysis across five multimodal LLMs comparing zero-shot, few-shot (Energy Wheel), and chain-of-thought (3 annotated CoT examples) prompting for construction hazard recognition; CoT produced the highest mean F1 and significantly outperformed the others.",
            "citation_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
            "mention_or_use": "use",
            "model_name": "aggregate (Claude-3 Opus, GPT-4.5, GPT-o3, GPT-4o, Gemini 2.0)",
            "model_size": null,
            "task_name": "Construction hazard recognition (image-based)",
            "task_description": "Compare how three prompt presentation formats affect multimodal LLM performance on image-based hazard detection, using precision, recall, and F1 averaged across models.",
            "presentation_format": "Three formats: Zero-shot (no context), Few-shot (Energy Wheel mnemonic + examples), Chain-of-thought (Energy Wheel + 3 annotated CoT examples with interactive confirmation). CoT training excluded the 3 training images from test (n=13).",
            "comparison_format": null,
            "performance": "Mean F1 by condition across models: Zero-shot mean F1 = 0.314 (SD 0.052); Few-shot mean F1 = 0.537 (SD 0.135); CoT mean F1 = 0.636 (SD 0.117).",
            "performance_comparison": "Pairwise differences: CoT vs Zero-shot ΔF1 = +0.3226 (p &lt; 0.001); Few-shot vs Zero-shot ΔF1 = +0.2235 (p &lt; 0.001); CoT vs Few-shot ΔF1 = +0.0992 (p = 0.0473).",
            "format_effect_size": "Absolute effect sizes reported: CoT vs Zero-shot +0.3226 F1 (highly significant), Few-shot vs Zero-shot +0.2235 F1 (highly significant), CoT vs Few-shot +0.0992 F1 (marginally significant).",
            "explanation_or_hypothesis": "Authors hypothesize that increased scaffolding (few-shot mnemonic then CoT examples) reduces task ambiguity, elicits stepwise scene decomposition, increases recall and interpretability, and reduces inter-model variance; CoT is the most effective because it explicitly models intermediate reasoning steps.",
            "null_or_negative_result": false,
            "experimental_details": "Repeated-measures ANOVA on mean F1 (within-subjects: same 5 models evaluated in all 3 conditions). Statistical assumptions checked (Shapiro-Wilk, Levene). Tukey HSD used for pairwise comparisons; CoT and few-shot significantly better than zero-shot; CoT marginally better than few-shot.",
            "uuid": "e9324.15",
            "source_info": {
                "paper_title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Large Language Models are Zero-Shot Reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
            "rating": 1,
            "sanitized_title": "pretrain_prompt_and_predict_a_systematic_survey_of_prompting_methods_in_natural_language_processing"
        },
        {
            "paper_title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
            "rating": 1,
            "sanitized_title": "towards_understanding_chainofthought_prompting_an_empirical_study_of_what_matters"
        }
    ],
    "cost": 0.02434,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition</p>
<p>Nishi Chaudhary nishi.chaudhary@colostate.edu 
Graduate Research Assistant
Department of Construction Management
Colorado State University
291 W Laurel StFort CollinsCO</p>
<p>S M Jamil Uddin 
Department of Construction Management
Colorado State University
291 W Laurel StFort CollinsCO</p>
<p>Sharath Sathvik sathvik-cvl@dayanandasagar.edu 
Chandra 
Department of Civil Engineering
Dayananda Sagar College of Engineering Bengaluru
560111KarnatakaIndia</p>
<p>Anto Ovid 
Graduate Research Assistant
Department of Civil
Construction, and Environmental Engineering
North Carolina State University
Fitts-Woolard Hall, 915 Partners Way27606RaleighNC</p>
<p>Alex Albert alex_albert@ncsu.edu 
Department of Civil
Construction, and Environmental Engineering
North Carolina State University
Fitts-Woolard Hall, 915 Partners Way27606RaleighNC</p>
<p>Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition
9D59AC318B1EF5F666EF337533C936FAMLLMGPTHazard RecognitionAIConstruction Safety
The recent emergence of multimodal large language models (LLMs) has introduced new opportunities for improving visual hazard recognition on construction sites.Unlike traditional computer vision models that rely on domain-specific training and extensive datasets, modern LLMs can interpret and describe complex visual scenes using simple natural language prompts.However, despite growing interest in their applications, there has been limited investigation into how different LLMs perform in safety-critical visual tasks within the construction domain.To address this gap, this study conducts a comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5, GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify potential hazards from real-world construction images.Each model was tested under three prompting strategies: zero-shot, few-shot, and chain-of-thought (CoT).Zero-shot prompting involved minimal instruction, few-shot incorporated basic safety context and a hazard source mnemonic, and CoT provided step-by-step reasoning examples to scaffold model thinking.Quantitative analysis was performed using precision, recall, and F1-score metrics across all conditions.Results reveal that prompting strategy significantly influenced performance, with CoT prompting consistently producing higher accuracy across models.Additionally, LLM performance varied under different conditions, with GPT-4.5 and GPT-o3 outperforming others in most settings.The findings also demonstrate the critical role of prompt design in enhancing the accuracy and consistency of multimodal LLMs for construction safety applications.This study offers actionable insights into the integration of prompt engineering and LLMs for practical hazard recognition, contributing to the development of more reliable AI-assisted safety systems.</p>
<p>Introduction and Motivation</p>
<p>The construction industry continues to be one of the most hazardous industries around the world.In the United States alone, approximately 1,000 people die at construction workplaces every year, which translates to approximately 3 lives lost every day (BLS 2024).The number goes well beyond 60,000 per year globally (ILO 2021;Lingard 2013).In addition to fatal accidents, the industry also faces a significant number of non-fatal injuries every year.These fatal and non-fatal injuries not only result in the tragic loss of human life but also impose significant economic burdens on the construction industry and society at large (Bhattacharya 2014;Wang et al. 2015).Beyond the immediate impact on workers and their families, such incidents lead to increased insurance premiums, project delays, productivity losses, legal liabilities, and substantial costs associated with medical treatment, workers' compensation, and regulatory fines (Asfaw et al. 2012;Foley et al. 2012;West et al. 2016).</p>
<p>One of the most critical and recurring contributors to these safety incidents is poor hazard recognition performance within the construction industry (Jeelani et al. 2017b).Studies have shown that workers frequently overlook existing dangers on the jobsite due to several reasons including inattention, lack of training, cognitive overload, etc. (Aroke et al. 2020;Hasanzadeh et al. 2019).In fact, several studies demonstrated that construction workers often miss approximately 40% construction hazards in the workplace; which in turn result in safety incidents (Albert et al. 2017(Albert et al. , 2020a, c;, c;b;Uddin et al. 2020).</p>
<p>To improve hazard recognition in construction, tools like Job Hazard Analysis (JHA) and safety checklists have long been used to guide workers in identifying risks.However, studies show these tools often fall short due to their reliance on human judgment and inability to capture the complexity of dynamic job sites, leaving over 40% of hazards unrecognized (Guo et al. 2016;Jeelani et al. 2017b;a;Rozenfeld et al. 2010;Uddin et al. 2020).Technology-based approaches such as BIM, eye tracking, AR, and VR have shown promise in enhancing hazard identification through better visualization and engagement (Jeelani et al. 2019;Kim et al. 2020;Li et al. 2018).Yet, these tools often demand significant expertise, time, and financial resources, limiting their scalability and adoption across the industry.</p>
<p>In recent years, computer vision and artificial intelligence (AI) have been explored as scalable tools to support safety monitoring and hazard detection (Arshad et al. 2023).Convolutional Neural Networks (CNNs) have been developed to detect Personal Protective Equipment (PPE) violations (Delhi et al. 2020;Gallo et al. 2022), unsafe worker-equipment proximities (Fang et al. 2018), and trip hazards from site images and video feeds (McMahon et al. 2018).While promising, these solutions typically require large, annotated datasets, model re-training for new environments, and technical expertise that smaller firms often lack (Alateeq et al. 2023;Lee and Lee 2023).Their deployment is therefore limited by cost, complexity, and lack of flexibility.</p>
<p>More recently, the emergence of Large Language Models (LLM) such as ChatGPT, Claude, Gemini etc. have offered a new paradigm.These models are pretrained on large scale datasets and capable of interacting with users through simple prompts and user-friendly interface (CloudFlare 2024; Google 2024).These modern LLMs now combine visual understanding with natural language reasoning and are able to interpret complex images and generate descriptive, human-readable outputs.This enables the models to analyze and understand visual cues from images, turning them into Multimodal LLMs (He et al. 2024).</p>
<p>Building on these advancements, researchers and practitioners across various domains have begun exploring how LLMs can be adapted to solve domain-specific challenges.In the construction industry, early studies have demonstrated the utility of LLMs for applications such as automated schedule generation (Prieto et al. 2023), safety training (Uddin et al. 2023), education (Uddin et al. 2024b), and hazard identification (Uddin et al. 2023(Uddin et al. , 2024a;;Zheng and Fischer 2023).</p>
<p>While the existing studies have demonstrated the potential of LLMs in various aspects of construction including workers' health and safety, several critical areas remain underexplored.First, most research has focused solely on textual inputs and outputs, overlooking the multimodal capabilities of modern LLMs to process and interpret visual data.Second, no prior studies have conducted a comparative analysis of multiple LLMs to evaluate their relative effectiveness in achieving safety-related outcomes.And third, while a number of studies suggest that prompting style can have a major effect on LLM performance (Gao 2023;White et al. 2023), yet its influence in visual hazard detection tasks has not been studied yet.</p>
<p>To address these gaps and to explore the full potential of multimodal LLMs in construction safety applications, this study focuses on the role of prompt engineering, specifically, how different prompting strategies affect a model's ability to identify hazards from real-world construction images.We evaluate three widely studied prompting techniques: zero-shot, few-shot, and chain-of-thought (CoT).In the zeroshot setting, the model is provided only with a basic task instruction and the image, without prior examples or context.Few-shot prompting builds on this by introducing limited contextual information, such as a hazard-source mnemonic or safety-related cues, to simulate a low-effort instructional setting.Chain-ofthought (CoT) prompting further scaffolds the model's reasoning process by supplying step-by-step annotated examples, guiding the model to articulate its analysis in a structured, human-like manner.While these prompting styles are known to influence model performance in tasks involving language and reasoning, their effect on visual hazard detection in the construction domain remains unexplored.</p>
<p>Additionally, this study conducts a comparative evaluation using five state-of-the-art LLMs with multimodal capabilities i.e., Claude-3 Opus, GPT-4.5, GPT-4o, GPT-o3, and Gemini 2.0 Pro.These models were selected for their popularity, technical maturity, availability, and multimodal capabilities.Each model was tested under all three prompting strategies using a curated set of construction site images containing diverse hazards to assess their multimodal capabilities.</p>
<p>Hence the research questions addressed by this study are as follows: RQ1: How do different prompting strategies (zero-shot, few-shot, and chain-of-thought) influence the hazard recognition performance of multimodal LLMs?RQ2: Which LLMs perform better under each prompting condition when applied to visual hazard identification in construction environments?</p>
<p>Background</p>
<p>LLM Applications in Construction Safety</p>
<p>In recent years, Large Language Models (LLM) have gained significant popularity across different industries and domains such as healthcare (Cascella et al. 2023), manufacturing (Wang et al. 2024b), and education (Neumann et al. 2024) among many others (Chkirbene et al. 2024).The construction industry is no different from others.Although the industry is in its very early stages of adopting LLMs on a full scale for diverse applications, several studies have explored the opportunities these LLMs present.For example, a number of studies explored the possibility of integrating LLM and BIM to support information retrieval from the building models (Rane et al. 2023;Zheng and Fischer 2023).Other studies have focused on leveraging different LLMs for project management tasks such as automated sequence planning (You et al. 2023), generating construction schedules (Prieto et al. 2023), automated classification of contractual risk clauses (Moon et al. 2022), automatic matching of look ahead planning tasks (Amer et al. 2021) etc.Additionally, some studies focused on how to effectively use LLMs to improve the construction education outcomes (Abril et al. 2024;Uddin et al. 2024b;Zhao et al. 2024).</p>
<p>On the safety front, several studies have examined the usability of LLMs to improve the health and safety condition of construction workplaces.For example, Uddin et al. (2023) conducted a controlled experiment with 42 construction engineering students.Their efforts demonstrated that LLMs can be particularly effective in aiding hazard recognition efforts.They also suggest that LLMs can be integrated as part of safety education for construction students, albeit with caution.Another study by Uddin et al. (2024) explored the usability of LLM in aiding construction hazard prevention through design efforts.The study demonstrated that LLM can improve the hazard recognition efforts during the design phase by approximately 40%.Wang et al. ( 2023) evaluated LLM's ability to extract causal factors from construction accident reports.The study found that LLM can perform well as an assisting tool, offering clear and reliable insights, but it still requires further development for professional applications like crane safety.The research highlights the potential of LLMs in construction while emphasizing the need for refinement to enhance their practical utility.Smetana et al., (2024) leveraged an LLM model to analyze textual data from OSHA's Severe Injury Reports (SIR) for highway construction accidents.Using advanced NLP techniques, clustering, and LLM prompting, they identified major accident types, including heat-related and struck-by injuries, while uncovering commonalities between incidents.The findings demonstrate the potential of AI and LLMs to enhance data-driven safety analysis and support the development of more effective prevention strategies in the highway construction industry.Hussain et al., (2024) developed a virtual reality-based safety training system incorporating LLM as a live AI instructor to address communication barriers and trainer limitations, particularly for migrant workers.Testing across five countries showed a 23% improvement in knowledge scores, demonstrating the system's effectiveness.The research highlights the system's potential to improve safety training globally, reduce construction site accidents, and advance immersive and AI-driven training methodologies.</p>
<p>While the aforementioned studies have highlighted the potential of LLMs in construction safety applications, they have largely focused on a single model and relied exclusively on text-based inputs and outputs.The use of image-based inputs for hazard recognition, despite the growing multimodal capabilities of modern LLMs, remains unexplored.As these models increasingly support visual reasoning, it is critical to evaluate their effectiveness in extracting safety-relevant information from construction imagery.</p>
<p>Prompt Engineering</p>
<p>Prompt engineering refers to the strategic design of input instructions or contextual cues provided to LLMs to elicit desired outputs without retraining or fine-tuning the underlying weights (Chen et al. 2023).Unlike traditional machine learning models that require retraining or fine-tuning to adapt to new tasks and which often requires a large amount of data, LLMs like ChatGPT, Claude, and Gemini can dynamically adjust their behavior by adjusting the wording, ordering, or contextual clues embedded in a prompt (Gao 2023;Wang et al. 2024a;White et al. 2023).Effective prompt design is therefore emerging as a lightweight yet powerful alternative to the costly process of collecting new data and re-optimizing model parameters (Chen et al. 2023;Yurchak et al. 2024).</p>
<p>Recent studies have demonstrated that well-crafted prompts can significantly improve model performance across a range of tasks, including natural language understanding (Liu et al. 2023), code generation (Guo 2024), visual question answering (Wang et al. 2023), and domain-specific applications such as risks detection (Yong et al. 2023).In this study, prompt engineering was used to test three levels of guidance i.e., zero-shot prompting (no context), few-shot prompting (minimal examples), and CoT prompting (step-bystep reasoning), to assess their effect on visual hazard recognition in construction scenarios.</p>
<p>Zero-shot prompting requires the model to complete a task based solely on high-level instruction, with no examples provided.For instance, the prompt "Translate the following sentence to French: 'Good morning'" is sufficient for many LLMs to produce accurate results, especially on simple, factual tasks.However, research has shown that zero-shot performance often degrades on tasks requiring abstract reasoning or multi-step logic (Kojima et al. 2022).</p>
<p>To overcome these limitations, few-shot prompting introduces a handful of examples within the prompt to demonstrate the task format and desired output.This technique claims that providing even 1-5 exemplars could significantly improve performance on classification, translation, and summarization tasks (Brown et al. 2020;Liu et al. 2023).For example, in a sentiment analysis task, a prompt might include: This setup helps the model infer task intent and structure, often outperforming zero-shot prompts.</p>
<p>Chain-of-thought (CoT) prompting takes the concept further by asking the model to reason through a problem step by step before providing an answer.Rather than jumping directly to a conclusion, the model is encouraged to produce intermediate reasoning (Wang et al. 2022;Wei et al. 2022;Yu et al. 2023).For example, when asked "If there are 3 cars, how many wheels are there in total?", a CoT prompt would look like:</p>
<p>"Each car has 4 wheels.With 3 cars, that's 3 × 4 = 12 wheels in total."This technique, introduced by Wei et al., (2022), has been shown to substantially boost performance on complex arithmetic, logical reasoning, and common-sense tasks, often exceeding few-shot methods in accuracy.</p>
<p>Prompt engineering can therefore act as a performance amplifier, as carefully chosen examples reduce task ambiguity, while a step-wise scaffold encourages the model to decompose complex scenes into manageable units.Empirical studies report gains of 15 to 30 percentage points on reasoning benchmarks when moving from zero-shot to CoT prompts (Wei et al. 2022) and similar improvements in multimodal visual question answering when concise visual exemplars are supplied (Xu et al. 2023).In the construction-safety context, these advantages are particularly appealing, because they offer a path to high-quality hazard recognition without the prohibitive cost of assembling large data sets and training and re-training the model according to environments.By systematically comparing zero-shot, few-shot, and CoT strategies across five state-ofthe-art LLMs, this study evaluates just how much leverage prompt engineering can provide for visual safety assessment on the jobsite.</p>
<p>Materials and Methodology</p>
<p>Data Set: Construction Case Images</p>
<p>To evaluate the construction hazard recognition performance of different LLMs under different prompt settings, the study utilized 16 case images representing a wide range of construction operations, originally collected in a prior investigation (Construction Industry Institute 2013).These images were curated by a panel of 17 expert construction professionals possessing collective experience of over 300 years.These images encompassed tasks such as excavation, gas welding, pipe laying, crane rigging, drilling, grinding, cutting etc.All images were sourced from active construction sites across the United States.</p>
<p>Following collection, the expert panel conducted a series of structured brainstorming sessions to preidentify the safety hazards present in each image and labeled them.Each image contained a minimum of five hazards spanning a wide range of hazards such as fall potential, struck by potential, pressurized piping, exposed cables and energized tools, welding fumes etc.The examination yielded a total of 120 safety hazards across the 16 construction case images.These pre-identified lists of hazards serve as ground truth in this study for LLM assessment experiment.An example of a representative case image with hazards labeled used in the study is shown on Figure 1.</p>
<p>Prompt Engineering Protocol Development</p>
<p>This study employed a structured prompt engineering framework to evaluate how varying levels of guidance affect visual hazard recognition of different LLM in construction scenarios.Specifically, three prompting strategies were tested: zero-shot prompting, in which the model received no prior context or examples; few-shot prompting, which provided minimal contextual information or illustrative cues; and chain-of-thought (CoT) prompting, which guided the model through step-by-step reasoning using annotated examples.These distinct prompting conditions were designed to simulate different levels of instructional support and assess their influence on model performance across real-world construction imagery.</p>
<p>Zero-shot Prompting</p>
<p>In zero-shot prompting, we did not feed the models any additional information regarding the goal of the study, construction safety information, or any hazard information.Zero-shot prompting is solely based on the existing knowledgebase of the LLMs.To ensure consistency and standardization, a systematic approach was developed to input all the images into these LLMs for hazard identification.First, the following prompt was provided:</p>
<p>You're a construction safety expert.You will be provided with 16 different construction images.You will have to analyze these images and identify the potential safety hazards for each image.</p>
<p>Then each image was fed into to LLM chat prompt along with the following instruction:</p>
<p>Identify all potential safety hazards for the construction activity in this image.</p>
<p>No additional contextual or domain-specific prompts were provided, ensuring the zero-shot learning approach remained intact.This standardized methodology facilitated comparability of outputs across all the models.</p>
<p>Few-shot Prompting</p>
<p>Few-shot prompting is a technique used to guide LLMs by providing them with minimal information or examples, typically between one and three, demonstrating the desired task or reasoning pattern (Xu et al. 2023;Yurchak et al. 2024).Unlike zero-shot prompting, which offers no context, few-shot prompts supply minimal but targeted information to help the model generalize its responses.This lightweight form of guidance enables LLMs to better understand task expectations and align their outputs, even without extensive training or fine-tuning.</p>
<p>In this study, each LLM was provided with a short textual prompt containing basic construction safety information, along with a visual mnemonic containing common hazard sources as part of the few-shot prompting.</p>
<p>You're a construction safety expert.First you will be provided with an energy mnemonic diagram which represents ten types of energy source that can result in hazard exposure in the workplace.You will also be provided with examples of sources of construction hazards related to each type of energy.After that you will be provided with 16 different construction images.You will have to analyze these images and identify the potential safety hazards for each image based on this information in addition to your existing knowledgebase.</p>
<p>To enhance the effectiveness of the few-shot prompting strategy, we incorporated the Energy Wheel mnemonic, a structured cognitive aid grounded in Haddon's energy theory (Haddon 1973).This theory conceptualizes all hazards as potential sources of energy capable of causing injury when released, such as gravity, motion, temperature, electrical, chemical, mechanical energy etc.By organizing hazards into discrete energy categories, the mnemonic helps deconstruct the abstract and cognitively demanding task of hazard recognition into manageable, sequential components.</p>
<p>Several studies have demonstrated that the energy wheel can improve the hazard recognition capabilities of human participants by a significant margin with minimal effort (Albert et al. 2014;Bayona et al. 2025;Hallowell and Hansen 2016;Tixier et al. 2018).In our study, the mnemonic was provided to LLMs in the few-shot prompting condition as part of the minimal knowledge input.</p>
<p>By integrating the Energy Wheel into our few-shot prompt, we aimed to simulate a common field-based safety practice in which workers are trained to forecast hazards by mentally scanning for hazards associated with each energy source.This structure offered a lightweight yet domain-relevant framework to improve the LLMs' situational awareness during image-based hazard recognition.Figure 2 below shows the energy mnemonic and Table 1 shows the list of energy sources and example hazards, extracted from Albert et al., (2014), we used in our few-shot prompting.</p>
<p>Once this information was fed into the LLMs, the following prompt was used with each image, Identify all potential safety hazards for the construction activity in this image.Sound is produced when a force causes an object or substance to vibrate, the energy is transferred through the substance in waves Examples: impact noise, vibration, high-pressure relief, equipment noise</p>
<p>Chain of Thought (CoT) Prompting</p>
<p>In the chain-of-thought (CoT) condition, models were guided through a structured reasoning process designed to simulate expert-like hazard identification process.The CoT prompt began with a brief contextual explanation underscoring the high incidence of construction-related injuries and the critical role of hazard recognition in improving safety outcomes.</p>
<p>The construction sector faces a significantly higher rate of safety incidents globally, even though it is vital to the economy.These incidents frequently result in both fatal and nonfatal injuries among workers.In the United States, around 1,000 workers lose their lives to accidents each year, while non-fatal injuries surpass 200,000.Research suggests that a major contributing factor to these accidents is the inability to recognize construction hazards.</p>
<p>Studies indicate that the industry performs poorly in identifying workplace risks, with over 70% of incidents linked to inadequate hazard awareness.Therefore, your task will be to pinpoint potential hazards from images depicting construction sites.</p>
<p>To perform the task, being consistent with few-shot prompting, each model was first introduced to the standardized energy wheel mnemonic (Figure 2) and the list of example hazards (Table 1).</p>
<p>First you will be provided with an energy mnemonic diagram which represents ten types of energy source of hazards present in the workplace.You will also be provided with examples of sources of construction hazards related to each type of energy.</p>
<p>Following this orientation, the LLMs were presented with three training examples randomly selected from the pool of sixteen images that included fully annotated hazard analyses (For example see Figure 1).Each example walked the model through a step-by-step reasoning process that includes (1) identifying key scene elements (e.g., workers, equipment, materials), and (2) understanding potential hazards using annotated case images.Throughout the process, the LLMs were prompted to confirm their understanding after each example to simulate interactive learning.Now I will provide you with three example images one by one along with their correct hazards labeled.After each example, I will ask you to confirm that you understand the reasoning and hazards identification process.If at any point you are unsure about a hazard or need clarification, you can ask me questions before finalizing your understanding.</p>
<p>After completing the training phase, the remaining thirteen images were provided one at a time.For each, the model was instructed to first describe the visible scene components and then systematically identify the potential hazards using the same structured approach.</p>
<p>First, describe the elements in the image (e.g., workers, equipment, materials, structures).</p>
<p>Then, use the training information provided along with your existing knowledgebase to identify potential hazards from the images.Your job is to determine the hazards present in the image; you do not need to determine compliance issues.</p>
<p>The CoT prompting method was designed to promote deeper contextual reasoning, mirroring cognitive processes observed in human experts, and has been shown in prior literature to enhance accuracy on complex, multimodal tasks (Wang et al. 2022;Wei et al. 2022).</p>
<p>Once each LLM returned its results for an image under a given condition (i.e., zero-shot, few-shot, or CoT), the identified hazards were immediately recorded by the research team, ensuring traceability and consistency for subsequent analysis.The process was conducted under controlled experimental conditions, ensuring that all models were evaluated using identical inputs and queries.To mitigate potential learning effects and ensure independent evaluations, different computers and separate user accounts were used for each prompting condition, thereby minimizing the influence of prior interactions on the models' responses.</p>
<p>Measured Martics</p>
<p>To quantitatively assess the hazard recognition performance of each LLM, we employed three widely used classification metrics: Precision, Recall, and the F1-score (Bronnec et al. 2024;Kostina et al. 2025).These metrics are particularly appropriate in safety-critical applications where both accuracy and completeness of hazard detection are essential.</p>
<p>Each model's output for a given image was compared against an expert-defined ground truth.From this comparison, we computed the number of true positives (TP), false positives (FP), and false negatives (FN) for every image analyzed by each LLM under all three conditions.</p>
<p>A true positive (TP) occurs when the model correctly identifies a hazard that is actually present in the image according to the expert-defined ground truth.For example: The ground truth includes "fall hazard due to unguarded edge," and the LLM also identifies "unguarded edge/fall hazard".</p>
<p>A false positive (FP) occurs when the model identifies a hazard that is not present in the ground truth.For example: The LLM predicts "chemical spill hazard," but the ground truth contains no such hazard in the image, this is considered an FP.</p>
<p>And a false negative (FN) occurs when the model fails to identify a hazard that is present in the ground truth.Example: The ground truth includes "overhead load hazard," but the LLM misses it entirely, this is labeled as an FN.</p>
<p>Once the TP, FP, and FN were measured for each LLM under each condition, we used the following equations, i.e, Eq 1, Eq 2 and Eq 3, to compute the Precision, Recall, and F1-Score.</p>
<p>Precision (P)</p>
<p>Precision is the proportion of correctly identified hazards out of all hazards predicted by the model for a given image.Precision captures the model's accuracy, reflecting its ability to avoid false alarms (i.e., overpredicting hazards that do not exist).</p>
<p>=     +   … … … .(1)</p>
<p>Recall (R)</p>
<p>Recall is the proportion of correctly identified hazards out of all hazards present in the ground truth.Recall measures the model's completeness, indicating how many actual hazards it successfully detected.
𝑅 𝑖 = 𝑇𝑃 𝑖 𝑇𝑃 𝑖 + 𝐹𝑁 𝑖 … … … (2)</p>
<p>F1-Score (F1)</p>
<p>F1-score is the harmonic means of precision and recall.The F1-Score provides a balanced indicator of performance, especially when precision and recall trade off, which is common in tasks like hazard detection.
𝐹1 𝑖 = 2 × 𝑃 𝑖 × 𝑅 𝑖 𝑃 𝑖 + 𝑅 𝑖 … … … (3)
Once the Precision, Recall, and F1-score values were computed for each image (), these values were then averaged to obtain the mean performance metrics for each LLM under each prompting condition.Specifically, for each model  and each prompting condition , the average Precision, Recall, and F1-score were calculated across  images using the following equations 4, 5, and 6:
𝐴𝑣𝑔. 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑚𝑗 = 1 𝑛 ∑ 𝑃 𝑖𝑗𝑚 𝑛 𝑖=1 … … … (4) 𝐴𝑣𝑔. 𝑅𝑒𝑐𝑎𝑙𝑙 𝑚𝑗 = 1 𝑛 ∑ 𝑅 𝑖𝑗𝑚 𝑛 𝑖=1 … … … (5) 𝐴𝑣𝑔. 𝐹1 − 𝑆𝑐𝑜𝑟𝑒 𝑚𝑗 = 1 𝑛 ∑ 𝐹1 𝑖𝑗𝑚 𝑛 𝑖=1 … … … (6)
where:</p>
<p>, , and 1 represent the Precision, Recall, and F1-score, respectively, for image , under prompting condition , by model , and  is the number of images: 16 for zero-shot and few-shot conditions, and 13 for chain-of-thought (CoT) prompting due to the exclusion of 3 training images.</p>
<p>Data Analysis and Results</p>
<p>Descriptive Analysis</p>
<p>As mentioned in the previous section, to evaluate the performance of LLMs in visual hazard recognition, we analyzed three metrics i.e., Precision, Recall, and F1-score.These metrics were computed for five LLMs, Claude-3 Opus, GPT-4.5, GPT-o3, GPT-4o, and Gemini 2.0, under three prompting conditions.The descriptive results are reported in Table 2 and the trends reveal meaningful insights into how prompting style potentially impacts model behavior.Under zero-shot prompting, where models received no prior examples or contextual cues, precision scores were low, ranging from 0.247 (GPT-4o) to 0.420 (Claude-3 Opus).This indicates that models frequently over-predicted hazards, often identifying risks that were not present.Even more notably, recall scores were uniformly poor, with most models retrieving fewer than one-third of the actual hazards in the images.For instance, Claude-3 Opus achieved a recall of just 0.288, and GPT-o3 scored only 0.326.As a result, F1scores were also weak, clustered around 0.30-0.34.These results reflect the models' difficulty in accurately and completely identifying hazards without any prior guidance, reinforcing that zero-shot LLMs are largely unreliable in high-stakes safety applications as a standalone safety tool.</p>
<p>In the few-shot conditions, where models received minimal instruction along with a mnemonic representing hazard types, all three metrics improved substantially.Precision rose across models, with GPT-4.5 and GPT-o3 scoring 0.599 and 0.565, respectively, showing a reduced rate of false alarms.Recall also increased sharply, especially for GPT-o3 (0.722) and GPT-4.5 (0.699), indicating that these models were able to detect the majority of hazards present.This translated into significantly higher F1-scores, particularly for GPT-o3 (0.629) and GPT-4.5 (0.623).Interestingly, Claude's recall (0.619) improved more than its precision (0.466), suggesting it retrieved many hazards but still misidentified several.These patterns demonstrate that even minimal contextual scaffolding helps certain models generalize better, improving both their accuracy and completeness.</p>
<p>The most substantial performance gains occurred under CoT prompting, where models were shown annotated examples with step-by-step reasoning.Precision reached its highest values for GPT-4.5 (0.592) and Claude (0.552), showing strong hazard detection specificity.More importantly, recall scores surged, with GPT-4.5 achieving 0.857 and Claude 0.798.These recall rates suggest that models were not only identifying correct hazards but capturing nearly all relevant hazards in the image.As a result, F1-scores peaked under CoT, with GPT-4.5 attaining 0.693, a 35% improvement over its zero-shot score.Even the lowest-performing model under CoT, GPT-4o, achieved a respectable F1 of 0.576, compared to just 0.269 in zero-shot.This illustrates that CoT prompting significantly improves model reasoning depth and reliability, narrowing the performance gap among models.</p>
<p>Which Prompting Condition Performs Better?</p>
<p>To further evaluate and compare the hazard recognition performance of LLMs under different prompting strategies, we selected the F1-score as the primary metric for statistical analysis.While both precision and recall provide important insights measuring a model's ability to avoid false alarms and to detect all relevant hazards, respectively each alone may present an incomplete or biased view of performance in open-ended detection tasks like hazard recognition.In contrast, the F1-score is the harmonic mean of precision and recall, and it captures a more balanced assessment of model performance by jointly penalizing both overprediction (false positives) and underprediction (false negatives) (Google 2025).</p>
<p>Given that construction hazard identification requires both accuracy (avoiding false hazard calls) and completeness (not missing true hazards), the F1-score offers a robust, single-value indicator of real-world usability.This metric has also been widely adopted in similar machine learning and safety-critical applications (Diallo et al. 2025;Liu et al. 2022; Yacouby Amazon Alexa and Axman Amazon Alexa 2020).</p>
<p>As such, F1-score was used as the dependent variable in all inferential statistical tests, including repeatedmeasures ANOVA and pairwise comparisons.</p>
<p>To address Research Question 1 i.e., How do different prompting strategies (zero-shot, few-shot, and chainof-thought) influence the hazard recognition performance of multimodal LLMs, a repeated-measures analysis of variance (ANOVA) was conducted using the mean F1-scores achieved by each large language model (LLM) across the three prompting strategies.Each of the five LLMs contributed one mean F1-Score value per condition, yielding a complete within-subjects design.</p>
<p>The repeated measure ANOVA was chosen because the same LLMs were evaluated under all three prompting conditions, constituting a classic repeated-measures design.This approach isolates the effect of prompting condition while controlling for model-level variability in baseline performance.Prior to conducting the ANOVA, the data were tested for statistical assumptions.Shapiro-Wilk tests confirmed that the residuals were normally distributed, and Levene's test indicated homogeneity of variances across conditions.These results justified the use of a parametric ANOVA model for inferential analysis.</p>
<p>As can be seen in Table 3, Chain-of-thought (CoT) prompting yielded the highest mean F1-score (M = 0.636, SD = 0.117), followed by few-shot prompting (M = 0.537, SD = 0.135).Zero-shot prompting produced the lowest performance (M = 0.314, SD = 0.052).</p>
<p>The ANOVA revealed a highly significant main effect of prompting conditions on hazard recognition performance, F(2, 8) = 87.392,p &lt; 0.001.This finding confirms that the choice of prompting strategy substantially influences LLM performance in visual hazard detection.To further investigate which prompting condition results in the best hazard recognition performance, we conducted a series of pairwise comparisons between the three prompting strategies: zero-shot, few-shot, and CoT.These comparisons were based on the mean F1-scores computed across all images for each LLM under each condition.Since each LLM experienced all three conditions, Tukey's HSD test was appropriate for comparing the mean F1-scores between conditions.Tukey's Honestly Significant Difference (HSD) test is a post-hoc statistical method used to determine which specific group means are significantly different from one another after a statistically significant ANOVA (Abdi and Williams 2010).Unlike simple t-tests, which compare pairs individually and inflate the risk of Type I error, Tukey's HSD controls the family-wise error rate across all pairwise comparisons.It adjusts the significance threshold to account for the number of comparisons, making it especially useful when comparing more than two groups.</p>
<p>As can be seen in Table 4, all three contrasts were statistically significant with p-value&lt; 0.05, indicating that each prompting strategy produced meaningfully different performance outcomes.The pairwise comparison confirms that CoT prompting substantially outperformed zero-shot prompting in hazard recognition (ΔF1 ≈ 0.32 on average).This result underscores the importance of structured, stepwise reasoning support for multimodal LLMs in complex visual tasks.</p>
<p>The few-shot prompt also significantly improved performance over zero-shot (ΔF1 ≈ 0.22 on average), suggesting that even a minimal additional information (basic safety context and mnemonic) can substantially enhance model output.</p>
<p>While the CoT and Few-shot performances are rather close (ΔF1 ≈ 0.10), CoT further provided a marginal statistically significant improvement over few-shot.This reinforces the added value of explicit reasoning steps, an insight aligned with previous literature on chain-of-thought prompting in textual and visual reasoning tasks.</p>
<p>Which LLM Performs Better Under Which Condition?</p>
<p>To address Research Question 2, which LLMs perform better under each prompting condition when applied to visual hazard identification in construction environments, we examined the mean F1-scores achieved by each LLM within the three prompting scenarios.</p>
<p>Building on the summary of F1-scores across prompting conditions, we next examined whether performance differences among LLMs were statistically significant within each prompting condition.A one-way repeated-measures ANOVA was conducted to assess differences in mean F1-score across the five LLMs.Table 5 shows the results of Zero-Shot condition, where models were asked to identify hazards from images without any prior examples, contextual training, or guidance.The analysis revealed a statistically significant main effect of model, F(4, 60) = 6.665, p &lt; 0.001.This finding indicates that, even in the absence of any prompting or priming, the LLMs demonstrated distinct levels of hazard recognition capability in the zero-shot setting.</p>
<p>To explore which specific LLM accounted for the observed differences in hazard recognition performance under the Zero-Shot condition, a post-hoc Tukey's HSD test was conducted and reported in Table 6.Both Claude and GPT-4.5 significantly outperformed GPT-4o, with mean differences of -0.0694 (p = 0.0006) and -0.0696 (p = 0.0005), respectively.These results suggest that GPT-4o was notably less effective at identifying hazards in a zero-shot setting compared to its counterparts, even when compared to models with similar architecture or release timeline.</p>
<p>Other model pairs, including Claude vs. GPT-4.5(mean diff.= 0.0002, p = 1.000) and Claude vs. GPT-o3 (mean diff.= -0.0306,p = 0.339), did not differ significantly, indicating comparable performance levels.Similarly, GPT-4.5 and GPT-o3 also performed at statistically indistinguishable levels (p = 0.333).The comparison between GPT-o3 and GPT-4o also suggested a comparable performance.</p>
<p>Interestingly, Gemini 2.0's performance fell between the top-performing models and GPT-4o, but no comparisons involving Gemini 2.0 reached statistical significance.The closest was GPT-4o vs. Gemini 2.0 (mean diff.= 0.0431, p = 0.073), which suggests that Gemini may outperform GPT-4o slightly, though this difference did not meet the 0.05 threshold.</p>
<p>These results reinforce GPT-4o's underperformance in zero-shot settings and position Claude, GPT-4.5, and GPT-o3 as statistically comparable and more effective in visual hazard recognition when no prior examples or context are provided.</p>
<p>To evaluate how models performed with minimal prompting support, we analyzed the results under the Few-Shot condition.Table 7 below shows the ANOVA analysis of LLMs under few-shot prompting.As shown in Table 7, the analysis revealed a statistically significant main effect of model, F(4, 60) = 11.823,p &lt; 0.001.These results confirm that, even with modest prompting, LLMs vary meaningfully in their hazard recognition capabilities.</p>
<p>Subsequently, we conducted a pairwise comparison test under few-shot prompting to explore which LLMs performed better or worse than others.Table 8 presents the outcome of the pairwise test.Under the few-shot prompting condition, Tukey's HSD analysis revealed more distinct stratifications in model performance.While Claude, GPT-4.5, and GPT-o3 all showed relatively high mean F1-scores, only some pairwise comparisons reached statistical significance after correction for multiple comparisons.</p>
<p>GPT-4.5 and GPT-o3 emerged as the top performers, with no significant difference between them (p = 0.9999, mean diff.= 0.0063), suggesting near-identical performance under few-shot prompting.Both models significantly outperformed GPT-4o and Gemini 2.0.Specifically, GPT-4.5 achieved significantly higher F1-scores than GPT-4o (p = 0.0002, mean diff.= -0.1819)and Gemini 2.0 (p = 0.0020, mean diff.= -0.1556).Similarly, GPT-o3 outperformed GPT-4o (p = 0.0001, mean diff.= -0.1881)and Gemini 2.0 (p = 0.0012, mean diff.= -0.1619),with strong statistical significance in both cases.</p>
<p>Claude's performance was more moderate.Although it showed higher mean F1-scores than GPT-4o and Gemini 2.0, these differences did not reach statistical significance (p &gt; 0.25).Likewise, while Claude slightly outperformed GPT-4.5 and GPT-o3 numerically, the differences were not significant (p = 0.1071 and 0.0747, respectively), placing Claude between the top-and mid-performing groups.</p>
<p>Notably, no significant difference was observed between GPT-4o and Gemini 2.0 (p = 0.9654), indicating that these models performed similarly at the lower end of the performance spectrum in few-shot scenarios.</p>
<p>Few-shot prompting clearly differentiated model capabilities, with GPT-4.5 and GPT-o3 forming a statistically superior tier, Claude occupying a middle position, and GPT-4o and Gemini 2.0 demonstrating significantly weaker performance.</p>
<p>The Chain-of-Thought (CoT) prompting condition involved training each LLM on three annotated example images before testing on the remaining images.Table 9 below shows that the LLM's performances are marginally statistically different after running the ANOVA test.While the ANOVA revealed a marginally significant overall effect of model on F1-score (F(4, 48) = 2.894, p = 0.032), suggesting that at least one LLM differed in performance, the subsequent pairwise comparisons failed to identify any statistically significant differences between individual model pairs.Across all ten model comparisons, the adjusted p-values exceeded 0.05, and the 95% confidence intervals for every pairwise contrast included zero.For example, although GPT-4.5 had the highest mean F1-score (0.693), its advantage over Claude (mean difference = 0.047, p = 0.831) and GPT-o3 (-0.052, p = 0.768) was not statistically meaningful.Similarly, GPT-4o, the lowest-performing model in this condition, did not differ significantly from any other model, including GPT-4.5 (p = 0.080) and Claude (p = 0.524), despite a consistent numerical gap in mean scores.</p>
<p>This apparent inconsistency between the ANOVA and post-hoc results is not uncommon in multi-group comparisons (Cohen 2008;Field 2024;Maxwell et al. 2017).The omnibus ANOVA tests whether any variation across group means exists, pooling variance across all comparisons.In contrast, the Tukey test evaluates each pair independently, applying stricter correction to maintain the family-wise error rate (Abdi and Williams 2010;Field 2024).As a result, subtle but distributed differences across multiple pairs may yield a significant ANOVA even if no single pairwise contrast is large enough to remain significant after adjustment.</p>
<p>The results indicate that while there is weak evidence of performance variation among LLMs under CoT, the effect is diffuse and not driven by any specific model pair.This reinforces the notion that structured, step-by-step prompting helps to equalize model performance, making CoT an effective strategy for reducing model variability in complex visual reasoning tasks such as construction hazard recognition.</p>
<p>In practical terms, these findings suggest that CoT prompting lifts the baseline performance of all LLMs to a comparable level, minimizing sharp performance gaps that were more evident in the zero-shot and fewshot conditions.While GPT-4.5 and Claude numerically outperformed other models, these differences were not statistically robust.The CoT structure likely benefited even lower-performing models like GPT-4o and Gemini 2.0, leading to a more uniform distribution of F1-scores.</p>
<p>LLM Performance Across Prompting Conditions</p>
<p>To synthesize and compare the performance of each LLM across all prompting conditions, we constructed a heatmap based on mean F1-scores (Figure 3).This visual representation provides an intuitive, side-byside view of how each model responded to zero-shot, few-shot, and chain-of-thought (CoT) prompting strategies.The heatmap clearly illustrates a progressive improvement in performance from right to left across all models, corresponding to the increasing support offered by the prompting strategy.In the zero-shot condition, F1-scores were relatively low across the board, with no model exceeding a score of 0.34.Claude and GPT-4.5 led marginally, but the differences among models were modest.</p>
<p>With few-shot prompting, performance rose substantially.GPT-o3 and GPT-4.5 stood out as the highest performers, with F1-scores above 0.62, while Claude and Gemini 2.0 showed moderate gains.GPT-4o, while improved, continued to lag behind the other models.</p>
<p>The most significant shift appears under CoT prompting, where all models demonstrated their highest F1scores.GPT-4.5 again achieved the top performance (0.693), closely followed by Claude, GPT-o3, and Gemini 2.0.Notably, even GPT-4o, previously the weakest performer, reached an F1 of 0.576 under CoT, reflecting the leveling effect of structured reasoning guidance.</p>
<p>The shading gradient in the heatmap reinforces these trends, i.e., darker cells indicate higher F1 performance, concentrated on the leftmost column (CoT) for each model.This visual evidence supports the statistical findings that CoT prompting not only elevates individual model performance but also reduces the performance gap among models, leading to a more uniform and reliable hazard recognition capability across LLMs.</p>
<p>Discussion</p>
<p>Our results show that chain-of-thought (CoT) prompting lifted every model to at least 0.57 F1 and, statistically, erased pair-wise gaps.This indicates that explicit, step-wise reasoning can compensate for architectural differences among LLMs.Practically, small businesses and contractors that cannot afford professional safety experts, expensive AI models, or dedicated software, can still approach state-of-the-art accuracy by investing time in prompt engineering.Our study shows the pathway towards the development of an open-source CoT template, anchored in the Energy Wheel mnemonic and hazard specific examples, that practitioners can reuse across projects.</p>
<p>Even two or three tailored exemplars (the few-shot condition) delivered sizeable gains, driven largely by recall.A lightweight "prompt-library" organized by task (excavation, electrical, working-at-height, etc.) and relevant hazards would let field supervisors mix-and-match examples on a tablet and realize most of the performance upside without incurring the token cost of full CoT reasoning.</p>
<p>Although few-shot and CoT prompting raised average hazard-recognition performance above the levels reported for human participants in prior studies (Albert et al. 2014;Hallowell and Hansen 2016;Uddin et al. 2020), the best CoT model still missed about 14% of hazards and generated roughly 39% false positives.</p>
<p>In safety-critical applications, high recall is essential, as overlooking actual hazards can lead to severe consequences.Even with slightly lower precision, over-identifying hazards is preferable to ensure that no critical threats go undetected.However, given the observed precision-recall trade-offs, LLMs should always be deployed as decision-support tools, not autonomous safety inspectors.Requiring a competent person to confirm or dismiss each AI-flagged hazard will both safeguard operations and create a curated feedback stream for future model fine-tuning.</p>
<p>Importantly, the structured reasoning provided by CoT prompting offers value beyond immediate hazard detection.Because CoT outputs break down how and why a hazard is identified, typically by referencing visual cues and mapping them to known hazard sources, they inherently generate explanatory content.These model-generated rationales could be repurposed as safety training material, particularly for frontline workers with limited formal safety education.For example, CoT-based outputs could be visualized as annotated overlays on images, walking users through the hazard identification process step by step.</p>
<p>Such outputs could be incorporated into toolbox talks, post-inspection briefings, or interactive digital training modules, effectively using AI not just as a detector but as a teacher.This dual use, hazard detection and safety education, could enhance hazard literacy among field crews over time, aligning AI integration with long-term safety culture development.</p>
<p>In this way, CoT prompting not only improves model performance, but also supports knowledge transfer, helping bridge the gap between experienced safety professionals and newer or less formally trained workers.This positions LLMs not merely as intelligent tools, but as adaptive safety mentors embedded within everyday site workflows.</p>
<p>Practical and Theoretical Contribution of the Study</p>
<p>The contribution of our study is three-fold.First, the study extends prompt engineering scholarship by demonstrating that structured prompts, especially chain-of-thought (CoT), can compensate for architectural disparities among multimodal LLMs.We provide empirical evidence that reasoning depth, rather than model scale alone, governs hazard recognition accuracy in open world visual tasks.Our study demonstrates that small businesses and contractors can approach premium model accuracy by using CoT prompt engineering, avoiding excessing developmental and operational cost for high powered AI tools while maintaining robust hazard recognition.</p>
<p>Second, by embedding the Energy Wheel mnemonic into few-shot and CoT prompts, we bridge cognitive ergonomics theory with AI reasoning.Our results suggest that domain specific cognitive scaffolds can be transposed into LLM prompts to elevate both recall and explanatory richness, offering a new theoretical lens on how expert heuristics can be digitized.Our study recommends a lightweight prompt library specific to different hazards (excavation, electrical, heights, etc.) that supervisors can use on the field.This modular approach delivers most of the CoT benefit without the token overhead, facilitating real-time site inspections.</p>
<p>Finally, the 16-image, expert-annotated dataset, plus our open analysis pipeline, constitutes the first scholarly benchmark comparing five commercial multimodal LLMs on real construction imagery.It lays a replicable foundation for future studies on prompt optimization, domain transfer, and fine-tuning methods in safety contexts.Additionally, CoT explanations can be repurposed as annotated overlays for toolbox talks and post-inspection reviews, transforming LLMs into adaptive safety mentors that elevate hazard literacy among crews.</p>
<p>Conclusion, Limitations, and Future Research Directions</p>
<p>This study evaluated the construction hazard recognition capabilities of five state-of-the-art multimodal LLMs across three prompting strategies, zero-shot, few-shot, and chain-of-thought (CoT), using a curated dataset of sixteen construction site images.The results demonstrated that prompting strategy significantly influences model performance.CoT prompting consistently outperformed zero-shot and few-shot approaches, improving recall, interpretability, and overall F1-scores across all models.Notably, even models with lower baseline performance were elevated under CoT to levels approaching those of premium architectures, highlighting the potential of prompt design as a performance equalizer.Few-shot prompting also produced meaningful gains, particularly in recall, reinforcing the value of minimal contextual guidance.</p>
<p>While the findings are promising, this study has several limitations.First, the dataset consisted of only 16 expert-annotated images, which may not capture the full diversity of hazards, environments, or construction activities encountered in the field.Second, the generalizability of results is currently limited to static visual inputs; real-world deployments may involve video, 3D data, or multimodal sensor streams.Third, we evaluated only five proprietary LLMs in this study.As open-weight multimodal LLMs continue to evolve, future research may yield different model rankings or prompt effectiveness.</p>
<p>Future research should pursue three major directions.First, expanding the dataset, both in image count and diversity, will enable more robust benchmarking and the development of task specific prompt strategies.This includes incorporating varied lighting conditions, geographic regions, project types, and cultural PPE standards.Second, further exploration into prompt distillation and retrieval augmented generation could reduce CoT's computational burden while preserving its reasoning quality.Finally, integrating model outputs into interactive workflows, such as active learning pipelines or semi-automated training modules, can close the loop between AI recognition and human decision-making, promoting safer and smarter construction environments.</p>
<p>This study provides both empirical evidence and practical tools for advancing the use of multimodal LLMs in construction safety.By combining prompt engineering insights with domain-specific knowledge structures like the Energy Wheel, we offer a scalable and interpretable foundation for AI-assisted hazard recognition in the field.</p>
<p>"</p>
<p>Example 1: 'The movie was amazing!' is a Positive sentiment.Example 2: 'It was a waste of time.' is a Negative sentiment.Now classify what type of sentiment is the following sentence: 'I really enjoyed the story.'"</p>
<p>Figure 1 .
1
Figure 1.Annotated Example Image Used in the LLM Assessment</p>
<p>Figure 2 .
2
Figure 2. Energy Wheel Used in Few-shot Prompting</p>
<p>Figure 3 .
3
Figure 3. Heatmap of LLM Performance across Prompting Conditions</p>
<p>Table 1 .
1
Energy Source of Hazards with Example
Energy SourceDefinition and ExamplesGravityForce caused by the attraction of all masses to the mass of the earthExamples: falling objects, collapsing roof, and body tripping or fallingMotionChange in position of objects or substancesExamples: vehicle, vessel or equipment movement, flowing water, wind, bodypositioning, lifting, straining, or bendingMechanicalEnergy of the components of a mechanical system, i.e., rotation, vibration, andmotion, within otherwise stationary pieces of equipment/machineryExamples: rotating equipment, compressed springs, drive belts, conveyors, motorsElectricalPresence and flow of an electric chargeExamples: power line, transformers, static charge, lightning, energized equipment,wiring, batteriesPressureEnergy applied by liquid or gas that has been compressed or is under a vacuumExamples: pressure piping, compressed gas cylinders, control lines, vessels, tanks,hoses, pneumatic and hydraulic equipmentTemperatureMeasurement of differences in the thermal energy of objects or the environment,which the human body senses as either heat or colddustsBiologicalLiving organisms that can present a hazardExamples: animals, bacteria, viruses, insects, blood-borne pathogens, improperlyhandled food, contaminated waterRadiationEnergy emitted from radioactive elements, or sources, and naturally occurringradioactive materialsExamples: lighting issues, welding arcs, X-rays, solar rays, microwaves, naturallyoccurring radioactive material (NORM) scale,or other nonionizing sourcesSound
Examples: open flame and ignition sources, hot or cold surface, liquids or gases, hot work, friction, general environmental conditions, steam, extreme and changing weather conditions Chemical Energy present in chemicals that inherently, or through reaction, has the potential to create physical or health hazards to people, equipment, or the environment Examples: flammable vapors, reactive hazards, carcinogens or other toxic compounds, corrosives, pyrophoric, combustibles, inert gas, welding fumes,</p>
<p>Table 2 :
2
Descriptive Statistics for each LLM under different Conditions
Zero ShotFew ShotCoTLLMPrecision RecallF1-ScorePrecision RecallF1-ScorePrecision RecallF1-ScoreClaude-3 Opus0.4200.2880.3390.4660.6190.5240.5520.7980.646GPT4.50.3150.3760.3390.5990.6990.6230.5920.8570.693GPTo30.2890.3260.3080.5650.7220.6290.5610.7580.641GT4o0.2470.2990.2690.3320.6840.4410.5030.6860.576Gemini 2.00.3090.3280.3120.3970.6220.4680.5550.7270.625</p>
<p>Table 3 :
3
Repeated Measure ANOVA for Condition Effects
ConditionNMean F1St DevF Valuedf1df2p-valueZero Shot800.3140.052Few Shot800.5370.13587.39228&lt; 0.001CoT650.6360.117</p>
<p>Table 4 :
4
Pairwise Comparison of the Prompts
ComparisonMean diff.p-value Lower limitUpper limitCoT vs Few-Shot-0.09920.0473-0.1972-0.0012CoT vs Zero-Shot-0.3226&lt; 0.001-0.4206-0.2246Few-Shot vs Zero-Shot-0.2235&lt; 0.001-0.3215-0.1255</p>
<p>Table 5 :
5
Comparison of LLMs Performance in Zero-Shot Prompting
LLMMean F1F-Valuedf1df2p-valueClaude-3 Opus0.339GPT4.50.339GPTo30.3086.665460&lt; 0.001GT4o0.269Gemini 2.00.312</p>
<p>Table 6 :
6
Tukey's HSD Test for Zero Shot Prompting
LLM1LLM2Mean diff.p-valueLower limit Upper limitClaude-3 OpusGPT4.50.00021.0000-0.04540.0458Claude-3 OpusGPTo3-0.03060.3388-0.07620.0150Claude-3 OpusGT4o-0.06940.0006<em>-0.1150-0.0238Claude-3 OpusGemini 2.0-0.02620.4967-0.07190.0194GPT4.5GPTo3-0.03080.3326-0.07640.0148GPT4.5GT4o-0.06960.0005</em>-0.1152-0.0239GPT4.5Gemini 2.0-0.02640.4895-0.07210.0192GPTo3GT4o-0.03880.1337-0.08440.0069GPTo3Gemini 2.00.00440.9988-0.04120.0500GT4oGemini 2.00.04310.0728-0.00250.0887*Statistically significant results</p>
<p>Table 7 :
7
Comparison of LLMs Performance in Few-Shot Prompting
LLMMean F1F-Valuedf1df2p-valueClaude-3 Opus0.524GPT4.50.623GPTo30.62911.823460&lt; 0.001GT4o0.441Gemini 2.00.468</p>
<p>Table 8 :
8
Tukey's HSD Test for Few-Shot Prompting
LLM1LLM2Mean diff.p-valueLower limitUpper limitClaude-3 OpusGPT4.50.09940.1071-0.01280.2116Claude-3 OpusGPTo30.10560.0747-0.00660.2178Claude-3 OpusGT4o-0.08250.2503-0.19470.0297Claude-3 OpusGemini 2.0-0.05630.6285-0.16840.0559GPT4.5GPTo30.00630.9999-0.10590.1184GPT4.5GT4o-0.18190.0002<em>-0.2941-0.0697GPT4.5Gemini 2.0-0.15560.0020</em>-0.2678-0.0434GPTo3GT4o-0.18810.0001<em>-0.3003-0.0759GPTo3Gemini 2.0-0.16190.0012</em>-0.2741-0.0497GT4oGemini 2.00.02620.9654-0.08590.1384*Statistically significant results</p>
<p>Table 9 :
9
Comparison of LLMs Performance in CoT Prompting
LLMMean F1F-Valuedf1df2p-valueClaude-3 Opus0.646GPT4.50.693GPTo30.6412.8944480.032GT4o0.576Gemini 2.00.625To further explore whether the observed differences in F1-scores among LLMs under Chain-of-Thoughtprompting were statistically meaningful, we conducted a Tukey's HSD post-hoc test (table 10) similar toother groups.Table 10: Tukey's HSD Test for CoT PromptingLLM1LLM2Mean diff.p-value Lower limitUpper limitClaude-3 OpusGPT4.50.04690.8307-0.07870.1726Claude-3 OpusGPTo3-0.00541.000-0.1310.1203Claude-3 OpusGT4o-0.070.5241-0.19560.0556Claude-3 OpusGemini 2.0-0.02150.9887-0.14720.1041GPT4.5GPTo3-0.05230.7677-0.17790.0733GPT4.5GT4o-0.11690.0799-0.24260.0087GPT4.5Gemini 2.0-0.06850.5459-0.19410.0572GPTo3GT4o-0.06460.6006-0.19030.061GPTo3Gemini 2.0-0.01620.9962-0.14180.1095GT4oGemini 2.00.04850.8136-0.07720.1741</p>
<p>Tukey's honestly significant difference (HSD) test. H Abdi, L J Williams, Encyclopedia of research design. Sage Thousand Oaks, CA20103</p>
<p>ChatGPT to Support Critical Thinking in Construction-Management Students. D E Abril, M A Guerra, S D Ballen, 2024 ASEE Annual Conference &amp; Exposition. 2024</p>
<p>Construction Site Hazards Identification Using Deep Learning and Computer Vision. M M Alateeq, F R Fathimathul, M A S Ali, 10.3390/SU15032358Sustainability. 15323582023. 2023Multidisciplinary Digital Publishing Institute</p>
<p>Enhancing construction hazard recognition and communication with energy-based cognitive mnemonics and safety meeting maturity model: Multiple baseline study. A Albert, M R Hallowell, B M Kleiner, 10.1061/(ASCE)CO.1943-7862.0000790J Constr Eng Manag. 21402014</p>
<p>Empirical measurement and improvement of hazard recognition skill. A Albert, M R Hallowell, M Skaggs, B Kleiner, 10.1016/j.ssci.2016.11.007Saf Sci. 932017Elsevier Ltd</p>
<p>Developing hazard recognition skill among the next-generation of construction professionals. A Albert, I Jeelani, K Han, Construction management and economics. 2020a38</p>
<p>Focus on the fatal-four: Implications for construction hazard recognition. A Albert, B Pandit, Y Patil, 10.1016/j.ssci.2020.104774Saf Sci. 1281047742020b. August 2019Elsevier</p>
<p>Does the potential safety risk affect whether particular construction hazards are recognized or not?. A Albert, B Pandit, Y Patil, J Louis, 10.1016/j.jsr.2020.10.004National Safety Council. 752020cElsevier LtdJ Safety Res</p>
<p>Transformer machine learning language model for autoalignment of long-term and short-term plans in construction. F Amer, Y Jung, M Golparvar-Fard, 10.1016/J.AUTCON.2021.103929Autom Constr. 1321039292021Elsevier</p>
<p>The role of work experience on hazard identification: Assessing the mediating effect of inattention under fall-hazard conditions. O Aroke, B Esmaeili, S Hasanzadeh, M D Dodd, R Brock, 10.1061/9780784482872.055;WEBSITE:WEBSITE:ASCE-SITEConstruction Research Congress 2020: Safety, Workforce, and Education -Selected Papers from the Construction Research Congress 2020. ASCE2020</p>
<p>Computer vision and IoT research landscape for health and safety management on construction sites. S Arshad, O Akinade, S Bello, M Bilal, 10.1016/J.JOBE.2023.107049Journal of Building Engineering. 761070492023Elsevier</p>
<p>Incidence and costs of family member hospitalization following injuries of workers' compensation claimants. A Asfaw, R Pana-Cryan, P T Bushnell, 10.1002/AJIM.22110;SUBPAGE:STRING:FULLAm J Ind Med. 55112012John Wiley &amp; Sons, Ltd</p>
<p>Impact of Energy-Based Safety Training on Quality of Prejob Safety Meetings and Control of Hazardous Energy in Construction: Multiple Baseline Experiment. A Bayona, S M Asce, M R Hallowell, A M Asce, S Bhandari, N Moyen, A Lien, 10.1061/JCEMD4.COENG-15563J Constr Eng Manag. 151740250862025American Society of Civil Engineers</p>
<p>Costs of occupational musculoskeletal disorders (MSDs) in the United States. A Bhattacharya, 10.1016/J.ERGON.2014.01.008Int J Ind Ergon. 4432014Elsevier</p>
<p>Injuries, Illnesses, and Fatalities. BLS. 2024U.S. Bureau of Labor Statistics. May 24, 2025</p>
<p>Exploring Precision and Recall to assess the quality and diversity of LLMs. F Bronnec, A Le, B Verine, Y Negrevergne, A Chevaleyre, Allauzen, 2024</p>
<p>Language Models are Few-Shot Learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Adv Neural Inf Process Syst. 332020</p>
<p>Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple Clinical and Research Scenarios. M Cascella, J Montomoli, V Bellini, E Bignami, 10.1007/S10916-023-01925-4/TABLES/2J Med Syst. 4712023Springer</p>
<p>Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. B Chen, Z Zhang, N Langrené, S Zhu, 10.1016/j.patter.2025.1012602023</p>
<p>Large Language Models (LLM) in Industry: A Survey of Applications, Challenges, and Trends. Z Chkirbene, R Hamila, A Gouissem, U Devrim, 10.1109/HONET63146.2024.108228852024 IEEE 21st International Conference on Smart Communities: Improving Quality of Life using AI, Robotics and IoT, HONET 2024. Institute of Electrical and Electronics Engineers Inc2024</p>
<p>What is an LLM (large language model)? | Cloudflare. Cloudflare, 2024. May 25, 2025CloudFlare</p>
<p>Explaining psychological statistics. B H Cohen, Construction Industry Institute. 2013. Strategies for Improving Hazard Recognition. John Wiley &amp; Sons2008Version 1.1</p>
<p>Detection of Personal Protective Equipment (PPE) Compliance on Construction Site Using Computer Vision Based Deep Learning Techniques. V S K Delhi, R Sankarlal, A Thomas, 10.3389/FBUIL.2020.00136/BIBTEXFront Built Environ. 65406032020</p>
<p>Machine Learning Evaluation of Imbalanced Health Data: A Comparative Analysis of Balanced Accuracy, MCC, and F1 Score. R Diallo, C Edalo, O O Awe, 10.1007/978-3-031-72215-8_12STEAM-H: Science, Technology, Engineering, Agriculture, Mathematics and Health, Part F4005. ChamSpringer2025</p>
<p>Automated detection of workers and heavy equipment on construction sites: A convolutional neural network approach. W Fang, L Ding, B Zhong, P E D Love, H Luo, 10.1016/J.AEI.2018.05.003Advanced Engineering Informatics. 372018Elsevier</p>
<p>Discovering statistics using IBM SPSS statistics. A Field, 2024Sage publications limited</p>
<p>The impact of regulatory enforcement and consultation visits on workers' compensation claims incidence rates and costs, 1999-2008. M Foley, Z J Fan, E Rauser, B Silverstein, 10.1002/AJIM.22084;PAGE:STRING:ARTICLE/CHAPTERAm J Ind Med. 55112012John Wiley &amp; Sons, Ltd</p>
<p>A Smart System for Personal Protective Equipment Detection in Industrial Environments Based on Deep Learning at the Edge. G Gallo, F Di Rienzo, F Garzelli, P Ducange, C Vallati, 10.1109/ACCESS.2022.3215148IEEE Access. 102022Institute of Electrical and Electronics Engineers Inc</p>
<p>Prompt Engineering for Large Language Models. A Gao, 10.2139/SSRN.4504303SSRN Electronic Journal. Elsevier BV. 2023</p>
<p>Large Language Models (LLMs) with Google AI | Google Cloud. Google, 2024. May 25, 2025</p>
<p>Classification: Accuracy, recall, precision, and related metrics | Machine Learning | Google for Developers. Google, 2025. May 25, 2025</p>
<p>Predicting safety behavior in the construction industry: Development and test of an integrative model. B H W Guo, T W Yiu, V A González, 10.1016/J.SSCI.2015.11.020Saf Sci. 842016Elsevier</p>
<p>An empirical study of prompt mode in code generation based on ChatGPT. H Guo, 10.54254/2755-2721/73/20240367Applied and Computational Engineering. 7312024EWA Publishing</p>
<p>Energy damage and the ten countermeasure strategies. W Haddon, 10.1097/00005373-197304000-00011Infection and Critical Care. 1341973Journal of Trauma -Injury</p>
<p>Measuring and improving designer hazard recognition skill: Critical competency to enable prevention through design. M R Hallowell, D Hansen, 10.1016/j.ssci.2015.09.005Saf Sci. 822016Elsevier Ltd</p>
<p>Role of Personality in Construction Safety: Investigating the Relationships between Personality, Attentional Failure, and Hazard Identification under Fall-Hazard Conditions. S Hasanzadeh, B Dao, B Esmaeili, M D Dodd, 10.1061/(ASCE)CO.1943-7862.0001673/ASSET/95CBA744-B982-4EA7-A90C-0115996ECDFE/ASSETS/IMAGES/LARGE/FIGURE3.JPGJ Constr Eng Manag. 145940190522019ASCE</p>
<p>Y He, Z Liu, J Chen, Z Tian, H Liu, X Chi, R Liu, R Yuan, Y Xing, W Wang, J Dai, Y Zhang, W Xue, Q Liu, Y Guo, Q Chen, LLMs Meet Multimodal Generation and Editing: A Survey. 2024</p>
<p>Conversational AI-based VR system to improve construction safety training of migrant workers. R Hussain, A Sabir, D Y Lee, S F A Zaidi, A Pedro, M S Abbas, C Park, 10.1016/J.AUTCON.2024.105315Autom Constr. 1601053152024Elsevier</p>
<p>Occupational Safety and Health Management in the Construction Sector | International Labour Organization. ILO. 2021May 28, 2025International Labour Organization</p>
<p>Development and Testing of a Personalized Hazard-Recognition Training Intervention. I Jeelani, A Albert, R Azevedo, E J Jaselskis, 10.1061/(asce)co.1943-7862.0001256J Constr Eng Manag. 143540161202017a</p>
<p>Why Do Construction Hazards Remain Unrecognized at the Work Interface?. I Jeelani, A Albert, J A Gambatese, 10.1061/(ASCE)CO.1943-7862.0001274J Constr Eng Manag. 14352017b</p>
<p>Real-Time Hazard Proximity Detection-Localization of Workers Using Visual Data. I Jeelani, H Ramshankar, K Han, A Albert, K Asadi, Computing in Civil Engineering. 2019. 2019American Society of Civil Engineers</p>
<p>BIM-based hazard recognition and evaluation methodology for automating construction site risk assessment. I Kim, Y Lee, J Choi, 10.3390/app10072335Applied Sciences (Switzerland). 7102020</p>
<p>Large Language Models are Zero-Shot Reasoners. T Kojima, S Shane Gu, M Reid Google Research, Y Matsuo, Y Iwasawa, Adv Neural Inf Process Syst. 352022</p>
<p>Large Language Models For Text Classification: Case Study And Comprehensive Review. A Kostina, M D Dikaiakos, D Stefanidis, G Pallis, 2025</p>
<p>Construction Site Safety Management: A Computer Vision and Deep Learning Approach. J Lee, S Lee, 10.3390/S23020944Sensors. 2329442023. 2023Multidisciplinary Digital Publishing Institute</p>
<p>A critical review of virtual and augmented reality (VR/AR) applications in construction safety. X Li, W Yi, H.-L Chi, X Wang, A P C Chan, 10.1016/j.autcon.2017.11.003Autom Constr. 862018Elsevier B</p>
<p>Occupational health and safety in the construction industry. H Lingard, 10.1080/01446193.2013.816435Construction Management and Economics. 3162013</p>
<p>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, 10.1145/3560815/SUPPL_FILE/3560815-APP.PDFACM Comput Surv. 9552023Association for Computing Machinery</p>
<p>An artificial intelligencebased risk prediction model of myocardial infarction. R Liu, M Wang, T Zheng, R Zhang, N Li, Z Chen, H Yan, Q Shi, 10.1186/S12859-022-04761-4/TABLES/7BMC Bioinformatics. 2312022BioMed Central Ltd</p>
<p>Designing experiments and analyzing data: A model comparison perspective. S E Maxwell, H D Delaney, K Kelley, 2017Routledge</p>
<p>Multimodal Trip Hazard Affordance Detection on Construction Sites. S Mcmahon, N Sunderhauf, B Upcroft, M Milford, 10.1109/LRA.2017.2719763IEEE Robot Autom Lett. 312018Institute of Electrical and Electronics Engineers Inc</p>
<p>Automated detection of contractual risk clauses from construction specifications using bidirectional encoder representations from transformers (BERT). S Moon, S Chi, S B Im, 10.1016/J.AUTCON.2022.104465142: 104465Autom Constr. 2022Elsevier</p>
<p>An LLM-Driven Chatbot in Higher Education for Databases and Information Systems. A T Neumann, Y Yin, S Sowe, S Decker, M Jarke, 10.1109/TE.2024.34679122024Institute of Electrical and Electronics Engineers Inc</p>
<p>Investigating the Use of ChatGPT for the Scheduling of Construction Projects. S A Prieto, E T Mengiste, B García De Soto, 10.3390/buildings13040857Buildings. 1348572023</p>
<p>Integrating Building Information Modelling (BIM) with ChatGPT, Bard, and similar generative artificial intelligence in the architecture, engineering, and construction industry: applications, a novel framework, challenges, and future scope. N Rane, S Choudhary, J Rane, 10.2139/ssrn.4645601SSRN Electronic Journal. 2023</p>
<p>Construction Job Safety Analysis. O Rozenfeld, R Sacks, Y Rosenfeld, H Baum, 10.1016/j.ssci.2009.12.017Saf Sci. 4842010Elsevier</p>
<p>Highway Construction Safety Analysis Using Large Language Models. M Smetana, L Salles De Salles, I Sukharev, L Khazanovich, 10.3390/APP14041352Applied Sciences. 14413522024. 2024Multidisciplinary Digital Publishing Institute</p>
<p>Proposing and Validating a New Way of Construction Hazard Recognition Training in Academia: Mixed-Method Approach. A J Tixier, .-P , A Albert, M R Hallowell, 10.1061/(ASCE)SC.1943-5576.0000347/ASSET/8CF63F3D-5290-4940-8AA6-1EEA11A7754D/ASSETS/IMAGES/LARGE/FIGURE8.JPGPractice Periodical on Structural Design and Construction. 23140170272018ASCE</p>
<p>Hazard Recognition Patterns Demonstrated by Construction Workers. S M J Uddin, A Albert, A Alsharef, B Pandit, Y Patil, C Nnaji, 10.3390/ijerph17217788Int J Environ Res Public Health. 172177882020</p>
<p>Leveraging ChatGPT to Aid Construction Hazard Recognition and Support Safety Education and Training. S M J Uddin, A Albert, A Ovid, A Alsharef, 10.3390/su15097121Sustainability. 15971212023</p>
<p>Harnessing the power of ChatGPT to promote Construction Hazard Prevention through Design (CHPtD). S M J Uddin, A Albert, M Tamanna, 10.1108/ECAM-03-2024-03142024aEngineering, Construction and Architectural Management</p>
<p>ChatGPT as an Educational Resource for Civil Engineering Students. S M J Uddin, A Albert, M Tamanna, A Ovid, A Alsharef, 2024bComputer Applications in Engineering Education</p>
<p>Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. B Wang, S Min, X Deng, J Shen, Y Wu, L Zettlemoyer, H Sun, 10.18653/v1/2023.acl-long.153Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics20221</p>
<p>Risk Assessment of Work-Related Musculoskeletal Disorders in Construction: State-of-the-Art Review. D Wang, F Dai, X Ning, 10.1061/(ASCE)CO.1943-7862.0000979/ASSET/7A457209-4BF7-4BE0-AFE6-ECA6E12E382B/ASSETS/IMAGES/LARGE/FIGURE5.JPGJ Constr Eng Manag. 141640150082015ASCE</p>
<p>Review of large vision models and visual prompt engineering. J Wang, Z Liu, L Zhao, Z Wu, C Ma, S Yu, H Dai, Q Yang, Y Liu, S Zhang, E Shi, Y Pan, T Zhang, D Zhu, X Li, X Jiang, B Ge, Y Yuan, D Shen, T Liu, S Zhang, 10.1016/j.metrad.2023.100047Meta-Radiology. 132023KeAi Publishing Communications Ltd</p>
<p>Prompt engineering in consistency and reliability with the evidence-based guideline for LLMs. L Wang, X Chen, X W Deng, H Wen, M K You, W Z Liu, Q Li, J Li, 10.1038/S41746-024-01029-4Nature Research. 712024aNPJ Digit Med</p>
<p>SUBJMETA=2778. KWRD=MATHEMATICS+AND+COMPUTING,PRECLIN ICAL+RESEARCH308</p>
<p>An LLM-based vision and language cobot navigation approach for Human-centric Smart Manufacturing. T Wang, J Fan, P Zheng, 10.1016/J.JMSY.2024.04.020J Manuf Syst. 752024bElsevier</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi Quoc, V Le, D Zhou, Adv Neural Inf Process Syst. 352022</p>
<p>An analysis of permanent work disability among construction sheet metal workers. G H West, J Dawson, C Teitelbaum, R Novello, K Hunting, L S Welch, 10.1002/AJIM.22545;REQUESTEDJOURNAL:JOURNAL:10970274;WGROUP:STRING:PUBLICATIONAm J Ind Med. 5932016Wiley-Liss Inc</p>
<p>J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, A Elnashar, J Spencer-Smith, D C Schmidt, A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. 2023</p>
<p>How to Unleash the Power of Large Language Models for Few-shot Relation Extraction. X Xu, Y Zhu, X Wang, N Zhang, 10.18653/v1/2023.sustainlp-1.13Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2023</p>
<p>Probabilistic Extension of Precision, Recall, and F1 Score for More Thorough Evaluation of Classification Models. R Yacouby Amazon Alexa, 10.18653/V1/2020.EVAL4NLP-1.92020Association for Computational LinguisticsAxman Amazon Alexa</p>
<p>Prompt engineering for zero-shot and few-shot defect detection and classification using a visual-language pretrained model. G Yong, K Jeon, D Gil, G Lee, 10.1111/MICE.12954Computer-Aided Civil and Infrastructure Engineering. 38112023John Wiley &amp; Sons, Ltd</p>
<p>Robot-Enabled Construction Assembly with Automated Sequence Planning Based on ChatGPT: RoboGPT. H You, Y Ye, T Zhou, Q Zhu, J Du, 10.3390/buildings13071772Buildings. 13717722023</p>
<p>Z Yu, L He, Z Wu, X Dai, J Chen, Towards Better Chain-of-Thought Prompting Strategies: A Survey. 2023</p>
<p>Capabilities and Limitations of Large Language Models. I Yurchak, O O Yu, V M Kychuk, A O Oksentyuk, Khich, 10.23939/csn2024.02.268Computer systems and network. 622024</p>
<p>Impact of ChatGPT on Student Writing in Construction Management: Analyzing Literature and Countermeasures for Writing Intensive Courses. T Zhao, M Ronald Chance, C Buckhalter, G Wang, Proceedings of the 60th Annual Associated Schools. the 60th Annual Associated Schools20245</p>
<p>BIM-GPT: a Prompt-Based Virtual Assistant Framework for BIM Information Retrieval. J Zheng, M Fischer, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-275 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-275</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-275</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-258762507</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.01665v3.pdf" target="_blank">Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks</a></p>
                <p><strong>Paper Abstract:</strong> Language models' (LMs) proficiency in handling deterministic symbolic reasoning and rule-based tasks remains limited due to their dependency implicit learning on textual data. To endow LMs with genuine rule comprehension abilities, we propose"Neural Comprehension"- a framework that synergistically integrates compiled neural networks (CoNNs) into the standard transformer architecture. CoNNs are neural modules designed to explicitly encode rules through artificially generated attention weights. By incorporating CoNN modules, the Neural Comprehension framework enables LMs to accurately and robustly execute rule-intensive symbolic tasks. Extensive experiments demonstrate the superiority of our approach over existing techniques in terms of length generalization, efficiency, and interpretability for symbolic operations. Furthermore, it can be applied to LMs across different model scales, outperforming tool-calling methods in arithmetic reasoning tasks while maintaining superior inference efficiency. Our work highlights the potential of seamlessly unifying explicit rule learning via CoNNs and implicit pattern learning in LMs, paving the way for true symbolic comprehension capabilities.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e275.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e275.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Comprehension (NC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Comprehension: integrating pretrained LMs with Compiled Neural Networks (CoNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MoE-style framework that plugs small, rule-compiled transformer modules (CoNNs) into a pretrained language model; CoNNs are activated by gating to deterministically execute rule-intensive symbolic and arithmetic operations while the LM handles language/context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Comprehension (LM + CoNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Mixture-style integration: pretrained LM (decoder or encoder-decoder) fused with transformer-based CoNN modules (attention + MLP)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Addition and Subtraction (explicit CoNNs implemented); authors note CoNNs can be extended to multiplication, division, carry/adder primitives and linear-algebraic computations</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>AddSub+ experiments: 1–20 digits (synthetic AddSub+), length-generalization experiments test up to 30–40 digits (training in-distribution typically 10–20 digits); CoNN addition supports natural numbers (no negatives), subtraction CoNN assumes minuend > subtrahend</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Integration of compiled transformer modules (CoNNs) into LM decoding through gating; token-by-token generation where each token can be produced by LM or a CoNN; CoNNs compiled from RASP/Tracr code implementing Select/Aggregate/Zipmap primitives; few-shot/scratchpad experiments also compared</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On symbolic operations (e.g., parity, reverse, addition/subtraction as symbolic sequence ops) NC attains near-perfect / "flawless" deterministic accuracy when correct CoNNs are provided; on arithmetic reasoning benchmarks NC improves solve rates over vanilla Chain-of-Thought baselines and in some experiments matches or exceeds tool-based methods' solve rates (e.g., reported improvements on AddSub variants and modest improvements on GSM8K/subsets). Inference overhead is small (reported latency increase ≈1–3%).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>CoNNs explicitly encode rules into attention weights and MLPs: attention implementations carry out 'Select' and 'Aggregate' operations while MLP layers implement 'Zipmap' (per-token mapping). Addition is implemented by an add_in_same_position CoNN plus an add_carry CoNN that is iterated L times to propagate carries. A gating mechanism decides when CoNN output (deterministic rule execution) replaces LM output. From a gradient perspective, NC decomposes gradients into text-derived (G_T) and rule-derived (G_R) components, providing explicit rule gradients that improve optimization for rule tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Framework shown applicable across model scales (T5-small/base/large; LLaMA-2 7B/70B; GLM-130B). Small CoNN parameter counts (1/1,000 to 1/1,500,000 of GPT-3) achieve deterministic rule execution; NC inference overhead proportionally decreases as LM size increases.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>CoNNs used in the paper have functional constraints: Addition CoNN supports only natural numbers; Subtraction CoNN requires minuend > subtrahend. AutoCoNN (automatic CoNN generation) fails on more complex CoNNs (Addition/Subtraction) in experiments. NC requires correct CoNN implementations; if CoNN is absent/miscompiled, LM fallback may be incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with vanilla fine-tuning (T5), few-shot prompting (GPT-3.5/GPT-4), Chain-of-Thought/Scratchpad/Algorithmic prompting, and tool-based approaches (PAL). NC outperforms vanilla fine-tune and few-shot on length-generalization and symbolic tasks, shows competitive or better efficiency relative to tool-based methods, and complements CoT techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Explicitly compiling arithmetic/symbolic rules into small transformer modules and integrating them into LMs yields deterministic, length-generalizing arithmetic behavior and substantially improves arithmetic reasoning and symbolic operation accuracy with minimal runtime overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e275.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e275.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compiled Neural Network (CoNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-structured neural module whose weights are hand-compiled (via RASP/Tracr) to implement deterministic sequence transformations (Select, Aggregate, Zipmap) so that human-specified symbolic rules execute perfectly within a neural architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Compiled Neural Network (CoNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Transformer-based module with artificially generated attention weights and MLPs (attention layers implement Select/Aggregate; MLP implements Zipmap); compiled from RASP via Tracr into transformer weights</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Digitwise addition and subtraction (split/atoi/add_in_same_position, add_carry loop), parity, reverse, last-letter-concatenation, copy; authors state CoNNs can be designed for multiplication/division and other linear-algebraic computations</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Addition CoNN designed for natural numbers; carry module applied cyclically L times to propagate carries; experiments used numbers up to tens of digits (AddSub CoNNs sized to handle L positions)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Rule programs written in RASP, compiled with Tracr into transformer weights; CoNNs saved deterministically (dropout=0) and plugged into LM via gating</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When CoNNs are correctly compiled and integrated they produce deterministic, exact outputs (e.g., parity, reversal, addition/subtraction on supported ranges) with near-100% accuracy; parameter-efficient (some CoNNs have millions to thousands of parameters and are orders of magnitude smaller than typical LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Provides a mechanistic mapping from symbolic primitives to transformer internals: Select realized by attention key-query matching, Aggregate by key-query multiplied with values, Zipmap by per-token MLP mappings. Addition implemented as two-stage process (positionwise add + carry propagation via a separate CoNN repeated L times).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Individual CoNN modules are tiny relative to large LMs (examples: parity CoNN ≈2.2M params ≈1/100,000 of GPT-3; addition modules can be ~1–2M or larger depending on design); therefore many CoNNs can be combined with low overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>CoNNs are only as general as the compiled program: authors limited some CoNNs (e.g., Add/Sub only natural numbers and some constraints). Also practical runtime limits: decode vocabulary size for CoNN outputs d_C was limited (<100) due to compute constraints in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Contrasted with learned LM behavior; compared as a plug-in replacement for algorithmic components normally learned implicitly by LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Compiling symbolic algorithms into attention/MLP weights produces small deterministic neural modules that perfectly implement arithmetic primitives (select, aggregate, per-token maps), enabling exact arithmetic behavior when integrated with LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e275.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e275.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5 (fine-tuned) length-generalization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 family (fine-tuned on synthetic arithmetic/symbolic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>T5 models fine-tuned on synthetic addition/subtraction (and other symbolic tasks) achieve high in-distribution accuracy but fail to generalize reliably to out-of-distribution lengths (longer or shorter sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>small: 60M; base: 220M; large: 770M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Encoder-decoder transformer (T5)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Addition and Subtraction (sequence-to-sequence symbolic forms), Parity, Reverse, Last-letter concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Trained in experiments on 10–20 digit addition/subtraction for length-generalization tests; other experiments used 3–30 digits for evaluation with in-distribution typically 10–20 digits (99k training samples per length in some setups)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Standard fine-tuning on synthetic datasets (Adafactor optimizer, up to 20 epochs), training data produced with controlled digit lengths</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Very high in-domain (10–20 digit) validation accuracy (authors report within-distribution accuracy approaching 100%), but performance deteriorates strongly for shorter and longer lengths (out-of-distribution); increasing training iterations or more in-domain data produced only marginal OOD improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Fine-tuned T5s appear to rely on statistical pattern learning and position biases learned from training data rather than learning underlying symbolic algorithms; gradient descent learns solutions specialized to training length distribution rather than length-general algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>All T5 sizes achieve in-domain convergence; OOD behavior remains poor across sizes and does not significantly improve by more training on in-distribution samples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Severe length generalization failure: degrade on very short and longer sequences than training distribution; fails to learn carry propagation robustly across arbitrary lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against few-shot LMs (GPT-3.5/GPT-4), CoT/Scratchpad/Algorithmic methods, and Neural Comprehension; baseline shows NC consistently outperforms vanilla fine-tuning on symbolic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Fine-tuning LMs on fixed-length synthetic arithmetic yields high in-distribution accuracy but poor extrapolation to different lengths, indicating statistical rather than algorithmic learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e275.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e275.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5/GPT-4 few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 / GPT-4 (few-shot prompting experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoder-only LLMs provided with a small number of in-context examples show competent arithmetic for in-distribution digit-lengths but performance drops with increasing digit length; Chain-of-Thought style prompting improves generalization but is computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4 (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Addition (length-generalization across 3–30 digits) and general arithmetic reasoning (benchmarks like AddSub, GSM8K subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Few-shot examples provided within 10 digits (8 examples in some experiments); tests extended to 3–30 digits (1000 samples per digit length)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Few-shot prompting (8 examples), Chain-of-Thought / Scratchpad and Algorithmic prompting evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Vanilla few-shot performance degrades for >10 digits; CoT-like methods (Scratchpad, Algorithmic) show more robust length generalization and better solve rates on long-digit tasks but are limited by context window and compute; exact numeric performance curves are reported in figures (performance falls as digit length increases).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>These LMs rely on pattern/statistical inference from examples; CoT/Scratchpad helps by forcing explicit stepwise decomposition and intermediate-state tracking (e.g., carry recording), which resembles algorithmic processing and improves generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Scaling to GPT-4 gives better absolute performance but length generalization remains a challenge even for very large models; CoT methods mitigate but do not fully solve OOD length scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Accuracy decreases with digit length and outside training/example lengths; constrained by context length limits and computational cost for long decompositions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to fine-tuned T5, NC-integrated LMs, CoT and tool-based methods; NC often outperforms vanilla few-shot and provides a complementary approach to CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Few-shot LLMs can solve arithmetic in-distribution but struggle to extrapolate to longer inputs; forcing step-by-step decomposition (CoT/Scratchpad) helps, but explicit compiled modules (NC/CoNN) yield deterministic correctness and better length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e275.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e275.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought / Scratchpad / Algorithmic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) / Scratchpad / Algorithmic prompting techniques</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategies that require models to produce intermediate reasoning steps (scratchpads) or to record algorithmic state (e.g., carry operations) to improve performance on multi-step arithmetic and length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT / Scratchpad / Algorithmic (prompting methods)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Multi-step arithmetic (addition with carries, algorithmic bookkeeping), general arithmetic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Applied to multi-digit arithmetic up to the experimental range (3–30 digits); helps particularly for >10-digit extrapolation</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Chain-of-Thought prompting, scratchpad-style stepwise generation, Algorithmic prompting that explicitly records carries</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported to substantially improve length generalization relative to vanilla few-shot and fine-tuning in experiments; capable of solving longer-digit addition tasks more robustly than direct prompting, but at the cost of extra computation and subject to context-window limits.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Effectiveness comes from explicit decomposition: by exposing intermediate states (digitwise sums, carries), the model can emulate an algorithmic process rather than relying on single-token statistical mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Requires more compute and longer context windows; scaling helps but context size remains a limiting factor for arbitrarily long digit sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High computational cost and inability to scale beyond context window; increased chance of compounding intermediate-step errors unless combined with verification/self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with vanilla few-shot, fine-tuned LMs, and Neural Comprehension; NC claims similar algorithmic robustness without needing stepwise generation and with smaller runtime overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Decomposing arithmetic into intermediate steps via CoT/Scratchpad improves extrapolation and carry-handling, but is resource-intensive and limited by input length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e275.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e275.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAL / Tool-based</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PAL: Program-aided Language models and other tool-based methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches where language models generate code or call external tools/interpreters to compute arithmetic exactly (e.g., run a program to calculate results), contrasted with NC's in-network compiled modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PAL / Tool-based methods</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>Arithmetic reasoning and arithmetic word problems solved via external code execution (addition/subtraction, general arithmetic, programmatic computation)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Can handle arbitrary numeric ranges limited by interpreter precision and LM code-generation correctness</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Tool-calling: generate code / call external interpreter (PAL); not end-to-end differentiable; requires models with code generation ability</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Tool-based approaches (PAL) yield strong arithmetic performance in prior work; in the paper NC is reported to match or outperform tool-based approaches on some arithmetic reasoning tasks while being more efficient and end-to-end differentiable (reported comparisons in Table 2 and text).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Tool methods outsource exact computation to external executors — accuracy depends on code-generation correctness and ability to call interpreters; not differentiable inside LM training loop.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Only applicable to models that can reliably generate and call code; NC advantage is applicability to smaller LMs that lack code-gen capability.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Relies on external interpreter and code-generation correctness; increased latency and engineering complexity; non-differentiable limits integration with learning-based adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Neural Comprehension and vanilla CoT; NC offers comparable symbolic processing performance but with higher inference efficiency and differentiability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Tool-based execution yields exact computation but requires code-gen and external execution; compiled in-network modules (NC/CoNNs) can deliver similar deterministic arithmetic behavior within the neural model with less engineering and better differentiability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring length generalization in large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tracr: Compiled transformers as a laboratory for interpretability <em>(Rating: 2)</em></li>
                <li>Looped Transformers as Programmable Computers <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided Language models <em>(Rating: 2)</em></li>
                <li>What can transformers learn in-context? a case study of simple function classes <em>(Rating: 2)</em></li>
                <li>Are nlp models really able to solve simple math word problems? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-275",
    "paper_id": "paper-258762507",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Neural Comprehension (NC)",
            "name_full": "Neural Comprehension: integrating pretrained LMs with Compiled Neural Networks (CoNNs)",
            "brief_description": "A MoE-style framework that plugs small, rule-compiled transformer modules (CoNNs) into a pretrained language model; CoNNs are activated by gating to deterministically execute rule-intensive symbolic and arithmetic operations while the LM handles language/context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Neural Comprehension (LM + CoNNs)",
            "model_size": null,
            "model_architecture": "Mixture-style integration: pretrained LM (decoder or encoder-decoder) fused with transformer-based CoNN modules (attention + MLP)",
            "arithmetic_operation_type": "Addition and Subtraction (explicit CoNNs implemented); authors note CoNNs can be extended to multiplication, division, carry/adder primitives and linear-algebraic computations",
            "number_range_or_complexity": "AddSub+ experiments: 1–20 digits (synthetic AddSub+), length-generalization experiments test up to 30–40 digits (training in-distribution typically 10–20 digits); CoNN addition supports natural numbers (no negatives), subtraction CoNN assumes minuend &gt; subtrahend",
            "method_or_intervention": "Integration of compiled transformer modules (CoNNs) into LM decoding through gating; token-by-token generation where each token can be produced by LM or a CoNN; CoNNs compiled from RASP/Tracr code implementing Select/Aggregate/Zipmap primitives; few-shot/scratchpad experiments also compared",
            "performance_result": "On symbolic operations (e.g., parity, reverse, addition/subtraction as symbolic sequence ops) NC attains near-perfect / \"flawless\" deterministic accuracy when correct CoNNs are provided; on arithmetic reasoning benchmarks NC improves solve rates over vanilla Chain-of-Thought baselines and in some experiments matches or exceeds tool-based methods' solve rates (e.g., reported improvements on AddSub variants and modest improvements on GSM8K/subsets). Inference overhead is small (reported latency increase ≈1–3%).",
            "mechanistic_insight": "CoNNs explicitly encode rules into attention weights and MLPs: attention implementations carry out 'Select' and 'Aggregate' operations while MLP layers implement 'Zipmap' (per-token mapping). Addition is implemented by an add_in_same_position CoNN plus an add_carry CoNN that is iterated L times to propagate carries. A gating mechanism decides when CoNN output (deterministic rule execution) replaces LM output. From a gradient perspective, NC decomposes gradients into text-derived (G_T) and rule-derived (G_R) components, providing explicit rule gradients that improve optimization for rule tasks.",
            "performance_scaling": "Framework shown applicable across model scales (T5-small/base/large; LLaMA-2 7B/70B; GLM-130B). Small CoNN parameter counts (1/1,000 to 1/1,500,000 of GPT-3) achieve deterministic rule execution; NC inference overhead proportionally decreases as LM size increases.",
            "failure_modes": "CoNNs used in the paper have functional constraints: Addition CoNN supports only natural numbers; Subtraction CoNN requires minuend &gt; subtrahend. AutoCoNN (automatic CoNN generation) fails on more complex CoNNs (Addition/Subtraction) in experiments. NC requires correct CoNN implementations; if CoNN is absent/miscompiled, LM fallback may be incorrect.",
            "comparison_baseline": "Compared with vanilla fine-tuning (T5), few-shot prompting (GPT-3.5/GPT-4), Chain-of-Thought/Scratchpad/Algorithmic prompting, and tool-based approaches (PAL). NC outperforms vanilla fine-tune and few-shot on length-generalization and symbolic tasks, shows competitive or better efficiency relative to tool-based methods, and complements CoT techniques.",
            "key_finding": "Explicitly compiling arithmetic/symbolic rules into small transformer modules and integrating them into LMs yields deterministic, length-generalizing arithmetic behavior and substantially improves arithmetic reasoning and symbolic operation accuracy with minimal runtime overhead.",
            "uuid": "e275.0",
            "source_info": {
                "paper_title": "Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "CoNN",
            "name_full": "Compiled Neural Network (CoNN)",
            "brief_description": "A transformer-structured neural module whose weights are hand-compiled (via RASP/Tracr) to implement deterministic sequence transformations (Select, Aggregate, Zipmap) so that human-specified symbolic rules execute perfectly within a neural architecture.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Compiled Neural Network (CoNN)",
            "model_size": null,
            "model_architecture": "Transformer-based module with artificially generated attention weights and MLPs (attention layers implement Select/Aggregate; MLP implements Zipmap); compiled from RASP via Tracr into transformer weights",
            "arithmetic_operation_type": "Digitwise addition and subtraction (split/atoi/add_in_same_position, add_carry loop), parity, reverse, last-letter-concatenation, copy; authors state CoNNs can be designed for multiplication/division and other linear-algebraic computations",
            "number_range_or_complexity": "Addition CoNN designed for natural numbers; carry module applied cyclically L times to propagate carries; experiments used numbers up to tens of digits (AddSub CoNNs sized to handle L positions)",
            "method_or_intervention": "Rule programs written in RASP, compiled with Tracr into transformer weights; CoNNs saved deterministically (dropout=0) and plugged into LM via gating",
            "performance_result": "When CoNNs are correctly compiled and integrated they produce deterministic, exact outputs (e.g., parity, reversal, addition/subtraction on supported ranges) with near-100% accuracy; parameter-efficient (some CoNNs have millions to thousands of parameters and are orders of magnitude smaller than typical LLMs).",
            "mechanistic_insight": "Provides a mechanistic mapping from symbolic primitives to transformer internals: Select realized by attention key-query matching, Aggregate by key-query multiplied with values, Zipmap by per-token MLP mappings. Addition implemented as two-stage process (positionwise add + carry propagation via a separate CoNN repeated L times).",
            "performance_scaling": "Individual CoNN modules are tiny relative to large LMs (examples: parity CoNN ≈2.2M params ≈1/100,000 of GPT-3; addition modules can be ~1–2M or larger depending on design); therefore many CoNNs can be combined with low overhead.",
            "failure_modes": "CoNNs are only as general as the compiled program: authors limited some CoNNs (e.g., Add/Sub only natural numbers and some constraints). Also practical runtime limits: decode vocabulary size for CoNN outputs d_C was limited (&lt;100) due to compute constraints in their experiments.",
            "comparison_baseline": "Contrasted with learned LM behavior; compared as a plug-in replacement for algorithmic components normally learned implicitly by LMs.",
            "key_finding": "Compiling symbolic algorithms into attention/MLP weights produces small deterministic neural modules that perfectly implement arithmetic primitives (select, aggregate, per-token maps), enabling exact arithmetic behavior when integrated with LMs.",
            "uuid": "e275.1",
            "source_info": {
                "paper_title": "Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "T5 (fine-tuned) length-generalization",
            "name_full": "T5 family (fine-tuned on synthetic arithmetic/symbolic tasks)",
            "brief_description": "T5 models fine-tuned on synthetic addition/subtraction (and other symbolic tasks) achieve high in-distribution accuracy but fail to generalize reliably to out-of-distribution lengths (longer or shorter sequences).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5 (fine-tuned)",
            "model_size": "small: 60M; base: 220M; large: 770M",
            "model_architecture": "Encoder-decoder transformer (T5)",
            "arithmetic_operation_type": "Addition and Subtraction (sequence-to-sequence symbolic forms), Parity, Reverse, Last-letter concatenation",
            "number_range_or_complexity": "Trained in experiments on 10–20 digit addition/subtraction for length-generalization tests; other experiments used 3–30 digits for evaluation with in-distribution typically 10–20 digits (99k training samples per length in some setups)",
            "method_or_intervention": "Standard fine-tuning on synthetic datasets (Adafactor optimizer, up to 20 epochs), training data produced with controlled digit lengths",
            "performance_result": "Very high in-domain (10–20 digit) validation accuracy (authors report within-distribution accuracy approaching 100%), but performance deteriorates strongly for shorter and longer lengths (out-of-distribution); increasing training iterations or more in-domain data produced only marginal OOD improvements.",
            "mechanistic_insight": "Fine-tuned T5s appear to rely on statistical pattern learning and position biases learned from training data rather than learning underlying symbolic algorithms; gradient descent learns solutions specialized to training length distribution rather than length-general algorithms.",
            "performance_scaling": "All T5 sizes achieve in-domain convergence; OOD behavior remains poor across sizes and does not significantly improve by more training on in-distribution samples.",
            "failure_modes": "Severe length generalization failure: degrade on very short and longer sequences than training distribution; fails to learn carry propagation robustly across arbitrary lengths.",
            "comparison_baseline": "Compared against few-shot LMs (GPT-3.5/GPT-4), CoT/Scratchpad/Algorithmic methods, and Neural Comprehension; baseline shows NC consistently outperforms vanilla fine-tuning on symbolic tasks.",
            "key_finding": "Fine-tuning LMs on fixed-length synthetic arithmetic yields high in-distribution accuracy but poor extrapolation to different lengths, indicating statistical rather than algorithmic learning.",
            "uuid": "e275.2",
            "source_info": {
                "paper_title": "Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "GPT-3.5/GPT-4 few-shot",
            "name_full": "GPT-3.5 / GPT-4 (few-shot prompting experiments)",
            "brief_description": "Decoder-only LLMs provided with a small number of in-context examples show competent arithmetic for in-distribution digit-lengths but performance drops with increasing digit length; Chain-of-Thought style prompting improves generalization but is computationally expensive.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 / GPT-4 (few-shot)",
            "model_size": null,
            "model_architecture": "Decoder-only transformer",
            "arithmetic_operation_type": "Addition (length-generalization across 3–30 digits) and general arithmetic reasoning (benchmarks like AddSub, GSM8K subsets)",
            "number_range_or_complexity": "Few-shot examples provided within 10 digits (8 examples in some experiments); tests extended to 3–30 digits (1000 samples per digit length)",
            "method_or_intervention": "Few-shot prompting (8 examples), Chain-of-Thought / Scratchpad and Algorithmic prompting evaluated",
            "performance_result": "Vanilla few-shot performance degrades for &gt;10 digits; CoT-like methods (Scratchpad, Algorithmic) show more robust length generalization and better solve rates on long-digit tasks but are limited by context window and compute; exact numeric performance curves are reported in figures (performance falls as digit length increases).",
            "mechanistic_insight": "These LMs rely on pattern/statistical inference from examples; CoT/Scratchpad helps by forcing explicit stepwise decomposition and intermediate-state tracking (e.g., carry recording), which resembles algorithmic processing and improves generalization.",
            "performance_scaling": "Scaling to GPT-4 gives better absolute performance but length generalization remains a challenge even for very large models; CoT methods mitigate but do not fully solve OOD length scaling.",
            "failure_modes": "Accuracy decreases with digit length and outside training/example lengths; constrained by context length limits and computational cost for long decompositions.",
            "comparison_baseline": "Compared to fine-tuned T5, NC-integrated LMs, CoT and tool-based methods; NC often outperforms vanilla few-shot and provides a complementary approach to CoT.",
            "key_finding": "Few-shot LLMs can solve arithmetic in-distribution but struggle to extrapolate to longer inputs; forcing step-by-step decomposition (CoT/Scratchpad) helps, but explicit compiled modules (NC/CoNN) yield deterministic correctness and better length generalization.",
            "uuid": "e275.3",
            "source_info": {
                "paper_title": "Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Chain-of-Thought / Scratchpad / Algorithmic",
            "name_full": "Chain-of-Thought (CoT) / Scratchpad / Algorithmic prompting techniques",
            "brief_description": "Prompting strategies that require models to produce intermediate reasoning steps (scratchpads) or to record algorithmic state (e.g., carry operations) to improve performance on multi-step arithmetic and length generalization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "CoT / Scratchpad / Algorithmic (prompting methods)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Multi-step arithmetic (addition with carries, algorithmic bookkeeping), general arithmetic reasoning",
            "number_range_or_complexity": "Applied to multi-digit arithmetic up to the experimental range (3–30 digits); helps particularly for &gt;10-digit extrapolation",
            "method_or_intervention": "Chain-of-Thought prompting, scratchpad-style stepwise generation, Algorithmic prompting that explicitly records carries",
            "performance_result": "Reported to substantially improve length generalization relative to vanilla few-shot and fine-tuning in experiments; capable of solving longer-digit addition tasks more robustly than direct prompting, but at the cost of extra computation and subject to context-window limits.",
            "mechanistic_insight": "Effectiveness comes from explicit decomposition: by exposing intermediate states (digitwise sums, carries), the model can emulate an algorithmic process rather than relying on single-token statistical mapping.",
            "performance_scaling": "Requires more compute and longer context windows; scaling helps but context size remains a limiting factor for arbitrarily long digit sequences.",
            "failure_modes": "High computational cost and inability to scale beyond context window; increased chance of compounding intermediate-step errors unless combined with verification/self-consistency.",
            "comparison_baseline": "Compared with vanilla few-shot, fine-tuned LMs, and Neural Comprehension; NC claims similar algorithmic robustness without needing stepwise generation and with smaller runtime overhead.",
            "key_finding": "Decomposing arithmetic into intermediate steps via CoT/Scratchpad improves extrapolation and carry-handling, but is resource-intensive and limited by input length constraints.",
            "uuid": "e275.4",
            "source_info": {
                "paper_title": "Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "PAL / Tool-based",
            "name_full": "PAL: Program-aided Language models and other tool-based methods",
            "brief_description": "Approaches where language models generate code or call external tools/interpreters to compute arithmetic exactly (e.g., run a program to calculate results), contrasted with NC's in-network compiled modules.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "PAL / Tool-based methods",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "Arithmetic reasoning and arithmetic word problems solved via external code execution (addition/subtraction, general arithmetic, programmatic computation)",
            "number_range_or_complexity": "Can handle arbitrary numeric ranges limited by interpreter precision and LM code-generation correctness",
            "method_or_intervention": "Tool-calling: generate code / call external interpreter (PAL); not end-to-end differentiable; requires models with code generation ability",
            "performance_result": "Tool-based approaches (PAL) yield strong arithmetic performance in prior work; in the paper NC is reported to match or outperform tool-based approaches on some arithmetic reasoning tasks while being more efficient and end-to-end differentiable (reported comparisons in Table 2 and text).",
            "mechanistic_insight": "Tool methods outsource exact computation to external executors — accuracy depends on code-generation correctness and ability to call interpreters; not differentiable inside LM training loop.",
            "performance_scaling": "Only applicable to models that can reliably generate and call code; NC advantage is applicability to smaller LMs that lack code-gen capability.",
            "failure_modes": "Relies on external interpreter and code-generation correctness; increased latency and engineering complexity; non-differentiable limits integration with learning-based adaptation.",
            "comparison_baseline": "Compared against Neural Comprehension and vanilla CoT; NC offers comparable symbolic processing performance but with higher inference efficiency and differentiability.",
            "key_finding": "Tool-based execution yields exact computation but requires code-gen and external execution; compiled in-network modules (NC/CoNNs) can deliver similar deterministic arithmetic behavior within the neural model with less engineering and better differentiability.",
            "uuid": "e275.5",
            "source_info": {
                "paper_title": "Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring length generalization in large language models",
            "rating": 2,
            "sanitized_title": "exploring_length_generalization_in_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tracr: Compiled transformers as a laboratory for interpretability",
            "rating": 2,
            "sanitized_title": "tracr_compiled_transformers_as_a_laboratory_for_interpretability"
        },
        {
            "paper_title": "Looped Transformers as Programmable Computers",
            "rating": 2,
            "sanitized_title": "looped_transformers_as_programmable_computers"
        },
        {
            "paper_title": "PAL: Program-aided Language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "What can transformers learn in-context? a case study of simple function classes",
            "rating": 2,
            "sanitized_title": "what_can_transformers_learn_incontext_a_case_study_of_simple_function_classes"
        },
        {
            "paper_title": "Are nlp models really able to solve simple math word problems?",
            "rating": 1,
            "sanitized_title": "are_nlp_models_really_able_to_solve_simple_math_word_problems"
        }
    ],
    "cost": 0.019888999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MASTERING SYMBOLIC OPERATIONS: AUGMENTING LANGUAGE MODELS WITH COMPILED NEURAL NETWORKS
9 Mar 2024</p>
<p>Yixuan Weng wengsyx@gmail.com 
The Laboratory of Cognition and Decision Intelligence for Complex Systems
IACAS</p>
<p>Minjun Zhu 
The Laboratory of Cognition and Decision Intelligence for Complex Systems
IACAS</p>
<p>School of Artificial Intelligence
University of Chinese Academy of Sciences</p>
<p>Fei Xia 
The Laboratory of Cognition and Decision Intelligence for Complex Systems
IACAS</p>
<p>School of Artificial Intelligence
University of Chinese Academy of Sciences</p>
<p>Bin Li 
College of Electrical and Information Engineering
Hunan University</p>
<p>Shizhu He shizhu.he@nlpr.ia.ac.cn 
The Laboratory of Cognition and Decision Intelligence for Complex Systems
IACAS</p>
<p>School of Artificial Intelligence
University of Chinese Academy of Sciences</p>
<p>Kang Liu kliu@nlpr.ia.ac.cn 
The Laboratory of Cognition and Decision Intelligence for Complex Systems
IACAS</p>
<p>School of Artificial Intelligence
University of Chinese Academy of Sciences</p>
<p>Jun Zhao jzhao@nlpr.ia.ac.cn 
The Laboratory of Cognition and Decision Intelligence for Complex Systems
IACAS</p>
<p>School of Artificial Intelligence
University of Chinese Academy of Sciences</p>
<p>MASTERING SYMBOLIC OPERATIONS: AUGMENTING LANGUAGE MODELS WITH COMPILED NEURAL NETWORKS
9 Mar 2024EC5267B9E2B2C89B8FD9BD5BD7CABF8CarXiv:2304.01665v3[cs.CL]
Language models' (LMs) proficiency in handling deterministic symbolic reasoning and rule-based tasks remains limited due to their dependency implicit learning on textual data.To endow LMs with genuine rule comprehension abilities, we propose "Neural Comprehension" -a framework that synergistically integrates compiled neural networks (CoNNs) into the standard transformer architecture.CoNNs are neural modules designed to explicitly encode rules through artificially generated attention weights.By incorporating CoNN modules, the Neural Comprehension framework enables LMs to accurately and robustly execute rule-intensive symbolic tasks.Extensive experiments demonstrate the superiority of our approach over existing techniques in terms of length generalization, efficiency, and interpretability for symbolic operations.Furthermore, it can be applied to LMs across different model scales, outperforming tool-calling methods in arithmetic reasoning tasks while maintaining superior inference efficiency.Our work highlights the potential of seamlessly unifying explicit rule learning via CoNNs and implicit pattern learning in LMs, paving the way for true symbolic comprehension capabilities.</p>
<p>INTRODUCTION</p>
<p>ŶƚĞƌdĞǆƚ&gt;ĞŶŐƚŚ;ŝŐŝƚƐͿ ^Žů|ĞZĂƚĞ;йͿ ф/ŶͲŝƐƚх фKƵƚͲŽĨͲŝƐƚх <em>37 </em>37</p>
<p>7EDVH</p>
<p>Figure 1: The length generalization of T5 (with finetune), GPT-3.5 and GPT-4 (with few-shot) on symbolic operations (Addition) tasks.To evaluate the model's proficiency, we conducted experiments on tasks ranging from 3 to 30 digits, with longer than 10 digits being out-of-distribution of training data.</p>
<p>Language models (LMs), particularly large language models (LLMs), have exhibited impressive performance on complex reasoning tasks (Brown et al., 2020;Zhang et al., 2022;Chowdhery et al., 2022;Wei et al., 2022a;Suzgun et al., 2022).Despite this, the proficiency of LMs in tackling deterministic symbolic reasoning and rule-based tasks is still limited (Welleck et al.;Razeghi et al., 2022).For example, GPT-3's arithmetic performance declines with higher digit numbers (Brown et al., 2020), and its mathematical accuracy is influenced by word frequency in training data (Razeghi et al., 2022).Moreover, length generalization (Anil et al., 2022) remains a challenge even for 100-billion-parameter models, such as GPT-4 (Bubeck et al., 2023).We hypothesize that these limitations stem from LMs' dependency on implicitly learning rules from textual data.As illustrated in Figure 1, a simple length generalization experiment using addition tasks with varying numbers of digits highlights this limitation.Performance deteriorates as test length increases, indicating that these models strongly rely on statistical patterns in the data rather than capturing fundamental logical structures.This implicit learning of statistical patterns constrains LMs' accuracy in executing symbolic operations tasks.</p>
<p>We propose a transformer-based language model framework, termed "Neural Comprehension", which synergistically integrates a pretrained LM (Li et al., 2021b) and compiled neural networks (CoNNs) (Weiss et al., 2021), combines their complementary strengths in a plug-and-play manner, to achieve high accuracy and robust performance.CoNNs are neural networks but the rules are explicitly coded through transformer-liked structures and attention.Therefore, the CoNN is human-controllable, executing rules through artificially generated attention weights, and can achieve perfect accuracy once compiled network is done.Neural Comprehension relys solely on neural networks without requiring additional tools.It employs a token-by-token generative method, analogous to GPT-3, where each token can be generated by either the pretrained LM or one of the CoNNs.The Neural Comprehension comprises a pretrained LM and multiple sets of CoNNs.The implementation of the Neural Comprehension framework facilitates the integration of rule-intensive abilities and reasoning capabilities into LMs, endowing them with genuine symbolic comprehension skills.</p>
<p>We conduct extensive experiments to evaluate the performance of our proposed Neural Comprehension method on a variety of rule-intensive tasks.Our experimental results demonstrate the effectiveness of our approach in comparison with existing state-of-the-art techniques, such as vanilla fine-tuning, few-shot learning, and Chain-of-Thought reasoning (Wei et al., 2022b).Specifically, Neural Comprehension outperforms these methods in terms of accuracy, efficiency, and interpretability, showcasing its superiority in handling rule-intensive tasks.On the other hand, compared to the Tool-Based method (Mialon et al., 2023), Neural Comprehension provides a unified end-to-end neural network framework, eliminating the need for external interpreters and having higher inference efficiency.Historically, LMs are far from mastering robust symbolic task, such as symbolic operations and arithmetic reasoning (Stolfo et al., 2023).Our research provides a compelling case for LMs in neural network frameworks mastering symbolic operations, highlighting its potential to transform the landscape of symbolic reasoning and numerical computation capabilities for LMs.</p>
<p>Contributions Our main contributions are as follows:</p>
<p>• We pioneer the implementation of flawless execution rule-intensive symbolic operations for language models that rely on neural networks.By employing a plug-and-play way, we successfully integrate CoNNs, which are interpretable and human-controllable, into the language model.Our method facilitates direct rule deduction without the need for learning from conditional probabilities, leading to a more robust and effective approach.(Section 4)</p>
<p>• We have built the AutoCoNN toolkit to make Neural Comprehension scalable, which leverages LLMs' contextual learning capabilities to automatically generate new CoNNs.</p>
<p>Our method can be easily extended to various symbolic operations tasks.(Seciton 4.3)</p>
<p>• Our experimental results on symbolic tasks and real-world arithmetic reasoning tasks demonstrate the superior performance of our method in comparison to existing techniques.Notably, our LM achieves flawless execution on symbolic reasoning tasks.(Section 5.1 5.2)</p>
<p>• It is worth noting that tool-based methods are only applicable to language models with code generation capabilities and require the cooperation of external interpreters.Our experiments demonstrate that the symbolic processing capabilities of neural understanding are on par with tool-based methods, but are applicable to models ranging from small ones like T5-Small (60M) to large ones like GLM-130B (130B) and .(Section 5.3)</p>
<p>• We also studied the potential of combining multiple CoNNs and found that adding correlated CoNNs can continuously increase performance, while adding uncorrelated CoNNs rarely leads to performance degradation.This provides a new approach for model fusion, enabling the model to easily acquire new knowledge.(Section 5.4)</p>
<p>RELATED WORKS 3 PRELIMINARIES</p>
<p>Example 1 The Parity CoNN</p>
<p>⋆</p>
<p>If we want the transformer to perform the Parity task full accurately:
Input: 1 0 1. Select(Indices,Indices,True) = 1 1 1 1 2. Aggregate 1 1 1 1 , [1 0] = [1 1] 3. Zipmap([1 1] , Lambda x : 0 if x % 2 == 0 else 1) = [1 1] Output: [1 1]</p>
<p>Compiled Neural Network (CoNN).</p>
<p>CoNN is a transformer-based neural network leveraging artificially compiled attention weights to execute rules.CoNN has multiple attention layers and Multi-Layer Perceptron (MLP) layers, and each attention layer facilitates interactions between tokens.For example, in Figure 2, the multiplication of query and key elements representing a "Select" operation in CoNN.Subsequent multiplication with value elements indicates an "Aggregate" operation.The MLP layer is responsible for the token itself and is referred to as the "Zipmap" operation (Weiss et al., 2021).By utilizing the three operations (Select, Aggregate, and Zipmap) to represent the sequenceto-sequence process, we can convert symbolic parity task into transformer weights (Lindner et al., 2023).CoNN can also stack multiple layers to address various human-defined rule problems, such as mathematical calculations and symbol operations1 .Figure 2 shows an example of Parity CoNN: The first step is to obtain a matrix of all ones to that of the sequence using the "Select" operation.Secondly, the "Aggregate" operation is used to combine the matrix obtained in the previous step with the input sequence (with the aim of calculating the total number of 0's and 1's in the sequence).The third step involves determining whether the total count is odd or even by "Zipmap".</p>
<p>METHOD</p>
<p>Language models excel in language understanding tasks, but lack robust capabilities for symbolic tasks.We propose a Neural Comprehension framework that make CoNN guided by abstract rules into language models in a plug-and-play fashion, which integrates the language model's implicit learning parameters and CoNNs' explicit learning parameters.In Neural Comprehension, we designed CoNNs in neural networks with weights and structures directly encoding the symbolic rules within the standard architecture of the LM to enhance deterministic rule comprehension and allow for deterministic execution of rule-based tasks.</p>
<p>NEURAL COMPREHENSION</p>
<p>Neural Comprehension is a MoE-style (Shazeer et al., 2017) neural network framework we designed, which is entirely composed of neural networks and maintains the generative seq2seq style.It uses predefined CoNNs as gating to determine when to output results from CoNNs.This approach is simple and plug-and-play, allowing combination with pretrained LMs.As illustrated in Figure 3, the language model encodes the context and produces the textual and reasoning process context D(x) step by step, a decoder architecture to generate the subsequent context step by step iteratively.And CoNNs handle sequence transformations involving rules, when a rule-required operation emerges, CoNN's attention is utilized to calculate specific values.For example, in Figure 3, when calculating 364425-216582, the only pretrained language model may output 148843, which is incorrect.However, the Subtraction CoNN can correct the result to 147843 in the neural comprehension framework.This dynamically encoded process improves intermediate results interpretability and final result accuracy.Neural Comprehension combines LM and CoNNs in a piecewise function to perform gradient update.LM's hidden state output is
H L = H L1 • • • H L d L ⊤ ∈ R d L , H Li ∈ (0, 1), and CoNN's output is H C = H C1 • • • H C d C ⊤ ∈ R d C , H Ci ∈ {0, 1}, î = argmax i I d L , 0 0, βI d C H L , H C , β ∈ {0, 1}(1)
where H C is a one-hot vector, and</p>
<p>GRADIENT MODIFICATION IN NEURAL COMPREHENSION</p>
<p>To better appreciate the benefits of our method in handling rule-intensive tasks and improving accuracy, it is crucial to understand the gradient perspective of In-Context Learning (ICL).Recent studies on ICL algorithms have shown that the learning process of language models within the optimization process in ICL can be viewed as a search for suitable gradients to minimize the loss function.(Garg et al., 2022;Von Oswald et al., 2023).Due to the implicit learning nature of standard ICL methods, gradients learned from data may not always be ideal for addressing rule-intensive tasks.Therefore, our proposed method introduces an explicit learning component to provide more appropriate gradient updates for such tasks, ultimately leading to enhanced overall performance.We focus on elucidating the changes in the gradient introduced by the Neural Comprehension model, the gradient of the model during the execution of ICL can be partitioned into two categories based on the origin of the gradients:
Gradient = G T Text: Language Model G R Rule: CoNNs (2)
Here, G T represents the gradients derived implicitly from the language model (LM) and corresponds to the text-based learning aspect of the model.Conversely, G R represents the gradients explicitly derived from the CoNNs, encoding rule-based knowledge.The specific computation process can be seen in Equation 1.Note that the gradients' decomposition is only approximate and may not reflect the true generating process of text.The Neural Comprehension framework integrates both gradient sources to optimize the ICL process.In linear regression problems (Akyürek et al., 2022), the loss function can be expressed as a piecewise function according to Equation 1, here P 1 (x) is the LM and P 2 (x) is CoNN, the In-context-learner can be separate into two process:
L = y − β ⊤ x 2 (3) = y − G ⊤ T x 2 x ∈ P 1 (x) y − G ⊤ R x 2 x ∈ P 2 (x)(4)
Based on the partitioned gradient as defined in Equation 2, the overall gradient of the Neural Comprehension model can be obtained by computing their individual gradients concerning the respective β: ∂L ∂β We conduct a length generalization experiment (Anil et al., 2022) to examine the distinctions between the Neural Comprehension and learning-based methods, as depicted in Figure 4. Our experimental design encompasses 1000 × 40 independent test sets, comprising problems with varying digit lengths from 1 to 40 digits.10 to 20 digits within the range are provided by us for methods based on implicit learning for training; during the testing phase, this range is called In-Dist.Furthermore, we present results for both Scratchpad (Anil et al., 2022) and Algorithmic (Zhou et al., 2022b) approaches.
Gradient = ∂L ∂G T x ∈ P 1 (x) ∂L ∂G R x ∈ P 2 (x)(5)
The results of our experiment demonstrate that the Vanilla Fine-tune (red lines) method performs optimally on the in-domain (10-20 digit) training set, while its performance deteriorates for both more simplistic and more intricate.This finding suggests that the absence of relevant samples in the training set may cause gradient descent-based language models to underperform on both simpler and more complex tasks.As further discussed in the appendix D.1, this phenomenon can be attributed to the inherent generalization limitations of statistical models and the position bias of language models.</p>
<p>Considering the Vanilla Few-shot method (green lines), we determine that its performance is less impacted by the prompt sample range compared to Vanilla Fine-tune.Large language models, which are trained on extensive text corpora, excel at solving more straightforward problems such as symbolic operations within a ten-digit range.Nevertheless, performance remains below par for test sets with more than ten digits, even when prompted with 10-20 digit samples.</p>
<p>Observing CoT-like methods (we use GPT-3.5),including Scratchpad and Algorithmic, unveils their robust length generalization capabilities.Scratchpad works by requiring large language models to record intermediate steps, while Algorithmic employs a similar approach to record the carry operations involved in the addition process.This can be primarily attributed to their proficiency in decomposing complex problems into smaller incremental steps and maintaining intermediate states.</p>
<p>However, these methods necessitate substantial computational resources, and extending the length beyond the input limit of the model becomes challenging.</p>
<p>Our study reveals that Neural Comprehension attains remarkably high accuracy in symbolic operations.This implies that Neural Comprehension, unlike conventional methods, does not rely on training data and remains unaffected by discrepancies in input lengths for in-distribution and out-of-distribution data.Consequently, it alleviates the requirement for step-by-step work tracking, and language models with CoNNs only need relatively fewer computational steps to execute sequence operations directly.Encoding rules into neural network modules endows us with greater interpretability, enabling language models to flawlessly perform purely symbolic operation tasks.The bleu line represents a language model that incorporates neural comprehension, and the red line represents the original language model.Additionally, we provide Direct, which is a direct prediction of the final result, as a reference.</p>
<p>SYMBOLIC REASONING</p>
<p>In this section, we investigate the performance of Neural Comprehension in terms of symbolic reasoning capabilities.Our hypothesis is that, although pretrained Language Models (LMs) demonstrate strong language understanding abilities, they lack the capacity to deduce and comprehend rules regarding symbolic reasoning tasks.Thus, we aim to evaluate whether the incorporation of compiled neural networks in the form of CoNNs can address this limitation and improve the LM's symbolic reasoning abilities.</p>
<p>To assess the performance of the rule comprehension component (CoNNs) in symbolic reasoning, we devise an experiment that measures the model's accuracy using intermediate processes and represents them in a "Chain of Thought"-like manner.</p>
<p>In doing so, the experiment decomposes language understanding and rule comprehension explicitly into simpler outputs, avoiding the complexities of reasoning and additional error propagation in the models.Example outputs from this approach can be found in Appendix F. We observed that neural comprehension improves the symbolic reasoning capabilities of pretrained language models in most cases (Neural Comprehension almost always outperforms Vanilla Fine-tune in Figure 5), and can fit faster.This observation suggests that the introduction of compiled neural networks has a positive impact on pretrained LMs, addressing rule comprehension limitations in symbolic reasoning tasks.</p>
<p>ARITHMETIC REASONING</p>
<p>Arithmetic reasoning serves as a suitable testbed for evaluating language models and their ability to address real-world problems.In this study, we examine the AddSub + dataset variants that involve different digit lengths, utilizing the Addition and Subtraction models from the CoNNs family.Notably, the capabilities of Neural Comprehension extend beyond these tasks, as CoNNs can also simulate calculators that support multiplication and division operations, and potentially perform
7KHQXPEHURIGLJLWV 6ROYH5DWH <em>37% 9DQLOOD&amp;R7 3$/ 1HXUDO&amp;RPSUHKHQVLRQ ,PSURYH3HUIRUPDQFH 7KHQXPEHURIGLJLWV 6ROYH5DWH </em>37% 7KH1XPEHURI'LJLWV 6ROYH5DWH */0%
Figure 6: We conducted simulations of the AddSub dataset with varying digits by modifying the "lEquations"</p>
<p>parameter.We then tested the performance of three LLMs with and without Neural Comprehension in generating CoT outputs for AddSub + .And we reported the solve rates of three LLMs and compared the solve rates of using additional tools (PAL (Gao et al., 2022)).linear algebra computations or even in-context learning algorithms that employ backpropagation (Giannou et al., 2023).</p>
<p>To evaluate the impact of Neural Comprehension on arithmetic reasoning, we compare the output of vanilla CoT language models and those incorporating Neural Comprehension, using the vanilla CoT baseline as a reference.As demonstrated in Figure 6 and Table 2, the vanilla CoT model struggles to extrapolate and solve arithmetic problems involving longer digit lengths.However, integrating Neural Comprehension significantly improves the performance of language models on such complex arithmetic tasks.Since we only incorporated the Addition and Subtraction CoNNs, we attribute the observed performance enhancement to the increased computational accuracy of the language model.For further evidence, we present additional experimental results on widelyused arithmetic reasoning datasets in Appendix E.2, which reinforce the benefits of using Neural Comprehension over the vanilla CoT model.</p>
<p>In comparison to language models employing external tools like PAL (Gao et al., 2022), our findings suggest that Neural Comprehension offers greater flexibility for LM.Firstly, by design, it minimizes the necessity for additional processing steps or external tools, leading to an efficient direct computational approach.This contrasts with Tool-based methods that often require additional programming and execution steps, increasing complexity and computational resources.Moreover, CoNN maintains end-to-end differentiability, crucial for models adapting to new data or tasks.In contrast, Tool-based methods are non-differentiable, limiting their adaptability in reinforcement learning settings or tasks with sparse delayed rewards (Chung et al., 2022;Ouyang et al., 2022).Furthermore, CoNN's modularity enhances performance across various model scales, applicable regardless of a language model's ability to call functions, unlike tools only operable in large, additionally code-trained models.Thus, the Neural Comprehension framework's efficiency, unified end-to-end neural network architecture, and extensive applicability constitute its distinct advantages over the Tool-based approach, offering a robust and scalable solution for a multitude of linguistic and computational challenges.</p>
<p>ABLATION AND ANALYSES: MODULE COMBINATION FOR NEURAL COMPREHENSION</p>
<p>Efficiently deploying multiple CoNNs is crucial for achieving exceptional Neural Comprehension performance.As depicted in Figure 7, the amalgamation of distinct CoNNs, tailored for both symbolic and arithmetic reasoning tasks within the language model framework, can lead to remarkable benefits.Similar to ToolFormer (Schick et al., 2023), we combine multiple different CoNNs into This can be ascribed to the refined design of the Neural Comprehension framework, which ensures the precise execution of assigned tasks by CoNNs without interference from irrelevant modules.Each CoNN module is adept at generating the appropriate output when needed, thereby preventing the emergence of erroneous results from unrelated components.Importantly, as seen in Appendix B.3, the parameter count for each CoNN module ranges from 1/1000 to 1/1000000 of that for GPT-3, and the experiments in Appendix D.3 show that the inference latency in the neural understanding framework only increases by 1%-3% compared to Vanilla.</p>
<p>This observation underscores the remarkable scalability of the Neural Comprehension framework, which possesses the capability to not only accommodate existing knowledge concepts but also assimilate novel ones as the number of CoNNs expands.Theoretically, the integration of tens of thousands of CoNN modules within language models holds the potential to foster a comprehensive understanding of concepts.</p>
<p>CONCLUSION</p>
<p>We have observed that language models lack an intrinsic comprehension of rule-based concepts and explored how Neural Comprehension can integrate compiled neural networks into the language model framework in a simple and plug-and-play manner.On the one hand, we demonstrated the superiority of our approach over existing learning-based methods, where our method implements comparable improvements to external tools within the neural network framework but does not require additional interpreters.This also enables language models without coding capabilities to possess symbolic manipulation abilities.On the other hand, compared to external tools, gradients can propagate without proxies, allowing better integration and full differentiability.The Neural Comprehension solves the issue of language models themselves being unable to perform robust symbolic operations and providing a foundation for future work on unifying both implicit and explicit learning in language models and facilitating seamless integration.A DISCUSSION OF LIMITATIONS AND FUTURE WORK</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>We have presented a novel framework that integrates Compiled Neural Networks (CoNNs) with existing language models to bolster their rule understanding abilities.Although our approach has shown promising performance improvements on symbolic and arithmetic reasoning tasks, there are several limitations and potential avenues for future research that warrant further exploration.</p>
<p>A significant limitation of our current framework lies in the more efficient and natural incorporation of CoNNs into language models.Currently, our method employs a sparse neural network that treats the pretrained language model and CoNNs as separate modules.A more desirable solution is to leverage a dense neural network, simultaneously utilizing the benefits of both components.Examining the large-scale applicability of CoNNs is a beneficial endeavor.Although our experiments have been conducted on a relatively small scale (up to five stacked CoNNs), the advancements and abilities language models may gain from larger-scale combinations of CoNNs remain unclear.Exploring the scalability of our method and the performance advantages of deploying more complex CoNN architectures in language models could provide valuable insights into their potential.</p>
<p>Another promising area of research is the inclusion of explicit knowledge into CoNNs.While the current implementation centers on encoding rules into CoNNs, future work could exploit techniques from knowledge graphs to compile explicit knowledge into language models.This may significantly enhance language models' interpretability and knowledge representation capabilities, potentially resulting in improved performance on an even broader range of tasks.</p>
<p>In conclusion, our work on enhancing language models' rule understanding capabilities through CoNN integration has yielded promising results, albeit with some limitations and remaining challenges.By addressing these areas and extending our approach, we believe that it can ultimately lead to the development of more powerful, interpretable, and knowledge-rich language models.</p>
<p>B COMPILED NEURAL NETWORKS</p>
<p>In this section, we will discuss the concept, implementation, and potential of Compiled Neural Networks (CoNN), a type of neural network inspired from previous works on transformers.CoNNs can perform diverse tasks such as computer arithmetic and linear algebra, demonstrating a wide range of applications in LMs and beyond.</p>
<p>B.1 INTRODUCTION</p>
<p>Transformers have garnered significant attention due to their ability to capture high-order relations and manage long-term dependencies across tokens through attention mechanisms.This enables transformers to model contextual information effectively.Pretrained language models, such as GPT-3 (Brown et al., 2020), exploit contextual learning to invoke various modules for different tasks, like performing arithmetic upon receiving arithmetic prompts.To further enhance rule comprehension in such models, CoNN-based modules are introduced as a part of Neural Comprehension.</p>
<p>Distinct from common models like BERT (Devlin et al., 2018), CoNNs leverage a transformer structure and derive their weights from specialized design rather than pretraining.Each Attention layer and Multilayer Perceptron (MLP) layer in a CoNN represents a specific sequence transformation, leading to a neural network module embodying explicit and interpretable operations.</p>
<p>RASP (Weiss et al., 2021) is a Restricted Access Sequence Processing Language that abstracts the computational model of Transformer-encoder by mapping its essential components, such as attention and feed-forward computation, into simple primitives like select, aggregate, and zipmap.This language enables RASP programs to perform various tasks like creating histograms, sorting, and even logical inference, as demonstrated by Clark et al. (2020).</p>
<p>Tracr (Lindner et al., 2023) serves as a compiler that converts human-readable RASP code into weights for a GPT-like transformer architecture with only a decoder module.The Tracr framework uses JAX to transform RASP-defined code into neural network weights.Our neural reasoning framework employs weights generated by Tracr, which are then converted into PyTorch weights to be compatible with the pretrained language model.</p>
<p>Looped Transformers as Programmable Computers (Giannou et al., 2023) introduces a novel transformer framework that simulates basic computing blocks, such as edit operations on input sequences, non-linear functions, function calls, program counters, and conditional branches.This is achieved by reverse engineering attention and hardcoding unique weights into the model, creating a looped structure.The resulting CoNN can emulate a general-purpose computer with just 13 layers of transformers, and even implement backpropagation-based context learning algorithms, showcasing the approach's vast application prospects.</p>
<p>Overall, the potential applications of CoNNs are extensive, given their capacity to perform a wide array of tasks beyond natural language processing.CoNNs offer increased interpretability and transparency through explicitly defined operations, which is vital in fields such as medical diagnosis and legal decision-making.Additionally, CoNNs can lead to more efficient and effective neural network architectures by reducing pretraining requirements and facilitating improved optimization of network parameters.</p>
<p>B.2 EXAMPLE</p>
<p>In this subsection, we briefly describe how computational processes can be represented using transformer code and demonstrate how new CoNN weights can be obtained with the aid of the Tracr compiler.</p>
<p>B.2.1 PA R I T Y CONN</p>
<p>In the introduction, we tried to introduce how to perform parity checking on a sequence containing [0 | 1] using a CoNN.Whenever we need to check the sequence, this CoNN can output the completely correct answer.</p>
<p>THE TRACR CODE OF PA R I T Y CONN</p>
<p>def parity(sop) -&gt; rasp.SOp: """Multiply the length of each token."""sop = rasp.SequenceMap(lambda x,y: x * y,sop,length).named('map_length')</p>
<p>"""Add each bit.""" out = rasp.numerical(rasp.Aggregate(rasp.Select(rasp.indices,rasp.indices,rasp.Comparison.TRUE).named('Select'),rasp.numerical(rasp.Map(lambda x: x, sop).named('map_length')),default=0).named('Aggregate'))</p>
<p>"""Calculate whether the remainder of dividing it by 2 is odd or even."""out = rasp.Map(lambda x: 0 if x % 2 == 0 else 1,out).named('Zipmap')"""Get the indices from back to front."""opp_idx = (length -rasp.indices).named("opp_idx")</p>
<p>"""opp_idx -1, so that the first digit of indices = 0.""" opp_idx = (opp_idx -1).named("opp_idx-1")</p>
<p>"""Use opp_idx to query indices, get the Select."""reverse_selector = rasp.Select(rasp.indices,opp_idx,rasp.Comparison.EQ).named("reverse_selector")</p>
<p>"""Aggregate the reverse_selector and sop""" return rasp.Aggregate(reverse_selector, sop).named("reverse")add_carry functions, which are used to calculate the addition and carry of pairs in the CoNN respectively.We divide the entire operation into two models.For the add_carry model, we refer to the approach of ALBERT.After the output of the add_in_the_same_position model, we cyclically use the add_carry model L times, where L is the length of the text, to ensure that all digits can carry.It is important to note that this particular Addition CoNN is only capable of performing addition operations on natural numbers.</p>
<p>B.2.4 SU B T R A C T I O N CONN</p>
<p>The subtraction CoNN is similar to the addition CoNN.First, each digit is subtracted from its corresponding digit, and then it is determined whether to carry over.For ease of experimentation, this subtraction CoNN only supports subtraction of natural numbers where the minuend is greater than the subtrahend.</p>
<p>B.3 CONN MODEL PARAMETERS</p>
<p>The parameter sizes of all CoNN models used in this work are listed in Table 3.It is noteworthy that even for GPT-3, which has parameters that are orders of magnitude larger, it remains challenging to THE TRACR CODE OF AD D I T I O N CONN def split(sop, token, index): """Match the position of target token""" target_position = rasp.Aggregate(rasp.Select(sop, rasp.Map(lambda x: token, sop), rasp.</p>
<p>Comparison.EQ), rasp.indices)</p>
<p>"""If need to match the front position."""if index == 0: out = rasp.Aggregate(rasp.Select(rasp.indices,rasp.indices-(length -target_position), rasp.Comparison.EQ), sop) # Move the sop on the left side of the token to the far right.return rasp.SequenceMap(lambda x, i: x if i == 2 else "<em>", out, rasp.categorical(rasp.SequenceMap(lambda x, i: 2 if x &gt;= i else 0, rasp.indices,length -target_position))) # Use "</em>" to fill the empty position on the left.</p>
<p>"""If need to match the finally number."""else: return rasp.SequenceMap(lambda x, i: x if i else "<em>", sop, rasp.SequenceMap(lambda x, i: 1 if x &gt; i else 0, rasp.indices,target_position)).named(f"shift") # Use "</em>" to fill the empty position on the left.def atoi(sop):</p>
<p>"""Converts all text to number, and uses 0 for strings of types other than numbers, It may be mixed with 'str' or 'int'.""" return rasp.SequenceMap(lambda x, i: int(x) if x.isdigit() else 0, sop, rasp.indices).</p>
<p>named( "atoi") def shift(sop):</p>
<p>"""Get the target indices."""idx = (rasp.indices-1).named("idx-1")</p>
<p>"""Use opp_idx to query indices, get the Select."""selector = rasp.Select(idx, rasp.indices,rasp.Comparison.EQ).named("shift_selector")</p>
<p>"""Aggregates the sops and selectors (converted from indexes)."""shift = rasp.Aggregate(selector, sop).named("shift")Table 4: For each CoNN model, we selected ten groups of models that were judged to be correct by AutoCoNN.We manually evaluated whether these models were indeed correct.The 'Example=x' means that x Examples were provided in the validation stage.</p>
<p>For experts, they may need to spend a lot of time writing code suitable for CoNN, while non-expert users find it hard to obtain or modify CoNN.These issues limit the efficient combination of CoNN and LM, so we utilized the few-shot ability of language models to make the AutoCoNN toolkit (Weng et al., 2023a) 'a','b','c','d','e','f','g'] EXAMPLE = [[['a','b','c'], ['c','c','c']], [['b','d'], ['d','d']]] auto = AutoCoNN() model,tokenizer = auto(instruct=INSTRUCT,vocab=VOCAB,example=EXAMPLE)</p>
<p>Table 5 shows the efficiency comparison between experts and AutoCoNN.This demonstrates that the AutoCoNN toolkit can generate various CoNNs faster.But we also found that for more difficult ones like Addition and Subtraction, it fails to successfully generate, which becomes one of the limitations of AutoCoNN.On the other hand, we tried providing only "Instruct" or "Example" for AutoCoNN to generate3 , and often "Instruct" can generate CoNN with higher accuracy, while "Example" cannot.</p>
<p>This shows that giving explicit operational instructions performs better than directly observing data in AutoCoNN.</p>
<p>D EXPERIMENTAL SETTINGS</p>
<p>In this study, we primarily explore the capacity of language models to address symbolic reasoning tasks, concentrating on three areas: symbolic operations, symbolic reasoning, and arithmetic reasoning.</p>
<p>Symbolic Operations Building upon the approaches developed by Anil et al. (2022) and Qian et al. (2022), we examine the following tasks: Parity, Reverse, Addition and Subtraction.These tasks do not require complex text understanding, but only require faithfully implementing symbolic operations and outputting the corresponding results.</p>
<p>Symbolic Reasoning</p>
<p>We employ the experimental framework of Wei et al. (2022b) for the two tasks, Last Letter Concatenation and Coin Flip.These tasks require a combination of language understanding and rule comprehension abilities.</p>
<p>Arithmetic Reasoning To evaluate the method's generalization ability from symbolic operations to arithmetic reasoning in addition and subtraction tasks, we use five established arithmetic reasoning datasets: AddSub (Hosseini et al., 2014), SingleEq (Koncel-Kedziorski et al., 2015), MultiArith (Roy &amp; Roth, 2016), GSM8K (Cobbe et al., 2021), and SVAMP (Arkil et al., 2021).Additionally, we introduce the AddSub + dataset, containing tasks of varying complexity based on the number of digits involved in arithmetic operations, ranging from 1-digit addition to 20-digit addition/subtraction tasks.The results in Figure 12 show that increasing the scale of the In-Dist training data leads to only marginal improvements in OOD performance.This finding is discouraging, suggesting that gradientbased language models face challenges in capturing the true underlying meaning of symbols and their transformation rules based on the data distribution alone.</p>
<p>E.2 REAL-WORLD ARITHMETIC REASONING TASKS</p>
<p>As model parameters, training calculations, and dataset sizes have increased, language models have gained new capabilities (Srivastava et al., 2022;Wei et al., 2022a), such as Machine Translation (Zhao et al., 2023;Li et al., 2024), complex QA (Zhu et al., 2022;Daull et al., 2023), Multimodal QA (Wang et al., 2023a;Li et al., 2023;Weng &amp; Li, 2023), coding (Li et al., 2022;Nijkamp et al., 2022), few-shot learning (Brown et al., 2020;Perez et al., 2021), medical diagnosis (Li et al., 2021a;Xia et al., 2022), and chain of thought (Wei et al., 2022b;Weng et al., 2023b).</p>
<p>In Table 6, we compared Vanilla CoT with the Neural Comprehension framework for arithmetic reasoning tasks.We integrated the Addition and Subtraction CoNNs with LLMs and observed improved performance across several tasks.This suggests that the proposed Neural Comprehension framework can compensate for the difficulties faced by large-scale language models in computational tasks.Nevertheless, the performance improvement is not as significant due to the choice of specific  Comprehension framework improves the gap between the data distribution learned by the language model during training through gradient descent and the real rules, it can also be combined with some existing logical improvements to language models, including self-consistency (Wang et al., 2023b), least-to-most (Zhou et al., 2022a), self-improve (Huang et al., 2022), and self-verification (Weng et al., 2023b).It can also be combined with some zero-shot methods (Kojima et al., 2022;Zhang et al., 2023).To evaluate the efficacy of Neural Comprehension, we conducted further experiments comparing the inference latency of both the Vanilla and Neural Comprehension frameworks on an equal number of sequences and equal sequence lengths using GPU and CPU configurations.We employed a batch size of 1 and assessed the inference latency of Neural Comprehension in conjunction with various T5 model sizes across two symbolic inference tasks to ascertain efficiency.The full results are detailed in Table 8.</p>
<p>Our findings reveal that implementing Neural Comprehension increases computational requirements, primarily attributed to the supplementary parameter volume and computational demands introduced by CoNNs.However, as the scale of pretrained language models expands, the proportion of δ Time within</p>
<p>F.1.2 FEW-SHOT IN-CONTEXT LEARNING</p>
<p>For the few-shot context learning on GPT and GLM-130B models, we employ an approach inspired by the recent work on GPT-3 (Brown et al., 2020).In this methodology, the models are provided a context consisting of a few examples of the input-output pairs in natural language format.Following this, the models are expected to generalize and perform well on the task without any explicit finetuning.We carefully design the context to include diverse examples that represent the range of input types and required model reasoning.Importantly, we limit the context length to be within the maximum token limits of the models.For instance, GPT-3 has a token limit of 2048.Due to limited access to the GPT family of models, we utilize the official API for these experiments5 ..For the GLM-130B, we employ the FasterTransformer framework to set up local inference with INT4 on eight NVIDIA GeForce RTX 3090 GPUs with 24GB of memory each.</p>
<p>To compare with CoT and PAL which experimented on GPT-3 series models, we simulated Neural Comprehension (NC) within the constraints of the API access.We treated the API's output as if it were part of the Neural Comprehension structure.This involved a simulated gating mechanism, where the output from the API was corrected using CoNNs form left to right, and then the adjusted response (Truncate the text after the altered text.) was changed into the API' input for continued generation.This simulation was to ensure that the benefits of NC could be compared fairly with the existing results from PAL.</p>
<p>F.2 TASKS AND DATASET</p>
<p>In this paper, all data sets related to length generalization consist of independent data sets with the same number of digits but different lengths, and each digit in the test set is unique.Therefore, there may be slight fluctuations between data sets of different lengths, but the overall trend is generally clear.</p>
<p>To further illustrate the differences between data sets of different lengths, the following examples are provided: Each input is a randomly generated word string of specified length, where the letters are selected uniformly at random from a set of 26 letters using a Bernoulli distribution (without necessarily having any actual meaning), and all letters are converted to lowercase.</p>
<p>Figure 2 :
2
Figure 2: Demonstration of the principles of Parity CoNN.</p>
<p>Figure 3 :
3
Figure 3: The architecture of the proposed Neural Comprehension framework.</p>
<p>Figure 5 :
5
Figure 5: In the iterative process of gradient descent during training.The bleu line represents a language model that incorporates neural comprehension, and the red line represents the original language model.Additionally, we provide Direct, which is a direct prediction of the final result, as a reference.</p>
<p>Figure 8 :
8
Figure 8: Input the [1,0,0,0,1] (target output = 0) for Parity CoNN.</p>
<p>Figure 9 :
9
Figure 9: Input the [1,0,1,0,1] (target output = 1) for Parity CoNN.</p>
<p>Figure 10 :
10
Figure 10: Input the ['hello',',','world'] for Reverse CoNN.</p>
<p>Figure 11 :
11
Figure 11: Input the ['r','e','v','e','r','s','e'] for Reverse CoNN.</p>
<p>return shift def add_in_the_same_position(sop): x = atoi(split(sop,'+',0)) + atoi(split(sop,'+',1)) return x def carry(sop): weight = shift(rasp.Map(lambda n:1 if n&gt;9 else 0,sop)) weight = rasp.Aggregate(rasp.Select(rasp.indices,rasp.indices,lambdakey,query:key == query),weight,default=0) x = rasp.Map(lambda n:n-10 if n&gt;9 else n,sop) return x + weight THE TRACR CODE OF SU B T R A C T I O N CONN def split(sop, token, index):... def atoi(sop):... def shift(sop):... def sub_in_the_same_position(sop): x = atoi(split(sop,'-',0)) -atoi(split(sop,'-',1)) return x def carry(sop): weight = shift(rasp.Map(lambda n:1 if n&lt;0 else 0,sop)) weight = rasp.Aggregate(rasp.Select(rasp.indices,rasp.indices,lambdakey,query:key == query),weight,default=0) x = rasp.Map(lambda n:n+10 if n&lt;0 else n,sop) return x -weight</p>
<p>Figure 12 :
12
Figure 12: Length Generalization Performance of Language Models with Different Dataset Sizes.</p>
<p>We sample instances of lengths 1-40 from a uniform Bernoulli distribution.We first uniformly sample the number of ones, and then randomly shuffle the positions of each one within a fixed-length bitstring.For the experiments in Figure5.1, we train T5 on lengths 10-20, with 99000 training samples per bit.For all methods, we test each bit using 1000 samples.Synthetic Reverse Dataset: We selected a dataset of instances with lengths ranging from 1 to 40.For the experiments in Figure5.1, the training set consists of 99000 samples each for lengths 10-20.</p>
<p>In this section, we will use gray font to represent the task input, yellow font to represent the neural network output during training, and blue background to represent the neural network output during generated.(LLM's few-shot prompt)---Q: 011110001010101101011 A: 0 Table 12: The example of Parity Q-211065324 A: 393967676 ----(LLM's few-shot prompt)----Q: 20021012-20021004 A: 8</p>
<p>d L and d C here refer to the vocabulary size of the Model's decode output.2TheNeural Comprehension combines the LLM's hidden state output, H L , and CoNN's output, H C , using identity matrices I d L (for d L ) and I d C (for d C ) to concatenate them for model fusion.Specifically, the hidden state representation matrix is obtained by extending the original hidden state representation matrix of the LM with the hidden state matrix on the CoNNs' vocabulary through a block matrix operation, resulting in a larger matrix.</p>
<p>Table 2 :
2
Experiments on the addition and subtraction subset for GSM8K-Hard dataset showing performance comparisons across different models and methods: Only Addition (left), Only Subtraction (center), and Mixed addition and subtraction (right), using Vanilla CoT, PAL, and NC (Neural Comprehension) methods.
Additionllama-2-7b llama-2-70b GLM-130B AvgSubtraction llama-2-7b llama-2-70b GLM-130B AvgMixedllama-2-7b llama-2-70b GLM-130B AvgCoT6.343.86.318.8CoT6.327.91.812.0CoT11.116.70.09.3PAL18.737.56.320.8PAL7.231.53.614.1PAL11.127.85.614.8NC (Ours)12.543.87.221.2NC (Ours)9.932.44.515.6NC (Ours)27.833.35.622.2</p>
<p>Pre-trained Model with a CoNN A Pre-trained Model with some CoNNs</p>
<p>In Neural Comprehension framework, the performance of multiple different module combination is demonstrated.The left side shows the effect of combining a pretrained language model with a CoNN, while the right side shows the impact of combining a language model with multiple CoNNs.For different tasks, we categorize CoNNs as Correlated (green) and Uncorrelated (red), indicating whether the CoNN is related to the current task or not.
/DVW/HWWHU&amp;RQFDWHQDWLRQ6ROYH5DWHA 7VPDOO07EDVH0+ 1 7ODUJH06ROYH5DWH7VPDOO07EDVH0+ 7ODUJH09DQLOOD)LQHWXQH&amp;RUUHODWHG/DVW:RUG&amp;RUUHODWHG&amp;RS\/HWWHU8QFRUUHODWHG3DULW\8QFRUUHODWHG$GG8QFRUUHODWHG6XE'LVWVRI$GG6XE6ROYH5DWH<em>37%</em>37%<em>/0%</em>37%<em>37%</em>/0%9DQLOOD&amp;R7&amp;RUUHODWHG$GG&amp;RUUHODWHG6XE8QFRUUHODWHG3DULW\8QFRUUHODWHG/DVW:RUG8QFRUUHODWHG&amp;RS\/HWWHUFigure 7:
(Increase from left to right) one framework, enabling the language model to have multiple capabilities.We conduct experiments on Last Letter Concatenation tass and AddSub + dataset, which shows the plug-and-play gating mechanism can still well control these CoNNs to output what should be output.It is observed that integrating pertinent CoNNs bolsters the performance of the initial language model, whereas the inclusion of unrelated language models rarely causes detrimental effects, regardless of whether single or multiple CoNNs are combined.</p>
<p>All CoNN models mentioned in this paper have been saved in Pytorch format in the Supplementary Materials, with dropout set to 0 to ensure deterministic outputs that conform to human-specified rules.The code for the AutoCoNN toolkit and Neural Comprehension framework in this paper can be found in the code portion of the Supplementary Materials.Details of all experiments setting referenced in this paper are included in Appendix F.1.Detailed descriptions of all tasks, datasets, and baselines mentioned in this paper are provided in Appendix F.2. Details of all few-shot prompts referenced in this paper are included in Appendix G. Fei Xia, Bin Li, Yixuan Weng, Shizhu He, Kang Liu, Bin Sun, Shutao Li, and Jun Zhao.Medconqa: Medical conversational question answering system based on knowledge graphs.In Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp.148-158, 2022.Kaiyu Yang and Jia Deng.Learning symbolic rules for reasoning in quasi-natural language.arXiv preprint arXiv:2111.12038,2021.Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang.Gpt can solve mathematical problems without a calculator, 2023.Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.Opt: Open pre-trained transformer language models.arXiv preprint arXiv:2205.01068,2022.Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola.Automatic chain of thought prompting in large language models.In The Eleventh International Conference on Learning Representations, 2023.URL https://openreview.net/forum?id=5NTt8GFjUHkr.Yang Zhao, Jiajun Zhang, and Chengqing Zong.Transformer: A general framework from machine translation to others.Machine Intelligence Research, 20(4):514-538, 2023.ISSN 2731-538X.doi: 10.1007/s11633-022-1393-5. URL https://www.mi-research.net/en/article/doi/10.1007/s11633-022-1393-5.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi.Least-to-most prompting enables complex reasoning in large language models.arXivpreprintarXiv:2205.10625,2022a.Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi.Teaching algorithmic reasoning via in-context learning.arXivpreprint arXiv:2211.09066,2022b.Minjun Zhu, Yixuan Weng, Shizhu He, Kang Liu, and Jun Zhao.Reasonchainqa: Text-based complex question answering with explainable evidence chains.arXiv preprint arXiv:2210.08763,2022.</p>
<p>ModelLayers Heads Vocabulary Size Window Size Hidden Size MLP Hidden Size # Parameters Compared to GPT-3
Pariity4144013219592.2M≈ 1/100,000Reverse41284029716404.3M≈ 1/50,000Last Letter3128161033262.6K≈ 1/3,000,000Copy11281669268.8K≈ 1/20,000,000Add_in_the_same_position711340535642251.8M≈1/3000Add_Carry311224013052117K≈1/1,500,000Sub_in_the_same_position711340535642251.8M≈1/3000Sub_Carry311224013052117K≈1/1,500,000</p>
<p>Table 3 :
3
We reported on a CoNN with a single function, including its actual parameter size and comparison with the parameters of GPT-3.solvesymbolicproblems.However, with the use of compiled neural networks, only a small number of parameters are needed to achieve Neural Comprehension.B.4 ENVIRONMENTAL AND HUMAN-CENTRIC BENEFITS OF COMPILED NEURAL NETWORKSCompiled Neural Networks (CoNNs) address concerns related to the environmental impact of training large models and the need for human-centric computing.CoNN models can reduce energy consumption and carbon emissions by minimizing extensive pretraining and decreasing parameter size, as seen in Table3.This reduction in computational power and energy requirements makes both the training and inference processes more environmentally friendly.Additionally, Neural Comprehension offers a more interpretable and transparent alternative to conventional deep learning models.CoNN's explicit operation definitions and specialized architecture enable users to comprehend the reasoning behind model decisions, fostering trust and facilitating human-AI collaboration.Increased interpretability also allows for scrutiny of model behavior, promoting the development of fair, accountable, and transparent systems aligned with ethical considerations and human values.'
C EXPERIMENT FOR AUTOCONNC.1 METHODCoNN ModelExample=1Example=2Example=5Parity Model5/1010/1010/10Reverse Model10/1010/1010/10Last Letter Model9/1010/1010/10Copy Model10/1010/1010/10</p>
<p>Working Time Success by AutoCoNN Can AutoCoNN solve AutoCoNN (w.Instruct) AutoCoNN (w.Example)
CoNN Model Expert's Parity Model 1 hours8/20✓✓7/203/20Reverse Model0.5 hour15/20✓✓16/2011/20Last Letter Model 0.5 hour13/20✓✓12/2010/20Copy Model0.2 hour17/20✓✓17/2015/20Addition Model48 hours0/20✗0/200/20Subtraction Model 48 hours0/20✗0/200/20</p>
<p>Table 5 :
5
Comparison between AutoCoNN and Expert Built CoNN.The column 'Expert's Working Time' refers to the time required for a trained engineer to write the CoNN code; 'Success by AutoCoNN' refers to the accuracy of 20 results generated by using GPT-3.5 for diverse decoding; 'Can AutoCoNN solve' refers to whether AutoCoNN can identify suitable CoNN code from the 20 results through validation.It is worth noting that in this experiment, we use sampling decoding with temperature=0.7 to generate 20 different CoNNs codes, which we convert to Pytorch versions of CoNNs models.We report the accuracy of the CoNNs codes through manual (expert) evaluation.It is difficult for non-expert users to assess the accuracy of the generated code, we automatically utilize the Example information to verify the accuracy of the CoNN model -checking whether the output result of the input sequence is exactly consistent with the Example.The results shown in Table4demonstrate that generally 2 Examples are sufficient to select an accurate CoNN model, which means it is very easy for users to use and demonstrate.However, considering the varying difficulty of different tasks, we still suggest non-expert users provide more Examples to ensure the accuracy of the generated CoNN.</p>
<p>CoNN models to ensure clarity in our experiments.Designing CoNN models to support more general arithmetic tasks could potentially yield more substantial improvements.In addition, since the Neural
MethodGSM8KSingleEqAddSubMultiArithSVAMPAveragePrevious SOTA (Fintune)35 a /57 b32.5 c94.9 d60.5 e57.4 f-GPT-3 Standard19.786.890.944.069.962.26GPT-3 (175B)CoT13.8462.0257.2245.8538.4243.47code-davinci-001CoT + Neural Comprehension 13.95 (+0.11)62.83 (+0.81)60.25 (+3.03)45.85 (+0.0)38.62 (+0.2)44.30 (+0.83)GPT-3.5 (175B)CoT60.2091.0182.7896.1375.8781.20code-davinci-002CoT + Neural Comprehension 60.42 (+0.22)91.01 (+0.0)82.78 (+0.0)96.13 (+0.0)76.09 (+0.22)81.29 (+0.09)</p>
<p>Table 6
6: Problem solve rate (%) on arithmetic reasoning datasets. The previous SoTA baselines areobtained from: (a) GPT-3 175B finetuned (Cobbe et al., 2021); (b) GPT-3 175B finetuned plus anadditional 175B verifier(Cobbe et al., 2021); (c) Hu et al. (2019); (d) Roy &amp; Roth (2016); (e) Roy &amp;Roth (2016); (f) Amini et al. (2019); (f) Pi et al. (2022)</p>
<p>Table 7 :
7
The test set problem-solving rate (%) of the T5 model on the GSM8K dataset.
To further evaluate the effectiveness of the NeuralComprehension framework, Table 7 presents theresults of fine-tuning T5 models with Additionand Subtraction CoNN on the GSM8K train-ing dataset. The comparison of three different-sizedmodels reveals that the framework can model deter-ministic rules defined by humans, thus avoiding theuncertainty associated with gradient descent learningfrom data distribution.E.3 THE EFFICIENCY OF NEURAL COMPREHENSIONVanillaNeural Comprehensionδ TimeModelParamsTaskGPUCPUGPUCPUGPUCPUT5-small60MCoin Flip5.280s5.720s5.431s5.872s 0.151s (2.86%) 0.152s (2.66%)T5-base220MCoin Flip7.865s13.767s8.010s13.939s 0.145s (1.84%) 0.172s (1.25%)T5-large770MCoin Flip14.055s32.953s 14.194s33.120s 0.139s (0.99%) 0.167s (0.51%)T5-small60MLast Letter Concatenation 16.233s28.309s 16.744s28.720s 0.511s (3.15%) 0.411s (1.45%)T5-base220M Last Letter Concatenation 28.912s55.660s 29.426s56.087s 0.514s (1.78%) 0.427s (0.77%)T5-large770M Last Letter Concatenation 49.584s 103.739s 50.066s104.134s 0.482s (0.97%) 0.395s (0.38%)</p>
<p>Table 8 :
8
In Neural Comprehension framework, the inference latency comparison of the T5 model.</p>
<p>Table 11 :
11
The test set for lengths 1-40 contains at least 1000 test samples.Arithmetical Reasoning Dataset Description.• AddSub: https://www.cs.washington.edu/nlp/arithmetic• MultiArith: http://cogcomp.cs.illinois.edu/page/resource_view/98 • SVAMP: https://github.com/arkilpatel/SVAMPG SOME EXAMPLES OF NEURAL COMPREHENSION
DatasetNumber of samples Average words Answer FormatLienceGSM8K131946.9NumberMIT LicenseSingleEq50827.4NumberMIT LicenseAddSub39531.5NumberUnspecifiedMultiArith60031.8NumberUnspecifiedSVAMP100031.8NumberMIT License</p>
<p>Table 15 :
15
The example of Subtraction</p>
<p>Appendix B provides a more detailed description of CoNN.
It is worth noting that in this paper, for ease of implementation, the output vocabulary size of CoNNs' decode d C is generally less than 100 due to limitations in computing resources (detailed information is shown in Appendix Table1).
In this experiment, the few-shot samples also contained only one of them
The specific configurations of the GPT series of models can be found at https://platform.openai. com/docs/model-index-for-researchers/model-index-for-researchers
OpenAI's API:https://openai.com/api/
ACKNOWLEDGEMENTSThis work was supported by the National Key R&amp;D Program of China (No.2022ZD0118501) and the National Natural Science Foundation of China (No.U1936207, No.62376270, No.62171183).Youth Innovation Promotion Association CAS, and OPPO Research Fund.ACKNOWLEDGEMENTS FOR COLLEAGUES We appreciate the interest shown in this work by our colleagues: • Haining Xie, Huanxuan Liao, Jiachun Li, Liang Gui, Pengfan Du, Pengfei Cao, Shaoru Guo, Wangtao Sun, Wenting Li, Xiusheng Huang, Yao Xu, Yifan Wei, Zhao Yang, Zhiqi Wang, Zhongtao Jiang, Zhuoran Jin, Ziyang Huang, who offered enthusiasm and backing.ACKNOWLEDGEMENTS FOR FRIENDSWe gratefully acknowledge the unwavering support and friendship from our community that served as the foundation for this research journey.Their companionship, through weekly board game and Werewolf gatherings, provided a reprieve from the rigors of research, allowing us to return refreshed and reinvigorated each week.Beyond the joyful times, they offered empathy during setbacks, perspective during challenges, and reassurance that progress awaits persistent effort.Their laughter, solidarity, and care kept us grounded, hopeful, and honest.The bonds formed enriched our lives immeasurably.We could not have navigated this meaningful journey without their understanding and encouragement.The impact of their compassion extends far beyond the technical contributions detailed herein.We express our sincerest appreciation, respect, and admiration for making this adventure a truly memorable experience:• Bingmei Sun, Boyuan Jiang, HaoChen Cao, Zixuan Cao, Donghui Li, Dongfang Suze, Ertan Zhuang, Jiajia Li, Mingwei Zhang, Lin Zhang, Mingwen Niu, Min Xiao, Qiaomu Tan, Tianyu Mu, Yuxin Liu, Xiaoyan Yu, Yuke Shi, Yixuan Li, Yang Zhou.Table9: Models.Description of the models evaluated in this effort: provenance for this information is provided in models.* indicates that we believe the associated OpenAI models are this size, but this has not been explicitly confirmed to our knowledge.In this section, we provide a detailed description of the experimental setup from a model and dataset perspective, ensuring repeatability of our experiments.F.1 MODELOur experiments primarily involve the T5, GPT, and GLM-130B families of models.Neural Comprehension framework supports seamless integration with language models having decoder structures, regardless of the scale of the language model.We fine-tune the T5 models, while the larger models with over 10 billion parameters are used for few-shot In-context learning.Table9presents a comprehensive list of all models used in our experiments 4 .F.1.1 FINE-TUNINGFor the T5 models, we employ the standard fine-tuning approach using the pretrained models as a starting point.We follow the pre-processing steps in the T5 original paper, which involves set the input text max length to 150 and using the tokenizer to process the data.We use a batch size of 64 for all models and the Adafactor optimizer(Shazeer &amp; Stern, 2018)with a learning rate of 1 × 10 −4 .The models are trained for a maximum of 20 epochs.We use a cosine learning rate schedule with a warm-up phase comprising 5% of the total number of training steps.We employ a dropout rate of 0.1 during training to mitigate overfitting.Our experiments utilize the PyTorch framework(Paszke et al., 2019)for training and inference.In Table10, we list the hyperparameters used to train the T5 model.We carefully selected these parameters to ensure that the within-distribution validation accuracy roughly converged.We report all peak validation set results, and in every experiment we ran, we found that withindistribution validation accuracy monotonically increased during training iterations (until it approached 100%), and we never observed overfitting.This may be due to the regular nature of the tasks we considered in the paper.We followed the Anil et al. (2022)'s setup and did not use OOD performance in model selection, as this would constitute "peaking at the test conditions".Regarding the number of training iterations, we also tried training the T5 model with more iterations in the addition task, but this did not lead to substantial differences in OOD performance (i.e., it was still equally poor).Synthetic Addition and subtraction Dataset: Addition and subtraction are fundamental arithmetic operations that are commonly taught in primary school.To generate the dataset, we takes as input the number of digits n and returns a list of 100000 examples.The function first calculates the remainder k when (n − 1) is divided by 2, and then divides (n − 1) by 2 if n is greater than 2, else it sets n to itself.This ensures that the length of the first number is either equal to or one digit longer than the length of the second number.The function then generates 100000 examples using randomly generated numbers.Specifically, it generates two numbers a and b where a is a random integer between 10 (n+k−1) and 10 (n+k) − 1, and b is a random integer between 10 (n−1) and 10 n − 1.It then appends each example to the list data in the form of a dictionary with the input as the string "a+b" and the label as the sum a+b.PARITY DATASETFor the experiments in Figure Figure1, we provided 99000 training data examples for addition with numbers ranging from 3 to 10 digits in length for the T5 model.For the GPT-3.5 and GPT-4 models, we provided 8 few-shot samples within 10 digits.We evaluated the performance of all three models on numbers ranging from 3 to 30 digits in length, with 1000 test samples per digit.On the other hand, for the experiments in Figure Figure5.1, we provided 99000 training data examples for addition with numbers ranging from 10 to 20 digits in length for the T5 model.For the GPT-3.5 and GPT-4 models, we provided 8 few-shot samples within the range of 10 to 20 digits.We evaluated the performance of all three models on numbers ranging from 3 to 30 digits in length, with 1000 test samples per digit.For subtraction, we use a similar approach.# generate the statement and label based on whether the coin was flipped or not if flip: sentence += f" {name.capitalize()}flips the coin."label.append(1)else:sentence += f" {name.capitalize()}does not flip the coin."label.append(0)sentence += ' Is the coin still heads up?' dataset.append({'question':sentence,'answer':{0:'yes',1:'no'}[sum(label)%2]}) <NAME> token.In our work, flipping a coin corresponds to 1 and not flipping a coin corresponds to 0. To make the inputs as close to English as possible without using too many symbols, we used the sentence models "<NAME> flips the coin."and "<NAME> does not flip the coin." to represent whether the coin was flipped or not.This task is similar to the parity task, but requires further semantic understanding.We constructed a training set of 1500 samples, with 500 samples for each of 2-4 coin flips.For the test set, we selected 100 non-overlapping samples for each of 2-4 coin flips, and evaluated the model every 5 steps.Last Letter Concatenation: We followedWei et al. (2022b)'s setup and randomly concatenated first and last names from the top 1000 names in the census data to create the <NAME> token.This task requires the model to connect the last letter of each word in a concatenated name.This task requires Neural Comprehension of rules in two aspects.First, it requires the model to correctly identify the last letter of each word.Second, it requires the model to concatenate all the last letters of the words together.We concatenated 2-5 first or last names, and constructed a training set of 1500 samples, with 500 samples for each name length of 2-4.For the test set, we selected 100 non-overlapping samples for each name length of 2-4, and evaluated the model every 5 steps.F.2.3 ARITHMETICAL REASONING DATASETIn Table11, we summarize the information of all arithmetic reasoning datasets used in this work.We provide the links to access these datasets:• GSM8K: https://github.com/openai/grade-school-math
What learning algorithm is in-context learning?. Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou, arXiv:2211.156612022arXiv preprintinvestigations with linear models</p>
<p>Mathqa: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, 2019the association for computational linguisticsnorth american chapter</p>
<p>Exploring length generalization in large language models. Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Are nlp models really able to solve simple math word problems?. Patel Arkil, Bhattamishra Satwik, Goyal Navin, 2021</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Sparks of artificial general intelligence: Early experiments with gpt-4. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, arXiv:2305.17126Large language models as tool makers. Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou, 2023. 2023arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Scaling instruction-finetuned language models</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/ijcai.2020/537Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. Christian Bessiere, the Twenty-Ninth International Joint Conference on Artificial Intelligence202020</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Complex qa and language models hybrid architectures, survey. Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, Elisabeth Murisasco, arXiv:2302.090512023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>Logical tasks for measuring extrapolation and rule comprehension. Ippei Fujisawa, Ryota Kanai, arXiv:2211.077272022arXiv preprint</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2211.10435Pal: Program-aided language models. 2022arXiv preprint</p>
<p>What can transformers learn in-context? a case study of simple function classes. Shivam Garg, Dimitris Tsipras, Percy S Liang, Gregory Valiant, Advances in Neural Information Processing Systems. 202235</p>
<p>Injecting numerical reasoning skills into language models. Mor Geva, Ankit Gupta, Jonathan Berant, arXiv:2004.044872020arXiv preprint</p>
<p>. Angeliki Giannou, Shashank Rajput, Jy Yong Sohn, Kangwook Lee, Jason D Lee, Dimitris Papailiopoulos, 2023Looped transformers as programmable computers</p>
<p>Learning to solve arithmetic word problems with verb categorization. empirical methods in natural language processing. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, 2014</p>
<p>A multi-type multi-span network for reading comprehension that requires discrete reasoning. empirical methods in natural language processing. Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, 2019</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, arXiv:2210.116102022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, Siena Dumas, Ang , 2015Transactions of the Association for Computational Linguistics</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.134612019BartarXiv preprint</p>
<p>Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, arXiv:2206.14858Cem Anil, Imanol Schlag. 2022arXiv preprint</p>
<p>More but correct: Generating diversified and entity-revised medical response. Bin Li, Encheng Chen, Hongru Liu, Yixuan Weng, Bin Sun, Shutao Li, Yongping Bai, Meiling Hu, arXiv:2108.012662021aarXiv preprint</p>
<p>Learning to locate visual answer in video corpus using question. Bin Li, Yixuan Weng, Bin Sun, Shutao Li, ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE2023</p>
<p>Towards better chinese-centric neural machine translation for low-resource languages. Bin Li, Yixuan Weng, Fei Xia, Hanjun Deng, Computer Speech &amp; Language. 841015662024</p>
<p>Pretrained language models for text generation: A survey. Junyi Li, Tianyi Tang, Wayne Xin Zhao, Ji-Rong Wen, 2021b</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>David Lindner, János Kramár, Matthew Rahtz, Thomas Mcgrath, Vladimir Mikulik, arXiv:2301.05062Tracr: Compiled transformers as a laboratory for interpretability. 2023arXiv preprint</p>
<p>Augmented language models: a survey. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Celikyilmaz, arXiv:2302.078422023arXiv preprint</p>
<p>Codegen: An open large language model for code with multi-turn program synthesis. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, arXiv:2203.134742022arXiv preprint</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.001142021arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>True few-shot learning with language models. Ethan Perez, Douwe Kiela, Kyunghyun Cho, Advances in neural information processing systems. 202134</p>
<p>. Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, Weizhu Chen, 2022Reasoning like program executors</p>
<p>Limitations of language models in arithmetic and symbolic induction. Jing Qian, Hong Wang, Zekun Li, Shiyang Li, Xifeng Yan, arXiv:2208.050512022arXiv preprint</p>
<p>Impact of pretraining term frequencies on few-shot reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, arXiv:2202.072062022arXiv preprint</p>
<p>Solving general arithmetic word problems. arXiv: Computation and Language. Subhro Roy, Dan Roth, 2016</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Adafactor: Adaptive learning rates with sublinear memory cost. Noam Shazeer, Mitchell Stern, 2018</p>
<p>Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V Le, Geoffrey E Hinton, Jeff Dean, CoRR, abs/1701.065382017</p>
<p>Hikaru Shindo, Devendra Singh Dhami, Kristian Kersting, arXiv:2110.09383Neuro-symbolic forward reasoning. 2021arXiv preprint</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karaka{ ¸s}, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bart{ł}omiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard De Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U Balis, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B Tenenbaum, Joshua S Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, D Kaustubh, Kevin Dhole, Kevin Gimpel, Kory Omondi, Kristen Mathewson, Ksenia Chiafullo, Kumar Shkaruta, Kyle Shridhar, Kyle Mcdonell, Laria Richardson, Leo Reynolds, Li Gao, Liam Zhang, Lianhui Dugan, Lidia Qin, Louis-Philippe Contreras-Ochando, Luca Morency, Lucas Moschella, Lucy Lam, Ludwig Noble, Luheng Schmidt, Luis He, Luke Oliveros Colón, Metz ; Maheen, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Maru, Jose Ramírez, Marie Quintana, Mario Tolkiehn, Martha Giulianelli, Martin Lewis, Matthew L Potthast, Matthias Leavitt, Mátyás Hagen, Medina Schubert, Melody Orduna Baitemirova, Melvin Arnaud, Michael A Mcelrath, Michael Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Micha{ł} Strube, Michele Sw{ ˛e}drowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mo Suzgun, Mohit Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T Varma, Nanyun Peng, Nathan Chi, Nayeon Lee, Neta Gur-, Ari Krakover, ; Niveditha, S Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi ; Rylan, Sahib Yang, Saif M Singh, Sajant Mohammad, Sam Anand, Sam Dillavou, Sam Shleifer, Samuel Wiseman, Gruetter, R Samuel, Samuel S Bowman, Sanghyun Schoenholz, Sanjeev Han, Sarah A Kwatra, Sarik Rous, Sayan Ghazarian, Sean Ghosh, Sebastian Casey, Sebastian Bischoff, Sebastian Gehrmann, Sepideh Schuster, Shadi Sadeghi, Sharon Hamdan, Shashank Zhou, Sherry Srivastava, Shikhar Shi, Shima Singh, Asaadi, Shane Shixiang, Shubh Gu, Shubham Pachchigar, Siamak Toshniwal ; Debnath, Simon Shakeri, Simone Thormeyer, Siva Melzi, Reddy, Priscilla Sneha, Soo-Hwan Makini, Spencer Lee, Sriharsha Torene, Stanislas Hatwar, Stefan Dehaene, Stefano Divic, Stella Ermon, Stephanie Biderman, Stephen Lin, Steven T Prasad, Piantadosi, M Stuart, Tao Shieber, Tao Li, Tariq Yu, Tatsu Ali, Hashimoto ; Xinran, Xinyi Zhao, Xudong Wu, Yadollah Shen, Yair Yaghoobzadeh, Yangqiu Lakretz, Yasaman Song, Yejin Bahri, Yichi Choi, Yiding Yang, Yifu Hao, Yonatan Chen, Yu Belinkov, Yufang Hou, Yuntao Hou, Zachary Bai, Zhuoye Seid, Zijian Zhao, Zijie J Wang, Zirui Wang, Ziyi Wang, Wu, Lütfi Kerem { ¸S}enel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve. Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Omer Levy, Owain Evans, Pablo Antonio Moreno Casares; Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton ChangTe-Lin Wu2022Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster,Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu. Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong,. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</p>
<p>A causal framework to quantify the robustness of mathematical reasoning with language models. Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schoelkopf, Mrinmaya Sachan, 10.18653/v1/2023.acl-long.32Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Paradigm shift in natural language processing. Tian-Xiang Sun, Xiang-Yang Liu, Xi-Peng Qiu, Xuan-Jing Huang, 10.1007/s11633-022-1331-6Machine Intelligence Research. 2731- 538X1932022</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Transformers learn in-context by gradient descent. Johannes Von, Oswald , Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov, International Conference on Machine Learning. PMLR2023</p>
<p>Large-scale multi-modal pre-trained models: A comprehensive survey. Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, Wen Gao, 10.1007/s11633-022-1410-8Machine Intelligence Research. 2731-538X2042023a</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b</p>
<p>Thinking like transformers. Gail Weiss, Yoav Goldberg, Eran Yahav, International Conference on Machine Learning. PMLR2021</p>
<p>Neural text generation with unlikelihood training. Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston, International Conference on Learning Representations. </p>
<p>Visual answer localization with cross-modal mutual knowledge transfer. Yixuan Weng, Bin Li, ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE2023</p>
<p>Lmtuner: An user-friendly and highly-integrable training framework for fine-tuning large language models. Yixuan Weng, Zhiqi Wang, Huanxuan Liao, Shizhu He, Shengping Liu, Kang Liu, Jun Zhao, arXiv:2308.102522023aarXiv preprint</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023b</p>            </div>
        </div>

    </div>
</body>
</html>
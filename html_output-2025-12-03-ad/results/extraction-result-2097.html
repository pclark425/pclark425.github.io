<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2097 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2097</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2097</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-276775839</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.01424v2.pdf" target="_blank">From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems</a></p>
                <p><strong>Paper Abstract:</strong> Research is a fundamental process driving the advancement of human civilization, yet it demands substantial time and effort from researchers. In recent years, the rapid development of artificial intelligence (AI) technologies has inspired researchers to explore how AI can accelerate and enhance research. To monitor relevant advancements, this paper presents a systematic review of the progress in this domain. Specifically, we organize the relevant studies into three main categories: hypothesis formulation, hypothesis validation, and manuscript publication. Hypothesis formulation involves knowledge synthesis and hypothesis generation. Hypothesis validation includes the verification of scientific claims, theorem proving, and experiment validation. Manuscript publication encompasses manuscript writing and the peer review process. Furthermore, we identify and discuss the current challenges faced in these areas, as well as potential future directions for research. Finally, we also offer a comprehensive overview of existing benchmarks and tools across various domains that support the integration of AI into the research process. We hope this paper serves as an introduction for beginners and fosters future research. Resources have been made publicly available at https://github.com/zkzhou126/AI-for-Research.</p>
                <p><strong>Cost:</strong> 0.031</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2097.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2097.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model (LLM) frequently cited as a general-purpose generator for research outputs (ideas, reviews, plans); widely used in the surveyed literature as an exemplar of modern LLM capabilities and limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / cross-domain (used across hypothesis generation, writing, peer review, experiment planning)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual outputs (scientific hypotheses, reviews, experimental plans, code, paper drafts)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (survey cites studies showing zero-shot hypothesis proposal and some novel idea generation) </td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompt- and in-context-learning based pattern extrapolation and recombination from large pretraining corpora</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>human expert review; downstream experimental/simulation verification where applicable; dataset-based benchmarks when used (but survey notes lack of standard benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Qualitative: reported as useful/helpful in many studies; pilot studies (referenced) report modest utility (e.g., 'slightly helpful' for peer-review assistance). No standardized numeric metrics reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not systematically reported in the survey; validation typically requires human inspection and external empirical tests; survey highlights hallucinations and incorrect citations as common failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Survey reports that validation reliability declines as outputs become more novel (more hallucinations and unverifiable claims for higher-novelty outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Survey documents an asymmetry: generation (idea-production) is rapid and broad, while validation (expert checking, experiments) is slower, costlier, and less mature — generation often outpaces reliable validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not generally provided by the model outputs in the cited works; survey notes limited or inconsistent uncertainty reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Survey cites hallucination and incorrect citations; model confidence calibration for novel outputs is not well-established and appears to degrade for out-of-distribution content.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degrades substantially; models prone to hallucinate or produce implausible claims for truly out-of-distribution (transformational) ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Plausibility/coherence, novelty and feasibility human scores, alignment with retrieved evidence (when retrieval-augmentation used); these proxies are commonly used instead of direct validity.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for most outputs; frequency and depth increase with output novelty and stakes.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Varies by application (mostly empirical/semi-formal across surveyed uses).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Human-in-the-loop post-editing; retrieval-augmentation and citation tracing; multi-agent self-checks; iterative feedback loops.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey repeatedly notes hallucinations, incorrect citations, and the absence of standardized benchmarks; many studies report that LLMs can propose plausible but unverified claims.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some downstream uses (e.g., structured retrieval-augmented pipelines or theorem-proving pipelines) show that generated outputs can be verified when integrated with external tools, suggesting the gap can be narrowed in specific settings.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not quantified in the survey; qualitative statement: validation (especially wet-lab experiments) is far more resource- and time-intensive than generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2097.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2097.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Openreviewer: A specialized large language model for generating critical scientific paper reviews</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM fine-tuned/specialized for drafting structured scientific peer reviews and meta-reviews; discussed in the survey as an example of models applied to automate aspects of peer review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openreviewer: A specialized large language model for generating critical scientific paper reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenReviewer (Openreviewer)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (specialized / fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>manuscript peer review / scholarly publishing</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>paper reviews, reviewer comments, meta-review drafts, scores/structured evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution outputs (drafting reviews that resemble human reviewers) rather than transformational novel scientific outputs</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>fine-tuning / instruction tuning of an LLM on review-style data and guided structured prompting</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>human evaluation (comparison to human reviewers), conformity to conference/journal guidelines; automated checklist-based alignment in some studies</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey cites finetuned models can produce structured reviews and comparative outputs; specific numeric metrics not reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Mixed: pilot studies cited (e.g., GPT-4 pilot) show modest assistance value; survey emphasizes that LLM reviews often provide vague feedback and can be biased.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Models perform acceptably for surface-level criticisms and clarity issues but struggle to detect deep novelty or methodological flaws; validation (final decisions) remains human-dominant.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation produces draft reviews quickly, but validation (accept/reject decisions, deep critique) requires human expertise; generation does not replace human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not typically provided in outputs; some systems structure outputs to highlight confidence but formal calibration not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; survey highlights risks of bias and vagueness rather than well-calibrated confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor for highly novel manuscripts or interdisciplinary claims where training data lacks similar examples.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Agreement with human reviewer scores, readability/structure measures, adherence to checklist items.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for all model outputs in reviewing; always used for final decisions per surveyed recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Semi-formal (structured evaluation criteria but heavily reliant on expert judgment).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Fine-tuning on review data, structured output formats, meta-review synthesis tools (MetaWriter/GLIMPSE), and human-in-the-loop workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey notes LLM-generated reviews provide vague suggestions and biases; human chair synthesis still recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Several systems (CycleResearcher, OpenReviewer) can generate structured comments and reconcile reviewer conflicts, indicating partial automation is feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2097.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2097.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-Prover</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek-prover: Advancing theorem proving in LLMs through large-scale synthetic data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that advances automated theorem proving by synthesizing large-scale proof data for fine-tuning LLMs and integrating with proof assistants; surveyed as representative of modern LLM approaches to formal proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepSeek-Prover</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model fine-tuned with synthetic theorem/proof data (neural-symbolic integration with proof assistants)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mathematics / formal theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>formal proofs, proof steps, proof scripts for proof assistants</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>varies from in-distribution (reconstructing known proofs) to moderately novel (generating new proof steps within known domains); survey flags generalization concerns for truly novel theorems</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>large-scale synthetic data creation + targeted fine-tuning; sometimes two-stage informal-to-formal generation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>formal verification by proof assistants (checking that generated proof scripts typecheck/verify), benchmark comparisons (FIMO, TRIGO, other proof corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey reports improvements from targeted data synthesis and fine-tuning (qualitative); no uniform numeric success rates provided in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>When proof assistant accepts a generated proof, correctness is effectively guaranteed; survey notes questions about generalizability and robustness across different assistants.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Validation (formal checking) is reliable for in-distribution proofs but declines for out-of-distribution or highly novel theorems where generated proofs may be incorrect or non-verifiable.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation can produce candidate proofs faster, but converting informal ideas to formal, machine-verifiable proofs remains a bottleneck — generation often outpaces reliable formal validation for novel results.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not commonly implemented; systems sometimes use search scores or proof-level value functions but explicit uncertainty estimates are rare.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not well characterized in surveyed literature; calibration likely degrades on novel conjectures per survey discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Limited; COPRA and related critiques in the survey cast doubt on generalizability to novel theorem families.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Proof acceptance by assistant, proof length, number of subgoals solved; benchmark success rates when reported in original works (not compiled quantitatively in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Often required during formalization, library management, and to interpret generated informal sketches; frequency increases with novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Highly formal (mathematics/proofs).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Two-stage generation (informal concept → formalization), theorem libraries & dynamic libraries, subgoal decomposition, human-in-the-loop assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey cites work (Wang et al., COPRA critique) showing search-based methods explore many irrelevant conjectures and that generated proofs' generalizability is limited, indicating generation > reliable validation for novel theorems.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Where formal verification succeeds, proof assistants provide deterministic validation — demonstrating that in formal domains the gap can be closed when translation to formal proofs is achieved.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not numerically reported; qualitative: formal checking is computationally cheap compared to large-scale search/generation but human formalization and library engineering are costly.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2097.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2097.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous chemical / lab systems</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous chemical research / autonomous laboratory systems (examples cited: 'Autonomous chemical research with large language models', 'An autonomous laboratory for the accelerated synthesis of novel materials')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Integrated systems combining LLMs/agents with laboratory automation to propose and execute experimental workflows, used to discover or synthesize new molecules/materials; the survey covers successes and limitations of such pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autonomous chemistry / laboratory automation systems (LLM + robotics/automation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid: large language models + automated lab/robotics + planning/decision agents</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry / materials science / experimental discovery</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>experimental protocols, synthesis plans, candidate molecules/materials, measured experimental results</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>can propose highly novel molecules/materials (out-of-distribution relative to training data) in some studies, but novelty raises feasibility and safety concerns</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM planning + search/optimization over chemical space + integration with automation for execution</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>wet-lab experimental execution and analytical measurement (e.g., spectrometry); reproducibility checks and human oversight</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey references demonstrations (including high-profile works) where novel compounds/materials were proposed; concrete numeric success rates not synthesized in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Empirical validation sometimes confirms generated candidates, but the survey emphasizes that generated experiments often lack methodological rigor, practical feasibility, and can require substantial human correction.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Higher novelty correlates with higher risk of infeasible/unverifiable experiments and increased validation/resource cost; validation success declines for more transformational proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Large asymmetry: generation is fast and cheap, experimental validation is slow, expensive, and sometimes unsafe; thus generation often dramatically outpaces validation capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Often limited; some systems incorporate experimental outcome feedback but explicit probabilistic uncertainty is rarely reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not well characterized; survey notes issues with feasibility/hallucinated experimental steps.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Mixed: some truly novel discoveries reported, but generally performance and reproducibility suffer for out-of-distribution candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Plausibility, synthetic accessibility scores, simulation-based checks, prior literature overlap; these proxies are frequently used in lieu of immediate wet-lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>High: human oversight for safety, feasibility, and interpretation is regularly recommended; frequency increases strongly with novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical (experimental sciences), low formalization relative to theorem proving.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Human-AI dialogue, iterative experiment-feedback loops, real-time adjustments, incorporation of domain-specific simulators and analytical tools.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey explicitly states experiment validation is time-consuming and generated experiments often lack rigor and practical feasibility, supporting a generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Survey cites autonomous lab successes where generated candidates were synthesized (e.g., cited Nature/Science works), showing the gap can be bridged in specific engineered systems.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not numerically specified; qualitatively: validation (wet-lab) is orders-of-magnitude more costly in time and resources than generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2097.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2097.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scientific claim verification systems</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific claim verification systems (examples cited: FactKG, SFAVEL, ClaimVer, MULTIVERS, MAGIC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of systems combining retrieval, knowledge graphs, and LLM reasoning to verify scientific claims by retrieving evidence, aligning claim-evidence, and producing verdicts with rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scientific claim verification systems (FactKG, SFAVEL, ClaimVer, MULTIVERS, MAGIC - representative examples)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid: retrieval-augmented LLMs, knowledge-graph reasoning systems, and weak supervision pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific claim fact-checking (cross-domain; examples in biomedicine, general science, news about science)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>claim veracity labels (supported/contradicted/insufficient), sentence- or document-level rationales, ranked evidence</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Primarily in-distribution verification (checking claims anchored to existing literature); poor performance on truly novel claims lacking prior evidence</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not generation of novel facts but retrieval and synthesis: evidence retrieval + LLM synthesis or graph reasoning to produce verdicts and rationales</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation against labeled fact-checking datasets (SciFact, Scifact-open, FEVER), human expert opinion, and evidence-attribution metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey reports improvements via RAG, knowledge graphs, and multi-argument generation (e.g., MAGIC), but emphasizes domain confinement and lack of generalizability; numeric metrics not collated here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Varies by dataset/domain; survey states many tools lack domain-general performance and are sensitive to retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Validation accuracy drops for claims that are novel or lack directly retrievable evidence; models often fail silently (hallucinate supporting evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Systems can generate plausible rationales/evidence but may misattribute or hallucinate — verification (ground-truth matching) lags behind fluent generation of explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Some systems surface relevance scores or top-k evidence confidences; explicit end-to-end uncertainty calibration is rarely reported across surveyed works.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Survey indicates calibration issues, susceptibility to being misled by flawed evidence and domain shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor; the survey emphasizes weak generalizability across domains and claim types.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Sentence-level rationale alignment, top-k retrieved fact precision, human agreement, and dataset accuracy/precision/recall.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended especially for high-impact or novel claims; frequent when retrieval is ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Semi-formal to empirical (textual scientific claims).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Structured KG-based evidence attribution, multi-argument generation with self-refinement (MAGIC), retrieval configuration tuning, and human expert-in-the-loop verification.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey notes poor generalizability, susceptibility to flawed evidence, and hallucination in LLM-based claim verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Knowledge-graph structuring (FactKG) and careful retrieval strategies can produce stronger evidence attributions that narrow the gap in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2097.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2097.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOOSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOOSE (and MOOSE-Chem) — LLM-based hypothesis/idea generation framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of LLM-driven systems for hypothesis and idea generation (including domain-specific variants like MOOSE-Chem) that focus on transforming literature-based inspiration into candidate hypotheses using interactive frameworks and feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MOOSE / MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model / multi-agent interactive frameworks (LLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>hypothesis generation (general; chemistry variant for molecular ideas)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>hypotheses, research ideas, candidate experimental directions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Aims for moderately novel outputs (recombining literature-derived concepts); novelty/feasibility trade-offs are highlighted in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-driven idea synthesis using knowledge graphs, prompts, and iterative feedback loops; domain-specific conditioning for chemistry in MOOSE-Chem.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison with existing literature, feedback/iterative evaluation (human or automated), occasional experimental outcome comparison in domain-specific variants.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey reports that such frameworks can produce comprehensive hypotheses; no standardized numeric metrics provided and evaluation often relies on subjective human scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Measured via manual novelty/feasibility scoring in most studies; objective validation is limited and variable across works.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Higher novelty tends to reduce feasibility and increases need for manual vetting; survey emphasizes the difficulty of quantifying novelty and feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation is efficient at producing many candidate hypotheses; validation (especially objective measures) is slower and often manual, creating a gap.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not generally provided; some systems use internal heuristics or novelty/feasibility proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; evaluation tends to be subjective.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Limited; systems struggle with truly out-of-literature innovations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Novelty and feasibility human scores, overlap with literature, plausibility heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Regularly required; increases for higher-novelty hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Varies by instantiation (empirical in chemistry; semi-formal in other fields).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Iterative human-AI feedback, automated peer-review style comments, comparison with literature and experimental outcomes when available.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey notes absence of unified benchmarks and objective metrics for hypothesis generation; many works use manual scoring, supporting a generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some iterative feedback frameworks (HypoGeniC, Fun-Search) show improved quality via evaluator loops, indicating gap can be reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2097.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2097.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist (Lu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end agent/LLM-based research pipeline concept (and implementations) that aims to autonomously propose hypotheses, design and run experiments, and report findings; surveyed as an aspirational archetype and a source of experimental prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI Scientist: Towards fully automated open-ended scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist (autonomous research agent)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM-based research agents integrated with simulation/lab tooling</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>open-ended scientific discovery (cross-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>hypotheses, experimental plans, synthesized results, manuscript drafts</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>Targets highly novel, open-ended outputs; actual novelty achieved varies and is constrained by validation bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autonomous multi-agent planning, large language model reasoning, search and optimization over hypothesis/experiment spaces</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Simulation, controlled experiments, and human expert review; survey indicates full end-to-end validation is often incomplete in prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey cites prototype systems that can produce end-to-end outputs; no uniform success metrics and many systems remain experimental.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Limited and variable; experimental confirmations are costly and not always provided, so validation performance is often low relative to generation volume.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>As novelty increases, experimental validation becomes more expensive and failure-prone; survey highlights this resource-driven decline.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Strong generation capabilities across multiple pipeline stages but validation (especially experimental confirmation and reproducibility) lags behind significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Generally limited; some prototypes use internal scoring or simulation uncertainties but systematic UQ not standard.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not well described in the survey; likely poor for transformational outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Limited due to training/data and validation constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Simulated experiment success, proxy scores (plausibility, synthetic accessibility), and eventual human confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>High; survey recommends human oversight at multiple pipeline stages, especially for novel findings.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Varies; typically empirical domains where formal verification is not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Incorporating feedback loops, multi-agent verification, human-in-the-loop checkpoints, and tighter integration with simulators/benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey explicitly states the field is in its infancy and that many approaches remain experimental with validation bottlenecks, supporting a generation > validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some systems successfully ran experiments and closed loops in narrowly scoped domains demonstrating that the gap can be closed with careful engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not quantified; qualitatively validation (experimentation) is much costlier than generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2097.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2097.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated experiment systems (MLR-Copilot, SANDBOX, CRISPR-GPT, MechAgents)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated experiment design/execution systems (examples: MLR-Copilot, SANDBOX, CRISPR-GPT, MechAgents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of agentic systems that use LLMs and tool integrations to design, configure, and in some cases execute experiments (wet-lab or computational); the survey reports both promising demonstrations and concerns about feasibility and rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mlr-copilot: Autonomous machine learning research based on large language models agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automated experiment systems (MLR-Copilot, SANDBOX, CRISPR-GPT, MechAgents — representative examples)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM agents + domain-specific toolchains / simulators / lab automation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>experiment validation across ML, biology, materials, physics (domain-specific variants exist)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>experiment protocols, ML experiment configurations, genome-editing protocols (CRISPR), simulation runs and analyses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>From incremental (automating routine experiments) to moderately/highly novel when proposing new protocols; novelty increases feasibility risk.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Task-directed LLM prompting and planning; integration with tool APIs, simulators, or lab hardware for execution (where available)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Execution in simulation or physical lab; comparison to expected/ground-truth outcomes; human expert oversight</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Survey indicates LLMs can plan and specify experiments but frequently propose steps lacking methodological rigor or feasibility; numeric metrics not aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Varies widely; simulated validations can be fast but wet-lab confirmations are costly and less frequently performed, limiting reported validation success.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Novel experimental designs are more likely to be infeasible or unsafe and have lower validation success; validation cost scales with novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation is computationally cheap; validation (running experiments) is expensive/time-consuming; significant gap noted in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Some systems incorporate analytical evaluations or simulation metrics as internal proxies, but explicit calibrated uncertainty estimates are rare.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not well characterized in surveyed works; survey warns about overconfidence in generated experiment plans.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Weak for truly novel experiments; systems rely on known patterns and often fail to account for practical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Simulation success rates, surrogate model predictions, plausibility/consistency checks, reviewer or expert scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>High, especially for wet-lab execution and novel protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>Empirical/experimental</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Human-AI dialogues, closed-loop experimental feedback, use of simulators and surrogate models to triage candidates before wet-lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Survey explicitly cites that automatically generated experiments often lack rigor and practical feasibility, supporting a generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Survey also notes some successful agent-generated experimental insights and iterative refinement that improved design quality, indicating partial mitigation is possible.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not numerically reported; qualitatively validation (execution) is much more expensive than generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Openreviewer: A specialized large language model for generating critical scientific paper reviews. <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report. <em>(Rating: 2)</em></li>
                <li>Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models. <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards fully automated open-ended scientific discovery. <em>(Rating: 2)</em></li>
                <li>Mlr-copilot: Autonomous machine learning research based on large language models agents. <em>(Rating: 2)</em></li>
                <li>Factkg: Fact verification via reasoning on knowledge graphs. <em>(Rating: 1)</em></li>
                <li>MAGIC: multiargument generation with self-refinement for domain generalization in automatic fact-checking. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2097",
    "paper_id": "paper-276775839",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A state-of-the-art large language model (LLM) frequently cited as a general-purpose generator for research outputs (ideas, reviews, plans); widely used in the surveyed literature as an exemplar of modern LLM capabilities and limitations.",
            "citation_title": "GPT-4 technical report.",
            "mention_or_use": "mention",
            "system_name": "GPT-4",
            "system_type": "large language model",
            "scientific_domain": "general / cross-domain (used across hypothesis generation, writing, peer review, experiment planning)",
            "output_type": "textual outputs (scientific hypotheses, reviews, experimental plans, code, paper drafts)",
            "novelty_level": "moderately novel (survey cites studies showing zero-shot hypothesis proposal and some novel idea generation) ",
            "generation_method": "prompt- and in-context-learning based pattern extrapolation and recombination from large pretraining corpora",
            "validation_method": "human expert review; downstream experimental/simulation verification where applicable; dataset-based benchmarks when used (but survey notes lack of standard benchmarks)",
            "generation_performance": "Qualitative: reported as useful/helpful in many studies; pilot studies (referenced) report modest utility (e.g., 'slightly helpful' for peer-review assistance). No standardized numeric metrics reported in the survey.",
            "validation_performance": "Not systematically reported in the survey; validation typically requires human inspection and external empirical tests; survey highlights hallucinations and incorrect citations as common failure modes.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Survey reports that validation reliability declines as outputs become more novel (more hallucinations and unverifiable claims for higher-novelty outputs).",
            "generation_validation_comparison": "Survey documents an asymmetry: generation (idea-production) is rapid and broad, while validation (expert checking, experiments) is slower, costlier, and less mature — generation often outpaces reliable validation.",
            "uncertainty_quantification": "Not generally provided by the model outputs in the cited works; survey notes limited or inconsistent uncertainty reporting.",
            "calibration_quality": "Survey cites hallucination and incorrect citations; model confidence calibration for novel outputs is not well-established and appears to degrade for out-of-distribution content.",
            "out_of_distribution_performance": "Degrades substantially; models prone to hallucinate or produce implausible claims for truly out-of-distribution (transformational) ideas.",
            "validation_proxy_metrics": "Plausibility/coherence, novelty and feasibility human scores, alignment with retrieved evidence (when retrieval-augmentation used); these proxies are commonly used instead of direct validity.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for most outputs; frequency and depth increase with output novelty and stakes.",
            "formal_verification_used": null,
            "domain_formalization_level": "Varies by application (mostly empirical/semi-formal across surveyed uses).",
            "gap_mitigation_strategies": "Human-in-the-loop post-editing; retrieval-augmentation and citation tracing; multi-agent self-checks; iterative feedback loops.",
            "evidence_supporting_gap": "Survey repeatedly notes hallucinations, incorrect citations, and the absence of standardized benchmarks; many studies report that LLMs can propose plausible but unverified claims.",
            "evidence_contradicting_gap": "Some downstream uses (e.g., structured retrieval-augmented pipelines or theorem-proving pipelines) show that generated outputs can be verified when integrated with external tools, suggesting the gap can be narrowed in specific settings.",
            "computational_cost_ratio": "Not quantified in the survey; qualitative statement: validation (especially wet-lab experiments) is far more resource- and time-intensive than generation.",
            "uuid": "e2097.0"
        },
        {
            "name_short": "OpenReviewer",
            "name_full": "Openreviewer: A specialized large language model for generating critical scientific paper reviews",
            "brief_description": "An LLM fine-tuned/specialized for drafting structured scientific peer reviews and meta-reviews; discussed in the survey as an example of models applied to automate aspects of peer review.",
            "citation_title": "Openreviewer: A specialized large language model for generating critical scientific paper reviews.",
            "mention_or_use": "mention",
            "system_name": "OpenReviewer (Openreviewer)",
            "system_type": "large language model (specialized / fine-tuned)",
            "scientific_domain": "manuscript peer review / scholarly publishing",
            "output_type": "paper reviews, reviewer comments, meta-review drafts, scores/structured evaluations",
            "novelty_level": "in-distribution outputs (drafting reviews that resemble human reviewers) rather than transformational novel scientific outputs",
            "generation_method": "fine-tuning / instruction tuning of an LLM on review-style data and guided structured prompting",
            "validation_method": "human evaluation (comparison to human reviewers), conformity to conference/journal guidelines; automated checklist-based alignment in some studies",
            "generation_performance": "Survey cites finetuned models can produce structured reviews and comparative outputs; specific numeric metrics not reported in the survey.",
            "validation_performance": "Mixed: pilot studies cited (e.g., GPT-4 pilot) show modest assistance value; survey emphasizes that LLM reviews often provide vague feedback and can be biased.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Models perform acceptably for surface-level criticisms and clarity issues but struggle to detect deep novelty or methodological flaws; validation (final decisions) remains human-dominant.",
            "generation_validation_comparison": "Generation produces draft reviews quickly, but validation (accept/reject decisions, deep critique) requires human expertise; generation does not replace human judgment.",
            "uncertainty_quantification": "Not typically provided in outputs; some systems structure outputs to highlight confidence but formal calibration not reported.",
            "calibration_quality": "Not reported; survey highlights risks of bias and vagueness rather than well-calibrated confidence.",
            "out_of_distribution_performance": "Poor for highly novel manuscripts or interdisciplinary claims where training data lacks similar examples.",
            "validation_proxy_metrics": "Agreement with human reviewer scores, readability/structure measures, adherence to checklist items.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for all model outputs in reviewing; always used for final decisions per surveyed recommendations.",
            "formal_verification_used": false,
            "domain_formalization_level": "Semi-formal (structured evaluation criteria but heavily reliant on expert judgment).",
            "gap_mitigation_strategies": "Fine-tuning on review data, structured output formats, meta-review synthesis tools (MetaWriter/GLIMPSE), and human-in-the-loop workflows.",
            "evidence_supporting_gap": "Survey notes LLM-generated reviews provide vague suggestions and biases; human chair synthesis still recommended.",
            "evidence_contradicting_gap": "Several systems (CycleResearcher, OpenReviewer) can generate structured comments and reconcile reviewer conflicts, indicating partial automation is feasible.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2097.1"
        },
        {
            "name_short": "DeepSeek-Prover",
            "name_full": "Deepseek-prover: Advancing theorem proving in LLMs through large-scale synthetic data",
            "brief_description": "A system that advances automated theorem proving by synthesizing large-scale proof data for fine-tuning LLMs and integrating with proof assistants; surveyed as representative of modern LLM approaches to formal proofs.",
            "citation_title": "Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data.",
            "mention_or_use": "mention",
            "system_name": "DeepSeek-Prover",
            "system_type": "large language model fine-tuned with synthetic theorem/proof data (neural-symbolic integration with proof assistants)",
            "scientific_domain": "mathematics / formal theorem proving",
            "output_type": "formal proofs, proof steps, proof scripts for proof assistants",
            "novelty_level": "varies from in-distribution (reconstructing known proofs) to moderately novel (generating new proof steps within known domains); survey flags generalization concerns for truly novel theorems",
            "generation_method": "large-scale synthetic data creation + targeted fine-tuning; sometimes two-stage informal-to-formal generation",
            "validation_method": "formal verification by proof assistants (checking that generated proof scripts typecheck/verify), benchmark comparisons (FIMO, TRIGO, other proof corpora)",
            "generation_performance": "Survey reports improvements from targeted data synthesis and fine-tuning (qualitative); no uniform numeric success rates provided in the survey text.",
            "validation_performance": "When proof assistant accepts a generated proof, correctness is effectively guaranteed; survey notes questions about generalizability and robustness across different assistants.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Validation (formal checking) is reliable for in-distribution proofs but declines for out-of-distribution or highly novel theorems where generated proofs may be incorrect or non-verifiable.",
            "generation_validation_comparison": "Generation can produce candidate proofs faster, but converting informal ideas to formal, machine-verifiable proofs remains a bottleneck — generation often outpaces reliable formal validation for novel results.",
            "uncertainty_quantification": "Not commonly implemented; systems sometimes use search scores or proof-level value functions but explicit uncertainty estimates are rare.",
            "calibration_quality": "Not well characterized in surveyed literature; calibration likely degrades on novel conjectures per survey discussion.",
            "out_of_distribution_performance": "Limited; COPRA and related critiques in the survey cast doubt on generalizability to novel theorem families.",
            "validation_proxy_metrics": "Proof acceptance by assistant, proof length, number of subgoals solved; benchmark success rates when reported in original works (not compiled quantitatively in this survey).",
            "human_validation_required": true,
            "human_validation_frequency": "Often required during formalization, library management, and to interpret generated informal sketches; frequency increases with novelty.",
            "formal_verification_used": true,
            "domain_formalization_level": "Highly formal (mathematics/proofs).",
            "gap_mitigation_strategies": "Two-stage generation (informal concept → formalization), theorem libraries & dynamic libraries, subgoal decomposition, human-in-the-loop assistance.",
            "evidence_supporting_gap": "Survey cites work (Wang et al., COPRA critique) showing search-based methods explore many irrelevant conjectures and that generated proofs' generalizability is limited, indicating generation &gt; reliable validation for novel theorems.",
            "evidence_contradicting_gap": "Where formal verification succeeds, proof assistants provide deterministic validation — demonstrating that in formal domains the gap can be closed when translation to formal proofs is achieved.",
            "computational_cost_ratio": "Not numerically reported; qualitative: formal checking is computationally cheap compared to large-scale search/generation but human formalization and library engineering are costly.",
            "uuid": "e2097.2"
        },
        {
            "name_short": "Autonomous chemical / lab systems",
            "name_full": "Autonomous chemical research / autonomous laboratory systems (examples cited: 'Autonomous chemical research with large language models', 'An autonomous laboratory for the accelerated synthesis of novel materials')",
            "brief_description": "Integrated systems combining LLMs/agents with laboratory automation to propose and execute experimental workflows, used to discover or synthesize new molecules/materials; the survey covers successes and limitations of such pipelines.",
            "citation_title": "Autonomous chemical research with large language models.",
            "mention_or_use": "mention",
            "system_name": "Autonomous chemistry / laboratory automation systems (LLM + robotics/automation)",
            "system_type": "hybrid: large language models + automated lab/robotics + planning/decision agents",
            "scientific_domain": "chemistry / materials science / experimental discovery",
            "output_type": "experimental protocols, synthesis plans, candidate molecules/materials, measured experimental results",
            "novelty_level": "can propose highly novel molecules/materials (out-of-distribution relative to training data) in some studies, but novelty raises feasibility and safety concerns",
            "generation_method": "LLM planning + search/optimization over chemical space + integration with automation for execution",
            "validation_method": "wet-lab experimental execution and analytical measurement (e.g., spectrometry); reproducibility checks and human oversight",
            "generation_performance": "Survey references demonstrations (including high-profile works) where novel compounds/materials were proposed; concrete numeric success rates not synthesized in this survey.",
            "validation_performance": "Empirical validation sometimes confirms generated candidates, but the survey emphasizes that generated experiments often lack methodological rigor, practical feasibility, and can require substantial human correction.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Higher novelty correlates with higher risk of infeasible/unverifiable experiments and increased validation/resource cost; validation success declines for more transformational proposals.",
            "generation_validation_comparison": "Large asymmetry: generation is fast and cheap, experimental validation is slow, expensive, and sometimes unsafe; thus generation often dramatically outpaces validation capacity.",
            "uncertainty_quantification": "Often limited; some systems incorporate experimental outcome feedback but explicit probabilistic uncertainty is rarely reported.",
            "calibration_quality": "Not well characterized; survey notes issues with feasibility/hallucinated experimental steps.",
            "out_of_distribution_performance": "Mixed: some truly novel discoveries reported, but generally performance and reproducibility suffer for out-of-distribution candidates.",
            "validation_proxy_metrics": "Plausibility, synthetic accessibility scores, simulation-based checks, prior literature overlap; these proxies are frequently used in lieu of immediate wet-lab validation.",
            "human_validation_required": true,
            "human_validation_frequency": "High: human oversight for safety, feasibility, and interpretation is regularly recommended; frequency increases strongly with novelty.",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical (experimental sciences), low formalization relative to theorem proving.",
            "gap_mitigation_strategies": "Human-AI dialogue, iterative experiment-feedback loops, real-time adjustments, incorporation of domain-specific simulators and analytical tools.",
            "evidence_supporting_gap": "Survey explicitly states experiment validation is time-consuming and generated experiments often lack rigor and practical feasibility, supporting a generation-validation gap.",
            "evidence_contradicting_gap": "Survey cites autonomous lab successes where generated candidates were synthesized (e.g., cited Nature/Science works), showing the gap can be bridged in specific engineered systems.",
            "computational_cost_ratio": "Not numerically specified; qualitatively: validation (wet-lab) is orders-of-magnitude more costly in time and resources than generation.",
            "uuid": "e2097.3"
        },
        {
            "name_short": "Scientific claim verification systems",
            "name_full": "Scientific claim verification systems (examples cited: FactKG, SFAVEL, ClaimVer, MULTIVERS, MAGIC)",
            "brief_description": "A class of systems combining retrieval, knowledge graphs, and LLM reasoning to verify scientific claims by retrieving evidence, aligning claim-evidence, and producing verdicts with rationales.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Scientific claim verification systems (FactKG, SFAVEL, ClaimVer, MULTIVERS, MAGIC - representative examples)",
            "system_type": "hybrid: retrieval-augmented LLMs, knowledge-graph reasoning systems, and weak supervision pipelines",
            "scientific_domain": "scientific claim fact-checking (cross-domain; examples in biomedicine, general science, news about science)",
            "output_type": "claim veracity labels (supported/contradicted/insufficient), sentence- or document-level rationales, ranked evidence",
            "novelty_level": "Primarily in-distribution verification (checking claims anchored to existing literature); poor performance on truly novel claims lacking prior evidence",
            "generation_method": "Not generation of novel facts but retrieval and synthesis: evidence retrieval + LLM synthesis or graph reasoning to produce verdicts and rationales",
            "validation_method": "Evaluation against labeled fact-checking datasets (SciFact, Scifact-open, FEVER), human expert opinion, and evidence-attribution metrics.",
            "generation_performance": "Survey reports improvements via RAG, knowledge graphs, and multi-argument generation (e.g., MAGIC), but emphasizes domain confinement and lack of generalizability; numeric metrics not collated here.",
            "validation_performance": "Varies by dataset/domain; survey states many tools lack domain-general performance and are sensitive to retrieval quality.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Validation accuracy drops for claims that are novel or lack directly retrievable evidence; models often fail silently (hallucinate supporting evidence).",
            "generation_validation_comparison": "Systems can generate plausible rationales/evidence but may misattribute or hallucinate — verification (ground-truth matching) lags behind fluent generation of explanations.",
            "uncertainty_quantification": "Some systems surface relevance scores or top-k evidence confidences; explicit end-to-end uncertainty calibration is rarely reported across surveyed works.",
            "calibration_quality": "Survey indicates calibration issues, susceptibility to being misled by flawed evidence and domain shifts.",
            "out_of_distribution_performance": "Poor; the survey emphasizes weak generalizability across domains and claim types.",
            "validation_proxy_metrics": "Sentence-level rationale alignment, top-k retrieved fact precision, human agreement, and dataset accuracy/precision/recall.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended especially for high-impact or novel claims; frequent when retrieval is ambiguous.",
            "formal_verification_used": false,
            "domain_formalization_level": "Semi-formal to empirical (textual scientific claims).",
            "gap_mitigation_strategies": "Structured KG-based evidence attribution, multi-argument generation with self-refinement (MAGIC), retrieval configuration tuning, and human expert-in-the-loop verification.",
            "evidence_supporting_gap": "Survey notes poor generalizability, susceptibility to flawed evidence, and hallucination in LLM-based claim verifiers.",
            "evidence_contradicting_gap": "Knowledge-graph structuring (FactKG) and careful retrieval strategies can produce stronger evidence attributions that narrow the gap in some settings.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2097.4"
        },
        {
            "name_short": "MOOSE",
            "name_full": "MOOSE (and MOOSE-Chem) — LLM-based hypothesis/idea generation framework",
            "brief_description": "A family of LLM-driven systems for hypothesis and idea generation (including domain-specific variants like MOOSE-Chem) that focus on transforming literature-based inspiration into candidate hypotheses using interactive frameworks and feedback.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "MOOSE / MOOSE-Chem",
            "system_type": "large language model / multi-agent interactive frameworks (LLM-based)",
            "scientific_domain": "hypothesis generation (general; chemistry variant for molecular ideas)",
            "output_type": "hypotheses, research ideas, candidate experimental directions",
            "novelty_level": "Aims for moderately novel outputs (recombining literature-derived concepts); novelty/feasibility trade-offs are highlighted in the survey.",
            "generation_method": "LLM-driven idea synthesis using knowledge graphs, prompts, and iterative feedback loops; domain-specific conditioning for chemistry in MOOSE-Chem.",
            "validation_method": "Comparison with existing literature, feedback/iterative evaluation (human or automated), occasional experimental outcome comparison in domain-specific variants.",
            "generation_performance": "Survey reports that such frameworks can produce comprehensive hypotheses; no standardized numeric metrics provided and evaluation often relies on subjective human scoring.",
            "validation_performance": "Measured via manual novelty/feasibility scoring in most studies; objective validation is limited and variable across works.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Higher novelty tends to reduce feasibility and increases need for manual vetting; survey emphasizes the difficulty of quantifying novelty and feasibility.",
            "generation_validation_comparison": "Generation is efficient at producing many candidate hypotheses; validation (especially objective measures) is slower and often manual, creating a gap.",
            "uncertainty_quantification": "Not generally provided; some systems use internal heuristics or novelty/feasibility proxies.",
            "calibration_quality": "Not reported; evaluation tends to be subjective.",
            "out_of_distribution_performance": "Limited; systems struggle with truly out-of-literature innovations.",
            "validation_proxy_metrics": "Novelty and feasibility human scores, overlap with literature, plausibility heuristics.",
            "human_validation_required": true,
            "human_validation_frequency": "Regularly required; increases for higher-novelty hypotheses.",
            "formal_verification_used": false,
            "domain_formalization_level": "Varies by instantiation (empirical in chemistry; semi-formal in other fields).",
            "gap_mitigation_strategies": "Iterative human-AI feedback, automated peer-review style comments, comparison with literature and experimental outcomes when available.",
            "evidence_supporting_gap": "Survey notes absence of unified benchmarks and objective metrics for hypothesis generation; many works use manual scoring, supporting a generation-validation gap.",
            "evidence_contradicting_gap": "Some iterative feedback frameworks (HypoGeniC, Fun-Search) show improved quality via evaluator loops, indicating gap can be reduced.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2097.5"
        },
        {
            "name_short": "AI Scientist (Lu et al.)",
            "name_full": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "brief_description": "An end-to-end agent/LLM-based research pipeline concept (and implementations) that aims to autonomously propose hypotheses, design and run experiments, and report findings; surveyed as an aspirational archetype and a source of experimental prototypes.",
            "citation_title": "The AI Scientist: Towards fully automated open-ended scientific discovery.",
            "mention_or_use": "mention",
            "system_name": "AI Scientist (autonomous research agent)",
            "system_type": "multi-agent LLM-based research agents integrated with simulation/lab tooling",
            "scientific_domain": "open-ended scientific discovery (cross-domain)",
            "output_type": "hypotheses, experimental plans, synthesized results, manuscript drafts",
            "novelty_level": "Targets highly novel, open-ended outputs; actual novelty achieved varies and is constrained by validation bottlenecks.",
            "generation_method": "Autonomous multi-agent planning, large language model reasoning, search and optimization over hypothesis/experiment spaces",
            "validation_method": "Simulation, controlled experiments, and human expert review; survey indicates full end-to-end validation is often incomplete in prototypes.",
            "generation_performance": "Survey cites prototype systems that can produce end-to-end outputs; no uniform success metrics and many systems remain experimental.",
            "validation_performance": "Limited and variable; experimental confirmations are costly and not always provided, so validation performance is often low relative to generation volume.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "As novelty increases, experimental validation becomes more expensive and failure-prone; survey highlights this resource-driven decline.",
            "generation_validation_comparison": "Strong generation capabilities across multiple pipeline stages but validation (especially experimental confirmation and reproducibility) lags behind significantly.",
            "uncertainty_quantification": "Generally limited; some prototypes use internal scoring or simulation uncertainties but systematic UQ not standard.",
            "calibration_quality": "Not well described in the survey; likely poor for transformational outputs.",
            "out_of_distribution_performance": "Limited due to training/data and validation constraints.",
            "validation_proxy_metrics": "Simulated experiment success, proxy scores (plausibility, synthetic accessibility), and eventual human confirmation.",
            "human_validation_required": true,
            "human_validation_frequency": "High; survey recommends human oversight at multiple pipeline stages, especially for novel findings.",
            "formal_verification_used": false,
            "domain_formalization_level": "Varies; typically empirical domains where formal verification is not applicable.",
            "gap_mitigation_strategies": "Incorporating feedback loops, multi-agent verification, human-in-the-loop checkpoints, and tighter integration with simulators/benchmarks.",
            "evidence_supporting_gap": "Survey explicitly states the field is in its infancy and that many approaches remain experimental with validation bottlenecks, supporting a generation &gt; validation gap.",
            "evidence_contradicting_gap": "Some systems successfully ran experiments and closed loops in narrowly scoped domains demonstrating that the gap can be closed with careful engineering.",
            "computational_cost_ratio": "Not quantified; qualitatively validation (experimentation) is much costlier than generation.",
            "uuid": "e2097.6"
        },
        {
            "name_short": "Automated experiment systems (MLR-Copilot, SANDBOX, CRISPR-GPT, MechAgents)",
            "name_full": "Automated experiment design/execution systems (examples: MLR-Copilot, SANDBOX, CRISPR-GPT, MechAgents)",
            "brief_description": "A class of agentic systems that use LLMs and tool integrations to design, configure, and in some cases execute experiments (wet-lab or computational); the survey reports both promising demonstrations and concerns about feasibility and rigor.",
            "citation_title": "Mlr-copilot: Autonomous machine learning research based on large language models agents.",
            "mention_or_use": "mention",
            "system_name": "Automated experiment systems (MLR-Copilot, SANDBOX, CRISPR-GPT, MechAgents — representative examples)",
            "system_type": "LLM agents + domain-specific toolchains / simulators / lab automation",
            "scientific_domain": "experiment validation across ML, biology, materials, physics (domain-specific variants exist)",
            "output_type": "experiment protocols, ML experiment configurations, genome-editing protocols (CRISPR), simulation runs and analyses",
            "novelty_level": "From incremental (automating routine experiments) to moderately/highly novel when proposing new protocols; novelty increases feasibility risk.",
            "generation_method": "Task-directed LLM prompting and planning; integration with tool APIs, simulators, or lab hardware for execution (where available)",
            "validation_method": "Execution in simulation or physical lab; comparison to expected/ground-truth outcomes; human expert oversight",
            "generation_performance": "Survey indicates LLMs can plan and specify experiments but frequently propose steps lacking methodological rigor or feasibility; numeric metrics not aggregated.",
            "validation_performance": "Varies widely; simulated validations can be fast but wet-lab confirmations are costly and less frequently performed, limiting reported validation success.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Novel experimental designs are more likely to be infeasible or unsafe and have lower validation success; validation cost scales with novelty.",
            "generation_validation_comparison": "Generation is computationally cheap; validation (running experiments) is expensive/time-consuming; significant gap noted in the survey.",
            "uncertainty_quantification": "Some systems incorporate analytical evaluations or simulation metrics as internal proxies, but explicit calibrated uncertainty estimates are rare.",
            "calibration_quality": "Not well characterized in surveyed works; survey warns about overconfidence in generated experiment plans.",
            "out_of_distribution_performance": "Weak for truly novel experiments; systems rely on known patterns and often fail to account for practical constraints.",
            "validation_proxy_metrics": "Simulation success rates, surrogate model predictions, plausibility/consistency checks, reviewer or expert scoring.",
            "human_validation_required": true,
            "human_validation_frequency": "High, especially for wet-lab execution and novel protocols.",
            "formal_verification_used": false,
            "domain_formalization_level": "Empirical/experimental",
            "gap_mitigation_strategies": "Human-AI dialogues, closed-loop experimental feedback, use of simulators and surrogate models to triage candidates before wet-lab validation.",
            "evidence_supporting_gap": "Survey explicitly cites that automatically generated experiments often lack rigor and practical feasibility, supporting a generation-validation gap.",
            "evidence_contradicting_gap": "Survey also notes some successful agent-generated experimental insights and iterative refinement that improved design quality, indicating partial mitigation is possible.",
            "computational_cost_ratio": "Not numerically reported; qualitatively validation (execution) is much more expensive than generation.",
            "uuid": "e2097.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Openreviewer: A specialized large language model for generating critical scientific paper reviews.",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report.",
            "rating": 2
        },
        {
            "paper_title": "Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data.",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models.",
            "rating": 2
        },
        {
            "paper_title": "The AI Scientist: Towards fully automated open-ended scientific discovery.",
            "rating": 2
        },
        {
            "paper_title": "Mlr-copilot: Autonomous machine learning research based on large language models agents.",
            "rating": 2
        },
        {
            "paper_title": "Factkg: Fact verification via reasoning on knowledge graphs.",
            "rating": 1
        },
        {
            "paper_title": "MAGIC: multiargument generation with self-refinement for domain generalization in automatic fact-checking.",
            "rating": 1
        }
    ],
    "cost": 0.030911249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems
23 Jul 2025</p>
<p>Zekun Zhou zkzhou@ir.hit.edu.cn 
Xiaocheng Feng xcfeng@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Peng Cheng Laboratory
ShenzhenChina</p>
<p>Lei Huang lhuang@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Xiachong Feng fengxc@hku.hk 
The University of Hong Kong
China</p>
<p>Ziyun Song zysong@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Ruihan Chen rhchen@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Liang Zhao lzhao@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Weitao Ma wtma@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Yuxuan Gu yxgu@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Baoxin Wang bxwang2@iflytek.com 
iFLYTEK Research
China</p>
<p>Dayong Wu 
iFLYTEK Research
China</p>
<p>Guoping Hu gphu@iflytek.com 
iFLYTEK Research
China</p>
<p>Ting Liu tliu@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Bing Qin qinb@ir.hit.edu.cn 
Harbin Institute of Technology
HarbinChina</p>
<p>Peng Cheng Laboratory
ShenzhenChina</p>
<p>Conner Ganjavi 
Michael B Eppler 
Asli Pekcan 
Brett Biedermann 
Andre Abreu 
Gary S Collins 
Inder- Bir S Gill 
Giovanni E 2024 Cacciamani 
Catherine A Gao 
Frederick M Howard 
Nikolay S Markov 
Emma C Dyer 
Siddhi Ramesh 
Yuan Luo 
Alexander T Pearson 
Mourad Gridach 
Jay Nanavati 
Khaldoun Zine 
El Abidine 
Lenon Mendes 
Sikun Guo 
Hassan Amir 
Guangzhi Shariatmadari 
Albert Xiong 
Eric Huang 
Stefan Xie 
Aidong Bekiranov 
Zhang 
Ideabench 
Rohun Gupta 
Isabel Herzog 
Joseph Weisberger 
John Chao 
Kongkrit Chaiyasate 
Edward S 2023 Lee 
Andrew Head 
Kyle Lo 
Dongyeop Kang 
Raymond Fok 
Sam Skjonsberg 
Daniel S Weld 
Aidan Hogan 
Eva Blomqvist 
Michael Cochez 
Clau- Dia D'amato 
Gerard De Melo 
Claudio Gutierrez 
Sabrina Kirrane 
José Emilio 
Labra Gayo 
Roberto Navigli 
Sebastian Neumaier 
Axel-Cyrille Ngonga Ngomo 
Harbin Institute of Technology
HarbinChina</p>
<p>Axel Polleres 
Sabbir M Rashid 
Anisa Rula 
Lukas Schmelzeisen 
Juan Sequeda 
Steffen Staab 
Antoine 2021 Zimmermann 
Knowledge 
Comput Acm 
Surv 
iFLYTEK Research
China</p>
<p>) Chao- 
Chun Hsu 
Erin Bransom 
Jenna Sparks 
Bailey Kuehl 
Chenhao Tan 
David Wadden 
Lucy Lu Wang 
Kaixuan Huang 
Yuanhao Qu 
Henry Cousins 
William A Johnson 
Di Yin 
Mihir Shah 
Denny Zhou 
Russ B Altman 
Mengdi Wang 
Tal Ifargan 
Lukas Hafner 
Maor Kern 
Ori Alcalay 
Roy 2024 Kishony 
Peter A Jansen 
Oyvind Tafjord 
Marissa Radensky 
Pao Siangliulue 
BhavanaTom Hope 
Dalvi Mishra 
Bod- Hisattwa Prasad Majumder 
Codescientist 
Marc-Alexandre Côté 
Tushar Khot 
Dalvi Bhavana 
Bod- Hisattwa Prasad Mishra 
Oyvind Majumder 
Peter Clark Tafjord 
Discoveryworld 
Albert Qiaochu Jiang 
MatejaWenda Li 
Szymon Tworkowski 
Konrad Czechowski 
Tomasz Odrzygózdz 
Piotr Mi- Los 
Yuhuai Wu 
Mateja 2022a Jamnik 
Thor 
Sean Welleck 
Jin Peng Zhou 
Timothée Lacroix 
Jiacheng Liu </p>
<p>Weijiang Yu, Weihong Zhong, Haotian Wang, Weihua PengLei Huang, Weitao Ma, Zhangyin Feng, Xiaocheng Feng, Bing Qin, and TingQianglong Chen</p>
<p>From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems
23 Jul 2025849F4D481CD613154990E0D345D8280FarXiv:2503.01424v2[cs.AI]Maximilian Idahl and Zahra Ahmadi. 2024. Openreviewer: A specialized large language model for generating critical scientific paper reviews. CoRR, abs/2412.11948.Research Paper Recommendation Systematic Literature Survey
Research is a fundamental process driving the advancement of human civilization, yet it demands substantial time and effort from researchers.In recent years, the rapid development of artificial intelligence (AI) technologies has inspired researchers to explore how AI can accelerate and enhance research.To monitor relevant advancements, this paper presents a systematic review of the progress in this domain.Specifically, we organize the relevant studies into three main categories: hypothesis formulation, hypothesis validation, and manuscript publication.Hypothesis formulation involves knowledge synthesis and hypothesis generation.Hypothesis validation includes the verification of scientific claims, theorem proving, and experiment validation.Manuscript publication encompasses manuscript writing and the peer review process.Furthermore, we identify and discuss the current challenges faced in these areas, as well as potential future directions for research.Finally, we also offer a comprehensive overview of existing benchmarks and tools across various domains that support the integration of AI into the research process.We hope this paper serves as an introduction for beginners and fosters future research.</p>
<p>Introduction</p>
<p>Research is creative and systematic work aimed at expanding knowledge and driving civilization's development (Eurostat, 2018).Researchers typically identify a topic, review relevant literature, synthesize existing knowledge, and formulate hypothesis, which are validated through theoretical and experimental methods.Findings are then documented in manuscripts that undergo peer review before publication (Benos et al., 2007;Boyko et al., 2023).However, this process is resource-intensive, requir-ing specialized expertise and posing entry barriers for researchers (Blaxter et al., 2010).</p>
<p>In recent years, artificial intelligence (AI) technologies, represented by large language models (LLMs), have experienced rapid development (Brown et al., 2020;OpenAI, 2023;Dubey et al., 2024;Yang et al., 2024a;DeepSeek-AI et al., 2024;Guo et al., 2025).These models exhibit exceptional capabilities in text understanding, reasoning, and generation (Schaeffer et al., 2023).In this context, AI is increasingly involving the entire research pipeline (Messeri and Crockett, 2024), sparking extensive discussion about its implications for research (Hutson, 2022;Williams et al., 2023;Morris, 2023;Fecher et al., 2023).Moreover, following the release of ChatGPT, approximately 20% of academic papers and peer-reviewed texts in certain fields have been modified by LLMs (Liang et al., 2024a,b).A study also reveals that 81% of researchers integrate LLMs into their workflows (Liao et al., 2024).</p>
<p>As the application of AI in research attracts increasing attention, a significant body of related studies has begun to emerge.To systematically synthesize existing research, we present comprehensive survey that emulates human researchers by using the research process as an organizing framework.Specifically, as depicted in Figure 1, the research process is divided into three key stages:</p>
<p>(1) Hypothesis Formulation, involving knowledge synthesis and hypothesis generation; (2) Hypothesis Validation, encompassing scientific claim verification, theorem proving, and experiment validation; (3) Manuscript Publication, which focuses on academic publications and is further divided into manuscript writing and peer review.</p>
<p>Comparing with Existing Surveys Although Luo et al. (2025) reviews the application of AI in research, it predominantly focuses on LLMs, while neglecting the knowledge synthesis that precedes hypothesis generation and the Figure 1: Overview of AI for research.The framework consists of three stages: hypothesis formulation, hypothesis validation, and manuscript publication.In the hypothesis formulation stage, knowledge integration leads to the proposal of an initial hypothesis after a topic is identified.The hypothesis validation stage involves verifying the hypothesis from three perspectives to ensure its correctness and validity.Finally, the manuscript publication stage focuses on drafting and publishing the validated hypothesis.theoretical validation of hypothesis.Other surveys concentrate on more specific areas, such as paper recommendation (Beel et al., 2016;Bai et al., 2019;Kreutz and Schenkel, 2022), scientific literature review (Altmami and Menai, 2022), hypothesis generation (Kulkarni et al., 2025), scientific claim verification (Vladika and Matthes, 2023;Dmonte et al., 2024), theorem proving (Li et al., 2024e), manuscript writing (Li and Ouyang, 2024), and peer review (Lin et al., 2023a;Kousha and Thelwall, 2024).Additionally, certain surveys emphasize the application of AI in scientific domains (Zheng et al., 2023b;Zhang et al., 2024d;Gridach et al., 2025).</p>
<p>Contributions Our contributions can be summarized as follows: (1) We align the relevant fields with the research process of human researchers, systematically integrating and extending these aspects while primarily focusing on the research process itself.(2) We introduce a meticulous taxonomy (shown in Figure 2).(3) We provide a summary of tools that can assist in the research process.(4) We discuss new frontiers, outline their challenges, and shed light on future research.</p>
<p>Survey Organization We first elaborate hypothesis formulation ( §2), followed by hypothesis validation ( §3) and manuscript publication ( §4).Additionally, we present benchmarks ( §5), and tools ( §6) that utilized in research.Finally, we outline challenges as well as future directions ( §7).In the Appendix, we provide further discussion on open questions ( §A), challenges faced in different domains ( §B), discusssion about relevant ethical considerations ( §C), and a comparison of capabilities among different methods ( §D).</p>
<p>Hypothesis Formulation</p>
<p>This stage centers on the process of hypothesis formulation.As illustrated in Figure 3, it commences with developing a comprehensive understanding of the domain, followed by identifying a specific aspect and generating pertinent hypothesis.This section is further structured into two key components: Knowledge Synthesis and Hypothesis Generation.</p>
<p>Knowledge Synthesis</p>
<p>Knowledge synthesis constitutes the foundational step in the research process.During this phase, researchers are required to identify and critically evaluate existing literature to establish a thorough understanding of the field.This step is pivotal for uncovering new research directions, refining methodologies, and supporting evidence-based decisionmaking (Asai et al., 2024).In this section, the process of knowledge synthesis is divided into two modules: Research Paper Recommendation and Systematic Literature Review.</p>
<p>Research Paper Recommendation</p>
<p>Research paper recommendation (RPR) identifies and recommends novel and seminal articles aligned with researchers' interests.Prior studies have shown that recommendation systems outperform keyword-based search engines in terms of efficiency and reliability when extracting valuable insights from large-scale datasets (Bai et al., 2019).Existing methodologies are broadly categorized into four paradigms: content-based filtering, collaborative filtering, graph-based approaches, and hybrid systems (Beel et al., 2016;Li and Zou, 2019;Bai et al., 2019;Shahid et al., 2020).Recent advancements propose multi-dimensional classification frameworks based on data source utilization (Kreutz and Schenkel, 2022).</p>
<p>Recent trends in research suggest a decline in publication volumes related to RPR (Sharma et al., 2023), alongside an increasing focus on usercentric optimizations.Existing studies emphasize the limitations of traditional paper-centric interaction models and advocate for more effective utilization of author relationship graphs (Kang et al., 2023).Multi-stage recommendation architectures Systematic Literature Review ( §2.1.2)E.g., AutoSurvey (Wang et al., 2024e) , OpenScholar (Asai et al., 2024) Hypotheses Generation ( §2.2) E.g., SciMON (Wang et al., 2024c) , MOOSE (Yang et al., 2024b) , Dolphin (Yuan et al., 2025) Hypothesis Validation ( §3)</p>
<p>Scientific</p>
<p>Claim Verification ( §3.1) E.g., FactKG (Kim et al., 2023) , SFAVEL (Bazaga et al., 2024) , ClaimVer (Dammu et al., 2024) Theorem Proving ( §3.2) E.g., Dt-solver (Wang et al., 2023b) , LEGO-Prover (Wang et al., 2024b) , DeepSeek-Prover (Xin et al., 2024) Experiment Validation ( §3.3) E.g., SANDBOX (Liu et al., 2023b) , CRISPR-GPT (Huang et al., 2024a) , MLR-Copilot (Li et al., 2024d) Manuscript Publication ( §4)</p>
<p>Manuscript Writing ( §4.1) E.g., Scilit (Gu and Hahnloser, 2023) , UR3WG (Shi et al., 2023) , STEP-BACK (Tang et al., 2024a) Peer Review ( §4.2) E.g., SWIF2T (Chamoun et al., 2024) , GLIMPSE (Darrin et al., 2024) , MetaWriter (Sun et al., 2024) Figure 2: Taxonomy of Hypothesis Formulation, Hypothesis Validation and Manuscript Publication (This is a simplified version, full version in Figure 6).</p>
<p>that integrate diverse methodologies have been shown to achieve superior performance (Pinedo et al., 2024;Stergiopoulos et al., 2024).Visualization techniques that link recommended papers to users' publication histories via knowledge graphs (Kang et al., 2022) and LLMs-powered comparative analysis frameworks (Lee et al., 2024) represent emerging directions for enhancing interpretability and contextual relevance.</p>
<p>Systematic Literature Review</p>
<p>Systematic literature review (SLR) constitutes a rigorous and structured methodology for evaluating and integrating prior research on a specific topic (Webster and Watson, 2002;Zhu et al., 2023;Bolaños et al., 2024).In contrast to single-document summaries (Elhadad et al., 2005), SLR entails synthesizing information across multiple related scientific documents (Altmami and Menai, 2022).SLR can further be divided into two stages: outline generation and full-text generation (Shao et al., 2024;Agarwal et al., 2024b;Block and Kuckertz, 2024).</p>
<p>Outline generation, especially structured outline generation, is highlighted by recent studies as a pivotal factor in enhancing the quality of surveys.Zhu et al. (2023) demonstrated that hierarchical frameworks substantially enhance survey coherence.AutoSurvey (Wang et al., 2024e) extended conventional outline generation by recommending both sub-chapter titles and detailed content descriptions, ensuring comprehensive topic coverage.Additionally, multi-level topic generation via clustering methodologies has been proposed as an effective strategy for organizing survey structures (Katz et al., 2024).Advanced systems such as STORM (Shao et al., 2024) employed LLMdriven outline drafting combined with multi-agent discussion cycles to iteratively refine the generated outlines.Tree-based hierarchical architectures have gained increasing adoption in this domain.For instance, CHIME (Hsu et al., 2024) optimized LLM-generated hierarchies through human-AI collaboration, while HiReview (Hu et al., 2024b) demonstrated the efficacy of multi-layer tree representations for systematic knowledge organization.</p>
<p>Full-text generation follows the outline generation stage.AutoSurvey and Lai et al. (2024) utilized LLMs with carefully designed prompts to construct comprehensive literature reviews step-bystep.PaperQA2 (Skarlinski et al., 2024) introduced an iterative agent-based approach for generating reviews, while STORM employed multiagent conversation data for this purpose.LitLLM (Agarwal et al., 2024a) and Agarwal et al. (2024b) adopted a plan-based search enhancement strategy.KGSum (Wang et al., 2022a) integrated knowledge graph information into paper encoding and used a two-stage decoder for summary generation.Bio-SIEVE (Robinson et al., 2023) and Susnjak et al. (2024) fine-tuned LLMs for automatic review generation.OpenScholar (Asai et al., 2024) developed a pipeline that trained a new model without relying on a dedicated survey-generation model.</p>
<p>Hypothesis Generation</p>
<p>Hypothesis generation, known as idea generation, refers to the process of coming up with new concepts, solutions, or approaches.It is the most important step in driving the progress of the entire research (Qi et al., 2023).</p>
<p>Early work focused more on predicting relationships between concepts, because researchers believed that new concepts come from links with old concepts (Henry and McInnes, 2017;Krenn et al., 2022).As language models became more powerful (Zhao et al., 2023a), researchers are beginning to focus on open-ended idea generation (Girotra et al., 2023;Si et al., 2024;Kumar et al., 2024).Recent advancements in AI-driven hypothesis generation highlight diverse approaches to research conceptualization.For instance, MOOSE-Chem (Yang et al., 2024c) and IdeaSynth (Pu et al., 2024) used LLMs to bridge inspiration-tohypothesis transformation via interactive frameworks.The remaining research primarily falls into two areas: enhancing input data quality and improving the quality of generated hypothesis.</p>
<p>Input data quality improvement is demonstrated by Majumder et al. (2024a); Liu et al. (2024a), who showed that LLMs can generate comprehensive hypothesis from existing academic data.Literature organization strategies have evolved through various methodologies, including triplet representations (Wang et al., 2024c), chain-based architectures (Li et al., 2024a), and complex database systems (Wang et al., 2024d).Knowledge graphs emerge as critical infrastructure (Hogan et al., 2021), enabling semantic relationship mapping via subgraph identification (Buehler, 2024;Ghafarollahi and Buehler, 2024).Notably, SciMuse (Gu and Krenn, 2024) pioneered researcher-specific hypothesis generation by constructing personalized knowledge graphs.</p>
<p>Hypothesis quality improvement has been addressed through feedback and iteration (Rabby et al., 2025), as proposed by HypoGeniC (Zhou et al., 2024) and MOOSE (Yang et al., 2024b).Feedback mechanisms include direct responses to hypothesis (Baek et al., 2024), experimental outcome evaluations (Ma et al., 2024;Yuan et al., 2025), comparison with the existing literature (Schmidgall and Moor, 2025), and automated peer review comments (Lu et al., 2024).Fun-Search (Romera-Paredes et al., 2024) generates solutions by iteratively combining the innovative capabilities of LLM with the verification capabilities of an evaluator.Beyond iterative feedback, collaborative efforts among researchers have also been recognized, leading to multi-agent hypothesis generation approaches (Nigam et al., 2024;Ghafarollahi and Buehler, 2024).VIRSCI (Su et al., 2024) further optimized this process by customizing knowledge for each agent.Additionally, the Nova framework (Hu et al., 2024a) was introduced to refine hypothesis by leveraging outputs from other research as input.</p>
<p>Knowledge synthesis and hypothesis generation comprise the hypothesis formulation phase.Research paper recommendation supports knowledge acquisition, while systematic literature review aids organization within knowledge synthesis.Recent advances integrate LLMs (de la Torre-López et al., 2023) to enhance knowledge integration (Huang and Tan, 2023;Gupta et al., 2023;Kacena et al., 2024;Tang et al., 2024b).By developing a deep understanding of a domain through knowledge synthesis, researchers can identify research directions and use hypothesis generation techniques to formulate hypothesis.Additionally, the distinction between scientific discovery and hypothesis generation is discussed in §A.</p>
<p>Hypothesis Validation</p>
<p>In scientific research, any proposed hypothesis must undergo rigorous validation to establish its validity.In some studies, this process is also referred to as 'falsification' (Liu et al., 2024d;Huang et al., 2025).As illustrated in Figure 4, this section explores the application of AI in verifying scientific hypothesis through three methodological components: Scientific Claim Verification, Theorem Proving, and Experiment Validation.</p>
<p>Scientific Claim Verification</p>
<p>Scientific claim verification, also referred to as scientific fact-checking or scientific contradiction detection, aims to assess the veracity of claims related to scientific knowledge.This process assists scientists in verifying research hypothesis, discovering evidence, and advancing scientific work (Wadden et al., 2020;Vladika and Matthes, 2023;Skarlinski et al., 2024).Research on scientific claim verification primarily focuses on three key elements: the claim, the evidence, and the validity of the claim (Dmonte et al., 2024).Claim Studies have highlighted that certain claims lack supporting evidence (Wührl et al., 2024a), while others have demonstrated the ability to perform claim-evidence alignment without annotated data (Bazaga et al., 2024).Additionally, methods such as HiSS (Zhang and Gao, 2023) and ProToCo (Zeng and Gao, 2023) proposed generating multiple claim variants to enhance verification.</p>
<p>Evidence Existing research has explored various aspects related to evidence, including evidentiary sources (Vladika and Matthes, 2024a), retrieval configurations (Vladika and Matthes, 2024b), strategies for identifying and mitigating flawed evidence (Glockner et al., 2022;Wührl et al., 2024b;Glockner et al., 2024a), and approaches for processing sentence-level (Pan et al., 2023b) versus document-level indicators (Wadden et al., 2022b).</p>
<p>Verification In the verification results generation phase, MAGIC (Kao and Yen, 2024) and SERIf (Cao et al., 2024b) proposed utilizing LLMs to synthesize evidence into more comprehensive information.FactKG (Kim et al., 2023) and Muharram and Purwarianti (2024) structured evidence into knowledge graphs, enabling claim attribution (Dammu et al., 2024;Wu et al., 2023).</p>
<p>Theorem Proving</p>
<p>Theorem proving constitutes a subtask of logical reasoning, aimed at reinforcing the validity of the underlying theory within a hypothesis (Pease et al., 2019;Yang et al., 2023c;Li et al., 2024e).</p>
<p>Following the proposal of GPT-f (Polu and Sutskever, 2020) to utilize generative language models for theorem proving, researchers initially combined search algorithms with language models (Lample et al., 2022;Wang et al., 2023b).However, a limitation in search-based approaches was later identified by Wang et al. (2024a), who highlighted their tendency to explore insignificant intermediate conjectures.This led some teams to abandon search algorithms entirely.Subsequently, alternative methods emerged, such as the two-stage framework proposed by Jiang et al. (2023) and Lin et al. (2024), which prioritized informal conceptual generation before formal proof construction.Thor (Jiang et al., 2022a) introduced theorem libraries to accelerate proof generation, an approach enhanced by Logo-power (Wang et al., 2024b) through dynamic libraries.Studies like Baldur (First et al., 2023), Mustard (Huang et al., 2024c), and DeepSeek-Prover (Xin et al., 2024) demonstrated improvements via targeted data synthesis and fine-tuning, though COPRA (Thakur et al., 2024) questioned their generalizability and proposed an environment-agnostic alternative.Complementary strategies included theoretical decomposition into sub-goals (Zhao et al., 2023b) and leveraging LLMs as collaborative assistants in interactive environments (Song et al., 2024).</p>
<p>Experiment Validation</p>
<p>Experiment validation involves designing and conducting experiments based on the hypothesis.The empirical validity of the hypothesis is then determined through analysis of the experimental results (Huang et al., 2024b).</p>
<p>Experiment validation represents a timeconsuming component of scientific research.Recent advancements in LLMs have enhanced their ability to plan and reason about experimental tasks (Kambhampati et al., 2024), prompting researchers to use these models for designing and implementing experiments (Ruan et al., 2024b).2024) leveraged agent-generated analytical insights to facilitate iterative hypothesis refinement.In contrast, social science research often uses LLMs as experimental subjects to simulate human participants (Liu et al., 2023b;Manning et al., 2024;Mou et al., 2024).</p>
<p>A hypothesis can be conceptualized as consisting of two key components: claims and theorems.Scientific claim verification and theorem proving offer theoretical validation of hypothesis through formal reasoning and logical deduction, whereas experiment validation provides comprehensive practical validation via empirical testing.</p>
<p>Manuscript Publication</p>
<p>Upon validating a hypothesis as feasible, researchers generally progress to the publication stage.As depicted in Figure 5, this section categorizes Manuscript Publication into two primary components: Manuscript Writing and Peer Review.</p>
<p>Manuscript Writing</p>
<p>Manuscript writing, also referred to as scientific or research writing.At this stage, researchers articulate the hypothesis they have formulated and the results they have validated in the form of a scholarly paper.This process is crucial, as it not only disseminates findings but also deepens researchers' understanding of their work (Colyar, 2009).</p>
<p>Early shared tasks focused on assisting researchers in writing or analyzing linguistic features (Dale and Kilgarriff, 2010;Daudaravicius, 2015).Recent advances have led to three main directions: citation text generation, related work generation, and complete manuscript generation.</p>
<p>Related Work Generation (Paragraph Level)</p>
<p>In contrast to citation text generation, several studies have focused on related work generation in scholarly writing, emphasizing the production of multiple citation texts and the systematic analysis of inter-citation relationships (Li andOuyang, 2022, 2024).The ScholaCite framework (Martin-Boyle et al., 2024) leveraged GPT-4 to cluster citation sources and generate draft literature review sections, although it required manually provided reference lists.By contrast, the UR3WG system (Shi et al., 2023) adopted a retrieval-augmented architecture integrated with large language models to autonomously acquire relevant references.</p>
<p>To improve the quality of generated related work sections, Yu et al. (2024b)</p>
<p>Peer Review</p>
<p>Peer review serves as a critical mechanism for improving the quality of academic manuscripts through feedback and evaluation, forming the cornerstone of quality control in scientific research.However, the process is hindered by its slow pace, high time consumption, and increasing strain due to the growing academic workload (Lin et al., 2023a;Kousha and Thelwall, 2024;Thelwall and Yaghi, 2024).To address these challenges and enhance manuscript quality, researchers have investigated the application of AI in peer review (Yuan et al., 2022;Liu and Shah, 2023;Niu et al., 2023;Kuznetsov et al., 2024;Thakkar et al., 2025).Peer review can be categorized into two main types: paper review generation and meta-review generation.</p>
<p>Paper Review Generation In paper review generation, reviewers provide both scores and evaluations for manuscripts.For instance, Setio and Tsuchiya (2022) 2024) modeled the review process as a dynamic, multi-turn dialogue.Furthermore, CycleResearcher (Weng et al., 2024) and OpenReviewer (Idahl and Ahmadi, 2024) finetuned models for comparative reviews and structured outputs aligned with conference guidelines.</p>
<p>Meta-Review Generation</p>
<p>In meta-review generation, chairs are tasked with identifying a paper's core contributions, strengths, and weaknesses while synthesizing expert opinions on manuscript quality.Meta-reviews are conceptualized as abstractions of comments, discussions, and paper abstracts (Li et al., 2023).Santu et al. (2024) investigated the use of LLMs for automated meta-review generation, while Zeng et al. (2023) proposed a guided, iterative prompting approach.MetaWriter (Sun et al., 2024) utilized LLMs to extract key reviewer arguments, whereas GLIMPSE (Darrin et al., 2024) and Kumar et al. (2023) focused on reconciling conflicting statements to ensure fairness.Additionally, Li et al. (2024b) introduced a three-layer sentiment consolidation framework for meta-review generation, and PeerArg (Sukpanichnant et al., 2024) integrated LLMs with knowledge representation to address subjectivity and bias via a multiparty argumentation framework (MPAF).DeepReview (Zhu et al., 2025) generates a comprehensive meta-review by simulating expert evaluation across multiple dimensions.</p>
<p>During the Manuscript Publication phase, researchers can leverage AI to systematically complete manuscript writing by incorporating validated hypothesis, related papers, and literature reviews.The manuscript is subsequently subjected to peer review, involving iterative revisions before culminating in its final publication.</p>
<p>Given that AI for research spans multiple disciplines, the tasks addressed within each domain vary significantly.To facilitate cross-domain exploration, we provide a summary of benchmarks associated with various areas, including research paper recommendation, systematic literature review, hypothesis generation, scientific claim verification, theorem proving, experiment verification, manuscript writing, and peer review.An overview of these benchmarks is presented in Table 9.</p>
<p>Tools</p>
<p>To accelerate the research workflow, we have curated a collection of tools designed to support various stages of the research process, with their applicability specified for each stage.To ensure practical relevance, our selection criteria emphasize tools that are publicly accessible or demonstrate significant influence on GitHub.A comprehensive overview of these tools is presented in Table 10.</p>
<p>Challenges</p>
<p>We identify several intriguing and promising avenues for future research.</p>
<p>Integration of Diverse Research Tasks</p>
<p>The research process is an integrated pipeline of interdependent stages.Paper recommendation and literature review provide an AI tool with a field overview and relevant works, ensuring that hypothesis generation is informed and of higher quality.Hypothesis validation assesses feasibility both logically and practically, with results feeding back to refine the hypothesis.In manuscript writing, validated hypotheses and prior outputs serve as key inputs.Peer review evaluates the manuscript and offers feedback across modules, enabling the hypothesis generator to adjust content accordingly (Lu et al., 2024).In addition, combinations can also be made between some small fields, for instance, meta-review generation could be integrated with scientific claim verification, experiment verification could be linked with hypothesis formulation (Jansen et al., 2025;Yuan et al., 2025;Liu et al., 2024d), and research paper recommendation systems could be connected with manuscript writing processes (Gu and Hahnloser, 2023).Furthermore, some studies have begun to emphasize the development of systems capable of covering mul-tiple stages of the research process (Jansen et al., 2024;Weng et al., 2024;Yu et al., 2024a).</p>
<p>Integration with Reasoning-Oriented Language Models</p>
<p>Research is a process that places a significant emphasis on logic and reasoning.Theorem proving serves as a subtask within logical reasoning (Li et al., 2024e), while hypothesis generation is widely recognized as the primary form of reasoning employed by scientists when observing the world and proposing hypothesis to explain these observations (Yang et al., 2024b).Experiment verification, in turn, demands a high degree of planning capability from models (Kambhampati et al., 2024).</p>
<p>Recent advances in reasoning-oriented language models, such as OpenAI-o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025), have substantially enhanced the reasoning abilities of these models.Consequently, we posit that integrating reasoning language models with reasoning tasks is a promising future direction.This prediction was validated by experiments conducted by Schmidgall et al. ( 2025) using o1-Preview.Furthermore, in Appendix §B, we provide a summary of the challenges in hypothesis formulation, validation, and manuscript publication.</p>
<p>Conclusion</p>
<p>This paper provides a systematic survey of existing research on AI for research, offering a comprehensive review of the advancements in the field.Within each category, we offer detailed descriptions of the associated subfields.In addition, we examine current challenges, ethical considerations, and potential directions for future research.To support researchers in exploring AI-driven research applications and enhancing workflow efficiency, we also summarize existing benchmarks and tools, accompanied by a comparative analysis of representative methods and their capabilities.</p>
<p>Furthermore, in the course of investigating various subfields within AI for research, we observed that this domain remains in its infancy.Research in numerous directions remains at an experimental stage, and substantial progress is necessary before these approaches can be effectively applied in practical scenarios.We hope that this survey serves as an introduction to the field for researchers and contributes to its continued advancement.</p>
<p>Limitation</p>
<p>This study presents a comprehensive survey of AI for research, based on the framework of the research process conducted by human researchers.</p>
<p>We have made our best effort, but there may still be some limitations.Due to space constraints, we provide only concise summaries of each method without detailed technical elaboration.Given the rapid progress in AI and the expanding research landscape, we primarily focus on works published after 2022, with earlier studies receiving less attention.To emphasize areas that closely mimic the human research process, some topics are excluded from the main text but briefly discussed in Appendix §A.Moreover, as AI for Research is still an emerging field, the lack of standardized benchmarks and evaluation metrics hinders direct comparison.Nonetheless, we offer a comparative analysis of representative methods across domains using attribute graphs in Appendix §D.</p>
<p>A Further Discussion</p>
<p>Open Question: What is the difference between AI for science and AI for research?We posit that AI for research constitutes a subset of AI for science.While AI for research primarily focuses on supporting or automating the research process, it is not domain-specific and places greater emphasis on methodological advancements.In contrast, AI for science extends beyond the research process to include result-oriented discovery processes within specific domains, such as materials design, drug discovery, biology, and the solution of partial differential equations (Zheng et al., 2023b;AI4Science and Quantum, 2023;Zhang et al., 2024d).</p>
<p>Open Question: What is the difference between hypothesis generation and scientific discovery?Hypothesis generation, which is primarily based on literature-based review (LBD) (Swanson, 1986;Sebastian et al., 2017), emphasizing the process by which researchers generate new concepts, solutions, or approaches through existing research and their own reasoning.Scientific discovery encompasses not only hypothesis generation, but also innovation in fields like molecular optimization and drug development (Ye et al., 2024;Liu et al., 2024b), driven by outcome-oriented results.</p>
<p>Open Question: What is the difference between systematic literature review and related work generation?Existing research frequently addresses the systematic literature survey, which constitutes a component of the knowledge synthesis process during hypothesis formulation, alongside the related work generation phase in manuscript writing (Luo et al., 2025).However, we argue that these two tasks are distinct in nature.The systematic literature survey primarily focuses on summarizing knowledge extracted from diverse scientific documents, thereby assisting researchers in acquiring an initial understanding of a specific field (Altmami and Menai, 2022).In contrast, related work generation focuses on the writing process, emphasizing selection of pertinent literature and effective content structuring (Nishimura et al., 2024).</p>
<p>Discussion: Potential links between artificial intelligence systems and human research practice</p>
<p>• In research paper recommendation, Paper-Weaver (Lee et al., 2024) offers an interactive page that allows users to modify the topics they are interested in.</p>
<p>• In systematic literature review, Block and Kuckertz (2024) highlights the significant role of humans, including setting correct questions and individualized problem-solving and theorizing.Meanwhile, Hsu et al. ( 2024) emphasizes manual correction during the outline generation process.</p>
<p>• In hypothesis generation, AI engages more closely with human researchers, ranging from scenarios where humans provide the core ideas and AI contributes by iteratively refining them (Pu et al., 2024), to more collaborative settings where humans and AI engage in dialogue to facilitate new scientific discoveries (Ni et al., 2024;Liu et al., 2024b;Ye et al., 2024).</p>
<p>• • In peer review, Kumar et al. (2023); Darrin et al. (2024) advocate for assigning the responsibility of generating meta-reviews to human researchers.The role of AI is to assist by identifying conflicts among reviewers' opinions and supporting the chair in the scoring process, rather than independently assigning scores.</p>
<p>Discussion:</p>
<p>The involvement of AI in manuscript writing The application of AI in manuscript writing has been accompanied by significant controversy.As LLMs demonstrated advanced capabilities, an increasing number of researchers began adopting these systems for scholarly composition (Liang et al., 2024b;Gao et al., 2023a).This trend raised concerns within the academic community (Salvagno et al., 2023), with scholars explicitly opposing the attribution of authorship to AI systems (Lee, 2023).Despite these reservations, the substantial time efficiencies offered by this technology led researchers to gradually accept AI-assisted writing practices (Gruda, 2024;Huang and Tan, 2023;Chen, 2023).This shift ultimately led to formal guidelines issued by leading academic journals (Ganjavi et al., 2024;Xu, 2025).</p>
<p>Discussion: Some areas that have not been discussed In addition to the eight areas discussed above, there are other lines of work that also aim to support scientific research, such as reading assistance (Kang et al., 2020;Head et al., 2021;Lo et al., 2023), which helps researchers read academic papers; literature processing1 , which handles documents in various formats to provide effective data for subsequent tasks; as well as code and data generation (Bauer et al., 2024;Zheng et al., 2023a), which serve as a foundation for experimental validation.However, as our focus is on the core process of scientific research, we have chosen to omit these aspects from the main text.</p>
<p>B Challenges B.1 Hypothesis Formulation</p>
<p>Knowledge Synthesize Existing paper recommendation tools predominantly rely on the metadata of existing publications to suggest related articles, which often results in a lack of user-specific targeting and insufficiently detailed presentation that hampers comprehension.Leveraging LLMs can facilitate the construction of dynamic user profiles, enabling personalized literature recommendations and enhancing the richness of the contextual information provided for each recommended article, ultimately improving the user experience.In the process of generating systematic literature reviews, our practical experience reveals that the outline generation tools often produces redundant results with insufficient hierarchical structure.Moreover, the full-text generation process is prone to hallucinations-for instance, statements may not correspond to the cited articles-a pervasive issue in large language models (Huang et al., 2023;Bolaños et al., 2024;Susnjak et al., 2024).This problem can be ameliorated by enhancing the foundational model capabilities or by incorporating citation tracing.</p>
<p>Hypothesis Generation Most existing tools generate hypotheses by designing prompts or construct-ing systematic frameworks, which heavily rely on the capabilities of pre-trained models.However, these methods struggle to balance the novelty, feasibility, and validity of the hypotheses (Li et al., 2024c).Furthermore, our investigation reveals that many current approaches adopt novelty and feasibility as evaluation metrics; these metrics are either difficult to quantify or require manual scoring, which can introduce bias.To date, there is no unified benchmark to compare the various methods, and we believe that future research should prioritize establishing a unified metric that objectively reflects the strengths and weaknesses of different approaches.</p>
<p>B.2 Hypothesis Validation</p>
<p>Most existing scientific claim verification tools are largely confined to specific domains, exhibiting poor generalizability, which limits their practical applicability (Vladika and Matthes, 2023).Theorem proving, the scarcity of relevant data adversely affects performance improvements through training , results across different proof assistants are not directly comparable, and the lack of standardized evaluation benchmarks presents numerous challenges.Moreover, current approaches remain predominantly in the research stage and lack practical tools that facilitate interaction with researchers (Li et al., 2024e).Experiment Validation, as automatically generated experiments often suffer from a lack of methodological rigor, practical feasibility, and alignment with the original research objectives (Lou et al., 2024).All these fields require rigorous logical reasoning, and I believe that the recent surge in advanced reasoning technologies could potentially address these issues.</p>
<p>B.3 Manuscript Publication</p>
<p>Similar to systematic literature surveys, manuscript writing is also adversely affected by hallucination issues (Athaluri et al., 2023;Huang et al., 2023).Even when forced citation generation is employed, incorrect references may still be introduced (Aljamaan et al., 2024).Furthermore, the text generated by models requires meticulous examination by researchers to avoid ethical concerns, such as plagiarism risks (Salvagno et al., 2023).AI-generated manuscript reviews frequently provide vague suggestions and are susceptible to biases (Chamoun et al., 2024;Drori and Te'eni, 2024).Additionally, during meta-review generation, models can be misled by erroneous information arising from the manuscript review process (Kumar et al., 2023).To address these issues, it may be necessary for the industry to establish appropriate regulations or to employ AI-based methods for detecting AIgenerated papers and reviews (Lin et al., 2023a).</p>
<p>C Ethical Considerations</p>
<p>AI has demonstrated significant potential in enhancing productivity by mitigating human limitations, thereby motivating increased investigation into its capacity to accelerate the research process (Messeri and Crockett, 2024).Nevertheless, its integration into scientific workflows introduces a range of ethical concerns (Fecher et al., 2023;Morris, 2023), including algorithmic biases, data privacy issues, risks of plagiarism, and the broader implications of AI-generated content for research communities.In this work, we examine these ethical challenges across the key stages of the research lifecycle: hypothesis formulation, validation, and publication.</p>
<p>During the hypothesis formulation stage, research paper recommendation systems and literature reviews are commonly employed; however, they often suffer from limitations that can lead to the formation of information bubbles and restrict exposure to diverse viewpoints.Furthermore, these systems tend to reinforce recognition disparities between prominent and lesser-known researchers and may inadvertently contribute to the dissemination of misinformation (Polonioli, 2021;Bolaños et al., 2024).To address these biases, recommendation algorithms can be enhanced by emphasizing contentbased rather than author-based recommendations and by incorporating robust evaluation mechanisms to strengthen the credibility of suggested materials.</p>
<p>In contrast, AI-driven hypothesis generation presents more pronounced ethical challenges.First, the attribution of intellectual property rights and authorship for AI-generated hypotheses remains ambiguous (Majumder et al., 2024a).Additionally, the widespread generation of low-quality content poses a risk of diluting the integrity of the academic landscape (Hu et al., 2024a), while the potential misuse of such technologies for illicit purposes cannot be overlooked (Si et al., 2024).Addressing these concerns necessitates the development of robust accountability frameworks, the assignment of clear responsibility for AI-generated outputs to researchers, and the establishment of appropriate legal and regulatory mechanisms.</p>
<p>During the hypothesis validation phase, automated systems for scientific fact-checking remain underdeveloped.This limitation may be exploited by malicious actors to create advanced misinformation generators capable of circumventing existing fact-checking tools (Wadden et al., 2022b).Likewise, in the context of experimental validation, there is a risk of unethical or legally questionable experiments being designed (Eger et al., 2025).These concerns underscore the need for continued research into model safety.</p>
<p>During the manuscript publication stage, several challenges remain.Text generated by AI models may carry a risk of plagiarism (Salvagno et al., 2023;Gupta and Pruthi, 2025), while AI-assisted peer reviews often offer vague feedback and exhibit inherent biases (Schintler et al., 2023;Drori and Te'eni, 2024;Pataranutaporn et al., 2025).To address these issues, the development of robust detection methods is essential.However, current detection tools are still in the early stages of maturity (Gupta and Pruthi, 2025).</p>
<p>D Ability Comparison</p>
<p>An effective survey should not only summarize existing methods within a field but also provide comparative analyses of different approaches.However, the domain of AI for Research remains in its early stages, with many areas lacking standardized benchmarks and even established evaluation metrics.To facilitate a clearer understanding of the distinctions among various methods, we draw on existing literature (Kang et al., 2023;Bolaños et al., 2024;Luo et al., 2025;Vladika and Matthes, 2023;Yang et al., 2023c;Li andOuyang, 2022, 2024;Lin et al., 2023a)
✓ ✓ ✓ ✓ MirrorThink ✓ ✓ ✓ ✓ ✓ SciSpace ✓ ✓ ✓ ✓ ✓ AskYourPDF ✓ ✓ ✓ ✓ ✓ ✓ Iflytek ✓ ✓ ✓ ✓ ✓ ✓ ✓ FutureHouse ✓ ✓ ✓ ✓ Enago Read ✓ ✓ ✓ ✓ ✓ ✓ Aminer ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ OpenRsearcher ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ResearchFlow ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ You.com ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ GPT Researcher
Figure 3 :
3
Figure 3: This figure illustrates the hypothesis formulation process, consisting of two stages: knowledge synthesis and hypothesis generation, which together produce an initial hypothesis related to a specific topic.</p>
<p>Figure 4 :
4
Figure 4: This figure illustrates the various perspectives for hypothesis validation during the hypothesis validation stage.A hypothesis is typically divided into scientific claims and theorems, with SCI-claim verification (scientific claim verification) and theorem proving ensuring theoretical correctness, while experiment validation assesses practical feasibility.</p>
<p>Furthermore, Atanasova et al. (2020); Krishna et al. (2022); Pan et al. (2023a); Eldifrawi et al. (2024); Zhang et al. (2024b) advocated for generating explanatory annotations alongside experimental out-comes during the verification process.Meanwhile, Das et al. (2023); Altuncu et al. (2023) emphasized the critical role of domain expertise in ensuring accurate verification.</p>
<p>To ensure accuracy, studies such asZhang et al.  (2023)  andArlt et al. (2024) imposed input/output constraints, though this reduced generalizability.To address this, Boiko et al. (2023); Bran et al. (2024); Huang et al. (2024a) integrated tools to expand model capabilities.Full automation was achieved by Ni and Buehler (2023); Li et al. (2024a); Lu et al. (2024) through prompt-guided multi-agent collaboration.Madaan et al. (2023); Yuan et al. (2025) further highlighted that the integration of feedback mechanisms demonstrated potential for enhancing design quality, while Zhang et al. (2024a); Liu et al. (2024c); Ni et al. (2024) employed experimental outcomes to refine hyperparameter configurations, and Szymanski et al. (2023); Li et al. (2024d); Baek et al. (</p>
<p>Figure 5 :
5
Figure 5: This figure shows the transformation of a validated hypothesis into a publication, leveraging outputs from the hypothesis formulation and validation stages.</p>
<p>In scientific claim verification,Altuncu et al. (2023);Das et al. (2023) highlight the critical role of experts in countering fake scientific news and advocate for the incorporation of expert opinions as a form of evidence.•In theorem proving,Song et al. (2024) proposes leveraging LLMs as assistants to human researchers by generating suggested proof steps throughout the proving process.• In experiment Validation, Ni et al. (2024) enhances the experimental setup through human-AI dialogue, whereas Li et al. (2024d) incorporates human input and real-time adjustments during the execution phase to optimize experimental design.• In manuscript writing, Ifargan et al. (2024); Feng et al. (2024); Du et al. (2022a) require human intervention to suggest improvements to AI-generated paragraphs and enhance their quality through interactive methods.</p>
<p>Table 1 :
1
and adopt attribute graphs to compare representative approaches within each subfield, as illustrated in table §1 to table §8.Research Paper Recommendation, we referred to Kang et al. (2023) for comparing different methods, where R represents Paper recommender score, Co represents Co-author relationship, and Ci represents Cited author relationship.
MethodHuman-Computer Interaction LLMRequired InformationReturn InformationRelevance SourceComLittee (Kang et al., 2023)✓-Authorship GraphsMeta data with relevant authorsR, Co, CiArZiGo (Pinedo et al., 2024)✓-User InterestMeta dataRPaperWeaver (Lee et al., 2024)✓✓Collected PapersMeta data with descriptionRKang et al. (2022)--Author's social network relationships +Reference relationshipMeta data with relevant authorsRMethodResearch Field Across Stages Human InteractionTaskInputOutputEvaluation MethodAutoSurvey (Wang et al., 2024e)Any✓-Outline Generation, +Full-text GenerationTitle &amp; Full ContentLiterature SurveyLLM &amp; HumanCHIME (Hsu et al., 2024)Biomedicine-✓Outline GenerationTitle &amp; Full Content Hierarchical Outline Automatic MetricsKnowledge Navigator (Katz et al., 2024)Any--Outline GenerationTitle &amp; Full Content Hierarchical Outline LLM &amp; HumanRelatedly (Palani et al., 2023)Any--Full-text Generation Title &amp; Related WorkLiterature SurveyHumanSTORM (Shao et al., 2024)Any--Outline Generation, +Full-text GenerationTitle &amp; Full ContentLiterature SurveyLLM &amp; Automatic Metrics</p>
<p>Table 2 :
2
Bolaños et al. (2024) Review, we referred toBolaños et al. (2024)and made modifications, thereby comparing different methods.
MethodResearch FieldAcross Stages Human Interaction Multi-agent Trained Model Online RAG Novelty Feasibility ValidityCOI (Li et al., 2024a)Any✓---✓✓✓✓Learn2Gen (Li et al., 2024c)Artification Intelligence✓--✓-✓✓✓MatPilot (Ni et al., 2024)Materials Science✓✓✓--✓✓-SciAgents (Ghafarollahi and Buehler, 2024)Any-✓✓--✓✓-SciMON (Wang et al., 2024c)Any---✓-✓--</p>
<p>Table 3 :
3
Luo et al. (2025)tion, we referred toLuo et al. (2025)and made modifications, thereby comparing different methods.
MethodInputDocument RetrievalHuman InteractionRationale SelectionEvidence FormatOutputMULTIVERS (Wadden et al., 2022b) Claim &amp; scientific abstractProvided-LongformerDocumentLabel &amp; sentence-level rationalesSFAVEL (Bazaga et al., 2024)ClaimPre-trained Language Model--knowledge graph Top-K Facts &amp; Corresponding Relevance ScoresProToCo (Zeng and Gao, 2023)Claim-Evidence PairProvided--SentenceLabelMAGIC (Kao and Yen, 2024)ClaimProvided-Dense Passage RetrieverSentenceLabelaedFaCT (Altuncu et al., 2023)News ArticleGoogle Search✓HumanDocumentEvidence</p>
<p>Table 4 :
4
Scientific Claim Verification, we referred toVladika and Matthes (2023)and made modifications, thereby comparing different methods.
MethodGeneration Based Stepwise Heuristic Search Informal or Formal Human-authored Realistic ProofIBR (Qu et al., 2022)-✓✓Informal-GPT-f (Polu and Sutskever, 2020)✓✓-Formal✓DT-Solver (Wang et al., 2023b)✓✓✓Formal✓POETRY (Wang et al., 2024a)✓--Formal✓</p>
<p>Table 5 :
5
Yang et al. (2023c) referred toYang et al. (2023c)and made modifications, thereby comparing different methods.
MethodResearch FieldAcross Stages Human Interaction Multi-agent TaskInputExternal toolsAutoML-GPT (Zhang et al., 2023)Artification Intelligence---Automated Machine Learning Task-oriented Prompts-Chemcrow (Bran et al., 2024)Chemistry-✓-Chemical TaskTask Description✓DOLPHIN (Yuan et al., 2025)Any✓-✓Automated Scientific Research Idea✓MechAgents (Ni and Buehler, 2023)Physics--✓Mechanical Problem--Manning et al. (2024)Social Science✓-✓Simulating Human--</p>
<p>Table 6 :
6
Experiment Validation: we use attribute diagrams to compare different schemes, and the table design refers to Hypothesis Generation.
MethodAcross Stages Human Interaction TaskInputEvaluation MethodAI Scientist (Lu et al., 2024)✓-Full-text GenerationManuscript Template &amp; Experimental Results &amp; Hypothesis LLMdata-to-paper (Ifargan et al., 2024)✓✓Full-text GenerationExperimental Results &amp; Hypothesis-ScholaCite (Martin-Boyle et al., 2024)--Related Work Generation Title &amp; Abstract &amp; CitationCitation Graph MetricsSciLit (Gu and Hahnloser, 2023)✓-Citation GenerationKeywordsAutomatic MetricsGu and Hahnloser (2024)--Citation GenerationCitation Intent &amp; KeywordsHuman</p>
<p>Table 7 :
7
Manuscript Writing, we referred toLi and Ouyang (2022, 2024)and made modifications, thereby comparing different methods.
✓</p>
<dl>
<dt>Table 10 :</dt>
<dt>10</dt>
<dt>Tools for Research Paper Assistance</dt>
<dt>Knowledge Synthe-size ( §2.1)HypothesisFormulation ( §2)A survey of AI for Research</dt>
<dt>https://sdproc.org</dt>
<dd>Benchmarking large language models for research idea generation.CoRR, abs/2411.02429.MethodAcross Stages Human Interaction Paper Review Meta ReviewMulti-agent Trained Model Output Gamma-Trans (Muangkammuen et al., 2022)--✓ --✓ Peer-review Score MARG(D'Arcy et al., 2024a)--✓ -✓ -Peer-review Comments CycleResearcher(Weng et al., 2024)✓ -✓ --✓ Peer-review Comments &amp; Score PeerArg(Sukpanichnant et al., 2024)---✓ --Final Decision GLIMPSE(Darrin et al., 2024)-✓ -✓ --Summary of Peer-review
Litllm: A toolkit for scientific literature review. References Shubham Agarwal, Issam H Laradji, Laurent Charlin, Christopher Pal, 10.48550/ARXIV.2402.01788CoRR, abs/2402.017882024a</dd>
</dl>
<p>Shubham Agarwal, Gaurav Sahu, Abhay Puri, H Issam, Laradji, D J Krishnamurthy, Jason Dvijotham, Laurent Stanley, Christopher Charlin, Pal, arXiv:2412.15249Llms for literature review: Are we there yet?. 2024barXiv preprint</p>
<p>Microsoft Research AI4Science and Microsoft Azure Quantum. 2023. The impact of large language models on scientific discovery: a preliminary study using GPT-4. 10.48550/ARXIV.2311.07361CoRR, abs/2311.07361</p>
<p>Khalid H Malki, and 1 others. 2024. Reference hallucination score for medical artificial intelligence chatbots: development and usability study. Mohamad-Hani Fadi Aljamaan, Ibraheem Temsah, Ayman Altamimi, Amr Al-Eyadhy, Khalid Jamal, Alhasan, A Tamer, Mohamed Mesallam, Farahat, JMIR Medical Informatics. 121e54345</p>
<p>Automatic summarization of scientific articles: A survey. 10.1016/J.JKSUCI.2020.04.020J. King Saud Univ. Comput. Inf. Sci. Nouf Ibrahim Altmami and Mohamed El Bachir Menai3442022</p>
<p>aedfact: Scientific fact-checking made easier via semi-automatic discovery of relevant expert opinions. Enes Altuncu, Jason R C Nurse, Meryem Bagriacik, Sophie Kaleba, Haiyue Yuan, Lisa Bonheme, Shujun Li, 10.48550/ARXIV.2305.07796CoRR, abs/2305.077962023</p>
<p>Meta-designing quantum experiments with language models. Sören Arlt, Haonan Duan, Felix Li, Sang Michael Xie, Yuhuai Wu, Mario Krenn, 10.48550/ARXIV.2406.02470CoRR, abs/2406.024702024</p>
<p>Openscholar: Synthesizing scientific literature with retrieval-augmented lms. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D' Arcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, 10.48550/ARXIV.2411.14199CoRR, abs/2411.141992024</p>
<p>Generating fact checking explanations. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein, 10.18653/V1/2020.ACL-MAIN.656Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020OnlineAssociation for Computational Linguistics2020. July 5-10, 2020</p>
<p>Exploring the boundaries of reality: investigating the phenomenon of artificial intelligence hallucination in scientific writing through chatgpt references. Sandeep Sai Anirudh Athaluri, Varma Manthena, Krishna Manoj Vsr, Vineel Kesapragada, Tirth Yarlagadda, Rama Dave, Siri Tulasi, Duddumpudi, 10.48550/ARXIV.2404.07738CoRR, abs/2404.07738Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2023. 202415Researchagent: Iterative research idea generation over scientific literature with large language models</p>
<p>Scientific paper recommendation: A survey. Xiaomei Bai, Mengyang Wang, Ivan Lee, Zhuo Yang, Xiangjie Kong, Feng Xia, 10.1109/ACCESS.2018.2890388IEEE Access. 72019</p>
<p>Comprehensive exploration of synthetic data generation: A survey. André Bauer, Simon Trapp, Michael Stenger, Robert Leppich, Samuel Kounev, Mark Leznik, Kyle Chard, Ian Foster, arXiv:2401.025242024arXiv preprint</p>
<p>Unsupervised pretraining for fact verification by language model distillation. Adrián Bazaga, Pietro Lio, Gos Micklem, The Twelfth International Conference on Learning Representations, ICLR 2024. Bela Gipp, Stefan Langer, Corinna Breitinger, Vienna, Austria2024. May 7-11, 2024. 201617Open-Review.net. Joeran BeelPaper recommender systems: a literature survey</p>
<p>Abigail Polter, and 1 others. 2007. The ups and downs of peer review. Edlira Dale J Benos, Jose M Bashari, Amit Chaves, Niren Gaggar, Martin Kapoor, Robert Lafrance, David Mans, Sara Mayhew, Mcgowan, Advances in physiology education. 312</p>
<p>How to research. Loraine Blaxter, Christina Hughes, Malcolm Tight, 2010McGraw-Hill EducationUK)</p>
<p>What is the future of human-generated systematic literature reviews in an age of artificial intelligence? Management Review Quarterly. Joern Block, Andreas Kuckertz, 2024</p>
<p>SUPER: evaluating agents on setting up and executing tasks from research repositories. Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, Tushar Khot, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024. 2024. November 12-16, 2024</p>
<p>Autonomous chemical research with large language models. A Daniil, Robert Boiko, Ben Macknight, Gabe Kline, Gomes, 10.1038/S41586-023-06792-0Nat. 62479922023</p>
<p>Artificial intelligence for literature reviews: opportunities and challenges. Francisco Bolaños, Angelo A Salatino, 10.1007/S10462-024-10902-3Artif. Intell. Rev. 5792592024Francesco Osborne, and Enrico Motta</p>
<p>Anastasia Visheratina, and Xin Xie. 2023. An interdisciplinary outlook on large language models for scientific research. James Boyko, Joseph Cohen, Nathan Fox, Maria Han Veiga, I-Hsiu Jennifer, Jing Li, Bernardo Liu, Andreas H Modenesi, Kenneth N Rauch, Soumi Reid, Tribedi, 10.48550/ARXIV.2311.04929CoRR, abs/2311.04929</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, 10.1038/S42256-024-00832-8Nat. Mac. Intell. 652024</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Advances in neural information processing systems. 202033Amanda Askell, and 1 others</p>
<p>Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning. Markus J Buehler, 10.1088/2632-2153/AD7228Mach. Learn. Sci. Technol. 53350832024</p>
<p>Qian Liu, and 4 others. 2024a. Spider2-v: How far are multimodal agents from automating data science and engineering workflows?. Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Wenjing Hu, Yuchen Mao, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida I Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024. NeurIPS; Vancouver, BC, Canada2024. December 10 -15, 2024</p>
<p>Can large language models detect misinformation in scientific news reporting?. Yupeng Cao, Muralidharan Aishwarya, Elyon Nair, Nastaran Eyimife, K P Jamalipour Soofi, John R Subbalakshmi, I I Wullert, Chumki Basu, David Shallcross, 10.48550/ARXIV.2402.14268CoRR, abs/2402.142682024b</p>
<p>Automated focused feedback generation for scientific writing assistance. Eric Chamoun, Michael Sejr Schlichtkrull, Andreas Vlachos, 10.18653/V1/2024.FINDINGS-ACL.580Findings of the Association for Computational Linguistics, ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024. 2024and virtual meeting, August 11-16</p>
<p>Chatgpt and other artificial intelligence applications speed up scientific writing. Tzeng-Ji Chen, Journal of the Chinese Medical Association. 8642023</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, The Thirteenth International Conference on Learning Representations, ICLR 2025. Singapore2025. April 24-28, 2025OpenReview.net</p>
<p>React: A review comment dataset for actionability (and more). Gautam Choudhary, Natwar Modani, Nitish Maurya, 10.1007/978-3-030-91560-5_24Web Information Systems Engineering -WISE 2021 -22nd International Conference on Web Information Systems Engineering, WISE 2021. Lecture Notes in Computer Science. Melbourne, VIC, Australia2021. October 26-29, 202113081Proceedings, Part II</p>
<p>Becoming writing, becoming writers. Julia Springer, Colyar, Qualitative Inquiry. 1522009</p>
<p>Relevai-reviewer: A benchmark on AI reviewers for survey paper relevance. Henrique Paulo, Quang Phuoc Couto, Nageeta Ho, Benedictus Kumari, Thanh Kent Rachmat, Gia Hieu, Ihsan Khuong, Lisheng Ullah, Sun-Hosoya, 10.48550/ARXIV.2406.10294CoRR, abs/2406.102942024</p>
<p>Helping our own: Text massaging for computational linguistics as a new shared task. Robert Dale, Adam Kilgarriff, Proceedings of the 6th International Natural Language Generation Conference. the 6th International Natural Language Generation Conference2010</p>
<p>Claimver: Explainable claim-level verification and evidence attribution of text through knowledge graphs. Preetam Prabhu, Srikar Dammu, Himanshu Naidu, Mouly Dewan, Youngmin Kim, Tanya Roosta, Aman Chadha, Chirag Shah, Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024. November 12-16, 2024</p>
<p>Corpusstudio: Surfacing emergent patterns in A corpus of prior work while writing. Hai Dang, Chelse Swoopes, Daniel Buschek, Elena L Glassman, 10.1145/3706598.37139741211:1-1211:19Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, CHI 2025. the 2025 CHI Conference on Human Factors in Computing Systems, CHI 2025ACM2025. 26 April 2025-1 May 2025</p>
<p>MARG: multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, 10.48550/ARXIV.2401.04259CoRR, abs/2401.042592024a</p>
<p>ARIES: A corpus of scientific paper edits made in response to peer reviews. D' Mike, Alexis Arcy, Erin Ross, Bailey Bransom, Jonathan Kuehl, Tom Bragg, Doug Hope, Downey, 10.18653/V1/2024.ACL-LONG.377Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024b. August 11-16, 20241ACL 2024</p>
<p>GLIMPSE: pragmatically informative multi-document summarization for scholarly reviews. Maxime Darrin, Ines Arous, Pablo Piantanida, Jackie Chi, Kit Cheung, 10.18653/V1/2024.ACL-LONG.688Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, Thailand2024. August 11-16, 20241ACL 2024. Association for Computational Linguistics</p>
<p>The state of human-centered NLP technology for fact-checking. Anubrata Das, Houjiang Liu, Venelin Kovatchev, Matthew Lease, 10.1016/J.IPM.2022.103219Inf. Process. Manag. 6021032192023</p>
<p>Automated evaluation of scientific writing: Aesw shared task proposal. Vidas Daudaravicius, Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications. the Tenth Workshop on Innovative Use of NLP for Building Educational Applications2015</p>
<p>Artificial intelligence to automate the systematic review of scientific literature. José De La Torre-López, Aurora Ramírez, José Raúl Romero, 10.1007/S00607-023-01181-XComputing. 105102023</p>
<p>Deepseek-Ai , Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, 10.48550/ARXIV.2412.19437CoRR, abs/2412.19437Fangyun Lin, Fucong Dai, and 81 others. 2024. Deepseek-v3 technical report. </p>
<p>Claim verification in the age of large language models: A survey. Alphaeus Dmonte, Roland Oruche, Marcos Zampieri, Prasad Calyam, Isabelle Augenstein, 10.48550/ARXIV.2408.14317CoRR, abs/2408.143172024</p>
<p>Human-in-the-loop AI reviewing: Feasibility, opportunities, and risks. Iddo Drori, Dov Te, ' Eni, J. Assoc. Inf. Syst. 25172024</p>
<p>Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision. Wanyu Du, Myung Zae, Vipul Kim, Dhruv Raheja, Dongyeop Kumar, Kang ; Dhruv, Zae Kumar, Myung Kim, Melissa Lopez, Dongyeop Kang, 10.18653/V1/2022.ACL-LONG.250CoRR, abs/2204.03685Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022a. 2022b. May 22-27, 2022ACL 2022</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, 10.48550/ARXIV.2407.21783CoRR, abs/2407.21783202482</p>
<p>Nlpeer: A unified resource for the computational study of peer review. Nils Dycke, Ilia Kuznetsov, Iryna Gurevych, 10.18653/V1/2023.ACL-LONG.277Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Transforming science with large language models: A survey on ai-assisted scientific discovery, experimentation, content generation, and evaluation. Steffen Eger, Yong Cao, D' Jennifer, Andreas Souza, Christian Geiger, Stephanie Greisinger, Yufang Gross, Brigitte Hou, Anne Krenn, Yizhi Lauscher, Chenghua Li, Nafise Sadat Lin, Wei Moosavi, Tristan Zhao, Miller, 10.48550/ARXIV.2502.05151CoRR, abs/2502.051512025</p>
<p>Automated justification production for claim veracity in fact checking: A survey on architectures and approaches. Islam Eldifrawi, Shengrui Wang, Amine Trabelsi, 10.18653/V1/2024.ACL-LONG.361Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 20241ACL 2024</p>
<p>Customization in a unified framework for summarizing medical literature. Artificial intelligence in medicine. Noemie Elhadad, M-Y Kan, Judith L Klavans, Kathleen R Mckeown, 200533</p>
<p>The measurement of scientific, technological and innovation activities Oslo manual 2018 guidelines for collecting, reporting and using data on innovation. Eurostat, 2018OECD publishing</p>
<p>Friend or foe? exploring the implications of large language models on the science system. Benedikt Fecher, Marcel Hebing, Melissa Laufer, Jörg Pohle, Fabian Sofsky, 10.48550/ARXIV.2306.09928CoRR, abs/2306.099282023</p>
<p>Cocoa: Co-planning and co-execution with AI agents. K J Kevin Feng, Kevin Pu, Matt Latzke, Tal August, Pao Siangliulue, Jonathan Bragg, Daniel S Weld, Amy X Zhang, Joseph Chee, Chang , 10.48550/ARXIV.2412.10999CoRR, abs/2412.109992024</p>
<p>CASIMIR: A corpus of scientific articles enhanced with multiple author-integrated revisions. Léane Isabelle, Jourdan , Florian Boudin, Nicolas Hernandez, Richard Dufour, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024Torino, ItalyELRA and ICCL2024. 20-25 May, 2024</p>
<p>Intentcontrollable citation text generation. Shing-Yun Jung, Ting-Han Lin, Chia-Hung Liao, Shyan-Ming Yuan, Chuen-Tsai Sun, Mathematics. 101017632022</p>
<p>The use of artificial intelligence in writing scientific review articles. Melissa A Kacena, Lilian I Plotkin, Jill C Fehrenbacher, Current Osteoporosis Reports. 2212024</p>
<p>A dataset of peer reviews (peerread): Collection, insights and NLP applications. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Saldyt, Anil Murthy, 10.18653/V1/N18-1149Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018. Long Papers. Net Openreview, Waleed Kang, Bhavana Ammar, Madeleine Dalvi, Sebastian Van Zuylen, Eduard H Kohlmeier, Roy Hovy, Schwartz, the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018Vienna, Austria; Louisiana, USAAssociation for Computational Linguistics2024. July 21-27, 2024. 2018. June 1-6, 20181Forty-first International Conference on Machine Learning, ICML 2024</p>
<p>Document-level definition detection in scholarly documents: Existing models, error analyses, and future directions. Dongyeop Kang, Andrew Head, Risham Sidhu, Kyle Lo, Daniel S Weld, Marti A Hearst, 10.18653/V1/2020.SDP-1.22Proceedings of the First Workshop on Scholarly Document Processing. the First Workshop on Scholarly Document ProcessingAssociation for Computational Linguistics2020. November 19. 2020SDP@EMNLP 2020</p>
<p>From who you know to what you read: Augmenting scientific recommendations with implicit social networks. B Hyeonsu, Rafal Kang, Andrew Kocielnik, Jiangjiang Head, Matt Yang, Aniket Latzke, Daniel S Kittur, Doug Weld, Jonathan Downey, Bragg, 10.1145/3544548.3581371Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023. . Acm Hyeonsu, B Kang, Nouran Soliman, Matt Latzke, Joseph Chee Chang, Jonathan Bragg, the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023New Orleans, LA, USA; Hamburg, GermanyACM2022. 29 April 2022 -5 May 2022. 2023. April 23-28, 202330220CHI '22: CHI Conference on Human Factors in Computing Systems</p>
<p>A hybrid approach for paper recommendation. Ying Kang, Aiqin Hou, Zimin Zhao, Daguang Gan, IEICE TRANSACTIONS on Information and Systems. 10482021</p>
<p>MAGIC: multiargument generation with self-refinement for domain generalization in automatic fact-checking. Wei-Yu Kao, An-Zi Yen, 10.18653/V1/2023.FINDINGS-ACL.418Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024Torino, Italy; Toronto, CanadaAssociation for Computational Linguistics2024. 20-25 May, 2024. 2023. July 9-14, 2023Findings of the Association for Computational Linguistics: ACL 2023</p>
<p>Knowledge navigator: Llm-guided browsing framework for exploratory search in scientific literature. Uri Katz, Mosh Levy, Yoav Goldberg, Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024. November 12-16, 2024</p>
<p>Factkg: Fact verification via reasoning on knowledge graphs. Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, Edward Choi, 10.18653/V1/2023.ACL-LONG.895Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Improving iterative text revision by learning where to edit from other revision tasks. Myung Zae, Wanyu Kim, Vipul Du, Raheja, 10.18653/V1/2022.EMNLP-MAIN.678Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022. the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDhruv Kumar, and Dongyeop Kang. 2022. December 7-11, 2022</p>
<p>Kayvan Kousha and Mike Thelwall. 2024. Artificial intelligence to support publishing and peer review: A summary and review. Ryan Koo, Anna Martin, Linghe Wang, Dongyeop , 10.1002/LEAP.1570CoRR, abs/2304.00121Learn. Publ. 3712023Decoding the end-to-end writing trajectory in scholarly manuscripts</p>
<p>Predicting the future of AI with AI: highquality link prediction in an exponentially growing knowledge network. Mario Krenn, Lorenzo Buffoni, Bruno C Coutinho, Sagi Eppel, Jacob Gates Foster, Andrew Gritsevskiy, Harlin Lee, Yichao Lu, João P Moutinho, Nima Sanjabi, Rishi Sonthalia, Ngoc Mai Tran, Francisco Valente, Yangxinyu Xie, Rose Yu, Michael Kopp, 10.48550/ARXIV.2210.00881CoRR, abs/2210.008812022</p>
<p>Scientific paper recommendation systems: a literature review of recent publications. Christin Katharina, Kreutz , Ralf Schenkel, 10.1007/S00799-022-00339-WInt. J. Digit. Libr. 2342022</p>
<p>Proofver: Natural logic theorem proving for fact verification. Amrith Krishna, Sebastian Riedel, Andreas Vlachos, 10.1162/TACL_A_00503Trans. Assoc. Comput. Linguistics. 102022</p>
<p>Adithya Kulkarni, Fatimah Alotaibi, Xinyue Zeng, Longfeng Wu, Tong Zeng, Barry Menglong Yao, Minqian Liu, Shuaicheng Zhang, Lifu Huang, Dawei Zhou, arXiv:2505.04651Scientific hypothesis generation and validation: Methods, datasets, and future directions. 2025arXiv preprint</p>
<p>When reviewers lock horns: Finding disagreements in scientific peer reviews. Sandeep Kumar, Tirthankar Ghosal, Asif Ekbal, 10.18653/V1/2023.EMNLP-MAIN.1038Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Can large language models unlock novel scientific research ideas?. Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal, 10.48550/ARXIV.2409.06185CoRR, abs/2409.061852024</p>
<p>Thamar Solorio, and 5 others. 2024. What can natural language processing. Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg, Tom Hope, Dirk Hovy, Jonathan K Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, Sheng Lu, Margot Mausam, Aurélie Mieskes, Danish Névéol, Lizhen Pruthi, Roy Qu, Noah A Schwartz, Smith, 10.48550/ARXIV.2405.06563CoRR, abs/2405.06563</p>
<p>Instruct large language models to generate scientific literature survey step by step. Yuxuan Lai, Yupeng Wu, Yidan Wang, Wenpeng Hu, Chen Zheng, 10.1007/978-981-97-9443-0_43Natural Language Processing and Chinese Computing -13th National CCF Conference, NLPCC 2024. Lecture Notes in Computer Science. Hangzhou, ChinaSpringer2024. November 1-3, 202415363Proceedings, Part V</p>
<p>Lab-bench: Measuring capabilities of language models for biology research. Guillaume Lample, Timothée Lacroix, Marie-Anne Lachaux, Aurélien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, Xavier Martinet, ; Jon, M Laurent, Joseph D Janizek, Michael Ruzo, Michaela M Hinks, Michael J Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D White, Samuel G Rodriques, 10.48550/ARXIV.2407.10362CoRR, abs/2407.10362Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022. 2024Hypertree proof search for neural theorem proving</p>
<p>Can an artificial intelligence chatbot be the author of a scholarly article. Ju Yoen, Lee , Journal. 202023</p>
<p>Paperweaver: Enriching topical paper alerts by contextualizing recommended papers with user-collected papers. Yoonjoo Lee, B Hyeonsu, Matt Kang, Juho Latzke, Jonathan Kim, Joseph Chee Bragg, Pao Chang, Siangliulue, 10.1145/3613904.3642196Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024. the CHI Conference on Human Factors in Computing Systems, CHI 2024Honolulu, HI, USAACM2024. May 11-16, 20241919</p>
<p>Text revision by on-the-fly representation optimization. Jingjing Li, Zichao Li, Tao Ge, Irwin King, Michael R Lyu, 10.1609/AAAI.V36I10.21343Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence. AAAI Press2022. February 22 -March 1, 20222022The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</p>
<p>. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming</p>
<p>Chain of ideas: Revolutionizing research via novel idea development with LLM agents. Yifei Jiang, Ronghao Xin, Deli Dang, Yu Zhao, Tian Rong, Lidong Feng, Bing, 10.48550/ARXIV.2410.13185CoRR, abs/2410.131852024a</p>
<p>Summarizing multiple documents with conversational structure for meta-review generation. Miao Li, Eduard H Hovy, Jey Han Lau, 10.18653/V1/2023.FINDINGS-EMNLP.472Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>A sentiment consolidation framework for meta-review generation. Miao Li, Jey Han Lau, Eduard H Hovy, 10.18653/V1/2024.ACL-LONG.547Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024b. August 11-16, 20241ACL 2024</p>
<p>Learning to generate research idea with dynamic control. Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du, 10.48550/ARXIV.2412.14626CoRR, abs/2412.146262024c</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, 10.48550/ARXIV.2408.14033CoRR, abs/2408.140332024d</p>
<p>Academic paper recommendation method combining heterogeneous network and temporal attributes. Weisheng Li, Chao Chang, Chaobo He, Zhengyang Wu, Jiongsheng Guo, Bo Peng, 10.1007/978-981-16-2540-4_33Computer Supported Cooperative Work and Social Computing -15th CCF Conference. Revised Selected Papers. Shenzhen, China2020. 2020. November 7-9, 20201330</p>
<p>Automatic related work generation: A meta study. Xiangci Springer, Jessica Li, Ouyang, CoRR, abs/2201.018802022</p>
<p>Related work and citation text generation: A survey. Xiangci Li, Jessica Ouyang, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024. 2024. November 12-16, 2024</p>
<p>A survey on deep learning for theorem proving. Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, Xujie Si, 10.48550/ARXIV.2404.09939CoRR, abs/2404.099392024e</p>
<p>A review on personalized academic paper recommendation. Zhi Li, Xiaozhu Zou, 10.5539/CIS.V12N1P33Comput. Inf. Sci. 1212019</p>
<p>Monitoring ai-modified content at scale: A case study on the impact of chatgpt on AI conference peer reviews. Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, Daniel A Mcfarland, James Y Zou, Yaohui Liang, Zhengxuan Zhang, Haley Wu, Wenlong Lepp, Xuandong Ji, Hancheng Zhao, Sheng Cao, Siyu Liu, Zhi He, Diyi Huang, Christopher Yang, Christopher D Potts, James Y Manning, Zou, 10.48550/ARXIV.2404.01268CoRR, abs/2404.01268Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024a. July 21-27, 2024. 2024bMapping the increasing use of llms in scientific papers</p>
<p>Can large language models provide useful feedback on research papers? A large-scale empirical analysis. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Scott Daniel, Yian Smith, Daniel A Yin, James Mcfarland, Zou, 10.48550/ARXIV.2310.01783CoRR, abs/2310.017832023</p>
<p>Zhehui Liao, Maria Antoniak, Inyoung Cheong, Evie Yu-Yen, Ai-Heng Cheng, Kyle Lee, Joseph Chee Lo, Amy X Chang, Zhang, 10.48550/ARXIV.2411.05025CoRR, abs/2411.05025Llms as research tools: A large scale survey of researchers' usage and perceptions. 2024</p>
<p>Lean-star: Learning to interleave thinking and proving. Haohan Lin, Zhiqing Sun, Yiming Yang, Sean Welleck, 10.48550/ARXIV.2407.10040CoRR, abs/2407.100402024</p>
<p>Automated scholarly paper review: Concepts, technologies, and challenges. Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, Xiaodong Shi, 10.1016/J.INFFUS.2023.101830Inf. Fusion. 981018302023a</p>
<p>MOPRD: A multidisciplinary open peer review dataset. Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, Xiaodong Shi, 10.1007/S00521-023-08891-5Neural Comput. Appl. 35342023b</p>
<p>Techniques for supercharging academic writing with generative ai. Zhicheng Lin, Nature Biomedical Engineering. 2024</p>
<p>FIMO: A challenge formal dataset for automated theorem proving. Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan, Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, Ming Zhang, Qun Liu, 10.48550/ARXIV.2309.04295CoRR, abs/2309.042952023a</p>
<p>Literature meets data: A synergistic approach to hypothesis generation. Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan, 10.48550/ARXIV.2410.17309CoRR, abs/2410.173092024a</p>
<p>Training socially aligned language models in simulated human society. Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, Soroush Vosoughi, 10.48550/ARXIV.2305.16960CoRR, abs/2305.169602023b</p>
<p>Reviewergpt? an exploratory study on using large language models for paper reviewing. Ryan Liu, Nihar B Shah, 10.48550/ARXIV.2306.00622CoRR, abs/2306.006222023</p>
<p>Generating a structured summary of numerous academic papers: Dataset and method. Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao ; Ruosong Yang, Zhiyuan Wen, 10.48550/ARXIV.2402.01881Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022. Siyi Org, Chen Liu, Yong Gao, Li, the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022Vienna, Austria; Vienna, Austria2024b. May 7-11, 2024. 2022. July 2022. 2024cOpen-Review.net. Shuaiqi LiuThe Twelfth International Conference on Learning Representations, ICLR 2024. Large language model agent for hyper-parameter optimization. CoRR, abs/2402.01881</p>
<p>AIGS: generating science from ai-powered automated falsification. Zijun Liu, Kaiming Liu, Yiqi Zhu, Xuanyu Lei, Zonghan Yang, Zhenhe Zhang, Peng Li, Yang Liu, 10.48550/ARXIV.2411.11910CoRR, abs/2411.119102024d</p>
<p>Rob Evans, and 36 others. 2023. The semantic reader project: Augmenting scholarly documents through ai-powered interactive reading interfaces. Kyle Lo, Joseph Chee Chang, Andrew Head, Jonathan Bragg, Amy X Zhang, Cassidy Trier, Chloe Anastasiades, Tal August, Russell Authur, Danielle Bragg, Erin Bransom, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Yen-Sung Chen, Evie Yu-Yen, Yvonne Cheng, Doug Chou, Downey, 10.48550/ARXIV.2303.14334CoRR, abs/2303.14334</p>
<p>Renze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yuxuan Sun, Yusen Zhang, Jihyun Janice Ahn, Hongchao Fang, Zhuoyang Zou, Wenchao Ma, Xi Li, Kai Zhang, 10.48550/ARXIV.2410.22394CoRR, abs/2410.22394Congying Xia, Lifu Huang, and Wenpeng Yin. 2024. AAAR-1.0: assessing ai's potential to assist research. </p>
<p>The AI scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 10.48550/ARXIV.2408.06292CoRR, abs/2408.062922024</p>
<p>SCITAB: A challenging benchmark for compositional reasoning and claim verification on scientific tables. Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, Min-Yen Kan, 10.18653/V1/2023.EMNLP-MAIN.483Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, Xinya Du, arXiv:2501.04306Llm4sr: A survey on large language models for scientific research. 2025arXiv preprint</p>
<p>LLM and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery. Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B Tenenbaum, Daniela Rus, Chuang Gan, Wojciech Matusik ; Prakhar, Skyler Gupta, Luyu Hallinan, Sarah Gao, Uri Wiegreffe, Nouha Alon, Shrimai Dziri, Yiming Prabhumoye, Shashank Yang, Gupta, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. Prasad Bodhisattwa, Katherine Majumder, Sean Hermann, Amir Welleck, Peter Yazdanbakhsh, Clark, Vienna, Austria; NeurIPS; New Orleans, LA, USA2024. July 21-27, 2024. 2023. 2023. December 10 -16, 2023Open-Review.net. Aman MadaanFortyfirst International Conference on Machine Learning, ICML 2024</p>
<p>Position: Data-driven discovery with large generative models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Sanchaita Agarwal, Ashish Hazra, Peter Sabharwal, Clark, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024a. July 21-27, 2024OpenReview.net</p>
<p>Discoverybench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, 10.48550/ARXIV.2407.01725CoRR, abs/2407.017252024b</p>
<p>Automated social science: Language models as scientist and subjects. Kehang Benjamin S Manning, John J Zhu, Horton, 2024National Bureau of Economic ResearchTechnical report</p>
<p>Shallow synthesis of knowledge in gpt-generated texts: A case study in automatic related work composition. Anna Martin-Boyle, Aahan Tyagi, Marti A Hearst, Dongyeop Kang, 10.48550/ARXIV.2402.12255CoRR, abs/2402.122552024</p>
<p>Bringing structure into summaries: a faceted summarization dataset for long scientific documents. Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, Daqing He, 10.18653/V1/2021.ACL-SHORT.137Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. Short Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021Association for Computational Linguistics2021. August 1-6, 20212Virtual Event</p>
<p>Artificial intelligence and illusions of understanding in scientific research. Lisa Messeri, Crockett, Nature. 62780022024</p>
<p>Scientists' perspectives on the potential for generative AI in their fields. Meredith Ringel, Morris , 10.48550/ARXIV.2304.01420CoRR, abs/2304.014202023</p>
<p>From individual to society: A survey on social simulation driven by large language model-based agents. Xinyi Mou, Xuanwen Ding, Qi He, Liang Wang, Jingcong Liang, Xinnong Zhang, Libo Sun, Jiayu Lin, Jie Zhou, Xuanjing Huang, Zhongyu Wei ; Panitan Muangkammuen, Fumiyo Fukumoto, Jiyi Li, Yoshimi Suzuki, 10.18653/V1/2022.FINDINGS-EMNLP.164CoRR, abs/2412.03563Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2024. 2022. December 7-11, 2022Exploiting labeled and unlabeled data via transformer fine-tuning for peerreview score prediction</p>
<p>Mechagents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge. Arief Purnama, Muharram , Ayu Purwarianti, 10.48550/ARXIV.2311.08166CoRR, abs/2311.08166Bo Ni and Markus J. Buehler2024. 2023Enhancing natural language inference performance with knowledge graph for COVID-19 automated fact-checking in indonesian language</p>
<p>Matpilot: an llm-enabled AI materials scientist under the framework of human-machine collaboration. Ziqi Ni, Yahao Li, Kaijia Hu, Kunyuan Han, Ming Xu, Xingyu Chen, Fengqi Liu, Yicong Ye, Shuxin Bai, 10.48550/ARXIV.2411.08063CoRR, abs/2411.080632024</p>
<p>Acceleron: A tool to accelerate research ideation. Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff, 10.48550/ARXIV.2403.04382CoRR, abs/2403.043822024</p>
<p>Toward related work generation with structure and novelty statement. Kazuya Nishimura, Kuniaki Saito, Tosho Hirasawa, Yoshitaka Ushiku, Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024). the Fourth Workshop on Scholarly Document Processing (SDP 2024)2024</p>
<p>Unveiling the sentinels: Assessing AI performance in cybersecurity peer review. Liang Niu, Nian Xue, Christina Pöpper, 10.48550/ARXIV.2309.05457CoRR, abs/2309.054572023</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Relatedly: Scaffolding literature reviews with existing related work sections. Srishti Palani, Aakanksha Naik, Doug Downey, Amy X Zhang, Jonathan Bragg, Joseph Chee, Chang , 10.1145/3544548.3580841Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023. the 2023 CHI Conference on Human Factors in Computing Systems, CHI 2023Hamburg, GermanyACM2023. April 23-28, 202374220</p>
<p>Investigating zero-and few-shot generalization in fact verification. Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, Preslav Nakov, 10.18653/V1/2023.IJCNLP-MAIN.34Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterToronto, Canada; BaliAssociation for Computational Linguistics2023a. July 9-14, 2023. 2023b. November 1 -4, 20231Nusa Dua</p>
<p>Can AI solve the peer review crisis? A large scale experiment on llm's performance and biases in evaluating economics papers. Pat Pataranutaporn, Nattavudh Powdthavee, Pattie Maes, 10.48550/ARXIV.2502.00070CoRR, abs/2502.000702025</p>
<p>The importance of applying computational creativity to scientific and mathematical domains. Alison Pease, Simon Colton, Chris Warburton, 10th International Conference on Computational Creativity, ICCC 2019. Irina Preda, Daniel Arnold, Daniel Winterstein, Mike Cook, Association for Computational Creativity2019Athanasios Nathanail</p>
<p>Arzigo: A recommendation system for scientific articles. Iratxe Pinedo, Mikel Larrañaga, Ana Arruarte, 10.1016/J.IS.2024.102367Inf. Syst. 1221023672024</p>
<p>A publishing infrastructure for artificial intelligence (ai)-assisted academic authoring. Milton Pividori, Casey S Greene, 10.1093/JAMIA/OCAE139J. Am. Medical Informatics Assoc. 3192024</p>
<p>The ethics of scientific recommender systems. Andrea Polonioli, 10.1007/S11192-020-03766-1Scientometrics. 12622021</p>
<p>Generative language modeling for automated theorem proving. Stanislas Polu, Ilya Sutskever, CoRR, abs/2009.033932020</p>
<p>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. Kevin Pu, K J Kevin Feng, Tovi Grossman, Tom Hope, Bhavana Dalvi Mishra, Matt Latzke, Jonathan Bragg, Joseph Chee Chang, Pao Siangliulue, 10.48550/ARXIV.2410.04025CoRR, abs/2410.040252024</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, 10.48550/ARXIV.2311.05965CoRR, abs/2311.059652023</p>
<p>Interpretable proof generation via iterative backward reasoning. Hanhao Qu, Yu Cao, Jun Gao, Liang Ding, Ruifeng Xu, 10.18653/V1/2022.NAACL-MAIN.216Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022Seattle, WA, United StatesAssociation for Computational Linguistics2022. July 10-15, 2022</p>
<p>Iterative hypothesis generation for scientific discovery with monte carlo nash equilibrium self-refining trees. Gollam Rabby, Diyana Muhammed, Prasenjit Mitra, Sören Auer, arXiv:2503.193092025arXiv preprint</p>
<p>R Dragomir, Pradeep Radev, Muthukrishnan, Amjad Vahed Qazvinian, Abu-Jbara, 10.1007/S10579-012-9211-2The ACL anthology network corpus. 201347</p>
<p>GPT4 is slightly helpful for peer-review assistance: A pilot study. Zachary Robertson, 10.48550/ARXIV.2307.05492CoRR, abs/2307.054922023</p>
<p>Bio-sieve: Exploring instruction tuning large language models for systematic review automation. Ambrose Robinson, William Thorne, Ben P Wu, Abdullah Pandor, Munira Essat, Mark Stevenson, Xingyi Song, 10.48550/ARXIV.2308.06610CoRR, abs/2308.066102023</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, and 1 others. 2024625</p>
<p>Liveideabench: Evaluating llms' scientific creativity and idea generation with minimal context. Kai Ruan, Xuan Wang, Jixiang Hong, Hao Sun, 10.48550/ARXIV.2412.17596CoRR, abs/2412.175962024a</p>
<p>Qun Fang, Hanyu Gao, and 1 others. 2024b. An automatic end-to-end chemical synthesis development platform powered by large language models. Yixiang Ruan, Chenyin Lu, Ning Xu, Yuchen He, Yixin Chen, Jian Zhang, Jun Xuan, Jianzhang Pan, 10.1038/s41467-024-54457-xNature communications. 15110160</p>
<p>Can artificial intelligence help for scientific writing?. Michele Salvagno, Fabio Silvio Taccone, Alberto Giovanni Gerli, Critical care. 271752023</p>
<p>Prompting llms to compose meta-review drafts from peer-review narratives of scholarly manuscripts. Shubhra Kanti, Karmaker Santu, Sanjeev Kumar Sinha, Naman Bansal, Alex Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Guttikonda, Mousumi Akter, Matthew Freestone, Matthew C WilliamsJr, 10.48550/ARXIV.2402.15589CoRR, abs/2402.155892024</p>
<p>Evidence-based fact-checking of health-related claims. Mourad Sarrouti, Asma Ben Abacha, Yassine Mrabet, Dina Demner-Fushman, 10.18653/V1/2021.FINDINGS-EMNLP.297Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana. Association for Computational Linguistics2021. 16-20 November, 2021</p>
<p>A critical examination of the ethics of ai-mediated peer review. Rylan Schaeffer, Brando Miranda, Sanmi Koyejo, Laurie A Schintler, Connie L Mcneely, James Witte, 10.48550/ARXIV.2309.12356arXiv:2503.18102Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023. New Orleans, LA, USA2023. December 10 -16, 2023. 2023arXiv preprintSamuel Schmidgall and Michael Moor. 2025. Agentrxiv: Towards collaborative autonomous research</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>Emerging approaches in literaturebased discovery: techniques and performance review. Yakub Sebastian, Eu-Gene Siew, Sylvester O Orimaye, 10.1017/S0269888917000042The Knowledge Engineering Review. 32e122017</p>
<p>The quality assist: A technology-assisted peer review based on citation functions to predict the paper quality. Basuki Setio, Masatoshi Tsuchiya, 10.1109/ACCESS.2022.3225871IEEE Access. 102022</p>
<p>Insights into relevant knowledge extraction techniques: a comprehensive review. Abdul Shahid, Muhammad Tanvir Afzal, Moloud Abdar, Mohammad Ehsan Basiri, Xujuan Zhou, Neil Y Yen, Jia-Wei Chang, The Journal of Supercomputing. 762020</p>
<p>Assisting in writing wikipedia-like articles from scratch with large language models. Yijia Shao, Yucheng Jiang, Theodore A Kanell, Peter Xu, Omar Khattab, Monica S Lam, 10.18653/V1/2024.NAACL-LONG.347Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024. June 16-21, 20241NAACL 2024</p>
<p>An anatomization of research paper recommender system: Overview, approaches and challenges. Ritu Sharma, Dinesh Gopalani, Yogesh Kumar, Meena , 10.1016/J.ENGAPPAI.2022.105641Eng. Appl. Artif. Intell. 1181056412023</p>
<p>Mred: A meta-review dataset for structure-controllable text generation. Chenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing, Yang You, Luo Si, 10.18653/V1/2022.FINDINGS-ACL.198Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 2022</p>
<p>Taskbench: Benchmarking large language models for task automation. Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, Yueting Zhuang, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2024. 2024. 2024. December 10 -15, 2024</p>
<p>Towards a unified framework for reference retrieval and related work generation. Zhengliang Shi, Shen Gao, Zhen Zhang, Xiuying Chen, Zhumin Chen, Pengjie Ren, Zhaochun Ren, 10.18653/V1/2023.FINDINGS-EMNLP.385Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Can llms generate novel research ideas? A large-scale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, 10.48550/ARXIV.2409.04109CoRR, abs/2409.041092024</p>
<p>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark. Zachary S Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan, 10.48550/ARXIV.2409.11363CoRR, abs/2409.113632024</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. D Michael, Sam Skarlinski, Jon M Cox, James D Laurent, Michaela M Braza, Michael J Hinks, Manvitha Hammerling, Ponnapati, G Samuel, Andrew D Rodriques, White, 10.48550/ARXIV.2409.13740CoRR, abs/2409.137402024</p>
<p>Towards large language models as copilots for theorem proving in lean. Peiyang Song, Kaiyu Yang, Anima Anandkumar, 10.48550/ARXIV.2404.12534CoRR, abs/2404.125342024</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, arXiv:2504.01848Paperbench: Evaluating ai's ability to replicate ai research. 1 others. 2025arXiv preprint</p>
<p>An academic recommender system on large citation data based on clustering, graph modeling and deep learning. Vaios Stergiopoulos, Michael Vassilakopoulos, Eleni Tousidou, Antonio Corral, 10.1007/S10115-024-02094-7Knowl. Inf. Syst. 6682024</p>
<p>Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, 10.48550/ARXIV.2410.09403CoRR, abs/2410.094032024</p>
<p>Peerarg: Argumentative peer review with llms. Purin Sukpanichnant, Anna Rapberger, Francesca Toni, 10.48550/ARXIV.2409.16813CoRR, abs/2409.168132024</p>
<p>Metawriter: Exploring the potential and perils of AI writing support in scientific peer review. Lu Sun, Stone Tao, Junjie Hu, Steven P Dow, 10.1145/3637371Proc. ACM Hum. Comput. Interact. 8CSCW12024</p>
<p>Automating research synthesis with domain-specific large language model finetuning. Teo Susnjak, Peter Hwang, Napoleon H Reyes, L C Andre, Timothy R Barczak, Surangika Mcintosh, Ranathunga, 10.48550/ARXIV.2404.08680CoRR, abs/2404.086802024</p>
<p>Undiscovered public knowledge. Don R Swanson, The Library Quarterly: Information, Community. 198656</p>
<p>Ekin Dogus Cubuk, Amil Merchant, and 1 others. 2023. An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Nature. 6247990</p>
<p>Peer review as A multi-turn and longcontext dialogue with role-based interactions. Cheng Tan, Dongxin Lyu, Siyuan Li, Zhangyang Gao, Jingxuan Wei, Siqi Ma, Zicheng Liu, Stan Z Li, 10.48550/ARXIV.2406.05688CoRR, abs/2406.056882024</p>
<p>Step-back profiling: Distilling user history for personalized scientific writing. Xiangru Tang, Xingyao Zhang, Yanjun Shao, Jie Wu, Yilun Zhao, Arman Cohan, Ming Gong, Dongmei Zhang, Mark Gerstein, 10.48550/ARXIV.2406.14275CoRR, abs/2406.142752024a</p>
<p>Xuemei Tang, Xufeng Duan, Zhenguang G Cai, arXiv:2412.13612Are llms good literature review writers? evaluating the literature review writing ability of large language models. 2024barXiv preprint</p>
<p>Paper recommend based on lda and pagerank. Min Tao, Xinmin Yang, Gao Gu, Bohan Li, Artificial Intelligence and Security: 6th International Conference. Hohhot, China2020. July 17-20, 20202020Proceedings, Part III 6</p>
<p>Can llm feedback enhance review quality? a randomized study of 20k reviews at iclr. Nitya Springer, Mert Thakkar, Jake Yuksekgonul, Animesh Silberg, Nanyun Garg, Fei Peng, Rose Sha, Carl Yu, James Vondrick, Zou, arXiv:2504.097372025. 2025arXiv preprint</p>
<p>An incontext learning agent for formal theorem-proving. Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, Swarat Chaudhuri, First Conference on Language Modeling. 2024</p>
<p>Evaluating the predictive capacity of chatgpt for academic peer review outcomes across multiple platforms. Mike Thelwall, Abdullah Yaghi, 10.48550/ARXIV.2411.09763CoRR, abs/2411.097632024</p>
<p>FEVER: a large-scale dataset for fact extraction and verification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, 10.18653/V1/N18-1074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018. Long Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018New Orleans, Louisiana, USAAssociation for Computational Linguistics2018. June 1-6, 20181</p>
<p>Overview of the NLPCC2024 shared task 6: Scientific literature survey generation. Yangjie Tian, Xungang Gu, Aijia Li, He Zhang, Ruohua Xu, Yunfeng Li, Ming Liu, 10.1007/978-981-97-9443-0_35Natural Language Processing and Chinese Computing -13th National CCF Conference, NLPCC 2024. Lecture Notes in Computer Science. Hangzhou, China2024. November 1-3, 202415363Proceedings, Part V</p>
<p>Quantemp: A real-world open-domain benchmark for fact-checking numerical claims. Springer, V Venktesh, Abhijit Anand, Avishek Anand, Vinay Setty, 10.18653/V1/2023.FINDINGS-ACL.387Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024Washington DC, USA; Toronto, CanadaAssociation for Computational Linguistics2024. July 14-18, 2024. 2023. July 9-14, 2023Findings of the Association for Computational Linguistics: ACL 2023</p>
<p>Comparing knowledge sources for open-domain scientific claim verification. Juraj Vladika, Florian Matthes, ; Long Papers, St Julian's, Malta, 10.18653/V1/2024.FINDINGS-NAACL.295Association for Computational Linguistics. Juraj Vladika and Florian Matthes. 2024b. Improving health question answering with reliable and timeaware evidence In Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, MexicoAssociation for Computational Linguistics2024a. March 17-22, 2024. June 16-21, 20241Proceedings of the 18th Conference of the European Chapter</p>
<p>Healthfc: Verifying health claims with evidence-based medical fact-checking. Juraj Vladika, Phillip Schneider, Florian Matthes, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024Torino, ItalyELRA and ICCL2024. 20-25 May, 2024</p>
<p>Fact or fiction: Verifying scientific claims. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine Van Zuylen, Arman Cohan, Hannaneh Hajishirzi, 10.18653/V1/2020.EMNLP-MAIN.609Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020. 2020. November 16-20, 2020</p>
<p>Scifact-open: Towards open-domain scientific claim verification. David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, Hannaneh Hajishirzi, 10.18653/V1/2022.FINDINGS-EMNLP.347Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022a. December 7-11, 2022</p>
<p>Multivers: Improving scientific claim verification with weak supervision and full-document context. David Wadden, Kyle Lo, Lucy Lu Wang, Arman Cohan, Iz Beltagy, Hannaneh Hajishirzi, 10.18653/V1/2022.FINDINGS-NAACL.6Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, WA, United StatesAssociation for Computational Linguistics2022b. July 10-15, 2022</p>
<p>Sciriff: A resource to enhance language model instruction-following over scientific literature. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, Arman Cohan, 10.48550/ARXIV.2406.07835CoRR, abs/2406.078352024</p>
<p>Check-covid: Fact-checking COVID-19 news claims with scientific evidence. Gengyu Wang, Kate Harwood, Lawrence Chillrud, Amith Ananthram, Melanie Subbiah, Kathleen R Mckeown, 10.18653/V1/2023.FINDINGS-ACL.888Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023a. July 9-14, 2023</p>
<p>Proving theorems recursively. Haiming Wang, Huajian Xin, Zhengying Liu, Wenda Li, Yinya Huang, Jianqiao Lu, Zhicheng Yang, Jing Tang, Jian Yin, Zhenguo Li, Xiaodan Liang, 10.48550/ARXIV.2405.14414CoRR, abs/2405.144142024a</p>
<p>Lego-prover: Neural theorem proving with growing libraries. Haiming Wang, Huajian Xin, Chuanyang Zheng, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, Xiaodan Liang, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, Austria2024b. May 7-11, 2024OpenReview.net</p>
<p>Dt-solver: Automated theorem proving with dynamic-tree sampling guided by proof-level value function. Haiming Wang, Ye Yuan, Zhengying Liu, Jianhao Shen, Yichun Yin, Jing Xiong, Enze Xie, Han Shi, Yujun Li, Lin Li, Jian Yin, Zhenguo Li, Xiaodan Liang, 10.18653/V1/2023.ACL-LONG.706Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b. July 9-14, 2023ACL 2023</p>
<p>Scholawrite: A dataset of end-to-end scholarly writing process. Linghe Wang, Minhwa Lee, Ross Volkov, Dongyeop Luan Tuyen Chau, Kang, 10.48550/ARXIV.2502.02904CoRR, abs/2502.029042025</p>
<p>Multi-document scientific summarization from a knowledge graph-centric view. Pancheng Wang, Shasha Li, Kunyuan Pang, Liangliang He, Dong Li, Jintao Tang, Ting Wang, Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022. the 29th International Conference on Computational Linguistics, COLING 2022Gyeongju, Republic of Korea2022a. October 12-17, 2022International Committee on Computational Linguistics</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/V1/2024.ACL-LONG.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024c. August 11-16, 20241ACL 2024</p>
<p>Scipip: An llm-based scientific paper idea proposer. Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye, 10.48550/ARXIV.2410.23166CoRR, abs/2410.231662024d</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang, 10.48550/ARXIV.2406.10252CoRR, abs/2406.102522024e</p>
<p>Disencite: Graph-based disentangled representation learning for context-specific citation generation. Yifan Wang, Yiping Song, Shuai Li, Chaoran Cheng, Wei Ju, Ming Zhang, Sheng Wang, 10.1609/AAAI.V36I10.21397Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event. AAAI Press2022b. February 22 -March 1, 2022</p>
<p>Analyzing the past to prepare for the future: Writing a literature review. Jane Webster, Richard T Watson, MIS Q. 2622002</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, 10.48550/ARXIV.2411.00816CoRR, abs/2411.008162024</p>
<p>Algorithmic ghost in the research shell: Large language models and academic knowledge creation in management research. Nigel L Williams, Stanislav Ivanov, Dimitrios Buhalis, 10.48550/ARXIV.2303.07304CoRR, abs/2303.073042023</p>
<p>Characterizing and verifying scientific claims: Qualitative causal structure is all you need. Jinxuan Wu, Wenhan Chao, Xian Zhou, Zhunchen Luo, 10.18653/V1/2023.EMNLP-MAIN.828Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Lean-github: Compiling github LEAN repositories for a versatile LEAN prover. Zijian Wu, Jiayu Wang, Dahua Lin, Kai Chen, 10.48550/ARXIV.2407.17227CoRR, abs/2407.172272024</p>
<p>What makes medical claims (un)verifiable? analyzing entity and relation properties for fact verification. Amelie Wührl, Yarik Menchaca Resendiz, Lara Grimminger, Roman Klinger, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024. Long Papers. the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024St. Julian's, MaltaAssociation for Computational Linguistics2024a. March 17-22, 20241</p>
<p>Understanding finegrained distortions in reports of scientific findings. Amelie Wührl, Dustin Wright, Roman Klinger, Isabelle Augenstein, 10.18653/V1/2024.FINDINGS-ACL.369Findings of the Association for Computational Linguistics, ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024b. August 11-16, 2024and virtual meeting</p>
<p>Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, Xiaodan Liang, 10.48550/ARXIV.2405.14333CoRR, abs/2405.143332024</p>
<p>TRIGO: benchmarking formal mathematical proof reduction for generative language models. Jing Xiong, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun Yin, Zhengying Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, Chuanyang Zheng, Xiaodan Liang, Ming Zhang, Qun Liu, 10.18653/V1/2023.EMNLP-MAIN.711Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Patterns and purposes: A crossjournal analysis of ai tool usage in academic writing. Ziyang Xu, arXiv:2502.006322025Preprint</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, 10.48550/ARXIV.2412.15115CoRR, abs/2412.15115others. 2024a. Qwen2.5 technical report. Junyang Lin, Kai Dang, 22</p>
<p>Scicap+: A knowledge augmented dataset to study the challenges of scientific figure captioning. Kaiyu Yang, Jia Deng, ; Kaiyu Pmlr, Aidan M Yang, Alex Swope, Rahul Gu, Peiyang Chalamala, Shixing Song, Saad Yu, Ryan J Godil, Animashree Prenger, Anandkumar, Proceedings of the Workshop on Scientific Document Understanding co-located with 37th AAAI Conference on Artificial Inteligence (AAAI 2023). CEUR Workshop Proceedings. CEUR-WS.org. Zhishen Yang, Raj Dabre, Hideki Tanaka, Naoaki Okazaki, the Workshop on Scientific Document Understanding co-located with 37th AAAI Conference on Artificial Inteligence (AAAI 2023)Long Beach, California, USA; New Orleans, LA, USA2019. 2019, 9-15 June 2019. 2023a. December 10 -16, 2023. 2023b. February 14, 202397Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023. Remote</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, 10.18653/V1/2024.FINDINGS-ACL.804Findings of the Association for Computational Linguistics, ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024b. August 11-16, 2024and virtual meeting</p>
<p>Jinjie Ni, and Erik Cambria. 2023c. Logical reasoning over natural language as knowledge representation: A survey. Zonglin Yang, Xinya Du, Rui Mao, 10.48550/ARXIV.2303.12023CoRR, abs/2303.12023</p>
<p>Moosechem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, 10.48550/ARXIV.2410.07076CoRR, abs/2410.070762024c</p>
<p>Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R Fabbri, Irene Li, Dan Friedman, Dragomir R Radev, 10.1609/AAAI.V33I01.33017386The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence. Honolulu, Hawaii, USAAAAI Press2019. January 27 -February 1, 20192019</p>
<p>Drugassist: A large language model for molecule optimization. Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, Xiangxiang Zeng, 10.48550/ARXIV.2401.10334CoRR, abs/2401.103342024</p>
<p>Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, 10.48550/ARXIV.2412.17767CoRR, abs/2412.17767Tao Feng, and Jiaxuan You. 2024a. Researchtown: Simulator of human research community. </p>
<p>Reinforced subject-aware graph neural network for related work generation. Luyao Yu, Qi Zhang, Chongyang Shi, An Lao, Liang Xiao, 10.1007/978-981-97-5492-2_16Knowledge Science, Engineering and Management -17th International Conference, KSEM 2024, Birmingham. Lecture Notes in Computer Science. UKSpringer2024b. August 16-18, 202414884Proceedings, Part I</p>
<p>Mengxia Yu, Wenhao Yu, Lingbo Tong, Meng Jiang, Scientific comparative argument generation. 2022</p>
<p>Dolphin: Closed-loop openended auto-research through thinking, practice, and feedback. Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, Bowen Zhou, arXiv:2501.039162025arXiv preprint</p>
<p>Kid-review: Knowledge-guided scientific review generation with oracle pre-training. Weizhe Yuan, Pengfei Liu, 10.1609/AAAI.V36I10.21418Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence. AAAI Press2022. February 22 -March 1, 20222022The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</p>
<p>Prompt to be consistent is better than self-consistent? few-shot and zero-shot fact verification with pre-trained language models. Weizhe Yuan, Pengfei Liu, Graham Neubig ; Fengzhu Zeng, Wei Gao, 10.18653/V1/2023.FINDINGS-ACL.278Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2022. 2023. July 9-14, 202375Can we automate scientific reviewing?</p>
<p>Scientific opinion summarization: Paper meta-review generation dataset, methods, and evaluation. Qi Zeng, Mankeerat Sidhu, Ansel Blume, Pong Hou, Lu Chan, Heng Wang, Ji, Artificial Intelligence for Research and Democracy: First International Workshop, AI4Research 2024, and 4th International Workshop, DemocrAI 2024, Held in Conjunction with IJCAI 2024. Jeju, South KoreaSpringer Nature2024. August 5, 202420</p>
<p>Meta-review generation with checklist-guided iterative introspection. Qi Zeng, Mankeerat Sidhu, Pong Hou, Lu Chan, Heng Wang, Ji, 10.48550/ARXIV.2305.14647CoRR, abs/2305.146472023</p>
<p>Mlcopilot: Unleashing the power of large language models in solving machine learning tasks. Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, Yuqing Yang, Proceedings of the 18th Conference of the European Chapter. Long Papers. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics2024a. March 17-22, 20241</p>
<p>Automlgpt: Automatic machine learning with GPT. Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, Mingyuan Zhou, 10.48550/ARXIV.2305.02499CoRR, abs/2305.024992023</p>
<p>Augmenting the veracity and explanations of complex fact checking via iterative self-revision with llms. Xiaocheng Zhang, Xi Wang, Yifei Lu, Zhuangzhuang Ye, Jianing Wang, Mengjiao Bao, Peng Yan, Xiaohong Su, 10.48550/ARXIV.2410.15135CoRR, abs/2410.151352024b</p>
<p>MASSW: A new dataset and benchmark tasks for ai-assisted scientific workflows. Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, Qiaozhu Mei, 10.48550/ARXIV.2406.06357CoRR, abs/2406.063572024c</p>
<p>Towards llm-based fact verification on news claims with a hierarchical step-by-step prompting method. Xuan Zhang, Wei Gao, 10.18653/V1/2023.IJCNLP-MAIN.64Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics. Long Papers. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational LinguisticsBali2023. November 1 -4, 20231Nusa Dua. Association for Computational Linguistics</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024d. 2024. November 12-16, 2024</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zhang, arXiv:2303.18223Zican Dong, and 1 others. 2023a. A survey of large language models. arXiv preprint</p>
<p>Decomposing the enigma: Subgoal-based demonstration learning for formal theorem proving. Xueliang Zhao, Wenda Li, Lingpeng Kong, 10.48550/ARXIV.2305.16366CoRR, abs/2305.163662023b</p>
<p>Codegeex: A pretrained model for code generation with multilingual benchmarking on humaneval-x. Kunhao Zheng, Jesse Michael Han, Stanislas Polu, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. 2022. April 25-29, 2022. 2023aOpen-Review.net. Qinkai ZhengProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</p>
<p>Large language models for scientific synthesis, inference and explanation. Yizhen Zheng, Yee Huan, Jiaxin Koh, Anh T N Ju, Lauren T Nguyen, Geoffrey I May, Shirui Webb, Pan, 10.48550/ARXIV.2310.07984CoRR, abs/2310.079842023b</p>
<p>Hypothesis generation with large language models. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, 10.48550/ARXIV.2404.04326CoRR, abs/2404.043262024</p>
<p>Hierarchical catalogue generation for literature review: A benchmark. Kun Zhu, Xiaocheng Feng, Xiachong Feng, Yingsheng Wu, Bing Qin, 10.18653/V1/2023.FINDINGS-EMNLP.453Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, ; Paperweaver, ( Lee, arXiv:2503.08569Stergiopoulos. BeelLi and Zou2025. 2024. 2024. 2023. 2024. 2022. 2022. 2020. 2019. 2019. 2016arXiv preprintResearch Paper Recommendation ( §2.1.1)</p>
<p>Knowledge Navigator (. Wang Autosurvey, Altmami and Menai. Lai, BolañosBlock and Kuckertz2024e. 2024. 2024b. 2024. 2024. 2024. 2024. 2024a. 2023. 2022a. 2024b. 2024. 2024. 2024. 2024. 2023. 2022Pa-perQA2</p>
<p>Yang , Hypotheses Generation ( §2.2) Other Works MOOSE-Chem. Henry and McInnes2024c. 2024. 2024. 2024. 2023. 2023. 2022. 2017</p>
<p>Wang Scipip, Input Data Quality. Buehler2024d. 2024a. 2024. 2024a. 2024c. 2024. 2024a</p>
<p>Re-searchAgent. Dolphin (yuan, Hypothesis Quality Agentrxiv. NovaSchmidgall and Moor2025. 2025. 2024. 2024a. 2024. 2024. 2024. 2024. 2024. 2024b</p>
<p>Hypothesis Validation. </p>
<p>Sfavel (bazaga, Scientific Claim Verification ( §3.1) Claim HiSS. ProToCo (Zeng and GaoZhang and Gao, 2023. 2024. 2023</p>
<p>Evidence Glockner, Vladika and Matthes. 2024a. 2024b. 2024a. 2024b. 2023b. 2022. 2022bVladika and Matthes</p>
<p>MAGIC (Kao and Yen. Verification Claimver, ( Dammu, Muharram and Purwarianti. 2024. 2024. 2023. 2023. 2023a. 2024b. 2024. 2024. 2024b. 2023. 2023</p>
<p>Xin, Theorem Proving ( §3.2) DeepSeek-Prover. 2024. 2024. 2022a. 2024. 2024b. 2023. 2024c. 2023b. 2022. 2020. 2024a. 2024. 2024e. 2023b. 2023c. 2023GPT-f (Polu and Sutskever</p>
<p>AutoML-GPT. ( Matpilot, Ni, CRISPR-GPT. Boiko, 2024. 2024a. 2024c. 2023. 2024a. 2024. 2024. 2024b. 2024b. 2023. 2024. 2023. 2023b. 2024. 2024ML-Copilot</p>
<p>Manuscript Publication ( §4) Manuscript Writing ( §4.1) Citation Text Generation SciLit. Wang Disencite, 2023. 2022b. 2024. 2023b. 2022. 2022Gu and HahnloserGao</p>
<p>Martin-Boyle , Li and Ouyang. Li and Ouyang2024. 2023. 2024. 2024b. 2024. 2022R3WRelated Work Generation ScholaCite</p>
<p>Feng, Complete Manuscripts Generation Cocoa. Greene Pividori, Lin2024. 2024a. 2024. 2024b. 2022. 2022a. 2024. 2024. 2024. 2023. 2024Step-Back. data-to-paper. R3 (Du</p>
<p>KID-Review. Weng, OpenReviewer (Idahl and Ahmadi, 2024) RelevAI-Reviewer. RobertsonYuan and Liu2024. 2024. 2024. 2024. 2024a. 2022. 2024. 2023. 2023. 2022Quality Assist (Setio and Tsuchiya, 2022)</p>
<p>. Zhu, Meta-Review Generation Deeprview. I22025. 2024. 2024. 2024. 2023. 2024b. 2024. 2023. 2023</p>
<p>Figure 6: Taxonomy of Hypothesis Formulation, Hypothesis Validation and Manuscript Publication. Full Edition</p>            </div>
        </div>

    </div>
</body>
</html>
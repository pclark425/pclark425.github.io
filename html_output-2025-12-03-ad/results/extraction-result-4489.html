<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4489 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4489</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4489</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-277313413</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.18865v3.pdf" target="_blank">Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of large language models offers new possibilities for structured exploration of scientific knowledge. Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights. Specifically, we investigate how knowledge unit--especially those tied to methodological design--can be modeled and recombined to yield research breakthroughs. Our proposed framework addresses two key challenges. First, we introduce a contrastive learning-based mechanism to identify distinguishing features of historically disruptive method combinations within problem-driven contexts. Second, we propose a reasoning-guided Monte Carlo search algorithm that leverages the chain-of-thought capability of LLMs to identify promising knowledge recombinations for new problem statements.Empirical studies across multiple domains show that the framework is capable of modeling the structural dynamics of innovation and successfully highlights combinations with high disruptive potential. This research provides a new path for computationally guided scientific ideation grounded in structured reasoning and historical data modeling.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4489.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4489.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disruption Index</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bibliometric metric that quantifies whether a publication displaces prior work (disruptive) or consolidates it (developmental) using citation patterns between the focal paper, its references, and subsequent papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A dynamic network measure of technological change</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Disruption Index (DI)-based evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute DI for a focal work using D = (n_i - n_j) / (n_i + n_j + n_k), where n_i = number of later papers citing only the focal work, n_j = number citing both the focal work and its references, and n_k = number citing only the references. In this paper the DI (ground-truth computed from citation data) is used as the target label: the framework (1) generates concise problem-method summaries, (2) retrieves and extracts reference information, (3) predicts DI with a fine-tuned model (LoRA adapters) using an entropy-weighted loss and secondary-learning + deviation-aware alignment, and (4) optimizes candidate method combinations to maximize predicted DI (using greedy or GPP search). High-disruptiveness is operationalized as DI > 0.5 for hit-rate evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary criterion is disruptiveness as measured by DI (degree to which new work supersedes prior references). For model training/evaluation they use predictive accuracy metrics (MSE, MAE) and weighted variants (WMSE, WMAE) to emphasize rare/high-DI samples; summary quality is measured by cosine similarity and ROUGE. For optimization, hit rate of DI>0.5 is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Multiple: computer science (DBLP / AI conferences), biomedicine (PubMed — depression), medical robotics patents (PatSnap)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Problem-method combinations / research hypotheses (methodological recombinations)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as ground-truth and target for prediction. The paper's DI-prediction framework (summarization + relevance extraction + bias-aware prediction + secondary learning + deviation-aware alignment) achieves low prediction errors (Table 3: DBLP MSE=0.0093, MAE=0.0111, WMSE=0.0941, WMAE=0.1364; PubMed MSE=0.0154, MAE=0.0342; PatSnap MSE=0.0218, MAE=0.0412). For method optimization, Greedy+GPP yields higher hit rates of high-disruptiveness (DI>0.5): DBLP 26.3%, PubMed 28.1%, PatSnap 24.6%, improving ≈9.0–9.1 percentage points over the best single-LLM baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Primarily automated: DI is computed from citation data and used as numeric supervision; model predictions are evaluated against ground-truth DI with standard error metrics. Historically DI has also been validated against expert assessment (cited prior work), so the approach is hybrid in interpretation but training/evaluation here is automated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation by comparing predicted DI to ground-truth DI computed from citation databases (DBLP, PubMed, PatSnap), ablation studies to test component contributions, and comparisons against multiple baselines (GPT variants, PLMs). The paper also cites prior expert-validation literature for DI's external validity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>DI depends on citation behaviors which are biased by field, team size, norms, negative citations, and time-lag; high-disruptiveness examples are rare, causing class imbalance; DI does not directly measure scientific truth or mechanistic quality; computational cost and data requirements for citation-based ground-truth; may be inapplicable in nascent fields with little citation history.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Three citation-based corpora used as ground-truth DI sources: DBLP (AI conference papers 2011–2021), PubMed (depression literature 2015–2025), PatSnap (medical robotics patents 2020–2025).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4489.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entropy-weight</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entropy-based weighted evaluation metric (entropy-weighted loss)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sample-weighting scheme that increases the training loss weight of rare (e.g., highly disruptive) samples by using a negative-log-probability weight, to mitigate class imbalance when predicting DI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Entropy-weighted loss for DI prediction</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>During DI model fine-tuning the loss is weighted per instance w_i = -log(p(ŷ_i)), where p(ŷ_i) is the predicted probability for the sample's label; the weighted loss L_entropy = sum_i w_i * ℓ(y_i, ŷ_i) increases emphasis on rare/high-DI instances to improve model sensitivity to disruptive cases.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Improvement in weighted predictive error metrics (WMSE, WMAE) and improved detection of high-DI samples relative to unweighted training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Same multi-domain corpora (DBLP, PubMed, PatSnap)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Problem-method DI prediction (supervised regression)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported as part of the full pipeline which attains low weighted errors (e.g., Table 3 WMSE/WMAE improvements). The authors state the metric 'assigns higher weights to rare but highly disruptive samples, enhancing the model's capability to identify high-disruptiveness cases.' No isolated numeric ablation solely for entropy-weight was provided.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated — used as a loss-weighting method during training.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Indicated implicitly via overall model performance (WMSE/WMAE) and inclusion in ablation studies of larger training pipeline; no separate human validation for weighting scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depends on reliable estimated probabilities p(ŷ); can amplify noise if probability estimates are poor; may over-emphasize outliers if not calibrated; requires tuning to avoid instability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4489.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SecondaryLearning+KL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Secondary learning mechanism with KL-divergence balancing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage reinforcement training procedure that selects high-error instances for additional training, while using a KL-divergence regularizer between primary and secondary prediction distributions to prevent prediction drift.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Secondary (hard-sample) learning with KL balancing</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>After primary training, select the top 20% of instances with highest prediction errors and perform reinforcement training on them; include a KL-divergence loss term L_KL = D_KL(D_primary || D_secondary) to align secondary model distribution with primary model and avoid destabilizing global predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Reduction in MSE/MAE and weighted errors on held-out sets; robustness to hard-to-classify cases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-7B (used in ablation experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Same experimental corpora</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Problem-method DI prediction</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Ablation shows removing secondary learning increases errors: DBLP MSE rises from 0.0093 (full) to 0.0187 (w/o secondary learning); similar degradations appear for PubMed and PatSnap (Table 4), demonstrating the mechanism improves predictive performance on difficult samples.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated — training procedure tested via held-out prediction metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ablation experiments comparing full framework vs without-secondary-learning across datasets and metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Adds computational overhead; threshold for 'top 20%' is heuristic; secondary training can still risk overfitting hard examples despite KL regularization; requires careful balancing to prevent drift.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4489.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deviation-aware</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deviation-aware alignment / deviation-awareness mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-evaluation mechanism that feeds prediction results back into the model so it can estimate and correct prediction deviations over iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Deviation-aware alignment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>After each prediction iteration the model computes deviation signals between predicted and target values and applies an update guided by the deviation gradient (θ_{t+1} = θ_t - η * ∇Deviation(ŷ_t, y_t)). This alignment module also forms part of the adaptive bias-aware prediction pipeline to correct systematic prediction errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Improvement in prediction consistency and reduced MSE/MAE; contribution assessed via ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Same datasets</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Prediction of DI for problem-method combos</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Ablation shows removing deviation-aware alignment increases DBLP MSE from 0.0093 to 0.0216 and similar increases on other datasets (Table 4), indicating a material contribution to predictive accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (iterative model calibration)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ablation testing within the same experimental evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Iterative updates increase computation and potential overfitting; effectiveness depends on reliable deviation signals; may require hyperparameter tuning for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4489.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy with Probabilistic Perturbation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stochastic greedy optimization strategy that with small probability ε accepts a non-optimal neighbor configuration to escape local optima while otherwise taking locally optimal moves to maximize a target score (here: predicted DI).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A probabilistic greedy search algorithm for combinatorial optimisation with application to the set covering problem.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>GPP optimization for method combination selection</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>At each iteration pick the neighbor M' in the neighborhood N(M_t) that maximizes predicted DI; with probability ε instead accept a random neighbor M_rand to permit exploration. Objective includes regularization penalty C(M) and adaptive weighting of DI feedback across iterations; used to search the combinatorial space of method components to maximize predicted disruptiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary measure is hit rate of identified method combinations with DI>0.5; also compare average predicted DI and convergence behavior. Benchmarked against standard greedy and LLM-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied to DBLP, PubMed, PatSnap experiments</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Method configuration optimization for disruptive research ideas</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Table 5: GPP hit rates: DBLP 26.3%, PubMed 28.1%, PatSnap 24.6%; standard greedy: 19.3%, 21.5%, 18.2%; top single-LLM baselines ranged ≈15–19%. GPP shows ≈+9 percentage point improvement over best LLM baseline per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated algorithmic optimization using model-predicted DI as reward signal.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical comparison (experiments) against standard greedy and multiple LLM/PLM baselines, reporting hit rates and improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires tuning probability ε and regularization weights; stochastic moves may increase variance and computation; effectiveness tied to DI prediction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4489.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cosine+ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cosine similarity and ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard automated metrics used to evaluate alignment and lexical overlap between generated problem-method summaries and ground-truth summaries: cosine on embedding vectors and ROUGE for n-gram overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Cosine similarity and ROUGE for summarization quality</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute embedding-based cosine similarity between generated and reference summaries to measure semantic closeness, and ROUGE (recall-oriented n-gram overlap) to quantify textual overlap; used to evaluate the problem-method summary generation model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Semantic similarity (cosine) and lexical overlap (ROUGE-n/ROUGE-L) scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-32B (example summarization evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Same corpora (DBLP, PubMed, PatSnap)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Summaries of problem-method pairs (supporting DI prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Table 1: Example numbers — Qwen-32B (Fine-tuned) achieved DBLP similarity 0.558 / ROUGE 0.325, PubMed similarity 0.500 / ROUGE 0.324, Patents similarity 0.612 / ROUGE 0.404; outperforming various other LLM baselines on these metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric comparison against expert-curated ground-truth summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Quantitative evaluation on held-out ground-truth summary sets; used for checkpoint selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Cosine/ROUGE measure surface or embedding-based similarity and may not capture logical validity, traceability to literature, or scientific rigor of generated summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4489.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ErrorMetrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MSE / MAE / WMSE / WMAE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prediction error metrics used to evaluate the DI regression model: mean squared error, mean absolute error, and their entropy-weighted variants which emphasize rare/high-DI samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MSE, MAE, weighted MSE (WMSE), weighted MAE (WMAE)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Standard pointwise regression metrics: MSE = mean((ŷ - y)^2), MAE = mean(|ŷ - y|). Weighted versions multiply per-sample loss by entropy-derived weights to upweight rare labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Lower is better; used across datasets to compare models and ablation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various compared models (GPT-4o, GPT-4 Turbo, Claude 3.5/3.7, SciBERT, RoBERTa, LLaMA 3, Qwen-7B; and fine-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Same experimental domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>DI prediction for problem-method combos</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Examples: Our Framework (Table 3) DBLP MSE=0.0093 MAE=0.0111 WMSE=0.0941 WMAE=0.1364; baseline GPT-4o DBLP MSE=0.1191 MAE=0.3062 etc. Fine-tuned PLMs show much lower errors than generic LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated numeric evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparative evaluation across held-out test sets and ablation studies; model selection by validation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Metrics sensitive to label distribution; weighted variants require careful definition of weights; numeric error does not directly measure qualitative scientific value.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4489.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IdeaBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IdeaBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark proposed to standardize the assessment of research ideas produced by LLMs, used in prior work to evaluate idea-generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Idea-bench: How far are generative models from professional designing?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>IdeaBench benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>A standardized benchmark that evaluates generated research ideas against tasks or reference ideas — intended to provide reproducible evaluation of LLM ideation quality (mentioned as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Quality dimensions typically include novelty, feasibility, and alignment to task prompts (exact criteria depend on benchmark design).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General research ideation (design / research idea generation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Research ideas / hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mentioned as existing benchmark in related work; no experimental use in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Benchmark likely includes automated and human evaluation components; paper references it as an example of prior standardized assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not applied in this paper (cited from prior literature).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper notes existing benchmarks focus on textual synthesis rather than structured problem-method recombination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4489.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CreativityIndex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Creativity Index (DPO-based evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed approach combining supervised fine-tuning and Direct Preference Optimization (DPO) to evaluate generative model outputs on originality, feasibility, impact, and reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Creativity Index (supervised + DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluates generated ideas by training models with preference signals (DPO) and supervised data to score dimensions such as originality, feasibility, impact, and reliability; cited as an example of recent evaluation frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Originality, feasibility, impact, reliability (explicitly mentioned in text).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Research ideation generally (cited in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Research hypotheses/ideas</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mentioned in related work; not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid approach (preference optimization implies human preference signals used in training).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Cited from prior literature; not directly validated here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper argues such approaches still rely on textual synthesis and human preference alignment and lack quantitative disruptive-impact measures like DI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4489.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOOSE-Chem multi-agent framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based multi-agent system to improve research-hypothesis generation in chemistry through problem decomposition and agent coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MOOSE-Chem multi-agent evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Uses task decomposition and multi-agent interactions to produce hypotheses; prior work uses domain-specific validation (chemistry) to assess quality of generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Quality of chemical hypotheses, rediscovery of known chemistry, novelty and plausibility (as discussed in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Scientific hypotheses (chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Cited as an example of structured LLM frameworks for hypothesis generation; not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Prior work uses automated rediscovery tests and domain validation; cited here as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not applied in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper argues domain-specific frameworks still lack structured problem-method recombination and objective disruptiveness metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4489.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoIAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Ideas (CoI) Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system that organizes literature into a structured developmental chain to simulate domain progress and improve ideation; reported to produce research ideas comparable in quality to human researchers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of ideas: Revolutionizing research via novel idea development with llm agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Chain-of-Ideas structured evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Constructs a chain of literature-driven ideas to support LLM generation, and evaluates output quality against human preferences and benchmarks; cited as a structured framework that enhances logical progression of LLM-generated ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Quality compared to human-generated ideas, coherence, novelty, and logical progression; evaluated with human-aligned protocols in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific research ideation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Research ideas / conceptual chains</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Cited as outperforming conventional methods and producing human-comparable research ideas in prior work; not directly used here.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid — includes human-preference-aligned evaluation in referenced studies.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Referenced prior experimental claims (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Authors note existing structured frameworks still lack fine-grained problem-method recombination and objective disruptive-impact metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4489.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IdeaArena</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Idea Arena protocol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation protocol for generated ideas that ensures criteria align with human research preferences (mentioned as prior work to evaluate generated ideas).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Idea Arena protocol</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>A human-aligned evaluation setup to judge generated ideas according to criteria shaped by researcher preferences; used as an example of human-centered evaluation approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human preference alignment (quality dimensions defined by researchers), novelty, clarity, and applicability (protocol-specific).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General research ideation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Research ideas/hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Mentioned in related work; Claim: Idea Arena helps align evaluations with human research preferences; not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based (protocol designed around human preferences).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not applied in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Authors cite reliance on human preference alignment as limited because it lacks objective measures of transformative impact.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4489.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e4489.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative LLM-driven framework that integrates an academic graph and retrieval to refine research ideas, employing multiple LLM reviewing agents for structured feedback aligned to human-defined criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ResearchAgent iterative evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Iteratively generates and refines ideas using retrieval and multiple reviewer agents; uses structured feedback aligned to human criteria to improve clarity, novelty, and validity of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Clarity, novelty, validity (evaluated via reviewing agents and alignment to human-defined criteria).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Research ideas/refinements</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Cited as improving generated idea quality through structured review; not experimentally applied in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid — automated reviewer agents trained/structured to reflect human criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not validated within this paper; cited from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A dynamic network measure of technological change <em>(Rating: 2)</em></li>
                <li>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models. <em>(Rating: 2)</em></li>
                <li>Idea-bench: How far are generative models from professional designing? <em>(Rating: 2)</em></li>
                <li>Empowering AI as autonomous researchers: Evaluating llms in generating novel research ideas through automated metrics. <em>(Rating: 2)</em></li>
                <li>Chain of ideas: Revolutionizing research via novel idea development with llm agents. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4489",
    "paper_id": "paper-277313413",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "DI",
            "name_full": "Disruption Index",
            "brief_description": "A bibliometric metric that quantifies whether a publication displaces prior work (disruptive) or consolidates it (developmental) using citation patterns between the focal paper, its references, and subsequent papers.",
            "citation_title": "A dynamic network measure of technological change",
            "mention_or_use": "use",
            "evaluation_method_name": "Disruption Index (DI)-based evaluation framework",
            "evaluation_method_description": "Compute DI for a focal work using D = (n_i - n_j) / (n_i + n_j + n_k), where n_i = number of later papers citing only the focal work, n_j = number citing both the focal work and its references, and n_k = number citing only the references. In this paper the DI (ground-truth computed from citation data) is used as the target label: the framework (1) generates concise problem-method summaries, (2) retrieves and extracts reference information, (3) predicts DI with a fine-tuned model (LoRA adapters) using an entropy-weighted loss and secondary-learning + deviation-aware alignment, and (4) optimizes candidate method combinations to maximize predicted DI (using greedy or GPP search). High-disruptiveness is operationalized as DI &gt; 0.5 for hit-rate evaluation.",
            "evaluation_criteria": "Primary criterion is disruptiveness as measured by DI (degree to which new work supersedes prior references). For model training/evaluation they use predictive accuracy metrics (MSE, MAE) and weighted variants (WMSE, WMAE) to emphasize rare/high-DI samples; summary quality is measured by cosine similarity and ROUGE. For optimization, hit rate of DI&gt;0.5 is reported.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Multiple: computer science (DBLP / AI conferences), biomedicine (PubMed — depression), medical robotics patents (PatSnap)",
            "theory_type": "Problem-method combinations / research hypotheses (methodological recombinations)",
            "human_comparison": true,
            "evaluation_results": "Used as ground-truth and target for prediction. The paper's DI-prediction framework (summarization + relevance extraction + bias-aware prediction + secondary learning + deviation-aware alignment) achieves low prediction errors (Table 3: DBLP MSE=0.0093, MAE=0.0111, WMSE=0.0941, WMAE=0.1364; PubMed MSE=0.0154, MAE=0.0342; PatSnap MSE=0.0218, MAE=0.0412). For method optimization, Greedy+GPP yields higher hit rates of high-disruptiveness (DI&gt;0.5): DBLP 26.3%, PubMed 28.1%, PatSnap 24.6%, improving ≈9.0–9.1 percentage points over the best single-LLM baselines.",
            "automated_vs_human_evaluation": "Primarily automated: DI is computed from citation data and used as numeric supervision; model predictions are evaluated against ground-truth DI with standard error metrics. Historically DI has also been validated against expert assessment (cited prior work), so the approach is hybrid in interpretation but training/evaluation here is automated.",
            "validation_method": "Validation by comparing predicted DI to ground-truth DI computed from citation databases (DBLP, PubMed, PatSnap), ablation studies to test component contributions, and comparisons against multiple baselines (GPT variants, PLMs). The paper also cites prior expert-validation literature for DI's external validity.",
            "limitations_challenges": "DI depends on citation behaviors which are biased by field, team size, norms, negative citations, and time-lag; high-disruptiveness examples are rare, causing class imbalance; DI does not directly measure scientific truth or mechanistic quality; computational cost and data requirements for citation-based ground-truth; may be inapplicable in nascent fields with little citation history.",
            "benchmark_dataset": "Three citation-based corpora used as ground-truth DI sources: DBLP (AI conference papers 2011–2021), PubMed (depression literature 2015–2025), PatSnap (medical robotics patents 2020–2025).",
            "uuid": "e4489.0",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Entropy-weight",
            "name_full": "Entropy-based weighted evaluation metric (entropy-weighted loss)",
            "brief_description": "A sample-weighting scheme that increases the training loss weight of rare (e.g., highly disruptive) samples by using a negative-log-probability weight, to mitigate class imbalance when predicting DI.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Entropy-weighted loss for DI prediction",
            "evaluation_method_description": "During DI model fine-tuning the loss is weighted per instance w_i = -log(p(ŷ_i)), where p(ŷ_i) is the predicted probability for the sample's label; the weighted loss L_entropy = sum_i w_i * ℓ(y_i, ŷ_i) increases emphasis on rare/high-DI instances to improve model sensitivity to disruptive cases.",
            "evaluation_criteria": "Improvement in weighted predictive error metrics (WMSE, WMAE) and improved detection of high-DI samples relative to unweighted training.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Same multi-domain corpora (DBLP, PubMed, PatSnap)",
            "theory_type": "Problem-method DI prediction (supervised regression)",
            "human_comparison": false,
            "evaluation_results": "Reported as part of the full pipeline which attains low weighted errors (e.g., Table 3 WMSE/WMAE improvements). The authors state the metric 'assigns higher weights to rare but highly disruptive samples, enhancing the model's capability to identify high-disruptiveness cases.' No isolated numeric ablation solely for entropy-weight was provided.",
            "automated_vs_human_evaluation": "Automated — used as a loss-weighting method during training.",
            "validation_method": "Indicated implicitly via overall model performance (WMSE/WMAE) and inclusion in ablation studies of larger training pipeline; no separate human validation for weighting scheme.",
            "limitations_challenges": "Depends on reliable estimated probabilities p(ŷ); can amplify noise if probability estimates are poor; may over-emphasize outliers if not calibrated; requires tuning to avoid instability.",
            "uuid": "e4489.1",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SecondaryLearning+KL",
            "name_full": "Secondary learning mechanism with KL-divergence balancing",
            "brief_description": "A two-stage reinforcement training procedure that selects high-error instances for additional training, while using a KL-divergence regularizer between primary and secondary prediction distributions to prevent prediction drift.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Secondary (hard-sample) learning with KL balancing",
            "evaluation_method_description": "After primary training, select the top 20% of instances with highest prediction errors and perform reinforcement training on them; include a KL-divergence loss term L_KL = D_KL(D_primary || D_secondary) to align secondary model distribution with primary model and avoid destabilizing global predictions.",
            "evaluation_criteria": "Reduction in MSE/MAE and weighted errors on held-out sets; robustness to hard-to-classify cases.",
            "model_name": "Qwen-7B (used in ablation experiments)",
            "model_size": "7B",
            "scientific_domain": "Same experimental corpora",
            "theory_type": "Problem-method DI prediction",
            "human_comparison": false,
            "evaluation_results": "Ablation shows removing secondary learning increases errors: DBLP MSE rises from 0.0093 (full) to 0.0187 (w/o secondary learning); similar degradations appear for PubMed and PatSnap (Table 4), demonstrating the mechanism improves predictive performance on difficult samples.",
            "automated_vs_human_evaluation": "Automated — training procedure tested via held-out prediction metrics.",
            "validation_method": "Ablation experiments comparing full framework vs without-secondary-learning across datasets and metrics.",
            "limitations_challenges": "Adds computational overhead; threshold for 'top 20%' is heuristic; secondary training can still risk overfitting hard examples despite KL regularization; requires careful balancing to prevent drift.",
            "uuid": "e4489.2",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Deviation-aware",
            "name_full": "Deviation-aware alignment / deviation-awareness mechanism",
            "brief_description": "An iterative self-evaluation mechanism that feeds prediction results back into the model so it can estimate and correct prediction deviations over iterations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Deviation-aware alignment",
            "evaluation_method_description": "After each prediction iteration the model computes deviation signals between predicted and target values and applies an update guided by the deviation gradient (θ_{t+1} = θ_t - η * ∇Deviation(ŷ_t, y_t)). This alignment module also forms part of the adaptive bias-aware prediction pipeline to correct systematic prediction errors.",
            "evaluation_criteria": "Improvement in prediction consistency and reduced MSE/MAE; contribution assessed via ablation.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Same datasets",
            "theory_type": "Prediction of DI for problem-method combos",
            "human_comparison": false,
            "evaluation_results": "Ablation shows removing deviation-aware alignment increases DBLP MSE from 0.0093 to 0.0216 and similar increases on other datasets (Table 4), indicating a material contribution to predictive accuracy.",
            "automated_vs_human_evaluation": "Automated (iterative model calibration)",
            "validation_method": "Ablation testing within the same experimental evaluations.",
            "limitations_challenges": "Iterative updates increase computation and potential overfitting; effectiveness depends on reliable deviation signals; may require hyperparameter tuning for stability.",
            "uuid": "e4489.3",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPP",
            "name_full": "Greedy with Probabilistic Perturbation",
            "brief_description": "A stochastic greedy optimization strategy that with small probability ε accepts a non-optimal neighbor configuration to escape local optima while otherwise taking locally optimal moves to maximize a target score (here: predicted DI).",
            "citation_title": "A probabilistic greedy search algorithm for combinatorial optimisation with application to the set covering problem.",
            "mention_or_use": "use",
            "evaluation_method_name": "GPP optimization for method combination selection",
            "evaluation_method_description": "At each iteration pick the neighbor M' in the neighborhood N(M_t) that maximizes predicted DI; with probability ε instead accept a random neighbor M_rand to permit exploration. Objective includes regularization penalty C(M) and adaptive weighting of DI feedback across iterations; used to search the combinatorial space of method components to maximize predicted disruptiveness.",
            "evaluation_criteria": "Primary measure is hit rate of identified method combinations with DI&gt;0.5; also compare average predicted DI and convergence behavior. Benchmarked against standard greedy and LLM-only baselines.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Applied to DBLP, PubMed, PatSnap experiments",
            "theory_type": "Method configuration optimization for disruptive research ideas",
            "human_comparison": false,
            "evaluation_results": "Table 5: GPP hit rates: DBLP 26.3%, PubMed 28.1%, PatSnap 24.6%; standard greedy: 19.3%, 21.5%, 18.2%; top single-LLM baselines ranged ≈15–19%. GPP shows ≈+9 percentage point improvement over best LLM baseline per dataset.",
            "automated_vs_human_evaluation": "Automated algorithmic optimization using model-predicted DI as reward signal.",
            "validation_method": "Empirical comparison (experiments) against standard greedy and multiple LLM/PLM baselines, reporting hit rates and improvements.",
            "limitations_challenges": "Requires tuning probability ε and regularization weights; stochastic moves may increase variance and computation; effectiveness tied to DI prediction accuracy.",
            "uuid": "e4489.4",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Cosine+ROUGE",
            "name_full": "Cosine similarity and ROUGE",
            "brief_description": "Standard automated metrics used to evaluate alignment and lexical overlap between generated problem-method summaries and ground-truth summaries: cosine on embedding vectors and ROUGE for n-gram overlap.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Cosine similarity and ROUGE for summarization quality",
            "evaluation_method_description": "Compute embedding-based cosine similarity between generated and reference summaries to measure semantic closeness, and ROUGE (recall-oriented n-gram overlap) to quantify textual overlap; used to evaluate the problem-method summary generation model.",
            "evaluation_criteria": "Semantic similarity (cosine) and lexical overlap (ROUGE-n/ROUGE-L) scores.",
            "model_name": "Qwen-32B (example summarization evaluation)",
            "model_size": "32B",
            "scientific_domain": "Same corpora (DBLP, PubMed, PatSnap)",
            "theory_type": "Summaries of problem-method pairs (supporting DI prediction)",
            "human_comparison": false,
            "evaluation_results": "Table 1: Example numbers — Qwen-32B (Fine-tuned) achieved DBLP similarity 0.558 / ROUGE 0.325, PubMed similarity 0.500 / ROUGE 0.324, Patents similarity 0.612 / ROUGE 0.404; outperforming various other LLM baselines on these metrics.",
            "automated_vs_human_evaluation": "Automated metric comparison against expert-curated ground-truth summaries.",
            "validation_method": "Quantitative evaluation on held-out ground-truth summary sets; used for checkpoint selection.",
            "limitations_challenges": "Cosine/ROUGE measure surface or embedding-based similarity and may not capture logical validity, traceability to literature, or scientific rigor of generated summaries.",
            "uuid": "e4489.5",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ErrorMetrics",
            "name_full": "MSE / MAE / WMSE / WMAE",
            "brief_description": "Prediction error metrics used to evaluate the DI regression model: mean squared error, mean absolute error, and their entropy-weighted variants which emphasize rare/high-DI samples.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "MSE, MAE, weighted MSE (WMSE), weighted MAE (WMAE)",
            "evaluation_method_description": "Standard pointwise regression metrics: MSE = mean((ŷ - y)^2), MAE = mean(|ŷ - y|). Weighted versions multiply per-sample loss by entropy-derived weights to upweight rare labels.",
            "evaluation_criteria": "Lower is better; used across datasets to compare models and ablation variants.",
            "model_name": "Various compared models (GPT-4o, GPT-4 Turbo, Claude 3.5/3.7, SciBERT, RoBERTa, LLaMA 3, Qwen-7B; and fine-tuned variants)",
            "model_size": null,
            "scientific_domain": "Same experimental domains",
            "theory_type": "DI prediction for problem-method combos",
            "human_comparison": false,
            "evaluation_results": "Examples: Our Framework (Table 3) DBLP MSE=0.0093 MAE=0.0111 WMSE=0.0941 WMAE=0.1364; baseline GPT-4o DBLP MSE=0.1191 MAE=0.3062 etc. Fine-tuned PLMs show much lower errors than generic LLMs.",
            "automated_vs_human_evaluation": "Automated numeric evaluation.",
            "validation_method": "Comparative evaluation across held-out test sets and ablation studies; model selection by validation performance.",
            "limitations_challenges": "Metrics sensitive to label distribution; weighted variants require careful definition of weights; numeric error does not directly measure qualitative scientific value.",
            "uuid": "e4489.6",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "IdeaBench",
            "name_full": "IdeaBench",
            "brief_description": "A benchmark proposed to standardize the assessment of research ideas produced by LLMs, used in prior work to evaluate idea-generation quality.",
            "citation_title": "Idea-bench: How far are generative models from professional designing?",
            "mention_or_use": "mention",
            "evaluation_method_name": "IdeaBench benchmark",
            "evaluation_method_description": "A standardized benchmark that evaluates generated research ideas against tasks or reference ideas — intended to provide reproducible evaluation of LLM ideation quality (mentioned as prior work).",
            "evaluation_criteria": "Quality dimensions typically include novelty, feasibility, and alignment to task prompts (exact criteria depend on benchmark design).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General research ideation (design / research idea generation)",
            "theory_type": "Research ideas / hypotheses",
            "human_comparison": true,
            "evaluation_results": "Mentioned as existing benchmark in related work; no experimental use in this paper.",
            "automated_vs_human_evaluation": "Benchmark likely includes automated and human evaluation components; paper references it as an example of prior standardized assessment.",
            "validation_method": "Not applied in this paper (cited from prior literature).",
            "limitations_challenges": "Paper notes existing benchmarks focus on textual synthesis rather than structured problem-method recombination.",
            "uuid": "e4489.7",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "CreativityIndex",
            "name_full": "Creativity Index (DPO-based evaluation)",
            "brief_description": "A previously proposed approach combining supervised fine-tuning and Direct Preference Optimization (DPO) to evaluate generative model outputs on originality, feasibility, impact, and reliability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Creativity Index (supervised + DPO)",
            "evaluation_method_description": "Evaluates generated ideas by training models with preference signals (DPO) and supervised data to score dimensions such as originality, feasibility, impact, and reliability; cited as an example of recent evaluation frameworks.",
            "evaluation_criteria": "Originality, feasibility, impact, reliability (explicitly mentioned in text).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Research ideation generally (cited in related work)",
            "theory_type": "Research hypotheses/ideas",
            "human_comparison": true,
            "evaluation_results": "Mentioned in related work; not used in experiments in this paper.",
            "automated_vs_human_evaluation": "Hybrid approach (preference optimization implies human preference signals used in training).",
            "validation_method": "Cited from prior literature; not directly validated here.",
            "limitations_challenges": "Paper argues such approaches still rely on textual synthesis and human preference alignment and lack quantitative disruptive-impact measures like DI.",
            "uuid": "e4489.8",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MOOSE-Chem",
            "name_full": "MOOSE-Chem multi-agent framework",
            "brief_description": "An LLM-based multi-agent system to improve research-hypothesis generation in chemistry through problem decomposition and agent coordination.",
            "citation_title": "Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.",
            "mention_or_use": "mention",
            "evaluation_method_name": "MOOSE-Chem multi-agent evaluation",
            "evaluation_method_description": "Uses task decomposition and multi-agent interactions to produce hypotheses; prior work uses domain-specific validation (chemistry) to assess quality of generated hypotheses.",
            "evaluation_criteria": "Quality of chemical hypotheses, rediscovery of known chemistry, novelty and plausibility (as discussed in cited work).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Chemistry (hypothesis generation)",
            "theory_type": "Scientific hypotheses (chemistry)",
            "human_comparison": false,
            "evaluation_results": "Cited as an example of structured LLM frameworks for hypothesis generation; not evaluated in this paper.",
            "automated_vs_human_evaluation": "Prior work uses automated rediscovery tests and domain validation; cited here as related work.",
            "validation_method": "Not applied in this paper.",
            "limitations_challenges": "Paper argues domain-specific frameworks still lack structured problem-method recombination and objective disruptiveness metrics.",
            "uuid": "e4489.9",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "CoIAgent",
            "name_full": "Chain-of-Ideas (CoI) Agent",
            "brief_description": "An LLM-based system that organizes literature into a structured developmental chain to simulate domain progress and improve ideation; reported to produce research ideas comparable in quality to human researchers.",
            "citation_title": "Chain of ideas: Revolutionizing research via novel idea development with llm agents.",
            "mention_or_use": "mention",
            "evaluation_method_name": "Chain-of-Ideas structured evaluation",
            "evaluation_method_description": "Constructs a chain of literature-driven ideas to support LLM generation, and evaluates output quality against human preferences and benchmarks; cited as a structured framework that enhances logical progression of LLM-generated ideas.",
            "evaluation_criteria": "Quality compared to human-generated ideas, coherence, novelty, and logical progression; evaluated with human-aligned protocols in prior work.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General scientific research ideation",
            "theory_type": "Research ideas / conceptual chains",
            "human_comparison": true,
            "evaluation_results": "Cited as outperforming conventional methods and producing human-comparable research ideas in prior work; not directly used here.",
            "automated_vs_human_evaluation": "Hybrid — includes human-preference-aligned evaluation in referenced studies.",
            "validation_method": "Referenced prior experimental claims (not reproduced here).",
            "limitations_challenges": "Authors note existing structured frameworks still lack fine-grained problem-method recombination and objective disruptive-impact metrics.",
            "uuid": "e4489.10",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "IdeaArena",
            "name_full": "Idea Arena protocol",
            "brief_description": "An evaluation protocol for generated ideas that ensures criteria align with human research preferences (mentioned as prior work to evaluate generated ideas).",
            "citation_title": "",
            "mention_or_use": "mention",
            "evaluation_method_name": "Idea Arena protocol",
            "evaluation_method_description": "A human-aligned evaluation setup to judge generated ideas according to criteria shaped by researcher preferences; used as an example of human-centered evaluation approaches.",
            "evaluation_criteria": "Human preference alignment (quality dimensions defined by researchers), novelty, clarity, and applicability (protocol-specific).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General research ideation",
            "theory_type": "Research ideas/hypotheses",
            "human_comparison": true,
            "evaluation_results": "Mentioned in related work; Claim: Idea Arena helps align evaluations with human research preferences; not used in experiments here.",
            "automated_vs_human_evaluation": "Human-based (protocol designed around human preferences).",
            "validation_method": "Not applied in this paper.",
            "limitations_challenges": "Authors cite reliance on human preference alignment as limited because it lacks objective measures of transformative impact.",
            "uuid": "e4489.11",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ResearchAgent",
            "name_full": "ResearchAgent",
            "brief_description": "An iterative LLM-driven framework that integrates an academic graph and retrieval to refine research ideas, employing multiple LLM reviewing agents for structured feedback aligned to human-defined criteria.",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models.",
            "mention_or_use": "mention",
            "evaluation_method_name": "ResearchAgent iterative evaluation",
            "evaluation_method_description": "Iteratively generates and refines ideas using retrieval and multiple reviewer agents; uses structured feedback aligned to human criteria to improve clarity, novelty, and validity of outputs.",
            "evaluation_criteria": "Clarity, novelty, validity (evaluated via reviewing agents and alignment to human-defined criteria).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General scientific literature",
            "theory_type": "Research ideas/refinements",
            "human_comparison": true,
            "evaluation_results": "Cited as improving generated idea quality through structured review; not experimentally applied in this paper.",
            "automated_vs_human_evaluation": "Hybrid — automated reviewer agents trained/structured to reflect human criteria.",
            "validation_method": "Not validated within this paper; cited from prior work.",
            "uuid": "e4489.12",
            "source_info": {
                "paper_title": "Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A dynamic network measure of technological change",
            "rating": 2,
            "sanitized_title": "a_dynamic_network_measure_of_technological_change"
        },
        {
            "paper_title": "Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses.",
            "rating": 2,
            "sanitized_title": "moosechem_large_language_models_for_rediscovering_unseen_chemistry_scientific_hypotheses"
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models.",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "Idea-bench: How far are generative models from professional designing?",
            "rating": 2,
            "sanitized_title": "ideabench_how_far_are_generative_models_from_professional_designing"
        },
        {
            "paper_title": "Empowering AI as autonomous researchers: Evaluating llms in generating novel research ideas through automated metrics.",
            "rating": 2,
            "sanitized_title": "empowering_ai_as_autonomous_researchers_evaluating_llms_in_generating_novel_research_ideas_through_automated_metrics"
        },
        {
            "paper_title": "Chain of ideas: Revolutionizing research via novel idea development with llm agents.",
            "rating": 1,
            "sanitized_title": "chain_of_ideas_revolutionizing_research_via_novel_idea_development_with_llm_agents"
        }
    ],
    "cost": 0.024208499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations
14 Apr 2025</p>
<p>Junlan Chen 
Sun Yat-sen University
GuangzhouChina</p>
<p>Kexin Zhang 
Sun Yat-sen University
GuangzhouChina</p>
<p>Daifeng Li lidaifeng@mail.sysu.edu.cn 
Sun Yat-sen University
GuangzhouChina</p>
<p>Yangyang Feng 
Sun Yat-sen University
GuangzhouChina</p>
<p>Yuxuan Zhang 
Sun Yat-sen University
GuangzhouChina</p>
<p>Bowen Deng 
Sun Yat-sen University
GuangzhouChina</p>
<p>Structuring Scientific Innovation: A Framework for Modeling and Discovering Impactful Knowledge Combinations
14 Apr 2025567B202A2AAC0C2C15CCA7E343CA733EarXiv:2503.18865v3[cs.AI]Scientific InnovationKnowledge RecombinationLLM ReasoningProblem-Method Structure
The emergence of large language models (LLMs) offers new possibilities for structured exploration of scientific knowledge.Rather than viewing scientific discovery as isolated ideas or content, we propose a structured approach that emphasizes the role of method combinations in shaping disruptive insights.Specifically, we investigate how knowledge units-especially those tied to methodological design-can be modeled and recombined to yield research breakthroughs.Our proposed framework addresses two key challenges.First, we introduce a contrastive learning-based mechanism to identify distinguishing features of historically disruptive method combinations within problemdriven contexts.Second, we propose a reasoning-guided Monte Carlo search algorithm that leverages the chain-of-thought capability of LLMs to identify promising knowledge recombinations for new problem statements.Empirical studies across multiple domains show that the framework is capable of modeling the structural dynamics of innovation and successfully highlights combinations with high disruptive potential.This research provides a new path for computationally guided scientific ideation grounded in structured reasoning and historical data modeling.</p>
<p>Introduction</p>
<p>Recent advances in Large Language Models (LLMs) have significantly enhanced text understanding and generation capabilities [27,33,2], demonstrating expertlevel performance across various domains [3].In scientific discovery, LLMs have been applied to generate research ideas and synthesize existing knowledge [3,39,8,17], confirming their potential in assisting scientific exploration.However, despite these advancements, existing approaches still exhibit several limitations: (1) the inability to systematically identify and integrate fine-grained knowledge components, resulting in scientific discovery that remains at a macro-level of idea generation rather than precise matching of research problems and methods; (2) the hallucination phenomenon in LLMs, where models generate problem-solving approaches that lack actual literature support, potentially leading research efforts astray; and (3) the absence of objective metrics to assess the transformative impact of newly proposed discoveries, as current methods predominantly rely on subjective expert alignment rather than quantitative evaluations of scientific breakthroughs.</p>
<p>Research has shown that the most influential scientific discoveries primarily stem from the combination-particularly atypical combinations-of traditional ideas from prior work [32,34,36].Innovation emerges when prior innovations or their components are assembled into an original design [16,24,35].Schumpeter [31], a pioneer in innovation theory, posited that innovation is fundamentally "the recombination of elements of production", meaning a novel combination of production elements or conditions.Similarly, Nelson and Winter [25] argued that "the creation of novelty in art, science, and technology largely depends on the recombination of pre-existing conceptual and physical materials."In scientific research, research questions and methods serve as fundamental building blocks, and their combination determines the scientific novelty of a publication [22].Despite this, existing studies largely focus on LLM-driven idea generation rather than systematically identifying, filtering, and combining problem-method pairs to enhance the effectiveness of scientific discovery.</p>
<p>To address this gap, we introduce the Disruptive Index (DI) to quantify whether a scientific discovery drives a paradigm shift.Disruptive innovation represents fundamental transformations in scientific and technological progress, distinct from incremental improvements that merely refine existing paradigms.Traditional impact metrics, such as citation counts, primarily measure the extent of a technology's adoption rather than its transformative potential.The Disruptive Index (DI), proposed by Funk and Owen-Smith [11], captures whether a scientific discovery supersedes previous approaches rather than merely reinforcing the status quo.A prominent example is Watson and Crick's (1953) discovery of the DNA double-helix structure, which superseded previous approaches, such as Pauling's triple-helix model, and fundamentally altered the field of molecular biology.Their study, with a DI score of 0.62 [28], exemplifies a highly disruptive scientific breakthrough, validated extensively through expert assessments [11,37].Therefore, beyond relying on LLM-generated research ideas, it is essential to construct a framework that integrates DI-based evaluations to systematically assess the transformative impact of problem-method combinations.</p>
<p>Building upon these insights, we propose a problem-method combination framework for scientific discovery, inspired by how scientific breakthroughs emerge from the recombination of existing knowledge.Given a research question, our framework first retrieves and synthesizes relevant papers, then employs an LLM assistant to determine whether specific papers can serve as sources for new scientific discoveries related to the question and extracts a candidate set of methods.Subsequently, we introduce an innovative disruptive index evaluation framework to quantify the disruptiveness of problem-method combinations.Specifically, through model fine-tuning, our assistant generates combination strategies based on research questions and candidate methods.To evaluate the disruptiveness of these strategies, we identify potential source literature in our database, analyze differences between source strategies and current strategies, and propose an adaptive bias-aware alignment model to predict disruptive indices based on these differences.Finally, we iteratively explore candidate method sets to identify the most disruptive problem-method combinations.</p>
<p>We conduct extensive experiments on publication databases across three scientific domains.Our results demonstrate that the proposed framework outperforms state-of-the-art methods in predicting the disruptiveness of problemmethod combinations.Furthermore, validation on real-world high-disruptiveness publications confirms the framework's ability to identify highly disruptive scientific discoveries.</p>
<p>The primary contributions of this research are as follows:</p>
<p>1.A novel framework for scientific discovery that systematically identifies and integrates problem-method combinations rather than relying solely on LLM-generated research ideas.2. A disruptive index evaluation framework that quantitatively assesses the potential disruptiveness of new scientific discoveries, improving upon traditional impact metrics.3. Extensive experimental validation demonstrating the effectiveness of our approach in identifying high-disruptiveness discoveries across multiple scientific domains.</p>
<p>2 Related Work</p>
<p>LLMs in Scientific Discovery and Research Ideation</p>
<p>Large Language Models (LLMs) have demonstrated significant potential in scientific discovery, particularly in generating novel research ideas.Various benchmarks and frameworks have been developed to evaluate the quality of LLMgenerated research hypotheses.One approach involves the establishment of Ide-aBench, a benchmark designed to standardize the assessment of research ideas produced by LLMs [18].Another method introduces the Creativity Index, which incorporates supervised fine-tuning and Direct Preference Optimization (DPO) to evaluate originality, feasibility, impact, and reliability in idea generation [9].Additionally, the MOOSE-Chem multi-agent framework has been implemented, utilizing problem decomposition strategies to improve the quality of LLM-generated research hypotheses in chemistry [39].These efforts reflect a broader trend toward leveraging LLMs for structured scientific ideation.However, despite advancements in benchmark development and evaluation methodologies, existing approaches remain largely reliant on textual synthesis rather than structured reasoning or methodological integration.</p>
<p>The need for systematic frameworks that enhance the logical progression of research ideation and ensure scientific rigor continues to be a critical challenge.</p>
<p>Beyond direct idea generation, LLMs have been incorporated into structured frameworks to enhance the logical progression of research ideation.One such approach is the Chain-of-Ideas (CoI) Agent, an LLM-based system that organizes relevant literature into a structured chain, simulating the progressive development of a research domain and strengthening ideation capabilities [17].To systematically evaluate generated ideas, the Idea Arena protocol has been designed, ensuring that evaluation criteria align with human research preferences.Experimental results indicate that the CoI Agent outperforms conventional methods and produces research ideas of comparable quality to those generated by human researchers.</p>
<p>Additionally, ResearchAgent has been introduced as an iterative framework that refines research ideas through the integration of an academic graph and knowledge retrieval mechanisms.Multiple LLM-powered reviewing agents are employed to provide structured feedback, aligning evaluations with human-defined criteria.This structured review process enhances the clarity, novelty, and validity of generated ideas, demonstrating effectiveness across multiple disciplines [3].</p>
<p>Despite the substantial advancements of Large Language Models (LLMs) in scientific discovery, existing methods still exhibit several critical limitations.First, current research primarily focuses on generating research hypotheses or ideas but fails to systematically explore the finer-grained composition of methodological elements tailored to specific research questions.This limitation results in a lack of effective knowledge recombination mechanisms, rendering LLMs incapable of constructing genuinely innovative research pathways that align with established scientific methodologies.</p>
<p>Second, current LLM-driven research ideation methods heavily rely on providing extensive background information [17], where LLMs are exposed to a large volume of related literature to enhance their inferential capabilities.While this strategy enriches the contextual breadth of generated content, it compromises the traceability of research ideas, making it difficult to directly associate LLM-generated hypotheses with specific prior studies.Consequently, researchers often face challenges in verifying the theoretical foundations and scientific validity of these generated ideas.</p>
<p>Finally, when evaluating LLM-generated research hypotheses, existing approaches predominantly rely on human preference alignment, where assessments are based on subjective ratings or semantic similarity measures.However, there is a notable lack of objective metrics to rigorously quantify the scientific impact of these ideas.In particular, current research lacks systematic methods to evaluate the transformative potential or disruptive impact of newly generated methodologies, thereby making it challenging to distinguish LLM-generated research ideas from truly groundbreaking scientific discoveries.</p>
<p>These limitations underscore the necessity of developing a more systematic and intelligent scientific discovery framework based on problem-method matching, ensuring traceability, scientific rigor, and quantitative evaluation of research ideas.Such a framework would enable a more structured integration of methodological elements while incorporating quantitative analysis to assess the potential impact of newly generated methodologies.</p>
<p>Knowledge Combination and Recombinant Innovation in Scientific Discovery</p>
<p>Innovation fundamentally arises from the combination and recombination of knowledge [38,30].The concept of knowledge recombination has gained increasing attention in the literature, with over 1,000 articles in top management journals leveraging this framework to analyze scientific innovation [38].Recombinant innovation is regarded as a major driver of new idea generation, and its frequent occurrence in scientific research underscores the necessity of understanding how scientific knowledge is integrated and combined in academic publications [7].</p>
<p>Within the domain of scientific discovery, research questions and research methods serve as the fundamental building blocks that determine the novelty and impact of scientific contributions [22].Existing studies have examined scientific novelty through various combination-based perspectives, but have yet to fully address the temporal evolution and semantic complexity of research questions and methods.To bridge this gap, recent work has proposed a life-index novelty measurement, incorporating the frequency and age of research questions and methods, alongside semantic novelty assessment using deep learning and representation learning techniques [22].These advancements highlight the importance of systematically integrating research questions and methods to characterize scientific novelty.Despite these insights, current methodologies predominantly focus on evaluating novelty at the level of individual concepts, rather than systematically modeling how methodological elements are combined to address specific research questions.While research questions and methods both constitute integral knowledge elements in scientific articles, existing studies rarely explore how their structured integration contributes to groundbreaking discoveries.This limitation suggests the need for a systematic framework that formalizes the combination of research questions and methods to assess their transformative potential.</p>
<p>Moreover, the lack of structured mechanisms for problem-method matching has hindered the ability to predict which methodological innovations lead to significant scientific breakthroughs.Although LLM-based approaches have been employed to generate research ideas, they primarily rely on retrieving or synthesizing prior knowledge, rather than systematically aligning methods with research questions to facilitate novel knowledge recombination.This gap underscores the necessity of developing an intelligent framework that systematically integrates research methods as core knowledge elements and models their structured composition for scientific discovery.</p>
<p>The Necessity of the Disruption Index (DI) in Evaluating Scientific Breakthroughs</p>
<p>Traditional metrics for assessing the impact of scientific research, such as citation counts, h-index, and journal impact factor, have long been used as standard measures of scientific influence.However, these indicators primarily capture the magnitude of a study's dissemination rather than its ability to challenge existing paradigms [13? , 40].While citation counts are intuitive and widely adopted, they suffer from inherent limitations, including bias toward incremental research, ignoring negative citations, and reinforcing conservative citation behavior [6,26,10].Consequently, traditional bibliometric indicators often fail to distinguish between studies that reinforce the status quo and those that disrupt established knowledge structures.</p>
<p>To address these limitations, Funk and Owen-Smith (2017)introduced the Disruption Index (DI) as a metric to quantify the extent to which new technological advancements displace or reinforce existing knowledge [11].Inspired by prior literature on technological shifts, they argued that the dichotomy between competence-enhancing and competence-destroying innovations was insufficient for characterizing real-world technological evolution.Instead, they proposed that disruptiveness exists on a continuum, where some innovations incrementally improve existing knowledge, while others render previous technologies obsolete [1].</p>
<p>Originally developed to measure technological innovation using vast patent databases such as the U.S. Patent Citations Data File, the Disruption Index was later extended to scientific research by Wu et al. (2019), who applied the metric to bibliometrics [37].They demonstrated that DI could effectively differentiate groundbreaking discoveries from incremental advancements by analyzing its values in Nobel Prize-winning papers and comparing disruption levels between review papers and their original research articles.</p>
<p>The DI quantifies disruptiveness using the following formulation:
D = n i − n j n i + n j + n k ,(1)
where n i is the number of papers that cite the focal paper exclusively, n j represents papers citing both the focal paper and its references, and n k denotes papers that cite only the references of the focal paper [37].This formulation allows DI to capture the extent to which a research contribution redefines its field, rather than merely accumulating citations.</p>
<p>The necessity of DI stems from its ability to quantitatively evaluate scientific breakthroughs, offering a more precise alternative to traditional citationbased measures.As disruptive innovation is characterized by a paradigm shift that redirects collective attentio*, DI provides a robust framework for distinguishing transformative research from incremental progress [20].Given that existing LLM-based research ideation models primarily focus on generating novel ideas without evaluating their potential to challenge existing scientific conventions, integrating DI into scientific discovery frameworks can significantly enhance the assessment of research novelty and impact.These considerations highlight the limitations of existing citation-based indicators in capturing scientific breakthroughs and underscore the importance of incorporating DI into intelligent scientific discovery frameworks.A disruptionaware approach could enable more systematic evaluations of research impact, ensuring that novel problem-method combinations are assessed not just for their feasibility but also for their potential to drive substantial scientific advancements.</p>
<p>Our research proposes a novel scientific discovery paradigm that not only provides methodological support for evaluating scientific innovation but also establishes a new technological paradigm for intelligent research tools.Through question-methodology combinatorial logic, we have constructed a framework that achieves objective quantitative assessment of the disruptive potential of research ideas and generates corresponding reasoning chains.This study introduces an improved framework with three core modules to enhance disruptive knowledge prediction and method combination exploration.</p>
<p>Methodology</p>
<p>The Problem-Driven Method Exploration Module identifies potential method combinations based on specific research questions, providing innovative and targeted strategies.</p>
<p>The Disruptive Knowledge Prediction Module predicts the disruptive potential of given problems and methods using a deviation-awareness mechanism and a secondary learning approach to ensure accuracy and reliability.</p>
<p>The Dynamic Method Optimization Module iteratively refines method combinations based on disruptive index feedback, enhancing their disruptive potential.</p>
<p>This framework offers a systematic approach for researchers to uncover disruptive knowledge and drive scientific innovation.</p>
<p>Problem-Driven Method Exploration Module</p>
<p>To enhance the efficiency of method exploration and reduce resource consumption, this study designs a Problem-Driven Method Exploration Module that constructs a paper database indexed by problems and methods.Traditional approaches often require repeated evaluation of the relationship between new problems and paper abstracts, which not only increases computational costs but also reduces retrieval efficiency.To address this, we propose an efficient retrieval mechanism that rapidly identifies potential method candidates relevant to specific research problems.</p>
<p>The paper database is built upon a large-scale collection of academic literature.First, we preprocess textual information such as paper titles, abstracts, and keywords to remove redundant and noisy data, ensuring data accuracy and consistency.Subsequently, natural language processing (NLP) techniques are employed to extract the research problem and research method from each paper.These two elements are then indexed as key entries to facilitate efficient retrieval through a problem-driven approach.</p>
<p>During the method exploration process, when a new research problem P new is proposed, the system embeds it into a semantic vector space:
v Pnew = Embed(P new )
Then, a similarity function is applied to retrieve the top-k similar problems:
P sim = {P i | sim(v Pnew , v Pi ) ≥ δ}
The associated methods M sim = {M i | P i ∈ P sim } are collected, and a heuristic filtering mechanism H is applied:
M final = H(M sim , sim(•), rule(•))
Finally, the system constructs candidate problem-method pairs:
C = {(P new , M ) | M ∈ M final }
These selected candidates support downstream disruptive knowledge prediction and method optimization.</p>
<p>Disruptive Index Prediction Model Methodology</p>
<p>This study introduces a disruptive index prediction model consisting of interconnected sub-modules designed to precisely evaluate the innovative potential of specific problem-method combinations.Specifically, the overall module comprises three sub-modules: problem-method summary generation, identification, extraction, and refinement of key reference information, and final prediction of the disruptive index.</p>
<p>The first sub-module aims to automatically generate highly concise summaries for given problem-method pairs.To enhance the accuracy and logical coherence of generated summaries, we employ a summary generation model fine-tuned using Low-Rank Adaptation (LoRA) [15].This model effectively learns from existing real-world literature summaries within a targeted downstream task context.Additionally, a set of meticulously crafted prompts guide the model stepby-step in describing the logical relationships and details involved in combining problems and methods (detailed prompt design is provided in the Appendix).</p>
<p>Formally, given a problem-method pair (P, M ) and a task-specific prompt Prompt, the summary generation model G θ produces a concise summary S:
S = G θ (P, M, Prompt)
Through this approach, our trained model significantly outperforms current state-of-the-art methods, demonstrating superior logical consistency and completeness of information.</p>
<p>The second sub-module integrates both the identification of key reference documents and the extraction of essential information.Given the computationally intensive nature of precise matching across large literature databases, we initially filter candidate references based on the semantic similarity between the given problem-method combination and the problem-method indices from a structured literature database.</p>
<p>Let D be the structured database and (P, M ) the input pair.We first compute the similarity and select the top-k references:
R top = {R i ∈ D | sim((P, M ), (P i , M i )) ≥ δ}
This approach ensures that only references with highly relevant problemmethod associations are considered, constraining the selection to 100 references.Subsequently, we employ a frozen pre-trained model to perform a fine-grained semantic comparison between the generated summaries and candidate reference summaries, thereby accurately identifying the key source references relevant to the problem-method combination.</p>
<p>Let F denote the frozen model and S the generated summary.We obtain the extracted information I:
I = F (S, R top )
Once the key references are identified, we further refine the extracted information to reduce computational burden and minimize noise introduced by large-scale raw textual inputs.Inspired by the RAHA approach [19], we leverage a frozen pre-trained model to compare the generated summary with the identified reference summaries and extract only the most critical, relevant information required for the model input.This integrated method effectively captures complex citation relationships while ensuring the quality of the extracted input information, ultimately improving the efficiency and accuracy of subsequent predictions.</p>
<p>The third sub-module utilizes the generated problem-method summaries and extracted critical information to predict the disruptive index for problem-method combinations.This sub-module employs a prediction model fine-tuned via LoRA.During fine-tuning, supervision is provided using real summaries and extracted reference information.Formally, the prediction model D ϕ outputs a disruption score y based on S and I: y = D ϕ (S, I)</p>
<p>Due to the scarcity of highly disruptive papers in real-world datasets, potentially biasing predictions towards low-disruptive outcomes, we introduce an entropy-based weighted evaluation metric [4].</p>
<p>The weighted loss for entropy-aware learning is given by:
L entropy = N i=1 w i • ℓ(y i , ŷi ), w i = − log(p(ŷ i ))
This metric assigns higher weights to rare but highly disruptive samples, enhancing the model's capability to identify high-disruptiveness cases.</p>
<p>Furthermore, we design a secondary learning mechanism for challenging-toclassify samples, selecting the top 20% of instances with the highest prediction errors for reinforcement training.To prevent prediction drift caused by secondary training, we introduce a KL-divergence-based balancing mechanism to stabilize training outcomes.The KL divergence loss between primary and secondary distributions is defined as:
L KL = D KL (D primary ϕ ∥ D secondary ϕ )
To further stimulate the cognitive and evaluative capabilities of the model, we propose an iterative deviation-awareness mechanism.Specifically, after each prediction iteration, the results are fed back to the model, prompting self-awareness and evaluation of prediction deviations.Let θ t be the model parameters at iteration t, then parameter update is guided by the deviation gradient:
θ t+1 = θ t − η • ∇Deviation(ŷ t , y t )
Experimental results demonstrate that, compared to existing state-of-the-art methods, our proposed framework significantly and comprehensively improves performance in predicting the disruptive index of problem-method combinations.</p>
<p>Dynamic Method Optimization Module</p>
<p>The dynamic method optimization module is designed to iteratively refine method combinations based on feedback from the disruptive index, enhancing their disruptive potential over multiple optimization cycles.This process ensures that method selection and adjustments remain continuously optimized within the problem-method space to maximize impact.</p>
<p>At the core of this module lies a feedback-driven iterative optimization mechanism.Specifically, the system evaluates the disruptive index of a given problemmethod combination at each iteration and utilizes this information to guide subsequent modifications.</p>
<p>Formally, let (P, M t ) represent the problem and current method configuration at iteration t, and let y t be the corresponding disruptive index:
y t = D ϕ (S t , I t )
The optimization process follows a greedy algorithm, selecting adjustments in each iteration that locally maximize the disruptive index:
M t+1 = arg max M ′ ∈N (Mt) D ϕ (S ′ , I ′ )
where N (M t ) denotes the neighborhood of candidate method variants generated by replacing, augmenting, or modifying components of M t .By progressively favoring configurations that yield higher indices, the framework systematically converges towards method combinations with enhanced disruptive impact.</p>
<p>However, traditional greedy algorithms are prone to getting trapped in local optima, leading to stagnation in suboptimal solutions.To address this limitation, we incorporate a Greedy with Probabilistic Perturbation (GPP) approach [14], enhancing the global search capability.</p>
<p>In GPP, with a small probability ϵ, a non-optimal method M rand ∈ N (M t ) is accepted:
M t+1 = arg max M ′ D ϕ (S ′ , I ′ ), with probability 1 − ϵ M rand , with probability ϵ
This probabilistic mechanism allows the optimization process to escape local optima and explore method configurations with long-term disruptive potential.By leveraging this stochastic perturbation strategy, the optimization process effectively balances local search with global exploration.</p>
<p>Furthermore, to prevent stagnation in local optima and ensure comprehensive exploration of the method space, the optimization process integrates adaptive constraints.</p>
<p>Let C(M t ) denote constraint-based penalty terms for overfitting, we define the total objective with regularization as:
L opt = −D ϕ (S t , I t ) + λ • C(M t )
This ensures that method updates remain meaningful and generalizable across problem contexts.</p>
<p>The optimization module also includes an adaptive learning component that dynamically adjusts the weight of disruptive index feedback based on observed trends over multiple iterations.Let w t denote the weight at time t, updated based on temporal smoothing:
w t+1 = α • w t + (1 − α) • y t
This mechanism enables the system to prioritize consistently improving adjustments while attenuating noise introduced by short-term evaluation anomalies.</p>
<p>Empirical evaluations indicate that the incorporation of the GPP mechanism effectively enhances the global search capability, enabling the algorithm to escape local optima while maintaining efficient optimization performance.Overall, this dynamic optimization strategy significantly improves the identification and refinement of disruptive method combinations.By integrating iterative feedback, local optimization, adaptive constraints, and stochastic perturbation, this module provides a more efficient and systematic solution for method selection, ultimately fostering the discovery of high-impact scientific innovations.</p>
<p>Experiments</p>
<p>Data Sources</p>
<p>To evaluate the effectiveness of our proposed framework, we conduct experiments on three citation-based datasets: DBLP, PubMed, and PatSnap.Given the broad scope of these datasets, we focus on specific domains to ensure targeted analysis.</p>
<p>For DBLP, we extract records from 2011 to 2021 covering 14,533 publications from CCF-A conferences in the field of artificial intelligence.From PubMed, we select 96,612 research articles related to depression, published between 2015 and 2025.Lastly, for PatSnap, we use 6,677 patent records on medical robotics, with legal status marked as active, covering the period from 2020 to 2025.Further details on dataset characteristics and preprocessing are provided in Appendix A.</p>
<p>Baselines</p>
<p>To comprehensively assess the performance of our framework, we compare it against a set of established baselines, including both general-purpose large language models and specialized pre-trained models for scientific and technical domains.</p>
<p>We consider the following baselines:</p>
<p>(1) General-purpose LLMs: GPT and Claude, widely used for natural language understanding and text generation tasks.</p>
<p>(2) SciBERT [29], a pre-trained language model designed specifically for scientific text processing, which has demonstrated strong performance in scientific literature comprehension and reasoning tasks.</p>
<p>(3) RoBERTa [21], an optimized variant of BERT that enhances training robustness and performance across multiple NLP tasks.</p>
<p>(4) LLaMA 3 [12], the latest iteration in the LLaMA series of large-scale language models, which offers improved efficiency and reasoning capabilities.</p>
<p>(5) Qwen-7B [5], an autoregressive generative language model based on masked language modeling, optimized for diverse text generation and completion tasks.</p>
<p>All of these models are publicly accessible, allowing for reproducible benchmarking and comparative evaluation of our proposed framework.</p>
<p>Experimental Setup</p>
<p>Our experiments are conducted using PyTorch on four NVIDIA A800 GPUs.The ablation study is performed based on Qwen-7B.The model optimization is implemented using the Adam optimizer, with a learning rate set to 1e-5 and a gradient clipping threshold fixed at 0.2.</p>
<p>For model configurations, the problem-method summary generation model is set to handle a maximum input length of 1000 tokens, while the disruptive index prediction model is configured with a maximum input length of 7000 tokens.The batch size is consistently set to 4 across all experiments.The adapter used in the second LLM is configured with a low-rank dimension of 64.</p>
<p>We employ the PEFT (Parameter-Efficient Fine-Tuning) library to insert adapters into the last attention or feedforward layers of the LLM [23].This analysis is performed based on a principled examination of the forward components.</p>
<p>Both training and testing iterations are set to K = 5.For other baseline models, the number of training epochs is fixed at 5, and the optimal model checkpoint is selected based on validation set performance metrics.</p>
<p>Main Results</p>
<p>We present the main results on the DBLP, PubMed, and PatSnap datasets in Table 1.Our framework, which integrates two models along with the overall system, consistently outperforms existing state-of-the-art LLMs and PLMs across multiple evaluation metrics.Table 1 reports the cosine similarity and ROUGE scores between problem-method summaries generated by our framework and their corresponding ground-truth summaries.We use Qwen-32B as an example for evaluating the summarization performance.The fine-tuned model not only surpasses existing general-purpose LLMs such as GPT and Claude but also demonstrates superior performance over non-fine-tuned pre-trained models.</p>
<p>This confirms the effectiveness of our approach in refining the alignment between problem-method pairs and their corresponding textual representations.As shown in Table 2, we evaluate the effectiveness of our disruptive index prediction model based on four key metrics: MSE, MAE, weighted MSE (WMSE), and weighted MAE (WMAE).Our model, incorporating adaptive bias awareness and secondary sample learning, achieves lower error rates across all metrics, demonstrating its superiority over general-purpose LLMs, pre-trained language models (PLMs), and large language models (LLMs) fine-tuned on scientific tasks.The improvements in these evaluation metrics indicate that our framework effectively captures the disruptive potential of problem-method combinations with higher accuracy and robustness.As shown in Table 3, our full framework, designed for disruptive index prediction, outperforms general LLMs across all evaluation metrics.The consistent improvements in MSE, MAE, WMSE, and WMAE confirm the effectiveness of integrating problem-method pairs and disruptive index prediction to enhance scientific discovery.</p>
<p>Ablation Study</p>
<p>To analyze the contributions of individual components within our framework, we conduct an ablation study, as shown in Table 4. (1) Framework w/o fine-tuning the problem-method summarization model: Removing fine-tuning from the problem-method summarization model results in a noticeable performance degradation across all datasets.This demonstrates the importance of task-specific adaptation in improving the alignment between problem-method pairs and their textual representations.Without fine-tuning, the generated summaries exhibit lower quality, impacting the overall framework's ability to capture meaningful research insights.</p>
<p>(2) Framework w/o relevance assessment and information extraction: Excluding the step of relevance judgment and structured information extraction significantly reduces the effectiveness of disruptive index prediction.The removal of this module leads to an increase in MSE and MAE, as the model is unable to accurately capture contextual knowledge essential for evaluating problem-method disruptiveness.This highlights the necessity of refining input information before disruptive potential estimation.</p>
<p>(3) Framework w/o secondary learning: The absence of secondary learning-where high-error samples undergo further training-results in higher prediction errors across all four key evaluation metrics: MSE, MAE, WMSE, and WMAE.This confirms that the secondary learning mechanism enhances model robustness by mitigating the impact of hard-to-classify instances, ensuring better generalization.</p>
<p>(4) Framework w/o deviation-aware alignment: Removing deviationaware alignment leads to a decline in performance by increasing error rates and reducing model consistency.Without this module, the framework struggles to adjust predictions based on previously observed discrepancies, limiting its ability to refine predictions dynamically.</p>
<p>Overall, the ablation study confirms that each component plays a crucial role in improving the framework's predictive performance.The degradation in results when removing any of these modules underscores their importance in systematically enhancing problem-method integration and disruptive potential assessment.</p>
<p>Greedy Algorithm Optimization Results</p>
<p>To evaluate the effectiveness of our dynamic method optimization module, we conduct experiments measuring its ability to identify high-disruptiveness method combinations.Table 5 reports the hit rate (Disruptive Index &gt; 0.5) on three datasets.</p>
<p>Our framework leverages a Greedy with Probabilistic Perturbation (GPP) approach to iteratively optimize problem-method combinations based on the disruptive index.Compared to a standard greedy algorithm, GPP significantly improves the ability to escape local optima, resulting in superior method selection.Specifically, the model incorporating GPP achieves a higher disruptive index score across all datasets, demonstrating improved long-term optimization capabilities.The observed performance gains are attributed to two key advantages of GPP: (1) Balancing exploration and exploitation, where the probabilistic acceptance of non-optimal choices prevents premature convergence, and (2) Adaptive weighting of disruptive index feedback, which enables a more refined and responsive optimization trajectory.These mechanisms collectively enhance the ability of our framework to discover novel, high-impact problem-method combinations.</p>
<p>Conclusion</p>
<p>In this study, we propose a novel framework for scientific discovery that systematically integrates problem-method combinations with disruptive index prediction.Our approach leverages fine-tuned LLMs for problem-method summarization, an adaptive bias-aware alignment model for disruptive index estimation, and a dynamic optimization strategy incorporating Greedy with Probabilistic Perturbation (GPP) to iteratively refine method selection.</p>
<p>Empirical results on DBLP, PubMed, and PatSnap confirm the effectiveness of our framework.Compared to existing general-purpose LLMs, pre-trained language models, and baseline methods, our approach consistently achieves higher accuracy in problem-method summarization, disruptive index prediction, and high-impact method identification.The introduction of GPP significantly enhances search efficiency by balancing exploitation and exploration, ensuring the discovery of truly novel and disruptive scientific insights.</p>
<p>Our findings contribute to the advancement of AI-driven scientific discovery by demonstrating the value of structured problem-method integration and adaptive learning strategies.Future research may explore expanding the framework to broader domains and improving interpretability to further assist researchers in generating groundbreaking discoveries.</p>
<p>Limitations</p>
<p>While our proposed framework demonstrates strong performance in integrating problem-method combinations with disruptive index prediction, it has two primary limitations.</p>
<p>First, for entirely emerging scientific fields with minimal prior work, our framework may encounter challenges due to a lack of sufficient historical data.The effectiveness of the problem-method integration and disruptive index prediction relies on existing structured research literature.In domains with scarce prior knowledge, the search space for potential method combinations becomes significantly larger, reducing search efficiency and increasing the likelihood of suboptimal results.</p>
<p>Second, our framework involves a multi-step process that includes problemmethod summarization, source validation, information extraction, secondary learning, and deviation-aware alignment.While each step enhances accuracy, it also increases computational complexity and execution time.The sequential nature of these processes results in higher processing overhead, which may limit the scalability of our approach when applied to large-scale real-time applications.</p>
<p>Future research should explore ways to mitigate these limitations, including optimizing search strategies for data-scarce fields and improving computational efficiency through parallelization and adaptive learning techniques.</p>
<p>Fig. 1 .
1
Fig. 1.Enter Caption</p>
<p>Table 1 .
1
Comparison of Question-Method Pair Summarization Performance Across Different Datasets Using Cosine Similarity and ROUGE
ModelDBLPPubMedPatentSimilarity ROUGE Similarity ROUGE Similarity ROUGEGPT-4o0.5030.2060.4320.2250.4470.133GPT-4 Turbo0.5200.2050.3450.1870.4360.133Claude 3.50.4690.2040.3810.1540.3430.095Claude 3.70.4600.2140.4910.1390.2990.085Qwen-32B (Pre-trained) 0.5520.3150.4230.2420.4560.158Qwen-32B (Fine-tuned)0.5580.3250.5000.3240.6120.404</p>
<p>Table 2 .
2
Disruptive Index Prediction Using Summarization and Information Across Different Datasets Using MSE, MAE, WMSE, and WMAE
ModelDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEGPT-4o0.3607 0.5728 0.5821 0.7175 0.3255 0.5369 0.2321 0.4026 0.3018 0.5162 0.0884 0.1770GPT-4 Turbo0.3607 0.5728 0.5821 0.7175 0.3549 0.5621 0.1742 0.2919 0.3455 0.5594 0.6520 0.7707Claude 3.50.4346 0.6115 0.3162 0.4318 0.4269 0.6077 0.3029 0.4219 0.4242 0.5887 0.9875 0.9295Claude 3.70.4699 0.6580 0.2653 0.4131 0.4751 0.6593 0.2728 0.3773 0.4242 0.5887 0.9875 0.9295SciBERT0.3125 0.4153 0.3641 0.3817 0.4218 0.1642 0.3486 0.4357 0.4871 0.5092 0.4217 0.4561RoBERTa0.4351 0.5715 0.3105 0.3751 0.4017 0.4521 0.3465 0.4213 0.4154 0.5184 0.4156 0.4364LLaMA 30.5612 0.6143 0.3155 0.4182 0.4832 0.5961 0.5942 0.4118 0.5624 0.7334 0.3284 0.3912Qwen-7B0.6845 0.8223 0.4558 0.4526 0.4839 0.6587 0.1587 0.2908 0.6843 0.8151 0.5172 0.5241SciBERT(Fine-tuned) 0.0142 0.0247 0.5472 0.6781 0.0093 0.0145 0.3148 0.4207 0.0135 0.0241 0.4151 0.5124RoBERTa(Fine-tuned) 0.0091 0.0154 0.6245 0.6578 0.0075 0.0921 0.2947 0.4814 0.0183 0.0214 0.4521 0.4873LLaMA 3(Fine-tuned) 0.0124 0.0325 0.5175 0.5412 0.0091 0.0124 0.3541 0.4168 0.0265 0.3457 0.5142 0.5321Qwen-7B (Fine-tuned) 0.0052 0.0121 0.6172 0.6739 0.0020 0.0072 0.2533 0.4604 0.0144 0.0181 0.4325 0.4512</p>
<p>Table 3 .
3
Utilizing Question-Method Pairs to Predict Disruptive Index Results with MSE, MAE, WMSE, and WMAE Metrics
ModelDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEGPT-4o0.1191 0.3062 0.5887 0.7399 0.2053 0.4182 0.6545 0.7036 0.1216 0.3044 1.0043 0.9041GPT-4 Turbo 0.1597 0.3628 0.4771 0.6453 0.3289 0.5196 0.3690 0.5558 0.1523 0.3556 1.0803 0.9312Claude 3.50.2291 0.4268 0.9919 0.8767 0.1731 0.3742 1.0523 0.9006 0.2176 0.4201 1.2142 0.9428Claude 3.70.4319 0.6143 0.1355 0.3382 0.1146 0.3021 0.6827 0.7224 0.4594 0.6559 0.1437 0.3431Our Framework 0.0093 0.0111 0.0941 0.1364 0.0154 0.0342 0.1147 0.2142 0.0218 0.0412 0.1241 0.2962</p>
<p>Table 4 .
4
Ablation study of our framework using Qwen-7B across DBLP, PubMed, and Patent datasets with MSE, MAE, WMSE, and WMAE metrics.
Model VariantDBLPPubMedPatentMSE MAE WMSE WMAE MSE MAE WMSE WMAE MSE MAE WMSE WMAEFull Framework (Ours)0.0093 0.0111 0.0941 0.1364 0.0154 0.0342 0.1147 0.2142 0.0218 0.0412 0.1241 0.2962w/o Summarization Fine-tuning 0.0274 0.0461 0.1403 0.2093 0.0412 0.0623 0.1889 0.2563 0.0586 0.0721 0.2034 0.3121w/o Relevance + Extraction0.0351 0.0592 0.1881 0.2672 0.0526 0.0783 0.2102 0.2907 0.0735 0.0897 0.2345 0.3374w/o Secondary Learning0.0187 0.0329 0.1187 0.1766 0.0291 0.0503 0.1506 0.2384 0.0375 0.0594 0.1741 0.3022w/o Deviation-Aware Alignment 0.0216 0.0382 0.1279 0.1917 0.0318 0.0562 0.1663 0.2496 0.0414 0.0638 0.1862 0.3195</p>
<p>Table 5 .
5
Hit Rate (%) of High-Disruptiveness Method Combinations (Disruptive Index &gt; 0.5) Identified by Various Methods
MethodDBLP PubMed PatSnapGPT-4o (ChatGPT)16.4% 18.2% 14.9%Claude 3.515.7% 17.1% 14.3%Claude 3.717.2% 19.0% 15.6%Standard Greedy19.3% 21.5% 18.2%Greedy + GPP (Ours)26.3% 28.1% 24.6%Improvement over best LLM +9.1% +9.1% +9.0%
J. Chen et al.</p>
<p>Extra credit for disruption: Trend of disruption in radiology academic journals. A Abu-Omar, P Kennedy, M Yakub, J Robbins, A Yassin, N Verma, M Scaglione, F Khosa, Clinical Radiology. 77122022</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. J Baek, S K Jauhar, S Cucerzan, S J Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>A characterization of entropy in terms of information loss. J C Baez, T Fritz, T Leinster, Entropy. 13112011</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, B Hui, L Ji, M Li, J Lin, R Lin, D Liu, G Liu, C Lu, K Lu, J Ma, R Men, X Ren, X Ren, C Tan, S Tan, J Tu, P Wang, S Wang, W Wang, S Wu, B Xu, J Xu, A Yang, H Yang, J Yang, S Yang, Y Yao, B Yu, H Yuan, Z Yuan, J Zhang, X Zhang, Y Zhang, Z Zhang, C Zhou, J Zhou, X Zhou, T Zhu, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>The incidence and role of negative citations in science. C Catalini, N Lacetera, A Oettl, Proceedings of the National Academy of Sciences. 112452015</p>
<p>Scientific knowledge combination in networks: New perspectives on analyzing knowledge absorption and integration. H Chen, J Liu, Z Liu, EEKE/AII@ JCDL. 2023</p>
<p>Empowering ai as autonomous researchers: Evaluating llms in generating novel research ideas through automated metrics. D Dasgupta, A Mondal, P P Chakrabarti, 2nd AI4Research Workshop: Towards a Knowledgegrounded Scientific Research Lifecycle. </p>
<p>Empowering AI as autonomous researchers: Evaluating LLMs in generating novel research ideas through automated metrics. D Dasgupta, A Mondal, P P Chakrabarti, 2nd AI4Research Workshop: Towards a Knowledge-grounded Scientific Research Lifecycle. 2024</p>
<p>Tradition and innovation in scientists' research strategies. J G Foster, A Rzhetsky, J A Evans, American sociological review. 8052015</p>
<p>A dynamic network measure of technological change. R J Funk, J Owen-Smith, Management science. 6332017</p>
<p>A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>The impact of collaboration and knowledge networks on citations. J Guan, Y Yan, J J Zhang, Journal of Informetrics. 1122017</p>
<p>A probabilistic greedy search algorithm for combinatorial optimisation with application to the set covering problem. M Haouari, J Chaouachi, Journal of the Operational Research Society. 5372002</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, ICLR. 1232022</p>
<p>The burden of knowledge and the "death of the renaissance man": Is innovation getting harder?. B F Jones, The Review of Economic Studies. 7612009</p>
<p>L Li, W Xu, J Guo, R Zhao, X Li, Y Yuan, B Zhang, Y Jiang, Y Xin, R Dang, arXiv:2410.13185Chain of ideas: Revolutionizing research via novel idea development with llm agents. 2024arXiv preprint</p>
<p>C Liang, L Huang, J Fang, H Dou, W Wang, Z F Wu, Y Shi, J Zhang, X Zhao, Y Liu, arXiv:2412.11767Idea-bench: How far are generative models from professional designing?. 2024arXiv preprint</p>
<p>C Lin, J Ren, G He, Z Jiang, H Yu, X Zhu, arXiv:2402.08874Recurrent alignment with hard attention for hierarchical text rating. 2024arXiv preprint</p>
<p>New directions in science emerge from disconnection and discord. Y Lin, J A Evans, L Wu, Journal of Informetrics. 1611012342022</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Combination of research questions and methods: A new measurement of scientific novelty. Z Luo, W Lu, J He, Y Wang, Journal of Informetrics. 1621012822022</p>
<p>Peft: State-of-the-art parameter-efficient fine-tuning methods. S Mangrulkar, S Gugger, L Debut, Y Belkada, S Paul, B Bossan, Peft: State-of-the-art parameter-efficient fine-tuning methods. 2022</p>
<p>A new method for identifying recombinations of existing knowledge associated with high-impact innovation. S Mukherjee, B Uzzi, B Jones, M Stringer, Journal of Product Innovation Management. 3322016</p>
<p>An evolutionary theory of economic change. R R Nelson, S G Winter, 1985harvard university press</p>
<p>Global citation inequality is on the rise. M W Nielsen, J P Andersen, Proceedings of the National Academy of Sciences. 1187e20122081182021</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 352022</p>
<p>Papers and patents are becoming less disruptive over time. M Park, E Leahey, R J Funk, Nature. 61379422023</p>
<p>N Reimers, I Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>Search and recombination process to innovate: a review of the empirical evidence and a research agenda. T Savino, A Messeni Petruzzelli, V Albino, International Journal of Management Reviews. 1912017</p>
<p>Business cycles: A theoretical, historical and statistical analysis of the capitalist process. J A Schumpeter, Acessado em. 41939. 1964</p>
<p>Creativity in science and the link to cited references: Is the creative potential of papers reflected in their cited references. I Tahamtan, L Bornmann, Journal of informetrics. 1232018</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Atypical combinations and scientific impact. B Uzzi, S Mukherjee, M Stringer, B Jones, Science. 34261572013</p>
<p>Collaboration and creativity: The small world problem. B Uzzi, J Spiro, American journal of sociology. 11122005</p>
<p>Bias against novelty in science: A cautionary tale for users of bibliometric indicators. J Wang, R Veugelers, P Stephan, Research Policy. 4682017</p>
<p>Large teams develop and small teams disrupt science and technology. L Wu, D Wang, J A Evans, Nature. 56677442019</p>
<p>A knowledge recombination perspective of innovation: review and new research directions. T Xiao, M Makhija, S Karim, Journal of Management. 4862022</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Z Yang, W Liu, B Gao, T Xie, Y Li, W Ouyang, S Poria, E Cambria, D Zhou, arXiv:2410.070762024arXiv preprint</p>
<p>Team size, research variety, and research performance: do coauthors' coauthors matter. N Zhu, C Liu, Z Yang, Journal of Informetrics. 1541012052021</p>            </div>
        </div>

    </div>
</body>
</html>
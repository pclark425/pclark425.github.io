<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1324 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1324</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1324</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-4625628163a2ee0e6cd320cd7a14b4ccded2a631</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4625628163a2ee0e6cd320cd7a14b4ccded2a631" target="_blank">Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper develops an off-policy meta-RL algorithm that disentangles task inference and control and performs online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience.</p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. The also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1324.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1324.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic Embeddings for Actor-Critic RL (PEARL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-policy meta-reinforcement learning algorithm that disentangles task inference from control via a probabilistic latent context Z; uses amortized variational inference to maintain a posterior over tasks and leverages posterior sampling over Z to perform temporally-extended (structured) exploration and fast adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PEARL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Off-policy actor-critic meta-RL that conditions an SAC actor pi_theta(a|s,z) and critic Q_theta(s,a,z) on a learned probabilistic latent context z inferred by an amortized, permutation-invariant encoder q_phi(z|c) (Gaussian factorization over transitions). The encoder is trained with critic gradients and a KL information bottleneck; the actor and critic are trained with off-policy replay while context batches are sampled from recent on-policy data.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Posterior sampling (Thompson sampling) over a probabilistic latent task variable (Z); probabilistic latent-context inference enabling temporally-extended exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At meta-test time sample z ~ prior r(z) for the first episode (holding z fixed for the episode) to generate diverse hypotheses and temporally-extended exploration; collect trajectory data, append to context c, update posterior q_phi(z|c), then sample new z for subsequent episodes—thus adapting by hypothesis sampling and posterior update using observed transitions (online Bayesian-style filtering via the amortized encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Various continuous-control meta-RL benchmarks (Half-Cheetah-Dir, Half-Cheetah-Vel, Humanoid-Dir-2D, Ant-Fwd-Back, Ant-Goal-2D, Walker-2D-Params) and a sparse 2D navigation goal-finding task.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Tasks drawn from a distribution of MDPs with unknown transition and/or reward functions (i.e., the task/MDP is latent → POMDP viewpoint); continuous state and action spaces; some benchmarks involve varying dynamics (system parameter randomization) or varying reward/goals; sparse rewards in 2D navigation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>High-dimensional continuous-control robot domains (Half-Cheetah, Ant, Humanoid, Walker) with episode horizon 200; meta-datasets of up to 100 training tasks and 30 test tasks for some domains; 2D navigation is lower-dimensional but sparse-reward and requires multi-trajectory adaptation (goal radius tested at 0.2 and 0.8).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>PEARL achieves 20–100× improvement in meta-training sample efficiency over prior meta-RL baselines and improves asymptotic performance by ~50–100% in five of six continuous-control domains (exact returns per domain in paper figures). In the sparse 2D navigation task PEARL typically begins to adapt after ~5 trajectories and attains higher final returns than MAESN; meta-training required ≈1e6 timesteps for PEARL on the sparse navigation task compared to ≈1e8 timesteps reported for MAESN.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines (on-policy meta-RL methods such as ProMP and MAML-TRPO, and recurrence-based RL^2) require 20–100× more meta-training samples to reach similar performance; MAESN uses ≈1e8 timesteps in the sparse navigation experiment versus PEARL's ≈1e6. A deterministic (point-estimate) version of the latent context fails on the sparse navigation task (unable to explore effectively).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Meta-training: PEARL converges using roughly 1e6 environment timesteps on sparse navigation and is reported 20–100× more sample efficient than on-policy baselines across continuous-control benchmarks; adaptation: aggregation of two trajectories is used to compute reported test-time performance (they report average returns after two aggregated trajectories), and in sparse navigation meaningful adaptation begins after ≈5 trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration is driven by epistemic uncertainty over latent task variable Z: sampling z from the prior/posterior (Thompson sampling) produces temporally-extended, hypothesis-driven exploration (act optimally under sampled hypothesis for an episode). Exploitation emerges as posterior uncertainty narrows and z-constrained policies act more optimally. A KL bottleneck on q(z|c) constrains information and regularizes exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>ProMP (proximal meta-policy search), MAML-TRPO (model-agnostic meta-learning with TRPO), RL^2 (recurrent meta-learner), MAESN (Gupta et al., 2018).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Probabilistic latent context + posterior sampling yields structured, temporally-extended exploration enabling rapid adaptation in partially observable/latent-task settings; decoupling encoder (inference) from policy allows efficient off-policy meta-training; careful context data sampling (recent on-policy transitions, distinct from RL minibatches) is crucial; deterministic latent contexts or fully off-policy context sampling hurt performance; PEARL attains major improvements in meta-training sample efficiency (20–100×) and better asymptotic performance across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Meta-training with sparse rewards is challenging — the authors used dense rewards during meta-training for the sparse navigation experiments (limitation). Sampling context fully off-policy significantly degrades performance. Deterministic latent context fails on sparse-reward tasks. Recurrent encoders (RNN) are slower to optimize and mixing trajectory-sampled RL batches causes large performance drops, indicating sensitivity to data sampling strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1324.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1324.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAESN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-reinforcement learning of structured exploration strategies (MAESN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior meta-RL method that models probabilistic task variables to induce structured exploration; adapts by optimizing task-specific latent variables via gradient descent and explores by sampling latent variables from a prior (on-policy meta-training).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Meta-reinforcement learning of structured exploration strategies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MAESN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Probabilistic latent-variable meta-RL method (Gupta et al., 2018) that learns a distribution over policy parameters or latent noise to induce structured exploration; adaptation proceeds by gradient-based optimization of per-task latent variables; trained with on-policy policy-gradient methods.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Gradient-based adaptation of latent task variables combined with sampling latent variables from a prior for exploration (a form of learned structured noise exploration rather than posterior sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>During adaptation the method adjusts latent variables via gradient descent (optimizing task-specific latent inputs) to improve returns; exploration is achieved initially by sampling latent variables from the learned prior over tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Sparse 2D navigation (comparison in this paper); MAESN originally evaluated on continuous-control tasks in its own paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Sparse-reward goal-finding tasks (unknown goal location), continuous state/action (2D point robot), partially-observable in the sense that the task/goal is latent and must be inferred from reward-bearing visits.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>2D point robot with goals on a semicircle; reward only when inside goal radius (tested radii 0.2 and 0.8); adaptation requires multiple trajectories to identify goal location.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>In the sparse navigation comparison MAESN adapts more slowly and reaches lower final returns than PEARL; MAESN's meta-training in the cited comparison required ≈1e8 timesteps versus PEARL's ≈1e6 for the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported to require far more meta-training data (≈1e8 timesteps in the sparse navigation comparison) compared to PEARL's ≈1e6 timesteps for comparable experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration initialized by sampling latent variables from a learned prior; exploitation achieved by gradient-based refinement of latent variables; lacks explicit posterior-sampling-based temporally-extended hypothesis testing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to PEARL in sparse navigation experiments (paper reports PEARL outperforms MAESN in adaptation speed, final returns, and meta-training efficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>MAESN provides a baseline of probabilistic latent-variable exploration but, in this paper's sparse navigation experiments, adapts slower and is far less meta-training sample-efficient than PEARL. MAESN adapts via gradient-based latent optimization rather than posterior sampling, which the authors argue is less effective for temporally-extended hypothesis-driven exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High meta-training sample requirement (on-policy inefficiency) and weaker structured exploration in sparse-reward tasks compared to posterior-sampling-based approaches such as PEARL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1324.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1324.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL^2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RL^2: Fast reinforcement learning via slow reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent meta-RL approach that embeds experience in RNN hidden state so the policy can adapt online; treats adaptation as learning an inner-loop via recurrence rather than explicit posterior inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RL^2 : Fast reinforcement learning via slow reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL^2 (recurrent meta-learner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A recurrent (RNN) policy that aggregates past transitions in its hidden state and thereby adapts policy behavior online; in this paper RL^2 is reimplemented with PPO and used as an on-policy baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Online adaptation via RNN hidden-state accumulation of experience (learned, policy-driven exploration); exploration emerges from the learned recurrent policy rather than explicit Bayesian/posterior sampling mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The RNN conditions actions on the sequence of past observations, actions, and rewards; adaptation is implicit via changes in hidden state across episodes, enabling the policy to change behavior as it receives experience.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Continuous-control meta-RL benchmarks used in the paper (Half-Cheetah, Ant, Humanoid, Walker variants, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Latent-task meta-RL settings where the MDP/reward may vary across tasks (POMDP perspective since task is unobserved), continuous states and actions, horizon 200, varying dynamics/rewards across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>High-dimensional continuous-control robot domains with episode length 200 and many training tasks; RNN backprop through 100 timesteps in the ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>RL^2 (reimplemented with PPO) performs better than some previously reported results, but remains much less sample-efficient than PEARL and generally attains lower asymptotic performance in the reported benchmarks; exact numerical returns are provided in the paper's figures rather than tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>On-policy baseline; requires substantially more meta-training samples (orders of magnitude more) than PEARL; reported on-policy baselines are 20–100× less sample-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Tradeoff is learned implicitly by the recurrent policy's internal state; no explicit epistemic uncertainty modeling or posterior-sampling mechanism, resulting in less structured temporally-extended exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to PEARL, MAML-TRPO, and ProMP in continuous-control benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Recurrent/meta-RNN adaptation can work but is difficult to scale with off-policy learning; RNN-based encoders in PEARL-like setups are slower to optimize and performance degrades if trajectories are used naively in off-policy RL batches, highlighting data-sampling sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Difficult to combine straightforwardly with off-policy value-based learning; sensitive to how context / RL batches are sampled (trajectory sampling can cause steep performance drops).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1324.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1324.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Posterior sampling (Thompson sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Posterior sampling for reinforcement learning (Thompson sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian exploration strategy that maintains a posterior over MDPs and acts according to an MDP sampled from this posterior for the duration of an episode, enabling temporally-extended (deep) exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>(more) efficient reinforcement learning via posterior sampling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Posterior sampling / Thompson sampling</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Classical Bayesian RL approach: maintain posterior over possible MDPs (or value functions), sample an MDP from this posterior and act optimally under that sample for extended periods to test hypotheses and explore deeply.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Posterior sampling (Thompson sampling) — sampling an MDP/hypothesis from posterior and executing its optimal policy for an episode.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Use prior/posterior over MDPs to select hypothesis (sampled MDP or latent variable) and act according to the sampled hypothesis; update posterior with observed transitions to refine future hypotheses and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>General MDP/POMDP settings cited as conceptual background (applied in meta-RL via PEARL's latent context).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown MDPs (latent task), partially observable when task parameters are unobserved; suitable for environments where temporally-extended experiments are needed to disambiguate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>General; classical theory applies to tabular and structured MDPs, while deep RL requires approximate posteriors / function approximators.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>The paper positions PEARL as a meta-learned variant of posterior sampling; empirical gains attributed to temporally-extended exploration via posterior sampling manifest in PEARL's faster adaptation and improved sample efficiency (20–100×). No separate numeric performance for classical posterior sampling is reported within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Posterior-sampling-based strategies are known to provide efficient exploration in theory (deep exploration); in the PEARL instantiation this translates to large empirical gains in meta-training and adaptation sample efficiency, though classical algorithms require tractable posteriors.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit epistemic-uncertainty-driven exploration via sampling from posterior; exploitation follows naturally as posterior uncertainty decreases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Referenced classical PSRL, bootstrapped DQN; PEARL is described as a meta-learned analog that learns priors and amortized posteriors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Posterior sampling is an effective paradigm for temporally-extended exploration; amortized inference over latent contexts enables applying this idea in meta-RL with function approximators, yielding empirical improvements in adaptation speed and meta-training efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Exact posterior over complex high-dimensional MDPs is intractable; requires approximate (amortized) inference and careful design to avoid distribution mismatch between meta-train and meta-test.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Meta-reinforcement learning of structured exploration strategies <em>(Rating: 2)</em></li>
                <li>(more) efficient reinforcement learning via posterior sampling <em>(Rating: 2)</em></li>
                <li>Deep exploration via bootstrapped DQN <em>(Rating: 2)</em></li>
                <li>Deep variational reinforcement learning for POMDPs <em>(Rating: 2)</em></li>
                <li>Probabilistic model-agnostic meta-learning <em>(Rating: 1)</em></li>
                <li>Learning an embedding space for transferable robot skills <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1324",
    "paper_id": "paper-4625628163a2ee0e6cd320cd7a14b4ccded2a631",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "PEARL",
            "name_full": "Probabilistic Embeddings for Actor-Critic RL (PEARL)",
            "brief_description": "An off-policy meta-reinforcement learning algorithm that disentangles task inference from control via a probabilistic latent context Z; uses amortized variational inference to maintain a posterior over tasks and leverages posterior sampling over Z to perform temporally-extended (structured) exploration and fast adaptation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PEARL",
            "agent_description": "Off-policy actor-critic meta-RL that conditions an SAC actor pi_theta(a|s,z) and critic Q_theta(s,a,z) on a learned probabilistic latent context z inferred by an amortized, permutation-invariant encoder q_phi(z|c) (Gaussian factorization over transitions). The encoder is trained with critic gradients and a KL information bottleneck; the actor and critic are trained with off-policy replay while context batches are sampled from recent on-policy data.",
            "adaptive_design_method": "Posterior sampling (Thompson sampling) over a probabilistic latent task variable (Z); probabilistic latent-context inference enabling temporally-extended exploration.",
            "adaptation_strategy_description": "At meta-test time sample z ~ prior r(z) for the first episode (holding z fixed for the episode) to generate diverse hypotheses and temporally-extended exploration; collect trajectory data, append to context c, update posterior q_phi(z|c), then sample new z for subsequent episodes—thus adapting by hypothesis sampling and posterior update using observed transitions (online Bayesian-style filtering via the amortized encoder).",
            "environment_name": "Various continuous-control meta-RL benchmarks (Half-Cheetah-Dir, Half-Cheetah-Vel, Humanoid-Dir-2D, Ant-Fwd-Back, Ant-Goal-2D, Walker-2D-Params) and a sparse 2D navigation goal-finding task.",
            "environment_characteristics": "Tasks drawn from a distribution of MDPs with unknown transition and/or reward functions (i.e., the task/MDP is latent → POMDP viewpoint); continuous state and action spaces; some benchmarks involve varying dynamics (system parameter randomization) or varying reward/goals; sparse rewards in 2D navigation experiments.",
            "environment_complexity": "High-dimensional continuous-control robot domains (Half-Cheetah, Ant, Humanoid, Walker) with episode horizon 200; meta-datasets of up to 100 training tasks and 30 test tasks for some domains; 2D navigation is lower-dimensional but sparse-reward and requires multi-trajectory adaptation (goal radius tested at 0.2 and 0.8).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "PEARL achieves 20–100× improvement in meta-training sample efficiency over prior meta-RL baselines and improves asymptotic performance by ~50–100% in five of six continuous-control domains (exact returns per domain in paper figures). In the sparse 2D navigation task PEARL typically begins to adapt after ~5 trajectories and attains higher final returns than MAESN; meta-training required ≈1e6 timesteps for PEARL on the sparse navigation task compared to ≈1e8 timesteps reported for MAESN.",
            "performance_without_adaptation": "Baselines (on-policy meta-RL methods such as ProMP and MAML-TRPO, and recurrence-based RL^2) require 20–100× more meta-training samples to reach similar performance; MAESN uses ≈1e8 timesteps in the sparse navigation experiment versus PEARL's ≈1e6. A deterministic (point-estimate) version of the latent context fails on the sparse navigation task (unable to explore effectively).",
            "sample_efficiency": "Meta-training: PEARL converges using roughly 1e6 environment timesteps on sparse navigation and is reported 20–100× more sample efficient than on-policy baselines across continuous-control benchmarks; adaptation: aggregation of two trajectories is used to compute reported test-time performance (they report average returns after two aggregated trajectories), and in sparse navigation meaningful adaptation begins after ≈5 trajectories.",
            "exploration_exploitation_tradeoff": "Exploration is driven by epistemic uncertainty over latent task variable Z: sampling z from the prior/posterior (Thompson sampling) produces temporally-extended, hypothesis-driven exploration (act optimally under sampled hypothesis for an episode). Exploitation emerges as posterior uncertainty narrows and z-constrained policies act more optimally. A KL bottleneck on q(z|c) constrains information and regularizes exploration.",
            "comparison_methods": "ProMP (proximal meta-policy search), MAML-TRPO (model-agnostic meta-learning with TRPO), RL^2 (recurrent meta-learner), MAESN (Gupta et al., 2018).",
            "key_results": "Probabilistic latent context + posterior sampling yields structured, temporally-extended exploration enabling rapid adaptation in partially observable/latent-task settings; decoupling encoder (inference) from policy allows efficient off-policy meta-training; careful context data sampling (recent on-policy transitions, distinct from RL minibatches) is crucial; deterministic latent contexts or fully off-policy context sampling hurt performance; PEARL attains major improvements in meta-training sample efficiency (20–100×) and better asymptotic performance across benchmarks.",
            "limitations_or_failures": "Meta-training with sparse rewards is challenging — the authors used dense rewards during meta-training for the sparse navigation experiments (limitation). Sampling context fully off-policy significantly degrades performance. Deterministic latent context fails on sparse-reward tasks. Recurrent encoders (RNN) are slower to optimize and mixing trajectory-sampled RL batches causes large performance drops, indicating sensitivity to data sampling strategies.",
            "uuid": "e1324.0",
            "source_info": {
                "paper_title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "MAESN",
            "name_full": "Meta-reinforcement learning of structured exploration strategies (MAESN)",
            "brief_description": "A prior meta-RL method that models probabilistic task variables to induce structured exploration; adapts by optimizing task-specific latent variables via gradient descent and explores by sampling latent variables from a prior (on-policy meta-training).",
            "citation_title": "Meta-reinforcement learning of structured exploration strategies",
            "mention_or_use": "use",
            "agent_name": "MAESN",
            "agent_description": "Probabilistic latent-variable meta-RL method (Gupta et al., 2018) that learns a distribution over policy parameters or latent noise to induce structured exploration; adaptation proceeds by gradient-based optimization of per-task latent variables; trained with on-policy policy-gradient methods.",
            "adaptive_design_method": "Gradient-based adaptation of latent task variables combined with sampling latent variables from a prior for exploration (a form of learned structured noise exploration rather than posterior sampling).",
            "adaptation_strategy_description": "During adaptation the method adjusts latent variables via gradient descent (optimizing task-specific latent inputs) to improve returns; exploration is achieved initially by sampling latent variables from the learned prior over tasks.",
            "environment_name": "Sparse 2D navigation (comparison in this paper); MAESN originally evaluated on continuous-control tasks in its own paper.",
            "environment_characteristics": "Sparse-reward goal-finding tasks (unknown goal location), continuous state/action (2D point robot), partially-observable in the sense that the task/goal is latent and must be inferred from reward-bearing visits.",
            "environment_complexity": "2D point robot with goals on a semicircle; reward only when inside goal radius (tested radii 0.2 and 0.8); adaptation requires multiple trajectories to identify goal location.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "In the sparse navigation comparison MAESN adapts more slowly and reaches lower final returns than PEARL; MAESN's meta-training in the cited comparison required ≈1e8 timesteps versus PEARL's ≈1e6 for the reported experiments.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Reported to require far more meta-training data (≈1e8 timesteps in the sparse navigation comparison) compared to PEARL's ≈1e6 timesteps for comparable experiments in this paper.",
            "exploration_exploitation_tradeoff": "Exploration initialized by sampling latent variables from a learned prior; exploitation achieved by gradient-based refinement of latent variables; lacks explicit posterior-sampling-based temporally-extended hypothesis testing.",
            "comparison_methods": "Compared to PEARL in sparse navigation experiments (paper reports PEARL outperforms MAESN in adaptation speed, final returns, and meta-training efficiency).",
            "key_results": "MAESN provides a baseline of probabilistic latent-variable exploration but, in this paper's sparse navigation experiments, adapts slower and is far less meta-training sample-efficient than PEARL. MAESN adapts via gradient-based latent optimization rather than posterior sampling, which the authors argue is less effective for temporally-extended hypothesis-driven exploration.",
            "limitations_or_failures": "High meta-training sample requirement (on-policy inefficiency) and weaker structured exploration in sparse-reward tasks compared to posterior-sampling-based approaches such as PEARL.",
            "uuid": "e1324.1",
            "source_info": {
                "paper_title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "RL^2",
            "name_full": "RL^2: Fast reinforcement learning via slow reinforcement learning",
            "brief_description": "A recurrent meta-RL approach that embeds experience in RNN hidden state so the policy can adapt online; treats adaptation as learning an inner-loop via recurrence rather than explicit posterior inference.",
            "citation_title": "RL^2 : Fast reinforcement learning via slow reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "RL^2 (recurrent meta-learner)",
            "agent_description": "A recurrent (RNN) policy that aggregates past transitions in its hidden state and thereby adapts policy behavior online; in this paper RL^2 is reimplemented with PPO and used as an on-policy baseline.",
            "adaptive_design_method": "Online adaptation via RNN hidden-state accumulation of experience (learned, policy-driven exploration); exploration emerges from the learned recurrent policy rather than explicit Bayesian/posterior sampling mechanisms.",
            "adaptation_strategy_description": "The RNN conditions actions on the sequence of past observations, actions, and rewards; adaptation is implicit via changes in hidden state across episodes, enabling the policy to change behavior as it receives experience.",
            "environment_name": "Continuous-control meta-RL benchmarks used in the paper (Half-Cheetah, Ant, Humanoid, Walker variants, etc.).",
            "environment_characteristics": "Latent-task meta-RL settings where the MDP/reward may vary across tasks (POMDP perspective since task is unobserved), continuous states and actions, horizon 200, varying dynamics/rewards across tasks.",
            "environment_complexity": "High-dimensional continuous-control robot domains with episode length 200 and many training tasks; RNN backprop through 100 timesteps in the ablation.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "RL^2 (reimplemented with PPO) performs better than some previously reported results, but remains much less sample-efficient than PEARL and generally attains lower asymptotic performance in the reported benchmarks; exact numerical returns are provided in the paper's figures rather than tabulated.",
            "performance_without_adaptation": null,
            "sample_efficiency": "On-policy baseline; requires substantially more meta-training samples (orders of magnitude more) than PEARL; reported on-policy baselines are 20–100× less sample-efficient.",
            "exploration_exploitation_tradeoff": "Tradeoff is learned implicitly by the recurrent policy's internal state; no explicit epistemic uncertainty modeling or posterior-sampling mechanism, resulting in less structured temporally-extended exploration.",
            "comparison_methods": "Compared to PEARL, MAML-TRPO, and ProMP in continuous-control benchmarks.",
            "key_results": "Recurrent/meta-RNN adaptation can work but is difficult to scale with off-policy learning; RNN-based encoders in PEARL-like setups are slower to optimize and performance degrades if trajectories are used naively in off-policy RL batches, highlighting data-sampling sensitivity.",
            "limitations_or_failures": "Difficult to combine straightforwardly with off-policy value-based learning; sensitive to how context / RL batches are sampled (trajectory sampling can cause steep performance drops).",
            "uuid": "e1324.2",
            "source_info": {
                "paper_title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "Posterior sampling (Thompson sampling)",
            "name_full": "Posterior sampling for reinforcement learning (Thompson sampling)",
            "brief_description": "A Bayesian exploration strategy that maintains a posterior over MDPs and acts according to an MDP sampled from this posterior for the duration of an episode, enabling temporally-extended (deep) exploration.",
            "citation_title": "(more) efficient reinforcement learning via posterior sampling",
            "mention_or_use": "mention",
            "agent_name": "Posterior sampling / Thompson sampling",
            "agent_description": "Classical Bayesian RL approach: maintain posterior over possible MDPs (or value functions), sample an MDP from this posterior and act optimally under that sample for extended periods to test hypotheses and explore deeply.",
            "adaptive_design_method": "Posterior sampling (Thompson sampling) — sampling an MDP/hypothesis from posterior and executing its optimal policy for an episode.",
            "adaptation_strategy_description": "Use prior/posterior over MDPs to select hypothesis (sampled MDP or latent variable) and act according to the sampled hypothesis; update posterior with observed transitions to refine future hypotheses and actions.",
            "environment_name": "General MDP/POMDP settings cited as conceptual background (applied in meta-RL via PEARL's latent context).",
            "environment_characteristics": "Unknown MDPs (latent task), partially observable when task parameters are unobserved; suitable for environments where temporally-extended experiments are needed to disambiguate hypotheses.",
            "environment_complexity": "General; classical theory applies to tabular and structured MDPs, while deep RL requires approximate posteriors / function approximators.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "The paper positions PEARL as a meta-learned variant of posterior sampling; empirical gains attributed to temporally-extended exploration via posterior sampling manifest in PEARL's faster adaptation and improved sample efficiency (20–100×). No separate numeric performance for classical posterior sampling is reported within this paper.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Posterior-sampling-based strategies are known to provide efficient exploration in theory (deep exploration); in the PEARL instantiation this translates to large empirical gains in meta-training and adaptation sample efficiency, though classical algorithms require tractable posteriors.",
            "exploration_exploitation_tradeoff": "Explicit epistemic-uncertainty-driven exploration via sampling from posterior; exploitation follows naturally as posterior uncertainty decreases.",
            "comparison_methods": "Referenced classical PSRL, bootstrapped DQN; PEARL is described as a meta-learned analog that learns priors and amortized posteriors.",
            "key_results": "Posterior sampling is an effective paradigm for temporally-extended exploration; amortized inference over latent contexts enables applying this idea in meta-RL with function approximators, yielding empirical improvements in adaptation speed and meta-training efficiency.",
            "limitations_or_failures": "Exact posterior over complex high-dimensional MDPs is intractable; requires approximate (amortized) inference and careful design to avoid distribution mismatch between meta-train and meta-test.",
            "uuid": "e1324.3",
            "source_info": {
                "paper_title": "Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables",
                "publication_date_yy_mm": "2019-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Meta-reinforcement learning of structured exploration strategies",
            "rating": 2
        },
        {
            "paper_title": "(more) efficient reinforcement learning via posterior sampling",
            "rating": 2
        },
        {
            "paper_title": "Deep exploration via bootstrapped DQN",
            "rating": 2
        },
        {
            "paper_title": "Deep variational reinforcement learning for POMDPs",
            "rating": 2
        },
        {
            "paper_title": "Probabilistic model-agnostic meta-learning",
            "rating": 1
        },
        {
            "paper_title": "Learning an embedding space for transferable robot skills",
            "rating": 1
        }
    ],
    "cost": 0.016753,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables</h1>
<p>Kate Rakelly ${ }^{1 <em>}$ Aurick Zhou ${ }^{1 </em>}$ Deirdre Quillen ${ }^{1}$ Chelsea Finn ${ }^{1}$ Sergey Levine ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While in principle meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. The also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness in sparse reward problems. In this paper, we address these challenges by developing an offpolicy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both metatraining and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.</p>
<h2>1. Introduction</h2>
<p>The combination of reinforcement learning (RL) with powerful non-linear function approximators has led to a wide range of advances in sequential decision making problems. However, conventional RL methods learn a separate policy per task, each often requiring millions of interactions with the environment. Learning large repertoires of behaviors with such methods quickly becomes prohibitive. Fortunately, many of the problems we would like our autonomous agents to solve share common structure. For</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>example screwing a cap on a bottle and turning a doorknob both involve grasping an object in the hand and rotating the wrist. Exploiting this structure to learn new tasks more quickly remains an open and pressing topic. Meta-learning methods learn this structure from experience by making use of large quantities of experience collected across a distribution of tasks. Once learned, these methods can adapt quickly to new tasks given a small amount of experience.</p>
<p>While meta-learned policies adapt to new tasks with only a few trials, during training, they require massive amounts of data drawn from a large set of distinct tasks, exacerbating the problem of sample efficiency that plagues RL algorithms. Most current meta-RL methods require on-policy data during both meta-training and adaptation (Finn et al., 2017; Wang et al., 2016; Duan et al., 2016; Mishra et al., 2018; Rothfuss et al., 2018; Houthooft et al., 2018), which makes them exceedingly inefficient during meta-training. However, making use of off-policy data for meta-RL poses new challenges. Meta-learning typically operates on the principle that meta-training time should match meta-test time - for example, an image classification meta-learner tested on classifying images from five examples should be meta-trained to take in sets of five examples and produce accurate predictions (Vinyals et al., 2016). This makes it inherently difficult to meta-train a policy to adapt using off-policy data, which is systematically different from the data the policy would see when it explores (on-policy) in a new task at meta-test time.</p>
<p>In this paper, we tackle the problem of efficient off-policy meta-reinforcement learning. To achieve both meta-training efficiency and rapid adaptation, we propose an approach that integrates online inference of probabilistic context variables with existing off-policy RL algorithms. Rapid adaptation requires reasoning about distributions: when exposed to a new task for the first time, the optimal meta-learned policy must carry out a stochastic exploration procedure to visit potentially rewarding states, as well as adapt to the task at hand (Gupta et al., 2018). During meta-training, we learn a probabilistic encoder that accumulates the necessary statistics from past experience into the context variables that enable the policy to perform the task. At meta-test time, when the agent is faced with an unseen task, the context variables can be sampled and held constant for the duration</p>
<p>of an episode, enabling temporally-extended exploration. The collected trajectories are used to update the posterior over the context variables, achieving rapid trajectory-level adaptation. In effect, our method adapts by sampling "task hypotheses," attempting those tasks, and then evaluating whether the hypotheses were correct or not. Disentangling task inference from action makes our approach particularly amenable to off-policy meta-learning; the policy can be optimized with off-policy data while the probabilistic encoder is trained with on-policy data to minimize distribution mismatch between meta-train and meta-test.</p>
<p>The primary contribution of our work is an off-policy metaRL algorithm called probabilistic embeddings for actorcritic RL (PEARL). Our method achieves excellent sample efficiency during meta-training, enables fast adaptation by accumulating experience online, and performs structured exploration by reasoning about uncertainty over tasks. In our experimental evaluation, we demonstrate state-of-the-art results with 20-100X improvement in meta-training sample efficiency and substantial increases in asymptotic performance over prior state-of-the-art on six continuous control metalearning environments. We further examine how our model conducts structured exploration to adapt rapidly to new tasks in a 2-D navigation environment with sparse rewards. Our open-source implementation of PEARL can be found at https://github.com/katerakelly/oyster.</p>
<h2>2. Related Work</h2>
<p>Meta-learning. Our work builds on the meta-learning framework (Schmidhuber, 1987; Bengio et al., 1990; Thrun \&amp; Pratt, 1998) in the context of reinforcement learning. Recently, meta-RL methods have been developed for metalearning dynamics models (Nagabandi et al., 2019; Sæmundsson et al., 2018) and policies (Finn et al., 2017; Duan et al., 2016; Mishra et al., 2018) that can quickly adapt to new tasks.</p>
<p>Recurrent (Duan et al., 2016; Wang et al., 2016) and recursive (Mishra et al., 2018) meta-RL methods adapt to new tasks by aggregating experience into a latent representation on which the policy is conditioned. These approaches can be categorized into what we will call context-based meta-RL methods, since a neural network is trained to take experience as input as a form of task-specific context. Similarly, our approach can also be considered context-based; however, we represent task contexts with probabilistic latent variables, enabling reasoning over task uncertainty. Instead of using recurrence, we leverage the Markov property in our permutation-invariant encoder to aggregate experience, enabling fast optimization especially for long-horizon tasks while mitigating overfitting. While prior work has studied methods that can train recurrent Q-functions with off-policy Q-learning methods, such methods have often been applied
to much simpler tasks (Heess et al., 2015), and in discrete environments (Hausknecht \&amp; Stone, 2015). Indeed, our own experiments in Section 6.3 demonstrate that straightforward incorporation of recurrent policies with off-policy learning is difficult. Contextual methods have also been applied to imitation learning by conditioning the policy on a learned embedding of a demonstration and optimizing with behavior cloning (Duan et al., 2017; James et al., 2018).</p>
<p>In contrast to context-based methods, gradient-based metaRL methods learn from aggregated experience using policy gradients (Finn et al., 2017; Stadie et al., 2018; Rothfuss et al., 2018; Xu et al., 2018a), meta-learned loss functions (Sung et al., 2017; Houthooft et al., 2018), or hyperparameters (Xu et al., 2018b). These methods focus on on-policy meta-learning. We instead focus on meta-learning from offpolicy data, which is non-trivial to do with methods based on policy gradients and evolutionary optimization algorithms. Beyond substantial sample efficiency improvements, we also empirically find that our context-based method is able to reach higher asymptotic performance, in comparison to methods using policy gradients.</p>
<p>Outside of RL, meta-learning methods for few-shot supervised learning problems have explored a wide variety of approaches and architectures (Santoro et al., 2016; Vinyals et al., 2016; Ravi \&amp; Larochelle, 2017; Oreshkin et al., 2018). Our permutation-invariant embedding function is inspired by the embedding function of prototypical networks (Snell et al., 2017). While they use a distance metric in a learned, deterministic embedding space to classify new inputs, our embedding is probabilistic and is used to condition the behavior of an RL agent. To our knowledge, no prior work has proposed this particular embedding function for meta-RL.</p>
<p>Probabilistic meta-learning. Prior work has applied probabilistic models to meta-learning in both supervised and reinforcement learning domains. Hierarchical Bayesian models have been used to model few-shot learning (Fei-Fei et al., 2003; Tenenbaum, 1999), including approaches that perform gradient-based adaptation (Grant et al., 2018; Yoon et al., 2018). For supervised learning, Rusu et al. (2019); Gordon et al. (2019); Finn et al. (2018) adapt model predictions using probabilistic latent task variables inferred via amortized approximate inference. We extend this idea to off-policy meta-RL. In the context of RL, Hausman et al. (2018) also conditions the policy on inferred task variables, but the aim is to compose tasks via the embedding space, while we focus on rapid adaptation to new tasks. While we infer task variables and explore via posterior sampling, MAESN (Gupta et al., 2018) adapts by optimizing the task variables with gradient descent and explores by sampling from the prior.</p>
<p>Posterior sampling. In classical RL, posterior sampling (Strens, 2000; Osband et al., 2013) maintains a posterior</p>
<p>over possible MDPs and enables temporally extended exploration by acting optimally according to a sampled MDP. Our approach can be interpreted as a meta-learned variant of this method; probabilistic context captures the current uncertainty over the task, allowing the agent to explore in new tasks in a similarly structured manner.</p>
<p>Partially observed MDPs. Adaptation at test time in metaRL can be viewed as a special case of RL in a POMDP (Kaelbling et al., 1998) by including the task as the unobserved part of the state. We use a variational approach related to Igl et al. (2018) to estimate belief over the task. While they focus on solving general POMDPs, we leverage the additional structure imposed by the meta-learning problem to simplify inference, and use posterior sampling for exploration in new tasks.</p>
<h2>3. Problem Statement</h2>
<p>Our approach is motivated by situations in which the agent can leverage varied experiences from previous tasks to adapt quickly to the new task at hand. Sample efficiency is central to our problem statement, both in terms of the number of samples from previous experience (meta-training efficiency), and in the amount of experience required in the new task (adaptation efficiency). To achieve meta-training efficiency, we leverage off-policy RL in our approach. Adaptation efficiency requires the agent to reason about its uncertainty over tasks, particularly in sparse reward settings. To capture uncertainty in our belief over the task, we learn a probabilistic latent representation of prior experience. We formalize the problem statement in this section, formulate our approach to adaptation as probabilistic inference in Section 4, and explain how our approach can be integrated with off-policy RL algorithms in Section 5.</p>
<p>Similar to previous meta-RL formulations, we assume a distribution of tasks $p(\mathcal{T})$, where each task is a Markov decision process (MDP), consisting of a set of states, actions, a transition function, and a bounded reward function. We assume that the transition and reward functions are unknown, but can be sampled by taking actions in the environment. Formally, a task $\mathcal{T}=\left{p\left(\mathbf{s}<em t_1="t+1">{0}\right), p\left(\mathbf{s}</em>} \mid \mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right), r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)\right}$ consists of an initial state distribution $p\left(\mathbf{s<em t_1="t+1">{0}\right)$, transition distribution $p\left(\mathbf{s}</em>} \mid \mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)$, and reward function $r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>}\right)$. Note that this problem definition encompasses task distributions with varying transition functions (e.g., robots with different dynamics) and varying reward functions (e.g., navigating to different locations). Given a set of training tasks sampled from $p(\mathcal{T})$, the meta-training process learns a policy that adapts to the task at hand by conditioning on the history of past transitions, which we refer to as context $\mathbf{c}$. Let $\mathbf{c<em c="c">{q</em>}}^{\mathcal{T}}=\left(\mathbf{s<em n="n">{n}, \mathbf{a}</em>}, r_{n}, \mathbf{s<em 1:="1:" N="N">{n}^{\prime}\right)$ be one transition in the task $\mathcal{T}$ so that $\mathbf{c}</em>)$.}^{\mathcal{T}}$ comprises the experience collected so far. At test-time, the policy must adapt to a new task drawn from $p(\mathcal{T</p>
<h2>4. Probabilistic Latent Context</h2>
<p>We capture knowledge about how the current task should be performed in a latent probabilistic context variable $Z$, on which we condition the policy as $\pi_{\theta}(\mathbf{a} \mid \mathbf{s}, \mathbf{z})$ in order to adapt its behavior to the task. Meta-training consists of leveraging data from a variety of training tasks to learn to infer the value of $Z$ from a recent history of experience in the new task, as well as optimizing the policy to solve the task given samples from the posterior over $Z$. In this section we describe the structure of the meta-trained inference mechanism. We address how meta-training can be performed with off-policy RL algorithms in Section 5.</p>
<h3>4.1. Modeling and Learning Latent Contexts</h3>
<p>To enable adaptation, the latent context $Z$ must encode salient information about the task. Recall that $\mathbf{c}<em _phi="\phi">{1: N}^{T}$ comprises experience collected so far; throughout this section we will often write $c$ for simplicity. We adopt an amortized variational inference approach (Kingma \&amp; Welling, 2014; Rezende et al., 2014; Alemi et al., 2016) to learn to infer $Z$. We train an inference network $q</em>)$ can be optimized in a model-free manner to model the state-action value functions or to maximize returns through the policy over the distribution of tasks. Assuming this objective to be a log-likelihood, the resulting variational lower bound is:}(\mathbf{z} \mid \mathbf{c})$, parameterized by $\phi$, that estimates the posterior $p(\mathbf{z} \mid \mathbf{c})$. In a generative approach, this can be achieved by optimizing $q_{\phi}(\mathbf{z} \mid \mathbf{c})$ to reconstruct the MDP by learning a predictive models of reward and dynamics. Alternatively, $q_{\phi}(\mathbf{z} \mid \mathbf{c</p>
<p>$$
\mathbb{E}<em _mathbf_z="\mathbf{z">{\mathcal{T}}\left[\mathbb{E}</em>)\right)\right]\right]
$$} \sim q_{\phi}\left(\mathbf{z} \mid \mathbf{c}^{\mathcal{T}}\right)}\left[R(\mathcal{T}, \mathbf{z})+\beta D_{\mathrm{KL}}\left(q_{\phi}\left(\mathbf{z} \mid \mathbf{c}^{\mathcal{T}}\right) | p(\mathbf{z</p>
<p>where $p(\mathbf{z})$ is a unit Gaussian prior over $Z$ and $R(\mathcal{T}, \mathbf{z})$ could be a variety of objectives, as discussed above. The KL divergence term can also be interpreted as the result of a variational approximation to an information bottleneck (Alemi et al., 2016) that constrains the mutual information between $Z$ and $\mathbf{c}$. Intuitively, this bottleneck constrains $\mathbf{z}$ to contain only information from the context that is necessary to adapt to the task at hand, mitigating overfitting to training tasks. While the parameters of $q_{\phi}$ are optimized during meta-training, at meta-test time the latent context for a new task is simply inferred from gathered experience.</p>
<p>In designing the architecture of the inference network $q_{\phi}(\mathbf{z} \mid \mathbf{c})$, we would like it to be expressive enough to capture minimal sufficient statistics of task-relevant information, without modeling irrelevant dependencies. We note that an encoding of a fully observed MDP should be permutation invariant: if we would like to infer what the task is, identify the MDP model, or train a value function, it is enough to have access to a collection of transitions $\left{\mathbf{s}<em i="i">{i}, \mathbf{a}</em>}, \mathbf{s<em i="i">{i}^{\prime}, r</em>\right}$, without regard for the order in which these transitions were observed. With this observation in mind, we choose a permutation-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Inference network architecture. The amortized inference network predicts the posterior over the latent context variables $q_{\phi}(\mathbf{z} \mid \mathbf{c})$ as a permutation-invariant function of prior experience.
invariant representation for $q_{\phi}\left(\mathbf{z} \mid \mathbf{c}_{1: N}\right)$, modeling it as a product of independent factors</p>
<p>$$
q_{\phi}\left(\mathbf{z} \mid \mathbf{c}<em n="1">{1: N}\right) \propto \Pi</em>\right)
$$}^{N} \Psi_{\phi}\left(\mathbf{z} \mid \mathbf{c}_{n</p>
<p>To keep the method tractable, we use Gaussian factors $\Psi_{\phi}\left(\mathbf{z} \mid \mathbf{c}<em _phi="\phi">{n}\right)=\mathcal{N}\left(f</em>}^{\mu}\left(\mathbf{c<em _phi="\phi">{n}\right), f</em>}^{\sigma}\left(\mathbf{c<em _phi="\phi">{n}\right)\right)$, which result in a Gaussian posterior. The function $f</em>$, is shown in Figure 1.}$, represented as a neural network parameterized by $\phi$, predicts the mean $\mu$ as well as the variance $\sigma$ as a function of the $\mathbf{c}_{n</p>
<h3>4.2. Posterior Sampling and Exploration via Latent Contexts</h3>
<p>Modeling the latent context as probabilistic allows us to make use of posterior sampling for efficient exploration at meta-test time. In classical RL, posterior sampling (Strens, 2000; Osband et al., 2013) begins with a prior distribution over MDPs, computes a posterior distribution conditioned on the experience it has seen so far, and executes the optimal policy for a sampled MDP for the duration of an episode as an efficient method for exploration. In particular, acting optimally according to a random MDP allows for temporally extended (or deep) exploration, meaning that the agent can act to test hypotheses even when the results of actions are not immediately informative of the task.</p>
<p>In the single-task deep RL setting, posterior sampling and the benefits of deep exploration has been explored by Osband et al. (2016), which maintains an approximate posterior over value functions via bootstraps. In contrast, our method PEARL directly infers a posterior over the latent context $Z$, which may encode the MDP itself if optimized for reconstruction, optimal behaviors if optimized for the policy, or the value function if optimized for a critic. Our meta-training procedure leverages training tasks to learn a prior over $Z$ that captures the distribution over tasks and also learns to efficiently use experience to infer new tasks. At meta-test time, we initially sample $\mathbf{z}$ 's from the prior and execute according to each $\mathbf{z}$ for an episode, thus exploring in a temporally extended and diverse manner. We can then use the collected experience to update our posterior and continue exploring coherently in a manner that acts more and more optimally as our belief narrows, akin to posterior sampling.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Meta-training procedure. The inference network $q_{\phi}$ uses context data to infer the posterior over the latent context variable $Z$, which conditions the actor and critic, and is optimized with gradients from the critic as well as from an information bottleneck on $Z$. De-coupling the data sampling strategies for context $\left(\mathcal{S}_{C}\right)$ and RL batches is important for off-policy learning.</p>
<h2>5. Off-Policy Meta-Reinforcement Learning</h2>
<p>While our probabilistic context model is straightforward to combine with on-policy policy gradient methods, a primary goal of our work is to enable efficient off-policy metareinforcement learning, where the number of samples for both meta-training and fast adaptation is minimal. The efficiency of the meta-training process is largely disregarded in prior work, which make use of stable but relatively inefficient on-policy algorithms (Duan et al., 2016; Finn et al., 2017; Gupta et al., 2018; Mishra et al., 2018). However, designing off-policy meta-RL algorithms is non-trivial partly because modern meta-learning is predicated on the assumption that the distribution of data used for adaptation will match across meta-training and meta-test. In RL, this implies that since at meta-test time on-policy data will be used to adapt, on-policy data should be used during meta-training as well. Furthermore, meta-RL requires the policy to reason about distributions, so as to learn effective stochastic exploration strategies. This problem inherently cannot be solved by off-policy RL methods that minimize temporaldifference error, as they do not have the ability to directly optimize for distributions of states visited. In contrast, policy gradient methods have direct control over the actions taken by the policy. Given these two challenges, a naive combination of meta-learning and value-based RL could be ineffective. In practice, we were unable to optimize such a method.</p>
<p>Our main insight in designing an off-policy meta-RL method with the probabilistic context in Section 4 is that the data used to train the encoder need not be the same as the data used to train the policy. The policy can treat the context $\mathbf{z}$ as part of the state in an off-policy RL loop, while the stochasticity of the exploration process is provided by the uncertainty in the encoder $q(\mathbf{z} \mid \mathbf{c})$. The actor and critic are always trained with off-policy data sampled from the entire replay buffer $\mathcal{B}$. We define a sampler $\mathcal{S}_{\mathbf{c}}$ to sample</p>
<h2>Algorithm 1 PEARL Meta-training</h2>
<p>Require: Batch of training tasks $\left{\mathcal{T}<em T="T" _ldots="\ldots" i="1">{i}\right}</em>$
1: Initialize replay buffers $\mathcal{B}^{i}$ for each training task
2: while not done do
3: for each $\mathcal{T}}$ from $p(\mathcal{T})$, learning rates $\alpha_{1}, \alpha_{2}, \alpha_{3<em _phi="\phi">{i}$ do
4: Initialize context $\mathbf{c}^{i}={ }$
5: for $k=1, \ldots, K$ do
6: Sample $\mathbf{z} \sim q</em>\right)$
7: Gather data from $\pi_{\theta}(\mathbf{a} \mid \mathbf{s}, \mathbf{z})$ and add to $\mathcal{B}^{i}$
8: Update $\mathbf{c}^{i}=\left{\left(\mathbf{s}}\left(\mathbf{z} \mid \mathbf{c}^{i<em j="j">{j}, \mathbf{a}</em>}, \mathbf{s<em j="j">{j}^{\prime}, r</em>\right)\right}<em i="i">{j: 1 \ldots N} \sim \mathcal{B}^{i}$
9: end for
10: end for
11: for step in training steps do
12: for each $\mathcal{T}</em>$ do
13: Sample context $\mathbf{c}^{i} \sim \mathcal{S}<em _phi="\phi">{c}\left(\mathcal{B}^{i}\right)$ and RL batch $b^{i} \sim$ $\mathcal{B}^{i}$
14: Sample $\mathbf{z} \sim q</em>\right)$
15: $\quad \mathcal{L}}\left(\mathbf{z} \mid \mathbf{c}^{i<em _actor="{actor" _text="\text">{\text {actor }}^{i}=\mathcal{L}</em>\right)$
16: $\quad \mathcal{L}}}\left(b^{i}, \mathbf{z<em _critic="{critic" _text="\text">{\text {critic }}^{i}=\mathcal{L}</em>\right)$
17: $\quad \mathcal{L}}}\left(b^{i}, \mathbf{z<em _mathrm_KL="\mathrm{KL">{K L}^{i}=\beta D</em>)\right)$
18: end for
19: $\quad \phi \leftarrow \phi-\alpha_{1} \nabla_{\phi} \sum_{i}\left(\mathcal{L}}}\left(q\left(\mathbf{z} \mid \mathbf{c}^{i}\right) | r(\mathbf{z<em K="K" L="L">{\text {critic }}^{i}+\mathcal{L}</em>\right)$
20: $\quad \theta_{\pi} \leftarrow \theta_{\pi}-\alpha_{2} \nabla_{\theta} \sum_{i} \mathcal{L}}^{i<em Q="Q">{\text {actor }}^{i}$
21: $\quad \theta</em>} \leftarrow \theta_{Q}-\alpha_{3} \nabla_{\theta} \sum_{i} \mathcal{L<em _mathbf_c="\mathbf{c">{\text {critic }}^{i}$
22: end for
23: end while
context batches for training the encoder. Allowing $\mathcal{S}</em>$ to sample from the entire buffer presents too extreme of a distribution mismatch with on-policy test data. However, the context does not need to be strictly on-policy; we find that an in-between strategy of sampling from a replay buffer of recently collected data retains on-policy performance with better efficiency. We summarize our training procedure in Figure 2 and Algorithm 1. Meta-testing is described in Algorithm 2.}</p>
<h3>5.1. Implementation</h3>
<p>We build our algorithm on top of the soft actor-critic algorithm (SAC) (Haarnoja et al., 2018), an off-policy actorcritic method based on the maximum entropy RL objective which augments the traditional sum of discounted returns with the entropy of the policy.</p>
<p>SAC exhibits good sample efficiency and stability, and further has a probabilistic interpretation which integrates well with probabilistic latent contexts. We optimize the parameters of the inference network $q(\mathbf{z} \mid \mathbf{c})$ jointly with the parameters of the actor $\pi_{\theta}(\mathbf{a} \mid \mathbf{s}, \mathbf{z})$ and critic $Q_{\theta}(\mathbf{s}, \mathbf{a}, \mathbf{z})$, using the reparameterization trick (Kingma \&amp; Welling, 2014) to compute gradients for parameters of $q_{\phi}(\mathbf{z} \mid \mathbf{c})$ through sampled $\mathbf{z}$ 's. We train the inference network using gradients from</p>
<h2>Algorithm 2 PEARL Meta-testing</h2>
<p>Require: test task $\mathcal{T} \sim p(\mathcal{T})$
1: Initialize context $\mathbf{c}^{\mathcal{T}}={ }$
2: for $k=1, \ldots, K$ do
3: Sample $z \sim q_{\phi}\left(\mathbf{z} \mid c^{\mathcal{T}}\right)$
4: Roll out policy $\pi_{\theta}(\mathbf{a} \mid \mathbf{s}, \mathbf{z})$ to collect data $D_{k}^{\mathcal{T}}=$ $\left{\left(\mathbf{s}<em j="j">{j}, \mathbf{a}</em>}, \mathbf{s<em j="j">{j}^{\prime}, r</em>\right)\right}<em k="k">{j: 1 \ldots N}$
5: Accumulate context $\mathbf{c}^{\mathcal{T}}=\mathbf{c}^{\mathcal{T}} \cup D</em>$
6: end for
the Bellman update for the critic. We found empirically that training the encoder to recover the state-action value function outperforms optimizing it to maximize actor returns, or reconstruct states and rewards. The critic loss can then be written as,}^{\mathcal{T}</p>
<p>$$
\mathcal{L}<em _phi="\phi">{\text {critic }}=\underset{\mathbf{z} \sim q</em>}(\mathbf{z} \mid \mathbf{c})}{\mathbb{E<em _theta="\theta">{\left(\mathbf{s}, \mathbf{a}, r, \mathbf{s}^{\prime}\right) \sim \mathcal{B}}\left[Q</em>
$$}(\mathbf{s}, \mathbf{a}, \mathbf{z})-\left(r+\bar{V}\left(\mathbf{s}^{\prime}, \overline{\mathbf{z}}\right)\right)\right]^{2}</p>
<p>where $\bar{V}$ is a target network and $\overline{\mathbf{z}}$ indicates that gradients are not being computed through it. The actor loss is nearly identical to SAC, with the additional dependence on $\mathbf{z}$ as a policy input.</p>
<p>$$
\mathcal{L}<em _phi="\phi">{\text {actor }}=\underset{\mathbf{z} \sim q</em>}(\mathbf{z} \mid \mathbf{c})}{\mathbb{E<em _theta="\theta">{\mathbf{s} \sim \mathbf{B}, \mathbf{a} \sim \pi</em>\right)\right]
$$}}}\left[D_{\mathrm{KL}}\left(\pi_{\theta}(\mathbf{a} \mid \mathbf{s}, \overline{\mathbf{z}})\left|\frac{\exp \left(Q_{\theta}(\mathbf{s}, \mathbf{a}, \overline{\mathbf{z}})\right)}{\mathcal{Z}_{\theta}(\mathbf{s})</p>
<p>Note that the context used to infer $q_{\phi}(\mathbf{z} \mid \mathbf{c})$ is distinct from the data used to construct the critic loss. As described in Section 5, during meta-training we sample context batches separately from RL batches. Concretely,the context data sampler $\mathcal{S}_{\mathbf{c}}$ samples uniformly from the most recently collected batch of data, recollected every 1000 meta-training optimization steps. The actor and critic are trained with batches of transitions drawn uniformly from the entire replay buffer.</p>
<h2>6. Experiments</h2>
<p>In our experiments, we assess the performance of our method and analyze its properties. We first evaluate how our approach compares to prior meta-RL methods, especially in terms of sample efficiency, on several benchmark meta-RL problems in Section 6.1. We examine how probabilistic context and posterior sampling enable rapid adaptation via structured exploration strategies in sparse reward settings in Section 6.2. Finally, in Section 6.3, we evaluate the specific design choices in our algorithm through ablations.</p>
<h3>6.1. Sample Efficiency and Performance</h3>
<p>Experimental setup. We evaluate PEARL on six continuous control environments focused around robotic locomotion, simulated via the MuJoCo simulator (Todorov et al.,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Meta-learning continuous control. Test-task performance vs. samples collected during meta-training. Our approach PEARL outperforms previous meta-RL methods both in terms of asymptotic performance and meta-training sample efficiency across six benchmark tasks. Dashed lines correspond to the maximum return achieved by each baseline after 1e8 steps. By leveraging off-policy data during meta-training, PEARL is $20-100 \mathrm{x}$ more sample efficient than the baselines, and achieves consistently better or equal final performance compared to the best performing prior method in each environment. See Appendix A for the full timescale version of this plot.
2012). These locomotion task families require adaptation across reward functions (walking direction for Half-Cheetah-Fwd-Back, Ant-Fwd-Back, Humanoid-Direc-2D, target velocity for Half-Cheetah-Vel, and goal location for Ant-Goal2D) or across dynamics (random system parameters for Walker-2D-Params). These meta-RL benchmarks were previously introduced by Finn et al. (2017) and Rothfuss et al. (2018). All tasks have horizon length 200. We compare to existing policy gradient meta-RL methods ProMP (Rothfuss et al., 2018) and MAML-TRPO (Finn et al., 2017) using publicly available code. We also re-implement the recurrence-based policy gradient $\mathrm{RL}^{2}$ method (Duan et al., 2016) with PPO (Schulman et al., 2017). The results of each algorithm are averaged across three random seeds. We attempted to adapt recurrent DDPG (Heess et al., 2015) to our setting, but were unable to obtain reasonable results with this method. We hypothesize that this is due to a combination of factors including the distribution mismatch in the adaptation data discussed in Section 5 and the difficulty of training with trajectories rather than decorrelated transitions. This approach does not explicitly infer a belief over the task as
we do, instead leaving the burden of both task inference and optimal behavior to the RNN. In PEARL, decoupling task inference from the policy allows us the freedom to choose the encoder data and objective that work best with off-policy learning. We experiment with recurrent architectures in the context of our own method in Section 6.3.</p>
<p>Results. To evaluate on the meta-testing tasks, we perform adaptation at the trajectory level, where the first trajectory is collected with context variable $\mathbf{z}$ sampled from the prior $r(\mathbf{z})$. Subsequent trajectories are collected with $\mathbf{z} \sim q(\mathbf{z} \mid \mathbf{c})$ where the context is aggregated over all trajectories collected. To compute final test-time performance, we report the average returns of trajectories collected after two trajectories have been aggregated into the context. Notably, we find $\mathrm{RL}^{2}$ to perform much better on these benchmarks than previously reported, possibly due to using PPO for optimization and selecting better hyper-parameters. We observe that PEARL significantly outperforms prior meta-RL methods across all domains in terms of both asymptotic performance and sample efficiency, as shown in Figure 3. Here we truncate the $x$-axis at the number of timesteps re-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Sparse 2D navigation. The agent must navigate to a previously unseen goal (dark blue, other test goals in light blue) with reward given only when inside the goal radius - radius of 0.2 (illustrated) and 0.8 are tested here. The agent is trained to navigate to a training set of goals, then tested on a distinct set of unseen test goals. By using posterior sampling to explore efficiently, PEARL is able to start adapting to the task after collecting on average only 5 trajectories, outperforming MAESN (Gupta et al., 2018).
quired for PEARL to converge; see Appendix A for the full timescale version of this plot. We find that PEARL uses 20-100x fewer samples during meta-training than previous meta-RL approaches while improving final asymptotic performance by 50-100\% in five of the six domains.</p>
<h3>6.2. Posterior Sampling For Exploration</h3>
<p>In this section we evaluate whether posterior sampling in our model enables effective exploration strategies in sparse reward MDPs. Intuitively, by sampling from the prior context distribution $r(\mathbf{z})$, the agent samples a hypothesis according to the distribution of training tasks it has seen before. As the agent acts in the environment, the context posterior $p(\mathbf{z} \mid \mathbf{c})$ is updated, allowing it to reason over multiple hypotheses to determine the task. We demonstrate this behavior with a 2-D navigation task in which a point robot must navigate to different goal locations on edge of a semi-circle. We sample training and testing sets of tasks, each consisting of 100 randomly sampled goals. A reward is given only when the agent is within a certain radius of the goal. We experiment with radius 0.2 and 0.8 . While our aim is to adapt to new tasks with sparse rewards, meta-training with sparse rewards is extremely difficult as it amounts to solving many sparse reward tasks from scratch. For simplicity we therefore assume access to the dense reward during meta-training, as
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Recurrent encoder ablation. We compare our encoder architecture to a recurrent network. We sample context as trajectories rather than unordered transitions. Sampling the RL batch as de-correlated transitions ("RNN tran") fares much better than sampling trajectories ("RNN traj").
done by Gupta et al. (2018), but this burden could also be mitigated with task-agnostic exploration strategies.</p>
<p>In this setting, we compare to MAESN (Gupta et al., 2018), a prior method that also models probabilistic task variables and performs on-policy gradient-based meta-learning. We demonstrate we are able to adapt to the new sparse goal in fewer trajectories. Even with fewer samples, PEARL also outperforms MAESN in terms of final performance. In Figure 4 we compare adaptation performance on test tasks. In addition to achieving higher returns and adapting faster, PEARL is also more efficient during meta-training. Our results were achieved with $\sim 1 e 6$ timesteps while MAESN uses $\sim 1 e 8$ timesteps.</p>
<h3>6.3. Ablations</h3>
<p>In this section we ablate the features of our approach to better understand the salient features of our method.</p>
<p>Inference network architecture. We examine our choice of permutation-invariant encoder for the latent context $Z$ by comparing it to a conventional choice for encoding MDPs, a recurrent network (Duan et al., 2016; Heess et al., 2015). Note that while in Section 6.1 we considered a recurrentbased baseline similar to recurrent DDPG (Heess et al., 2015), here we retain all other features of our method and ablate only the encoder structure. We backprop through the RNN to 100 timesteps. We sample the context as full trajectories rather than unordered transitions as in PEARL. We experiment with two options for sampling the RL batch:</p>
<ul>
<li>unordered transitions as in PEARL ("RNN tran")</li>
<li>sets of trajectories ("RNN traj")</li>
</ul>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Context sampling ablation. PEARL samples context batches of recently collected transitions de-correlated with the batches sampled for RL. We compare to sampling context from the entire history ("off-policy"), as well as using the same sampled batch for the context and the RL batch ("off-policy RL").</p>
<p>In Figure 5, we compare the test task performance in the Half-Cheetah-Vel domain as a function of the number of meta-training samples. Replacing our encoder with an RNN results in comparable performance to PEARL, at the cost of slower optimization. However, sampling trajectories for the RL batch results in a steep drop in performance. This result demonstrates the importance of decorrelating the samples used for the RL objective.</p>
<p>Data sampling strategies. In our next experiment, we ablate the context sampling strategy used during training. With sampler $\mathcal{S}<em _mathrm_e="\mathrm{e">{\mathrm{e}}$, PEARL samples batches of unordered transitions that are (1) restricted to samples recently collected by the policy, and (2) distinct from the set of transitions collected by the RL mini-batch sampler. We consider two other options for $\mathcal{S}</em>$ :}</p>
<ul>
<li>sample fully off-policy data from the entire replay buffer, but distinct from the RL batch ("off-policy")</li>
<li>use the same off-policy RL batch as the context ("offpolicy RL-batch")</li>
</ul>
<p>Results are shown in Figure 6. Sampling context off-policy significantly hurts performance. Using the same batch for RL and context in this case helps, perhaps because the correlation makes learning easier. Overall these results demonstrate the importance of careful data sampling in off-policy meta-RL.</p>
<p>Deterministic context. Finally, we examine the importance of modeling the latent context as probabilistic. As discussed in Section 4, we hypothesize that a probabilistic context is particularly important in sparse reward settings because it allows the agent to model a distribution over tasks and conduct exploration via posterior sampling. To test
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Deterministic latent context. We compare PEARL to a variant with deterministic latent context on the sparse reward 2D navigation domain. As expected, without a mechanism for reasoning about uncertainty over tasks, this approach is unable to explore effectively and performs poorly.
this empirically, we train a deterministic version of PEARL by reducing the distribution $q_{\phi}(\mathbf{z} \mid \mathbf{c})$ to a point estimate. We compare probabilistic and deterministic context on the sparse navigation domain in Figure 7. With no stochasticity in the latent context variable, the only stochasticity comes from the policy and is thus time-invariant, hindering temporally extended exploration. As a result this approach is unable to solve a sparse reward navigation task.</p>
<h2>7. Conclusion</h2>
<p>In this paper, we propose a novel meta-RL algorithm, PEARL, which adapts by performing inference over a latent context variable on which the policy is conditioned. Our approach is particularly amenable to off-policy RL algorithms as it decouples the problems of inferring the task and solving it, allowing for off-policy meta-training while minimizing mismatch between train and test context distributions. Modeling the context as probabilistic enables posterior sampling for exploration at test time, resulting in temporally extended exploration behaviors that enhance adaptation efficiency. Our approach obtains superior results compared to prior meta-RL algorithms while requiring far less experience on a diverse set of continuous control meta-RL domains.</p>
<p>Acknowledgements We gratefully acknowledge Ignasi Clavera for running the MAML, ProMP, and RL ${ }^{2}$ baselines. We thank Ignasi Clavera, Abhishek Gupta, and Rowan McAllister for insightful discussions, and Coline Devin, Kelvin Xu, Vitchyr Pong, and Karol Hausman for feedback on early drafts. This research was supported by NSF IIS1651843 and IIS-1614653, Berkeley DeepDrive, the Office of Naval Research, ARL DCIST CRA W911NF-17-2-0181, Amazon, Google, and NVIDIA.</p>
<h2>References</h2>
<p>Alemi, A. A., Fischer, I., Dillon, J. V., and Murphy, K. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016.</p>
<p>Bengio, Y., Bengio, S., and Cloutier, J. Learning a synaptic learning rule. Université de Montréal, 1990.</p>
<p>Duan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., and Abbeel, P. Rl ${ }^{2}$ : Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.</p>
<p>Duan, Y., Andrychowicz, M., Stadie, B., Ho, O. J., Schneider, J., Sutskever, I., Abbeel, P., and Zaremba, W. Oneshot imitation learning. In Advances in neural information processing systems, 2017.</p>
<p>Fei-Fei, L. et al. A bayesian approach to unsupervised oneshot learning of object categories. In Proceedings Ninth IEEE International Conference on Computer Vision, pp. 1134-1141. IEEE, 2003.</p>
<p>Finn, C., Abbeel, P., and Levine, S. Model-agnostic metalearning for fast adaptation of deep networks. In International Conference on Machine Learning, 2017.</p>
<p>Finn, C., Xu, K., and Levine, S. Probabilistic modelagnostic meta-learning. In Advances in Neural Information Processing Systems, pp. 9537-9548, 2018.</p>
<p>Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and Turner, R. Meta-learning probabilistic inference for prediction. In International Conference on Learning Representations, 2019.</p>
<p>Grant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T. Recasting gradient-based meta-learning as hierarchical bayes. In International Conference on Learning Representations, 2018.</p>
<p>Gupta, A., Mendonca, R., Liu, Y., Abbeel, P., and Levine, S. Meta-reinforcement learning of structured exploration strategies. In Advances in Neural Information Processing Systems, 2018.</p>
<p>Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, 2018.</p>
<p>Hausknecht, M. and Stone, P. Deep recurrent q-learning for partially observable mdps. In 2015 AAAI Fall Symposium Series, 2015.</p>
<p>Hausman, K., Springenberg, J. T., Wang, Z., Heess, N., and Riedmiller, M. Learning an embedding space for transferable robot skills. In International Conference on Learning Representations, 2018.</p>
<p>Heess, N., Hunt, J. J., Lillicrap, T. P., and Silver, D. Memorybased control with recurrent neural networks. arXiv preprint arXiv:1512.04455, 2015.</p>
<p>Houthooft, R., Chen, R. Y., Isola, P., Stadie, B. C., Wolski, F., Ho, J., and Abbeel, P. Evolved policy gradients. In Neural Information Processing Systems (NIPS), 2018.</p>
<p>Igl, M., Zintgraf, L., Le, T. A., Wood, F., and Whiteson, S. Deep variational reinforcement learning for pomdps. In International Conference on Machine Learning, 2018.</p>
<p>James, S., Bloesch, M., and Davison, A. J. Task-embedded control networks for few-shot imitation learning. In Conference on Robot Learning, 2018.</p>
<p>Kaelbling, L. P., Littman, M., and Cassandra, A. Planning and acting in partially observable stochastic domains. volume 101, pp. 99-134, 1998.</p>
<p>Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014.</p>
<p>Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P. A simple neural attentive meta-learner. In International Conference on Learning Representations, 2018.</p>
<p>Nagabandi, A., Clavera, I., Liu, S., Fearing, R. S., Abbeel, P., Levine, S., and Finn, C. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. In International Conference on Learning Representations (ICLR), 2019.</p>
<p>Oreshkin, B. N., Lacoste, A., and Rodriguez, P. Tadam: Task dependent adaptive metric for improved few-shot learning. 2018.</p>
<p>Osband, I., Van Roy, B., and Russo, D. (more) efficient reinforcement learning via posterior sampling. In Advances in Neural Information Processing Systems, 2013.</p>
<p>Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep exploration via bootstrapped dqn. In Advances in Neural Information Processing Systems, 2016.</p>
<p>Ravi, S. and Larochelle, H. Optimization as a model for fewshot learning. In International Conference on Learning Representations, 2017.</p>
<p>Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, 2014.</p>
<p>Rothfuss, J., Lee, D., Clavera, I., Asfour, T., and Abbeel, P. Promp: Proximal meta-policy search. In International Conference on Learning Representations, 2018.</p>
<p>Rusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S., and Hadsell, R. Meta-learning with latent embedding optimization. In International Conference on Learning Representations, 2019.</p>
<p>Sæmundsson, S., Hofmann, K., and Deisenroth, M. P. Meta reinforcement learning with latent variable gaussian processes. In Conference on Uncertainty in Artificial Intelligence (UAI), 2018.</p>
<p>Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neural networks. In International Conference on Machine Learning, 2016.</p>
<p>Schmidhuber, J. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987.</p>
<p>Schulman, J., Wolski, F., Dhariwal, P. D., Radford, A. R., and Klimov, O. Proximal policy optimization algorithms. arXiv:1707.06347, 2017.</p>
<p>Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, 2017.</p>
<p>Stadie, B. C., Yang, G., Houthooft, R., Chen, X., Duan, Y., Wu, Y., Abbeel, P., and Sutskever, I. Some considerations on learning to explore via meta-reinforcement learning. arXiv preprint arXiv:1803.01118, 2018.</p>
<p>Strens, M. A bayesian framework for reinforcement learning. In International Conference on Machine Learning, 2000.</p>
<p>Sung, F., Zhang, L., Xiang, T., Hospedales, T., and Yang, Y. Learning to learn: Meta-critic networks for sample efficient learning. arXiv preprint arXiv:1706.09529, 2017.</p>
<p>Tenenbaum, J. B. A Bayesian framework for concept learning. PhD thesis, Massachusetts Institute of Technology, 1999.</p>
<p>Thrun, S. and Pratt, L. Learning to learn. Springer Science \&amp; Business Media, 1998.</p>
<p>Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In International Conference on Intelligent Robots and Systems (IROS), pp. 5026-5033, 2012.</p>
<p>Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Advances in neural information processing systems, 2016.</p>
<p>Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., Blundell, C., Kumaran, D., and Botvinick, M. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.</p>
<p>Xu, T., Liu, Q., Zhao, L., and Peng, J. Learning to explore via meta-policy gradient. In International Conference on Machine Learning, pp. 5459-5468, 2018a.</p>
<p>Xu, Z., van Hasselt, H., and Silver, D. Meta-gradient reinforcement learning. arXiv:1805.09801, 2018b.</p>
<p>Yoon, J., Kim, T., Dia, O., Kim, S., Bengio, Y., and Ahn, S. Bayesian model-agnostic meta-learning. In Advances in Neural Information Processing Systems, pp. 7343-7353, 2018.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Continuous control tasks: left-to-right: the half-cheetah, humanoid, ant, and walker robots used in our evaluation.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Meta-learning continuous control. Test task performance vs. samples collected during meta-training. While in the main paper we truncate the x -axis to better illustrate the performance of PEARL, here we plot PEARL against the on-policy methods run for the full number of time steps ( $1 \epsilon 8$ ). PEARL is 20-100 times more sample efficient. Note that the x -axis is in $\log$ scale.</p>
<h2>A. Experimental Details</h2>
<p>The on-policy baseline approaches require many more samples to learn the benchmark tasks. Here we plot the same data as in Figure 3 for the full number of time steps used by the baselines, in Figure 9. The agents used in these continuous control domains are visualized in Figure 8. Here we describe each meta-learning domain.</p>
<ul>
<li>Half-Cheetah-Dir: move forward and backward (2 tasks)</li>
<li>Half-Cheetah-Vel: achieve a target velocity running forward (100 train tasks, 30 test tasks)</li>
<li>Humanoid-Dir-2D: run in a target direction on 2D grid (100 train tasks, 30 test tasks)</li>
<li>Ant-Fwd-Back: move forward and backward (2 tasks)</li>
<li>Ant-Goal-2D: navigate to a target goal location on 2D grid (100 train tasks, 30 test tasks)</li>
<li>Walker-2D-Params: agent is initialized with some system dynamics parameters randomized and must move forward (40 train tasks, 10 test tasks)</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{*}$ Equal contribution ${ }^{1}$ EECS Department, UC Berkeley, Berkeley, CA, USA. Correspondence to: Kate Rakelly $&lt;$ rakelly@eecs.berkeley.edu&gt;.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5665 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5665</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5665</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-f9525ae89e7fae32c1a081ee44ceca2f1823c675</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f9525ae89e7fae32c1a081ee44ceca2f1823c675" target="_blank">ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.</p>
                <p><strong>Paper Abstract:</strong> ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate metal-organic frameworks (MOFs). By leveraging a large-scale language model (GPT-4 and GPT-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generations. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5665.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5665.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale transformer-based language model used in this work as the central planning, reasoning, and code-generation agent to operate toolkits for MOF data retrieval, property prediction orchestration, and inverse-design workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model employed as the agent (planner/regulator/evaluator) that interprets free-text queries, generates Python code (pandas, ASE, etc.), selects appropriate ML predictors (MOFTransformer), formulates strategies (e.g., genetic-algorithm workflows), and interprets results to produce final answers. The paper also characterizes its superior planning and reasoning compared to GPT-3.5-turbo (e.g., distribution-based strategies to avoid token overflow).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Materials science — metal-organic frameworks (MOFs)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-mediated simulation/control tasks: table search and extraction from MOF databases (look-up), orchestration of property prediction by selecting and querying ML predictors (MOFTransformer) for properties like CO2 Henry coefficients, and directing inverse-design / generative workflows (genetic-algorithm based MOF generation, converting generated genes to structures and estimating properties via ML). The LLM itself does not run ab initio simulations but acts as a text-based simulator/agent to plan and execute simulation pipelines via toolkits.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Task-level accuracy based on human-defined labels: True / False (Token Limit Exceeded) / False (Logic Error); reported as percent accuracy (number of 'True' responses over total, with some reported values excluding token-limit errors).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>For GPT-4 integrated ChatMOF (excluding 'Token Limit Exceeded' cases): Search task 96.9% (on 100 samples, 4 token-limit failures), Prediction task 95.7% (on 100 samples, 6 token-limit failures), Generation task 87.5% (on 10 samples, 2 token-limit failures). Absolute token-limit counts: search 4/100, prediction 6/100, generation 2/10.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Token limits (excessively long outputs or code leading to LLM token overflow), plan/logic quality (erroneous strategy selection or code generation), incorrect selection/loading of downstream ML model (wrong fine-tuned MOFTransformer variant or incompatible input), insufficient or missing database entries (no parent genes for generation), model reasoning capacity (GPT-4 better than GPT-3.5-turbo), and the intrinsic lack of complete material-specific textual representation in single LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Quantitative error analysis labeling outputs as 'Token Limit Exceeded' vs 'Logic Error' across 100/100/10 sampled queries per task; comparison of GPT-4 vs GPT-3.5-turbo accuracies showing GPT-4 higher on all tasks; qualitative example where GPT-3.5-turbo attempted to list values for many materials causing a token error while GPT-4 used distributional summary statistics (mean/variance/quartiles) to produce a compact, informative response, demonstrating reasoning strategy differences that reduce token errors; descriptions of failure modes when wrong fine-tuned ML model was chosen or when generated genes had no parents in the database.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated/structured evaluation on synthetic question sets (questions generated using GPT-4) with 100 sample questions for search and prediction tasks and 10 for generation task; outputs were classified into three labels (True, False (Token Limit Exceeded), False (Logic Error)) and aggregated to compute accuracy (counts and percentages). Some qualitative error analysis and illustrative examples were provided to explain failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Frequent failure modes include 'Token Limit Exceeded' when outputs (code or lists) exceed the LLM token budget, 'Logic Error' when generated plans or code are flawed (e.g., wrong strategy to retrieve required number of items, using an incompatible ML model or bad input formatting), inability to find parent genes for constrained generation objectives, and inherent limitations of a single LLM lacking domain-specific material data (mitigated in part by delegating numeric prediction to dedicated ML models).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct head-to-head comparison in the paper between GPT-4 and GPT-3.5-turbo: GPT-4 outperformed GPT-3.5-turbo on all three tasks (excluding token-limit cases): GPT-4: search 96.9%, prediction 95.7%, generation 87.5%; GPT-3.5-turbo: search 95%, prediction 91%, generation 77.8%. Qualitative example shows GPT-4 using distributional summaries vs GPT-3.5 attempting exhaustive listings (leading to token errors).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use a stronger-reasoning LLM (GPT-4) for planning and concise summarization to avoid token-limit failures; delegate numeric/property estimation to specialized pre-trained/fine-tuned ML models (e.g., MOFTransformer) rather than asking the LLM to simulate physics directly; select the correct fine-tuned predictor model for the requested property; keep generated code/output compact to stay within token limits; adapt strategies when search/generation returns no parents (expand search/data), and perform explicit error handling for logic/code errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5665.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5665.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior-generation transformer LLM used in the study as an alternative agent; performs planning/code generation but exhibits weaker reasoning and more token/logic failures compared to GPT-4 in the ChatMOF pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model used as an agent in ChatMOF for parsing user queries, generating code, selecting toolkits, and orchestrating workflows; characterized in the paper as having inferior planning/strategy compared to GPT-4, leading to higher rates of logic errors and token-limit failures in complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Materials science — metal-organic frameworks (MOFs)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Same agent tasks as GPT-4: table/database search orchestration, selection/usage of ML predictors (MOFTransformer) for property estimation, and directing generative workflows for inverse design; acts as a text-based controller rather than performing physics-based simulations itself.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Task-level accuracy with labels True / False (Token Limit Exceeded) / False (Logic Error); percent accuracy reported excluding token-limit cases in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Excluding 'Token Limit Exceeded' cases, ChatMOF with GPT-3.5-turbo achieved: Search task 95%, Prediction task 91%, Generation task 77.8% (numbers reported in the paper as comparison to GPT-4). Token-limit instances occurred but exact counts for GPT-3.5-turbo were not separately enumerated beyond these summary accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Weaker planning/reasoning compared to GPT-4 leading to less efficient strategies (e.g., trying to list values exhaustively causing token overflow), susceptibility to token limits, and higher probability of logic/code generation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Reported comparative accuracies showing lower performance across tasks; a specific illustrative example where GPT-3.5-turbo chose an exhaustive retrieval strategy that caused a token error, whereas GPT-4 used distributional summaries to respond without overflow.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same evaluation protocol as for GPT-4: sets of synthetic questions (100/100/10 for search/prediction/generation) with outputs labeled True/False(Token Limit)/False(Logic Error) and aggregated to compute accuracy percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prone to token-limit failures when asked to enumerate many items or to provide exhaustive lists, more frequent logic errors in strategy/code generation, and lower success in complex inverse-design (generation) tasks relative to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to GPT-4 in the paper; GPT-4 consistently outperformed GPT-3.5-turbo on search, prediction, and generation tasks (see reported accuracies). Example analysis contrasts the two models' planning strategies for a pore-limiting-diameter comparison query.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer higher-reasoning LLMs (e.g., GPT-4) for planning and complex multi-step workflows; constrain output verbosity and use summarization/aggregation strategies instead of exhaustive listing to avoid token-limit failures; rely on specialized ML predictors for numeric/material property inference instead of LLM-only estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>HuggingGPT: Solving AI Tasks with ChatGPT and Hugging Face Models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5665",
    "paper_id": "paper-f9525ae89e7fae32c1a081ee44ceca2f1823c675",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A large-scale transformer-based language model used in this work as the central planning, reasoning, and code-generation agent to operate toolkits for MOF data retrieval, property prediction orchestration, and inverse-design workflows.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Transformer-based large language model employed as the agent (planner/regulator/evaluator) that interprets free-text queries, generates Python code (pandas, ASE, etc.), selects appropriate ML predictors (MOFTransformer), formulates strategies (e.g., genetic-algorithm workflows), and interprets results to produce final answers. The paper also characterizes its superior planning and reasoning compared to GPT-3.5-turbo (e.g., distribution-based strategies to avoid token overflow).",
            "model_size": null,
            "scientific_subdomain": "Materials science — metal-organic frameworks (MOFs)",
            "simulation_task": "Text-mediated simulation/control tasks: table search and extraction from MOF databases (look-up), orchestration of property prediction by selecting and querying ML predictors (MOFTransformer) for properties like CO2 Henry coefficients, and directing inverse-design / generative workflows (genetic-algorithm based MOF generation, converting generated genes to structures and estimating properties via ML). The LLM itself does not run ab initio simulations but acts as a text-based simulator/agent to plan and execute simulation pipelines via toolkits.",
            "accuracy_metric": "Task-level accuracy based on human-defined labels: True / False (Token Limit Exceeded) / False (Logic Error); reported as percent accuracy (number of 'True' responses over total, with some reported values excluding token-limit errors).",
            "reported_accuracy": "For GPT-4 integrated ChatMOF (excluding 'Token Limit Exceeded' cases): Search task 96.9% (on 100 samples, 4 token-limit failures), Prediction task 95.7% (on 100 samples, 6 token-limit failures), Generation task 87.5% (on 10 samples, 2 token-limit failures). Absolute token-limit counts: search 4/100, prediction 6/100, generation 2/10.",
            "factors_affecting_accuracy": "Token limits (excessively long outputs or code leading to LLM token overflow), plan/logic quality (erroneous strategy selection or code generation), incorrect selection/loading of downstream ML model (wrong fine-tuned MOFTransformer variant or incompatible input), insufficient or missing database entries (no parent genes for generation), model reasoning capacity (GPT-4 better than GPT-3.5-turbo), and the intrinsic lack of complete material-specific textual representation in single LLMs.",
            "evidence_for_factors": "Quantitative error analysis labeling outputs as 'Token Limit Exceeded' vs 'Logic Error' across 100/100/10 sampled queries per task; comparison of GPT-4 vs GPT-3.5-turbo accuracies showing GPT-4 higher on all tasks; qualitative example where GPT-3.5-turbo attempted to list values for many materials causing a token error while GPT-4 used distributional summary statistics (mean/variance/quartiles) to produce a compact, informative response, demonstrating reasoning strategy differences that reduce token errors; descriptions of failure modes when wrong fine-tuned ML model was chosen or when generated genes had no parents in the database.",
            "evaluation_method": "Automated/structured evaluation on synthetic question sets (questions generated using GPT-4) with 100 sample questions for search and prediction tasks and 10 for generation task; outputs were classified into three labels (True, False (Token Limit Exceeded), False (Logic Error)) and aggregated to compute accuracy (counts and percentages). Some qualitative error analysis and illustrative examples were provided to explain failure modes.",
            "limitations_or_failure_cases": "Frequent failure modes include 'Token Limit Exceeded' when outputs (code or lists) exceed the LLM token budget, 'Logic Error' when generated plans or code are flawed (e.g., wrong strategy to retrieve required number of items, using an incompatible ML model or bad input formatting), inability to find parent genes for constrained generation objectives, and inherent limitations of a single LLM lacking domain-specific material data (mitigated in part by delegating numeric prediction to dedicated ML models).",
            "comparisons": "Direct head-to-head comparison in the paper between GPT-4 and GPT-3.5-turbo: GPT-4 outperformed GPT-3.5-turbo on all three tasks (excluding token-limit cases): GPT-4: search 96.9%, prediction 95.7%, generation 87.5%; GPT-3.5-turbo: search 95%, prediction 91%, generation 77.8%. Qualitative example shows GPT-4 using distributional summaries vs GPT-3.5 attempting exhaustive listings (leading to token errors).",
            "recommendations_or_best_practices": "Use a stronger-reasoning LLM (GPT-4) for planning and concise summarization to avoid token-limit failures; delegate numeric/property estimation to specialized pre-trained/fine-tuned ML models (e.g., MOFTransformer) rather than asking the LLM to simulate physics directly; select the correct fine-tuned predictor model for the requested property; keep generated code/output compact to stay within token limits; adapt strategies when search/generation returns no parents (expand search/data), and perform explicit error handling for logic/code errors.",
            "uuid": "e5665.0",
            "source_info": {
                "paper_title": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "A prior-generation transformer LLM used in the study as an alternative agent; performs planning/code generation but exhibits weaker reasoning and more token/logic failures compared to GPT-4 in the ChatMOF pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Transformer-based large language model used as an agent in ChatMOF for parsing user queries, generating code, selecting toolkits, and orchestrating workflows; characterized in the paper as having inferior planning/strategy compared to GPT-4, leading to higher rates of logic errors and token-limit failures in complex tasks.",
            "model_size": null,
            "scientific_subdomain": "Materials science — metal-organic frameworks (MOFs)",
            "simulation_task": "Same agent tasks as GPT-4: table/database search orchestration, selection/usage of ML predictors (MOFTransformer) for property estimation, and directing generative workflows for inverse design; acts as a text-based controller rather than performing physics-based simulations itself.",
            "accuracy_metric": "Task-level accuracy with labels True / False (Token Limit Exceeded) / False (Logic Error); percent accuracy reported excluding token-limit cases in the paper.",
            "reported_accuracy": "Excluding 'Token Limit Exceeded' cases, ChatMOF with GPT-3.5-turbo achieved: Search task 95%, Prediction task 91%, Generation task 77.8% (numbers reported in the paper as comparison to GPT-4). Token-limit instances occurred but exact counts for GPT-3.5-turbo were not separately enumerated beyond these summary accuracies.",
            "factors_affecting_accuracy": "Weaker planning/reasoning compared to GPT-4 leading to less efficient strategies (e.g., trying to list values exhaustively causing token overflow), susceptibility to token limits, and higher probability of logic/code generation errors.",
            "evidence_for_factors": "Reported comparative accuracies showing lower performance across tasks; a specific illustrative example where GPT-3.5-turbo chose an exhaustive retrieval strategy that caused a token error, whereas GPT-4 used distributional summaries to respond without overflow.",
            "evaluation_method": "Same evaluation protocol as for GPT-4: sets of synthetic questions (100/100/10 for search/prediction/generation) with outputs labeled True/False(Token Limit)/False(Logic Error) and aggregated to compute accuracy percentages.",
            "limitations_or_failure_cases": "Prone to token-limit failures when asked to enumerate many items or to provide exhaustive lists, more frequent logic errors in strategy/code generation, and lower success in complex inverse-design (generation) tasks relative to GPT-4.",
            "comparisons": "Directly compared to GPT-4 in the paper; GPT-4 consistently outperformed GPT-3.5-turbo on search, prediction, and generation tasks (see reported accuracies). Example analysis contrasts the two models' planning strategies for a pore-limiting-diameter comparison query.",
            "recommendations_or_best_practices": "Prefer higher-reasoning LLMs (e.g., GPT-4) for planning and complex multi-step workflows; constrain output verbosity and use summarization/aggregation strategies instead of exhaustive listing to avoid token-limit failures; rely on specialized ML predictors for numeric/material property inference instead of LLM-only estimation.",
            "uuid": "e5665.1",
            "source_info": {
                "paper_title": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2
        },
        {
            "paper_title": "HuggingGPT: Solving AI Tasks with ChatGPT and Hugging Face Models",
            "rating": 2
        }
    ],
    "cost": 0.00980325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChatMOF: An Autonomous AI System for</h1>
<h2>Predicting and Generating Metal-Organic</h2>
<h2>Frameworks</h2>
<p>Yeonghun Kang, Jihan Kim*</p>
<p>Department of Chemical and Biomolecular Engineering, Korea Advanced Institute of Science and Technology (KAIST), 291, Daehak-ro, Yuseong-gu, Daejeon 34141, Republic of Korea</p>
<h1>ABSTRACT</h1>
<p>ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate metal-organic frameworks (MOFs). By leveraging a large-scale language model (GPT-4 and GPT-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generations. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.</p>
<h1>Introduction</h1>
<p>The realm of generative artificial intelligence (AI) is witnessing an unprecedented surge, predominantly fostered by a new generation of computational tools known as large-scale language models (LLMs) ${ }^{1-5}$. These innovative models are deeply rooted in a novel architectural design paradigm, referred to as transformer models ${ }^{6}$. Their capabilities, however, stretch far beyond the domain of basic language tasks. Emulating human thought processes, they exhibit logical functions such as learning from a limited number of examples (few-shot learning), or in certain situations, learning without any examples (zero-shot learning). ${ }^{3,5,7}$ This is achieved through the understanding of vast volumes of text data, underscoring the immense potential held by these models. A significant development in this rapidly advancing field is the rise of autonomous LLM agents, which uses LLMs for a wide range of tasks ${ }^{8-10}$. This system, powered by multiple LLMs, excels in making decisions on its own. As such, the idea of using LLMs to independently reason, find information, and produce answers is becoming more popular across different areas of research fields ${ }^{11-14}$.</p>
<p>Despite marked progress in application of LLM across diverse fields ${ }^{15,16}$, the full potential of its advanced technology within materials science remains largely untapped. This limitation primarily stems from two considerable challenges. Firstly, the inherent complexity of materials poses a significant hurdle as these materials often lack sufficient text-compatible input representations, thereby failing to fully encapsulate their complete properties ${ }^{17-20}$. This difficulty in encoding materials for LLMs restricts their understanding and processing capabilities. Secondly, there is a notable scarcity of material-specific training data in the field. In comparison to other disciplines, materials science lags behind due to fewer dedicated databases and their associated data, exacerbating the challenge of representing this scant data in a text format suitable for LLMs.</p>
<p>Despite these obstacles, there are ongoing attempts to leverage the capabilities of LLMs in materials science. ${ }^{21-23}$ However, so far, these efforts have primarily focused on extracting data from scientific literature and generating responses based on this extracted data, with the actual material itself remaining a largely untouched resource. As such, the exploration and realization of the full potential of LLMs within the sphere of materials science still beckons.</p>
<p>In this work, we highlight the development of an innovative methodology that utilizes autonomous AI to generate new materials and to predict properties of the materials with the focus specifically on metal-organic frameworks (MOFs) ${ }^{24-26}$. MOFs are used in many chemical applications ${ }^{27-30}$ due to their large porosity ${ }^{31-33}$, high surface area ${ }^{32}$, and exceptional tunability ${ }^{34}$. To this end, we have developed the autonomous AI system for MOF (called ChatMOF), which holds the potential to predict MOF properties from text-based inquiries and to generate MOFs with specified properties (i.e. inverse design). This pioneering approach can potentially significantly bridge the gap between the novice users and the computational and machine learning tools, which can potentially facilitate the progress in developing new materials for various applications.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. A Conceptual image that explains the ChatMOF. When a user poses a textual question about the properties of a MOF, an appropriate answer is provided by ChatMOF. If a user desires to generate a new MOF, ChatMOF is capable of creating a new MOF that satisfies the condition.</p>
<h1>Results and Discussion</h1>
<h2>Design for ChatMOF</h2>
<p>The effectiveness of autonomous LLM agents is predicated on its capability to accurately extract essential details from textual inputs and offer relevant responses, irrespective of the presence of a rigidly structured query. ${ }^{21}$ This concept is vividly illustrated in ChatMOF, as demonstrated in Figure 1. A user may pose a query in textual form regarding the properties of a material, to which ChatMOF responds by supplying a detailed description related to the material in question. Moreover, the operational scope of this system extends beyond the simple retrieval of information. When a user expresses the need to generate a MOF with specific properties, ChatMOF is capable of generating the requested material structure accordingly.</p>
<p>In the context of ChatMOF, LLMs performs the role of a central planner, regulator and evaluator, akin to a human brain. Although LLM effectively fulfills the role of reasoning, its proficiency in specialized tasks falls short. While a single LLM's proficiency in specialized tasks may be limited ${ }^{15}$, LLMs offer an effective way of integrating and employing various databases and machine learning models seamlessly due to their inherent capacity for reasoning ${ }^{35,36}$. ChatMOF utilizes the LLM to orchestrate a comprehensive plan and employ toolkits for information gathering, comparable to an expert devising a strategy, writing the code, and executing the plan. This synergy allows the system to precisely predict material properties, deduce synthesis techniques, and fabricate new materials with preset properties.</p>
<p>As depicted in Figure 2, ChatMOF is composed of three main components: an agent, toolkit, and an evaluator. The agent responds to human queries by traversing four primary steps (i.e. thoughts, actions, action inputs, and observation), mirroring the methodology outlined in the ReAct $^{37}$ and MRKL papers ${ }^{38}$. Initially, the user's query is set as the end goal, followed by</p>
<p>contemplation about the path to achieve this goal. Subsequently, ChatMOF decides on the appropriate toolkit to employ from the available options. After the chosen toolkit is executed, the observed results serve as the basis for evaluating whether a final answer can be generated. If feasible, the final answer is presented, otherwise, the process cycles back to the thought step to formulate a new strategy.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. The schematic image of ChatMOF. ChatMOF comprises three core components: an agent, toolkits, and an evaluator. Upon receiving a query from human, the agent formulates a plan and selects a suitable toolkit. Subsequently, the toolkit generates outputs following the proposed plan, and the evaluator makes these results into a final response.</p>
<h1>Toolkits</h1>
<p>ChatMOF employs an assortment of toolkits to acquire, predict, or generate material information. These toolkits can be primarily classified into five categories: table-searcher, internet-searcher, predictor, generator, and others. Table-searcher involves obtaining desired information from existing data, while internet-searcher employs internet resources, such as Google and Bing, to produce desired information. The predictor utilizes machine learning models to obtain specified properties. The generator refers to the toolkit that constructs material structures fulfilling certain properties. Lastly, the others encompass a variety of aids like calculators, file saving and reading functions, and visualization toolkits.</p>
<p>Due to the facile synthesis MOF structures, there are many different database associated with the MOF structures: (1) computational-ready experimental MOFs (CoREMOF) ${ }^{39,40}$ and (2) quantum MOF (QMOF) database ${ }^{41}$. The CoREMOF database is an archive of synthesized materials present in a CSD MOF subset ${ }^{42}$, encompassing computations of various properties of MOFs including geometric descriptors. The QMOF database is populated with electrical property data, such as bandgap, formation energy, homo, and lumo, derived through DFT calculations. When a user demands these electrical properties, ChatMOF seeks and extracts them from the QMOF database.</p>
<p>As such, if a user seeks information about a specific MOF that is included in these databases, ChatMOF can locate and deliver the desired information from the pre-tabulated data. Figure 3 provides an illustrative example of a table-search operation conducted by ChatMOF. When faced with a user query, ChatMOF devises a strategic approach to extract the requisite data. Subsequently, it creates a Python code tailored to retrieve specific information from the database in accordance with the premeditated strategy. This code typically uses the "pandas ${ }^{43 \text { " }}$ library to</p>
<p>extract or filter relevant details. The drafted code is then executed within the ChatMOF's designated executor. Upon observing the outcome of this operation, ChatMOF contemplates the necessary steps to derive the final answer, culminating in the delivery of the requested response.</p>
<p>The versatility of ChatMOF extends to handling diverse table data derived from text mining or rule-based coding processes. For questions related to the building blocks of a MOF, the MOFkey ${ }^{17}$ database proves to be instrumental. This particular database leverages rule-based methods to obtain insights about the organic linkers and metal clusters of a MOF, providing details about its topology and the potential presence or absence of interpenetration. In addition, for users seeking guidance on MOF synthesis, the DigiMOF ${ }^{44}$ database becomes a valuable resource. DigiMOF provides an array of synthesis conditions, extracted via text mining techniques from MOF-related academic papers, and includes information on organic and metal precursors, and solvent.</p>
<h1>Question: How high is the accessible surface area of JUKPAI?</h1>
<p>Thought: I need to find the accessible surface area of JUKPAI
Action: search_csv
Action Input: "Search name JUKPAI and provide information of its accessible surface area"
[Table Searcher] Thought: To search for a specific value in the DataFrame, we can use the ... [Table Searcher] Input:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">python</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">print (df:loc[df['name'] == 'JUKPAI'; <br> 'Accessible Surface Area (m^2/cm^3''] lo_markdown())</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>[Table Searcher] Observation:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: left;">Accessible Surface Area (m^2/cm^3)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">4837</td>
<td style="text-align: left;">1474.22</td>
</tr>
</tbody>
</table>
<p>[Table Searcher] Final Thought: The accessible surface area of name 'JUKPAI' is 1474.22 ...
Thought: I now know the final answer</p>
<p>Answer : The accessible surface area for JUKPAI is $1474.22 \mathrm{~m}^{2} / \mathrm{cm}^{2}$</p>
<p>Figure 3. Example of a Table Searcher for the question "How high is the surface area of JUKPAI?". The human's question prompts the system to devise a strategy. From this, Python code is generated and executed to extract the desired information, which is then returned as the final answer.</p>
<p>The accuracy of the look-up table search is contingent on the pre-calculated values available in the specific files. And for queries regarding the properties of MOFs that are not available, computational simulation can serve as an attractive alternative method, but unfortunately, simulations are a time-intensive process and an abundance of computational resources ${ }^{45}$. The best resolution to such challenges is the application of machine learning models, which enable highaccuracy predictions grounded in extensive data. In the case of pre-trained machine learning models, predictions are quick and can be made for a significant volume of substances simultaneously, making it an excellent toolkit for integration into ChatMOF.</p>
<p>As an appropriate toolkit for the prediction task, ChatMOF uses the MOFTransformer ${ }^{46,47}$ model that has been developed in our group for the universal prediction of MOF properties. This model leverages both local features, such as atoms and bonds, and global features like surface area and topology. Having undergone pre-training with one million hypothetical MOFs and subsequent fine-tuning for specific properties, MOFTransformer shows high performance in predicting various properties. Moreover, it affords the convenience of predicting the desired property by retaining the model structure while altering the weights of a model that's been fine-tuned for the desired property.</p>
<p>The key to generating accurate responses is selecting the appropriate fine-tuned model with the MOFTransformer and the material to which it will be applied, based on the query. Similar to HuggingGPT ${ }^{10}$, ChatMOF does not directly compute the material properties, but it rather selects the appropriate machine learning model. Figure 4 shows the example prompt for ChatMOF using the MOFTransformer predictor. When a user asks a question such as "At room temperature (298K), what's the CO2 Henry coefficient for XEGKUR?", generator develops a strategy for solving the problem. The optimal fine-tuned model of MOFTransformer that aligns with the plan, in this case,</p>
<p>'CO2_henry_coefficient_298K' is chosen, and the material to predict (XEGKUR) is decided. Following this, the selected model is employed to make prediction. Finally, it make a response to the query using a language learning model based on the observed values from the machine learning model. Since this is a logarithmic value, ChatMOF selects the "Python_REPL" tool and calculates an exponential value to get the final answer.</p>
<p>Question: At room temperature (298K), what's the $\mathrm{CO}_{2}$ Henry coefficient for XEGKUR?</p>
<p>Thought: I need to find the $\mathrm{CO}<em 2="2">{2}$ Henry coefficient for XEGKUR at room temperature Action: search_csv
Thought: The search_csv tool did not provide any information on the $\mathrm{CO}</em>$ Henry coefficient... Action: predictor
Action Input: Predict the $\mathrm{CO}_{2}$ Henry coefficient for XEGKUR at 298 K
[predictor] Thought: I need to gather data on XEGKUR and train a machine learning to pre... [predictor] Property: CO2_henry_coefficient_298K [predictor] Materials: XEGKUR
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Answer : The CO2 Henry coefficient for XEGKUR at room temperature (298K) approximately $0.027 \mathrm{~mol} / \mathrm{Kg} \cdot \mathrm{Pa}$</p>
<p>Figure 4. Example of a predictor for the question "At room temperature (298K), what's the CO2 Henry coefficient for XEGKUR?" The predictor sets up a plan to solve the question, an appropriate model, and target material. Based on this, it uses machine learning to predict the value, which is then used to derive the final answer.</p>
<p>Finally, a key aspiration among researchers in the field of MOFs is the inverse design of MOFs exhibiting desired properties. In materials science, various generative models, including Generative Adversarial Networks (GAN) ${ }^{48,49}$ and Diffusion models ${ }^{50}$, have been employed for inverse design. However, due to the inherent complexity of MOFs, which includes a large number of atoms, large void fraction, and complex topologies, an atom-by-atom inverse design approach has been elusive. As a workaround, MOF inverse design has been facilitated top-down approaches leveraging through genetic algorithms ${ }^{51-53}$, Variational Autoencoders ${ }^{54}$ (VAE), or reinforcement learning ${ }^{55}$ for the selection of building blocks and their placement into suitable topologies.</p>
<p>Genetic algorithms are notably suitable for integration with LLMs. As a bio-inspired optimization methodology, genetic algorithms operate on a selection, mutation, and crossover principle, making them adaptable and efficient. ${ }^{56}$ For their application to MOFs, these frameworks must be delineated by genes comprising topology and building blocks. For instance, a representative MOF, HKUST-1, can be depicted as tbo+N17+N10, with tbo representing topology and N17 and N10 representing the building block notations. As these gene representations are textual, they facilitate the application of genetic algorithms using an LLM.</p>
<p>Figure 5 showcases the utilization of a genetic algorithm by ChatMOF to fabricate a MOF per user specifications. Upon a user query, the system develops a strategy optimized for the genetic algorithm. It also identifies the target property and determines the loss function most suited for the objective, such as choosing the maximum, minimum, or closest value. Guided by this plan, ChatMOF selects parent genes from the existing database that align with the loss function. Using LLM, it produces children genes from the selected parent genes that are most likely to achieve the target. These children are then transformed into a structure file, and their properties are estimated through machine learning. This procedure is reiterated a fixed number of times, generating</p>
<p>multiple generations of children with each generation yielding MOFs progressively nearer to the target. From the created structures, the one that aligns most closely with the question is finally chosen and presented as the response.</p>
<p>Question: Can you generate the structures with the largest surface area?
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5. Example of a generator for the question "Can you generate the structures with the largest surface area". The generator establishes a plan, objective and property for the human question. Based on this, it finds parents that satisfy the objective. It uses a genetic algorithm to create children genes and generate structures. This is repeated for a number of cycles to generate new MOFs, which are used to derive the final answer.</p>
<p>Moreover, ChatMOF is engineered to perform a diverse set of toolkits, which extend beyond the realms of LLMs. This includes capabilities such as file search, Internet search, and even simple calculations. These additional functionalities are primarily enabled by leveraging the varied capabilities provided by LangChain ${ }^{57}$, enhancing the overall functionality and utility of ChatMOF. Thus, it is not merely a material analysis tool, but a comprehensive system that can accommodate a wide array of tasks and operations.</p>
<p>In addition, ChatMOF incorporates the Atomic Simulation Environment (ASE) ${ }^{58}$ library as an integral toolkit to facilitate diverse operations on material structure data. The ASE library holds considerable importance in the field of materials science due to its capabilities, including atom manipulation, cell information acquisition, and visualization, among others. Similar to the function of a table searcher, when confronted with a query, ChatMOF devises a strategic plan and constructs suitable Python code utilizing the ASE library to fulfil the query's demands. Subsequently, this code is executed.</p>
<h1>Evaluation</h1>
<p>To evaluate performance of ChatMOF, analysis was conducted for "search task", "prediction task", and "generation task". For evaluation purposes, questions for ChatMOF were created utilizing GPT-4.0 to generate various sentences about the given properties of a MOF. The respective questions for each task can be found in Table S1-3. Accuracy, gauging how adequately the logic responded to the question, was measured for each task. An analysis of the accuracy of ChatMOF utilized three labels: "True", "False (token limit exceeded)", and "False (logic error)". The label "True" signifies that ChatMOF's logic was precise and the yielded answer was accurate. The term "False (Token Limit Exceeded)" was used when the token count in LLM surpassed the maximum allowance of 4,000, thus obstructing further progress. Lastly, the "False (Logic Error)" label designated situations where an error in ChatMOF's logic resulted in an incorrect response or an anomaly. Such situations typically occur when an erroneous plan for obtaining an answer was devised or when an error in output interpretation diverts the system from the desired direction.</p>
<p>Figure 6 presents the accuracy measurements for the three tasks using ChatMOF with GPT-4. Accuracy was measured for 100 sample questions for the search and prediction tasks, and 10 sample questions for the generation task. The number in the bar graph indicates the number of each question in each class. Both the search and prediction tasks rendered accurate answers with high frequency. Excluding 'Token Limit Exceeded' instances (4 out of 100, 6 out of 100, and 2 out of 100, for search, prediction, and generation tasks respectively), they exhibit high accuracies of $96.9 \%$ and $95.7 \%$, respectively. For the generation task, the accuracy stood at $87.5 \%$. Given its complexity, relative to the other two tasks, the accuracy rate is lower. Regardless, all three tasks report high accuracy rates, and these tasks carry significance weight because these are tasks that a typical LLM fails to execute flawlessly. A single LLM fails to provide precise information since</p>
<p>it lacks specific information about the material, particularly for obtaining property information, which is challenging to source via an internet search.</p>
<p>Also ChatMOF, when integrated with GPT-4, exhibits superior performance compared to its integration with GPT-3.5-turbo. As evidenced in Figure S2, the accuracy of ChatMOF with GPT-3.5-turbo stands at $95 \%, 91 \%$, and $77.8 \%$ for the search, prediction, and generation tasks respectively, excluding instances of "Token Limit Exceeded". Across all tasks, GPT-4 consistently outperforms GPT-3.5-turbo in accuracy. This enhanced accuracy of GPT-4 can be attributed to its refined reasoning and comprehension capabilities, particularly during the planning phase. Figure S3 illustrates the distinct approaches that GPT-4 and GPT-3.5-turbo take when presented with the same query: "How does the pore limiting diameter of YUSGID_clean compare with other materials?". While GPT-3.5-turbo seeks the values for all materials mentioned in the query, leading to a token error and subsequent inability to provide an answer, GPT-4 adopts a more holistic strategy. It assesses the distribution of all materials, leveraging metrics such as mean, variance, and quartile values of the property in question. This approach enables GPT-4 to determine the relative position of the target material in the overall distribution, thus delivering a more informative response to the user.</p>
<p>For the "search task," the writing of code utilizing the pandas library significantly impacts the accuracy. 'Token Limit Exceeded' generally occurs when the output code surpasses the permissible token count. This frequently arises when all relevant materials that satisfy a given condition are provided (for example, when a list of materials with a particular property is listed), or when the question contains a comparative clause such as "compared to other materials." 'Logic Error' typically surfaces when there is a flawed strategic approach or a code error. An instance of this</p>
<p>would be when a request to provide 10 specific items is met with a misguided strategy that solely aims to "extract high values," failing to retrieve the specified number of items.</p>
<p>During the "prediction task," difficulties often occur in the interpretation process of the observed values using machine learning techniques. Both the 'Token Limit Exceeded' and 'Logic Error' occurrences can stem from the effort to draw the correct answer from the table based on the estimated values. 'Logic Errors' can manifest not only during the table search phase but also during the strategy formulation stage. An erroneous strategy could either lead to the loading of an unsuitable model or to the generation of an input that is incompatible with the intended model.</p>
<p>The "generation task" presents a more intricate set of challenges, inviting a variety of errors. A frequently observed 'Logic Error' appears when no parent genes can be retrieved from database. If the objective function aims for maximum or minimum values, a satisfying parent gene can always be found. However, if the goal is to get close to a certain value or to fall within a specific range, the selected range might not yield any satisfying parent genes. In such scenarios, the strategy is adapted to incorporate more data. However, if no suitable parent genes are found even after modifying the strategy, it results in an error. Further, both 'Token Limit Exceeded' and 'Logic Error' might occur during the extraction of the most suitable MOF from the generated MOFs, aligning with the objective function.</p>            </div>
        </div>

    </div>
</body>
</html>
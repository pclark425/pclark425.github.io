<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6391 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6391</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6391</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-277349284</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.21380v2.pdf" target="_blank">Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1, OpenAI's o3-mini and Gemini 2.5 Pro Exp demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the benchmark, evaluation code, detailed results and a data visualization tool at https://github.com/RUCAIBox/OlymMATH.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6391.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6391.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini 2.5 Pro Exp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 2.5 Pro Experimental (0325)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source state-of-the-art reasoning LLM evaluated on OlymMATH; shows the strongest performance among tested models on the hard Olympiad subset but still far from human-level on the hardest problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 2.5 Pro Exp 0325</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper; generally pretrained on large multilingual web and curated corpora (paper notes English-dominant pretraining benefits).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>OlymMATH (EN-HARD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step math problems (number theory, algebra, geometry, combinatorics)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language text problems in MATH-format with numeric (real) answers</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level (HARD subset)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Evaluated in 'slow-thinking' / long chain-of-thought modes; sampling with multiple seeds (Pass@1 reported; Cons@k consensus via majority voting also used).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@1 accuracy (also Cons@8/Cons@64 consensus reported elsewhere in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>58.4% accuracy on OlymMATH-EN-HARD (Pass@1)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic probes reported (no attention/logit-lens/activation probing). Analysis is behavioral: models produce long chain-of-thought traces which are collected and inspected qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Prone to heuristic/shortcut reasoning in some cases; examples in the paper show reliance on intuition rather than fully rigorous derivations in other benchmarks, but on OlymMATH-HARD remaining errors are substantial (missing rigorous derivations or making incorrect symmetry/assumption shortcuts).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Among evaluated models, Gemini 2.5 Pro Exp is one of the top performers, supporting the paper's observation that larger/more-recent models and slow-thinking modes improve performance but do not solve Olympiad-hard problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6391.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6391.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o3-mini (high)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o3-mini (high)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cost‑effective OpenAI model evaluated under long chain-of-thought / slow-thinking modes; achieves moderate accuracy on OlymMATH but shows systematic reasoning shortcuts and failures on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o3-mini (high)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in paper; described as a modern OpenAI model trained on large web corpora; paper notes that such models often benefit from English-dominated pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>OlymMATH (EN-HARD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step math problems (including modular arithmetic and optimization problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language problems rendered in MATH-style; answers are numeric and verifiable</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level (HARD subset)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Assessed in slow-thinking / long chain-of-thought setups; generated multiple samples (8 or 64) and reported Pass@1 and consensus metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@1 accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>31.2% accuracy on OlymMATH-EN-HARD (Pass@1)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic probing; qualitative case studies of model outputs show reasoning traces and omissions (e.g., omitted proofs or unstated assumptions).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Empirical guessing and heuristic shortcuts (example: assumed symmetry b = c in a geometry optimization without proof); omitted necessary proof steps (e.g., forgot to prove coprimality in a modular sequence example); sometimes fabricates or asserts unproven identities after observing early terms.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Per paper, o3-mini (high) underperforms larger/newer models (e.g., Gemini) on OlymMATH-HARD; slow-thinking modes help but do not close the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6391.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6391.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1 (reasoning‑incentivized model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-focused model trained with reinforcement learning incentives for deliberative thinking (reported and evaluated in this paper); shows limited accuracy on the hardest Olympiad problems despite targeted training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Paper notes RL-based training to 'incentivize reasoning' (Reinforcement Learning from Reward to improve long chain-of-thought behavior); exact pretraining corpora not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>OlymMATH (EN-HARD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step math problems (number theory, algebra, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language MATH-format problems with numeric answers</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level (HARD subset)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Evaluated in slow-thinking / long chain-of-thought regime; sampled multiple generations (8 or 64); Pass@1 and consensus metrics computed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@1 accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>19.5% accuracy on OlymMATH-EN-HARD (Pass@1) as reported in abstract/summary</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No direct mechanistic interpretability (no activations/attention probe); behavioral analysis via large corpus of CoT traces (582,400 samples) showing patterns of reasoning and mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still prone to non-rigorous heuristics and guessing on some problems; errors persist on problems designed to defeat simple heuristic symmetry/shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Reinforcement learning and targeted long chain-of-thought fine-tuning improve performance relative to baseline models of similar size (paper references RL-based gains), but DeepSeek-R1 remains far below top models on hardest problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6391.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6391.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen3 (235B-A22B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen3 (235B-A22B, 'Think' variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large Qwen family model evaluated in a 'Think' (reasoning) configuration; achieves substantially higher accuracy than many other locally evaluated models, indicating scaling + reasoning-focused variants help on hard math.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen3 (235B-A22B, Think)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>235B (approx.)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not detailed in this paper; Qwen family models are described in referenced technical reports as trained on large multilingual corpora with math-focused fine-tuning for some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>OlymMATH (EN-HARD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Olympiad-level multi-step math problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language MATH-format (numeric answers)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Olympiad-level (HARD subset)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Evaluated with 'Think' variant (slow-thinking), long chain-of-thought generations, multi-sample aggregation (Pass@1 and consensus).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@1 accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Approximately 48.0% (Pass@1) on some OlymMATH splits according to the result table (Qwen3 variants show high performance relative to many models).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic probes reported; analysis is behavioral and comparative across problems/languages.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Although stronger than many models, still makes errors on hard OlymMATH problems; likely failures include reliance on insufficiently rigorous heuristics when formal derivations are required (paper reports general heuristics/fabrications across models).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Performance increases with model size and 'Think' reasoning variants; Qwen3 large variants rank among the better-performing models on OlymMATH.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6391.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6391.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OlymMATH Eval Setup</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OlymMATH evaluation methodology (Pass@1, Cons@k, long CoT sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The evaluation protocol used in this paper: generate multiple CoT answers per problem (mostly 64 samples), compute Pass@1 and consensus accuracy (Cons@64/Cons@8) via majority voting, and verify numeric answers using sympy/numeric checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>OlymMATH (EN & ZH; EASY & HARD splits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step Olympiad math problem solving (numeric verification)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language MATH-format problems; answers restricted to real numbers/intervals for automated verification</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>AIME-level (EASY) and harder Olympiad-level (HARD)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Long chain-of-thought / slow-thinking prompts and sampling; some models evaluated via APIs with expanded max_tokens; majority-vote consensus (Cons@k) and Pass@1 reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@1 accuracy, Cons@8/Cons@64 consensus accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported per-model results; aggregated examples: Gemini 2.5 Pro Exp 58.4% (EN-HARD), o3-mini 31.2% (EN-HARD), DeepSeek-R1 19.5% (EN-HARD)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Dataset-level behavioral analyses (language gap between EN and ZH, prevalence of heuristic guesses, correlation with AIME performance). No internal neuron-level analyses provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Benchmark designed to defeat simple heuristics; observed model failures include empirical guessing, unjustified symmetry assumptions, omission of crucial proof steps, and fabrication of intermediate claims.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Consensus and larger models improve accuracy; slow-thinking / long CoT and RL fine-tuning give gains but do not eliminate failure on HARD problems; English versions yield higher performance on average.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6391.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6391.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregate LLM Arithmetic Observations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavioral findings about how LLMs perform arithmetic and multi-step math</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-model behavioral findings: models often produce long chain-of-thought traces but sometimes rely on heuristics/guessing rather than rigorous derivations; English prompts tend to yield higher accuracy than Chinese; scaling and reasoning-focused fine-tuning improve but do not solve hard Olympiad problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>OlymMATH / AIME comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Arithmetic, modular arithmetic, algebraic manipulations, multi-step proofs</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language mathematical problems; some originally symbolic (reformulated to text)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>From AIME-level to harder Olympiad-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero/few-shot not explicitly contrasted; primary emphasis on chain-of-thought / slow-thinking, sample aggregation (Pass@1, Cons@k), and expanded max_tokens for API-evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@1 accuracy; dataset-level correlations with AIME performance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Varies by model; many state-of-the-art models achieve <60% on OlymMATH-EN-HARD, with top values ~58.4% (Gemini) and many models far lower.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No low-level mechanistic interpretability provided; analysis is based on a corpus of 582,400 long-CoT samples allowing qualitative case studies (e.g., missing proofs, guessed expressions). The paper highlights the need for process-level supervision and mechanistic probes but does not perform them.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Empirical guessing (heuristics, symmetry assumptions), omission of required proof steps (e.g., failure to prove gcd = 1), fabrication of unproven formulas after seeing initial terms, language-dependent performance gaps (EN > ZH).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Clear trend: larger/more-recent models and those with reasoning-specialized training (long-CoT, RL, 'Think' variants) perform better, but performance gains plateau on truly hard Olympiad problems; existing benchmarks (GSM8K, MATH, AIME) are partially saturated, motivating OlymMATH.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. <em>(Rating: 2)</em></li>
                <li>Openai o1 system card. <em>(Rating: 1)</em></li>
                <li>Gemini 2.5: Our most intelligent ai model. <em>(Rating: 1)</em></li>
                <li>Measuring mathematical problem solving with the MATH dataset. <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems. <em>(Rating: 1)</em></li>
                <li>Acemath: Advancing frontier math reasoning with post-training and reward modeling. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6391",
    "paper_id": "paper-277349284",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "Gemini 2.5 Pro Exp",
            "name_full": "Gemini 2.5 Pro Experimental (0325)",
            "brief_description": "A closed-source state-of-the-art reasoning LLM evaluated on OlymMATH; shows the strongest performance among tested models on the hard Olympiad subset but still far from human-level on the hardest problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini 2.5 Pro Exp 0325",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "Not specified in this paper; generally pretrained on large multilingual web and curated corpora (paper notes English-dominant pretraining benefits).",
            "benchmark_name": "OlymMATH (EN-HARD)",
            "task_type": "Olympiad-level multi-step math problems (number theory, algebra, geometry, combinatorics)",
            "problem_format": "Natural-language text problems in MATH-format with numeric (real) answers",
            "difficulty_level": "Olympiad-level (HARD subset)",
            "prompting_method": "Evaluated in 'slow-thinking' / long chain-of-thought modes; sampling with multiple seeds (Pass@1 reported; Cons@k consensus via majority voting also used).",
            "performance_metric": "Pass@1 accuracy (also Cons@8/Cons@64 consensus reported elsewhere in paper)",
            "performance_value": "58.4% accuracy on OlymMATH-EN-HARD (Pass@1)",
            "internal_analysis": "No mechanistic probes reported (no attention/logit-lens/activation probing). Analysis is behavioral: models produce long chain-of-thought traces which are collected and inspected qualitatively.",
            "failure_modes": "Prone to heuristic/shortcut reasoning in some cases; examples in the paper show reliance on intuition rather than fully rigorous derivations in other benchmarks, but on OlymMATH-HARD remaining errors are substantial (missing rigorous derivations or making incorrect symmetry/assumption shortcuts).",
            "scaling_trend": "Among evaluated models, Gemini 2.5 Pro Exp is one of the top performers, supporting the paper's observation that larger/more-recent models and slow-thinking modes improve performance but do not solve Olympiad-hard problems.",
            "uuid": "e6391.0",
            "source_info": {
                "paper_title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "o3-mini (high)",
            "name_full": "OpenAI o3-mini (high)",
            "brief_description": "A cost‑effective OpenAI model evaluated under long chain-of-thought / slow-thinking modes; achieves moderate accuracy on OlymMATH but shows systematic reasoning shortcuts and failures on complex tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o3-mini (high)",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "Not specified in paper; described as a modern OpenAI model trained on large web corpora; paper notes that such models often benefit from English-dominated pretraining.",
            "benchmark_name": "OlymMATH (EN-HARD)",
            "task_type": "Olympiad-level multi-step math problems (including modular arithmetic and optimization problems)",
            "problem_format": "Natural-language problems rendered in MATH-style; answers are numeric and verifiable",
            "difficulty_level": "Olympiad-level (HARD subset)",
            "prompting_method": "Assessed in slow-thinking / long chain-of-thought setups; generated multiple samples (8 or 64) and reported Pass@1 and consensus metrics",
            "performance_metric": "Pass@1 accuracy",
            "performance_value": "31.2% accuracy on OlymMATH-EN-HARD (Pass@1)",
            "internal_analysis": "No internal mechanistic probing; qualitative case studies of model outputs show reasoning traces and omissions (e.g., omitted proofs or unstated assumptions).",
            "failure_modes": "Empirical guessing and heuristic shortcuts (example: assumed symmetry b = c in a geometry optimization without proof); omitted necessary proof steps (e.g., forgot to prove coprimality in a modular sequence example); sometimes fabricates or asserts unproven identities after observing early terms.",
            "scaling_trend": "Per paper, o3-mini (high) underperforms larger/newer models (e.g., Gemini) on OlymMATH-HARD; slow-thinking modes help but do not close the gap.",
            "uuid": "e6391.1",
            "source_info": {
                "paper_title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1 (reasoning‑incentivized model)",
            "brief_description": "A reasoning-focused model trained with reinforcement learning incentives for deliberative thinking (reported and evaluated in this paper); shows limited accuracy on the hardest Olympiad problems despite targeted training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1",
            "model_family": "transformer (decoder-only)",
            "model_size": null,
            "training_data_description": "Paper notes RL-based training to 'incentivize reasoning' (Reinforcement Learning from Reward to improve long chain-of-thought behavior); exact pretraining corpora not specified here.",
            "benchmark_name": "OlymMATH (EN-HARD)",
            "task_type": "Olympiad-level multi-step math problems (number theory, algebra, etc.)",
            "problem_format": "Natural-language MATH-format problems with numeric answers",
            "difficulty_level": "Olympiad-level (HARD subset)",
            "prompting_method": "Evaluated in slow-thinking / long chain-of-thought regime; sampled multiple generations (8 or 64); Pass@1 and consensus metrics computed.",
            "performance_metric": "Pass@1 accuracy",
            "performance_value": "19.5% accuracy on OlymMATH-EN-HARD (Pass@1) as reported in abstract/summary",
            "internal_analysis": "No direct mechanistic interpretability (no activations/attention probe); behavioral analysis via large corpus of CoT traces (582,400 samples) showing patterns of reasoning and mistakes.",
            "failure_modes": "Still prone to non-rigorous heuristics and guessing on some problems; errors persist on problems designed to defeat simple heuristic symmetry/shortcuts.",
            "scaling_trend": "Reinforcement learning and targeted long chain-of-thought fine-tuning improve performance relative to baseline models of similar size (paper references RL-based gains), but DeepSeek-R1 remains far below top models on hardest problems.",
            "uuid": "e6391.2",
            "source_info": {
                "paper_title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Qwen3 (235B-A22B)",
            "name_full": "Qwen3 (235B-A22B, 'Think' variant)",
            "brief_description": "A large Qwen family model evaluated in a 'Think' (reasoning) configuration; achieves substantially higher accuracy than many other locally evaluated models, indicating scaling + reasoning-focused variants help on hard math.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen3 (235B-A22B, Think)",
            "model_family": "transformer (decoder-only)",
            "model_size": "235B (approx.)",
            "training_data_description": "Not detailed in this paper; Qwen family models are described in referenced technical reports as trained on large multilingual corpora with math-focused fine-tuning for some variants.",
            "benchmark_name": "OlymMATH (EN-HARD)",
            "task_type": "Olympiad-level multi-step math problems",
            "problem_format": "Natural-language MATH-format (numeric answers)",
            "difficulty_level": "Olympiad-level (HARD subset)",
            "prompting_method": "Evaluated with 'Think' variant (slow-thinking), long chain-of-thought generations, multi-sample aggregation (Pass@1 and consensus).",
            "performance_metric": "Pass@1 accuracy",
            "performance_value": "Approximately 48.0% (Pass@1) on some OlymMATH splits according to the result table (Qwen3 variants show high performance relative to many models).",
            "internal_analysis": "No internal mechanistic probes reported; analysis is behavioral and comparative across problems/languages.",
            "failure_modes": "Although stronger than many models, still makes errors on hard OlymMATH problems; likely failures include reliance on insufficiently rigorous heuristics when formal derivations are required (paper reports general heuristics/fabrications across models).",
            "scaling_trend": "Performance increases with model size and 'Think' reasoning variants; Qwen3 large variants rank among the better-performing models on OlymMATH.",
            "uuid": "e6391.3",
            "source_info": {
                "paper_title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "OlymMATH Eval Setup",
            "name_full": "OlymMATH evaluation methodology (Pass@1, Cons@k, long CoT sampling)",
            "brief_description": "The evaluation protocol used in this paper: generate multiple CoT answers per problem (mostly 64 samples), compute Pass@1 and consensus accuracy (Cons@64/Cons@8) via majority voting, and verify numeric answers using sympy/numeric checks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "OlymMATH (EN & ZH; EASY & HARD splits)",
            "task_type": "Multi-step Olympiad math problem solving (numeric verification)",
            "problem_format": "Natural-language MATH-format problems; answers restricted to real numbers/intervals for automated verification",
            "difficulty_level": "AIME-level (EASY) and harder Olympiad-level (HARD)",
            "prompting_method": "Long chain-of-thought / slow-thinking prompts and sampling; some models evaluated via APIs with expanded max_tokens; majority-vote consensus (Cons@k) and Pass@1 reported",
            "performance_metric": "Pass@1 accuracy, Cons@8/Cons@64 consensus accuracy",
            "performance_value": "Reported per-model results; aggregated examples: Gemini 2.5 Pro Exp 58.4% (EN-HARD), o3-mini 31.2% (EN-HARD), DeepSeek-R1 19.5% (EN-HARD)",
            "internal_analysis": "Dataset-level behavioral analyses (language gap between EN and ZH, prevalence of heuristic guesses, correlation with AIME performance). No internal neuron-level analyses provided.",
            "failure_modes": "Benchmark designed to defeat simple heuristics; observed model failures include empirical guessing, unjustified symmetry assumptions, omission of crucial proof steps, and fabrication of intermediate claims.",
            "scaling_trend": "Consensus and larger models improve accuracy; slow-thinking / long CoT and RL fine-tuning give gains but do not eliminate failure on HARD problems; English versions yield higher performance on average.",
            "uuid": "e6391.4",
            "source_info": {
                "paper_title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Aggregate LLM Arithmetic Observations",
            "name_full": "Behavioral findings about how LLMs perform arithmetic and multi-step math",
            "brief_description": "Cross-model behavioral findings: models often produce long chain-of-thought traces but sometimes rely on heuristics/guessing rather than rigorous derivations; English prompts tend to yield higher accuracy than Chinese; scaling and reasoning-focused fine-tuning improve but do not solve hard Olympiad problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "OlymMATH / AIME comparisons",
            "task_type": "Arithmetic, modular arithmetic, algebraic manipulations, multi-step proofs",
            "problem_format": "Natural-language mathematical problems; some originally symbolic (reformulated to text)",
            "difficulty_level": "From AIME-level to harder Olympiad-level",
            "prompting_method": "Zero/few-shot not explicitly contrasted; primary emphasis on chain-of-thought / slow-thinking, sample aggregation (Pass@1, Cons@k), and expanded max_tokens for API-evaluated models.",
            "performance_metric": "Pass@1 accuracy; dataset-level correlations with AIME performance",
            "performance_value": "Varies by model; many state-of-the-art models achieve &lt;60% on OlymMATH-EN-HARD, with top values ~58.4% (Gemini) and many models far lower.",
            "internal_analysis": "No low-level mechanistic interpretability provided; analysis is based on a corpus of 582,400 long-CoT samples allowing qualitative case studies (e.g., missing proofs, guessed expressions). The paper highlights the need for process-level supervision and mechanistic probes but does not perform them.",
            "failure_modes": "Empirical guessing (heuristics, symmetry assumptions), omission of required proof steps (e.g., failure to prove gcd = 1), fabrication of unproven formulas after seeing initial terms, language-dependent performance gaps (EN &gt; ZH).",
            "scaling_trend": "Clear trend: larger/more-recent models and those with reasoning-specialized training (long-CoT, RL, 'Think' variants) perform better, but performance gains plateau on truly hard Olympiad problems; existing benchmarks (GSM8K, MATH, AIME) are partially saturated, motivating OlymMATH.",
            "uuid": "e6391.5",
            "source_info": {
                "paper_title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Qwen2.5-math technical report: Toward mathematical expert model via self-improvement.",
            "rating": 2,
            "sanitized_title": "qwen25math_technical_report_toward_mathematical_expert_model_via_selfimprovement"
        },
        {
            "paper_title": "Openai o1 system card.",
            "rating": 1,
            "sanitized_title": "openai_o1_system_card"
        },
        {
            "paper_title": "Gemini 2.5: Our most intelligent ai model.",
            "rating": 1,
            "sanitized_title": "gemini_25_our_most_intelligent_ai_model"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the MATH dataset.",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Acemath: Advancing frontier math reasoning with post-training and reward modeling.",
            "rating": 2,
            "sanitized_title": "acemath_advancing_frontier_math_reasoning_with_posttraining_and_reward_modeling"
        }
    ],
    "cost": 0.01541425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models
19 May 2025</p>
<p>Haoxiang Sun 
School of Information
Renmin University of China</p>
<p>Yingqian Min 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Zhipeng Chen 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Wayne Xin Zhao 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Lei Fang 
Zheng Liu 
Zhongyuan Wang 
Ji-Rong Wen 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models
19 May 20252813870E92125EFC3F71B40F63925E46arXiv:2503.21380v2[cs.CL]
In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks.To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs.OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions.The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models.In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation.Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1, OpenAI's o3-mini and Gemini 2.5 Pro Exp demonstrating notably limited accuracy on the hard subset.Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks.We release the benchmark, evaluation code, detailed results and a data visualization tool at https://github.com/RUCAIBox/OlymMATH.Effective assessment of LLM reasoning necessitates reliable and verifiable evaluation benchmarks.Reliability ensures accurately designed problems and solutions, free from ambiguities or errors.Verifiability demands that the evaluation process be easily constructed, replicated, and validated, Preprint.</p>
<p>Introduction</p>
<p>The advent of large language models (LLMs) [1] has marked a significant leap forward in the capabilities of artificial intelligence, showcasing exceptional performance across a broad spectrum of tasks, and in some cases, even rivaling or exceeding human-level proficiency [2,3].Among the myriad of capabilities demonstrated by LLMs, mathematical reasoning has surfaced as a particularly pivotal and demanding area of research [4,5,6].In recent years, the evaluation and enhancement of mathematical reasoning abilities have become a central focus in the development of LLMs [7].often relying on easily parsable answer formats.Many benchmarks adopt a single-answer format, like "The answer is 3 ", to simplify parsing and enhance reproducibility.</p>
<p>Evaluation benchmarks are primarily established to identify LLM limitations, and guiding future improvements.Over recent years, numerous high-quality mathematical benchmarks, such as GSM8K [8] and MATH [9], have been pivotal in advancing LLM reasoning capabilities [10,11].However, a significant trend is the saturation of many benchmarks, including those currently in use, due to rapid LLM advancements.For example, GSM8K [8], once a standard for earlier models like Llama 1 and 2, is now largely mastered by state-of-the-art models.Similarly, MATH [9], initially challenging for GPT-4-level models, has also become saturated by today's leading models.This saturation is further compounded by slow-thinking models like DeepSeek-R1 [4], OpenAI's o3-mini [12], and Gemini 2.5 Pro Experimental [13].These models, which promote deliberate step-by-step reasoning, show that enhancing the reasoning process yields substantial performance gains, thereby diminishing the effectiveness of existing benchmarks in differentiating cutting-edge capabilities.</p>
<p>To better evaluate the performance of advanced reasoning models, more rigorous and challenging benchmarks are needed to assess their mathematical reasoning capabilities.The AIME dataset has emerged as a more demanding benchmark by incorporating problems from the American Invitational Mathematics Examination (AIME), which presents a higher level of difficulty.Due to their complexity and rigor, AIME problems continue to challenge state-of-the-art models under standard prompting.Nevertheless, the AIME dataset has three major limitations.First, the limited scale of the current dataset (containing merely 30 problems from AIME 2024) may compromise the statistical reliability and robustness of the evaluation results.Second, as reasoning models rapidly improve-through methods like fine-tuning with long chain-of-thought data [14] or reinforcement learning scaling [4]-the benchmark's original performance ceiling is being surpassed.For example, models such as Gemini 2.5 Pro Exp now achieve 92% accuracy with single attempt, demonstrating that current top-performing models are approaching the limits of what AIME can effectively measure.Third, the dataset exclusively features English problems, leaving multilingual reasoning capabilities unassessed despite their importance for a comprehensive evaluation.</p>
<p>To overcome these limitations, we present OlymMATH: a rigorously curated, bilingual (English and Chinese) benchmark for Olympiad-level reasoning, comprising 200 problems split into easy (OlymMATH-EASY) and hard (OlymMATH-HARD) levels with parallel bilingual sets (EN &amp; ZH).To prevent data leakage, problems were manually sourced from printed publications and expertverified.OlymMATH requires precise numerical answers for reliable verification, covers four major mathematical fields, and adheres to the MATH dataset [9] format for compatibility (see Figure 2).</p>
<p>By leveraging the OlymMATH benchmark, we conduct extensive experiments to evaluate the performance of several state-of-the-art models (see Figure 1).The results underscore our benchmark's difficulty, with advanced models like DeepSeek-R1 [4], o3-mini [12], and Gemini 2.5 Pro Exp [13] achieving only 19.5%, 31.2%, and 58.4% accuracy, respectively, on OlymMATH-EN-HARD, indicating Olympiad-level math remains a significant challenge necessitating further research.Our</p>
<p>MATH Dataset</p>
<p>Problem: Compute:
1 − 2 + 3 − 4 + 5 − • • • + 99 − 100.
Answer: −50.</p>
<p>Problem: Let n be a positive integer.Simplify the expression
(2 4 + 1 4 )(4 4 + 1 4 ) • • • [(2n) 4 + 1 4 ] (1 4 + 1 4 )(3 4 + 1 4 ) • • • [(2n − 1) 4 + 1 4 ]
.</p>
<p>Answer: 8n 2 + 4n + 1.</p>
<p>OlymMATH-HARD (Ours)</p>
<p>Problem-EN: Find the remainder of 1234 k=0 2016×1234 2016k modulo 2017 2 (provide the value in the range [0, 2017 2 )).Answer: 1581330.Subject: Number Theory.multilingual comparison showed a consistent performance gap, with higher accuracy on English problems versus Chinese, highlighting the need for multilingual evaluation.Furthermore, case studies revealed models sometimes use heuristic "guessing" to reach answers without rigorous proofs.This underscores the importance of process-level inspection for accurate LLM capability assessment.</p>
<p>OlymMATH-EASY (Ours)
Problem-ZH: 设 O 为 △ABC 的内心, AB = 3, AC = 4, BC = 5, − − → OP = x −→ OA + y − − → OB + z − − → OC, 0 ⩽ x,
In summary, our contribution are as follows:</p>
<p>• We introduce OlymMATH, a manually curated, Olympiad-level mathematical benchmark.It features parallel English and Chinese versions for objective, bilingual evaluation of LLM mathematical reasoning, with answers efficiently verifiable using sympy-based tools.</p>
<p>• Experiments demonstrate OlymMATH's reliability (aligned with AIME) and strong discriminative power; even state-of-the-art models achieve only moderate scores, highlighting OlymMATH's potential to drive LLM reasoning advancements.</p>
<p>• Detailed analyses and case studies reveal key model limitations in complex problem-solving, including performance disparities between English and Chinese problems and instances of heuristic "guessing" rather than rigorous deduction.</p>
<p>• We open-source evaluation results and resources, including sampled long chain-of-thought reasoning trajectories (582,400 entries from 28 models on 400 problems), a data visualization tool, and standard solutions for problems where all LLMs struggled, to facilitate community research and analysis on diverse reasoning patterns and common reasoning issues.</p>
<p>Benchmark Construction</p>
<p>In this section, we describe the OlymMATH dataset in detail, including its construction methodology, problem composition, categorical distribution, and evaluation approach.Our dataset is specifically designed to provide a rigorous yet objectively verifiable benchmark for assessing the mathematical reasoning capabilities of LLMs.Additionally, we offer two parallel evaluation sets containing 200 problems each in English and Chinese as supplementary data to facilitate a comparative analysis of performance gaps between the two languages.Table 1 presents a basic comparison of our proposed OlymMATH benchmark and other mathematical reasoning benchmarks.</p>
<p>Reliability: Contamination and Verification</p>
<p>Contamination OlymMATH comprises 200 high-quality mathematical problems at the Olympiad level, meticulously curated from printed resources to ensure both quality and originality.These problems were manually gathered from a range of authoritative sources, including specialized magazines, textbooks, and official competition materials.To minimize the risk of data contamination, online repositories and forums were intentionally excluded from the sourcing process.This methodology ensures that the problems are intellectually challenging and representative of advanced mathematical reasoning, while also minimizing prior exposure on publicly accessible digital platforms.Consequently, OlymMATH serves as a reliable benchmark for evaluating the real capabilities of LLMs in solving complex mathematical tasks.Verification To enhance dataset reliability, we invited a China Mathematical Olympiad silver medalist and two provincial first-prize winners to verify and revise the problems and solutions.Since the answers to the problems were already provided, the verification difficulty was reduced, making the expertise of reviewers sufficient for this task.Each problem was reviewed by at least two reviewers.Additionally, official solutions for challenging problems are published for community oversight.</p>
<p>Problem Categories and Distribution</p>
<p>OlymMATH problems span four key high-school Olympiad mathematical fields-algebra, geometry, number theory, and combinatorics-classified by experts (not LLMs) for reliability.Problems are selected for their challenge, suitability for simple-answer verification, and topic diversity (e.g., inequalities, sequences, and more in algebra).Figure-based problems within this set are textreformulated for LLM compatibility, with non-convertible ones excluded (e.g., Figure 6 in Appendix).</p>
<p>For refined evaluation, problems are categorized by difficulty: easy, designed to challenge standard prompting in mainstream models, and hard, tailored to test advanced reasoning (e.g., slow-thinking modes) in state-of-the-art models.The distribution details are described in Table 2.</p>
<p>Format and Verification Methodology</p>
<p>OlymMATH adopts the MATH dataset format (see Figure 2) for seamless integration with existing pipelines and enhancing clarity and processing efficiency.All problems are text-based, including geometry reformulated from diagrams to align with LLM evaluation, as mentioned previously.For consistent, objective assessment, answers are restricted to real numbers and intervals (see Table 3), avoiding ambiguous formats and enabling reliable sympy-based and numerical verification.</p>
<p>To make the evaluation more challenging, OlymMATH includes problems with multiple numerical answers.These problems are modified to require a summary of all potential outcomes (e.g., sums, sums of squares; see Figure 7 in Appendix).This method effectively assesses whether models can consider all possible answers, thereby providing a robust evaluation of their reasoning capabilities.</p>
<p>Bilingual Extension</p>
<p>Originating from Chinese-language problems, the OlymMATH benchmark includes both original Chinese and translated English versions for comprehensive bilingual evaluation.Our LLM-based translation pipeline first uses Claude Sonnet 3.7 for initial English translations, which are then iteratively refined with GPT-4o.Finally, a crucial human verification stage by two expert annotators ensures mathematical accuracy, rigor, and linguistic fluency.These resulting parallel sets, OlymMATH-EN (English) and OlymMATH-ZH (Chinese) (see Figure 2), facilitate systematic comparison of cross-lingual reasoning, with their union denoted as OlymMATH (full set).</p>
<p>Experiments</p>
<p>In this section, we assess the performance of leading reasoning models using the OlymMATH benchmark and then provide a detailed analysis of their capabilities.</p>
<p>Experimental Setup</p>
<p>Models.To conduct a thorough evaluation, we assess a range of representative LLMs.For opensource models, we investigated recent work on reasoning models, and evaluated DeepSeek-R1 series [4], STILL-3-Preview [21], DeepScaleR-Preview [22], QwQ [23], Light-R1 series [24],</p>
<p>OpenThinker2 series [25], Skywork-OR1 series [26], GLM-Z1-Air [27], AceMath-RL [28], OpenMath-Nemotron series [29], and Qwen3 series [30].For closed-source models, we include o3-mini (high) [12], Gemini 2.5 Pro Experimental 0325 [13] in our evaluation.</p>
<p>Evaluation Details.Our evaluation pipeline follows a systematic approach: for each problem, we generate 64 distinct responses from each comparison model, with the exception of certain models (i.e., OpenMath-Nemotron-32B, Qwen3-235B-A22B, GLM-Z1-Air, DeepSeek-R1, o3-mini (high) and Gemini 2.5 Pro Exp), for which, due to resource limitations and the relatively large scale of our dataset, we only conducted 8 samples.For the Pass@1 metric, we compute the mean accuracy across all sampled responses to derive the final accuracy score.For the Cons@64 and Cons@8 metric, we implement majority voting to determine a consensus answer for each problem, subsequently calculating the average accuracy across the entire dataset.For generation hyperparameters, we adhere to established practices from previous research [4,23], configuring locally-evaluated models with temperature, top_p, min_p, and max_token set to 0.6, 0.95, 0, and 32768, respectively.For api-evaluated models (i.e., GLM-Z1-Air, DeepSeek-R1, o3-mini (high) and Gemini 2.5 Pro Exp), we expand their max_token limit to the maximum extent possible to unleash their reasoning capabilities better.We have open-sourced all the samples (a dataset of 582,400 math reasoning samples with long chain-of-thought, generated from the 400 problems in OlymMATH across 28 models), an online data visualization tool and standard solutions for problems where all LLMs struggled in our repository, aiming to help the community analyze the problem-solving patterns and characteristics of LLMs (see Section 4 for further information).</p>
<p>Evaluation Results</p>
<p>In this part, we assess the performance of reasoning models on our benchmark.We present the evaluation results of OlymMATH-EN and OlymMATH-ZH in Table 4 and Table 5, respectively.</p>
<p>First, we observe that all tested models exhibit relatively poor performance, with even OpenAI o3-mini (high) and Gemini 2.5 Pro Exp achieving only 31.2% and 58.4% on OlymMATH-EN-HARD.This underscores the high overall difficulty of our benchmark, which demands stronger reasoning abilities Avg.P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k Qwen3 (0.6B, Think) and a deeper understanding of mathematical knowledge to solve the problems effectively.In contrast, the performance of these advanced reasoning models on OlymMATH-EN-EASY is more modest and comparable to that on AIME 2024 and AIME 2025, suggesting that OlymMATH-EN-EASY is well-suited for evaluating the capabilities of less advanced reasoning models.</p>
<p>Second, by comparing the performance of LLMs on OlymMATH-EN and OlymMATH-ZH, we find that language can influence the reasoning performance of LLMs to some extent (see Figure 3).Overall, all models tend to achieve higher performance on the English benchmarks.A potential reason for this is that English corpora still dominate existing pre-training datasets, making the English-based task-solving capabilities of LLMs generally more superior compared to other languages.This finding highlights the importance of considering performance across different languages when conducting a comprehensive evaluation of LLMs.</p>
<p>Benchmark Comparison</p>
<p>To comprehensively evaluate OlymMATH against existing benchmarks, we compare state-of-the-art model performance across widely used mathematical benchmarks (see Figure 1).Results are sourced from research reports or the MathArena platform 1 .</p>
<p>Figure 1 illustrates that OlymMATH is more challenging, yielding lower accuracy compared to saturated benchmarks like MATH-500 (where even DeepSeek-R1-Distill-Qwen-7B exceeds 92% accuracy [4]) or AIME24 (where top LLMs reach 92% with single attempt).Unlike these benchmarks whose high performance limits discriminative power, OlymMATH elicits more varied scores, offering superior differentiation of reasoning capabilities.For example, while Gemini 2.5 Pro Exp and o3mini (high) achieve similar AIME24 accuracy (92.0% vs. 87.3%),their OlymMATH-EN-HARD performance diverges significantly (58.4% vs. 31.2%).Avg.P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k P@1 C@k Qwen3 (0.6B, Think) 2.   This strong correlation suggests OlymMATH measures similar underlying mathematical reasoning abilities as the respected AIME24 dataset, validating its use for LLM evaluation (see Figure 8 in Appendix for more information).Despite this alignment, OlymMATH, particularly the HARD subset, remains significantly more challenging than AIME24 for most models, reinforcing its superior ability to differentiate state-of-the-art capabilities.</p>
<p>Case Study</p>
<p>During our data collection and preliminary experiments, we empirically observed that LLMs sometimes resort to empirical guesses-such as heuristics, symmetry assumptions, or even fabrication-rather than rigorous reasoning.For instance, o3-mini-high merely "guessed" b = c due to symmetry in a geometric optimization problem (see Figure 9 in Appendix).While such intuitive approaches might yield correct answers, they lack logical rigor and this case becomes problematic when employing rule-based or LLM-as-judge methods, as neither can effectively assess the quality of rigorous reasoning, thus potentially leading to an illusory improvement in accuracy via "shortcuts".</p>
<p>Similar issues were observed in the AIME 2025 and Omni-MATH benchmarks (see Figure 10 and 11 in Appendix), indicating that despite performance gains, LLMs still exhibit deficiencies in deliberative thinking.This underscores the importance of process-level supervision, though its scalability remains a challenge.Currently, we do not accurately measure the proportion of "guesses" in these benchmarks, leaving this as an important direction for future work.</p>
<p>Notably, these guessing strategies often fail on our OlymMATH dataset.For example, a model incorrectly assumed symmetry for a complex optimization problem in OlymMATH, yielding 3081 instead of the correct 2625 (see Figure 12 in Appendix).OlymMATH problems, particularly in the  HARD subset, are selected and designed so that their reasoning steps are difficult to "hack" through empirical guessing, thus providing a more robust evaluation of genuine reasoning capabilities.</p>
<p>Usability and Accessibility</p>
<p>To support research into LLM reasoning, we have open-sourced the OlymMATH-eval dataset at https://hf.co/datasets/RUC-AIBOX/OlymMATH-eval, with 582,400 entries from 28 models, to help compare reasoning capabilities across different models and mathematical domains.</p>
<p>Furthermore, we provide the OlymMATH-demo visualization tool (https://hf.co/spaces/RUC-AIBOX/OlymMATH-demo; see Figure 5) to facilitate in-depth analysis of LLM reasoning.This interactive interface enables: (1) Side-by-side comparison of two selected LLMs on the same L A T E Xrendered problem, with access to reference answers.(2) Color-coded "Problem Grids" for each model, displaying per-problem accuracy for quick identification of challenging areas.(3) Examination of individual model-generated reasoning samples, including correctness, extracted answers, and token counts, crucial for understanding solution processes and identifying flaws.The tool also includes standard solutions for difficult problems and supports local deployment.OlymMATH-demo is thus a valuable asset for dissecting reasoning patterns, diagnosing errors, and guiding LLM development.</p>
<p>Conclusion</p>
<p>We introduced OlymMATH, a challenging math benchmark for LLMs, uniquely curated from printed materials.It includes 200 problems across four fields, with easy (AIME-level) and hard (more challenging) subsets, in parallel English and Chinese versions.Our experiments with state-of-the-art LLMs, especially in slow-thinking modes, show significant struggles.Analysis highlights languagespecific strengths and universal limitations like empirical guessing, identifying weaknesses in LLMs' multi-step reasoning and logical consistency.Meanwhile, to facilitate community research, we have open-sourced over 580k reasoning data, a visualization tool, and solutions for challenging problems.</p>
<p>As part of our STILL project, OlymMATH affirms our belief in benchmarks' pivotal role in advancing LLMs' reasoning capabilities.We advocate for benchmarks to evolve faster than methodologies, guiding the field's progress.Our planned expansion of OlymMATH embodies this commitment, aiming to further cultivate the development of more robust reasoning models and continue pushing the boundaries of language intelligence.</p>
<p>A Appendix</p>
<p>This part presents the detailed content of the dataset and the case study examples mentioned before.</p>
<p>Problem: Given that two vertices of an equilateral triangle are on the parabola y 2 = 4x, and the third vertex is on the directrix of the parabola, and the distance from the center of the triangle to the directrix equals 1 9 of the perimeter.Find the area of the triangle.Subject: Geometry   In what follows we briefly describe one way (via a "miraculous" change-of-variables) to "solve" the recurrence; the details need not be reproduced fully here.A good hint is that the recurrence x k+1 = 1 3 x k + 1 x k − 1 has the curious symmetry T (x) =
1 3 x + 1 x − 1 = T 1
x , so the expression x + 1 x (which is "symmetric" in x and its reciprocal) naturally appears.In fact, one may rewrite the recurrence in the form 3x k+1 + 1 = x k + 1</p>
<p>x k .A useful idea is then to "complete the square" by writing
√ x k − 1 √ x k 2 = x k + 1 x k − 2, so that 3x k+1 +1 = √ x k − 1 √ x k 2 +2. Defining u k = √ x k − 1 √ x k , one obtains the "linear-looking" relation u 2 k = 3x k+1 − 1.
With some additional work it turns out that if we write x k = m k n k (in lowest terms), then one may prove by induction that m k + n k = 36 • 12 (2 k−1 −1) .(In our case one checks that m1 + n1 = 25 + 11 = 36, m2 + n2 = 432, m3 + n3 = 62208, and indeed 36, 432, 62208 equal 36 • 12 0 , 36 • 12 1 , 36 • 12 3 respectively.)Thus, for k = 2025 we have m + n = 36 • 12 (2 2024 −1) .Even though this number is astronomically large, we are only asked to compute m + n (mod 1000).Using modular reductions, we compute m + n modulo 1000.First, modulo 8, we find 36 • 12 (2 2024 −1) ≡ 0 (mod 8).Modulo 125, Euler's theorem helps reduce 12 N to 12 15 ≡ 118 (mod 125), giving 36 • 12 N ≡ 123 (mod 125).Using the Chinese Remainder Theorem, solving 125k + 123 ≡ 0 (mod 8) gives k ≡ 1 (mod 8), leading to the final answer: 248 .Figure 10: An example from AIME 2025.o3-mini (high) forgot to prove that (m k , n k ) = 1 and it claimed that m k + n k = 36 • 12 (2 k−1 −1) without providing the complete proof (highlighted in red).Instead, the model "guessed" this expression by calculating the first several terms in the sequence one by one.Due to length constraints, we provide only a summary of its unimportant steps.</p>
<p>Source: 2016 IMO Shortlist A7 Problem: Find all functions f : R → R such that f (0) ̸ = 0 and for all x, y ∈ R,
f (x + y) 2 = 2f (x)f (y) + max f (x 2 + y 2 ), f (x 2 ) + f (y 2 ) . Answer: f (x) = −1 and f (x) = x − 1.
Solution: (Provided in Omni-MATH) To find all functions f : R → R satisfying the given functional equation, we start with the condition: f (0) ̸ = 0, and the functional equation for all x, y ∈ R:
f (x + y) 2 = 2f (x)f (y) + max f (x 2 + y 2 ), f (x 2 ) + f (y 2 ) .
Step 1: Test Simple Functions Let's start by testing the functions given in the reference answer: f
(x) = −1 and f (x) = x − 1. Case 1: f (x) = −1. Substitute f (x) = −1 into the equation: • • • Case 2: f (x) = x − 1. Substituting f (x) = x − 1: • • • confirming f (x) = x − 1 is another valid solution.</p>
<p>Conclusion</p>
<p>After verifying the two functions, we conclude that the functions satisfying the original functional equation are:</p>
<p>f (x) = −1 and f (x) = x − 1.Thus, the solutions are:
f (x) = −1 and f (x) = x − 1 .
Model Output: We will show that the only functions f : R → R with f (0) ̸ = 0, satisfying
f (x + y) 2 = 2f (x)f (y) + max{ f (x 2 + y 2 ), f (x 2 ) + f (y 2 ) } for all x, y ∈ R, are f (x) = −1 for all x or f (x) = x − 1.
Below is an outline of one way to arrive at this result.</p>
<p>Step 1. Plug in y = 0 Let y = 0 in the given equation:
f (x) 2 = 2f (x)f (0) + max{f (x 2 ), f (x 2 ) + f (0)}.
Write c = f (0) ̸ = 0. Notice that the two arguments inside the maximum differ by f (0); therefore, the larger one depends on the sign of c:
-<strong>If c &gt; 0:</strong> Then f (x 2 ) + c &gt; f (x 2 ) so the equation becomes f (x) 2 = 2c f (x) + f (x 2 ) + c. -<strong>If c &lt; 0:</strong> Then f (x 2 ) + c &lt; f (x 2 ) so we have f (x) 2 = 2c f (x) + f (x 2 ).
Thus, in each case one obtains a relationship between f (x) and f (x 2 ).</p>
<p>Step 2. Check constant solutions • • • Thus, the constant solution is f (x) = −1 for all x.</p>
<p>Step 3. Look for non-constant solutions A natural next guess is to try a linear function.A natural idea is to look for a "symmetric-like" solution.(Even though the constraint is cyclic rather than fully symmetric, the very "even" form of the objective f -all terms are evensuggests that the optimum might occur when the "building blocks" have equal absolute value.)One obvious candidate is to try a = b = c = t.Then the constraint becomes Also, one might try "extremal" cases such as letting one variable vanish (say, c = 0).In that case, the constraint reduces to a 3 b = 3 so that a and b cannot be arbitrarily small (or close in absolute value to 1), and a short calculation shows that the value of f then exceeds 3081.</p>
<p>Figure 1 :
1
Figure1: Performance comparisons of mainstream reasoning models between our OlymMATH (English version) and other Olympiad-level mathematical benchmarks.Our OlymMATH dataset provides test results that align with those on existing benchmarks and features a significantly larger number of problems.</p>
<p>Figure 2 :
2
Figure 2: Examples from the MATH dataset and our OlymMATH dataset.</p>
<p>2.5 0.0 2.1 4.0 6.6 8.0 0.2 0.0 2.8 3.0 15.5 20.0 5.6 15.2 24.5 38.5 5.2 6.9 10.4 17.0 DS-R1-Distill (1.5B) 1.9 0.0 1.8 0.0 1.8 0.0 0.4 0.0 1.5 0.0 20.8 40.0 12.6 21.2 32.6 61.5 8.2 24.1 16.0 32.0 STILL-3-Pre.(1.5B) 3.7 0.0 4.9 4.0 5.8 8.0 0.8 0.0 3.8 3.0 22.7 36.0 14.8 30.3 37.6 69.2 10.3 17.2 18.4 33.0 DeepScaleR-Pre.(1.5B) 3.4 4.0 4.2 8.0 8.2 4.0 0.4 0.0 4.1 4.0 19.9 16.0 18.5 21.2 44.6 46.2 18.9 31.0 22.3 26.0 OpenMath-Nemo.(1.5B) 14.5 24.0 13.6 16.0 10.9 16.0 2.6 4.0 10.4 15.0 70.9 100.0 59.3 90.9 81.6 100.0 40.6 58.6 59.7 85.0 Qwen3 (4B, Think) 18.1 20.0 14.8 12.0 19.8 28.0 3.1 4.0 13.9 16.0 76.4 92.0 79.1 97.0 85.1 84.6 57.1 72.4 72.8 87.0 DS-R1-Distill (7B) 15.6 36.0 12.6 24.0 13.1 24.0 3.1 4.0 11.1 22.0 52.8 84.0 49.6 84.8 62.5 84.6 33.9 58.6 47.5 77.0 Light-R1-DS (7B) 17.1 28.0 15.2 16.0 12.8 24.0 3.6 4.0 12.2 18.0 57.1 84.0 53.6 93.9 73.7 84.6 39.5 51.7 53.0 78.0 OpenThinker2 (7B) 16.0 20.0 16.8 28.0 14.0 20.0 2.8 4.0 12.4 18.0 65.3 96.0 60.5 97.0 79.1 84.6 42.3 58.6 58.9 84.0 Skywork-OR1-Pre.(7B) 14.4 20.0 12.5 12.0 11.7 24.0 1.6 0.0 10.0 14.0 61.6 88.0 55.9 78.8 74.3 92.3 36.9 48.3 54.2 74.0 Skywork-OR1-Math (7B) 17.4 20.0 17.1 20.0 13.6 28.0 0.9 0.0 12.2 17.0 67.9 92.0 67.4 93.9 76.6 92.3 47.6 62.1 63.0 84.0 AceMath-RL (7B) 19.4 32.0 19.3 32.0 14.4 24.0 3.5 4.0 14.2 23.0 69.7 96.0 63.7 93.9 79.0 84.6 44.2 69.0 61.5 86.0 OpenMath-Nemo.(7B) 26.9 36.0 18.6 28.0 19.8 28.0 4.4 4.0 17.4 24.0 86.4 100.0 76.4 97.0 91.5 100.0 55.3 72.4 74.7 91.0 DS-R1-Distill (14B) 16.1 16.0 17.0 16.0 18.1 32.0 2.1 4.0 13.3 17.0 69.0 96.0 65.1 97.0 79.4 92.3 44.0 65.5 61.8 87.0 Light-R1-DS (14B) 21.8 24.0 22.2 28.0 17.8 36.0 2.6 4.0 16.1 23.0 72.3 88.0 73.0 100.0 84.3 92.3 47.6 65.5 66.9 86.0 OpenMath-Nemo.(14B) 28.7 40.0 22.1 32.0 21.0 32.0 3.4 4.0 18.8 27.0 87.9 100.0 78.5 93.9 95.8 100.0 59.9 86.2 77.7 94.0 Qwen3 (30B-A3B, Think) 38.8 44.0 33.8 44.0 26.7 36.0 5.9 4.0 26.3 32.0 91.4 100.0 92.9 100.0 90.9 92.3 75.6 93.1 87.2 97.0 DS-R1-Distill (32B) 22.4 32.0 21.4 24.0 20.3 40.0 3.4 4.0 16.9 25.0 73.6 100.0 71.8 97.0 84.5 92.3 49.0 69.0 67.3 89.0 QwQ (32B) 32.9 28.0 26.6 36.0 26.7 44.0 6.2 4.0 23.1 28.0 91.8 100.0 87.0 100.0 95.0 100.0 69.0 89.7 84.0 97.0 Light-R1-DS (32B) 28.9 44.0 31.1 52.0 24.1 36.0 5.2 8.0 22.3 35.0 84.2 100.0 83.3 100.0 92.5 100.0 62.1 82.8 78.6 95.0 OpenThinker2 (32B) 24.1 32.0 22.9 32.0 18.0 20.0 2.6 4.0 16.9 22.0 79.4 96.0 74.0 100.0 90.4 92.3 56.5 79.3 72.4 92.0 Skywork-OR1-Pre.(32B) 37.2 52.0 32.3 48.0 27.0 40.0 4.2 4.0 25.2 36.0 89.3 100.0 87.3 100.0 92.4 100.0 63.9 82.8 81.7 95.0 GLM-Z1-Air (32B) 35.0 44.0 21.5 32.0 19.5 24.0 4.5 4.0 20.1 26.0 86.5 100.0 79.5 90.9 90.4 100.0 59.1 75.9 76.8 90.0 OpenMath-Nemo.(32B) 22.0 36.0 21.0 28.0 20.0 24.0 3.5 4.0 16.6 23.0 75.5 100.0 60.6 90.9 89.4 100.0 42.2 69.0 62.7 88.0 Qwen3 (235B-A22B, Think) 48.0 52.0 49.5 60.0 38.0 36.0 10.5 16.0 36.5 41.0 93.5 100.0 92.4 100.0 99.0 100.0 81.9 93.1 90.5 98.0 DeepSeek R1 30.0 40.0 25.5 32.0 18.5 24.0 4.0 4.0 19.5 25.0 90.5 100.0 82.2 97.0 94.2 100.0 60.8 72.4 79.6 91.0 OpenAI o3-mini (high) 29.5 32.0 29.0 44.0 49.5 60.0 17.0 20.0 31.2 39.0 93.0 92.0 89.8 100.0 97.1 100.0 89.2 96.6 91.4 97.0 Gemini 2.5 Pro Exp 0325 71.5 76.0 75.5 84.0 59.0 72.0 27.5 36.0 58.4 67.0 92.0 100.0 97.0 100.0 98.1 100.0 84.5 89.7 92.2 97.0</p>
<p>13 . 6 Figure 4
1364
Figure 4  further demonstrates OlymMATH's reliability by comparing OlymMATH-EN performance against AIME24.The close clustering of data points around linear trend lines indicates consistent relative model performance across both benchmarks.This strong correlation suggests OlymMATH measures similar underlying mathematical reasoning abilities as the respected AIME24 dataset, validating its use for LLM evaluation (see Figure8in Appendix for more information).Despite this alignment, OlymMATH, particularly the HARD subset, remains significantly more challenging than AIME24 for most models, reinforcing its superior ability to differentiate state-of-the-art capabilities.</p>
<p>Figure 3 :
3
Figure 3: Pass@1 accuracy on OlymMATH EN (y) vs. ZH (x), the dashed line shows parity.Points above favor English, below favor Chinese.Solid circles (local dense models, colored by size) indicate larger models trend towards higher accuracy.Hollow diamonds are MoE or API evaluated models.</p>
<p>Figure 4 :
4
Figure 4: Correlation of Pass@1 performance: OlymMATH-EN vs. AIME24.Dashed lines indicate linear trends per dataset.Solid shapes are local dense models (size = model size, color = release date).Hollow shapes denote MoE or API evaluated models.Stars mark the best overall model.</p>
<p>Figure 5 :
5
Figure 5: The OlymMATH-demo interface.It is currently being maintained on HuggingFace Spaces.</p>
<p>Figure 6 :
6
Figure 6: A geometry problem described precisely in text from OlymMATH.</p>
<p>Figure 7 :Figure 8 : 2 + b 2 + c 2 = 4 .a = 2 √ 5 , b = c = 6 5 ) 2 + b 2 + c 2 = 4 .
7822242552224
Figure 7: An OlymMATH-HARD example testing model's identification of all possible answers.</p>
<p>Figure 9 :
9
Figure 9: An example during our data collection.o3-mini (high) found the correct answer without rigorously solving the problem.Due to length constraints, we provide only a summary of its original solution steps.</p>
<p>1 .
1
• • • A quick check verifies that f (x) = x − 1 satisfies the original equation.• • • which equals (x + y − 1)2 , matching the left-hand side.ConclusionThe only functions f : R → R with f (0) ̸ = 0 that satisfy • • • for all real numbers x and y are:f (x) = −1 for all x or f (x) = x −These are the complete solutions.</p>
<p>Figure 11 :
11
Figure 11: An example from Omni-MATH.The solution provided by Omni-MATH itself is flawed and problematic.Both the reference solution and o3-mini (high) failed to prove that there are no other possible function f that satisfies the condition (highlighted in red).Due to length constraints, we have omitted its unimportant steps.</p>
<p>Problem: 4 √ 7 sin 2 4π 7 , b = ± 4 √ 7 sin 2 2π 7 , c = ± 4 √ 7 sin 2 π 7
47747477
Let a, b, c ∈ R, a 3 b + b 3 c + c 3 a = 3, find the minimum value of the expression f (a, b, c) = ( a 4 ) 4 + 1000 a 2 b 2 .Reference Answer: 2625.(Equality condition: a = ± and its cyclic permutations) Model Output: We wish to minimize f (a, b, c) = a 4 + b 4 + c 4 4 + 1000 a 2 b 2 + b 2 c 2 + c 2 a 2 subject to the (cyclic) constraint a 3 b + b 3 c + c 3 a = 3, with a, b, c ∈ R.</p>
<p>a 3 b 3 ? 3 .
333
+ b 3 c + c 3 a = 3t 4 = 3 =⇒ t 4 = 1.Thus t = 1 or t = −1.(In R the equation t 4 = 1 has the two solutions t = 1 and t = −1.)Now compute the value of f at a = b = c = t.Notice that a 4 + b 4 + c 4 = t 4 + t 4 + t 4 = 3, a 2 b 2 + b 2 c 2 + c 2 a 2 = t 4 + t 4 + t 4 = 3 (since t 4 = 1).Then f = (3) 4 + 1000 • 3 = 81 + 3000 = 3081.Before concluding that 3081 is the minimum, one may ask: Could there be a choice of a, b, c satisfying a 3 b + b 3 c + c 3 a = 3 for which a 4 + b 4 + c 4 &lt; 3 or a 2 b 2 + b 2 c 2 + c 2 a 2 &lt; Since a 4 , b 4 , c 4 are nonnegative and the constraint (a homogeneous form of degree 4) "forces" the variables to be large enough in a certain sense, one may show by a careful (often inequalitybased) argument that if a 3 b + b 3 c + c 3 a = 3, then one must have a 4 + b 4 + c 4 ≥ 3 and a 2 b 2 + b 2 c 2 + c 2 a 2 ≥ In fact, equality in these "loss-inequalities" forces |a| = |b| = |c| = 1, and it turns out (by checking the signs in the constraint) that the only possibility for equality is a = b = c = 1 or a = b = c = −1.</p>
<p>Thus, the candidate a = b = c = 1 (or a = b = c = −1) gives the minimal value.In conclusion, the minimum value of f is 3081, and it is attained exactly when a, b, c are all equal with |a| = 1 and the signs arranged so that a 3 b = 1 (that is, a = b = c = 1 or a = b = c = −1).</p>
<p>Figure 12 :
12
Figure 12: An example from OlymMATH-EN-HARD subset.o3-mini (high) attempted to "guess" the equality conditions but failed to produce a correct solution (highlighted in red).</p>
<p>Table 1 :
1
Comparison of existing benchmarks.EN and ZH denote English and Chinese, respectively.
Name# Problems # Field Language EvaluationDifficultyGSM8K [8]1319-ENRuleGrade SchoolMATH [9]50006ENRuleCompetitionAIME 2024 [15]30-ENRuleOlympiadAIME 2025 [16]30-ENRuleOlympiadHMMT 202502 [17]30-ENRuleOlympiadUSAMO 2025 [18]6-ENLLMOlympiadOlympiad Bench [19]84763ZH &amp; ENRuleCEE &amp; OlympiadOmni-MATH [20]442833+ENLLMOlympiadOlymMATH-EN2004ENRuleOlympiadOlymMATH-ZH2004ZHRuleOlympiad</p>
<p>Table 2 :
2
The distribution of contest problems by category.
CategoryTopic# HARD # EASY # TotalAlgebra (Alg.)Inequality, Sequence, Trigonometry, etc.252550Geometry (Geo.)Solid Geometry, Analytic Geometry, etc.253358Number Theory (Num.)Divisibility, Diophantine Equation, etc.251338Combinatorics (Com.)Graph Theory, Permutation, etc.252954Total100100200</p>
<p>Table 3 :
3
The included and excluded formats of the final answer.
Type &amp; ExampleType &amp; ExampleIncludedReal number: 16 • , 2 2017 + arctan 2Interval: [√ 33, +∞), (4, 5π]ExcludedSet Operations: {4, 5} ∪ {1, 8} Complex number: 9 + 4i, √ −4Variable: 3 √ Text: East, Alice 5a 2 , p 2 − pq, n! + 2</p>
<p>Table 4 :
4
Model performance on OlymMATH-EN.Models within each model size group are sorted by release time.The abbreviations "Alg.","Geo.","Num.", and "Com."represent the four categories in OlymMATH.Highest accuracy per model size is bolded.The second highest accuracy per model size is underlined.Models sampled only 8 times are marked in gray to indicate potential instability.
OlymMATH-EN-HARDOlymMATH-EN-EASYModelAlg.Geo.Num.Com.Avg.Alg.Geo.Num.Com.</p>
<p>Table 5 :
5
Model performance on OlymMATH-ZH.Models within each model size group are sorted by release time.The abbreviations "Alg.","Geo.","Num.", and "Com."represent the four categories in OlymMATH.Highest accuracy per model size is bolded.The second highest accuracy per model size is underlined.Models sampled only 8 times are marked in gray to indicate potential instability.
OlymMATH-ZH-HARDOlymMATH-ZH-EASYModelAlg.Geo.Num.Com.Avg.Alg.Geo.Num.Com.
https://matharena.ai/</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, A survey of large language models. 2025</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, arXiv:2412.15115Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. Xingzhang Ren, Xuancheng Ren,2024arXiv preprint</p>
<p>. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Bethany Baptiste Rozière, Binh Biron, Bobbie Tang, Charlotte Chern, Chaya Caucheteux, Chloe Nayak, Chris Bi, Chris Marra, Christian Mcconnell, Christophe Keller, Chunyang Touret, Corinne Wu, Cristian Canton Wong, Cyrus Ferrer, Damien Nikolaidis, Daniel Allonsius, Danielle Song, Danny Pintz, David Livshits, Dhruv Esiobu, Dhruv Choudhary, Diego Mahajan, Diego Garcia-Olano, Dieuwke Perino, Egor Hupkes, Ehab Lakomkin, Elina Albadawy, Emily Lobanova, Eric Michael Dinan, Filip Smith, Frank Radenovic, Gabriel Zhang, Gabrielle Synnaeve, Georgia Lee, Graeme Lewis Anderson, Grégoire Nail, Guan Mialon, Guillem Pang, Hailey Cucurell, Hannah Nguyen, Hu Korevaar, Hugo Xu, Iliyan Touvron, Zarov, Arrieta Imanol, Isabel M Ibarra, Ishan Kloumann, Ivan Misra, Jana Evtimov, Jason Vranes, Jay Park, Jeet Mahadeokar, Jelmer Shah, Jennifer Van Der Linde, Jenny Billock, Jenya Hong, Jeremy Lee, Jianfeng Fu, Jianyu Chi, Jiawen Huang, Jie Liu, Jiecao Wang, Joanna Yu, Joe Bitton, Jongsoo Spisak, Joseph Park, Joshua Rocca, Joshua Johnstun, Junteng Saxe, Jia, Jan Geffert,. 2024Kartikeya UpasaniJade Copet, Jaewon Lee; Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stoneand et al. The llama 3 herd of models. CoRR, abs/2407.21783</p>
<p>. Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z F Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J L Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R J Chen, R L Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, 2025Shengfeng Ye, Shiyu Wang, Shuiping YuShunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948</p>
<p>Openai o1 system card. Openai, 2024</p>
<p>Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, Ji-Rong Wen, An empirical study on eliciting and improving r1-like reasoning models. 2025</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. 2024</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021December 2021, virtual, 2021</p>
<p>Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, Kai Zou, 2024</p>
<p>Have llms advanced enough? a challenging problem solving benchmark for large language models. Daman Arora, Himanshu Gaurav Singh, Mausam , 2023</p>
<p>Openai o3-mini: Pushing the frontier of cost-effective reasoning. Openai, 12025</p>
<p>Gemini 2.5: Our most intelligent ai model. Google Deepmind, 32025</p>
<p>Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen, Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems. 2024</p>
<p>. Hmmt, Hmmt, 202502. 2025</p>
<p>Olympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAugust 11-16, 2024. 20241ACL 2024</p>
<p>Omni-math: A universal olympiad level mathematic benchmark for large language models. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, arXiv:2410.079852024arXiv preprint</p>
<p>Still-3-1.5b-preview: Enhancing slow thinking abilities of small models through reinforcement learning. Rucaibox, Team, 2025</p>
<p>Deepscaler: Surpassing o1-preview with a 1.5b model by scaling rl. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. 2025Notion Blog</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Qwen Team, March 2025</p>
<p>Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. 2025</p>
<p>Skywork open reasoner series. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, Yahui Zhou, 2025Notion Blog</p>
<p>. Glm Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang, 2024Chatglm: A family of large language models from glm-130b to glm-4 all tools</p>
<p>Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping, Acemath: Advancing frontier math reasoning with post-training and reward modeling. 2024arXiv preprint</p>
<p>Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, Igor Gitman, 2025</p>            </div>
        </div>

    </div>
</body>
</html>
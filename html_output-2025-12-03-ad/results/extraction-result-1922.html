<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1922 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1922</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1922</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-280565752</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.07626v1.pdf" target="_blank">AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Visual Robot Manipulation (VRM) aims to enable a robot to follow natural language instructions based on robot states and visual observations, and therefore requires costly multi-modal data. To compensate for the deficiency of robot data, existing approaches have employed vision-language pretraining with large-scale data. However, they either utilize web data that differs from robotic tasks, or train the model in an implicit way (e.g., predicting future frames at the pixel level), thus showing limited generalization ability under insufficient robot data. In this paper, we propose to learn from large-scale human action video datasets in an explicit way (i.e., imitating human actions from hand keypoints), introducing Visual Robot Manipulation with Analogical Reasoning (AR-VRM). To acquire action knowledge explicitly from human action videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme, enabling the VLM to learn human action knowledge and directly predict human hand keypoints. During fine-tuning on robot data, to facilitate the robotic arm in imitating the action patterns of human motions, we first retrieve human action videos that perform similar manipulation tasks and have similar historical observations , and then learn the Analogical Reasoning (AR) map between human hand keypoints and robot components. Taking advantage of focusing on action keypoints instead of irrelevant visual cues, our method achieves leading performance on the CALVIN benchmark {and real-world experiments}. In few-shot scenarios, our AR-VRM outperforms previous methods by large margins , underscoring the effectiveness of explicitly imitating human actions under data scarcity.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1922.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1922.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AR-VRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Robot Manipulation with Analogical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-conditioned visual robot manipulation system that explicitly learns human action knowledge by pretraining a vision-language model to predict 3D human hand keypoints and then fine-tunes via retrieval of human demonstrations plus an analogical map that aligns human keypoints to robot components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AR-VRM (Keypoint VLM + Analogical Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal transformer-based VLM that concatenates CLIP text embeddings, ViT image tokens (MAE-pretrained), and HandFormer keypoint embeddings; pretrained to autoregressively predict future 3D hand keypoints from egocentric human video + language, and fine-tuned to predict robot state deltas using retrieved human demonstrations and a learnable analogical mapping matrix between human keypoints and robot components.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal video-text pretraining with explicit 3D hand-keypoint supervision (keypoint VLM pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on Ego4D egocentric human action videos (paper uses ~800k clips, ~8M frames) with natural language annotations; data contains dense human-object interaction sequences (manipulation actions, approach/grasp/release motions, affordance-rich behaviors) suitable for learning manipulation dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic visual manipulation (CALVIN benchmark and real robot manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Long-horizon, language-conditioned robot manipulation (CALVIN): multi-task, sequential object manipulation, articulated-object manipulation and transportation; 7-DOF continuous control where actions are continuous end-effector ∆-pose and binary gripper open/close; evaluated in simulated CALVIN scenes (seen and unseen) and in real-robot object and drawer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Yes — the paper explicitly selects Ego4D because its egocentric human manipulation videos share viewpoint, object interactions and action types with robotic tasks; during fine-tuning they retrieve human clips by combined language and visual-frame similarity to maximize overlap in actions and context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported downstream gains: full-data multitask (ABCD→D) average success rate 85.4% (AR-VRM) vs prior SOTA 84.2%; unseen-scene generalization average success rate 65.9% (AR-VRM) vs prior SOTA 61.2% (↑4.7% abs). Few-shot (10% robot-training data) average success rate 45.6% for AR-VRM (completing 5 tasks metric) and average completed task length 2.28.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Ablations show degraded performance when human keypoint pretraining is removed: e.g., an ablated model trained only on robot data performs worse in the ABCD→D setting (ablation reported avg.rate ≈78.6% vs full AR-VRM 85.4%); baseline methods without this explicit keypoint pretraining (e.g., GR-1 in some settings) achieve lower few-shot performance (GR-1 10% avg.rate 40.0% vs AR-VRM 45.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Yes — quantitative few-shot comparison: with only 10% of robot training data AR-VRM achieves avg.success-rate 45.6% versus GR-1 baseline 40.0% (absolute +5.6%); ablation variants lacking keypoint pretraining or analogical mapping show lower sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No standard transformer attention maps are presented as an analysis target; instead the paper visualizes the learned analogical map (m) between human keypoint nodes and robot component nodes, showing semantically sensible correspondences (e.g., robot grasper strongly linked to human fingertips; robot root linked to human palm).</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Partial — the method uses learned feature vectors for keypoint nodes (f_k) and robot state nodes (f_s) and inspects the analogical mapping matrix that operates on these features; no extensive clustering / PCA / manifold analyses are reported beyond qualitative visualization of the analogical map.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Yes — explicit evidence: (1) pretraining objective predicts future 3D hand keypoints (explicit action representation), (2) during fine-tuning retrieved human demonstrations are used and a learned analogical mapping translates human keypoint features into imitated robot-state features, (3) qualitative examples (e.g., drawer grasp) and improved few-shot/unseen-scene metrics support that language-conditioned action semantics (approach/grasp) are grounded in predicted keypoint motion patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No detailed analysis across low- vs high-level feature hierarchies is provided; the paper focuses on keypoint-level action representations rather than disentangling texture/edge vs object-level features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer success depends on domain/task similarity and retrieval quality: using egocentric Ego4D videos (viewpoint and manipulation content aligned with robot tasks) and retrieving human clips by combined language+visual similarity improves transfer; design choices such as freezing the keypoint encoder/head and fine-tuning the transformer also affect transfer (freezing keypoint encoder improves stability; freezing the VLM hurts performance).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper reports real-robot tests on seen object instances, unseen instances of the same category, and unseen categories. It reports minimal performance drop for unseen instances of trained categories (good in-category generalization) but larger drops for unseen object categories; exact numeric deltas for these real-robot splits are given qualitatively in the text (no comprehensive numeric table in main text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot: demonstrated significant gains in sample-limited regime (10% training data). The paper also reports improved generalization to unseen instruction languages (unseen-language generalization), but not a pure zero-shot policy that requires zero robot data for all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Yes — ablations where different components are frozen show: (a) freezing the keypoint encoder/head during fine-tuning improves convergence and alignment of the analogical map, (b) freezing the VLM (transformer) causes large performance drops, indicating transformer fine-tuning is critical for transferring human keypoint knowledge to robot state prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit evidence that language-pretraining or keypoint pretraining harms performance is reported; ablations indicate removal of pretraining reduces performance (i.e., no negative transfer cases documented).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Indirect comparisons: the paper contrasts its explicit keypoint VLM pretraining with previous methods that used web image-text pretraining or implicit human-video pretraining (contrastive or pixel-level future prediction) and reports that explicit keypoint pretraining yields better generalization and few-shot performance; there is no head-to-head controlled comparison against a pure vision-only ImageNet-style baseline in the presented excerpts.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>The pretraining task explicitly models temporal dynamics by predicting future hand keypoints given past frames and language; however, the paper does not present a detailed analysis of how representations evolve during fine-tuning over time beyond reporting final metrics and the role of replaying human video samples to avoid catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No measurements of representation dimensionality (PCA / intrinsic dimension) are reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1922.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1922.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Keypoint VLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Keypoint Vision-Language Model (Keypoint VLM pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining scheme introduced in this work that augments a VLM to directly predict future 3D human hand keypoints (from HandFormer features) conditioned on past frames and language, enabling explicit action-centric representation learning from large-scale human egocentric videos.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Keypoint Vision-Language Model (Keypoint VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal transformer that ingests CLIP text embeddings, ViT image CLS & patch tokens (resampled), and HandFormer keypoint embeddings; trained with an MSE loss to predict future hand keypoint vectors as explicit action tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal video-text with explicit 3D keypoint supervision (hand-pose prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Ego4D egocentric clips annotated with language descriptions; preprocessed to extract 3D hand keypoints via InterHand and encoded by HandFormer; these clips contain sequences of manipulation actions and affordance-rich interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Pretraining objective for downstream robotic manipulation (language-conditioned imitation/fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Downstream usage: fine-tuning on language-conditioned robot state prediction tasks (CALVIN) where predicted keypoints guide robot action imitation via analogical mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Explicit design aligns pretraining objective with manipulation semantics by focusing on hand keypoints (action primitives) rather than pixel-level prediction, increasing semantic overlap with robot manipulation behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Ablations attribute significant downstream improvements to keypoint pretraining; removing pretraining reduces ABCD→D average success rate (example ablation avg.rate ≈78.6% vs full 85.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Without keypoint pretraining the downstream VLM trained only on robot data performs noticeably worse (see ablation results in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Keypoint pretraining improves few-shot learning — with 10% robot data the full approach outperforms baselines (AR-VRM 45.6% vs GR-1 40.0%); ablations without keypoint pretraining lower sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit transformer-attention visualization for the keypoint-VLM beyond qualitative discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>The pretraining produces keypoint feature vectors used downstream, but no systematic embedding-space clustering or dimensionality analyses are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Yes — by training to predict future 3D hand keypoints the model learns explicit action dynamics (approach, grasp trajectories) that are then mapped to robot components during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not explicitly analyzed; emphasis is on keypoint/action-level representations over low-level pixel features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Pretraining is most effective when pretraining data matches robotic viewpoints and manipulation content (hence Ego4D egocentric choice) and when retrieval finds semantically-similar human clips during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not directly analyzed for the pretraining stage alone; downstream experiments indicate better in-category generalization when keypoint pretraining is used.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Improves few-shot downstream learning (10% data regime); no claim of full zero-shot control using only keypoint VLM in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Paper freezes the keypoint encoder/head during fine-tuning (stabilizing learned keypoint representations) and fine-tunes transformer layers; this design choice is validated in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No negative transfer reported for keypoint pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Paper argues explicit keypoint pretraining avoids irrelevant background/pixel-level noise that implicit video pretraining (pixel prediction) suffers from, but no direct numeric comparison vs an ImageNet-only vision-pretrained baseline is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>The objective explicitly models temporal prediction of future keypoints (next-token/keypoint prediction) and uses causal transformer layers to learn temporal action dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit dimensionality analysis is reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1922.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1922.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP (text encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning transferable visual models from natural language supervision (CLIP) — text encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The CLIP text encoder is used as a frozen language encoder to turn natural-language instructions into embeddings consumed by the AR-VRM transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP text encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastively pretrained text encoder over large-scale image-text pairs; used here (frozen) to produce instruction embeddings fed to the multimodal transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on large-scale image-text pairs (contrastive)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Web-scale image-caption pairs containing object descriptions, scene text, and diverse semantic concepts; contains object and action language but not tuned specifically to egocentric manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Used as language embedding module for AR-VRM (downstream robotic manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Encodes natural-language instructions (task descriptions) which are concatenated with visual/keypoint tokens for downstream robot-state prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>CLIP provides general language-visual alignment; the paper fixes CLIP during pretraining/fine-tuning so alignment is leveraged but not adapted to robot-specific language.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not evaluated in isolation in this paper; used as part of AR-VRM where overall system benefits are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported separately in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported for CLIP component alone.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No CLIP-specific attention analysis is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space analysis of CLIP embeddings is provided beyond using them as fixed instruction tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect — CLIP supplies language grounding to visual features in the VLM, but explicit action grounding is achieved via keypoint prediction and analogical mapping rather than CLIP alone.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>CLIP is used frozen; paper does not probe when CLIP alignment helps vs hurts beyond overall system performance.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not assessed specifically for CLIP in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>CLIP's pretraining supports language generalization tasks, but paper-level zero/few-shot claims concern the entire AR-VRM system rather than CLIP directly.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>CLIP parameters are frozen during pretraining and fine-tuning in AR-VRM (per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>None reported specifically for CLIP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CLIP is a vision-language model; paper contrasts web-scale (general) vision-language pretraining vs their explicit keypoint video pretraining and argues task-specificity advantages for the latter.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>CLIP provides static language embeddings — temporal modeling is handled by the VLM transformer and keypoint prediction head.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality analysis of CLIP encodings is provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1922.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1922.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GR-1 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GR-1 (baseline language-conditioned robot policy reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior state-of-the-art baseline for language-conditioned robot manipulation used for comparison in CALVIN experiments; reported as the main competitor in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GR-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced baseline method for language-conditioned manipulation; the paper compares AR-VRM to GR-1 but does not re-implement or deeply describe GR-1's architecture in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper (baseline cited from prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation (CALVIN benchmark comparisons in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same CALVIN long-horizon manipulation tasks used to evaluate AR-VRM.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed here (baseline from prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in this paper's tables: ABCD→D (full-data) avg success rate 84.2% (GR-1) vs AR-VRM 85.4%; unseen-scene avg success rate 61.2% (GR-1) vs AR-VRM 65.9%; few-shot 10% avg success rate 40.0% (GR-1) vs AR-VRM 45.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable (baseline reported as-is).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper reports GR-1 baseline numbers in the 10% data regime for direct comparison (see above differences).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided for this baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided for this baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Baseline methods are compared by downstream success rates; GR-1's internal grounding mechanisms are not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>GR-1 performance is used as a reference to show AR-VRM's improved transfer on unseen scenes and few-shot regimes; details of GR-1's transfer conditions come from the baseline's original sources.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>GR-1 real-robot baseline numbers are used in comparisons (paper reports GR-1 lower few-shot and unseen-scene performance than AR-VRM).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Paper reports GR-1's 10%-data few-shot results for comparison (see performance numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-wise ablation of GR-1 is performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative-transfer claim about GR-1 is made in this paper; it just performs worse than AR-VRM under low-data and unseen-scene settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>GR-1 is a baseline representation of previous methods; the paper positions AR-VRM as improving upon baselines (including GR-1) by using explicit keypoint pretraining and analogical mapping.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1922.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1922.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MT-R3M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MT-R3M (multi-task R3M variant referenced in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior visual representation approach referenced as a baseline that leverages human video data for robot manipulation representation learning; compared in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MT-R3M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced prior method for pretraining visual representations for manipulation tasks (paper cites MT-R3M as an approach using human video data but trained implicitly); the AR-VRM paper compares to it but does not reimplement or deeply describe its internals.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Reported in paper as prior human-video-based pretraining, typically implicit (contrastive or pixel-level future prediction) rather than explicit keypoint supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Human action video datasets (e.g., Ego4D) or similar; prior methods train implicitly on pixels/features rather than explicit keypoints.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation (CALVIN benchmark comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same CALVIN tasks used in this work for evaluation; MT-R3M is shown as a baseline in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper argues MT-R3M-style implicit video pretraining includes background and pixel noise and thus is less semantically focused on action keypoints compared to AR-VRM's explicit keypoint supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in paper tables as a baseline (values vary by split); AR-VRM outperforms MT-R3M in reported CALVIN evaluations, though exact per-split numbers should be read from the paper's tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not separately reported here for MT-R3M.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>MT-R3M is shown to be inferior to AR-VRM in data-scarce regimes per paper claims and tables; explicit numbers are provided in the paper tables for direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided for MT-R3M in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper claims MT-R3M-style implicit pretraining does not focus on explicit action keypoints and thus provides weaker grounding for robot manipulation compared to keypoint-based pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper suggests implicit video pretraining is more sensitive to background differences and pixel noise; AR-VRM's explicit keypoint focus yields stronger transfer under limited robot data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not specifically analyzed for MT-R3M within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>MT-R3M's few-shot performance is reported in baseline comparisons (worse than AR-VRM in 10% data regime per paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided for MT-R3M in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper argues implicit pixel-level pretraining can introduce irrelevant cues harming generalization, but no quantified negative transfer numbers for MT-R3M are supplied here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Paper contrasts MT-R3M (implicit video pretraining) and other web-data vision-language pretraining approaches, arguing AR-VRM's explicit keypoint pretraining yields better downstream manipulation performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Robotics transformer for real-world control at scale <em>(Rating: 2)</em></li>
                <li>Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks <em>(Rating: 2)</em></li>
                <li>R3m: A universal visual representation for robot manipulation <em>(Rating: 2)</em></li>
                <li>Physically grounded vision-language models for robotic manipulation <em>(Rating: 2)</em></li>
                <li>Ego4D: Around the world in 3,000 hours of egocentric video <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1922",
    "paper_id": "paper-280565752",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "AR-VRM",
            "name_full": "Visual Robot Manipulation with Analogical Reasoning",
            "brief_description": "A language-conditioned visual robot manipulation system that explicitly learns human action knowledge by pretraining a vision-language model to predict 3D human hand keypoints and then fine-tunes via retrieval of human demonstrations plus an analogical map that aligns human keypoints to robot components.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AR-VRM (Keypoint VLM + Analogical Reasoning)",
            "model_description": "A multimodal transformer-based VLM that concatenates CLIP text embeddings, ViT image tokens (MAE-pretrained), and HandFormer keypoint embeddings; pretrained to autoregressively predict future 3D hand keypoints from egocentric human video + language, and fine-tuned to predict robot state deltas using retrieved human demonstrations and a learnable analogical mapping matrix between human keypoints and robot components.",
            "pretraining_type": "multimodal video-text pretraining with explicit 3D hand-keypoint supervision (keypoint VLM pretraining)",
            "pretraining_data_description": "Pretrained on Ego4D egocentric human action videos (paper uses ~800k clips, ~8M frames) with natural language annotations; data contains dense human-object interaction sequences (manipulation actions, approach/grasp/release motions, affordance-rich behaviors) suitable for learning manipulation dynamics.",
            "target_task_name": "Language-conditioned robotic visual manipulation (CALVIN benchmark and real robot manipulation)",
            "target_task_description": "Long-horizon, language-conditioned robot manipulation (CALVIN): multi-task, sequential object manipulation, articulated-object manipulation and transportation; 7-DOF continuous control where actions are continuous end-effector ∆-pose and binary gripper open/close; evaluated in simulated CALVIN scenes (seen and unseen) and in real-robot object and drawer tasks.",
            "semantic_alignment": "Yes — the paper explicitly selects Ego4D because its egocentric human manipulation videos share viewpoint, object interactions and action types with robotic tasks; during fine-tuning they retrieve human clips by combined language and visual-frame similarity to maximize overlap in actions and context.",
            "performance_with_language_pretraining": "Reported downstream gains: full-data multitask (ABCD→D) average success rate 85.4% (AR-VRM) vs prior SOTA 84.2%; unseen-scene generalization average success rate 65.9% (AR-VRM) vs prior SOTA 61.2% (↑4.7% abs). Few-shot (10% robot-training data) average success rate 45.6% for AR-VRM (completing 5 tasks metric) and average completed task length 2.28.",
            "performance_without_language_pretraining": "Ablations show degraded performance when human keypoint pretraining is removed: e.g., an ablated model trained only on robot data performs worse in the ABCD→D setting (ablation reported avg.rate ≈78.6% vs full AR-VRM 85.4%); baseline methods without this explicit keypoint pretraining (e.g., GR-1 in some settings) achieve lower few-shot performance (GR-1 10% avg.rate 40.0% vs AR-VRM 45.6%).",
            "sample_efficiency_comparison": "Yes — quantitative few-shot comparison: with only 10% of robot training data AR-VRM achieves avg.success-rate 45.6% versus GR-1 baseline 40.0% (absolute +5.6%); ablation variants lacking keypoint pretraining or analogical mapping show lower sample efficiency.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No standard transformer attention maps are presented as an analysis target; instead the paper visualizes the learned analogical map (m) between human keypoint nodes and robot component nodes, showing semantically sensible correspondences (e.g., robot grasper strongly linked to human fingertips; robot root linked to human palm).",
            "embedding_space_analysis": "Partial — the method uses learned feature vectors for keypoint nodes (f_k) and robot state nodes (f_s) and inspects the analogical mapping matrix that operates on these features; no extensive clustering / PCA / manifold analyses are reported beyond qualitative visualization of the analogical map.",
            "action_grounding_evidence": "Yes — explicit evidence: (1) pretraining objective predicts future 3D hand keypoints (explicit action representation), (2) during fine-tuning retrieved human demonstrations are used and a learned analogical mapping translates human keypoint features into imitated robot-state features, (3) qualitative examples (e.g., drawer grasp) and improved few-shot/unseen-scene metrics support that language-conditioned action semantics (approach/grasp) are grounded in predicted keypoint motion patterns.",
            "hierarchical_features_evidence": "No detailed analysis across low- vs high-level feature hierarchies is provided; the paper focuses on keypoint-level action representations rather than disentangling texture/edge vs object-level features.",
            "transfer_conditions": "Transfer success depends on domain/task similarity and retrieval quality: using egocentric Ego4D videos (viewpoint and manipulation content aligned with robot tasks) and retrieving human clips by combined language+visual similarity improves transfer; design choices such as freezing the keypoint encoder/head and fine-tuning the transformer also affect transfer (freezing keypoint encoder improves stability; freezing the VLM hurts performance).",
            "novel_vs_familiar_objects": "Paper reports real-robot tests on seen object instances, unseen instances of the same category, and unseen categories. It reports minimal performance drop for unseen instances of trained categories (good in-category generalization) but larger drops for unseen object categories; exact numeric deltas for these real-robot splits are given qualitatively in the text (no comprehensive numeric table in main text excerpt).",
            "zero_shot_or_few_shot": "Few-shot: demonstrated significant gains in sample-limited regime (10% training data). The paper also reports improved generalization to unseen instruction languages (unseen-language generalization), but not a pure zero-shot policy that requires zero robot data for all tasks.",
            "layer_analysis": "Yes — ablations where different components are frozen show: (a) freezing the keypoint encoder/head during fine-tuning improves convergence and alignment of the analogical map, (b) freezing the VLM (transformer) causes large performance drops, indicating transformer fine-tuning is critical for transferring human keypoint knowledge to robot state prediction.",
            "negative_transfer_evidence": "No explicit evidence that language-pretraining or keypoint pretraining harms performance is reported; ablations indicate removal of pretraining reduces performance (i.e., no negative transfer cases documented).",
            "comparison_to_vision_only": "Indirect comparisons: the paper contrasts its explicit keypoint VLM pretraining with previous methods that used web image-text pretraining or implicit human-video pretraining (contrastive or pixel-level future prediction) and reports that explicit keypoint pretraining yields better generalization and few-shot performance; there is no head-to-head controlled comparison against a pure vision-only ImageNet-style baseline in the presented excerpts.",
            "temporal_dynamics": "The pretraining task explicitly models temporal dynamics by predicting future hand keypoints given past frames and language; however, the paper does not present a detailed analysis of how representations evolve during fine-tuning over time beyond reporting final metrics and the role of replaying human video samples to avoid catastrophic forgetting.",
            "dimensionality_analysis": "No measurements of representation dimensionality (PCA / intrinsic dimension) are reported.",
            "uuid": "e1922.0"
        },
        {
            "name_short": "Keypoint VLM",
            "name_full": "Keypoint Vision-Language Model (Keypoint VLM pretraining)",
            "brief_description": "A pretraining scheme introduced in this work that augments a VLM to directly predict future 3D human hand keypoints (from HandFormer features) conditioned on past frames and language, enabling explicit action-centric representation learning from large-scale human egocentric videos.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Keypoint Vision-Language Model (Keypoint VLM)",
            "model_description": "Multimodal transformer that ingests CLIP text embeddings, ViT image CLS & patch tokens (resampled), and HandFormer keypoint embeddings; trained with an MSE loss to predict future hand keypoint vectors as explicit action tokens.",
            "pretraining_type": "Multimodal video-text with explicit 3D keypoint supervision (hand-pose prediction)",
            "pretraining_data_description": "Ego4D egocentric clips annotated with language descriptions; preprocessed to extract 3D hand keypoints via InterHand and encoded by HandFormer; these clips contain sequences of manipulation actions and affordance-rich interactions.",
            "target_task_name": "Pretraining objective for downstream robotic manipulation (language-conditioned imitation/fine-tuning)",
            "target_task_description": "Downstream usage: fine-tuning on language-conditioned robot state prediction tasks (CALVIN) where predicted keypoints guide robot action imitation via analogical mapping.",
            "semantic_alignment": "Explicit design aligns pretraining objective with manipulation semantics by focusing on hand keypoints (action primitives) rather than pixel-level prediction, increasing semantic overlap with robot manipulation behaviors.",
            "performance_with_language_pretraining": "Ablations attribute significant downstream improvements to keypoint pretraining; removing pretraining reduces ABCD→D average success rate (example ablation avg.rate ≈78.6% vs full 85.4%).",
            "performance_without_language_pretraining": "Without keypoint pretraining the downstream VLM trained only on robot data performs noticeably worse (see ablation results in the paper).",
            "sample_efficiency_comparison": "Keypoint pretraining improves few-shot learning — with 10% robot data the full approach outperforms baselines (AR-VRM 45.6% vs GR-1 40.0%); ablations without keypoint pretraining lower sample efficiency.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No explicit transformer-attention visualization for the keypoint-VLM beyond qualitative discussion.",
            "embedding_space_analysis": "The pretraining produces keypoint feature vectors used downstream, but no systematic embedding-space clustering or dimensionality analyses are reported.",
            "action_grounding_evidence": "Yes — by training to predict future 3D hand keypoints the model learns explicit action dynamics (approach, grasp trajectories) that are then mapped to robot components during fine-tuning.",
            "hierarchical_features_evidence": "Not explicitly analyzed; emphasis is on keypoint/action-level representations over low-level pixel features.",
            "transfer_conditions": "Pretraining is most effective when pretraining data matches robotic viewpoints and manipulation content (hence Ego4D egocentric choice) and when retrieval finds semantically-similar human clips during fine-tuning.",
            "novel_vs_familiar_objects": "Not directly analyzed for the pretraining stage alone; downstream experiments indicate better in-category generalization when keypoint pretraining is used.",
            "zero_shot_or_few_shot": "Improves few-shot downstream learning (10% data regime); no claim of full zero-shot control using only keypoint VLM in the paper.",
            "layer_analysis": "Paper freezes the keypoint encoder/head during fine-tuning (stabilizing learned keypoint representations) and fine-tunes transformer layers; this design choice is validated in ablations.",
            "negative_transfer_evidence": "No negative transfer reported for keypoint pretraining.",
            "comparison_to_vision_only": "Paper argues explicit keypoint pretraining avoids irrelevant background/pixel-level noise that implicit video pretraining (pixel prediction) suffers from, but no direct numeric comparison vs an ImageNet-only vision-pretrained baseline is reported.",
            "temporal_dynamics": "The objective explicitly models temporal prediction of future keypoints (next-token/keypoint prediction) and uses causal transformer layers to learn temporal action dynamics.",
            "dimensionality_analysis": "No explicit dimensionality analysis is reported.",
            "uuid": "e1922.1"
        },
        {
            "name_short": "CLIP (text encoder)",
            "name_full": "Learning transferable visual models from natural language supervision (CLIP) — text encoder",
            "brief_description": "The CLIP text encoder is used as a frozen language encoder to turn natural-language instructions into embeddings consumed by the AR-VRM transformer.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_name": "CLIP text encoder",
            "model_description": "Contrastively pretrained text encoder over large-scale image-text pairs; used here (frozen) to produce instruction embeddings fed to the multimodal transformer.",
            "pretraining_type": "Vision-language pretraining on large-scale image-text pairs (contrastive)",
            "pretraining_data_description": "Web-scale image-caption pairs containing object descriptions, scene text, and diverse semantic concepts; contains object and action language but not tuned specifically to egocentric manipulation.",
            "target_task_name": "Used as language embedding module for AR-VRM (downstream robotic manipulation)",
            "target_task_description": "Encodes natural-language instructions (task descriptions) which are concatenated with visual/keypoint tokens for downstream robot-state prediction.",
            "semantic_alignment": "CLIP provides general language-visual alignment; the paper fixes CLIP during pretraining/fine-tuning so alignment is leveraged but not adapted to robot-specific language.",
            "performance_with_language_pretraining": "Not evaluated in isolation in this paper; used as part of AR-VRM where overall system benefits are reported.",
            "performance_without_language_pretraining": "Not reported separately in this paper.",
            "sample_efficiency_comparison": "Not reported for CLIP component alone.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No CLIP-specific attention analysis is provided.",
            "embedding_space_analysis": "No embedding-space analysis of CLIP embeddings is provided beyond using them as fixed instruction tokens.",
            "action_grounding_evidence": "Indirect — CLIP supplies language grounding to visual features in the VLM, but explicit action grounding is achieved via keypoint prediction and analogical mapping rather than CLIP alone.",
            "hierarchical_features_evidence": "Not analyzed in this paper.",
            "transfer_conditions": "CLIP is used frozen; paper does not probe when CLIP alignment helps vs hurts beyond overall system performance.",
            "novel_vs_familiar_objects": "Not assessed specifically for CLIP in this work.",
            "zero_shot_or_few_shot": "CLIP's pretraining supports language generalization tasks, but paper-level zero/few-shot claims concern the entire AR-VRM system rather than CLIP directly.",
            "layer_analysis": "CLIP parameters are frozen during pretraining and fine-tuning in AR-VRM (per paper).",
            "negative_transfer_evidence": "None reported specifically for CLIP.",
            "comparison_to_vision_only": "CLIP is a vision-language model; paper contrasts web-scale (general) vision-language pretraining vs their explicit keypoint video pretraining and argues task-specificity advantages for the latter.",
            "temporal_dynamics": "CLIP provides static language embeddings — temporal modeling is handled by the VLM transformer and keypoint prediction head.",
            "dimensionality_analysis": "No dimensionality analysis of CLIP encodings is provided.",
            "uuid": "e1922.2"
        },
        {
            "name_short": "GR-1 (baseline)",
            "name_full": "GR-1 (baseline language-conditioned robot policy reported in paper)",
            "brief_description": "A prior state-of-the-art baseline for language-conditioned robot manipulation used for comparison in CALVIN experiments; reported as the main competitor in tables.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GR-1",
            "model_description": "Referenced baseline method for language-conditioned manipulation; the paper compares AR-VRM to GR-1 but does not re-implement or deeply describe GR-1's architecture in the main text.",
            "pretraining_type": "Not specified in this paper (baseline cited from prior work).",
            "pretraining_data_description": "Not specified here.",
            "target_task_name": "Language-conditioned robotic manipulation (CALVIN benchmark comparisons in this paper).",
            "target_task_description": "Same CALVIN long-horizon manipulation tasks used to evaluate AR-VRM.",
            "semantic_alignment": "Not analyzed here (baseline from prior work).",
            "performance_with_language_pretraining": "Reported in this paper's tables: ABCD→D (full-data) avg success rate 84.2% (GR-1) vs AR-VRM 85.4%; unseen-scene avg success rate 61.2% (GR-1) vs AR-VRM 65.9%; few-shot 10% avg success rate 40.0% (GR-1) vs AR-VRM 45.6%.",
            "performance_without_language_pretraining": "Not applicable (baseline reported as-is).",
            "sample_efficiency_comparison": "Paper reports GR-1 baseline numbers in the 10% data regime for direct comparison (see above differences).",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Not provided for this baseline in this paper.",
            "embedding_space_analysis": "Not provided for this baseline in this paper.",
            "action_grounding_evidence": "Baseline methods are compared by downstream success rates; GR-1's internal grounding mechanisms are not analyzed in this paper.",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "GR-1 performance is used as a reference to show AR-VRM's improved transfer on unseen scenes and few-shot regimes; details of GR-1's transfer conditions come from the baseline's original sources.",
            "novel_vs_familiar_objects": "GR-1 real-robot baseline numbers are used in comparisons (paper reports GR-1 lower few-shot and unseen-scene performance than AR-VRM).",
            "zero_shot_or_few_shot": "Paper reports GR-1's 10%-data few-shot results for comparison (see performance numbers above).",
            "layer_analysis": "No layer-wise ablation of GR-1 is performed in this paper.",
            "negative_transfer_evidence": "No explicit negative-transfer claim about GR-1 is made in this paper; it just performs worse than AR-VRM under low-data and unseen-scene settings.",
            "comparison_to_vision_only": "GR-1 is a baseline representation of previous methods; the paper positions AR-VRM as improving upon baselines (including GR-1) by using explicit keypoint pretraining and analogical mapping.",
            "uuid": "e1922.3"
        },
        {
            "name_short": "MT-R3M",
            "name_full": "MT-R3M (multi-task R3M variant referenced in paper)",
            "brief_description": "A prior visual representation approach referenced as a baseline that leverages human video data for robot manipulation representation learning; compared in tables.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MT-R3M",
            "model_description": "Referenced prior method for pretraining visual representations for manipulation tasks (paper cites MT-R3M as an approach using human video data but trained implicitly); the AR-VRM paper compares to it but does not reimplement or deeply describe its internals.",
            "pretraining_type": "Reported in paper as prior human-video-based pretraining, typically implicit (contrastive or pixel-level future prediction) rather than explicit keypoint supervision.",
            "pretraining_data_description": "Human action video datasets (e.g., Ego4D) or similar; prior methods train implicitly on pixels/features rather than explicit keypoints.",
            "target_task_name": "Language-conditioned robotic manipulation (CALVIN benchmark comparisons).",
            "target_task_description": "Same CALVIN tasks used in this work for evaluation; MT-R3M is shown as a baseline in tables.",
            "semantic_alignment": "Paper argues MT-R3M-style implicit video pretraining includes background and pixel noise and thus is less semantically focused on action keypoints compared to AR-VRM's explicit keypoint supervision.",
            "performance_with_language_pretraining": "Reported in paper tables as a baseline (values vary by split); AR-VRM outperforms MT-R3M in reported CALVIN evaluations, though exact per-split numbers should be read from the paper's tables.",
            "performance_without_language_pretraining": "Not separately reported here for MT-R3M.",
            "sample_efficiency_comparison": "MT-R3M is shown to be inferior to AR-VRM in data-scarce regimes per paper claims and tables; explicit numbers are provided in the paper tables for direct comparison.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Not provided for MT-R3M in this paper.",
            "embedding_space_analysis": "Not provided here.",
            "action_grounding_evidence": "Paper claims MT-R3M-style implicit pretraining does not focus on explicit action keypoints and thus provides weaker grounding for robot manipulation compared to keypoint-based pretraining.",
            "hierarchical_features_evidence": "Not provided here.",
            "transfer_conditions": "Paper suggests implicit video pretraining is more sensitive to background differences and pixel noise; AR-VRM's explicit keypoint focus yields stronger transfer under limited robot data.",
            "novel_vs_familiar_objects": "Not specifically analyzed for MT-R3M within this paper.",
            "zero_shot_or_few_shot": "MT-R3M's few-shot performance is reported in baseline comparisons (worse than AR-VRM in 10% data regime per paper tables).",
            "layer_analysis": "Not provided for MT-R3M in this work.",
            "negative_transfer_evidence": "Paper argues implicit pixel-level pretraining can introduce irrelevant cues harming generalization, but no quantified negative transfer numbers for MT-R3M are supplied here.",
            "comparison_to_vision_only": "Paper contrasts MT-R3M (implicit video pretraining) and other web-data vision-language pretraining approaches, arguing AR-VRM's explicit keypoint pretraining yields better downstream manipulation performance.",
            "uuid": "e1922.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Robotics transformer for real-world control at scale",
            "rating": 2
        },
        {
            "paper_title": "Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks",
            "rating": 2
        },
        {
            "paper_title": "R3m: A universal visual representation for robot manipulation",
            "rating": 2
        },
        {
            "paper_title": "Physically grounded vision-language models for robotic manipulation",
            "rating": 2
        },
        {
            "paper_title": "Ego4D: Around the world in 3,000 hours of egocentric video",
            "rating": 1
        }
    ],
    "cost": 0.0236875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning
11 Aug 2025</p>
<p>Dejie Yang 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>Zijing Zhao zijingzhao@stu.pku.edu.cn 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>Yang Liu yangliu@pku.edu.cn 
Wangxuan Institute of Computer Technology
Peking University</p>
<p>State Key Laboratory of General Artificial Intelligence
Peking University</p>
<p>Keypoint Head 
AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning
11 Aug 2025CB45B77BE48968C6714C39A7DC55E846arXiv:2508.07626v1[cs.CV]
Visual Robot Manipulation (VRM) aims to enable a robot to follow natural language instructions based on robot states and visual observations, and therefore requires costly multimodal data.To compensate for the deficiency of robot data, existing approaches have employed vision-language pretraining with large-scale data.However, they either utilize web data that differs from robotic tasks, or train the model in an implicit way (e.g., predicting future frames at the pixel level), thus showing limited generalization ability under insufficient robot data.In this paper, we propose to learn from large-scale human action video datasets in an explicit way (i.e., imitating human actions from hand keypoints), introducing Visual Robot Manipulation with Analogical Reasoning (AR-VRM).To acquire action knowledge explicitly from human action videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme, enabling the VLM to learn human action knowledge and directly predict human hand keypoints.During fine-tuning on robot data, to facilitate the robotic arm in imitating the action patterns of human motions, we first retrieve human action videos that perform similar manipulation tasks and have similar historical observations , and then learn the Analogical Reasoning (AR) map between human hand keypoints and robot components.Taking advantage of focusing on action keypoints instead of irrelevant visual cues, our method achieves leading performance on the CALVIN benchmark and real-world experiments.In few-shot scenarios, our AR-VRM outperforms previous methods by large margins , underscoring the effectiveness of explicitly imitating human actions under data scarcity.Code available at https://github.com/idejie/ar.</p>
<p>Introduction</p>
<p>Visual robot manipulation (VRM) is an essential task in the robotics field [3,5,11].Based on current state and vi-Figure 1. Demonstration of the differences between our framework and previous methods: we propose to learn from human actions explicitly by hand keypoints with analogical reasoning.sual inputs, the robot is required to execute some actions according to human natural language instructions, including tasks such as object grasping, placement, and assembly [1,2,23,31].Training such task requires multi-modal data, including paired images, natural language instructions, and robot action states under specific scenarios where robots work, which is costly and requires time-consuming human demonstrations to manipulate robots.Therefore, the performance of VRM is often limited by the scarcity of robot trajectories and annotations for training [1,31].</p>
<p>To compensate for the deficiency of robot data, existing approaches utilize large-scale vision-language data pretraining and fine-tune the model under robot scenarios.But, these pretraining data do not directly reflect object manipulation tasks, such as visual-question-and-answer datasets about animals and diets, and it is hard to provide relevant knowledge guidance for robot manipulation tasks.Some studies [20,31] have utilized human action video datasets that are more similar to robotic manipulation tasks for pretraining, but they train the model in implicit ways, either by contrastive learning in feature space or by predicting future frames with pixel-level generative models.Though acquiring human action knowledge, these approaches inevitably introduce irrelevant background information or pixel-level noise, which limits their performance on data-scarce VRM tasks.</p>
<p>In this paper, we propose to imitate human motions explicitly from a large-scale human action video dataset, training the model with human hand keypoints to learn from the motion itself, ignoring the irrelevant visual information.To achieve this goal, two main challenges exist: (1) How to extract human action knowledge in terms of hand keypoints from large-scale human video datasets?(2) How to train the robot, which differs from the human arm, to imitate human actions, i.e., building the correlation between the robotic components and human keypoints for the manipulation task?</p>
<p>To address these challenges in learning from human actions, we propose Visual Robot Manipulation with Analogical Reasoning (AR-VRM).Human hand keypoints and robotic actions share underlying similarities, especially when it comes to manipulating objects.Specifically, to extract action knowledge explicitly from a large-scale human video dataset, we propose a keypoint Visual-Language Model (VLM) pretraining scheme.We use a large-scale egocentric human action video dataset Ego4D [7] with common operation videos by human hands, which have similar manipulation tasks and environment views to robotic applications.We detect human hand keypoints in instruction videos, and pretrain a VLM to predict the keypoints of future actions based on the current visual inputs and language instructions.During the fine-tuning stage, to provide demonstrations under manipulation tasks, we first retrieve relevant human action videos that have similar history observations to the robot situation.Moreover, to bridge the gap between robotic arm and human, we propose an Analogical Reasoning (AR) map between robotic arm components and human hand keypoints to learn geometric and functional correlation .Learning this correspondence helps to guide the robot to imitate human actions explicitly, e.g., the process of approaching the object and the grasping operation, to achieve manipulation tasks.</p>
<p>We perform extensive experiments on the challenging CALVIN benchmark.Our AR-VRM achieves state-of-theart performance under every experimental setting.Specifically, under 10% robot training data as well as unseen scenarios, our method outperforms previous approaches by large margins, improving success rate from 40.0% to 45.6% under 10% training data, and from 61.2% to 65.9% under unseen scenes.These results demonstrate that our idea of explicitly imitating human actions by keypoints has stronger generalization ability under limited robot data and under new scenarios.In addition, our model also outperforms previous SOTA methods on real robot experiments.</p>
<p>We conclude the contributions of our paper as follows: (1) We propose AR-VRM, the first visual robot manipulation approach that explicitly imitates human actions in terms of hand keypoints from large-scale human action video datasets.</p>
<p>(2) We propose the keypoint vision language model pretraining scheme to extract action knowledge from human videos, and the analogical reasoning module to align robotic arms with human hand keypoints.(3) Our method achieves state-of-the-art performance on CALVIN benchmark and real-world robot experiments, especially under limited robot training data, demonstrating the effectiveness and generalization ability.</p>
<p>Related Work</p>
<p>Visual Robot Manipulation</p>
<p>The visual robot manipulation task aims to train the robot to follow natural language instructions.It is a flexible and intuitive way for non-expert humans to instruct the robot to execute tasks such as object grasping, placement, and assembly [3,5,11].Some pioneer approaches delved into diverse visual encoder pre-training methods, aiming to learn useful visual representations by masked image modeling [23,24] and contrastive learning [10,20], and reward signals for reinforcement learning [4].In recent years, some studies introduced large language models as the action planner [6,28].ATM [30] proposes to imitate human actions for robot manipulation tasks, but requires paired human-robot action data under specific operation scenarios, limiting its generalization potential.These methods all have limited performance due to the insufficient multimodal data, which is difficult to obtain and costly to collect.In this paper, we employ large-scale human action video data for pretraining to compensate for the insufficiency of robot data.</p>
<p>Vision Language Pretraining for Robotics</p>
<p>Vision-Language Models (VLMs) are pretrained on largescale datasets with texts and images or videos, thus showing strong generalization ability on multiple downstream tasks, like video understanding [29,34,35] , interaction grounding [12,13] and generation [14,32], and 3D scene understanding [18,33].In the robotics field, some approaches introduce large-scale vision language pretraining to enhance robot intelligence.[2] employs Internet-scale web data with images and texts to pretrain the model for basic visual-language understanding, and fine-tune the model on robot data.But the web data, for example, vision question answering data, differs from robot manipulation tasks, leading to limited performance gains.[20,31] utilize human action video dataset, i.e., Ego4D [7] which is closely related to manipulation task.However, they train the robotic model in an implicit way, either by representation contrastive learning or by predicting pixel-level future frames, introducing irrelevant background information, and are unable to focus on the action itself.In this paper, we propose to explicitly learn from human actions in terms of hand keypoints, enhancing the generalization ability under insufficient robot data.</p>
<p>Method</p>
<p>Problem Formulation and Method Overview</p>
<p>We formulate the Visual Robot Manipulation (VRM) task as follows: At timestep t, a robot model R maps a language instruction l and a sequence of history visual observations o 1:t and robot states s 1:t from the starting timestep to current timestep t to a robot action a t :
R(l, o 1:t , s 1:t ) → a t(1)
where o t ∈ R H×W ×3 is the input image and s t = {s e t , s g t } is the corresponding robot state with the 6D pose of the robot end-effector s e t ∈ R 6 and a binary state of the gripper
s g t ∈ {0, 1} at timestep t, t ∈ [1, T ]. Action a t = ∆(s t ) = s t+1 − s t is the variation of state parameters. The complete robot dataset D R = {τ R i } |D R |
i=1 contains paired language instructions, visual inputs, and robot states:
τ R = {l, o 1 , s 1 , o 2 , s 2 , ..., o T , s T }(2)
These paired data with ground truth robot actions are difficult to obtain and costly to collect, thus have limited scale.</p>
<p>In this paper, we propose to employ vision language pretraining using large-scale human action videos and fine-tune the model on robot data, learning from explicit human action knowledge in terms of hand keypoints.As overviewed in Figure 2, our framework AR-VRM contains a keypoint Vision-Language Model (VLM) pretraining scheme, and an Analogical Reasoning (AR) module during fine-tuning.</p>
<p>In the pretraining stage, we first extract human hand keypoints in the large-scale human action videos, formulating sequences of human action data, and then introduce a keypoint head for the VLM to directly predict future human body keypoints, acquiring human action knowledge and focusing on the explicit action keypoints.In the fine-tuning stage, given the limited robot data, we first retrieve human actions that have a similar manipulation task as well as history observations from the human instructional video database, letting the pretrained keypoint VLM predict future actions, and introduce analogical reasoning to map the correspondence between the human hand keypoints and robot components.In this way, the model learns from explicit human actions and can generalize well under insufficient robot data.</p>
<p>Keypoint Vision-Language Model Pretraining</p>
<p>We employ a large-scale human egocentric video dataset Ego4D [7].The video samples in Ego4D are human operations of massive-scale manipulation tasks, with similar environments and views to robotic applications.In the human action video dataset
D H = {τ H i } |D H |
i=1 , a video of human hands accomplishing a task is provided:
τ H = {l, o 1 , o 2 , ..., o T } (3)
where l is the language description of the task, and o t is the captured image at timestep t.Unlike previous methods that learn implicitly from visual information, which inevitably contains irrelevant background information, we propose to explicitly learn from the human action itself with hand keypoints.We employ an offline hand pose estimation model InterHand [19] to extract 3D hand keypoints in each video frame is:
k t = InterHand(o t ), k t ∈ R K , Kτ H * = {l, o 1 , k 1 , o 2 , k 2 ..., o T , k T }(4)
The keypoints extracted by [19] are under image coordinate system with 3D coordinates, which could be imitated by robotic arms, for example, mimicking the process of approaching an object and performing operations.Given the data from three different modalities, we pretrain a keypoint Vision-Language Model (VLM), enabling the VLM to understand and directly predict hand keypoints in a human action sequence.For language instruction l, following [26,27,31], we use CLIP [22] text encoder to extract language embeddings, followed by a multi-layer perceptron (MLP) to project the embedding to dimension d:
z l = MLP(CLIP text (l)), z l ∈ R d(5)
For each visual input o, we employ a vision transformer (ViT) pretrained with MAE [8] as the image encoder.We take the output CLS token z CLS as the global representation and output patch tokens z p 1:V as local representations, resampled by a preceiver resampler (PR) [9] to reduce token numbers to M .All the outputs are projected by MLP to dimension d:
z CLS , z p 1:V = ViT(o t )(6)z CLS ot = MLP(z CLS ), z CLS ot ∈ R d (7) z p 1:M ot = MLP(PR(z p 1:V )), z p ot ∈ R d(8)
For hand keypoints k, we employ HandFormer [25] as the keypoint encoder, and also project the embedding to dimension d:
z kt = MLP(HandFormer(k t )), z kt ∈ R d(9)
With the aligned dimension of tokens from the three modalities z l , z o = (z CLS o , z p 1:M o ), z k , we concatenate them into a sequence of tokens and feed them to Transformer layers and perform next token prediction for pretraining.Specifically, at timestep t, the model with self-attention layers h Atten predict the keypoint token ẑk based on the previous token sequences, and a keypoint prediction head (MLP) project the keypoint token back to keypoint vectors, and finally compute the mean squre error (MSE) with the ground truth keypoint.The pretraining loss can be formulated as:
ẑ kt = h Atten (z l , z o1 , z k1 , z o2 , z k2 ..., z ot−1 , z kt−1 ) (10) L pretrain = T t=1 L M SE (KeypointHead( ẑ kt ), k t ) (11)
During training we fix the parameters of the CLIP text encoder and pretrained image encoder.With the keypoint VLM pretraining, we acquire human action knowledge from a large-scale human action video dataset and enable the VLM to explicitly predict action keypoints.</p>
<p>Robot Fine-tuning with Analogical Reasoning</p>
<p>With the keypoint VLM that understands action sequences by large-scale human data pretraining, we could then finetune the model on the robot data to accomplish the visual robot manipulation task.In the robot dataset D R , we extract the language and vision embeddings and produce tokens z l , z o same as the pretraining stage.For robot state s = {s e , s g }, we employ two MLPs E e , E g to encode s e and s g respectively, and then project the encoded vectors into dimension d by another MLP:
z s = MLP(E e (s e ), E g (s g )), z s ∈ R d(12)
With the aligned dimension of tokens z l , z o , z s , we feed them to the pretrained VLM to predict the state token ẑs .</p>
<p>Introducing the robot state head (MLP), we project the state token back to robot states and compute the MSE loss with the ground truth states:
ẑ s T = h Atten (z l , z o1 , z s1 , z o2 , z s2 ..., z o T −1 , z s T −1 ) (13) L state = L M SE (StateHead( ẑ s T ), s T )(14)
In addition to utilizing the knowledge from pretrained weights of the VLM, we propose to explicitly learn from the human actions in terms of hand keypoints.Specifically, given the robot data sample τ R , we first retrieve the human action videos from the large-scale database based on the language introduction and the visual frame features, with the similarity:
sim(τ R , τ H ) = cos(z R l , z H l ) + t cos(z R ot , z H ot )(15)
where cos(•, •) is the cosine similarity.In this way, we could retrieve human action videos that have both similar manipulation tasks and similar visual observations with the robotic data sample.We retrieve the top J similar samples {τ H j } J j=1 .After the forward pass of the human action samples τ H j and robot data sample τ R , the features of the last layer of keypoint head and robot state head f kj ∈ R K×d , f s ∈ R S×d serve as the representation of each keypoint node and robot state node, where K is the number of keypoints of human hands and S is the number of robot arms components.We introduce a learnable analogical map matrix m ∈ R S×K to represent the mapping between the human hand keypoints and robot arm components, where each element represents the influence of the corresponding hand keypoint on the robot arm component.We compute the imitated robot state features f * sj as follow:
f * sj = (1 − α) • m • f kj + α • f s(16)
where α ∈ R is also a learnable parameter to weight the overall influence of keypoint features.We then employ a new linear layer for new robot states, and compute MSE as the analogical reasoning loss:
L AR = J j=1 L M SE (Linear(f * s j,T ), s T )(17)
The overall fine-tuning loss is the weighted sum of the robot state loss and the analogical reasoning loss:
L f inetune = L state + β • L AR (18)
where β is the hyper-parameter.Discussion on the fine-tuning method design: Note that during fine-tuning, we fix the parameters of the keypoint encoder and keypoint head, and fine-tune the Transformer layers of the VLM.Fixing the parameters of the keypoint encoder and keypoint head helps maintain the ability to encode and predict keypoint parameters that have already been accomplished in the pretraining stage.By fine-tuning the Transformers of the VLM with human action video samples, we not only allow the keypoint features to guide the training of the robot state head, but also act as a data replay operation to prevent the VLM from over-fitting to the limited robotic data and forgetting the vision-language understanding and action prediction knowledge gained from pretraining.Detailed experimental discussions will be in Section 4.</p>
<p>Experiment</p>
<p>Dataset and Benchmark</p>
<p>We conduct experiment on the challenging long-horizon tasks benchmark CALVIN [17].CALVIN is a simulated language-conditioned robot manipulation benchmark that combines natural language conditioning, multi-modal highdimensional inputs, 7-DOF continuous control, and longhorizon robotic object manipulation in both seen and unseen environments.It contains 34 subtasks and 5 horizon evaluation sequences, providing a range of sensors commonly utilized for visuomotor control, and allows testing zero-shot generalization by leveraging a range of 4 manipulation environments and unseen language instructions.Robot manipulation dataset requires costly paired multimodal data, and thus has limited scale.We employ a largescale human action video dataset Ego4D [7] for pretraining.Ego4D contains massive-scale human-object interaction videos of 3,500 hours, each video containing a natural language annotation describing the human action.Following [31], we use a total of 800,000 video clips containing 8M frames for pretraining and as the human action video database for retrieval during fine-tuning.For real-world experiments, we use a plate with three objects: an orange, an apple, and a green jujube.We collected 1,200 moving-object demonstrations and 1,400 drawer-opening/closing trajectories.</p>
<p>Implementation Details</p>
<p>For network architecture, we use a pretrained CLIP [22] text encoder and a pretrained ViT-Base image encoder [8] to extract language instruction tokens and visual input tokens.For local image patch tokens, we use [9] to reduce the token numbers.For human keypoints, we use the pretrained HandFormer [25] as the encoder.For robot states, we follow [31] to encode the robot arm and grasper parameters by MLPs.We use a 12-layer transformer with causal attention mechanism and checkpoints loaded from [21].During the pretraining stage, we sample uniformly spaced frames with 3 fps from video clips in the Ego4D dataset.And we set the learning rate to 1e − 4 with AdamW, training the model with a batch size of 512 on NVIDIA A800 GPUs with 100 epochs.During the fine-tuning phase, freezing multiple encoders that process human data and the keypoint prediction head serves to stabilize the model's existing performance while focusing updates on robotic manipulation prediction.And we adjust the learning rate to 1e − 5 and execute 50 epochs.</p>
<p>Results Comparing with Other Methods</p>
<p>Experiment setup: We evaluate our method on the four different settings of the CALVIN benchmark, including full dataset multitask learning, unseen scene generalization, dataefficient few-shot learning, and unseen language generalization.The training data includes "A", "B", "C" and "D" scenes with 34 specific instruction languages.We use the success rate of sequentially completing 1, 2, 3, 4, and 5 tasks, and the average length of successfully completed tasks as the evaluation metrics following previous methods.</p>
<p>Full dataset multitask learning: As shown in Table 1 "Experiment ABCD → D" following [31], we train the model on under 4 different scenes and evaluate it on a certain scene.Our method outperforms all the baseline methods, raising the average success rate improvement by +1.2%, demonstrating the effectiveness of our design of introducing explicit human hand keypoints for robot manipulation.</p>
<p>Unseen scene generalization: As shown in ("A", "B", and "C") and evaluate it on scene "D".Thanks to the replaying design of human videos during fine-tuning, our method further improves the performance by a large margin (Avg.Rate from 61.2% to 65.9%) compared with previous SOTA, indicating that our keypoint VLM pretraining and analogical reasoning have strong generalization ability across different scenes.</p>
<p>Approach</p>
<p>Transportation Articulated Seen Unseen Manipulation Obj.Ins.Cate.</p>
<p>RT-1 [1] 0.27 0.13 0.00 0.35 MT-R3M [25] 0.15 0.13 0.10 0.30 GR-1 [31] 0.79 0.73 0.30 0.75 AR-VRM(Ours) 0.95 0.91 0.53 0.82 Real-robot experiments.(1)Object Transportation: Seen Objects.The robot transported the three trained objects (orange, apple, jujube) in two disturbed scenes: with distractors (tomato, corn, yellow peach), and with an altered background (wooden board, bowl).Unseen Instances.Evaluated generalization to new instances of the same object categories (different orange, apple, jujube).Unseen Categories.Tested generalization to untrained categories (tomato, yellow peach).Quantitative results (Table 2) show AR-VRM outperformed baselines (RT-1, MT-R3M, GR-1).Baselines often failed due to incorrect object selection/placement or collisions (e.g., with plate/desk).AR-VRM maintained high success rates for seen objects and showed minimal performance drop for unseen instances, highlighting strong in-category generalization.(2)Articulated Manipulation: It surpassed baselines significantly but exhibited two failure modes: in-complete drawer closure during closing tasks and missed handle engagement during opening tasks.Despite these, AR-VRM demonstrated superior robustness in articulated manipulation.</p>
<p>Ablation Studies and Qualitative Analysis</p>
<p>Effectiveness of our proposed modules: In Table 3, we show the results of ablating each of our proposed modules.Line 1 represents training the VLM directly by robot data without any pretraining and guidance from human action videos.Due to the lack of large-scale data, it has limited performance.In line 2, with the help of human action video pretraining with hand keypoints, the VLM gains visionlanguage understanding knowledge, and the performance shows a significant gain.Line 3 stands for the introduction of human action videos during fine-tuning by retrieval, but only training the separate keypoint head instead of the analogical mapping to guide the robot state prediction.This method helps to maintain the knowledge learned by pretraining and prevents the over-fitting to small-scale robot data, thus showing performance gain compared with line 3, but still has room for improvement.Line 4 stands for our complete design of keypoint VLM pretraining with large-scale data and fine-tuning with retrieved human action videos and analogical reasoning.It achieves the best result, demonstrating the effectiveness of each of our proposed modules.</p>
<p>Analysis of design choices of robot fine-tuning with analogical reasoning: As noted in Section 3, we freeze the keypoint encoder/head and fine-tune the VLM during analogical reasoning adaptation.Results in Table 4 show: Freezing the VLM (lines 1 vs 4, 2 vs 3) causes significant performance drops, confirming that VLM fine-tuning is critical for transferring human knowledge to robots.Freezing keypoints (lines 1 vs 2, 3 vs 4) boosts performance, likely     because stabilized keypoint features aid convergence of the analogical map, improving human-robot component alignment.</p>
<p>Date efficient few-shot learning: As shown in Table 5, we train the model with only 10% of the already small dataset to train our model.Our method outperforms all the baseline methods, raising the success rate of completing 5 tasks to 45.6% and the average length of completed tasks to 2.28.The results show that our analogical reasoning can provide a map from the diversity and complexity of actions in human keypoints to insufficient robot action, which can enable the robot to quickly learn how to cope with different tasks.</p>
<p>Unseen instruction language generalization: As shown in Table 6, we evaluate the model on the unseen instruction languages during training.Our method consistently achieves state-of-the-art, indicating that the relevant human action demonstration significantly help the robot generalize to multiple new instructional tasks.</p>
<p>Visualization of our prediction results: In Figure 3, we provide an example of robot manipulation task of "grasp the blue block in drawer".Our method retrieves relevant human action videos such as "pick up a tool/receipt/cloth/knife" from the database.By imitating the human action of picking up objects from the drawer, which contains process actions   of approaching the drawer and grasping the object, the robot could successfully follow the language instruction to pick up the blue block in the drawer.</p>
<p>Visualization of analogical map: In Figure 4, we show an analogical map partial example from hand keypoints to a robotic arm, and the map is normalized in a column-wise manner and shows the correlation between the estimated hand keypoints and visible robotic arm components.The coding of keypoint nodes and robotic arm component nodes is illustrated in (a) and (b), and in (c), the map elements that are highlighted by red circles indicate a strong relationship between the two nodes.The grasper of the robot (code 0) is strongly related to the fingertips of human hands (code 4, 8, 12, 20) which operates the grasping in manipulation tasks, and the root segment of the robotic arm (code 3) relates to the human palm (code 0, 1, 5, 9, 17) deciding the direction of the movement.This visualization indicates that our method learns a reasonable relationship by mapping the human action to guide the robotic arm.</p>
<p>Conclusion</p>
<p>To explore key information in human interaction processes from videos to guide robot manipulation, we introduce a Visual Robot Manipulation with Analogical Reasoning (AR-VRM), leveraging large-scale human action video datasets to explicitly learn action knowledge through hand keypoints.We use a keypoint pretraining scheme to enhance generalization and performance.With Analogical Reasoning to map human hand keypoints to robot components, our method achieves SOTA results on the CALVIN and real robot experiments, especially in few-shot data scenarios, demonstrating our effectiveness and generalization.</p>
<p>LanguageFigure 3 .
3
Figure 3. Example of robot manipulation with our retrieved human action videos and action prediction result.</p>
<p>Figure 4 .
4
Figure 4. Visualization of analogical mapping.(a) Human hand keypoints code 0-20.(b) Robotic visible components code 0-3.(c) The learned analogical map.The highlighted map elements by red circles indicate a strong relationship between the two nodes.</p>
<p>Keypoint Vision Language Model Pretrain Analogical Reasoning for Robot Fine-Tuning Figure
Hand Pose EstimationHuman Hand DatabaseQueryRobotic Data Language Instruction Robotic StateFrames Instruction KeypointsImageTextStateEncoderEncoderEncoderImage EncoderText EncoderKeypoint EncoderMultiple Encoders. #. ". !Next TimestampAnalogicalStateStateReasoningHead! !"#$"%&amp;'! (&amp;'#$)'#
2. AR-VRM: Visual Robot Manipulation with Analogical Reasoning.</p>
<p>is the number of keypoints in human hands.The preprocessed human action video dataset D H * = {τ H * i }
|D H  *  |i=1</p>
<p>Table 1 "
1
Experiment ABC → D", we train the model on under 3 scenes
MethodExperimentSuccess rate of tasks completed in a row Avg.Len. 1 2 3 4 5Avg. RateMCIL[15]0.373 0.027 0.002 0.000 0.0000.408.0%RT-1[1]0.844 0.617 0.438 0.323 0.2272.4549.0%HULC[16]0.889 0.733 0.587 0.475 0.3833.0761.3%MT-R3M[25]ABCD→D0.752 0.527 0.375 0.258 0.1632.0741.5%GR-1[31]0.949 0.896 0.844 0.789 0.7314.2184.2%Ours0.951 0.915 0.855 0.800 0.7514.2785.4% (+1.2%)MCIL[15]0.304 0.013 0.002 0.000 0.0000.316.4%RT-1[15]0.533 0.222 0.094 0.038 0.0130.9018.0%HULC[16]0.418 0.165 0.057 0.019 0.0110.6713.4%MT-R3M[25]ABC→D0.529 0.234 0.105 0.043 0.0180.9318.6%GR-1[31]0.854 0.712 0.596 0.497 0.4013.0661.2%Ours0.901 0.759 0.642 0.531 0.4613.2965.9% (+4.7%)</p>
<p>Table 1 .
1
Performance comparisons on CALVIN.Bold represents the best results and underline represents the second-best.</p>
<p>Table 2 .
2
Real robot experiment success rates</p>
<p>Table 3 .
3
Ablation studies on the effectiveness of our proposed modules."Pretrain", "Retrieval" and "AR" denotes keypoint VLM pretraining with large-scale human data, retrieving human action videos for fine-tuning and analogical reasoning, respectively.
Keypoint params VLM paramsSuccess rate of tasks completed in a row Avg.Len. Avg. Rate 1 2 3 4 5frozenfrozen0.942 0.845 0.803 0.741 0.5983.9378.6%trainablefrozen0.931 0.832 0.788 0.726 0.5763.8577.1%trainabletrainable0.939 0.847 0.821 0.772 0.7304.1182.2%frozentrainable0.951 0.915 0.855 0.800 0.7514.2785.4%</p>
<p>Table 4 .
4
Ablation study on the design choices of robot fine-tuning with analogical reasoning.
Method DataSuccess rate of tasks completed in a row Avg.Len. Avg. Rate 1 2 3 4 5GR-1 Ours100%0.949 0.896 0.844 0.789 0.731 0.951 0.915 0.855 0.800 0.7514.21 4.2784.2% 85.4%GR-1 Ours10%0.778 0.533 0.332 0.218 0.139 0.809 0.589 0.381 0.287 0.2132.00 2.2840.0% 45.6%</p>
<p>Table 5 .
5
Data efficient few-shot learning with only 10% train data on ABCD→D.
MethodLangSuccess rate of tasks completed in a row Avg.Len. Avg. Rate 1 2 3 4 5GR-1 OursSeen0.949 0.896 0.844 0.789 0.731 0.951 0.915 0.855 0.800 0.7514.21 4.2784.2% 85.4%GR-1 OursUnseen0.764 0.555 0.381 0.270 0.196 0.802 0.575 0.430 0.313 0.2092.17 2.3343.3% 46.6%</p>
<p>Table 6 .
6
Unseen instruction language generalization on ABCD→D.</p>
<p>Acknowledgements.This work was supported by the grants from the National Natural Science Foundation of China 62372014, Beijing Nova Program and Beijing Natural Science Foundation 4252040.
Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.06817202216arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.1581820231arXiv preprint</p>
<p>A comprehensive study of 3-d vision-based robot manipulation. Yang Cong, Ronghan Chen, Bingtao Ma, Hongsen Liu, Dongdong Hou, Chenguang Yang, IEEE Transactions on Cybernetics. 5332021</p>
<p>Danijar Hafner, and Pieter Abbeel. Video prediction models as rewards for reinforcement learning. Alejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay Jain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee, Advances in Neural Information Processing Systems. 202436</p>
<p>Survey of imitation learning for robotic manipulation. Bin Fang, Shidong Jia, Di Guo, Muhua Xu, Shuhuan Wen, Fuchun Sun, International Journal of Intelligent Robotics and Applications. 322019</p>
<p>Physically grounded vision-language models for robotic manipulation. Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh, 2024 IEEE International Conference on Robotics and Automation (ICRA). 2024</p>
<p>Ego4d: Around the world in 3,000 hours of egocentric video. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202235</p>
<p>Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202245</p>
<p>Perceiver: General perception with iterative attention. Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, Joao Carreira, International conference on machine learning. PMLR202145</p>
<p>Exploring visual pre-training for robot manipulation: Datasets, models and methods. Ya Jing, Xuelin Zhu, Xingbin Liu, Qie Sima, Taozheng Yang, Yunhai Feng, Tao Kong, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2023</p>
<p>A review of robot learning for manipulation: Challenges, representations, and algorithms. Oliver Kroemer, Scott Niekum, George Konidaris, Journal of machine learning research. 22302021</p>
<p>Exploring the potential of large foundation models for open-vocabulary hoi detection. Ting Lei, Shaofeng Yin, Yang Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Exploring conditional multi-modal prompts for zero-shot hoi detection. Ting Lei, Shaofeng Yin, Yuxin Peng, Yang Liu, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2024</p>
<p>Core4d: A 4d human-object-human interaction dataset for collaborative object rearrangement. Yun Liu, Chengwen Zhang, Ruofan Xing, Bingda Tang, Bowen Yang, Li Yi, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Language conditioned imitation learning over unstructured data. Robotics: Science and Systems. Corey Lynch, Pierre Sermanet, 2021</p>
<p>What matters in language conditioned robotic imitation learning over unstructured data. Oier Mees, Lukas Hermann, Wolfram Burgard, IEEE Robotics and Automation Letters (RA-L). 742022</p>
<p>Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. Oier Mees, Lukas Hermann, Erick Rosete-Beas, Wolfram Burgard, IEEE Robotics and Automation Letters. 732022</p>
<p>Bridging the gap between 2d and 3d visual question answering: A fusion approach for 3d vqa. Wentao Mo, Yang Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Interhand2. 6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image. Gyeongsik Moon, -I Shoou, He Yu, Takaaki Wen, Kyoung Shiratori, Lee Mu, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part XX 16</p>
<p>R3m: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, arXiv:2203.12601202223arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR202145</p>
<p>Real-world robot learning with masked visual pre-training. Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, Trevor Darrell, Conference on Robot Learning. PMLR20231</p>
<p>Multi-view masked world models for visual robotic manipulation. Younggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, Pieter Abbeel, International Conference on Machine Learning. 2023</p>
<p>On the utility of 3d hand poses for action recognition. Salman Md, Dibyadip Shamil, Fadime Chatterjee, Shugao Sener, Angela Ma, Yao, European Conference on Computer Vision. Springer202546</p>
<p>Cliport: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, PMLR, 2022. 4Conference on robot learning. </p>
<p>Perceiveractor: A multi-task transformer for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, PMLR, 2023. 4Conference on Robot Learning. </p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>
<p>Video understanding with large language models: A survey. Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Technology. 22025</p>
<p>Any-point trajectory modeling for policy learning. Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, Pieter Abbeel, arXiv:2401.000252023arXiv preprint</p>
<p>Unleashing large-scale video generative pretraining for visual robot manipulation. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, Tao Kong, arXiv:2312.131392023. 1, 2, 3, 4, 5, 6arXiv preprint</p>
<p>Semantic-aware human object interaction image generation. Zhu Xu, Qingchao Chen, Yuxin Peng, Yang Liu, Forty-first International Conference on Machine Learning. 2024</p>
<p>3d vision and language pretraining with large-scale synthetic data. Dejie Yang, Zhu Xu, Wentao Mo, Qingchao Chen, Siyuan Huang, Yang Liu, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence2024</p>
<p>Planllm: Video procedure planning with refinable large language models. Dejie Yang, Zijing Zhao, Yang Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2025</p>
<p>Phrase-level temporal relationship mining for temporal sentence localization. Minghang Zheng, Sizhe Li, Qingchao Chen, Yuxin Pengand, Yang Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023</p>            </div>
        </div>

    </div>
</body>
</html>
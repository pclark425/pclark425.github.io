<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9049 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9049</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9049</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-269928651</p>
                <p><strong>Paper Title:</strong> Testing theory of mind in large language models and humans</p>
                <p><strong>Paper Abstract:</strong> At the core of what defines us as humans is the concept of theory of mind: the ability to track other people’s mental states. The recent development of large language models (LLMs) such as ChatGPT has led to intense debate about the possibility that these models exhibit behaviour that is indistinguishable from human behaviour in theory of mind tasks. Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities, from understanding false beliefs to interpreting indirect requests and recognizing irony and faux pas. We tested two families of LLMs (GPT and LLaMA2) repeatedly against these measures and compared their performance with those from a sample of 1,907 human participants. Across the battery of theory of mind tests, we found that GPT-4 models performed at, or even sometimes above, human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas. Faux pas, however, was the only test where LLaMA2 outperformed humans. Follow-up manipulations of the belief likelihood revealed that the superiority of LLaMA2 was illusory, possibly reflecting a bias towards attributing ignorance. By contrast, the poor performance of GPT originated from a hyperconservative approach towards committing to conclusions rather than from a genuine failure of inference. These findings not only demonstrate that LLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans but also highlight the importance of systematic testing to ensure a non-superficial comparison between human and artificial intelligences.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9049.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9049.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - False belief</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on False Belief test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was evaluated on classic false-belief vignettes adapted for a species-fair, text-based comparison and achieved ceiling/near-perfect accuracy across original and novel items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source generative pre-trained transformer from OpenAI, chat-enabled; enhanced reasoning and comprehension relative to previous GPT versions (used via ChatGPT web interface).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>False belief task (classic false/true belief scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Language-based false-belief stories assessing ability to infer that an agent holds a belief that differs from reality (belief attribution).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Ceiling / near-perfect accuracy across original and novel items (all LLM sessions correctly reported the agent's false belief).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Near-ceiling (only a few human errors; e.g., 5 participants out of 51 made one error on novel items).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM matches or equals human ceiling performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Each item delivered in separate chat sessions; 15 independent sessions; novel items generated to avoid training-set memorization; scoring followed human test protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>High accuracy can reflect heuristic/scripted solutions rather than human-like belief tracking; prior work and control perturbations show GPTs can fail under minor structural changes to the task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9049.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 - False belief</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 evaluated on False Belief test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 (ChatGPT predecessor) was tested on false-belief vignettes and performed at ceiling/near-perfect levels on the standard tasks and novel items in this battery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Earlier ChatGPT model (default at time of testing), closed-source, chat-enabled via web interface.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>False belief task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Language-based false-belief vignettes assessing belief attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Ceiling / near-perfect accuracy on standard and novel false-belief items.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Near-ceiling (small number of human errors reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM matches human ceiling performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>15 independent chat sessions per test; items alternated between false and true belief; novel items generated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Prior literature and control perturbations show GPT-3.5 can fail on small alterations to the task; ceiling performance on standard items may not reflect robust belief-tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9049.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-70B - False belief</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-Chat (70B) evaluated on False Belief test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The open-weight LLaMA2-70B chat model achieved near-perfect performance on the false-belief tasks in the battery, matching human ceiling performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-70B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight LLaMA2-Chat family (tested 70B, also 13B and 7B reported in supplement); responses collected with defined decoding parameters and 'You are a helpful AI assistant' prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>False belief task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Language-based false-belief vignettes assessing belief attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Near-perfect / ceiling performance on standard and novel false-belief items.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Near-ceiling (few human errors).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM matches human ceiling performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Responses gathered with Langchain conversation chain (for 70B within-session memory); temperature 0.7, max tokens 512, repetition penalty 1.1, TopP 0.9; 15 independent sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Same caveat: regular structure of task may permit non-mentalistic heuristics; smaller LLaMA2 variants produced many non-responses (data quality concerns).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9049.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - Irony</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on Irony comprehension test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 outperformed human participants on an irony comprehension test derived from an eye-tracking study, showing superior detection/interpretation of ironic utterances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-enabled GPT-4 used via ChatGPT web interface.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Irony comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Vignettes containing ironic or non-ironic utterances requiring inference of speaker's mocking/non-literal intent (pragmatic/mentalizing domain).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Statistically above human level (Z = 0.00, P = 0.040, r = 0.32; model reached high accuracy on irony detection).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Baseline human sample; GPT-4 performance significantly exceeded this baseline (statistics reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM outperforms human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>12 items adapted from prior study; novel items not applicable; outputs coded 1/0; occasional internally contradictory model replies were coded based on explanation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>GPT-4's superior performance may reflect pattern learning from training data; GPT-3.5 and LLaMA2 showed worse-than-human performance, showing model variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9049.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 - Irony</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 evaluated on Irony comprehension test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 performed below human baseline at recognizing irony in the adapted vignettes, though it performed perfectly at recognizing non-ironic control statements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT predecessor tested via web interface.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Irony comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Ironic vs non-ironic utterance comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Below human levels (Z = −0.17, P = 2.37 × 10^-6, r = 0.64); perfect at detecting non-ironic controls but made errors on irony detection.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans performed better on irony detection than GPT-3.5 in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM below human baseline for irony detection.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Items taken from prior eye-tracking study; responses coded for appreciation of irony.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Order effects observed (GPT-3.5 made more errors on earlier trials); some internal inconsistency in generated answers required careful coding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9049.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-70B - Irony</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-Chat (70B) evaluated on Irony comprehension test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA2-70B performed below human baseline on irony comprehension, showing poor discrimination between ironic and non-ironic statements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-70B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight LLaMA2-Chat (70B) with Langchain delivery and fixed generation parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Irony comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Detection of ironic versus non-ironic utterances.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Below human levels (Z = −0.42, P = 2.39 × 10^-7, r = 0.70); errors for both ironic and non-ironic controls.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans outperformed LLaMA2-70B on irony discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM below human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Automated delivery; responses sometimes non-codable were regenerated; coding focused on whether explanation showed appreciation of irony.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>LLaMA2-70B exhibited overall poor discrimination, suggesting model-specific weaknesses in pragmatic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9049.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - Faux pas (original)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on Faux Pas recognition (original test)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 identified victims' hurt but often refused to commit to speaker's lack of knowledge when asked directly, yielding below-human scores on the original yes/no belief question.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-enabled GPT-4 used via web interface with per-chat memory but no cross-chat memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Faux pas recognition (Baron-Cohen et al. test)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Vignettes where a speaker says something unintentionally offensive; requires representing speaker's ignorance and victim's hurt (pragmatic/mentalizing domain).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Below human levels on original framing (belief yes/no) (Z = −0.40, P = 5.42 × 10^-5, r = 0.55); often answered 'insufficient information' rather than 'no'.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans performed significantly better on the original belief question (majority answered correctly that speaker did not know).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM below human baseline on original faux pas question (due to conservative refusal to commit).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>10 original + 5 novel faux pas items; coding focused on final belief question (correct answer = 'no'); 15 independent sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Failure reflects hyperconservatism (mitigation/hesitancy) rather than inability to infer mental states; alternative framings and follow-up tests show GPT-4 can infer but refuses to commit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9049.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 - Faux pas (original)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 evaluated on Faux Pas recognition (original test)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 largely failed the original faux pas belief question, performing near floor except on one item, often refusing to commit or producing incorrect/noisy answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT predecessor with poorer calibration/mitigation compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Faux pas recognition (original)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Faux pas vignettes requiring belief attribution about speaker's knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Near-floor performance on original belief question (Z = −0.80, P = 5.95 × 10^-8, r = 0.72); most answers incorrect or noncommittal.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans performed substantially better.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM well below human baseline on original faux pas question.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same administration as GPT-4; occasional partial successes; coding focused on final belief question.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Later likelihood rephrasing improved GPT-3.5 performance; original failure may combine uncertainty avoidance and calibration issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9049.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-70B - Faux pas (original)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-Chat (70B) evaluated on Faux Pas recognition (original test)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA2-70B outperformed humans on the original faux pas test, reaching near-perfect scores (100% accuracy in all but one run), but follow-up tests suggested this superiority may be due to a biased tendency to attribute ignorance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-70B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight LLaMA2-Chat 70B with Langchain delivery parameters; other smaller LLaMA2 variants produced poorer data quality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Faux pas recognition (original)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Recognition of unintentional offensive remarks and the speaker's ignorance.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Outperformed humans (Z = 0.10, P = 0.002, r = 0.44); achieved 100% accuracy in all but one run on original test items.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans were less accurate than LLaMA2-70B on this original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM superficially outperforms human baseline on original faux pas items.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Automated delivery via Langchain; responses regenerations applied for non-codable responses; scoring focused on belief question.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Follow-up belief-likelihood variants showed LLaMA2-70B did not discriminate knowledge-implied vs neutral variants and always selected one explanation (bias towards 'didn't know'), suggesting the original superiority is illusory and due to response bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9049.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - Faux pas likelihood test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on Faux Pas likelihood-framed question</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When the faux pas belief question was reframed to ask which explanation was more likely, GPT-4 reliably identified that 'speaker did not know' was the likeliest explanation (perfect performance reported).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-enabled GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Faux pas likelihood test (likelihood framing of belief question)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Rephrasing of the belief question to ask whether 'knew' or 'didn't know' is more likely, focusing on graded likelihood judgment rather than binary commitment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Perfect performance (all responses indicated 'more likely that speaker did not know' without prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans typically endorsed 'didn't know' in faux pas variants but sometimes expressed uncertainty; GPT-4 matched expected likelihood judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM matches or exceeds human-level inference when asked about likelihood; shows ability to infer but avoids categorical commitment in original framing.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>15 sessions; follow-up prompts used rarely for prompting unclear reasoning; coding gave 2 points unprompted, 1 prompted correct, 0 incorrect after prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Demonstrates hyperconservatism in direct yes/no framing (models can compute but avoid categorical commitment), suggesting mitigation/hallucination-avoidance policies affect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9049.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 - Faux pas likelihood test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 evaluated on Faux Pas likelihood-framed question</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 improved on the likelihood-framed question compared with the original yes/no framing, often identifying 'didn't know' as more likely, though it sometimes required prompting (~3% items) and occasionally failed (~9% items).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT predecessor.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Faux pas likelihood test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Likelihood-framed belief question to reveal whether models commit to the likeliest explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Improved relative to original; required prompting on ~3% of items; occasional failures (~9% of items).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans endorsed 'didn't know' for faux pas but were more likely to report uncertainty in some neutral cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM approaches human-level inference under likelihood framing but is noisier than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Follow-up prompt delivered in rare cases; coding scheme allowed partial credit if correct after prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>GPT-3.5 never reported uncertainty in belief-likelihood variants (always committed to one explanation), differing from human pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9049.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-70B - Faux pas likelihood test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-Chat (70B) evaluated on Faux Pas likelihood-framed question</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA2-70B tended to endorse 'didn't know' in the likelihood-framed faux pas items but showed biased behavior (always selected one explanation and failed to express uncertainty), raising concerns about a response bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-70B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight LLaMA2-Chat 70B with specified decoding parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Faux pas likelihood test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Likelihood-framed belief judgment variants (faux pas vs neutral vs knowledge-implied).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>More likely to select 'didn't know' for faux pas than neutral (χ^2(1)=20.20, P=2.81×10^-5), but did not differentiate neutral vs knowledge-implied; never reported uncertainty (always chose one explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans discriminated across variants and reported uncertainty more in neutral conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM superficially aligns with humans on faux pas variant but shows biased behavior and fails to discriminate knowledge-implied cases.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Responses coded as -1/0/+1 for didn't know/unsure/knew; administered with independent sessions to avoid cross-variant contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Performance likely driven by bias toward attributing ignorance; the original perfect performance appears illusory when controlling for variant types.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e9049.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - Belief likelihood (variant discrimination)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on Belief Likelihood test (faux pas / neutral / knowledge-implied variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 discriminated between faux pas, neutral, and knowledge-implied variants similarly to humans: endorsing 'didn't know' for faux pas and 'knew' for knowledge-implied, and showing uncertainty primarily in neutral cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-enabled GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Belief likelihood test (faux pas / neutral / knowledge-implied variants)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Controlled variants manipulating likelihood that speaker knew vs didn't know; response coded -1/0/+1.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Discriminated across variants: more 'didn't know' responses in faux pas than neutral (χ^2(2)=109, P=1.54×10^-23) and more 'knew' in knowledge-implied than neutral (χ^2(2)=18.10, P=3.57×10^-4); reported uncertainty often in neutral condition.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans showed the expected discrimination (significant chi-square), with some uncertainty in neutral trials.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM matches human pattern of sensitivity to manipulated belief likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Six stories adapted into three variants; 15 LLM sessions per item; responses numerically coded and averaged to directional scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>GPT-4 still reports uncertainty in neutral condition and shows hyperconservatism in categorical yes/no framings; performance depends on framing of the question.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e9049.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 - Belief likelihood (variant discrimination)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 evaluated on Belief Likelihood test (faux pas / neutral / knowledge-implied)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 showed variant discrimination similar to humans (endorsing 'didn't know' for faux pas and 'knew' for knowledge-implied) but never reported uncertainty (always committed to one explanation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT predecessor.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Belief likelihood test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Controlled manipulation of belief-likelihood across three story variants.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>More 'didn't know' for faux pas vs neutral (χ^2(1)=8.44, P=0.007) and more 'knew' for knowledge-implied vs neutral (χ^2(1)=21.50, P=1.82×10^-5); never reported 'unsure'.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans reported some uncertainty in neutral condition, unlike GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM shows correct likelihood discrimination but differs qualitatively from humans by always committing.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Responses coded -1/0/+1; models run with independent sessions per item.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Lack of expressed uncertainty suggests a qualitative difference from human responses despite similar discrimination of variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e9049.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-70B - Belief likelihood (variant discrimination)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-Chat (70B) evaluated on Belief Likelihood test (faux pas / neutral / knowledge-implied)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA2-70B demonstrated a bias toward endorsing 'didn't know' for faux pas and failed to differentiate neutral versus knowledge-implied variants, never expressing uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-70B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight LLaMA2-Chat 70B with specified decoding parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Belief likelihood test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Variant manipulation test to probe whether models integrate story information to discriminate belief likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>More 'didn't know' responses for faux pas vs neutral (χ^2(1)=20.20, P=2.81×10^-5); no differentiation between neutral and knowledge-implied (χ^2(1)=1.80, P=0.180); never reported uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans discriminated across variants and reported uncertainty more often in neutral trials.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM behavior indicates biased tendency rather than true discrimination; unlike humans, LLaMA2 fails to shift toward 'knew' in knowledge-implied variants.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Administered to new human sample (N=900 total human participants for this control) and to LLMs across 15 sessions; coding used numeric scores -1/0/+1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Perfect performance on original faux pas appears illusory; LLaMA2's response bias limits interpretability as true mental-state inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e9049.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - Hinting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on Hinting task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 outperformed humans on the hinting task which measures understanding of indirect speech acts and intended requests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-enabled GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Hinting task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Ten-to-sixteen vignettes where a speaker makes a hint; correct answers identify intended meaning and requested action (pragmatics / indirect speech).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Statistically above human level (Z = 0.00, P = 0.040, r = 0.32).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants' baseline performance used for comparison; GPT-4 exceeded it.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM outperforms human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Removed follow-up prompting used in original clinical administration; responses coded conservatively as binary correct/incorrect; included novel items (6 novel added).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Coding was conservative (no iterative prompting) which may underestimate human-capable hint comprehension under prompted conditions; GPT hyperconservatism affects other tasks but hinting allows open-ended generation that suits LLM strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e9049.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 - Hinting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 evaluated on Hinting task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 performance did not significantly differ from human baseline on the hinting task under the conservative scoring used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT predecessor.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Hinting task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Indirect request comprehension requiring identification of intended action.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>No significant difference from humans (Z = 0.00, P = 0.626, r = 0.06; BF10 = 0.33).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants served as baseline; GPT-3.5 matched average human performance under this scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM matches human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Conservative binary coding without follow-up prompts; 16 items including novel ones.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Performance may change if prompting allowed or if coding followed original clinical iterative prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e9049.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-70B - Hinting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-Chat (70B) evaluated on Hinting task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA2-70B scored significantly below human levels on the hinting task, indicating poorer comprehension of indirect requests under the test conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-70B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight LLaMA2-Chat 70B with controlled decoding params.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Hinting task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Indirect speech / hint comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Significantly below human levels (Z = −0.20, P = 5.42 × 10^-5, r = 0.57).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans performed better than LLaMA2-70B on hinting under the conservative coding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM below human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Novel items included; automated delivery with occasional regeneration of non-codable responses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Some novel items were easier and item difficulty may have influenced comparisons; smaller LLaMA variants had more non-responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e9049.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - Strange stories</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 evaluated on Strange Stories test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 significantly outperformed humans on Strange Stories, a test measuring advanced mentalizing (misdirection, lying, manipulation) requiring multi-step mental-state explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-enabled GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Short vignettes requiring explanations of why characters say/do things not literally true; measures higher-order mentalizing and pragmatic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Significantly above human level (Z = 0.13, P = 1.04 × 10^-5, r = 0.60).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants provided baseline scores; GPT-4 exceeded them.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM outperforms human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>8 original mental stories + 4 novel items; scoring 0/1/2 per question according to mentalistic content; session scores converted to proportional scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Partial successes were infrequent; high LLM performance might reflect pattern extrapolation rather than human-like spontaneous mentalizing; open-ended format may favor LLM generation abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e9049.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 - Strange stories</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 evaluated on Strange Stories test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 performance on Strange Stories did not significantly differ from human baseline under the scoring scheme used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT predecessor.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Advanced mentalizing stories requiring explicit mental-state explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>No significant difference from humans (Z = −0.06, P = 0.110, r = 0.24; BF10 = 0.47).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human baseline scores used; GPT-3.5 matched average human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM matches human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Scoring allowed partial credit; 12 items total (original + novel).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Some qualitative differences in response types noted; inter-rater coding disagreement highest on these items but still >88% agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9049.20">
                <h3 class="extraction-instance">Extracted Data Instance 20 (e9049.20)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-70B - Strange stories</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-Chat (70B) evaluated on Strange Stories test</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA2-70B scored significantly below humans on Strange Stories, indicating weaker performance on higher-order mentalizing and pragmatic explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-70B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight LLaMA2-Chat 70B.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Strange Stories</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories targeting complex mental-state reasoning (misdirection, deception, manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Significantly below human levels (Z = −0.13, P = 0.005, r = 0.41).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans scored higher than LLaMA2-70B on this battery component.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM below human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>12 items; partial success scoring infrequent and more likely for LLaMA2-70B than other models according to supplement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Lower performance may reflect difficulties in generating full mentalistic explanations; partial-credit scoring and qualitative response patterns important to interpret results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing theory of mind in large language models and humans', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theory of mind may have spontaneously emerged in large language models <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks <em>(Rating: 2)</em></li>
                <li>Clever Hans or neural theory of mind? Stress testing social reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Exploring GPT-3 model's capability in passing the Sally-Anne Test A preliminary study in two languages <em>(Rating: 2)</em></li>
                <li>How well do large language models perform on faux pas tests? <em>(Rating: 2)</em></li>
                <li>FANToM: a benchmark for stress-testing machine theory of mind in interactions <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: early experiments with GPT-4 <em>(Rating: 1)</em></li>
                <li>Understanding social reasoning in language models with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9049",
    "paper_id": "paper-269928651",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-4 - False belief",
            "name_full": "OpenAI GPT-4 evaluated on False Belief test",
            "brief_description": "GPT-4 was evaluated on classic false-belief vignettes adapted for a species-fair, text-based comparison and achieved ceiling/near-perfect accuracy across original and novel items.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source generative pre-trained transformer from OpenAI, chat-enabled; enhanced reasoning and comprehension relative to previous GPT versions (used via ChatGPT web interface).",
            "model_size": null,
            "test_battery_name": "False belief task (classic false/true belief scenarios)",
            "test_description": "Language-based false-belief stories assessing ability to infer that an agent holds a belief that differs from reality (belief attribution).",
            "llm_performance": "Ceiling / near-perfect accuracy across original and novel items (all LLM sessions correctly reported the agent's false belief).",
            "human_baseline_performance": "Near-ceiling (only a few human errors; e.g., 5 participants out of 51 made one error on novel items).",
            "performance_comparison": "LLM matches or equals human ceiling performance.",
            "experimental_details": "Each item delivered in separate chat sessions; 15 independent sessions; novel items generated to avoid training-set memorization; scoring followed human test protocols.",
            "limitations_or_caveats": "High accuracy can reflect heuristic/scripted solutions rather than human-like belief tracking; prior work and control perturbations show GPTs can fail under minor structural changes to the task.",
            "uuid": "e9049.0",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-3.5 - False belief",
            "name_full": "OpenAI GPT-3.5 evaluated on False Belief test",
            "brief_description": "GPT-3.5 (ChatGPT predecessor) was tested on false-belief vignettes and performed at ceiling/near-perfect levels on the standard tasks and novel items in this battery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Earlier ChatGPT model (default at time of testing), closed-source, chat-enabled via web interface.",
            "model_size": null,
            "test_battery_name": "False belief task",
            "test_description": "Language-based false-belief vignettes assessing belief attribution.",
            "llm_performance": "Ceiling / near-perfect accuracy on standard and novel false-belief items.",
            "human_baseline_performance": "Near-ceiling (small number of human errors reported).",
            "performance_comparison": "LLM matches human ceiling performance.",
            "experimental_details": "15 independent chat sessions per test; items alternated between false and true belief; novel items generated.",
            "limitations_or_caveats": "Prior literature and control perturbations show GPT-3.5 can fail on small alterations to the task; ceiling performance on standard items may not reflect robust belief-tracking.",
            "uuid": "e9049.1",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA2-70B - False belief",
            "name_full": "LLaMA2-Chat (70B) evaluated on False Belief test",
            "brief_description": "The open-weight LLaMA2-70B chat model achieved near-perfect performance on the false-belief tasks in the battery, matching human ceiling performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-70B-Chat",
            "model_description": "Open-weight LLaMA2-Chat family (tested 70B, also 13B and 7B reported in supplement); responses collected with defined decoding parameters and 'You are a helpful AI assistant' prompt.",
            "model_size": "70B",
            "test_battery_name": "False belief task",
            "test_description": "Language-based false-belief vignettes assessing belief attribution.",
            "llm_performance": "Near-perfect / ceiling performance on standard and novel false-belief items.",
            "human_baseline_performance": "Near-ceiling (few human errors).",
            "performance_comparison": "LLM matches human ceiling performance.",
            "experimental_details": "Responses gathered with Langchain conversation chain (for 70B within-session memory); temperature 0.7, max tokens 512, repetition penalty 1.1, TopP 0.9; 15 independent sessions.",
            "limitations_or_caveats": "Same caveat: regular structure of task may permit non-mentalistic heuristics; smaller LLaMA2 variants produced many non-responses (data quality concerns).",
            "uuid": "e9049.2",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4 - Irony",
            "name_full": "OpenAI GPT-4 evaluated on Irony comprehension test",
            "brief_description": "GPT-4 outperformed human participants on an irony comprehension test derived from an eye-tracking study, showing superior detection/interpretation of ironic utterances.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Chat-enabled GPT-4 used via ChatGPT web interface.",
            "model_size": null,
            "test_battery_name": "Irony comprehension",
            "test_description": "Vignettes containing ironic or non-ironic utterances requiring inference of speaker's mocking/non-literal intent (pragmatic/mentalizing domain).",
            "llm_performance": "Statistically above human level (Z = 0.00, P = 0.040, r = 0.32; model reached high accuracy on irony detection).",
            "human_baseline_performance": "Baseline human sample; GPT-4 performance significantly exceeded this baseline (statistics reported).",
            "performance_comparison": "LLM outperforms human baseline.",
            "experimental_details": "12 items adapted from prior study; novel items not applicable; outputs coded 1/0; occasional internally contradictory model replies were coded based on explanation quality.",
            "limitations_or_caveats": "GPT-4's superior performance may reflect pattern learning from training data; GPT-3.5 and LLaMA2 showed worse-than-human performance, showing model variability.",
            "uuid": "e9049.3",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-3.5 - Irony",
            "name_full": "OpenAI GPT-3.5 evaluated on Irony comprehension test",
            "brief_description": "GPT-3.5 performed below human baseline at recognizing irony in the adapted vignettes, though it performed perfectly at recognizing non-ironic control statements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "ChatGPT predecessor tested via web interface.",
            "model_size": null,
            "test_battery_name": "Irony comprehension",
            "test_description": "Ironic vs non-ironic utterance comprehension.",
            "llm_performance": "Below human levels (Z = −0.17, P = 2.37 × 10^-6, r = 0.64); perfect at detecting non-ironic controls but made errors on irony detection.",
            "human_baseline_performance": "Humans performed better on irony detection than GPT-3.5 in this study.",
            "performance_comparison": "LLM below human baseline for irony detection.",
            "experimental_details": "Items taken from prior eye-tracking study; responses coded for appreciation of irony.",
            "limitations_or_caveats": "Order effects observed (GPT-3.5 made more errors on earlier trials); some internal inconsistency in generated answers required careful coding.",
            "uuid": "e9049.4",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA2-70B - Irony",
            "name_full": "LLaMA2-Chat (70B) evaluated on Irony comprehension test",
            "brief_description": "LLaMA2-70B performed below human baseline on irony comprehension, showing poor discrimination between ironic and non-ironic statements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-70B-Chat",
            "model_description": "Open-weight LLaMA2-Chat (70B) with Langchain delivery and fixed generation parameters.",
            "model_size": "70B",
            "test_battery_name": "Irony comprehension",
            "test_description": "Detection of ironic versus non-ironic utterances.",
            "llm_performance": "Below human levels (Z = −0.42, P = 2.39 × 10^-7, r = 0.70); errors for both ironic and non-ironic controls.",
            "human_baseline_performance": "Humans outperformed LLaMA2-70B on irony discrimination.",
            "performance_comparison": "LLM below human baseline.",
            "experimental_details": "Automated delivery; responses sometimes non-codable were regenerated; coding focused on whether explanation showed appreciation of irony.",
            "limitations_or_caveats": "LLaMA2-70B exhibited overall poor discrimination, suggesting model-specific weaknesses in pragmatic inference.",
            "uuid": "e9049.5",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4 - Faux pas (original)",
            "name_full": "OpenAI GPT-4 evaluated on Faux Pas recognition (original test)",
            "brief_description": "GPT-4 identified victims' hurt but often refused to commit to speaker's lack of knowledge when asked directly, yielding below-human scores on the original yes/no belief question.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Chat-enabled GPT-4 used via web interface with per-chat memory but no cross-chat memory.",
            "model_size": null,
            "test_battery_name": "Faux pas recognition (Baron-Cohen et al. test)",
            "test_description": "Vignettes where a speaker says something unintentionally offensive; requires representing speaker's ignorance and victim's hurt (pragmatic/mentalizing domain).",
            "llm_performance": "Below human levels on original framing (belief yes/no) (Z = −0.40, P = 5.42 × 10^-5, r = 0.55); often answered 'insufficient information' rather than 'no'.",
            "human_baseline_performance": "Humans performed significantly better on the original belief question (majority answered correctly that speaker did not know).",
            "performance_comparison": "LLM below human baseline on original faux pas question (due to conservative refusal to commit).",
            "experimental_details": "10 original + 5 novel faux pas items; coding focused on final belief question (correct answer = 'no'); 15 independent sessions.",
            "limitations_or_caveats": "Failure reflects hyperconservatism (mitigation/hesitancy) rather than inability to infer mental states; alternative framings and follow-up tests show GPT-4 can infer but refuses to commit.",
            "uuid": "e9049.6",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-3.5 - Faux pas (original)",
            "name_full": "OpenAI GPT-3.5 evaluated on Faux Pas recognition (original test)",
            "brief_description": "GPT-3.5 largely failed the original faux pas belief question, performing near floor except on one item, often refusing to commit or producing incorrect/noisy answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "ChatGPT predecessor with poorer calibration/mitigation compared to GPT-4.",
            "model_size": null,
            "test_battery_name": "Faux pas recognition (original)",
            "test_description": "Faux pas vignettes requiring belief attribution about speaker's knowledge.",
            "llm_performance": "Near-floor performance on original belief question (Z = −0.80, P = 5.95 × 10^-8, r = 0.72); most answers incorrect or noncommittal.",
            "human_baseline_performance": "Humans performed substantially better.",
            "performance_comparison": "LLM well below human baseline on original faux pas question.",
            "experimental_details": "Same administration as GPT-4; occasional partial successes; coding focused on final belief question.",
            "limitations_or_caveats": "Later likelihood rephrasing improved GPT-3.5 performance; original failure may combine uncertainty avoidance and calibration issues.",
            "uuid": "e9049.7",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA2-70B - Faux pas (original)",
            "name_full": "LLaMA2-Chat (70B) evaluated on Faux Pas recognition (original test)",
            "brief_description": "LLaMA2-70B outperformed humans on the original faux pas test, reaching near-perfect scores (100% accuracy in all but one run), but follow-up tests suggested this superiority may be due to a biased tendency to attribute ignorance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-70B-Chat",
            "model_description": "Open-weight LLaMA2-Chat 70B with Langchain delivery parameters; other smaller LLaMA2 variants produced poorer data quality.",
            "model_size": "70B",
            "test_battery_name": "Faux pas recognition (original)",
            "test_description": "Recognition of unintentional offensive remarks and the speaker's ignorance.",
            "llm_performance": "Outperformed humans (Z = 0.10, P = 0.002, r = 0.44); achieved 100% accuracy in all but one run on original test items.",
            "human_baseline_performance": "Humans were less accurate than LLaMA2-70B on this original formulation.",
            "performance_comparison": "LLM superficially outperforms human baseline on original faux pas items.",
            "experimental_details": "Automated delivery via Langchain; responses regenerations applied for non-codable responses; scoring focused on belief question.",
            "limitations_or_caveats": "Follow-up belief-likelihood variants showed LLaMA2-70B did not discriminate knowledge-implied vs neutral variants and always selected one explanation (bias towards 'didn't know'), suggesting the original superiority is illusory and due to response bias.",
            "uuid": "e9049.8",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4 - Faux pas likelihood test",
            "name_full": "OpenAI GPT-4 evaluated on Faux Pas likelihood-framed question",
            "brief_description": "When the faux pas belief question was reframed to ask which explanation was more likely, GPT-4 reliably identified that 'speaker did not know' was the likeliest explanation (perfect performance reported).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Chat-enabled GPT-4.",
            "model_size": null,
            "test_battery_name": "Faux pas likelihood test (likelihood framing of belief question)",
            "test_description": "Rephrasing of the belief question to ask whether 'knew' or 'didn't know' is more likely, focusing on graded likelihood judgment rather than binary commitment.",
            "llm_performance": "Perfect performance (all responses indicated 'more likely that speaker did not know' without prompting).",
            "human_baseline_performance": "Humans typically endorsed 'didn't know' in faux pas variants but sometimes expressed uncertainty; GPT-4 matched expected likelihood judgments.",
            "performance_comparison": "LLM matches or exceeds human-level inference when asked about likelihood; shows ability to infer but avoids categorical commitment in original framing.",
            "experimental_details": "15 sessions; follow-up prompts used rarely for prompting unclear reasoning; coding gave 2 points unprompted, 1 prompted correct, 0 incorrect after prompt.",
            "limitations_or_caveats": "Demonstrates hyperconservatism in direct yes/no framing (models can compute but avoid categorical commitment), suggesting mitigation/hallucination-avoidance policies affect outputs.",
            "uuid": "e9049.9",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-3.5 - Faux pas likelihood test",
            "name_full": "OpenAI GPT-3.5 evaluated on Faux Pas likelihood-framed question",
            "brief_description": "GPT-3.5 improved on the likelihood-framed question compared with the original yes/no framing, often identifying 'didn't know' as more likely, though it sometimes required prompting (~3% items) and occasionally failed (~9% items).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "ChatGPT predecessor.",
            "model_size": null,
            "test_battery_name": "Faux pas likelihood test",
            "test_description": "Likelihood-framed belief question to reveal whether models commit to the likeliest explanation.",
            "llm_performance": "Improved relative to original; required prompting on ~3% of items; occasional failures (~9% of items).",
            "human_baseline_performance": "Humans endorsed 'didn't know' for faux pas but were more likely to report uncertainty in some neutral cases.",
            "performance_comparison": "LLM approaches human-level inference under likelihood framing but is noisier than GPT-4.",
            "experimental_details": "Follow-up prompt delivered in rare cases; coding scheme allowed partial credit if correct after prompt.",
            "limitations_or_caveats": "GPT-3.5 never reported uncertainty in belief-likelihood variants (always committed to one explanation), differing from human pattern.",
            "uuid": "e9049.10",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA2-70B - Faux pas likelihood test",
            "name_full": "LLaMA2-Chat (70B) evaluated on Faux Pas likelihood-framed question",
            "brief_description": "LLaMA2-70B tended to endorse 'didn't know' in the likelihood-framed faux pas items but showed biased behavior (always selected one explanation and failed to express uncertainty), raising concerns about a response bias.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-70B-Chat",
            "model_description": "Open-weight LLaMA2-Chat 70B with specified decoding parameters.",
            "model_size": "70B",
            "test_battery_name": "Faux pas likelihood test",
            "test_description": "Likelihood-framed belief judgment variants (faux pas vs neutral vs knowledge-implied).",
            "llm_performance": "More likely to select 'didn't know' for faux pas than neutral (χ^2(1)=20.20, P=2.81×10^-5), but did not differentiate neutral vs knowledge-implied; never reported uncertainty (always chose one explanation).",
            "human_baseline_performance": "Humans discriminated across variants and reported uncertainty more in neutral conditions.",
            "performance_comparison": "LLM superficially aligns with humans on faux pas variant but shows biased behavior and fails to discriminate knowledge-implied cases.",
            "experimental_details": "Responses coded as -1/0/+1 for didn't know/unsure/knew; administered with independent sessions to avoid cross-variant contamination.",
            "limitations_or_caveats": "Performance likely driven by bias toward attributing ignorance; the original perfect performance appears illusory when controlling for variant types.",
            "uuid": "e9049.11",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4 - Belief likelihood (variant discrimination)",
            "name_full": "OpenAI GPT-4 evaluated on Belief Likelihood test (faux pas / neutral / knowledge-implied variants)",
            "brief_description": "GPT-4 discriminated between faux pas, neutral, and knowledge-implied variants similarly to humans: endorsing 'didn't know' for faux pas and 'knew' for knowledge-implied, and showing uncertainty primarily in neutral cases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Chat-enabled GPT-4.",
            "model_size": null,
            "test_battery_name": "Belief likelihood test (faux pas / neutral / knowledge-implied variants)",
            "test_description": "Controlled variants manipulating likelihood that speaker knew vs didn't know; response coded -1/0/+1.",
            "llm_performance": "Discriminated across variants: more 'didn't know' responses in faux pas than neutral (χ^2(2)=109, P=1.54×10^-23) and more 'knew' in knowledge-implied than neutral (χ^2(2)=18.10, P=3.57×10^-4); reported uncertainty often in neutral condition.",
            "human_baseline_performance": "Humans showed the expected discrimination (significant chi-square), with some uncertainty in neutral trials.",
            "performance_comparison": "LLM matches human pattern of sensitivity to manipulated belief likelihoods.",
            "experimental_details": "Six stories adapted into three variants; 15 LLM sessions per item; responses numerically coded and averaged to directional scores.",
            "limitations_or_caveats": "GPT-4 still reports uncertainty in neutral condition and shows hyperconservatism in categorical yes/no framings; performance depends on framing of the question.",
            "uuid": "e9049.12",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-3.5 - Belief likelihood (variant discrimination)",
            "name_full": "OpenAI GPT-3.5 evaluated on Belief Likelihood test (faux pas / neutral / knowledge-implied)",
            "brief_description": "GPT-3.5 showed variant discrimination similar to humans (endorsing 'didn't know' for faux pas and 'knew' for knowledge-implied) but never reported uncertainty (always committed to one explanation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "ChatGPT predecessor.",
            "model_size": null,
            "test_battery_name": "Belief likelihood test",
            "test_description": "Controlled manipulation of belief-likelihood across three story variants.",
            "llm_performance": "More 'didn't know' for faux pas vs neutral (χ^2(1)=8.44, P=0.007) and more 'knew' for knowledge-implied vs neutral (χ^2(1)=21.50, P=1.82×10^-5); never reported 'unsure'.",
            "human_baseline_performance": "Humans reported some uncertainty in neutral condition, unlike GPT-3.5.",
            "performance_comparison": "LLM shows correct likelihood discrimination but differs qualitatively from humans by always committing.",
            "experimental_details": "Responses coded -1/0/+1; models run with independent sessions per item.",
            "limitations_or_caveats": "Lack of expressed uncertainty suggests a qualitative difference from human responses despite similar discrimination of variants.",
            "uuid": "e9049.13",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA2-70B - Belief likelihood (variant discrimination)",
            "name_full": "LLaMA2-Chat (70B) evaluated on Belief Likelihood test (faux pas / neutral / knowledge-implied)",
            "brief_description": "LLaMA2-70B demonstrated a bias toward endorsing 'didn't know' for faux pas and failed to differentiate neutral versus knowledge-implied variants, never expressing uncertainty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-70B-Chat",
            "model_description": "Open-weight LLaMA2-Chat 70B with specified decoding parameters.",
            "model_size": "70B",
            "test_battery_name": "Belief likelihood test",
            "test_description": "Variant manipulation test to probe whether models integrate story information to discriminate belief likelihoods.",
            "llm_performance": "More 'didn't know' responses for faux pas vs neutral (χ^2(1)=20.20, P=2.81×10^-5); no differentiation between neutral and knowledge-implied (χ^2(1)=1.80, P=0.180); never reported uncertainty.",
            "human_baseline_performance": "Humans discriminated across variants and reported uncertainty more often in neutral trials.",
            "performance_comparison": "LLM behavior indicates biased tendency rather than true discrimination; unlike humans, LLaMA2 fails to shift toward 'knew' in knowledge-implied variants.",
            "experimental_details": "Administered to new human sample (N=900 total human participants for this control) and to LLMs across 15 sessions; coding used numeric scores -1/0/+1.",
            "limitations_or_caveats": "Perfect performance on original faux pas appears illusory; LLaMA2's response bias limits interpretability as true mental-state inference.",
            "uuid": "e9049.14",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4 - Hinting",
            "name_full": "OpenAI GPT-4 evaluated on Hinting task",
            "brief_description": "GPT-4 outperformed humans on the hinting task which measures understanding of indirect speech acts and intended requests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Chat-enabled GPT-4.",
            "model_size": null,
            "test_battery_name": "Hinting task",
            "test_description": "Ten-to-sixteen vignettes where a speaker makes a hint; correct answers identify intended meaning and requested action (pragmatics / indirect speech).",
            "llm_performance": "Statistically above human level (Z = 0.00, P = 0.040, r = 0.32).",
            "human_baseline_performance": "Human participants' baseline performance used for comparison; GPT-4 exceeded it.",
            "performance_comparison": "LLM outperforms human baseline.",
            "experimental_details": "Removed follow-up prompting used in original clinical administration; responses coded conservatively as binary correct/incorrect; included novel items (6 novel added).",
            "limitations_or_caveats": "Coding was conservative (no iterative prompting) which may underestimate human-capable hint comprehension under prompted conditions; GPT hyperconservatism affects other tasks but hinting allows open-ended generation that suits LLM strengths.",
            "uuid": "e9049.15",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-3.5 - Hinting",
            "name_full": "OpenAI GPT-3.5 evaluated on Hinting task",
            "brief_description": "GPT-3.5 performance did not significantly differ from human baseline on the hinting task under the conservative scoring used.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "ChatGPT predecessor.",
            "model_size": null,
            "test_battery_name": "Hinting task",
            "test_description": "Indirect request comprehension requiring identification of intended action.",
            "llm_performance": "No significant difference from humans (Z = 0.00, P = 0.626, r = 0.06; BF10 = 0.33).",
            "human_baseline_performance": "Human participants served as baseline; GPT-3.5 matched average human performance under this scoring.",
            "performance_comparison": "LLM matches human baseline.",
            "experimental_details": "Conservative binary coding without follow-up prompts; 16 items including novel ones.",
            "limitations_or_caveats": "Performance may change if prompting allowed or if coding followed original clinical iterative prompts.",
            "uuid": "e9049.16",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA2-70B - Hinting",
            "name_full": "LLaMA2-Chat (70B) evaluated on Hinting task",
            "brief_description": "LLaMA2-70B scored significantly below human levels on the hinting task, indicating poorer comprehension of indirect requests under the test conditions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-70B-Chat",
            "model_description": "Open-weight LLaMA2-Chat 70B with controlled decoding params.",
            "model_size": "70B",
            "test_battery_name": "Hinting task",
            "test_description": "Indirect speech / hint comprehension.",
            "llm_performance": "Significantly below human levels (Z = −0.20, P = 5.42 × 10^-5, r = 0.57).",
            "human_baseline_performance": "Humans performed better than LLaMA2-70B on hinting under the conservative coding.",
            "performance_comparison": "LLM below human baseline.",
            "experimental_details": "Novel items included; automated delivery with occasional regeneration of non-codable responses.",
            "limitations_or_caveats": "Some novel items were easier and item difficulty may have influenced comparisons; smaller LLaMA variants had more non-responses.",
            "uuid": "e9049.17",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4 - Strange stories",
            "name_full": "OpenAI GPT-4 evaluated on Strange Stories test",
            "brief_description": "GPT-4 significantly outperformed humans on Strange Stories, a test measuring advanced mentalizing (misdirection, lying, manipulation) requiring multi-step mental-state explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Chat-enabled GPT-4.",
            "model_size": null,
            "test_battery_name": "Strange Stories",
            "test_description": "Short vignettes requiring explanations of why characters say/do things not literally true; measures higher-order mentalizing and pragmatic inference.",
            "llm_performance": "Significantly above human level (Z = 0.13, P = 1.04 × 10^-5, r = 0.60).",
            "human_baseline_performance": "Human participants provided baseline scores; GPT-4 exceeded them.",
            "performance_comparison": "LLM outperforms human baseline.",
            "experimental_details": "8 original mental stories + 4 novel items; scoring 0/1/2 per question according to mentalistic content; session scores converted to proportional scores.",
            "limitations_or_caveats": "Partial successes were infrequent; high LLM performance might reflect pattern extrapolation rather than human-like spontaneous mentalizing; open-ended format may favor LLM generation abilities.",
            "uuid": "e9049.18",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-3.5 - Strange stories",
            "name_full": "OpenAI GPT-3.5 evaluated on Strange Stories test",
            "brief_description": "GPT-3.5 performance on Strange Stories did not significantly differ from human baseline under the scoring scheme used.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "ChatGPT predecessor.",
            "model_size": null,
            "test_battery_name": "Strange Stories",
            "test_description": "Advanced mentalizing stories requiring explicit mental-state explanations.",
            "llm_performance": "No significant difference from humans (Z = −0.06, P = 0.110, r = 0.24; BF10 = 0.47).",
            "human_baseline_performance": "Human baseline scores used; GPT-3.5 matched average human performance.",
            "performance_comparison": "LLM matches human baseline.",
            "experimental_details": "Scoring allowed partial credit; 12 items total (original + novel).",
            "limitations_or_caveats": "Some qualitative differences in response types noted; inter-rater coding disagreement highest on these items but still &gt;88% agreement.",
            "uuid": "e9049.19",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA2-70B - Strange stories",
            "name_full": "LLaMA2-Chat (70B) evaluated on Strange Stories test",
            "brief_description": "LLaMA2-70B scored significantly below humans on Strange Stories, indicating weaker performance on higher-order mentalizing and pragmatic explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2-70B-Chat",
            "model_description": "Open-weight LLaMA2-Chat 70B.",
            "model_size": "70B",
            "test_battery_name": "Strange Stories",
            "test_description": "Stories targeting complex mental-state reasoning (misdirection, deception, manipulation).",
            "llm_performance": "Significantly below human levels (Z = −0.13, P = 0.005, r = 0.41).",
            "human_baseline_performance": "Humans scored higher than LLaMA2-70B on this battery component.",
            "performance_comparison": "LLM below human baseline.",
            "experimental_details": "12 items; partial success scoring infrequent and more likely for LLaMA2-70B than other models according to supplement.",
            "limitations_or_caveats": "Lower performance may reflect difficulties in generating full mentalistic explanations; partial-credit scoring and qualitative response patterns important to interpret results.",
            "uuid": "e9049.20",
            "source_info": {
                "paper_title": "Testing theory of mind in large language models and humans",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theory of mind may have spontaneously emerged in large language models",
            "rating": 2,
            "sanitized_title": "theory_of_mind_may_have_spontaneously_emerged_in_large_language_models"
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "rating": 2,
            "sanitized_title": "large_language_models_fail_on_trivial_alterations_to_theoryofmind_tasks"
        },
        {
            "paper_title": "Clever Hans or neural theory of mind? Stress testing social reasoning in large language models",
            "rating": 2,
            "sanitized_title": "clever_hans_or_neural_theory_of_mind_stress_testing_social_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Exploring GPT-3 model's capability in passing the Sally-Anne Test A preliminary study in two languages",
            "rating": 2,
            "sanitized_title": "exploring_gpt3_models_capability_in_passing_the_sallyanne_test_a_preliminary_study_in_two_languages"
        },
        {
            "paper_title": "How well do large language models perform on faux pas tests?",
            "rating": 2,
            "sanitized_title": "how_well_do_large_language_models_perform_on_faux_pas_tests"
        },
        {
            "paper_title": "FANToM: a benchmark for stress-testing machine theory of mind in interactions",
            "rating": 2,
            "sanitized_title": "fantom_a_benchmark_for_stresstesting_machine_theory_of_mind_in_interactions"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: early experiments with GPT-4",
            "rating": 1,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Understanding social reasoning in language models with language models",
            "rating": 1,
            "sanitized_title": "understanding_social_reasoning_in_language_models_with_language_models"
        }
    ],
    "cost": 0.02430325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>nature human behaviour
April 2023</p>
<p>James W A Strach 0000-0001-6092-1889
and</p>
<p>Dalila Alber 0000-0002-4977-2197
and</p>
<p>Giulia Borghi 0000-0001-6978-4243
and</p>
<p>Oriana Pansardi 0000-0001-6092-1889
and</p>
<p>Eugenio Scalit 0000-0003-1700-8909
and</p>
<p>Saurabh Gupt 0009-0009-2927-3380
and</p>
<p>Krati Saxena 0000-0001-7049-9685
and</p>
<p>Alessandro Rufo 0009-0003-8565-4192
and</p>
<p>Stefano Panzeri 0000-0003-1700-8909
and</p>
<p>Gui Manzi 0009-0009-2927-3380
and</p>
<p>Michael S A Graziano 
and</p>
<p>Cristina Becchio c.becchio@uke.de 0000-0002-6845-0521
and</p>
<p>and</p>
<p>and</p>
<p>James W A Strachan james.wa.strachan@gmail.com 
and</p>
<p>and</p>
<p>nature human behaviour
April 2023519AE2409FE83FD0E9695B91136CF2E710.1038/s41562-024-01882-zReceived: 14 August 2023 Accepted: 5 April 2024</p>
<p>At the core of what defines us as humans is the concept of theory of mind: the ability to track other people's mental states.The recent development of large language models (LLMs) such as ChatGPT has led to intense debate about the possibility that these models exhibit behaviour that is indistinguishable from human behaviour in theory of mind tasks.Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities, from understanding false beliefs to interpreting indirect requests and recognizing irony and faux pas.We tested two families of LLMs (GPT and LLaMA2) repeatedly against these measures and compared their performance with those from a sample of 1,907 human participants.Across the battery of theory of mind tests, we found that GPT-4 models performed at, or even sometimes above, human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas.Faux pas, however, was the only test where LLaMA2 outperformed humans.Follow-up manipulations of the belief likelihood revealed that the superiority of LLaMA2 was illusory, possibly reflecting a bias towards attributing ignorance.By contrast, the poor performance of GPT originated from a h yp erconservative approach towards committing to conclusions rather than from a genuine failure of inference.These findings not only demonstrate that LLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans but also highlight the importance of systematic testing to ensure a non-superficial comparison between human and artificial intelligences.</p>
<p>People care about what other people think and expend a lot of effort thinking about what is going on in other minds.Everyday life is full of social interactions that only make sense when considered in light of our capacity to represent other minds: when you are standing near a closed window and a friend says, 'It's a bit hot in here', it is your ability to think about her beliefs and desires that allows you to recognize that she is not just commenting on the temperature but politely asking you to open the window 1 .</p>
<p>Article</p>
<p>https://doi.org/10.1038/s41562-024-01882-zirony comprehension using stimuli adapted from a previous study 34 .Each test was administered separately to GPT-4, GPT-3.5 and LLaMA2-70B-Chat (hereafter LLaMA2-70B) across 15 chats.We also tested two other sizes of LLaMA2 model (7B and 13B), the results of which are reported in Supplementary Information section 1.Because each chat is a separate and independent session, and information about previous sessions is not retained, this allowed us to treat each chat (session) as an independent observation.Responses were scored in accordance with the scoring protocols for each test in humans (Methods) and compared with those collected from a sample of 250 human participants.Tests were administered by presenting each item sequentially in a written format that ensured a species-fair comparison 35 (Methods) between LLMs and human participants.</p>
<p>Performance across theory of mind tests</p>
<p>Except for the irony test, all other tests in our battery are publicly available tests accessible within open databases and scholarly journal articles.To ensure that models did not merely replicate training set data, we generated novel items for each published test (Methods).These novel test items matched the logic of the original test items but used a different semantic content.The text of original and novel items and the coded responses are available on the OSF (methods and resource availability).</p>
<p>Figure 1a compares the performance of LLMs against the performance of human participants across all tests included in the battery.Differences in performance on original items versus novel items, separately for each test and model, are shown in Fig. 1b.</p>
<p>False belief.Both human participants and LLMs performed at ceiling on this test (Fig. 1a).All LLMs correctly reported that an agent who left the room while the object was moved would later look for the object in the place where they remembered seeing it, even though it no longer matched the current location.Performance on novel items was also near perfect (Fig. 1b), with only 5 human participants out of 51 making one error, typically by failing to specify one of the two locations (for example, 'He'll look in the room'; Supplementary Information section 2).</p>
<p>In humans, success on the false belief task requires inhibiting one's own belief about reality in order to use one's knowledge about the character's mental state to derive predictions about their behaviour.However, with LLMs, performance may be explained by lower-level explanations than belief tracking 27 .Supporting this interpretation, LLMs such as ChatGPT have been shown to be susceptible to minor alterations to the false belief formulation 25,27 , such as making the containers where the object is hidden transparent or asking about the belief of the character who moved the object rather than the one who was out of the room.Such perturbations of the standard false belief structure are assumed not to matter for humans (who possess a theory of mind) 25 .In a control study using these perturbation variants (Supplementary Information section 4 and Supplementary Appendix 1), we replicated the poor performance of GPT models found in previous studies 25 .However, we found that human participants (N = 757) also failed on half of these perturbations.Understanding these failures and the similarities and differences in how humans and LLMs may arrive at the same outcome requires further systematic investigation.For example, because these perturbations also involve changes in the physical properties of the environment, it is difficult to establish whether LLMs (and humans) failed because they were sticking to the familiar script and were unable to automatically attribute an updated belief, or because they did not consider physical principles (for example, transparency).</p>
<p>Irony.GPT-4 performed significantly better than human levels (Z = 0.00, P = 0.040, r = 0.32, 95% confidence interval (CI) 0.14-0.48).By contrast, both GPT-3.5 (Z = −0.17,P = 2.37 × 10 −6 , r = 0.64, 95% CI 0.49-0.77)and LLaMA2-70B (Z = −0.42,P = 2.39 × 10 −7 , r = 0.70, 95% CI 0.55-0.79)performed below human levels (Fig. 1a).GPT-3.5 performed perfectly at This ability for tracking other people's mental states is known as theory of mind.Theory of mind is central to human social interactions-from communication to empathy to social decision-makingand has long been of interest to developmental, social and clinical psychologists.Far from being a unitary construct, theory of mind refers to an interconnected set of notions that are combined to explain, predict, and justify the behaviour of others 2 .Since the term 'theory of mind' was first introduced in 1978 (ref.</p>
<p>3), dozens of tasks have been developed to study it, including indirect measures of belief attribution using reaction times [4][5][6] and looking or searching behaviour [7][8][9] , tasks examining the ability to infer mental states from photographs of eyes 10 , and language-based tasks assessing false belief understanding 11,12 and pragmatic language comprehension [13][14][15][16] .These measures are proposed to test early, efficient but inflexible implicit processes as well as later-developing, flexible and demanding explicit abilities that are crucial for the generation and comprehension of complex behavioural interactions 17,18 involving phenomena such as misdirection, irony, implicature and deception.</p>
<p>The recent rise of large language models (LLMs), such as generative pre-trained transformer (GPT) models, has shown some promise that artificial theory of mind may not be too distant an idea.Generative LLMs exhibit performance that is characteristic of sophisticated decision-making and reasoning abilities 19,20 including solving tasks widely used to test theory of mind in humans [21][22][23][24] .However, the mixed success of these models 23 , along with their vulnerability to small perturbations to the provided prompts, including simple changes in characters' perceptual access 25 , raises concerns about the robustness and interpretability of the observed successes.Even in cases where these models are capable of solving complex tasks 20 that are cognitively demanding even for human adults 17 , it cannot be taken for granted that they will not be tripped up by a simpler task that a human would find trivial 26 .As a result, work in LLMs has begun to question whether these models rely on shallow heuristics rather than robust performance that parallels human theory of mind abilities 27 .</p>
<p>In the service of the broader multidisciplinary study of machine behaviour 28 , there have been recent calls for a 'machine psychology' 29 that have argued for using tools and paradigms from experimental psychology to systematically investigate the capacities and limits of LLMs 30 .A systematic experimental approach to studying theory of mind in LLMs involves using a diverse set of theory of mind measures, delivering multiple repetitions of each test, and having clearly defined benchmarks of human performance against which to compare 31 .In this Article, we adopt such an approach to test the performance of LLMs in a wide range of theory of mind tasks.We tested the chat-enabled version of GPT-4, the latest LLM in the GPT family of models, and its predecessor ChatGPT-3.5 (hereafter GPT-3.5) in a comprehensive set of psychological tests spanning different theory of mind abilities, from those that are less cognitively demanding for humans such as understanding indirect requests to more cognitively demanding abilities such as recognizing and articulating complex mental states like misdirection or irony 17 .GPT models are closed, evolving systems.In the interest of reproducibility 32 , we also tested the open-weight LLaMA2-Chat models on the same tests.To understand the variability and boundary limitations of LLMs' social reasoning capacities, we exposed each model to multiple repetitions of each test across independent sessions and compared their performance with that of a sample of human participants (total N = 1,907).Using variants of the tests considered, we were able to examine the processes behind the models' successes and failures in these tests.</p>
<p>Results</p>
<p>Theory of mind battery</p>
<p>We selected a set of well-established theory of mind tests spanning different abilities: the hinting task 14 , the false belief task 11,33 , the recognition of faux pas 13 , and the strange stories 15,16 .We also included a test of Article https://doi.org/10.1038/s41562-024-01882-zrecognizing non-ironic control statements but made errors at recognizing ironic utterances (Supplementary Information section 2).Control analysis revealed a significant order effect, whereby GPT-3.5 made more errors on earlier trials than later ones (Supplementary Information section 3).LLaMA2-70B made errors when recognizing both ironic and non-ironic control statements, suggesting an overall poor discrimination of irony.</p>
<p>Faux Pas.On this test, GPT-4 scored notably lower than human levels (Z = −0.40,P = 5.42 × 10 −5 , r = 0.55, 95% CI 0.33-0.71)with isolated ceiling effects on specific items (Supplementary Information section 2).GPT-3.5 scored even worse, with its performance nearly at floor (Z = −0.80,P = 5.95 × 10 −8 , r = 0.72, 95% CI 0.58-0.81)on all items except one.By contrast, LLaMA2-70B outperformed humans (Z = 0.10, P = 0.002, r = 0.44, 95% CI 0.24-0.61)achieving 100% accuracy in all but one run.</p>
<p>The pattern of results for novel items was qualitatively similar (Fig. 1b).Compared with original items, the novel items proved slightly easier for humans (Z = −0.10,P = 0.029, r = 0.29, 95% CI 0.10-0.50)and more difficult for GPT-3.5 (Z = 0.10, P = 0.002, r = 0.69, 95% CI 0.49-0.88),but not for GPT-4 and LLaMA2-70B (P &gt; 0.462; Bayes factor (BF 10 ) of 0.77 and 0.43, respectively).Given the poor performance of GPT-3.5 of the original test items, this difference was unlikely to be explained by a prior familiarity with the original items.These results were robust to alternative coding schemes (Supplementary Information section 5).</p>
<p>Hinting.On this test, GPT-4 performance was significantly better than humans (Z = 0.00, P = 0.040, r = 0.32, 95% CI 0.12-0.50).GPT-3.5 performance did not significantly differ from human performance (Z = 0.00, P = 0.626, r = 0.06, 95% CI 0.01-0.33,BF 10 0.33).Only LLaMA2-70B scored significantly below human levels of performance on this test (Z = −0.20,P = 5.42 × 10 −5 , r = 0.57, 95% CI 0.41-0.72).</p>
<p>Novel items proved easier than original items for both humans (Z = −0.10,P = 0.008, r = 0.34, 95% CI 0.14-0.53)and LLaMA2-70B (Z = −0.20,P = 9.18 × 10 −4 , r = 0.73, 95% CI 0.50-0.87)(Fig. 1b).Scores on novel items did not differ from the original test items for GPT-3.5 (Z = −0.03,P = 0.955, r = 0.24, 95% CI 0.02-0.59,BF 10 0.61) or GPT-4 (Z = −0.10,P = 0.123, r = 0.44, 95% CI 0.07-0.75,BF 10 0.91).Given that better performance on novel items is the opposite of what a prior familiarity explanation would predict, it is likely that this difference for LLaMA2-70B was driven by differences in item difficulty.</p>
<p>Strange stories.GPT-4 significantly outperformed humans on this test (Z = 0.13, P = 1.04 × 10 −5 , r = 0.60, 95% CI 0.46-0.72).The performance of GPT-3.5 did not significantly differ from humans (Z = −0.06,P = 0.110, r = 0.24, 95% CI 0.03-0.44,BF 10 0.47), while LLaMA2-70B scored significantly lower than humans (Z = −0.13,P = 0.005, r = 0.41, 95% CI 0.24-0.60).There were no differences between original and novel items for any model (all P &gt; 0.085; BF 10 : human 0.22, GPT-3.5 1.46, LLaMA2-70B 0.46; the variance for GPT-4 was too low to compute</p>
<p>Article</p>
<p>https://doi.org/10.1038/s41562-024-01882-z a Bayes factor).As reported in Supplementary Information section 6, partial successes were infrequent and more likely for LLaMA2-70B than for other models.</p>
<p>Understanding faux pas</p>
<p>In line with previous findings that GPT models struggle with faux pas 36 , in our battery, faux pas was the only test in which GPT-4 did not match or exceed human performance.Surprisingly, faux pas was also the only test in which LLaMA2-70B, which was otherwise the poorest-performing model, scored better than humans (Fig. 1).</p>
<p>The faux pas test consists of vignettes describing an interaction where one character (the speaker) says something they should not have said, not knowing or not realizing that they should not say it.To understand that a faux pas has occurred, one must represent two mental states: that the speaker does not know that they should not say what they said, and that the person hearing it (the victim) would feel insulted or hurt by what was said.Consider the following example of a story describing a faux pas:</p>
<p>Jill had just moved into a new house.She went shopping with her Mum and bought some new curtains.When Jill had just put them up, her best friend Lisa came round and said, "Oh, those curtains are horrible, I hope you're going to get some new ones."Jill asked, "Do you like the rest of my bedroom?"Following the presentation of the story, four comprehension questions are asked.The first question is, 'In the story, did somebody say something they should not have said?' to which the correct answer is always yes.The second question asks the respondent to report what the person said that they should not have said, and the third question is a comprehension question specific to the content of the story.The fourth and key question, which was our focus for coding, relates to the speaker's belief when they made the inappropriate utterance: 'Did [Lisa] know that [the curtains were new]?'The correct answer to this final question is always no.</p>
<p>Both GPT-4 and GPT-3.5 correctly identified that the victim would feel insulted or hurt, sometimes going so far as to provide additional details about why the utterance might cause offence.However, when asked whether the speaker was aware of the context that made their statement offensive (such as Lisa knowing that Jill had just bought the curtains), they failed to produce a correct answer.A closer look revealed that the overwhelming majority of the errors on this question reported that there was not enough information provided to be sure, for example: Did Lisa know the curtains were new?GPT-4: […] It is unclear from the story whether Lisa knew the curtains were new or not.</p>
<p>Only two responses out of 349 reported that, yes, the character did know.We consider three alternative hypotheses for why GPT models, and specifically GPT-4, fail to answer this question correctly.</p>
<p>The first hypothesis, which we term the failure of inference hypothesis, is that models fail to generate inferences about the mental state of the speaker (note that we refer to inference here not in the sense of the processes by which biological organisms infer hidden states from their environment, but rather as any process of reasoning whereby conclusions are derived from a set of propositional premises).Recognizing a faux pas in this test relies on contextual information beyond that encoded within the story (for example, about social norms).For example, in the above example there is no information in the story to indicate that saying that the newly bought curtains are horrible is inappropriate, but this is a necessary proposition that must be accepted in order to accurately infer the mental states of the characters.This inability to use non-embedded information would fundamentally impair the ability of GPT-4 to compute inferences.</p>
<p>The second hypothesis, which we term the Buridan's ass hypothesis, is that models are capable of inferring mental states but cannot choose between them, as with the eponymous rational agent caught between two equally appetitive bales of hay that starves because it cannot resolve the paradox of making a decision in the absence of a clear preference 37 .Under this hypothesis, GPT models can propose the correct answer (a faux pas) as one among several possible alternatives but do not rank these alternatives in terms of likelihood.In partial support of this hypothesis, responses from both GPT models occasionally indicate that the speaker may not know or remember but present this as one hypothesis among alternatives (Supplementary Information section 5).</p>
<p>The third hypothesis, which we term the hyperconservatism hypothesis, is that GPT models are able both to compute inferences about the mental states of characters and recognise a false belief or lack of knowledge as the likeliest explanation among competing alternatives but refrain from committing to a single explanation out of an excess of caution.GPT models are powerful language generators, but they are also subject to inhibitory mitigation processes 38 .It is possible that such processes could lead to an overly conservative stance where GPT models do not commit to the likeliest explanation despite being able to generate it.</p>
<p>To differentiate between these hypotheses, we devised a variant of the faux pas test where the question assessing performance on the faux pas test was formulated in terms of likelihood (hereafter, the faux pas likelihood test).Specifically, rather than ask whether the speaker knew or did not know, we asked whether it was more likely that the speaker knew or did not know.Under the hyperconservatism hypothesis, GPT models should be able to both make the inference that the speaker did not know and identify it as more likely among alternatives, and so we would expect the models to respond accurately that it was more likely that the speaker did not know.In case of uncertainty or incorrect responses, we further prompted models to describe the most likely explanation.Under the Buridan's ass hypothesis, we expected this question would elicit multiple alternative explanations that would be presented as equally plausible, while under the failure of inference hypothesis, we expected that GPT would not be able to generate the right answer at all as a plausible explanation.</p>
<p>As shown in Fig. 2a, on the faux pas likelihood test GPT-4 demonstrated perfect performance, with all responses identifying without any prompting that it was more likely that the speaker did not know the context.GPT-3.5 also showed improved performance, although it did require prompting in a few instances (~3% of items) and occasionally failed to recognize the faux pas (~9% of items; see Supplementary Information section 7 for a qualitative analysis of response types).</p>
<p>Taken together, these results support the hyperconservatism hypothesis, as they indicate that GPT-4, and to a lesser but still notable extent GPT-3.5, successfully generated inferences about the mental states of the speaker and identified that an unintentional offence was more likely than an intentional insult.Thus, failure to respond correctly to the original phrasing of the question does not reflect a failure of inference, nor indecision among alternatives the model considered equally plausible, but an overly conservative approach that prevented commitment to the most likely explanation.</p>
<p>Testing information integration</p>
<p>A potential confound of the above results is that, as the faux pas test includes only items where a faux pas occurs, any model biased towards attributing ignorance would demonstrate perfect performance without having to integrate the information provided by the story.This potential bias could explain the perfect performance of LLaMA2-70B in the original faux pas test (where the correct answer is always, 'no') as well as GPT-4's perfect and GPT-3.5'sgood performance on the faux Article https://doi.org/10.1038/s41562-024-01882-zpas likelihood test (where the correct answer is always 'more likely that they didn't know').</p>
<p>To control for this, we developed a novel set of variants of the faux pas likelihood test manipulating the likelihood that the speaker knew or did not know (hereafter the belief likelihood test).For each test item, all newly generated for this control study, we created three variants: a 'faux pas' variant, a 'neutral' variant, and a 'knowledge-implied' variant (Methods).In the faux pas variant, the utterance suggested that the speaker did not know the context.In the neutral variant, the utterance suggested neither that they knew nor did not know.In the knowledge-implied variant, the utterance suggested that the speaker knew (for the full text of all items, see Supplementary Appendix 2).</p>
<p>If the models' responses reflect a true discrimination of the relative likelihood of the two explanations (that the person knew versus that they didn't know, hereafter 'knew' and 'didn't know'), then the distribution of 'knew' and 'didn't know' responses should be different across variants.Specifically, relative to the neutral variant, 'didn't know' responses should predominate for the faux pas, and 'knew' responses should predominate for the knowledge-implied variant.If the responses of the models do not discriminate between the three variants, or discriminate only partially, then it is likely that responses are affected by a bias or heuristic unrelated to the story content.</p>
<p>We adapted the three variants (faux pas, neutral and knowledge implied) for six stories, administering each test item separately to each LLM and a new sample of human participants (total N = 900).Responses were coded using a numeric code to indicate which, if either, of the knew/didn't know explanations the response endorsed (−1, didn't know; 0, unsure or impossible to tell; +1, knew).These coded scores were then averaged for each story to give a directional score for each variant such that negative values indicated the model was more likely to endorse the 'didn't know' explanation, while positive values indicated the model was more likely to endorse the 'knew' explanation.These results are shown in Fig. 2b.As expected, humans were more likely to report that the speaker did not know for faux pas than for neutral (χ 2 (2) = 56.20,P = 3.82 × 10 −12 ) and more likely to report that the speaker did know for knowledge implied than for neutral (χ 2 (2) = 143, P = 6.60 × 10 −31 ).Humans also reported uncertainty on a small proportion of trials, with a higher proportion in the neutral condition (28 out of 303 responses) than in the other variants (11 out of 303 for faux pas, and 0 out of 298 for knowledge implied).</p>
<p>Similarly to humans, GPT-4 was more likely to endorse the 'didn't know' explanation for faux pas than for neutral (χ 2 (2) = 109, P = 1.54 × 10 −23 ) and more likely to endorse the 'knew' explanation for knowledge implied than for neutral (χ 2 (2) = 18.10,P = 3.57 × 10 −4 ).GPT-4 was also more likely to report uncertainty in the neutral condition than responding randomly (42 out of 90 responses, versus 6 and 17 in the faux pas and knowledge-implied variants, respectively).</p>
<p>The pattern of responses for GPT-3.5 was similar, with the model being more likely to report that the speaker didn't know for faux pas than for neutral (χ 2 (1) = 8.44, P = 0.007) and more likely that the character knew for knowledge implied than for neutral (χ 2 (1) = 21.50,P = 1.82 × 10 −5 ).Unlike GPT-4, GPT-3.5 never reported uncertainty in response to any variants and always selected one of the two explanations as the likelier even in the neutral condition.</p>
<p>LLaMA2-70B was also more likely to report that the speaker didn't know in response to faux pas than neutral (χ 2 (1) = 20.20,P = 2.81 × 10 −5 ), which was consistent with this model's ceiling performance in the original formulation of the test.However, it showed no differentiation between neutral and knowledge implied (χ 2 (1) = 1.80,P = 0.180, BF 10 0.56).As with GPT-3.5, LLaMA2-70B never reported uncertainty in response to any variants and always selected one of the two explanations as the likelier.</p>
<p>Furthermore, the responses of LLaMA2-70B and, to a lesser extent, GPT-3.5 appeared to be subject to a response bias towards affirming that someone had said something they should not have said.Although the responses to the first question (which involved recognising that there was an offensive remark made) were of secondary interest to our study, it was notable that, although all models could correctly identify that an offensive remark had been made in the faux pas condition (all LLMs 100%, humans 83.61%),only GPT-4 reliably reported that there was no offensive statement in the neutral and knowledge-implied conditions (15.47% and 27.78%, respectively), with similar proportions to human Article https://doi.org/10.1038/s41562-024-01882-zresponses (neutral 19.27%, knowledge implied 30.10%).GPT-3.5 was more likely to report that somebody made an offensive remark in all conditions (neutral 71.11%, knowledge implied 87.78%), and LLaMA2-70B always reported that somebody in the story had made an offensive remark.</p>
<p>Discussion</p>
<p>We collated a battery of tests to comprehensively measure performance in theory of mind tasks in three LLMs (GPT-4, GPT-3.5 and LLaMA2-70B) and compared these against the performance of a large sample of human participants.Our findings validate the methodological approach taken in this study using a battery of multiple tests spanning theory of mind abilities, exposing language models to multiple sessions and variations in both structure and content, and implementing procedures to ensure a fair, non-superficial comparison between humans and machines 35 .This approach enabled us to reveal the existence of specific deviations from human-like behaviour that would have remained hidden using a single theory of mind test, or a single run of each test.</p>
<p>Both GPT models exhibited impressive performance in tasks involving beliefs, intentions and non-literal utterances, with GPT-4 exceeding human levels in the irony, hinting and strange stories.Both GPT-4 and GPT-3.5 failed only on the faux pas test.Conversely, LLaMA2-70B, which was otherwise the poorest-performing model, outperformed humans on the faux pas.Understanding a faux pas involves two aspects: recognizing that one person (the victim) feels insulted or upset and understanding that another person (the speaker) holds a mistaken belief or lacks some relevant knowledge.To examine the nature of models' successes and failures on this test, we developed and tested new variants of the faux pas test in a set of control experiments.</p>
<p>Our first control experiment using a likelihood framing of the belief question (faux pas likelihood test), showed that GPT-4, and to a lesser extent GPT-3.5, correctly identified the mental state of both the victim and the speaker and selected as the most likely explanation the speaker not knowing or remembering the relevant knowledge that made their statement inappropriate.Despite this, both models consistently provided an incorrect response (at least when compared against human responses) when asked whether the speaker knew or remembered this knowledge, responding that there was insufficient information provided.In line with the hyperconservatism hypothesis, these findings imply that, while GPT models can identify unintentional offence as the most likely explanation, their default responses do not commit to this explanation.This finding is consistent with longitudinal evidence that GPT models have become more reluctant to answer opinion questions over time 39 .</p>
<p>Further supporting that the failures of GPT at recognizing faux pas were due to hyperconservatism in answering the belief question rather than a failure of inference, a second experiment using the belief likelihood test showed that GPT responses integrated information in the story to accurately interpret the speaker's mental state.When the utterance suggested that the speaker knew, GPT responses acknowledged the higher likelihood of the 'knew' explanation.LLaMA2-70B, on the other hand, did not differentiate between scenarios where the speaker was implied to know and when there was no information one way or another, raising the concern that the perfect performance of LLaMA2-70B on this task may be illusory.</p>
<p>The pattern of failures and successes of GPT models on the faux pas test and its variants may be the result of their underlying architecture.In addition to transformers (generative algorithms that produce text output), GPT models also include mitigation measures to improve factuality and avoid users' overreliance on them as sources 38 .These measures include training to reduce hallucinations, the propensity of GPT models to produce nonsensical content or fabricate details that are not true in relation to the provided content.Failure on the faux pas test may be an exercise of caution driven by these mitigation measures, as passing the test requires committing to an explanation that lacks full evidence.This caution can also explain differences between tasks: both the faux pas and hinting tests require speculation to generate correct answers from incomplete information.However, while the hinting task allows for open-ended generation of text in ways to which LLMs are well suited, answering the faux pas test requires going beyond this speculation in order to commit to a conclusion.</p>
<p>The cautionary epistemic policy guiding the responses of GPT models introduces a fundamental difference in the way that humans and GPT models respond to social uncertainty 40 .In humans, thinking is, first and last, for the sake of doing 41,42 .Humans generally find uncertainty in social environments to be aversive and will incur additional costs to reduce it 43 .Theory of mind is crucial in reducing such uncertainty; the ability to reason about mental states-in combination with information about context, past experience and knowledge of social norms-helps individual reduce uncertainty and commit to likely hypotheses, allowing for successful navigation of the social environment as active agents 44,45 .GPT models, on the other hand, respond conservatively despite having access to tools to reduce uncertainty.The dissociation we describe between speculative reasoning and commitment mirrors recent evidence that, while GPT models demonstrate sophisticated and accurate performance in reasoning tasks about belief states, they struggle to translate this reasoning into strategic decisions and actions 46 .</p>
<p>These findings highlight a dissociation between competence and performance 35 , suggesting that GPT models may be competent, that is, have the technical sophistication to compute mentalistic-like inferences but perform differently from humans under uncertain circumstances as they do not compute these inferences spontaneously to reduce uncertainty.Such a distinction can be difficult to capture with quantitative approaches that code only for target response features, as machine failures and successes are the result of non-human-like processes 30 (see Supplementary Information section 7 for a preliminary qualitative breakdown of how GPT models' successes on the new version of the faux pas test may not necessarily reflect perfect or human-like reasoning).</p>
<p>While LLMs are designed to emulate human-like responses, this does not mean that this analogy extends to the underlying cognition giving rise to those responses 47 .In this context, our findings imply a difference in how humans and GPT models trade off the costs associated with social uncertainty against the costs associated with prolonged deliberation 48 .This difference is perhaps not surprising considering that resolving uncertainty is a priority for brains adapted to deal with embodied decisions, such as deciding whether to approach or avoid, fight or flight, or cooperate or defect.GPT models and other LLMs do not operate within an environment and are not subject to the processing constraints that biological agents face to resolve competition between action choices, so may have limited advantages in narrowing the future prediction space 46,49,50 .</p>
<p>The dis-embodied cognition of GPT models could explain failures in recognizing faux pas, but they may also underlie their success on other tests.One example is the false belief test, one of the most widely used tools so far for testing the performance of LLMs on social cognitive tasks 19,[21][22][23]25,51,52 . In this est, participants are presented with a story where a character's belief about the world (the location of the item) differs from the participant's own belief.The challenge in these stories is not remembering where the character last saw the item but rather in reconciling the incongruence between conflicting mental states.This is challenging for humans, who have their own perspective, their own sense of self and their own ability to track out-of-sight objects.However, if a machine does not have its own self-perspective because it is not subject to the constraints of navigating a body through an environment, as with GPT 53 , then tracking the belief of a character in a story does not pose the same challenge.</p>
<p>An important direction for future research will be to examine the impact of these non-human decision behaviours on second-person, Article https://doi.org/10.1038/s41562-024-01882-zreal-time human-machine interactions 54,55 .Failure of commitment by GPT models, for example, may lead to negative affect in human conversational partners.However, it may also foster curiosity 40 .Understanding how GPTs' performance on mentalistic inferences (or their absences) influences human social cognition in dynamically unfolding social interactions is an open challenge for future work.</p>
<p>The LLM landscape is fast-moving.Our findings highlight the importance of systematic testing and proper validation in human samples as a necessary foundation.As artificial intelligence (AI) continues to evolve, it also becomes increasingly important to heed calls for open science and open access to these models 32 .Direct access to the parameters, data and documentation used to construct models can allow for targeted probing and experimentation into the key parameters affecting social reasoning, informed by and building on comparisons with human data.</p>
<p>As such, open models can not only serve to accelerate the development of future AI technologies but also serve as models of human cognition.</p>
<p>Methods</p>
<p>Ethical compliance</p>
<p>The research was approved by the local ethical committee (ASL 3 Genovese; protocol no.192REG2015) and was carried out in accordance with the principles of the revised Helsinki Declaration.</p>
<p>Experimental model details</p>
<p>We tested two versions of OpenAI's GPT: version 3.5, which was the default model at the time of testing, and version 4, which was the state-of-the-art model with enhanced reasoning, creativity and comprehension relative to previous models (https://chat.openai.com/).Each test was delivered in a separate chat: GPT is capable of learning within a chat session, as it can remember both its own and the user's previous messages to adapt its responses accordingly, but it does not retain this memory across new chats.As such, each new iteration of a test may be considered a blank slate with a new naive participant.The dates of data collection for the different stages are reported in Table 1.</p>
<p>Three LLaMA2-Chat models were tested.These models were trained on sets of different sizes: 70, 13 and 7 billion tokens.All LLaMA2-Chat responses were collected using set parameters with the prompt, 'You are a helpful AI assistant', a temperature of 0.7, the maximum number of new tokens set at 512, a repetition penalty of 1.1, and a Top P of 0.9.Langchain's conversation chain was used to create a memory context within individual chat sessions.Responses from all LLaMA2-Chat models were found to include a number of non-codable responses (for example, repeating the question without answering it), and these were regenerated individually and included with the full response set.For the 70B model, these non-responses were rare, but for the 13B and 7B models they were common enough to cause concern about the quality of these data.As such, only the responses of the 70B model are reported in the main manuscript and a comparison of this model against the smaller two is reported in Supplementary Information section 1.Details and dates of data collection are reported in Table 1.</p>
<p>For each test, we collected 15 sessions for each LLM.A session involved delivering all items of a single test within the same chat window.GPT-4 was subject to a 25-message limit per 3 h; to minimize interference, a single experimenter delivered all tests for GPT-4, while four other experimenters shared the duty of collecting responses from GPT-3.5.</p>
<p>Human participants were recruited online through the Prolific platform and the study was hosted on SoSci.We recruited native English speakers between the ages of 18 and 70 years with no history of psychiatric conditions and no history of dyslexia in particular.Further demographic data were not collected.We aimed to collect around 50 participants per test (theory of mind battery) or item (belief likelihood test, false belief perturbations).Thirteen participants who appeared to have generated their answers using LLMs or whose responses did not answer the questions were excluded.The final human sample was N = 1,907 (Table 1).All participants provided informed consent through the online survey and received monetary compensation in return for their participation at a rate of GBP£12 h −1 .</p>
<p>Theory of mind battery</p>
<p>We selected a series of tests typically used in evaluating theory of mind capacity in human participants.</p>
<p>False belief.</p>
<p>False belief assess the ability to infer that another person possesses knowledge that differs from the participant's own (true) knowledge of the world.These tests consist of test items that follow a particular structure: character A and character B are together, character A deposits an item inside a hidden location (for example, a box), character A leaves, character B moves the item to a second hidden location (for example, a cupboard) and then character A returns.The question asked to the participant is: when character A returns, will they look for the item in the new location (where it truly is, matching the participant's true belief) or the old location (where it was, matching character A's false belief)?</p>
<p>In addition to the false belief condition, the test also uses a true belief control condition, where rather than move the item that character A hid, character B moves a different item to a new location.This is important for interpreting failures of false belief attribution as they ensure that any failures are not due to a recency effect (referring to the last location reported) but instead reflect an accurate belief tracking.</p>
<p>We adapted four false/true belief scenarios from the sandbox task used by Bernstein 33 and generated three novel items, each with false and</p>
<p>Article</p>
<p>https://doi.org/10.1038/s41562-024-01882-ztrue belief versions.These novel items followed the same structure as the original published items but with different details such as names, locations or objects to control for familiarity with the text of published items.Two story lists (false belief A, false belief B) were generated for this test such that each story only appeared once within a testing session and alternated between false and true belief depending on the session.In addition to the standard false/true belief scenarios, two additional catch stories were tested that involved minor alterations to the story structure.The results of these items are not reported here as they go beyond the goals of the current study.</p>
<p>Irony.Comprehending an ironic remark requires inferring the true meaning of an utterance (typically the opposite of what is said) and detecting the speaker's mocking attitude, and this has been raised as a key challenge for AI and LLMs 19 .Irony comprehension items were adapted from an eye-tracking study 34 in which participants read vignettes where a character made an ironic or non-ironic statement.Twelve items were taken from these stimuli that in the original study were used as comprehension checks.Items were abbreviated to end following the ironic or non-ironic utterance.</p>
<p>Two story lists were generated for this test (irony A, irony B) such that each story only appeared once within a testing session and alternated between ironic and non-ironic depending on the session.Responses were coded as 1 (correct) or 0 (incorrect).During coding, we noted some inconsistencies in the formulation of both GPT models' responses where in response to the question of whether the speaker believed what they had said, they might respond with, 'Yes, they did not believe that…'.Such internally contradictory responses, where the models responded with a 'yes' or 'no' that was incompatible with the followup explanation, were coded on the basis of whether or not the explanation showed appreciation of the irony-the linguistic failures of these models in generating a coherent answer are not of direct interest to the current study as these failures (1) were rare and (2) did not render the responses incomprehensible.</p>
<p>Faux pas.The faux pas test 13 presents a context in which one character makes an utterance that is unintentionally offensive to the listener because the speaker does not know or does not remember some key piece of information.</p>
<p>Following the presentation of the scenario, we presented four questions: These questions were asked at the same time as the story was presented.Under the original coding criteria, participants must answer all four questions correctly for their answer to be considered correct.However, in the current study we were interested primarily in the response to the final question testing whether the responder understood the speaker's mental state.When examining the human data, we noticed that several participants responded incorrectly to the first item owing to an apparent unwillingness to attribute blame (for example 'No, he didn't say anything wrong because he forgot').To focus on the key aspect of faux pas understanding that was relevant to the current study, we restricted our coding to only the last question (1 (correct if the answer was no) or 0 (for anything else); see Supplementary Information section 5 for an alternative coding that follows the original criteria, as well as a recoding where we coded as correct responses where the correct answer was mentioned as a possible explanation but was not explicitly endorsed).</p>
<p>As well as the 10 original items used in Baron-Cohen et al. 13 , we generated five novel items for this test that followed the same structure and logic as the original items, resulting in 15 items overall.</p>
<p>Hinting task.The hinting task 14 assesses the understanding of indirect speech requests through the presentation of ten vignettes depicting everyday social interactions that are presented sequentially.Each vignette ends with a remark that can be interpreted as a hint.</p>
<p>A correct response identifies both the intended meaning of the remark and the action that it is attempting to elicit.In the original test, if the participant failed to answer the question fully the first time, they were prompted with additional questioning 14,56 .In our adapted implementation, we removed this additional questioning and coded responses as a binary (1 (correct) or 0 (incorrect)) using the evaluation criteria listed in Gil et al. 56 .Note that this coding offers more conservative estimates of hint comprehension than in previous studies.</p>
<p>In addition to 10 original items sourced from Corcoran 14 , we generated a further 6 novel hinting test items, resulting in 16 items overall.</p>
<p>Strange stories.The strange stories 15,16 offer a means of testing more advanced mentalizing abilities such as reasoning about misdirection, manipulation, lying and misunderstanding, as well as second-or higher-order mental states (for example, A knows that B believes X…).The advanced abilities that these stories measure make them suitable for testing higher-functioning children and adults.In this test, participants are presented with a short vignette and are asked to explain why a character says or does something that is not literally true.</p>
<p>Each question comes with a specific set of coding criteria and responses can be awarded 0, 1 or 2 points depending on how fully it explains the utterance and whether or not it explains it in mentalistic terms 16 .See Supplementary Information section 6 for a description of the frequency of partial successes.</p>
<p>In addition to the 8 original mental stories, we generated 4 novel items, resulting in 12 items overall.The maximum number of points possible was 24, and individual session scores were converted to a proportional score for analysis.</p>
<p>Testing protocol.For the theory of mind battery, the order of items was set for each test, with original items delivered first and novel items delivered last.Each item was preceded by a preamble that remained consistent across all tests.This was then followed by the story description and the relevant question(s).After each item was delivered, the model would respond and then the session advanced to the next item.</p>
<p>For GPT models, items were delivered using the chat web interface.For LLaMA2-Chat models, delivery of items was automated through a custom script.For humans, items were presented with free text response boxes on separate pages of a survey so that participants could write out their responses to each question (with a minimum character count of 2).</p>
<p>Faux pas likelihood test</p>
<p>To test alternative hypotheses of why the tested models performed poorly at the faux pas test, we ran a follow-up study replicating just the faux pas test.This replication followed the same procedure as the main study with one major difference.</p>
<p>The original wording of the question was phrased as a straightforward yes/no question that tested the subject's awareness of a speaker's false belief (for example, 'Did Richard remember James had given him the toy aeroplane for his birthday?').To test whether the low scores on this question were due to the models' refusing to commit to a single explanation in the face of ambiguity, we reworded this to ask in terms Article https://doi.org/10.1038/s41562-024-01882-z of likelihood: 'Is it more likely that Richard remembered or did not remember that James had given him the toy aeroplane for his birthday?'</p>
<p>Another difference from the original study was that we included a follow-up prompt in the rare cases where the model failed to provide clear reasoning on an incorrect response.The coding criteria for this follow-up were in line with coding schemes used in other studies with a prompt system 14 , where an unprompted correct answer was given 2 points, a correct answer following a prompt was given 1 point and incorrect answers following a prompt were given 0 points.These points were then rescaled to a proportional score to allow comparison against the original wording.</p>
<p>During coding by the human experimenters, a qualitative description of different subtypes of response (beyond 0-1-2 points) emerged, particularly noting recurring patterns in responses that were marked as successes.This exploratory qualitative breakdown is reported along with further detail on the prompting protocol in Supplementary Information section 7.</p>
<p>Belief likelihood test</p>
<p>To manipulate the likelihood that the speaker knew or did not know, we developed a new set of variants of the faux pas likelihood test.For each test item, all newly generated for this control study, we created three variants: a faux pas variant, a neutral variant and a knowledge-implied variant.In the faux pas variant, the utterance suggested that the speaker did not know the context.In the neutral variant, the utterance suggested neither that they knew nor did not know.In the knowledge-implied variant, the utterance suggested that the speaker knew (for the full text of all items, see Supplementary Appendix 2).For each variant, the core story remained unchanged, for example:</p>
<p>Michael was a very awkward child when he was at high school.He struggled with making friends and spent his time alone writing poetry.However, after he left he became a lot more confident and sociable.At his ten-year high school reunion he met Amanda, who had been in his English class.Over drinks, she said to him, followed by the utterance, which varied across conditions: Faux Pas: 'I don't know if you remember this guy from school.He was in my English class.He wrote poetry and he was super awkward.I hope he isn't here tonight.'</p>
<p>Neutral:</p>
<p>'Do you know where the bar is?'</p>
<p>Knowledge implied:</p>
<p>'Do you still write poetry?'</p>
<p>The belief likelihood test was administered in the same way as with previous tests with the exception that responses were kept independent so that there was no risk of responses being influenced by other variants.For ChatGPT models, this involved delivering each item within a separate chat session for 15 repetitions of each item.For LLaMA2-70B, this involved removing the Langchain conversation chain allowing for within-session memory context.Human participants were recruited separately to answer a single test item, with at least 50 responses collected for each item (total N = 900).All other details of the protocol were the same.</p>
<p>Quantification and statistical analysis</p>
<p>Response coding.After each session in the theory of mind battery and faux pas likelihood test, the responses were collated and coded by five human experimenters according to the pre-defined coding criteria for each test.Each experimenter was responsible for coding 100% of sessions for one test and 20% of sessions for another.Inter-coder per cent agreement was calculated on the 20% of shared sessions, and items where coders showed disagreement were evaluated by all raters and recoded.The data available on the OSF are the results of this recoding.Experimenters also flagged individual responses for group evaluation if they were unclear or unusual cases, as and when they arose.Inter-rater agreement was computed by calculating the item-wise agreement between coders as 1 or 0 and using this to calculate a percentage score.Initial agreement across all double-coded items was over 95%.The lowest agreement was for the human and GPT-3.5 responses of strange stories, but even here agreement was over 88%.Committee evaluation by the group of experimenters resolved all remaining ambiguities.</p>
<p>For the belief likelihood test, responses were coded according to whether they endorsed the 'knew' explanation or 'didn't know' explanation, or whether they did not endorse either as more likely than the other.Outcomes 'knew', 'unsure' and 'didn't know' were assigned a numerical coding of +1, 0 and −1, respectively.GPT models adhered closely to the framing of the question in their answer, but humans were more variable and sometimes provided ambiguous responses (for example, 'yes', 'more likely' and 'not really') or did not answer the question at all ('It doesn't matter' and 'She didn't care').These responses were rare, constituting only ~2.5% of responses and were coded as endorsing the 'knew' explanation if they were affirmative ('yes') and the 'didn't know' explanation if they were negative.</p>
<p>Statistical analysis</p>
<p>Comparing LLMs against human performance.Scores for individual responses were scaled and averaged to obtain a proportional score for each test session in order to create a performance metric that could be compared directly across different theory of mind tests.Our goal was to compare LLMs' performance across different tests against human performance to see how these models performed on theory of mind tests relative to humans.For each test, we compared the performance of each of the three LLMs against human performance using a set of Holm-corrected two-way Wilcoxon tests.Effect sizes for Wilcoxon tests were calculated by dividing the test statistic Z by the square root of the total sample size, and 95% CIs of the effect size were bootstrapped over 1,000 iterations.All non-significant results were further examined using corresponding Bayesian tests represented as a Bayes factor (BF 10 ) under continuous prior distribution (Cauchy prior width r = 0.707).Bayes factors were computed in JASP 0.18.3 with a random seed value of 1.The results of the false belief test were not subjected to inferential statistics owing to the ceiling performance and lack of variance across models.</p>
<p>Novel items.</p>
<p>For each publicly available test (all tests except for irony), we generated novel items that followed the same logic as the original text but with different details and text to control for low-level familiarity with the scenarios through inclusion in the LLM training sets.For each of these tests, we compared the performance of all LLMs on these novel items against the validated test items using Holm-corrected two-way Wilcoxon tests.Non-significant results were followed up with corresponding Bayesian tests in JASP.Significantly poorer performance on novel items than original items would indicate a strong likelihood that the good performance of a language model can be attributed to inclusion of these texts in the training set.Note that, while the open-ended format of more complex tasks like hinting and strange stories makes this a convincing control for these tests, they are of limited strength for tasks like false belief and faux pas that use a regular internal structure that make heuristics or 'Clever Hans' solutions possible 27,36 .</p>
<p>Belief likelihood test.We calculated the count frequency of the different response types ('didn't know', 'unsure' and 'knew') for each variant and each model.Then, for each model we conducted two chi-square nature portfolio | reporting summary April 2023 tidyr_1.3.0 dplyr_1.1.4ggtext_0.1.2Null results reported in the main manuscript were subjected to follow-up corresponding Bayesian analyses to compute Bayes Factors (BF10).This analysis was done using JASP v0.18.3 (JASP Team, 2024)   For manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and reviewers.We strongly encourage code deposition in a community repository (e.g.GitHub).See the Nature Portfolio guidelines for submitting code &amp; software for further information.</p>
<p>nature portfolio | reporting summary</p>
<p>April 2023</p>
<p>Timing and spatial scale Indicate the start and stop dates of data collection, noting the frequency and periodicity of sampling and providing a rationale for these choices.If there is a gap between collection periods, state the dates for each sample cohort.Specify the spatial scale from which the data are taken nature portfolio | reporting summary</p>
<p>April 2023</p>
<p>Peak calling parameters Specify the command line program and parameters used for read mapping and peak calling, including the ChIP, control and index files used.</p>
<p>nature portfolio | reporting summary</p>
<p>April 2023</p>
<p>Normalization</p>
<p>If data were normalized/standardized, describe the approach(es): specify linear or non-linear and define image types used for transformation OR indicate that data were not normalized and explain rationale for lack of normalization.</p>
<p>Normalization template</p>
<p>Describe the template used for normalization/transformation, specifying subject space or group standardized space (e.g.original Talairach, MNI305, ICBM152) OR indicate that the data were not normalized.</p>
<p>Noise and artifact removal</p>
<p>Describe your procedure(s) for artifact and structured noise removal, specifying motion parameters, tissue signals and physiological signals (heart rate, respiration).</p>
<p>Volume censoring</p>
<p>Define your software and/or method and criteria for volume censoring, and state the extent of such censoring.</p>
<p>Statistical modeling &amp; inference</p>
<p>Model type and settings Specify type (mass univariate, multivariate, RSA, predictive, etc.) and describe essential details of the model at the first and second levels (e.g.fixed, random or mixed effects; drift or auto-correlation).</p>
<p>Effect(s) tested</p>
<p>Define precise effect in terms of the task or stimulus conditions instead of psychological concepts and indicate whether ANOVA or factorial designs were used.</p>
<p>Specify type of analysis:</p>
<p>Whole</p>
<p>Graph analysis</p>
<p>Report the dependent variable and connectivity measure, specifying weighted graph or binarized graph, subject-or group-level, and the global and/or node summaries used (e.g.clustering coefficient, efficiency, etc.).</p>
<p>Multivariate modeling and predictive analysis Specify independent variables, features extraction and dimension reduction, model, training and evaluation metrics.</p>
<p>Fig. 1 |
1
Fig. 1 | Performance of human (purple), GPT-4 (dark blue), GPT-3.5 (light blue) and LLaMA2-70B (green) on the battery of theory of mind tests.a, Original test items for each test showing the distribution of test scores for individual sessions and participants.Coloured dots show the average response score across all test items for each individual test session (LLMs) or participant (humans).Black dots indicate the median for each condition.P values were computed from Holmcorrected Wilcoxon two-way tests comparing LLM scores (n = 15 LLM observations) against human scores (irony, N = 50 human participants; faux pas, N = 51 human participants; hinting, N = 48 human participants; strange stories, N = 50 human participants).Tests are ordered in descending order of human performance.b, Interquartile ranges of the average scores on the original published items (dark colours) and novel items (pale colours) across each test (for LLMs, n = 15 LLM observations; for humans, false belief, N = 49 human participants; faux pas, N = 51 human participants; hinting, N = 48 human participants; strange stories, N = 50 human participants).Empty diamonds indicate the median scores, and filled circles indicate the upper and lower bounds of the interquartile range.P values shown are from Holm-corrected Wilcoxon two-way tests comparing performance on original items against the novel items generated as controls for this study.</p>
<p>Fig. 2 |
2
Fig. 2 | Results of the variants of the faux pas test.a, Scores of the two GPT models on the original framing of the faux pas question ('Did they know…?') and the likelihood framing ('Is it more likely that they knew or didn't know…?').Dots show average score across trials (n = 15 LLM observations) on particular items to allow comparison between the original faux pas test and the new faux pas likelihood test.Halfeye plots show distributions, medians (black points), 66% (thick grey lines) and 99% quantiles (thin grey lines) of the response scores on different items (n = 15 different stories involving faux pas).b, Response scores to three variants of the faux pas test: faux pas (pink), neutral (grey) and</p>
<p>Table 1 | Data collection details for each model
1TestModelN/nItemsDates of datacollectionHuman2507-16June to July 2023GPT-4757-16April 2023Theory of mindbatteryGPT-3.5757-16April 2023LLaMA2757-16October to November2023GPT-41515April to May 2023Faux pas likelihoodGPT-3.51515April to May 2023testLLaMA21515October to November2023Human9001November 2023GPT-42701October to November2023Belief likelihoodtestGPT-3.52701October to November2023LLaMA22701October to November2023Item order analysis GPT-3.51812-15April to May 2023Human7571November 2023GPT-42251October to November2023False beliefperturbationsGPT-3.52251October to November2023LLaMA22251October to November2023n(independent observations of LLM responses), number of items administered to eachindividual observation (ranges where multiple tests were administered) and dates of datacollection. Information is the same for LlaMA2-70B, LlaMA2-13B and LlaMA2-7B. Analysis ofthe data in the item order analysis and false belief perturbations is reported in SupplementaryInformation sections 3 and 4.
N, human participants; n, independent LLM observations.Details of data collection for each model at each stage of the study are shown, including N (human participants)/</p>
<p>Specify voxel-wise or cluster-wise and report all relevant parameters for cluster-wise methods.CorrectionDescribe the type of correction and how it is obtained for multiple comparisons (e.g.FWE, FDR, permutation or Monte Carlo).
brainROI-basedBothStatistic type for inference(See Eklund et al. 2016)Models &amp; analysisn/a Involved in the studyFunctional and/or effective connectivityGraph analysisMultivariate modeling or predictive analysisFunctional and/or effective connectivity
Report the measures of dependence used and the model details (e.g.Pearson correlation, partial correlation, mutual information).</p>
<p>AcknowledgementsThis work is supported by the European Commission through Project ASTOUND (101071191-HORIZON-EIC-2021-PATHFINDERCHALLENGES-01 to A.R., G.M., C.B. and S.P.).J.W.A.S. was supported by a Humboldt Research Fellowship for Experienced Researchers provided by the Alexander von Humboldt Foundation.The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.Data availabilityAll resources are available on a repository stored on the Open Science Framework (OSF) under a Creative Commons Attribution Non-Commercial 4.0 International (CC-BY-NC) license at https://osf.io/fwj6v.This repository contains all test items, data and code reported in this study.Test items and data are available in an Excel file that includes the text of every item delivered in each test, the full text responses to each item and the code assigned to each response.This file is available at https://osf.io/dbn92Source data are provided with this paper.Code availabilityThe code used for all analysis in the main manuscript and Supplementary Information is included as a Markdown file at https://osf.io/fwj6v.The data used by the analysis files are available as a number of CSV files under 'scored_data/' in the repository, and all materials necessary for replicating the analysis can be downloaded as a single .zipfile within the main repository titled 'Full R Project Code.zip' at https://osf.io/j3vhq.Data Policy information about availability of dataAll manuscripts must include a data availability statement.This statement should provide the following information, where applicable:-Accession codes, unique identifiers, or web links for publicly available datasets -A description of any restrictions on data availability -For clinical datasets or third party data, please ensure that the statement adheres to our policy All data reported in the current study can be found in an OSF repository under a Creative Commons Attribution Non-Commercial 4.0 International license (CC-BY-NC).The repository can be accessed at the following URL: https://osf.io/fwj6v/The full text of question items, the full text of responses from GPT models, LLaMA2 models, and human participants, and the scores assigned to each response can be downloaded as a single file from the following URL: https://osf.io/dbn92Data files with scores alone, which can be used to recreate the analysis, are stored in the OSF repository in the folder scored_data/Data exclusionsIf no data were excluded from the analyses, state so OR if data were excluded, describe the exclusions and the rationale behind them, indicating whether exclusion criteria were pre-established.Data qualityDescribe the methods used to ensure data quality in full detail, including how many peaks are at FDR 5% and above 5-fold enrichment.FundingOpen access funding provided by Universitätsklinikum Hamburg-Eppendorf (UKE).Articlehttps://doi.org/10.1038/s41562-024-01882-ztests that compared the distribution of these categorical responses to the faux pas variant against the neutral, and to the neutral variant against the knowledge implied.A Holm correction was applied to the eight chi-square tests to account for multiple comparisons.The non-significant result was further examined with a Bayesian contingency table in JASP.Reporting summaryFurther information on research design is available in the Nature Portfolio Reporting Summary linked to this article.Author contributionsCompeting interestsThe authors declare no competing interests.Additional informationSupplementary informationThe online version contains supplementary material available at https://doi.org/10.1038/s41562-024-01882-z.Reporting SummaryNature Portfolio wishes to improve the reproducibility of the work that we publish.This form provides structure for consistency and transparency in reporting.For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.StatisticsFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.n/a ConfirmedThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement A statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedlyThe statistical test(s) used AND whether they are one-or two-sided Only common tests should be described solely by name; describe more complex techniques in the Methods section.A description of all covariates testedA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons A full description of the statistical parameters including central tendency (e.g.means) or other basic estimates (e.g.regression coefficient) AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g.confidence intervals) For null hypothesis testing, the test statistic (e.g.F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted Give P values as exact values whenever suitable.For Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings For hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes Estimates of effect sizes (e.g.Cohen's d, Pearson's r), indicating how they were calculated Our web collection on statistics for biologists contains articles on many of the points above.Software and codePolicy information about availability of computer code Data collection Human behavioural data was collected in online experiments using the Prolific platform directing to a survey hosted on the SoSci platform.Data from GPT models was collected through the chat web interface at http://chat.openai.com.A custom script automated the delivery of questions and collection of data for LLaMA2-Chat models, which are available from https://www.llama2.ai/Data analysisWe used R for data analysis and for creating the figures R version 4.1.Research involving human participants, their data, or biological material Policy information about studies with human participants or human data.See also policy information about sex, gender (identity/presentation), and sexual orientation and race, ethnicity and racism.Reporting on sex and genderData on sex and gender were not collected.Reporting on race, ethnicity, or other socially relevant groupingsData on race and ethnicity were not collected.Population characteristicsWe recruited native English speakers between the ages of 18 and 70 with no history of psychiatric conditions and no history of dyslexia.Further demographic data were not collected.RecruitmentParticipants were recruited through the online platform Prolific and were compensated at an adjusted rate of GBP£12/hr (between £2-£6).To our knowledge, there were no significant sources of self-selection bias that would be likely to impact the study findings as a result of this recruitment procedure.Ethics oversightThe research was approved by the local ethics committee (ASL 3 Genovese) and was carried out in accordance with the principles of the revised Helsinki Declaration.Note that full information on the approval of the study protocol must also be provided in the manuscript.Field-specific reportingPlease select the one below that is the best fit for your research.If you are not sure, read the appropriate sections before making your selection.Life sciencesBehavioural &amp; social sciences Ecological, evolutionary &amp; environmental sciencesFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdfLife sciences study designAll studies must disclose on these points even when the disclosure is negative.Sample sizeDescribe how sample size was determined, detailing any statistical methods used to predetermine sample size OR if no sample-size calculation was performed, describe how sample sizes were chosen and provide a rationale for why these sample sizes are sufficient.Data exclusions Describe any data exclusions.If no data were excluded from the analyses, state so OR if data were excluded, describe the exclusions and the rationale behind them, indicating whether exclusion criteria were pre-established.ReplicationDescribe the measures taken to verify the reproducibility of the experimental findings.If all attempts at replication were successful, confirm this OR if there are any findings that were not replicated or cannot be reproduced, note this and describe why.Randomization Describe how samples/organisms/participants were allocated into experimental groups.If allocation was not random, describe how covariates were controlled OR if this is not relevant to your study, explain why.April 2023Blinding Describe whether the investigators were blinded to group allocation during data collection and/or analysis.If blinding was not possible, describe why OR explain why blinding was not relevant to your study.Behavioural &amp; social sciences study designAll studies must disclose on these points even when the disclosure is negative.Study descriptionThe data consist of full-text responses to questions on a set of Theory of Mind tests.Data reported in the manuscript are quantitative numeric scores assigned to each text response according to published coding criteria, with any deviations from validated procedures clearly highlighted in the Methods of the main manuscript.The design is a between-samples comparison of three Large Language Models (LLMs) against a baseline sample of human respondents.Research sampleLLMs: GPT-4, GPT-3.5, LLaMA2-70B (and other LLaMA2 models reported in Supplementary Information): 15 administrations of each test (sessions); Humans: target N of 50 unique participants for each test, total N=1907 (between-subjects).No additional demographic information was collected, but only native English speakers between 18 and 70 with no history of dyslexia or psychiatric conditions were recruited in order to ensure that they could complete the task and read the stories.We did not specify particular demographics or collect this data because the main comparison of interest was human vs. LLM performance and we had no reason to build a priori hypotheses about specific demographics.Recruitment was not restricted to any country and was not restricted to reflect a representative distribution of UK or US census data.Sampling strategyConvenience sample through the Prolific platform.Participants were paid GBP£12/hr for participation (between £2-£6, depending on the test).The sample size was set based on the control adult sample size of White et al. (2009), which recruited 40 neurotypical adults for an update and validation of the Strange Stories task (which, as the most difficult task of the battery, we considered the most likely to show variability).To account for any data quality issues posed by online data collection, we rounded up the target sample size to N=50 per test.Data collectionFor each test we collected 15 sessions for each LLM and ~50 human subjects through Prolific.GPT models were tested through the OpenAI ChatGPT web interface, and a session involved delivering all items of a single test within the same chat window.LLaMA models were tested using Langchain using set parameters with the prompt, "You are a helpful AI assistant", a temperature of 0.7, the maximum number of new tokens set at 512, a repetition penalty of 1.1, and a top P of 0.9.For humans, all items were presented sequentially through an online survey built and hosted through the SoSci platform.Experimenters were not blinded to the experimental conditions as there was no reciprocal interaction with the participants.In the case of the Faux Pas Likelihood test, which included the experimenter delivering a follow-up prompt in the case of unclear reasoning on an incorrect answer from GPT models, criteria for deciding to deliver the follow-up were set a priori and evaluated afterwards by other experimenters to check that the prompt had been valid.TimingThe GPT data on the full battery reported in the main manuscript and in the supplementary material were collected betweenData exclusionsThirteen (13) human subjects were excluded from final analysis following initial examination of the data.Theory of Mind Battery: two (2) subjects who used GPT or another LLM to answer the questions and one (1) subject who just responded 'Yes' to every question; Belief Likelihood Test: seven (7) participants who were believed to use GPT or another LLM to generate their responses; False Belief Perturbations: three (3) participants who were believed to use GPT or another LLM to generate their responses.Non-participationNo participants dropped out or declined participation.RandomizationParticipants were not assigned to experimental groups, but volunteered to complete one of the five Theory of Mind tests.This was a random opportunity sample, and individuals who had participated in one test were excluded from participating again.Ecological, evolutionary &amp; environmental sciences study designAll studies must disclose on these points even when the disclosure is negative.Study descriptionBriefly describe the study.For quantitative data include treatment factors and interactions, design structure (e.g.factorial, nested, hierarchical), nature and number of experimental units and replicates.Research sampleSampling strategyNote the sampling procedure.Describe the statistical methods that were used to predetermine sample size OR if no sample-size calculation was performed, describe how sample sizes were chosen and provide a rationale for why these sample sizes are sufficient.Data collectionDescribe the data collection procedure, including who recorded the data and how.ReproducibilityDescribe the measures taken to verify the reproducibility of experimental findings.For each experiment, note whether any attempts to repeat the experiment failed OR state that all attempts to repeat the experiment were successful.RandomizationDescribe how samples/organisms/participants were allocated into groups.If allocation was not random, describe how covariates were controlled.If this is not relevant to your study, explain why.BlindingDescribe the extent of blinding used during data acquisition and analysis.If blinding was not possible, describe why OR explain why blinding was not relevant to your study.Did the study involve field work?Yes NoField work, collection and transportField conditionsDescribe the study conditions for field work, providing relevant parameters (e.g.temperature, rainfall).LocationState the location of the sampling or experiment, providing relevant parameters (e.g.latitude and longitude, elevation, water depth).Access &amp; import/export Describe the efforts you have made to access habitats and to collect and import/export your samples in a responsible manner and in compliance with local, national and international laws, noting any permits that were obtained (give the name of the issuing authority, the date of issue, and any identifying information).DisturbanceDescribe any disturbance caused by the study and how it was minimized.Reporting for specific materials, systems and methodsWe require information from authors about some types of materials, experimental systems and methods used in many studies.Here, indicate whether each material, system or method listed is relevant to your study.ValidationDescribe the validation of each primary antibody for the species and application, noting any validation statements on the manufacturer's website, relevant citations, antibody profiles in online databases, or data provided in the manuscript.Eukaryotic cell linesPolicy information about cell lines and Sex and Gender in ResearchCell line source(s)State the source of each cell line used and the sex of all primary cell lines and cells derived from human participants or vertebrate models.AuthenticationDescribe the authentication procedures for each cell line used OR declare that none of the cell lines used were authenticated.nature portfolio | reporting summaryApril 2023Mycoplasma contaminationConfirm that all cell lines tested negative for mycoplasma contamination OR describe the results of the testing for mycoplasma contamination OR declare that the cell lines were not tested for mycoplasma contamination.Commonly misidentified lines (See ICLAC register)Name any commonly misidentified cell lines used in the study and provide a rationale for their use.Palaeontology and ArchaeologySpecimen provenanceProvide provenance information for specimens and describe permits that were obtained for the work (including the name of the issuing authority, the date of issue, and any identifying information).Permits should encompass collection and, where applicable, export.Specimen depositionIndicate where the specimens have been deposited to permit free access by other researchers.Dating methodsIf new dates are provided, describe how they were obtained (e.g.collection, storage, sample pretreatment and measurement), where they were obtained (i.e.lab name), the calibration program and the protocol for quality assurance OR state that no new dates are provided.Tick this box to confirm that the raw and calibrated dates are available in the paper or in Supplementary Information.Ethics oversightIdentify the organization(s) that approved or provided guidance on the study protocol, OR state that no ethical approval or guidance was required and explain why not.Note that full information on the approval of the study protocol must also be provided in the manuscript.Animals and other research organismsPolicy information about studies involving animals; ARRIVE guidelines recommended for reporting animal research, and Sex and Gender in ResearchLaboratory animalsFor laboratory animals, report species, strain and age OR state that the study did not involve laboratory animals.Wild animalsProvide details on animals observed in or captured in the field; report species and age where possible.Describe how animals were caught and transported and what happened to captive animals after the study (if killed, explain why and describe method; if released, say where and when) OR state that the study did not involve wild animals.Reporting on sexIndicate if findings apply to only one sex; describe whether sex was considered in study design, methods used for assigning sex.Provide data disaggregated for sex where this information has been collected in the source data as appropriate; provide overall numbers in this Reporting Summary.Please state if this information has not been collected.Report sex-based analyses where performed, justify reasons for lack of sex-based analysis.Field-collected samples For laboratory work with field-collected samples, describe all relevant parameters such as housing, maintenance, temperature, photoperiod and end-of-experiment protocol OR state that the study did not involve samples collected from the field.Ethics oversightIdentify the organization(s) that approved or provided guidance on the study protocol, OR state that no ethical approval or guidance was required and explain why not.Note that full information on the approval of the study protocol must also be provided in the manuscript.Clinical data Policy information about clinical studiesAll manuscripts should comply with the ICMJE guidelines for publication of clinical research and a completed CONSORT checklist must be included with all submissions.Clinical trial registration Provide the trial registration number from ClinicalTrials.gov or an equivalent agency.Study protocolNote where the full trial protocol can be accessed OR if not available, explain why.Data collectionDescribe the settings and locales of data collection, noting the time periods of recruitment and data collection.OutcomesDescribe how you pre-defined primary and secondary outcome measures and how you assessed these measures.Dual use research of concernNovel plant genotypesDescribe the methods by which all novel plant genotypes were produced.This includes those generated by transgenic approaches, gene editing, chemical/radiation-based mutagenesis and hybridization.For transgenic lines, describe the transformation method, the number of independent lines analyzed and the generation upon which experiments were performed.For gene-edited lines, describe the editor used, the endogenous sequence targeted for editing, the targeting guide RNA sequence (if applicable) and how the editor was applied.AuthenticationDescribe any authentication procedures for each seed stock used or novel genotype generated.Describe any experiments used to assess the effect of a mutation and, where applicable, how potential secondary effects (e.g.second site T-DNA insertions, mosiacism, off-target gene editing) were examined.ChIP-seqData depositionConfirm that both raw and final processed data have been deposited in a public database such as GEO.Confirm that you have deposited or provided access to graph files (e.g.BED files) for the called peaks.Data access linksMay remain private before publication.For "Initial submission" or "Revised version" documents, provide reviewer access links.For your "Final submission" document, provide a link to the deposited data.Files in database submissionProvide a list of all files available in the database submission.Genome browser session (e.g. UCSC)Provide a link to an anonymized genome browser session for "Initial submission" and "Revised version" documents only, to enable peer review.Write "no longer applicable" for "Final submission" documents.MethodologyReplicates Describe the experimental replicates, specifying number, type and replicate agreement.Sequencing depthDescribe the sequencing depth for each experiment, providing the total number of reads, uniquely mapped reads, length of reads and whether they were paired-or single-end.AntibodiesDescribe the antibodies used for the ChIP-seq experiments; as applicable, provide supplier name, catalog number, clone name, and lot number.SoftwareDescribe the software used to collect and analyze the ChIP-seq data.For custom code that has been deposited into a community repository, provide accession details.Flow Cytometry PlotsConfirm that:The axis labels state the marker and fluorochrome used (e.g.CD4-FITC).The axis scales are clearly visible.Include numbers along axes only for bottom left plot of group (a 'group' is an analysis of identical markers).All plots are contour plots with outliers or pseudocolor plots.A numerical value for number of cells or percentage (with statistics) is provided.Methodology Sample preparationDescribe the sample preparation, detailing the biological source of the cells and any tissue processing steps used.InstrumentIdentify the instrument used for data collection, specifying make and model number.SoftwareDescribe the software used to collect and analyze the flow cytometry data.For custom code that has been deposited into a community repository, provide accession details.Cell population abundanceDescribe the abundance of the relevant cell populations within post-sort fractions, providing details on the purity of the samples and how it was determined.Gating strategyDescribe the gating strategy used for all relevant experiments, specifying the preliminary FSC/SSC gates of the starting cell population, indicating where boundaries between "positive" and "negative" staining cell populations are defined.Tick this box to confirm that a figure exemplifying the gating strategy is provided in the Supplementary Information.Magnetic resonance imagingExperimental designDesign typeIndicate task or resting state; event-related or block design.Design specificationsSpecify the number of blocks, trials or experimental units per session and/or subject, and specify the length of each trial or block (if trials are blocked) and interval between trials.Behavioral performance measures State number and/or type of variables recorded (e.g.correct button press, response time) and what statistics were used to establish that the subjects were performing the task as expected (e.g.mean, range, and/or standard deviation across subjects).Acquisition Imaging type(s)Specify: functional, structural, diffusion, perfusion.Field strengthSpecify in TeslaSequence &amp; imaging parameters Specify the pulse sequence type (gradient echo, spin echo, etc.), imaging type (EPI, spiral, etc.), field of view, matrix size, slice thickness, orientation and TE/TR/flip angle.Area of acquisitionState whether a whole brain scan was used OR define the area of acquisition, describing how the region was determined.Diffusion MRI Used Not usedPreprocessing Preprocessing softwareProvide detail on software version and revision number and on specific parameters (model/functions, brain extraction, segmentation, smoothing kernel size, etc.).
Pragmatics in action: indirect requests engage theory of mind areas and the cortical motor network. M J Van Ackeren, D Casasanto, H Bekkering, P Hagoort, S.-A Rueschemeyer, J. Cogn. Neurosci. 242012</p>
<p>What is 'theory of mind'? Concepts, cognitive processes and individual differences. I A Apperly, Q. J. Exp. Psychol. 652012</p>
<p>Does the chimpanzee have a theory of mind?. D Premack, G Woodruff, Behav. Brain Sci. 11978</p>
<p>Is belief reasoning automatic?. I A Apperly, K J Riggs, A Simpson, C Chiavarino, D Samson, Psychol. Sci. 172006</p>
<p>The social sense: susceptibility to others' beliefs in human infants and adults. Á M Kovács, E Téglás, A D Endress, Science. 3302010</p>
<p>Developmental continuity in theory of mind: speed and accuracy of belief-desire reasoning in children and adults. I A Apperly, F Warren, B J Andrews, J Grant, S Todd, Child Dev. 822011</p>
<p>Action anticipation through attribution of false belief by 2-year-olds. V Southgate, A Senju, G Csibra, Psychol. Sci. 182007</p>
<p>A two-lab direct replication attempt of Southgate, Senju and Csibra. D Kampis, P Kármán, G Csibra, V Southgate, M Hernik, R. Soc. Open Sci. 82101902007. 2021</p>
<p>Can infants adopt underspecified contents into attributed beliefs? Representational prerequisites of theory of mind. Á M Kovács, E Téglás, G Csibra, Cognition. 2131046402021</p>
<p>The 'Reading the Mind in the Eyes' Test revised version: a study with normal adults, and adults with Asperger syndrome or high-functioning autism. S Baron-Cohen, S Wheelwright, J Hill, Y Raste, I Plumb, J. Child Psychol. Psychiatry Allied Discip. 422001</p>
<p>Beliefs about beliefs: representation and constraining function of wrong beliefs in young children's understanding of deception. H Wimmer, J Perner, Cognition. 131983</p>
<p>Three-year-olds' difficulty with false belief: the case for a conceptual deficit. J Perner, S R Leekam, H Wimmer, Br. J. Dev. Psychol. 51987</p>
<p>Recognition of faux pas by normally developing children and children with asperger syndrome or high-functioning autism. S Baron-Cohen, M O'riordan, V Stone, R Jones, K Plaisted, J. Autism Dev. Disord. 291999</p>
<p>Inductive reasoning and the understanding of intention in schizophrenia. R Corcoran, Cogn. Neuropsychiatry. 82003</p>
<p>An advanced test of theory of mind: understanding of story characters' thoughts and feelings by able autistic, mentally handicapped, and normal children and adults. F G E Happé, J. Autism Dev. Disord. 241994</p>
<p>Revisiting the strange stories: revealing mentalizing impairments in autism. S White, E Hill, F Happé, U Frith, Child Dev. 802009</p>
<p>Do humans have two systems to track beliefs and belief-like states?. I A Apperly, S A Butterfill, Psychol. Rev. 1169532009</p>
<p>Two systems for thinking about others' thoughts in the developing brain. C G Wiesmann, A D Friederici, T Singer, N Steinbeis, Proc. Natl Acad. Sci. USA. 1172020</p>
<p>Sparks of artificial general intelligence: early experiments with GPT-4. S Bubeck, 10.48550/arXiv.2303.127122023</p>
<p>Beyond the imitation game: quantifying and extrapolating the capabilities of language models. A Srivastava, 10.48550/arXiv.2206.046152022Preprint at</p>
<p>Exploring GPT-3 model's capability in passing the Sally-Anne Test A preliminary study in two languages. Z Dou, 10.31219/osf.io/8r3ma2023Preprint at OSF</p>
<p>Theory of mind may have spontaneously emerged in large language models. M Kosinski, 10.48550/arXiv.2302.020832023Preprint at</p>
<p>Neural theory-of-mind? On the limits of social intelligence in large LMs. M Sap, R Lebras, D Fried, Y Choi, Proc. 2022 Conference on Empirical Methods in Natural Language Processing. 2022 Conference on Empirical Methods in Natural Language essingAssociation for Computational Linguistics2022</p>
<p>Understanding social reasoning in language models with language models. K Gandhi, J.-P Fränken, T Gerstenberg, N D Goodman, Advances in Neural Information Processing Systems. MIT Press202336</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. T Ullman, 10.48550/arXiv.2302.083992023Preprint at</p>
<p>How Not to Test GPT-3. G Marcus, E Davis, 2023Marcus on AI</p>
<p>Clever Hans or neural theory of mind? Stress testing social reasoning in large language models. N Shapira, 10.48550/arXiv.2305.147632023Preprint at</p>
<p>Machine behaviour. I Rahwan, Nature. 5682019</p>
<p>Machine psychology: investigating emergent capabilities and behavior in large language models using psychological methods. T Hagendorff, 10.48550/arXiv.2303.139882023Preprint at</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, Proc. Natl Acad. Sci. USA. 120e22185231202023</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, Nat. Hum. Behav. 72023</p>
<p>Openly accessible LLMs can help us to understand human cognition. M C Frank, Nat. Hum. Behav. 72023</p>
<p>Theory of mind through the ages: older and middle-aged adults exhibit more errors than do younger adults on a continuous false belief task. D M Bernstein, W L Thornton, J A Sommerville, Exp. Aging Res. 372011</p>
<p>10.1038/s41562-024-01882-zArticle. </p>
<p>Processing of written irony in autism spectrum disorder: an eye-movement study: processing irony in autism spectrum disorders. S K Au-Yeung, J K Kaakinen, S P Liversedge, V Benson, Autism Res. 82015</p>
<p>Performance vs. competence in human-machine comparisons. C Firestone, Proc. Natl Acad. Sci. USA. 1172020</p>
<p>How well do large language models perform on faux pas tests?. N Shapira, G Zwirn, Y Goldberg, Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics2023</p>
<p>Choice without preference. a study of the history and of the logic of the problem of 'Buridan's ass'. Kant Stud. N Rescher, 196051</p>
<p>10.48550/arXiv.2303.08774GPT-4 technical report. 2023OpenAIPreprint at</p>
<p>How is ChatGPT's behavior changing over time?. L Chen, M Zaharia, J Zou, 10.48550/arXiv.2307.090092023Preprint at</p>
<p>Resolving uncertainty in a social world. Feldman Hall, O Shenhav, A , Nat. Hum. Behav. 32019</p>
<p>W James, The Principles of Psychology. Henry Holt &amp; Co18902</p>
<p>Thinking is for doing: portraits of social cognition from daguerreotype to laserphoto. S T Fiske, J. Personal. Soc. Psychol. 631992</p>
<p>When uncertainty in social contexts increases exploration and decreases obtained rewards. R C Plate, H Ham, A C Jenkins, J. Exp. Psychol. Gen. 1522023</p>
<p>The neural basis of mentalizing. C D Frith, U Frith, Neuron. 502006</p>
<p>Theory of mind: a neural prediction problem. J Koster-Hale, R Saxe, Neuron. 792013</p>
<p>How far are large language models from agents with theory-of-mind?. P Zhou, 10.48550/arXiv.2310.030512023Preprint at</p>
<p>Machine thinking, fast and slow. J.-F Bonnefon, I Rahwan, Trends Cogn. Sci. 242020</p>
<p>Elapsed decision time affects the weighting of prior probability in a perceptual decision task. T D Hanks, M E Mazurek, R Kiani, E Hopp, M N Shadlen, J. Neurosci. 312011</p>
<p>Generating meaning: active inference and the scope and limits of passive AI. G Pezzulo, T Parr, P Cisek, A Clark, K Friston, Trends Cogn. Sci. 282023</p>
<p>LLMs differ from human cognition because they are not embodied. A Chemero, Nat. Hum. Behav. 72023</p>
<p>E Brunet-Gouet, N Vidal, P Roux, Human and Artificial Rationalities. HAR 2023. Lecture Notes in Computer Science. J Baratgin, Springer202414522</p>
<p>FANToM: a benchmark for stress-testing machine theory of mind in interactions. H Kim, Proc. 2023 Conference on Empirical Methods in Natural Language Processing. 2023 Conference on Empirical Methods in Natural Language essingAssociation for Computational Linguistics2023</p>
<p>Transmission versus truth, imitation versus nnovation: what children can do that large language and language-and-vision models cannot (yet). E Yiu, E Kosoy, A Gopnik, 10.1177/17456916231201401Perspect. Psychol. Sci. 2023</p>
<p>Using second-person neuroscience to elucidate the mechanisms of social interaction. E Redcay, L Schilbach, Nat. Rev. Neurosci. 202019</p>
<p>Toward a second-person neuroscience. L Schilbach, Behav. Brain Sci. 362013</p>
<p>Adaptation of the hinting task theory of the mind test to Spanish. D Gil, M Fernández-Modamio, R Bengochea, M Arrieta, Rev. Psiquiatr. Salud Ment. Engl. Ed. 52012</p>            </div>
        </div>

    </div>
</body>
</html>
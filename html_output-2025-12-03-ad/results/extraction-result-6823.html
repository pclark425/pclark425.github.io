<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6823 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6823</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6823</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-219720954</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2006.10022v2.pdf" target="_blank">Conversational Neuro-Symbolic Commonsense Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> One aspect of human commonsense reasoning is the ability to make presumptions about daily experiences, activities and social interactions with others. We propose a new commonsense reasoning benchmark where the task is to uncover commonsense presumptions implied by imprecisely stated natural language commands in the form of if-then-because statements. For example, in the command"If it snows at night then wake me up early because I don't want to be late for work"the speaker relies on commonsense reasoning of the listener to infer the implicit presumption that it must snow enough to cause traffic slowdowns. Such if-then-because commands are particularly important when users instruct conversational agents. We release a benchmark data set for this task, collected from humans and annotated with commonsense presumptions. We develop a neuro-symbolic theorem prover that extracts multi-hop reasoning chains and apply it to this problem. We further develop an interactive conversational framework that evokes commonsense knowledge from humans for completing reasoning chains.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6823.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6823.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CORGI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COmmonsense ReasoninG by Instruction (CORGI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic theorem prover that performs soft logical inference by learning embeddings for rules and variables, integrating a modified Prolog engine with neural networks and a conversational user-feedback loop to elicit missing commonsense knowledge and extract multi-hop proof traces (commonsense presumptions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CORGI neuro-symbolic theorem prover</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro-symbolic system: LSTM core with feed-forward networks, character-RNN predicate encoder, learned rule embedding matrix (M_rule, m1=256) and variable/atom embedding matrix (M_var, m2=300 initialized with GloVe), performing soft unification via cosine similarity and integrated with a modified Prolog engine (sPyrolog) and a conversational knowledge elicitation loop.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>neuro-symbolic (LSTM + learned rule/variable embeddings + soft unification + modified Prolog backward-chaining)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Supervised on Prolog proof traces generated from the paper's handcrafted knowledge base K (228 facts/rules); M_var initialized with GloVe embeddings; training traces produced by proving automatically generated queries against K using sPyrolog.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Soft backward-chaining theorem proving: learn to predict next rule and variable bindings from proof traces; perform soft unification by cosine similarity between argument embeddings; combine with conversationally-elicited facts to complete proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Integrates a modified Prolog engine (sPyrolog) as the symbolic KB and backward-chaining framework; the neural component learns from Prolog proof traces and proposes candidate rules/variable bindings which are soft-unified against rule heads.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>if-then-because commonsense presumptions benchmark (this paper's dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Dataset of human-collected if-then-because commands (two sets: 83 restricted-domain, 77 everyday-domain) annotated with missing commonsense presumptions and organized into logic templates; task is to extract unspoken presumptions by producing a proof chain (proof tree) that connects state+action to goal.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-hop commonsense proof generation / extraction of implicit presumptions (neuro-symbolic proof-trace generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task success rate (percentage of reasoning tasks for which a proof trace leading from goal to state+action was completed)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>No-feedback: 0% (novice & expert). Soft unification CORGI: 15.61% (novice users), 35.00% (expert users). Oracle unification: 21.62% (novice), 45.71% (expert). Up to ~45% success for expert+oracle setting reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to no-feedback (0%), soft unification enabled nonzero success (15.61% novice, 35% expert). Oracle unification (perfect soft-unify) further improves success (21.62% novice, 45.71% expert), indicating both conversational feedback and improved unification increase performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A neuro-symbolic soft theorem prover combined with conversational elicitation of missing facts can recover multi-hop commonsense reasoning chains and uncover implicit presumptions; soft unification (learned embeddings) improves robustness to natural language variation relative to strict symbolic unification, and human feedback substantially improves proof completion over no-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Small handcrafted KB (228 facts/rules) limiting coverage; heavy dependence on natural language parsing (SpaCy) causing failure modes; user-dependent conversational elicitation quality; inability to extract certain hidden actions for some logic templates; knowledge is user-specific (not shared); limited proof search parameters (proof depth n set to 3 in experiments), discrete thresholds (k=5 candidates, similarity threshold T1=0.9) and moderate overall success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Neuro-Symbolic Commonsense Reasoning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6823.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6823.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neuro-symbolic theorem prover (method)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-symbolic theorem proving via learned rule/variable embeddings and soft unification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that trains a neural controller (LSTM + feedforward nets) on symbolic proof traces to predict rule selection and variable bindings, using continuous embeddings and cosine-similarity-based 'soft' unification to allow tolerant matching of natural language variations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neuro-symbolic theorem prover (paper's core method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LSTM-based sequence model that consumes depth-first proof traces; predicts termination probability, next-rule distribution (r_{t+1}) and probabilistic argument bindings (v_{i,t+1}); uses learned M_rule and M_var embedding matrices and character-RNN embedding for predicate names.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>neuro-symbolic (sequence model + learnable embeddings + soft unification over symbolic rules)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Supervised Prolog proof traces from the paper's KB (generated with sPyrolog).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Soft backward-chaining with learned rule/variable embeddings and soft-unify (cosine similarity) to choose candidate rules and bind variables; conversational loop to add missing rules to KB.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Relies on sPyrolog (modified Prolog) to provide structure of rules/facts and to generate supervised proof traces used for training; the neural component scores and ranks rules and variable matches.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>if-then-because commonsense presumptions benchmark (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Human-authored commands annotated with implicit presumptions; tasks require producing proof traces connecting goal to observed state/action.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Soft logical deduction / proof-trace generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task success rate (completion of a valid proof trace)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>See CORGI entries (soft unification vs oracle vs no-feedback results reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Soft unification improves over strict no-feedback proving (0%); oracle unification yields higher success indicating room for embedding/selection improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning to predict rule selection and variable bindings from symbolic proof traces enables tolerant matching to NL variations and improves proof recovery when combined with conversationally-provided missing facts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires supervised proof traces for training; performance limited by KB coverage and NL parsing; soft unification uses thresholds (e.g., T1=0.9) and top-k rule selection (k=5) which can constrain recall; does not fully solve hidden-action extraction for some templates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Neuro-Symbolic Commonsense Reasoning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6823.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6823.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COMET (Commonsense Transformers for Automatic Knowledge Graph Construction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative transformer model trained to produce commonsense implications/facts by learning from ConceptNet and ATOMIC knowledge graphs; referenced as a KB/knowledge-generation approach and compared qualitatively in examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Comet: Commonsense transformers for automatic knowledge graph construction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>COMET (generative transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based generative model trained to produce commonsense inferences conditioned on graph relations and nodes (trained on ATOMIC and ConceptNet).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (generative)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>ATOMIC and ConceptNet (as noted in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generative commonsense inference over natural language prompts to produce candidate facts/relations.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Commonsense knowledge generation (if-then style inferences)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper reports qualitative mismatch: COMET-ConceptNet and COMET-ATOMIC returned plausible but context-irrelevant suggestions for certain examples (e.g., 'keep house warm' vs 'keep house dry'), illustrating KB outputs can be irrelevant to task context.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generative KB models like COMET can produce commonsense but may return contextually irrelevant facts; large KGs do not guarantee task-relevant coverage for specific reasoning prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Generative outputs can be correct but not applicable to the target reasoning context; KB/response relevance and disambiguation are challenges for using such models in task-specific inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Neuro-Symbolic Commonsense Reasoning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6823.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6823.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConceptNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ConceptNet (commonsense knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large multilingual commonsense knowledge graph used as background knowledge for commonsense tasks; mentioned as one of the KBs that CORGI compares to qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptnet: a practical commonsense reasoning tool-kit</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ConceptNet (knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open, multilingual semantic network of commonsense relations among concepts; used as training/source data for models like COMET and as background KB in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Provides structured edges/facts for downstream reasoning systems; not itself a reasoning neural model in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Commonsense fact retrieval and background knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper notes known incompleteness and potential irrelevance of facts when queried for task-specific reasoning; CORGI may still need conversationally-elicited facts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large KBs like ConceptNet are useful but incomplete and sometimes return irrelevant facts for narrow conversational reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Incomplete coverage and ambiguities/inconsistencies require contextual clarification; not sufficient alone for the paper's benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Neuro-Symbolic Commonsense Reasoning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6823.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6823.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TensorLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TensorLog: A differentiable deductive database</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differentiable logic system that converts a first-order logical database into a factor graph and performs differentiable belief propagation for learning and inference; cited as a related neuro-symbolic approach but not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tensorlog: A differentiable deductive database</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TensorLog</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Converts logical facts/rules into factor graphs enabling differentiable message-passing/inference suitable for integration with neural learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>differentiable logic / factor-graph based neuro-symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Differentiable belief propagation over factor graphs derived from logical databases.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>An external differentiable reasoning framework that can be integrated with neural components to perform learned logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Differentiable logical inference / deductive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper positions TensorLog as prior work in differentiable reasoning but notes that TensorLog and DeepProbLog do not learn embeddings for logical rules in the same way CORGI does (i.e., CORGI learns rule embeddings for robustness to NL variation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mentioned as not directly learning rule embeddings for natural language robustness in the way CORGI does (as stated by the authors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Neuro-Symbolic Commonsense Reasoning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6823.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6823.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepProbLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepProbLog (Neural probabilistic logic programming)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic logic programming language that combines neural predicates with probabilistic logic programming (ProbLog), enabling end-to-end training on tasks with categorical variables; cited as related prior work but not used here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepproblog: Neural probabilistic logic programming</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepProbLog</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Probabilistic logic programming augmented with neural network components for perceptual predicates, allowing probabilistic logical inference with learned neural components.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>neuro-symbolic / probabilistic logic programming</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Combines neural predicate models with ProbLog for probabilistic logical reasoning and end-to-end training.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Serves as an external neuro-symbolic framework that couples neural perceptual modules with probabilistic logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Probabilistic logical inference / reasoning with neural predicates</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a powerful hybrid approach for settings with categorical variables; authors contrast it with CORGI which learns rule embeddings for NL variation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not designed in the referenced description to learn embeddings for logical rules specifically for robustness to NL variation (per the paper's comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Neuro-Symbolic Commonsense Reasoning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6823.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6823.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Programmer-Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Programmer-Interpreter (NPI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture that learns to execute algorithms by training on execution traces, cited as conceptually similar to CORGI's approach of learning from proof traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural programmer-interpreters</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Programmer-Interpreter (Reed & De Freitas)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A neural controller that learns to call subroutines (programs) and execute algorithmic traces; trained on execution traces of algorithms (e.g., addition, sort).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>sequence model with hierarchical program-calling structure (neural interpreter)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Execution traces of algorithms (as in original NPI work)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Learning to predict next program action from execution trace supervision (supervised trace learning).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Algorithmic trace execution / learned program execution</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper cites NPI as closest in spirit to CORGI because both learn from execution/proof traces, but NPI is algorithmic rather than targeting natural-language logical rules.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Trace supervision is an effective way to learn structured stepwise execution; inspires CORGI's learning-from-proof-trace approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>NPI is not designed for open-domain natural language variation or integration with symbolic KBs as CORGI is.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Neuro-Symbolic Commonsense Reasoning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6823.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6823.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformers as soft reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformers as soft reasoners over language (Clark et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recent work applying transformer architectures to temporal and multi-hop logical reasoning using natural-language statements of facts and rules; cited with noted practical limitations in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers as soft reasoners over language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based soft reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer models that perform reasoning by ingesting facts/rules expressed in natural language and producing inferences or proofs in a connectionist (soft) manner.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Purely connectionist multi-hop reasoning over language (no symbolic Prolog); can be trained to perform temporal or multi-hop inference.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Temporal logic reasoning / multi-hop reasoning over natural-language facts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper notes that purely connectionist transformer approaches are limited by input token size (restricting KB size) and have difficulty generalizing to arbitrary numbers of variables or arbitrary inference depth, compared to symbolic approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transformers can be used as soft reasoners but face practical scalability and generalization issues when applied to large KBs or unbounded reasoning depth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Input token-size limit confines knowledge base size; generalizing to arbitrary number of variables and arbitrary inference depth is not trivial for transformer-only approaches (as noted in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Neuro-Symbolic Commonsense Reasoning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6823.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6823.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>End-to-end differentiable proving</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end differentiable proving (Rocktschel & Riedel)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach to differentiable logical proving where proving operations are relaxed to continuous operations enabling gradient-based learning; cited in related work as relevant prior neuro-symbolic reasoning research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>End-to-end differentiable proving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>End-to-end differentiable prover</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework that makes logical proving differentiable so that rule and fact representations can be learned end-to-end with gradient descent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>differentiable/neuro-symbolic proving</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Differentiable relaxation of logical inference allowing learning of embeddings for symbols and rules.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Differentiable logical proving / learned inference</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Relevant prior work showing viability of end-to-end learning for proving; motivates CORGI's learning from symbolic traces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not evaluated in this paper; no experimental results reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Neuro-Symbolic Commonsense Reasoning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6823.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6823.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLProlog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLProlog (weak/soft unification for QA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses weak/soft unification in a Prolog-like reasoning system to allow reasoning over natural-language inputs; cited as related work addressing weak unification for NL QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nlprolog: Reasoning with weak unification for question answering in natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NLProlog</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prolog-style reasoner augmented with weak/unification mechanisms to match natural language expressions to KB predicates for QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>symbolic with weak unification / neuro-symbolic hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Soft/weak unification to match NL to KB predicates, enabling symbolic proofs over noisier NL inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Augmented Prolog-like system for NL-aware reasoning; used as prior art demonstrating benefits of weak unification.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Question answering with weak/unified Prolog reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates practical utility of weaker unification strategies for aligning NL to symbolic KBs; referenced as related prior work to CORGI's soft unification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not evaluated in this paper; cited as relevant background.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Neuro-Symbolic Commonsense Reasoning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6823.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6823.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepLogic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepLogic (end-to-end differentiable logical reasoning research)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Family of approaches learning representations for logical rules using neural networks and exploring end-to-end differentiable reasoning; cited as related prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deeplogic: Towards end-to-end differentiable logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepLogic</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural methods that learn low-dimensional representations of logical rules and seek end-to-end differentiable reasoning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>neural representations for logic / differentiable reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Learn rule embeddings and perform reasoning via neural modules.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Differentiable logical reasoning / rule embedding learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Positioned as prior work that learns representations for logical rules; CORGI extends these ideas to Prolog trace supervision and conversational KB completion.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not experimentally compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Conversational Neuro-Symbolic Commonsense Reasoning', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Comet: Commonsense transformers for automatic knowledge graph construction <em>(Rating: 2)</em></li>
                <li>Tensorlog: A differentiable deductive database <em>(Rating: 2)</em></li>
                <li>Deepproblog: Neural probabilistic logic programming <em>(Rating: 2)</em></li>
                <li>Neural programmer-interpreters <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>End-to-end differentiable proving <em>(Rating: 2)</em></li>
                <li>Deeplogic: Towards end-to-end differentiable logical reasoning <em>(Rating: 1)</em></li>
                <li>Teaching temporal logics to neural networks <em>(Rating: 1)</em></li>
                <li>Nlprolog: Reasoning with weak unification for question answering in natural language <em>(Rating: 1)</em></li>
                <li>Neural logic machines <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6823",
    "paper_id": "paper-219720954",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "CORGI",
            "name_full": "COmmonsense ReasoninG by Instruction (CORGI)",
            "brief_description": "A neuro-symbolic theorem prover that performs soft logical inference by learning embeddings for rules and variables, integrating a modified Prolog engine with neural networks and a conversational user-feedback loop to elicit missing commonsense knowledge and extract multi-hop proof traces (commonsense presumptions).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CORGI neuro-symbolic theorem prover",
            "model_description": "Neuro-symbolic system: LSTM core with feed-forward networks, character-RNN predicate encoder, learned rule embedding matrix (M_rule, m1=256) and variable/atom embedding matrix (M_var, m2=300 initialized with GloVe), performing soft unification via cosine similarity and integrated with a modified Prolog engine (sPyrolog) and a conversational knowledge elicitation loop.",
            "model_size": null,
            "architecture_type": "neuro-symbolic (LSTM + learned rule/variable embeddings + soft unification + modified Prolog backward-chaining)",
            "training_data": "Supervised on Prolog proof traces generated from the paper's handcrafted knowledge base K (228 facts/rules); M_var initialized with GloVe embeddings; training traces produced by proving automatically generated queries against K using sPyrolog.",
            "reasoning_method": "Soft backward-chaining theorem proving: learn to predict next rule and variable bindings from proof traces; perform soft unification by cosine similarity between argument embeddings; combine with conversationally-elicited facts to complete proofs.",
            "external_tool_used": true,
            "external_tool_description": "Integrates a modified Prolog engine (sPyrolog) as the symbolic KB and backward-chaining framework; the neural component learns from Prolog proof traces and proposes candidate rules/variable bindings which are soft-unified against rule heads.",
            "benchmark_name": "if-then-because commonsense presumptions benchmark (this paper's dataset)",
            "benchmark_description": "Dataset of human-collected if-then-because commands (two sets: 83 restricted-domain, 77 everyday-domain) annotated with missing commonsense presumptions and organized into logic templates; task is to extract unspoken presumptions by producing a proof chain (proof tree) that connects state+action to goal.",
            "task_type": "Multi-hop commonsense proof generation / extraction of implicit presumptions (neuro-symbolic proof-trace generation)",
            "performance_metric": "Task success rate (percentage of reasoning tasks for which a proof trace leading from goal to state+action was completed)",
            "performance_value": "No-feedback: 0% (novice & expert). Soft unification CORGI: 15.61% (novice users), 35.00% (expert users). Oracle unification: 21.62% (novice), 45.71% (expert). Up to ~45% success for expert+oracle setting reported.",
            "comparison_with_baseline": "Compared to no-feedback (0%), soft unification enabled nonzero success (15.61% novice, 35% expert). Oracle unification (perfect soft-unify) further improves success (21.62% novice, 45.71% expert), indicating both conversational feedback and improved unification increase performance.",
            "key_findings": "A neuro-symbolic soft theorem prover combined with conversational elicitation of missing facts can recover multi-hop commonsense reasoning chains and uncover implicit presumptions; soft unification (learned embeddings) improves robustness to natural language variation relative to strict symbolic unification, and human feedback substantially improves proof completion over no-feedback.",
            "limitations": "Small handcrafted KB (228 facts/rules) limiting coverage; heavy dependence on natural language parsing (SpaCy) causing failure modes; user-dependent conversational elicitation quality; inability to extract certain hidden actions for some logic templates; knowledge is user-specific (not shared); limited proof search parameters (proof depth n set to 3 in experiments), discrete thresholds (k=5 candidates, similarity threshold T1=0.9) and moderate overall success rates.",
            "uuid": "e6823.0",
            "source_info": {
                "paper_title": "Conversational Neuro-Symbolic Commonsense Reasoning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Neuro-symbolic theorem prover (method)",
            "name_full": "Neuro-symbolic theorem proving via learned rule/variable embeddings and soft unification",
            "brief_description": "Approach that trains a neural controller (LSTM + feedforward nets) on symbolic proof traces to predict rule selection and variable bindings, using continuous embeddings and cosine-similarity-based 'soft' unification to allow tolerant matching of natural language variations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Neuro-symbolic theorem prover (paper's core method)",
            "model_description": "LSTM-based sequence model that consumes depth-first proof traces; predicts termination probability, next-rule distribution (r_{t+1}) and probabilistic argument bindings (v_{i,t+1}); uses learned M_rule and M_var embedding matrices and character-RNN embedding for predicate names.",
            "model_size": null,
            "architecture_type": "neuro-symbolic (sequence model + learnable embeddings + soft unification over symbolic rules)",
            "training_data": "Supervised Prolog proof traces from the paper's KB (generated with sPyrolog).",
            "reasoning_method": "Soft backward-chaining with learned rule/variable embeddings and soft-unify (cosine similarity) to choose candidate rules and bind variables; conversational loop to add missing rules to KB.",
            "external_tool_used": true,
            "external_tool_description": "Relies on sPyrolog (modified Prolog) to provide structure of rules/facts and to generate supervised proof traces used for training; the neural component scores and ranks rules and variable matches.",
            "benchmark_name": "if-then-because commonsense presumptions benchmark (this paper)",
            "benchmark_description": "Human-authored commands annotated with implicit presumptions; tasks require producing proof traces connecting goal to observed state/action.",
            "task_type": "Soft logical deduction / proof-trace generation",
            "performance_metric": "Task success rate (completion of a valid proof trace)",
            "performance_value": "See CORGI entries (soft unification vs oracle vs no-feedback results reported in paper).",
            "comparison_with_baseline": "Soft unification improves over strict no-feedback proving (0%); oracle unification yields higher success indicating room for embedding/selection improvement.",
            "key_findings": "Learning to predict rule selection and variable bindings from symbolic proof traces enables tolerant matching to NL variations and improves proof recovery when combined with conversationally-provided missing facts.",
            "limitations": "Requires supervised proof traces for training; performance limited by KB coverage and NL parsing; soft unification uses thresholds (e.g., T1=0.9) and top-k rule selection (k=5) which can constrain recall; does not fully solve hidden-action extraction for some templates.",
            "uuid": "e6823.1",
            "source_info": {
                "paper_title": "Conversational Neuro-Symbolic Commonsense Reasoning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "COMET",
            "name_full": "COMET (Commonsense Transformers for Automatic Knowledge Graph Construction)",
            "brief_description": "A generative transformer model trained to produce commonsense implications/facts by learning from ConceptNet and ATOMIC knowledge graphs; referenced as a KB/knowledge-generation approach and compared qualitatively in examples.",
            "citation_title": "Comet: Commonsense transformers for automatic knowledge graph construction",
            "mention_or_use": "mention",
            "model_name": "COMET (generative transformer)",
            "model_description": "Transformer-based generative model trained to produce commonsense inferences conditioned on graph relations and nodes (trained on ATOMIC and ConceptNet).",
            "model_size": null,
            "architecture_type": "transformer (generative)",
            "training_data": "ATOMIC and ConceptNet (as noted in paper)",
            "reasoning_method": "Generative commonsense inference over natural language prompts to produce candidate facts/relations.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Commonsense knowledge generation (if-then style inferences)",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Paper reports qualitative mismatch: COMET-ConceptNet and COMET-ATOMIC returned plausible but context-irrelevant suggestions for certain examples (e.g., 'keep house warm' vs 'keep house dry'), illustrating KB outputs can be irrelevant to task context.",
            "key_findings": "Generative KB models like COMET can produce commonsense but may return contextually irrelevant facts; large KGs do not guarantee task-relevant coverage for specific reasoning prompts.",
            "limitations": "Generative outputs can be correct but not applicable to the target reasoning context; KB/response relevance and disambiguation are challenges for using such models in task-specific inference.",
            "uuid": "e6823.2",
            "source_info": {
                "paper_title": "Conversational Neuro-Symbolic Commonsense Reasoning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "ConceptNet",
            "name_full": "ConceptNet (commonsense knowledge graph)",
            "brief_description": "A large multilingual commonsense knowledge graph used as background knowledge for commonsense tasks; mentioned as one of the KBs that CORGI compares to qualitatively.",
            "citation_title": "Conceptnet: a practical commonsense reasoning tool-kit",
            "mention_or_use": "mention",
            "model_name": "ConceptNet (knowledge graph)",
            "model_description": "Open, multilingual semantic network of commonsense relations among concepts; used as training/source data for models like COMET and as background KB in related work.",
            "model_size": null,
            "architecture_type": "knowledge graph",
            "training_data": null,
            "reasoning_method": "Provides structured edges/facts for downstream reasoning systems; not itself a reasoning neural model in this paper.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Commonsense fact retrieval and background knowledge",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Paper notes known incompleteness and potential irrelevance of facts when queried for task-specific reasoning; CORGI may still need conversationally-elicited facts.",
            "key_findings": "Large KBs like ConceptNet are useful but incomplete and sometimes return irrelevant facts for narrow conversational reasoning tasks.",
            "limitations": "Incomplete coverage and ambiguities/inconsistencies require contextual clarification; not sufficient alone for the paper's benchmark.",
            "uuid": "e6823.3",
            "source_info": {
                "paper_title": "Conversational Neuro-Symbolic Commonsense Reasoning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "TensorLog",
            "name_full": "TensorLog: A differentiable deductive database",
            "brief_description": "Differentiable logic system that converts a first-order logical database into a factor graph and performs differentiable belief propagation for learning and inference; cited as a related neuro-symbolic approach but not used in experiments.",
            "citation_title": "Tensorlog: A differentiable deductive database",
            "mention_or_use": "mention",
            "model_name": "TensorLog",
            "model_description": "Converts logical facts/rules into factor graphs enabling differentiable message-passing/inference suitable for integration with neural learning.",
            "model_size": null,
            "architecture_type": "differentiable logic / factor-graph based neuro-symbolic",
            "training_data": null,
            "reasoning_method": "Differentiable belief propagation over factor graphs derived from logical databases.",
            "external_tool_used": true,
            "external_tool_description": "An external differentiable reasoning framework that can be integrated with neural components to perform learned logical inference.",
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Differentiable logical inference / deductive reasoning",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Paper positions TensorLog as prior work in differentiable reasoning but notes that TensorLog and DeepProbLog do not learn embeddings for logical rules in the same way CORGI does (i.e., CORGI learns rule embeddings for robustness to NL variation).",
            "limitations": "Mentioned as not directly learning rule embeddings for natural language robustness in the way CORGI does (as stated by the authors).",
            "uuid": "e6823.4",
            "source_info": {
                "paper_title": "Conversational Neuro-Symbolic Commonsense Reasoning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "DeepProbLog",
            "name_full": "DeepProbLog (Neural probabilistic logic programming)",
            "brief_description": "A probabilistic logic programming language that combines neural predicates with probabilistic logic programming (ProbLog), enabling end-to-end training on tasks with categorical variables; cited as related prior work but not used here.",
            "citation_title": "Deepproblog: Neural probabilistic logic programming",
            "mention_or_use": "mention",
            "model_name": "DeepProbLog",
            "model_description": "Probabilistic logic programming augmented with neural network components for perceptual predicates, allowing probabilistic logical inference with learned neural components.",
            "model_size": null,
            "architecture_type": "neuro-symbolic / probabilistic logic programming",
            "training_data": null,
            "reasoning_method": "Combines neural predicate models with ProbLog for probabilistic logical reasoning and end-to-end training.",
            "external_tool_used": true,
            "external_tool_description": "Serves as an external neuro-symbolic framework that couples neural perceptual modules with probabilistic logical inference.",
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Probabilistic logical inference / reasoning with neural predicates",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Mentioned as a powerful hybrid approach for settings with categorical variables; authors contrast it with CORGI which learns rule embeddings for NL variation.",
            "limitations": "Not designed in the referenced description to learn embeddings for logical rules specifically for robustness to NL variation (per the paper's comparison).",
            "uuid": "e6823.5",
            "source_info": {
                "paper_title": "Conversational Neuro-Symbolic Commonsense Reasoning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Neural Programmer-Interpreter",
            "name_full": "Neural Programmer-Interpreter (NPI)",
            "brief_description": "A neural architecture that learns to execute algorithms by training on execution traces, cited as conceptually similar to CORGI's approach of learning from proof traces.",
            "citation_title": "Neural programmer-interpreters",
            "mention_or_use": "mention",
            "model_name": "Neural Programmer-Interpreter (Reed & De Freitas)",
            "model_description": "A neural controller that learns to call subroutines (programs) and execute algorithmic traces; trained on execution traces of algorithms (e.g., addition, sort).",
            "model_size": null,
            "architecture_type": "sequence model with hierarchical program-calling structure (neural interpreter)",
            "training_data": "Execution traces of algorithms (as in original NPI work)",
            "reasoning_method": "Learning to predict next program action from execution trace supervision (supervised trace learning).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Algorithmic trace execution / learned program execution",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Paper cites NPI as closest in spirit to CORGI because both learn from execution/proof traces, but NPI is algorithmic rather than targeting natural-language logical rules.",
            "key_findings": "Trace supervision is an effective way to learn structured stepwise execution; inspires CORGI's learning-from-proof-trace approach.",
            "limitations": "NPI is not designed for open-domain natural language variation or integration with symbolic KBs as CORGI is.",
            "uuid": "e6823.6",
            "source_info": {
                "paper_title": "Conversational Neuro-Symbolic Commonsense Reasoning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "Transformers as soft reasoners",
            "name_full": "Transformers as soft reasoners over language (Clark et al.)",
            "brief_description": "Recent work applying transformer architectures to temporal and multi-hop logical reasoning using natural-language statements of facts and rules; cited with noted practical limitations in the paper.",
            "citation_title": "Transformers as soft reasoners over language",
            "mention_or_use": "mention",
            "model_name": "Transformer-based soft reasoners",
            "model_description": "Transformer models that perform reasoning by ingesting facts/rules expressed in natural language and producing inferences or proofs in a connectionist (soft) manner.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Purely connectionist multi-hop reasoning over language (no symbolic Prolog); can be trained to perform temporal or multi-hop inference.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Temporal logic reasoning / multi-hop reasoning over natural-language facts",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": "Paper notes that purely connectionist transformer approaches are limited by input token size (restricting KB size) and have difficulty generalizing to arbitrary numbers of variables or arbitrary inference depth, compared to symbolic approaches.",
            "key_findings": "Transformers can be used as soft reasoners but face practical scalability and generalization issues when applied to large KBs or unbounded reasoning depth.",
            "limitations": "Input token-size limit confines knowledge base size; generalizing to arbitrary number of variables and arbitrary inference depth is not trivial for transformer-only approaches (as noted in this paper).",
            "uuid": "e6823.7",
            "source_info": {
                "paper_title": "Conversational Neuro-Symbolic Commonsense Reasoning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "End-to-end differentiable proving",
            "name_full": "End-to-end differentiable proving (Rocktschel & Riedel)",
            "brief_description": "Approach to differentiable logical proving where proving operations are relaxed to continuous operations enabling gradient-based learning; cited in related work as relevant prior neuro-symbolic reasoning research.",
            "citation_title": "End-to-end differentiable proving",
            "mention_or_use": "mention",
            "model_name": "End-to-end differentiable prover",
            "model_description": "Framework that makes logical proving differentiable so that rule and fact representations can be learned end-to-end with gradient descent.",
            "model_size": null,
            "architecture_type": "differentiable/neuro-symbolic proving",
            "training_data": null,
            "reasoning_method": "Differentiable relaxation of logical inference allowing learning of embeddings for symbols and rules.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Differentiable logical proving / learned inference",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Relevant prior work showing viability of end-to-end learning for proving; motivates CORGI's learning from symbolic traces.",
            "limitations": "Not evaluated in this paper; no experimental results reported here.",
            "uuid": "e6823.8",
            "source_info": {
                "paper_title": "Conversational Neuro-Symbolic Commonsense Reasoning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "NLProlog",
            "name_full": "NLProlog (weak/soft unification for QA)",
            "brief_description": "A system that uses weak/soft unification in a Prolog-like reasoning system to allow reasoning over natural-language inputs; cited as related work addressing weak unification for NL QA.",
            "citation_title": "Nlprolog: Reasoning with weak unification for question answering in natural language",
            "mention_or_use": "mention",
            "model_name": "NLProlog",
            "model_description": "Prolog-style reasoner augmented with weak/unification mechanisms to match natural language expressions to KB predicates for QA tasks.",
            "model_size": null,
            "architecture_type": "symbolic with weak unification / neuro-symbolic hybrid",
            "training_data": null,
            "reasoning_method": "Soft/weak unification to match NL to KB predicates, enabling symbolic proofs over noisier NL inputs.",
            "external_tool_used": true,
            "external_tool_description": "Augmented Prolog-like system for NL-aware reasoning; used as prior art demonstrating benefits of weak unification.",
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Question answering with weak/unified Prolog reasoning",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Demonstrates practical utility of weaker unification strategies for aligning NL to symbolic KBs; referenced as related prior work to CORGI's soft unification.",
            "limitations": "Not evaluated in this paper; cited as relevant background.",
            "uuid": "e6823.9",
            "source_info": {
                "paper_title": "Conversational Neuro-Symbolic Commonsense Reasoning",
                "publication_date_yy_mm": "2020-06"
            }
        },
        {
            "name_short": "DeepLogic",
            "name_full": "DeepLogic (end-to-end differentiable logical reasoning research)",
            "brief_description": "Family of approaches learning representations for logical rules using neural networks and exploring end-to-end differentiable reasoning; cited as related prior work.",
            "citation_title": "Deeplogic: Towards end-to-end differentiable logical reasoning",
            "mention_or_use": "mention",
            "model_name": "DeepLogic",
            "model_description": "Neural methods that learn low-dimensional representations of logical rules and seek end-to-end differentiable reasoning capabilities.",
            "model_size": null,
            "architecture_type": "neural representations for logic / differentiable reasoning",
            "training_data": null,
            "reasoning_method": "Learn rule embeddings and perform reasoning via neural modules.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": null,
            "benchmark_description": null,
            "task_type": "Differentiable logical reasoning / rule embedding learning",
            "performance_metric": null,
            "performance_value": null,
            "comparison_with_baseline": null,
            "key_findings": "Positioned as prior work that learns representations for logical rules; CORGI extends these ideas to Prolog trace supervision and conversational KB completion.",
            "limitations": "Not experimentally compared in this paper.",
            "uuid": "e6823.10",
            "source_info": {
                "paper_title": "Conversational Neuro-Symbolic Commonsense Reasoning",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Comet: Commonsense transformers for automatic knowledge graph construction",
            "rating": 2,
            "sanitized_title": "comet_commonsense_transformers_for_automatic_knowledge_graph_construction"
        },
        {
            "paper_title": "Tensorlog: A differentiable deductive database",
            "rating": 2,
            "sanitized_title": "tensorlog_a_differentiable_deductive_database"
        },
        {
            "paper_title": "Deepproblog: Neural probabilistic logic programming",
            "rating": 2,
            "sanitized_title": "deepproblog_neural_probabilistic_logic_programming"
        },
        {
            "paper_title": "Neural programmer-interpreters",
            "rating": 2,
            "sanitized_title": "neural_programmerinterpreters"
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "End-to-end differentiable proving",
            "rating": 2,
            "sanitized_title": "endtoend_differentiable_proving"
        },
        {
            "paper_title": "Deeplogic: Towards end-to-end differentiable logical reasoning",
            "rating": 1,
            "sanitized_title": "deeplogic_towards_endtoend_differentiable_logical_reasoning"
        },
        {
            "paper_title": "Teaching temporal logics to neural networks",
            "rating": 1,
            "sanitized_title": "teaching_temporal_logics_to_neural_networks"
        },
        {
            "paper_title": "Nlprolog: Reasoning with weak unification for question answering in natural language",
            "rating": 1,
            "sanitized_title": "nlprolog_reasoning_with_weak_unification_for_question_answering_in_natural_language"
        },
        {
            "paper_title": "Neural logic machines",
            "rating": 1,
            "sanitized_title": "neural_logic_machines"
        }
    ],
    "cost": 0.02050975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Conversational Neuro-Symbolic Commonsense Reasoning
19 Jun 2020</p>
<p>Forough Arabshahi farabsha@cs.cmu.edu 
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University Pittsburgh
Ariel University
Carnegie Mellon University
15213PA</p>
<p>Jennifer Lee jennife4@cs.cmu.edu 
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University Pittsburgh
Ariel University
Carnegie Mellon University
15213PA</p>
<p>Mikayla Gawarecki mgawarec@andrew.cmu.edu 
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University Pittsburgh
Ariel University
Carnegie Mellon University
15213PA</p>
<p>Kathryn Mazaitis 
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University Pittsburgh
Ariel University
Carnegie Mellon University
15213PA</p>
<p>Amos Azaria amos.azaria@ariel.ac.il 
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University Pittsburgh
Ariel University
Carnegie Mellon University
15213PA</p>
<p>Tom Mitchell tom.mitchell@cs.cmu.edu 
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University
Carnegie Mellon University Pittsburgh
Ariel University
Carnegie Mellon University
15213PA</p>
<p>Conversational Neuro-Symbolic Commonsense Reasoning
19 Jun 2020
One aspect of human commonsense reasoning is the ability to make presumptions about daily experiences, activities and social interactions with others. We propose a new commonsense reasoning benchmark where the task is to uncover commonsense presumptions implied by imprecisely stated natural language commands in the form of if-then-because statements. For example, in the command "If it snows at night then wake me up early because I don't want to be late for work" the speaker relies on commonsense reasoning of the listener to infer the implicit presumption that it must snow enough to cause traffic slowdowns. Such if-thenbecause commands are particularly important when users instruct conversational agents. We release a benchmark data set for this task, collected from humans and annotated with commonsense presumptions. We develop a neuro-symbolic theorem prover that extracts multi-hop reasoning chains and apply it to this problem. We further develop an interactive conversational framework that evokes commonsense knowledge from humans for completing reasoning chains.</p>
<p>Introduction</p>
<p>Despite the remarkable success of artificial intelligence (AI) and machine learning in the last few decades, commonsense reasoning remains an unsolved problem at the heart of AI [1][2][3]. Common sense allows us humans to engage in conversations with one another and to convey our thoughts efficiently, without the need to specify much detail [4]. For example, if Alice asks Bob to "wake her up early whenever it snows at night" so that she can get to work on time, Alice assumes that Bob will wake her up only if it snows enough to cause traffic slowdowns, and only if it is a working day. Alice does not explicitly state these conditions since Bob makes such presumptions without much effort thanks to his common sense. A study, in which we collected if-then commands from human subjects, revealed that humans often under-specify conditions in their statements; perhaps because they are used to speaking with other humans who possess the common sense needed to infer their more specific intent by making presumptions about their statement. The inability to make these presumptions is one of the main reasons why it is challenging for computers to engage in natural sounding conversations with humans.</p>
<p>In an attempt to enable this, we propose a new commonsense reasoning benchmark where the task is to infer commonsense presumptions in commands of the form "If state holds Then perform action Because I want to achieve goal ." The reason for including the "because" clause in the commands is that some presumptions are ambiguous without knowing the user's purpose, or goal. For instance, if Alice's goal in the above example was to see snow for the first time, Bob would have presumed that even a snow flurry would be excuse enough to wake her up. Since humans frequently omit details when stating such commands, a computer possessing common sense should be able Table 1: Statistics of if-then-because commands collected from a pool of human subjects. The table shows four distinct types of because-clauses we found, the count of commands of each type, examples of each and their corresponding commonsense presumption annotations. Restricted domain includes commands whose state is limited to checking email, calendar, maps, alarms, and weather. Everyday domain includes commands concerning more general day-to-day activities. Annotations are tuples of (index, presumption) where index shows the starting word index of where the missing presumption should be in the command -highlighted with a red arrow. Index starts at 0 and is calculated for the original command. then remind me to pay it (   )</p>
<p>because I don't want to pay a late fee (7, in the next few days) (13, before the bill payment deadline) because it will be difficult to find ubers.</p>
<p>(3, take off time) ( then remind me to register (   ) and vote (   )</p>
<p>because I want my voice to be heard. (6, in the next few months) (6, and I am eligible to vote) (11, to vote), (13,  then remind me to send her an email (   )</p>
<p>because we forgot to schedule our next chat (21, in the next few days) (29,  We propose an approach that infers such missing presumptions, by extracting a chain of reasoning that shows how the commanded action will achieve the desired goal when the state holds. Whenever any additional reasoning steps appear in this reasoning chain, they are output by our system as assumed implicit presumptions associated with the command. For our reasoning method we propose a neuro-symbolic interactive, conversational approach, in which the computer combines its own common sense knowledge with conversationally evoked knowledge provided by a human user. The reasoning chain is extracted using our neuro-symbolic theorem prover that learns sub-symbolic representations (embeddings) for logical statements, making it robust to variations of natural language encountered in a conversational interaction setting.</p>
<p>Contributions We have three main contributions. 1) We propose a benchmark task for commonsense reasoning and release a data set containing if-then-because commands, annotated with commonsense presumptions. 2) We present a system called CORGI (COmmonsense ReasoninG by Instruction) that performs soft logical inference. We propose a neuro-symbolic theorem prover and apply it to extract a multi-hop reasoning chain that reveals commonsense presumptions. 3) We equip CORGI with a conversational interaction mechanism that enables it to collect just-in-time commonsense knowledge from humans. Our user-study shows (a) the plausibility of relying on humans to evoke commonsense knowledge and (b) the effectiveness of our theorem prover, enabling us to extract reasoning chains for up to 45% of the studied tasks 1 .</p>
<p>Related Work</p>
<p>The literature on commonsense reasoning dates back to the very beginning of the field of AI [2,5,6] and is studied in several contexts. One aspect focuses on building a large knowledge base (KB) of commonsense facts. Projects like CYC [7], ConceptNet [8][9][10] and ATOMIC [11,12] are examples of such KBs (see [2] for a comprehensive list). Recently, [13] proposed COMET, a generative model trained on ConceptNet and ATOMIC, that generates commonsense facts. These KBs provide background knowledge for tasks requiring common sense. However, it is known that knowledge bases are incomplete, and most have ambiguities and inconsistencies [2] that must be clarified for particular reasoning tasks. Therefore, we argue that reasoning engines can benefit greatly from a conversational interaction strategy to ask humans about their missing or inconsistent knowledge. Closest in nature to this proposal is the work by Hixon et al., [14] on relation extraction through conversation for question answering. The advent of intelligent agents and advancements in natural language processing have given learning from conversational interactions a good momentum in the last few years [15][16][17][18][19][20][21][22][23].</p>
<p>A current challenge in commonsense reasoning is lack of benchmarks [2]. Benchmark tasks in commonsense reasoning include the Winograd Schema Challenge (WSC) [1], its variations [24], and its recently scaled up counterpart, Winogrande [3] ; ROCStories [25], COPA [26], and ART [27], where the task is to choose a plausible outcome, cause or explanation for an input scenario. Most of these benchmarks have a multiple choice design format. However, in the real world the computer is usually not given multiple choice questions. None of these benchmarks targets the extraction of unspoken details in a natural language statement, which is a challenging task for computers known since the 1970's [4].</p>
<p>CORGI has a neuro-symbolic logic theorem prover. Neuro-symbolic systems are hybrid models that leverage the robustness of connectionist methods and the soundness of symbolic reasoning to effectively integrate learning and reasoning [28,29]. They have shown promise in different areas of logical reasoning ranging from classical logic to propositional logic, probabilistic logic, abductive logic, and inductive logic [30][31][32][33][34][35]. To the best of our knowledge, neuro-symbolic solutions for commonsense reasoning have not been proposed before. Examples of commonsense reasoning engines are: AnalogySpace [36,37] that uses dimensionality reduction, and [6] that uses the event calculus formal language. TensorLog [38] converts a first-order logical database into a factor graph and proposes a differentiable strategy for belief propagation over the graph. DeepProbLog [31] developed a probabilistic logic programming language that is suitable for applications containing categorical variables. Contrary to our approach, both these methods do not learn embeddings for logical rules that are needed to make CORGI robust to natural language variations. Therefore, we propose an endto-end differentiable solution that uses a Prolog [39] proof trace to learn rule embeddings from data. Our proposal is closest to the neural programmer interpreter [40] that uses the trace of algorithms such as addition and sort to learn their execution. The use of Prolog for performing multi-hop logical reasoning has been studied in [41,42]. These methods perform Inductive Logic Programming to learn rules from data, and are not applicable to our problem. DeepLogic [43] and [44,45] also learn representations for logical rules using neural networks. Very recently, transformers were used for temporal logic [46] and to do multi-hop reasoning [47] using logical facts and rules stated in natural language. A purely connectionist approach to reasoning suffers from some limitations. For example, the input token size limit of transformers restricts [47] to small knowledge bases. Moreover, generalizing to arbitrary number of variables or an arbitrary inference depth is not trivial for them. Since symbolic reasoning can inherently handle all these challenges, a hybrid approach to reasoning takes the burden of handling them off of the neural component.</p>
<p>Proposed Commonsense Reasoning Benchmark</p>
<p>The benchmark task that we propose in this work is that of uncovering hidden commonsense presumptions given an input if-then-because command. Formally, the commands follow the general format "if state holds then perform action because I want to achieve goal ". We refer to the if -clause as the state , the then-clause as the action and the because-clause as the goal . These natural language commands were collected from a pool of human subjects (more details in the Appendix). The data is annotated with unspoken commonsense presumptions by a team of annotators. Tab. 1 shows the statistics of the data and annotated examples from the data. We collected two sets of if-then-because commands. The first set contains 83 commands targeted at a state that can be observed by a computer/mobile phone (which is checking emails, calendar, maps, alarms, and weather). The second set contains 77 commands whose state is about day-to-day events and activities. 81% of the commands over both sets qualify as "if state then action because goal ". The remaining 19% differ in the categorization of the because-clause (see Tab. 1); common alternate clause types included anti-goals ("...because I don't want to be late"), modifications of the state or action ("... because it will be difficult to find an Uber"), or conjunctions including at least one non-goal type. Note that we did not instruct the subjects to give us data from these categories, rather we uncovered them after data collection. Note that commonsense benchmarks such as the Winograd Schema Challenge [1] included a similar number of examples (100) when first introduced [24].</p>
<p>Lastly, after collecting the data we discovered that the if-then-because commands given by humans can be categorized into several different logic templates. The discovered logic templates are given in Table 5 in the Appendix. Our neuro-symbolic theorem prover uses a general reasoning strategy input: If state then action because goal Parse Statement:
S(X)  state A(Y )  action G(Z)  goal Is G in K? i &gt; n?
Ask the user for more information G  (Z). The input is an if-then-because command e.g., "if it snows tonight then wake me up early because I want to get to work on time". The input is parsed into its logical form representation (for the prev example, S(X) = weather(snow, Precipitation)). If CORGI succeeds, it outputs a proof tree for the because-clause or goal (parsed into G(Z)=get(i,work,on_time)). The output proof tree contains commonsense presumptions for the input statement (Fig 2 shows an example). If the predicate G does not exist in the knowledge base, K, (Is G in K?), we have missing knowledge and cannot find a proof. Therefore, we extract it from a human in the user feedback loop. At the heart of CORGI is a neuro-symbolic theorem prover that learns rule and variable embeddings to perform a proof (Alg.1). goalStack and the loop variable i are initialized to empty and 0 respectively, and n = 3. italic text in the figure represents descriptions that are referred to in the main text.
i = i + 1 goalStack.push(G(Z)) G(Z) = G  (Z)
that can address all reasoning templates. However, in an extended discussion in the Appendix, we explain how a reasoning system, including ours, could potentially benefit from these logic templates.</p>
<p>Method</p>
<p>Background and notation The system's commonsense knowledge is a KB, denoted K, programmed in a Prolog-like syntax. We have developed a modified version of Prolog, which has been augmented to support several special features (types, soft-matched predicates and atoms, etc). Prolog [39] is a declarative logic programming language that consists of a set of predicates whose arguments are atoms, variables or predicates. A predicate is defined by a set of rules (Head : Body.) and facts (Head.), where Head is a predicate, Body is a conjuction of predicates, and : is logical implication. We use the notation S(X), A(Y ) and G(Z) to represent the logical form of the state , action and goal , respectively where S, A and G are predicate names and X, Y and Z indicate the list of arguments of each predicate. For example, for goal ="I want to get to work on time", we have G(Z) =get(i, work, on_time). Prolog can be used to logically "prove" a query (e.g., to prove G(Z)) using the backward chaining algorithm (see the Appendix -Prolog Background).</p>
<p>CORGI: COmmonsense Reasoning by Instruction</p>
<p>CORGI takes as input a natural language command of the form "if state then action because goal " and infers commonsense presumptions by extracting a chain of commonsense knowledge that explains how the commanded action achieves the goal when the state holds. For example from a high level, for the command in Fig. 2 CORGI outputs (1) if it snows more than two inches, then there will be traffic, (2) if there is traffic, then my commute time to work increases, (3) if my commute time to work increases then I need to leave the house earlier to ensure I get to work on time (4) if I wake up earlier then I will leave the house earlier. Formally, this reasoning chain is a proof tree (proof trace) shown in Fig.2. As shown, the proof tree includes the commonsense presumptions.</p>
<p>CORGI's architecture is depicted in Figure 1. In the first step, the if-then-because command goes through a parser that extracts the state , action and goal from it and converts them to their logical form representations S(X), A(Y ) and G(Z), respectively. For example, the action "wake me up early" is converted to wake(me, early). The parser is presented in the Appendix (Sec. Parsing).</p>
<p>The proof trace is obtained by finding a proof for G(Z), using K and the context of the input if-then-because command. In other words, S(X)  A(Y )  K  G(Z). One challenge is that even the largest knowledge bases gathered to date are incomplete, making it virtually infeasible to prove an arbitrary input G(Z). Therefore, CORGI is equipped with a conversational interaction strategy which enables it to prove a query by combining its own commonsense knowledge with get(Person, ToPlace, on_time) t = 0 arrive(Person, , , ToPlace, ArriveAt) t = 1
ready(Person, LeaveAt, PrepTime) t = 2 alarm(Perosn, Time) t = 3 alarm(i,8) t = 4 LeaveAt = Time + PrepTime. t = 5 commute(Person, FromPlace, ToPlace, With, CommuteTime) t = 6 commute(i, home, work, car, 1) t = 7 traffic(LeaveAt, ToPlace, With, TrTime) t = 8 weather(snow, Precipitation) t = 9
Precipitation &gt;= 2 t = 10 TrTime = 1 t = 11
ArriveAt = LeaveAt + CommuteTime + TrTime t = 12
calendarEntry(Person, ToPlace, ArriveAt) t = 13 calendarEntry(i, work, 9) t = 14 Figure 2: Sample proof tree for the because-clause of the statement: "If it snows tonight then wake me up early because I want to get to work on time". Proof traversal is depth-first from left to right (t gives the order). Each node in the tree indicates a rule's head, and its children indicate the rule's body. For example, the nodes highlighted in green indicate the rule ready(Person,LeaveAt,PrepTime) : alarm(Person, Time)  LeaveAt = Time+PrepTime. The goal we want to prove, G(Z)=get(Person, ToPlace, on_time), is in the tree's root. If a proof is successful, the variables in G(Z) get grounded (here Person and ToPlace are grounded to i and work, respectively). The highlighted orange nodes are the uncovered commonsense presumptions. conversationally evoked knowledge provided by a human user in response to a question from CORGI (user feedback loop in Fig.1). There are 4 possible scenarios that could occur when designing such a conversational knowledge extraction strategy.</p>
<p>A The user understands the question, but does not know the answer. B The user misunderstands the question and responds with an undesired answer. C The user understands the question and provides a correct answer, but the system fails to understand the user due to: C.1 limitations of natural language understanding. C.2 variations in natural language, which result in misalignment of the data schema in the knowledge base and the data schema in the user's mind.</p>
<p>D The user understands the questions and provides the correct answer and the system successfully parses and understands it.</p>
<p>CORGI's different components are designed such that they address the above challenges, as explained below. Since our benchmark data set deals with day-to-day activities, it is unlikely for scenario A to occur. If the task required more specific domain knowledge, A could have been addressed by choosing a pool of domain experts. Scenario B is addressed by asking informative questions from users. Scenario C.1 is addressed by trying to extract small chunks of knowledge from the users piece-by-piece. Specifically, the choice of what to ask the user in the user feedback loop is deterministically computed from the user's goal . The first step is to ask how to achieve the user's stated goal , and CORGI expects an answer that gives a sub-goal . In the next step, CORGI asks how to achieve the sub-goal the user just mentioned. The reason for this piece-by-piece knowledge extraction is to ensure that the language understanding component can correctly parse the user's response. CORGI then adds the extracted knowledge from the user to K in the knowledge update loop shown in Fig.1. Missing knowledge outside this goal /sub-goal path is not handled, although it is an interesting future direction. Moreover, the model is user specific and the knowledge extracted from different users are not shared among them. Sharing knowledge raises interesting privacy issues and requires handling personalized conflicts and falls out of the scope of our current study.</p>
<p>Scenario C.2, caused by the variations of natural language, results in semantically similar statements to get mapped into different logical forms, which is unwanted. For example, "make sure I am awake early morning" vs. "wake me up early morning" will be parsed into different logical forms awake(i,early_morning) and wake(me, early_morning), respectively although they are semantically similar. This mismatch prevents a logical proof from succeeding since the proof strategy relies on exact match in the unification operation (see Appendix). This is addressed by our neuro-symbolic theorem prover (Fig.1) that learns vector representations (embeddings) for logical rules and variables and uses them to perform a logical proof through soft unification. If the theorem prover can prove the user's goal , G(Z), CORGI outputs the proof trace ( Fig.2) returned by its theorem prover and succeeds. In the next section, we explain our theorem prover in detail. We revisit scenarios A  D in detail in the discussion section and show real examples from our user study.</p>
<p>Neuro-Symbolic Theorem Proving</p>
<p>Our Neuro-Symbolic theorem prover is a neural modification of backward chaining and uses the vector similarity between rule and variable embeddings for unification. In order to learn these embeddings, our theorem prover learns a general proving strategy by training on proof traces of successful proofs. From a high level, for a given query our model maximizes the probability of choosing the correct rule to pick in each step of the backward chaining algorithm. This proposal is an adaptation of Reed et al.'s Neural Programmer-Interpreter [40] that learns to execute algorithms such as addition and sort, by training on their execution trace.</p>
<p>In what follows, we represent scalars with lowercase letters, vectors with bold lowercase letters and matrices with bold uppercase letters. M rule  R n1m1 denotes the embedding matrix for the rules and facts, where n 1 is the number of rules and facts and m 1 is the embedding dimension. M var  R n2m2 denotes the variable embedding matrix, where n 2 is the number of all the atoms and variables in the knowledge base and m 2 is the variable embedding dimension. Our knowledge base is type coerced, therefore the variable names are associated with their types (e.g., alarm(Person,Time))</p>
<p>Learning The model's core consists of an LSTM network whose hidden state indicates the next rule in the proof trace and a proof termination probability, given a query as input. The model has a feed forward network that makes variable binding decisions. The model's training is fully supervised by the proof trace of a query given in a depth-first-traversal order from left to right (Fig. 2). The trace is sequentially input to the model in the traversal order as explained in what follows. In step t  [0, T ] of the proof, the model's input is  inp
t = q t , r t , (v 1 t , . . . , v  t )
and T is the total number of proof steps. q t is the query's embedding and is computed by feeding the predicate name of the query into a character RNN. r t is the concatenated embeddings of the rules in the parent and the left sister nodes in the proof trace, looked up from M rule . For example in Fig.2, q 3 represents the node at proof step t = 3, r 3 represents the rule highlighted in green (parent rule), and r 4 represents the fact alarm(i, 8). The reason for including the left sister node in r t is that the proof is conducted in a left-to-right depth first order. Therefore, the decision of what next rule to choose in each node is dependent on both the left sisters and the parent (e.g. the parent and the left sisters of the node at step t = 8 in Fig. 2 are the rules at nodes t = 1, t = 2, and t = 6, respectively). The arguments of the query are presented in (v 1 t , . . . , v  t ) where  is the arity of the query predicate. For example, v 1 3 in Fig 2 is the embedding of the variable Person. Each v i t for i  [0, ], is looked up from the embedding matrix M var . The output of the model in step t is  out t = c t , r t+1 , (v 1 t+1 , . . . , v  t+1 )) and is computed through the following equations
s t = f enc (q t , ), h t = f lstm (s t , r t , h t1 ),(1)c t = f end (h t ), r t+1 = f rule (h t ), v i t+1 = f var (v i t ),(2)
where v i t+1 is a probability vector over all the variables and atoms for the i th argument, r t+1 is a probability vector over all the rules and facts and c t is a scalar probability of terminating the proof at step t. f enc , f end , f rule and f var are feed forward networks with two fully connected layers, and f lstm is an LSTM network. The trainable parameters of the model are the parameters of the feed forward neural networks, the LSTM network, the character RNN that embeds q t and the rule and variable embedding matrices M rule and M var .</p>
<p>Our model is trained end-to-end. In order to train the model parameters and the embeddings, we maximize the log likelihood probability given below
 * = argmax   out , in log(P ( out | in ; )),(3)
where the summation is over all the proof traces in the training set and  is the trainable parameters of the model. We have
log(P ( out | in ; )) = T t=1 log P ( out t | in 1 . . .  in t1 ; )(4)
log P ( out t | in 1 . . .  in t1 ; ) = log P ( out t | in t1 ; ) = log P (c t |h t ) + log P (r t+1 |h t ) + log  Okay, I will perform "remind me to bring an umbrella" in order to achieve "I remain dry".
i P (v i t+1 |v i t ).(5)</p>
<p>Failed task</p>
<p>If it's going to rain in the afternoon then remind me to bring an umbrella because I want to remain dry. Where the probabilities in Equation (5) are given in Equations (2). The inference algorithm for porving is given in the Appendix, section Inference.</p>
<p>Experiment Design</p>
<p>The knowledge base, K, used for all experiments is a small handcrafted set of commonsense knowledge. See Tab.6 in the Appendix for examples. K includes general information about time, restricteddomains such as setting alarms and notifications, emails, and so on, as well as commonsense knowledge about day-to-day activities. K contains a total of 228 facts and rules. Among these, there are 189 everyday-domain and 39 restricted domain facts and rules. We observed that most of the if-thenbecause commands require everyday-domain knowledge for reasoning, even if they are restricteddomain commands (see Table 3 for example).</p>
<p>Our Neuro-Symbolic theorem prover is trained on proof traces collected by proving automatically generated query's to K using sPyrolog 2 . M rule and M var are initialized randomly and with GloVe embeddings [48], respectively, where m 1 = 256 and m 2 = 300. Since K is type-coerced (e.g. Time, Location, . . . ), initializing the variables with pre-trained word embeddings helps capture their semantics and improves the performance. The neural components of the theorem prover is implemented in PyTorch [49] and the prover is built on top of sPyrolog.</p>
<p>User Study In order to assess CORGI's performance, we ran a user study. We selected 10 goaltype if-then-because commands from the dataset in Table 1 and used each as the prompt for a reasoning task. We had 28 participants in the study, 4 of which were experts closely familiar with CORGI and its capabilities. The rest were undergraduate and graduate students with the majority being in engineering or computer science fields and some that majored in business administration or psychology. These users had never interacted with CORGI prior to the study (novice users). Each person was issued the 10 reasoning tasks, taking on average 20 minutes to complete all 10.</p>
<p>Solving a reasoning task consists of participating in a dialog with CORGI as the system attempts to complete a proof for the goal of the current task; see sample dialogs in Tab. 3. The task succeeds if CORGI is able to use the answers provided by the participant to construct a reasoning chain (proof) leading from the goal to the state and action . We collected 469 dialogues in our study.</p>
<p>The user study was run with the architecture shown in Fig. 1. We used the participant responses from the study to run a few more experiments. We (1) Replace our theorem prover with an oracle prover that selects the optimal rule at each proof step in Alg. 1 and (2) attempt to prove the goal without using any participant responses (no-feedback). Tab. 2 shows the success rate in each setting.</p>
<p>Discussion</p>
<p>In this section, we analyze the results from the study and provide examples of the 4 scenarios in Section 3.1 that we encountered. As hypothesized there, scenario A hardly occurred. We did encounter scenario B, however. The study's dialogs show that some users provided means of sensing the goal rather than the cause of the goal . For example, for the reasoning task "If there are thunderstorms in the forecast within a few hours then remind me to close the windows because I want to keep my home dry", in response to the system's prompt "How do I know if 'I keep my home dry'?" a user responded "if the floor is not wet" as opposed to an answer such as "if the windows are closed". Moreover, some users did not pay attention to the context of the reasoning task. For example, another user responded to the above prompt (same reasoning task) with "if the temperature is above 80"! Overall, we noticed that CORGI's ability to successfully reason about an if-then-because statement was heavily dependent on whether the user knew how to give the system what it needed, and not necessarily what it asked for; see Table 3 for an example. As it can be seen in Table 2, expert users are able to more effectively provide answers that complete CORGI's reasoning chain, likely because they know that regardless of what CORGI asks, the object of the dialog is to connect the because goal back to the knowledge base in some series of if-then rules (goal /sub-goal path in Sec.3.1). Therefore, one interesting future direction is to develop a dynamic context-dependent Natural Language Generation method for asking more effective questions.</p>
<p>We would like to emphasize that although it seems to us, humans, that the previous example requires very simple background knowledge that likely exists in SOTA large commonsense knowledge graphs such as ConcepNet 3 , ATOMIC 4 or COMET [13], this is not the case (verifiable by querying them online). For example, for queries such as "the windows are closed", COMET-ConceptNet generative model 5 returns knowledge about blocking the sun, and COMET-ATOMIC generative model 6 returns knowledge about keeping the house warm or avoiding to get hot; which while being correct, is not applicable in this context. For "my home is dry", both COMET-ConceptNet and COMET-ATOMIC generative models return knowledge about house cleaning or house comfort. On the other hand, the fact that 40% of the novice users in our study were able to help CORGI reason about this example with responses such as "If I close the windows" to CORGI's prompt, is an interesting result. This tells us that conversational interactions with humans could pave the way for commonsense reasoning and enable computers to extract just-in-time commonsense knowledge, which would likely either not exist in large knowledge bases or be irrelevant in the context of the particular reasoning task. Lastly, we re-iterate that as conversational agents (such as Siri and Alexa) enter people's lives, leveraging conversational interactions for learning has become a more realistic opportunity than ever before.</p>
<p>In order to address scenario C.1, the conversational prompts of CORGI ask for specific small pieces of knowledge that can be easily parsed into a predicate and a set of arguments. However, some users in our study tried to provide additional details, which challenged CORGI's natural language understanding. For example, for the reasoning task "If I receive an email about water shut off then remind me about it a day before because I want to make sure I have access to water when I need it.", in response to the system's prompt "How do I know if 'I have access to water when I need it.'?" one user responded "If I am reminded about a water shut off I can fill bottles". This is a successful knowledge transfer. However, the parser expected this to be broken down into two steps. If this user responded to the prompt with "If I fill bottles" first, CORGI would have asked "How do I know if 'I fill bottles'?" and if the user then responded "if I am reminded about a water shut off" CORGI would have succeeded. The success from such conversational interactions are not reflected in the overall performance mainly due to the limitations of natural language understanding. Table 2 evaluates the effectiveness of conversational interactions for proving compared to the nofeedback model. The 0% success rate there reflects the incompleteness of K. The improvement in task success rate between the no-feedback case and the other rows indicates that when it is possible for users to contribute useful common-sense knowledge to the system, performance improves. The users contributed a total number of 96 rules to our knowledge base, 31 of which were unique rules. Scenario C.2 occurs when there is variation in the user's natural language statement and is addressed with our neuro-symbolic theorem prover. Rows 2-3 in Table 2 evaluate our theorem prover (soft unification). Having access to the optimal rule for unification does still better, but the task success rate is not 100%, mainly due to the limitations of natural language understanding explained earlier.</p>
<p>Conclusions</p>
<p>In this paper, we introduced a benchmark task for commonsense reasoning that aims at uncovering unspoken intents that humans can easily uncover in a given statement by making presumptions supported by their common sense. In order to solve this task, we propose CORGI (COmmon-sense ReasoninG by Instruction) which is a neuro-symbolic theorem prover and performs commonsense reasoning by initiating a conversation with a user. CORGI has access to a small knowledge base of commonsense facts and completes it through time as she interacts with the user. We further conduct a user study that indicates the possibility of using conversational interactions with humans for evoking commonsense knowledge and verifies the effectiveness of our proposed theorem prover.</p>
<p>Appendix Data Collection</p>
<p>Data collection was done in two stages. In the first stage, we collected if-then-because commands from humans subjects. In the second stage, a team of annotators annotated the data with commonsense presumptions. Below we explain the details of the data collection and annotation process.</p>
<p>In the data collection stage, we asked a pool of human subjects to write commands that follow the general format: if state holds then perform action because i want to achieve goal . The subjects were given the following instructions at the time of data collection:</p>
<p>" Imagine the two following scenarios: Scenario 1: Imagine you had a personal assistant that has access to your email, calendar, alarm, weather and navigation apps, what are the tasks you would like the assistant to perform for your day-to-day life? And why? Scenario 2: Now imagine you have an assistant/friend that can understand anything. What would you like that assistant/friend to do for you?</p>
<p>Our goal is to collect data in the format "If . . . . then . . . . because . . . ." "</p>
<p>After the data was collected, a team of annotators annotated the commands with additional presumptions that the human subjects have left unspoken. These presumptions were either in the if -clause and/or the then-clause and examples of them are shown in Tables 1 and 4   Table 4: Example if-then-because commands in the data and their annotations. Annotations are tuples of (index, missing text) where index shows the starting word index of where the missing text should be in the command. Index starts at 0 and is calculated for the original utterance.</p>
<p>Utterance Annotation</p>
<p>If the temperature (   ) is above 30 degrees (   )</p>
<p>then remind me to put the leftovers from last night into the fridge because I want the leftovers to stay fresh (2, inside) (7, Celsius)
If it snows (   ) tonight (   )
then wake me up early because I want to arrive to work early (3, more than two inches) (4, and it is a working day)</p>
<p>If it's going to rain in the afternoon (   ) then remind me to bring an umbrella (   )</p>
<p>because I want to stay dry (8, when I am outside) (15, before I leave the house)</p>
<p>Logic Templates</p>
<p>As explained in the main text, we uncovered 5 different logic templates, that reflect humans' reasoning, from the data after data collection. The templates are listed in Table 5. In what follows, we will explain each template in detail using the examples of each template listed in Tab. 5.</p>
<p>In the blue template (Template 1), the state results in a "bad state" that causes the not of the goal. The speaker asks for the action in order to avoid the bad state and achieve the goal . For instance, consider the example for the blue template in Table 5. The state of snowing a lot at night, will result in a bad state of traffic slowdowns which in turn causes the speaker to be late for work. In order to overcome this bad state. The speaker would like to take the action , waking up earlier, to account for the possible slowdowns cause by snow and get to work on time.</p>
<p>In the orange template (Template 2), performing the action when the state holds allows the speaker to achieve the goal and not performing the action when the state holds prevents the speaker from achieving the goal . For instance, in the example for the orange template in Table 5  Table 5: Different reasoning templates of the statements that we uncovered, presumably reflecting how humans logically reason. , , : indicate logical and, negation, and implication, respectively. action h is an action that is hidden in the main utterance and action (state ) indicates performing the action when the state holds.</p>
<p>Logic template</p>
<p>Example Count</p>
<p>1.</p>
<p>((goal ) : state ) (goal : action (state ))</p>
<p>If it snows tonight then wake me up early because I want to arrive to work on time 65 2.</p>
<p>(goal : action (state )) ((goal ) : (action (state )))</p>
<p>If I am walking to a meeting then remind me who else is there because I want to be prepared for the meeting 50 3.  If I am at the grocery store but I have a trip coming up in the next week then remind me not to buy perishables because they will go bad while I am away 5</p>
<p>other</p>
<p>If tomorrow is a holiday then ask me if I want to disable or change my alarms because I don't want to wake up early if I don't need to go to work early. 23 the speaker would like to know who the attendees of a meeting are when the speaker is walking to that meeting so that the speaker is prepared for the meeting and that if the speaker is not reminded of this, he/she will not be able to properly prepare for the meeting.</p>
<p>In the green template (Template 3), performing the action when the state holds allows the speaker to take a hidden action that enables him/her to achieve the desired goal . For example, if the speaker is reminded to buy flower bulbs close to the Fall season, he/she will buy and plant the flowers (hidden action s) that allows the speaker to have a pretty spring garden.</p>
<p>In the purple template (Template 4), the goal that the speaker has stated is actually a goal that they want to avoid. In this case, the state causes the speaker's goal , but the speaker would like to take the action when the state holds to achieve the opposite of the goal . For the example in Tab. 1, if the speaker has a trip coming up and he/she buys perishables the perishables would go bad. In order for this not to happen, the speaker would like to be reminded not to buy perishables to avoid them going bad while he/she is away.</p>
<p>The rest of the statements are categorized under the "other" category. The majority of these statements contain conjunction in their state and are a mix of the above templates. A reasoning engine could potentially benefit from these logic templates when performing reasoning. We provide more detail about this in the Extended Discussion section in the Appendix.</p>
<p>Prolog Background</p>
<p>Prolog [39] is a declarative logic programming language. A Prolog program consists of a set of predicates. A predicate has a name (functor) and N  0 arguments. N is referred to as the arity of the predicate. A predicate with functor name F and arity N is represented as
F (T 1 , . . . , T N ) where T i 's, for i  [1, N ]
, are the arguments that are arbitrary Prolog terms. A Prolog term is either an atom, a variable or a compound term (a predicate with arguments). A variable starts with a capital letter (e.g., Time) and atoms start with small letters (e.g. monday). A predicate defines a relationship between its arguments. For example, isBefore(monday, tuesday) indicates that the relationship between Monday and Tuesday is that, the former is before the latter.</p>
<p>A predicate is defined by a set of clauses. A clause is either a Prolog fact or a Prolog rule. A Prolog rule is denoted with Head : Body., where the Head is a predicate, the Body is a conjunction () of predicates, : is logical implication, and period indicates the end of the clause. The previous rule is an if-then statement that reads "if the Body holds then the Head holds". A fact is a rule whose body always holds, and is indicated by Head. , which is equivalent to Head : true. Rows 1-4 in Table 6 are rules and rows 5-8 are facts.</p>
<p>Prolog can be used to logically "prove" whether a specific query holds or not (For example, to prove that isAfter(wednesday,thursday)? is false or that status(i, dry, tuesday)? is true using the  Figure 3: Sample simplified proof tree for query status(i, dry, tuesday). dashed edges show successful unification, orange nodes show the head of the rule or fact that is retrieved by the unification operator in each step and green nodes show the query in each proof step. This proof tree is obtained using the Prolog program or K shown in Tab. 6. In the first step, unification goes through all the rules and facts in the table and retrieves rule number 2 whose head unifies with the query. This is because the query and the rule head's functor name is status and they both have 3 arguments. Moreover, the arguments all match since Person1 grounds to atom i, grounded atom dry matches in both and variable Date1 grounds to tuesday. In the next step, the proof iterates through the predicates in the rule's body, which are isInside(i, Building1, tuesday) and building(Building1), to recursively prove them one by one using the same strategy. Each of the predicates in the body become the new query to prove and proof succeeds if all the predicates in the body are proved. Note that once the variables are grounded in the head of the rule they are also grounded in the rule's body.</p>
<p>Program in Table 6). The proof is performed through backward chaining, which is a backtracking algorithm that usually employs a depth-first search strategy implemented recursively. In each step of the recursion, the input is a query (goal) to prove and the output is the proof's success/failure. in order to prove a query, a rule or fact whose head unifies with the query is retrieved from the Prolog program. The proof continues recursively for each predicate in the body of the retrieved rule and succeeds if all the statements in the body of a rule are true. The base case (leaf) is when a fact is retrieved from the program.</p>
<p>At the heart of backward chaining is the unification operator, which matches the query with a rule's head. Unification first checks if the functor of the query is the same as the functor of the rule head. If they are the same, unification checks the arguments. If the number of arguments or the arity of the predicates do not match unification fails. Otherwise it iterates through the arguments. For each argument pair, if both are grounded atoms unification succeeds if they are exactly the same grounded atoms. If one is a variable and the other is a grounded atom, unification grounds the variable to the atom and succeeds. If both are variables unification succeeds without any variable grounding. The backwards chaining algorithm and the unification operator is depicted in Figure 3.</p>
<p>Parsing</p>
<p>The goal of our parser is to extract the state , action and goal from the input utterance and convert them to their logical forms S(X), A(Y ), and G(Z), respectively. The parser is built using Spacy [50]. We implement a relation extraction method that uses Spacy's built-in dependency parser. The language model that we used is the en_coref_lg3.0.0 released by Hugging face 7 . The predicate name is typically the sentence verb or the sentence root. The predicate's arguments are the subject, objects, named entities and noun chunks extracted by Spacy. The output of the relation extractor is matched against the knowledge base through rule-based mechanisms including string matching to decide weather the parsed logical form exists in the knowledge base. If a match is found, the parser re-orders the arguments to match the order of the arguments of the predicate retrieved from the knowledge base. This re-ordering is done through a type coercion method. In order to do type coercion, we use the types released by Allen AI in the Aristo tuple KB v1.03 Mar 2017 Release [51] and have added more entries to it to cover more nouns. The released types file is a dictionary that maps different nouns to their types. For example, doctor is of type person and Tuesday is of type</p>
<p>Algorithm 1 Neuro-Symbolic Theorem Prover</p>
<p>Input: goal G(Z), M rule , M var , Model parameters, threshold T 1 , T 2 , k Output: Proof P r 0  0  0 is a vector of 0s P = PROVE(G(Z), r 0 , []) function PROVE(Q, r t , stack) embed Q using the character RNN to obtain q t input q t and r t to the model and compute r t+1 (Equation (2)) compute c t (Equation (2) date. If no match is found, the parsed predicate will be kept as is and CORGI tries to evoke relevant rules conversationally from humans in the user feedback loop in Figure 1.</p>
<p>We would like to note that we refrained from using a grammar parser, particularly because we want to enable open-domain discussions with the users and save the time required for them to learn the system's language. As a result, the system will learn to adapt to the user's language over time since the background knowledge will be accumulated through user interactions, therefore it will be adapted to that user. A negative effect, however, is that if the parser makes a mistake, error will propagate onto the system's future knowledge. This is an interesting future direction that we are planning to address.</p>
<p>Inference</p>
<p>The inference algorithm for our proposed neuro-symbolic theorem prover is given in Alg. 1. In each step t of the proof, given a query Q, we calculate q t and r t from the trained model to compute r t+1 . Next, we choose k entries of M rule corresponding to the top k entries of r t+1 as candidates for the next proof trace. k is set to 5 and is a tuning parameter. For each rule in the top k rules, we attempt to do variable/argument unification by computing the cosine similarity between the arguments of Q and the arguments of the rule's head. If all the corresponding pair of arguments in Q and the rule's head have a similarity higher than threshold, T 1 = 0.9, unification succeeds, otherwise it fails. If unification succeeds, we move to prove the body of that rule. If not, we move to the next rule. Table 7 shows the performance breakdown with respect to the logic templates in Table 5. Currently, CORGI uses a general theorem prover that can prove all the templates. The large variation in performance indicates that taking into account the different templates would improve the performance. For example, the low performance on the green template is expected, since CORGI currently does not support the extraction of a hidden action from the user, and interactions only support extraction 4 notify(Person1, corgi, Action1) :-email(Person1, Action1).</p>
<p>Extended Discussion</p>
<p>5 isBefore(monday, tuesday).</p>
<p>6 has(house, window).</p>
<p>7 isInside(i, home, tuesday).</p>
<p>8 building(home).</p>
<p>of missing goal s. This interesting observation indicates that, even within the same benchmark, we might need to develop several reasoning strategies to solve reasoning problems. Therefore, even if CORGI adapts a general theorem prover, accounting for logic templates in the conversational knowledge extraction component would allow it to achieve better performance on other templates. </p>
<p>'s going to rain in the afternoon (   ) then remind me to bring an umbrella (   ) have an upcoming bill payment (   )</p>
<p>flight (   ) is from 2am to 4am then book me a supershuttle (   )</p>
<p>Figure 1 :
1CORGI's flowchart.</p>
<p>(
goal : action h) (action h : action (state ))If we are approaching Fall then remind me to buy flower bulbs because I want to make sure I have a pretty Spring garden.</p>
<p>: state ) ((goal ) : action (state ))</p>
<p>13, for 2 hours before my flight take off time)2 </p>
<p>Restricted domain 
state 
action conjunction </p>
<p>If I receive emails about sales on basketball shoes (  
 ) </p>
<p>then let me know (  
 ) </p>
<p>because I need them and I want to save money. </p>
<p>(9, my size) 
(13, there is a sale) </p>
<p>2 </p>
<p>SUM 
83 </p>
<p>Everyday domain 
state 
action 
goal </p>
<p>If there is an upcoming election (  
 ) </p>
<p>to schedule our next appointment) then when it comes back on remind me to restart the house furnace because it doesn't come back on by itself and I want to stay warm to infer the hidden presumptions; that is, the additional unstated conditions on the If and/or Then portion of the command (refer to Tab. 1 to see some examples).4 </p>
<p>Everyday domain 
state 
action 
modifier </p>
<p>If I have difficulty sleeping (  
 ) </p>
<p>then play a lullaby 
because it soothes me. </p>
<p>(5, at night) 
12 </p>
<p>Everyday domain 
state 
action conjunction </p>
<p>If the power goes out (  
 ) </p>
<p>(5, in the Winter) 
6 </p>
<p>SUM 
77 </p>
<p>Table 2 :
2percentage of successful rea-
soning tasks for different user types. In 
no-feedback, user responses are not con-
sidered in the proof attempt. in soft 
unification CORGI uses our proposed 
neuro-symbolic theorem prover. In the 
Oracle scenario, the theorem prover has 
access to oracle embeddings and soft 
unification is 100% accurate. </p>
<p>CORGI variations Novice User Expert User </p>
<p>No-feedback 
0% 
0% 
Soft unification 
15.61% 
35.00% 
Oracle unification 
21.62% 
45.71% </p>
<p>Table 3 :
3Sample dialogs of 2 novice users in our study. CORGI's responses are noted in italics.If it's going to rain in the afternoon then remind me to bring an umbrella because I want to remain dry.How do I know if "I remain dry"? If I have my umbrella.How do I know if "I have my umbrella"? If you remind me to bring an umbrella.Successful task </p>
<p>) {R 1 . . . R k }  From M rule retrieve k entries corresponding to the top k entries of r t+1 for i  [0, k] do SU  SOFT_UNIFY(Q, head(R i )) if SU == F alse then continue to i + 1 else if c t &gt; T 2 then return stack add R i to stack PROVE(Body(R i ), r t+1 , stack)  Prove the body of R i return stack function SOFT_UNIFY(G, H)if arity(G) = arity(H) then return False Use M var to compute cosine similarity S i for all corresponding variable pairs in G and H if S i &gt; T 1  i  [0, arity(G)] then return True else return False</p>
<p>Table 6 :
6Examples of the commonsense rules and facts in K isEarlierThan(Time1,Time2) :-isBefore(Time1,Time3), isEarlierThan(Time3,Time2).1 </p>
<p>2 </p>
<p>status(Person1, dry, Date1) :-isInside(Person1, Building1, Date1), 
building(Building1). </p>
<p>3 </p>
<p>status(Person1, dry, Date1) :-weatherBad(Date1, _), 
carry(Person1, umbrella, Date1), 
isOutside(Person1, Date1). </p>
<p>Table 7 :
7Number of successful reasoning tasks vs number of attempts under different scenarios. In CORGI's Oracle unification, soft unification is 100% accurate. LT stands for Logic Template and LTi refers to template i inTable 5.CORGI 
LT1 
LT2 
LT3 
LT5 </p>
<p>Oracle Unification 24% 
38% 
11% 
0% </p>
<p>Our code and data are publicly available here https://github.com/ForoughA/CORGI
https://github.com/leonweber/spyrolog
http://conceptnet.io/ 4 https://mosaickg.apps.allenai.org/kg_atomic 5 https://mosaickg.apps.allenai.org/comet_conceptnet 6 https://mosaickg.apps.allenai.org/comet_atomic
https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_lg-3.0.0/en_coref_lg-3.0.0.tar.gz</p>
<p>The winograd schema challenge. Hector Levesque, Ernest Davis, Leora Morgenstern, Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning. Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012.</p>
<p>Commonsense reasoning and commonsense knowledge in artificial intelligence. Ernest Davis, Gary Marcus, Communications of the ACM. 589Ernest Davis and Gary Marcus. Commonsense reasoning and commonsense knowledge in artificial intelligence. Communications of the ACM, 58(9):92-103, 2015.</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. 2019.</p>
<p>Logic and conversation. P Herbert, Grice, Speech acts. BrillHerbert P Grice. Logic and conversation. In Speech acts, pages 41-58. Brill, 1975.</p>
<p>Understanding natural language. Terry Winograd, Cognitive psychology. 31Terry Winograd. Understanding natural language. Cognitive psychology, 3(1):1-191, 1972.</p>
<p>Commonsense reasoning: an event calculus based approach. T Erik, Mueller, Morgan KaufmannErik T Mueller. Commonsense reasoning: an event calculus based approach. Morgan Kauf- mann, 2014.</p>
<p>Cyc: toward programs with common sense. B Douglas, Lenat, V Ramanathan, Karen Guha, Dexter Pittman, Mary Pratt, Shepherd, Communications of the ACM. 338Douglas B Lenat, Ramanathan V. Guha, Karen Pittman, Dexter Pratt, and Mary Shepherd. Cyc: toward programs with common sense. Communications of the ACM, 33(8):30-49, 1990.</p>
<p>Conceptnet-a practical commonsense reasoning tool-kit. Hugo Liu, Push Singh, BT technology journal. 224Hugo Liu and Push Singh. Conceptnet-a practical commonsense reasoning tool-kit. BT technology journal, 22(4):211-226, 2004.</p>
<p>Conceptnet 3: a flexible, multilingual semantic network for common sense knowledge. Catherine Havasi, Robyn Speer, Jason Alonso, Recent advances in natural language processing. CiteseerCatherine Havasi, Robyn Speer, and Jason Alonso. Conceptnet 3: a flexible, multilingual semantic network for common sense knowledge. In Recent advances in natural language processing, pages 27-29. Citeseer, 2007.</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. pages 4444-4451, 2017.</p>
<p>Atomic: An atlas of machine commonsense for if-then reasoning. Maarten Sap, Ronan Lebras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, A Noah, Yejin Smith, Choi, arXiv:1811.00146arXiv preprintMaarten Sap, Ronan LeBras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Han- nah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. Atomic: An atlas of machine commonsense for if-then reasoning. arXiv preprint arXiv:1811.00146, 2018.</p>
<p>Maarten Hannah Rashkin, Emily Sap, Allaway, A Noah, Yejin Smith, Choi, Event2mind, arXiv:1805.06939Commonsense inference on events, intents, and reactions. arXiv preprintHannah Rashkin, Maarten Sap, Emily Allaway, Noah A Smith, and Yejin Choi. Event2mind: Commonsense inference on events, intents, and reactions. arXiv preprint arXiv:1805.06939, 2018.</p>
<p>Comet: Commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, arXiv:1906.05317arXiv preprintAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. Comet: Commonsense transformers for automatic knowledge graph construction. arXiv preprint arXiv:1906.05317, 2019.</p>
<p>Learning knowledge graphs for question answering through conversational dialog. Ben Hixon, Peter Clark, Hannaneh Hajishirzi, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesBen Hixon, Peter Clark, and Hannaneh Hajishirzi. Learning knowledge graphs for question answering through conversational dialog. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 851-861, 2015.</p>
<p>Instructable intelligent personal agent. Amos Azaria, Jayant Krishnamurthy, Tom M Mitchell, Thirtieth AAAI Conference on Artificial Intelligence. Amos Azaria, Jayant Krishnamurthy, and Tom M Mitchell. Instructable intelligent personal agent. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.</p>
<p>Lia: A natural language programmable personal assistant. Igor Labutov, Shashank Srivastava, Tom Mitchell, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2018 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsIgor Labutov, Shashank Srivastava, and Tom Mitchell. Lia: A natural language programmable personal assistant. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 145-150, 2018.</p>
<p>Teaching Machines to Classify from Natural Language Interactions. Shashank Srivastava, Samsung Electronics. PhD thesisShashank Srivastava. Teaching Machines to Classify from Natural Language Interactions. PhD thesis, Samsung Electronics, 2018.</p>
<p>Learning from natural instructions. Dan Goldwasser, Dan Roth, Machine learning. 942Dan Goldwasser and Dan Roth. Learning from natural instructions. Machine learning, 94(2):205-232, 2014.</p>
<p>Look before you hop: Conversational question answering over knowledge graphs using judicious context expansion. Philipp Christmann, Rishiraj Saha Roy, Abdalghani Abujabal, Jyotsna Singh, Gerhard Weikum, Proceedings of the 28th ACM International Conference on Information and Knowledge Management. the 28th ACM International Conference on Information and Knowledge ManagementPhilipp Christmann, Rishiraj Saha Roy, Abdalghani Abujabal, Jyotsna Singh, and Gerhard Weikum. Look before you hop: Conversational question answering over knowledge graphs using judicious context expansion. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 729-738, 2019.</p>
<p>Dialog-to-action: Conversational question answering over a large-scale knowledge base. Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, Jian Yin, Advances in Neural Information Processing Systems. Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, and Jian Yin. Dialog-to-action: Conversational question answering over a large-scale knowledge base. In Advances in Neural Information Processing Systems, pages 2942-2951, 2018.</p>
<p>Appinite: A multi-modal interface for specifying data descriptions in programming by demonstration using natural language instructions. Toby Jia-Jun Li, Igor Labutov, Nancy Xiaohan, Xiaoyi Li, Wenze Zhang, Wanling Shi, Ding, M Tom, Brad A Mitchell, Myers, 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). IEEEToby Jia-Jun Li, Igor Labutov, Xiaohan Nancy Li, Xiaoyi Zhang, Wenze Shi, Wanling Ding, Tom M Mitchell, and Brad A Myers. Appinite: A multi-modal interface for specifying data descriptions in programming by demonstration using natural language instructions. In 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), pages 105- 114. IEEE, 2018.</p>
<p>Programming iot devices by demonstration using mobile apps. Toby Jia-Jun Li, Yuanchun Li, Fanglin Chen, Brad A Myers, International Symposium on End User Development. SpringerToby Jia-Jun Li, Yuanchun Li, Fanglin Chen, and Brad A Myers. Programming iot devices by demonstration using mobile apps. In International Symposium on End User Development, pages 3-17. Springer, 2017.</p>
<p>Sugilite: creating multimodal smartphone automation by demonstration. Toby Jia-Jun Li, Amos Azaria, Brad A Myers, Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. the 2017 CHI Conference on Human Factors in Computing SystemsACMToby Jia-Jun Li, Amos Azaria, and Brad A Myers. Sugilite: creating multimodal smartphone automation by demonstration. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pages 6038-6049. ACM, 2017.</p>
<p>A review of winograd schema challenge datasets and approaches. Vid Kocijan, Thomas Lukasiewicz, Ernest Davis, Gary Marcus, Leora Morgenstern, arXiv:2004.13831arXiv preprintVid Kocijan, Thomas Lukasiewicz, Ernest Davis, Gary Marcus, and Leora Morgenstern. A re- view of winograd schema challenge datasets and approaches. arXiv preprint arXiv:2004.13831, 2020.</p>
<p>Lsdsem 2017 shared task: The story cloze test. Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, James Allen, Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics. the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level SemanticsNasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46-51, 2017.</p>
<p>Choice of plausible alternatives: An evaluation of commonsense causal reasoning. Melissa Roemmele, Andrew S Cosmin Adrian Bejan, Gordon, 2011 AAAI Spring Symposium Series. Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alter- natives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.</p>
<p>Abductive commonsense reasoning. Chandra Bhagavatula, Chaitanya Ronan Le Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Scott Downey, Yih Wen-Tau, Yejin Choi, International Conference on Learning Representations (ICLR. 2020Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen-tau Yih, and Yejin Choi. Abductive commonsense reasoning. In International Conference on Learning Representations (ICLR), 2020.</p>
<p>Neuralsymbolic learning and reasoning: contributions and challenges. Artur D&apos;avila Garcez, R Tarek, Luc Besold, Peter De Raedt, Pascal Fldiak, Thomas Hitzler, Kai-Uwe Icard, Khnberger, C Luis, Risto Lamb, Daniel L Miikkulainen, Silver, 2015 AAAI Spring Symposium Series. Artur d'Avila Garcez, Tarek R Besold, Luc De Raedt, Peter Fldiak, Pascal Hitzler, Thomas Icard, Kai-Uwe Khnberger, Luis C Lamb, Risto Miikkulainen, and Daniel L Silver. Neural- symbolic learning and reasoning: contributions and challenges. In 2015 AAAI Spring Sympo- sium Series, 2015.</p>
<p>Artur D&apos;avila Tarek R Besold, Sebastian Garcez, Howard Bader, Pedro Bowman, Pascal Domingos, Kai-Uwe Hitzler, Khnberger, C Luis, Daniel Lamb, Priscila Machado Vieira Lowd, Lima, arXiv:1711.03902Neural-symbolic learning and reasoning: A survey and interpretation. arXiv preprintTarek R Besold, Artur d'Avila Garcez, Sebastian Bader, Howard Bowman, Pedro Domingos, Pascal Hitzler, Kai-Uwe Khnberger, Luis C Lamb, Daniel Lowd, Priscila Machado Vieira Lima, et al. Neural-symbolic learning and reasoning: A survey and interpretation. arXiv preprint arXiv:1711.03902, 2017.</p>
<p>The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, Jiajun Wu, Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neuro- symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. 2019.</p>
<p>Deepproblog: Neural probabilistic logic programming. Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De Raedt, Advances in Neural Information Processing Systems. Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. Deepproblog: Neural probabilistic logic programming. In Advances in Neural Information Processing Systems, pages 3749-3759, 2018.</p>
<p>Neural logic machines. Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, Denny Zhou, International Conference on Learning Representations (ICLR). Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic machines. In International Conference on Learning Representations (ICLR), 2019.</p>
<p>Integrating learning and reasoning with deep logic models. Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Marco Gori, arXiv:1901.04195arXiv preprintGiuseppe Marra, Francesco Giannini, Michelangelo Diligenti, and Marco Gori. Integrating learning and reasoning with deep logic models. arXiv preprint arXiv:1901.04195, 2019.</p>
<p>Abductive learning: towards bridging machine learning and logical reasoning. Zhi-Hua Zhou, Science China Information Sciences. 62776101Zhi-Hua Zhou. Abductive learning: towards bridging machine learning and logical reasoning. Science China Information Sciences, 62(7):76101, 2019.</p>
<p>Learning explanatory rules from noisy data. Richard Evans, Edward Grefenstette, Journal of Artificial Intelligence Research. 61Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal of Artificial Intelligence Research, 61:1-64, 2018.</p>
<p>Analogyspace: Reducing the dimensionality of common sense knowledge. Robyn Speer, Catherine Havasi, Henry Lieberman, AAAI. 8Robyn Speer, Catherine Havasi, and Henry Lieberman. Analogyspace: Reducing the dimen- sionality of common sense knowledge. In AAAI, volume 8, pages 548-553, 2008.</p>
<p>Digital intuition: Applying common sense using dimensionality reduction. Catherine Havasi, Robyn Speer, James Pustejovsky, Henry Lieberman, IEEE Intelligent systems. 244Catherine Havasi, Robyn Speer, James Pustejovsky, and Henry Lieberman. Digital intuition: Applying common sense using dimensionality reduction. IEEE Intelligent systems, 24(4):24- 35, 2009.</p>
<p>William W Cohen, arXiv:1605.06523Tensorlog: A differentiable deductive database. arXiv preprintWilliam W Cohen. Tensorlog: A differentiable deductive database. arXiv preprint arXiv:1605.06523, 2016.</p>
<p>An introduction to prolog iii. Alain Colmerauer, Computational Logic. SpringerAlain Colmerauer. An introduction to prolog iii. In Computational Logic, pages 37-79. Springer, 1990.</p>
<p>. Scott Reed, Nando De Freitas, arXiv:1511.06279Neural programmer-interpreters. arXiv preprintScott Reed and Nando De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015.</p>
<p>End-to-end differentiable proving. Tim Rocktschel, Sebastian Riedel, Advances in Neural Information Processing Systems. Tim Rocktschel and Sebastian Riedel. End-to-end differentiable proving. In Advances in Neural Information Processing Systems, pages 3788-3800, 2017.</p>
<p>Nlprolog: Reasoning with weak unification for question answering in natural language. L Weber, Minervini, Mnchmeyer, Leser, Proceedings of the 57th. the 57thL Weber, P Minervini, J Mnchmeyer, U Leser, and T Rocktschel. Nlprolog: Reasoning with weak unification for question answering in natural language. In Proceedings of the 57th</p>
<p>Annual Meeting of the Association for Computational Linguistics. Florence, ItalyAssociation for Computational Linguistics1Annual Meeting of the Association for Computational Linguistics, ACL 2019, Florence, Italy, Volume 1: Long Papers, volume 57. ACL (Association for Computational Linguistics), 2019.</p>
<p>Deeplogic: Towards end-to-end differentiable logical reasoning. Nuri Cingillioglu, Alessandra Russo, arXiv:1805.07433arXiv preprintNuri Cingillioglu and Alessandra Russo. Deeplogic: Towards end-to-end differentiable logical reasoning. arXiv preprint arXiv:1805.07433, 2018.</p>
<p>Low-dimensional embeddings of logic. Tim Rocktschel, Matko Bonjak, Sameer Singh, Sebastian Riedel, Proceedings of the ACL 2014 Workshop on Semantic Parsing. the ACL 2014 Workshop on Semantic ParsingTim Rocktschel, Matko Bonjak, Sameer Singh, and Sebastian Riedel. Low-dimensional embeddings of logic. In Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 45-49, 2014.</p>
<p>Learning first-order logic embeddings via matrix factorization. Yang William, William W Wang, Cohen, IJCAI. William Yang Wang and William W Cohen. Learning first-order logic embeddings via matrix factorization. In IJCAI, pages 2132-2138, 2016.</p>
<p>Bernd Finkbeiner, Christopher Hahn, Markus N Rabe, Frederik Schmitt, arXiv:2003.04218Teaching temporal logics to neural networks. arXiv preprintBernd Finkbeiner, Christopher Hahn, Markus N Rabe, and Frederik Schmitt. Teaching tempo- ral logics to neural networks. arXiv preprint arXiv:2003.04218, 2020.</p>
<p>Peter Clark, Oyvind Tafjord, Kyle Richardson, arXiv:2002.05867Transformers as soft reasoners over language. arXiv preprintPeter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over lan- guage. arXiv preprint arXiv:2002.05867, 2020.</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543, 2014.</p>
<p>Automatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.</p>
<p>spacy 2: Natural language understanding with bloom embeddings. Matthew Honnibal, Ines Montani, Convolutional Neural Networks and Incremental Parsing. Matthew Honnibal and Ines Montani. spacy 2: Natural language understanding with bloom embeddings. Convolutional Neural Networks and Incremental Parsing, 2017.</p>
<p>Domain-targeted, high precision knowledge extraction. Niket Bhavana Dalvi Mishra, Peter Tandon, Clark, Transactions of the Association for Computational Linguistics. 5Bhavana Dalvi Mishra, Niket Tandon, and Peter Clark. Domain-targeted, high precision knowl- edge extraction. Transactions of the Association for Computational Linguistics, 5:233-246, 2017.</p>            </div>
        </div>

    </div>
</body>
</html>
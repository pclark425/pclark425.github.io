<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9735 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9735</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9735</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-267898009</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14860v4.pdf" target="_blank">Ranking Large Language Models without Ground Truth</a></p>
                <p><strong>Paper Abstract:</strong> Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover close to true rankings without reference data. This points to a viable low-resource mechanism for practical use.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9735.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9735.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prometheus-2 vs Humans (summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prometheus-2 LLM evaluator compared to human annotations on summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison in this paper between a state-of-the-art LLM evaluator (Prometheus-2) used as an automatic judge and human annotations/ground-truth-derived rankings on CNN/DM summarization (and SummEval human judgments); shows lower agreement and systematic differences (e.g., overly pessimistic judgments) when using the LLM as judge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prometheus 2: An open source language model specialized in evaluating other language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Summarization (CNN/DailyMail; comparison to SummEval human annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Prometheus-2</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise comparisons: Prometheus-2 was prompted with two model summaries and a rubric (prompt template provided) and asked to produce feedback and declare a winner ('Which is the better response?'). Pairwise judgements were collected for all pairs of LLMs on a sample of 50 CNN/DM questions for a set of 10 models; win rates were aggregated into a ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations from the SummEval-style protocol: humans evaluate summaries along attributes such as coherence, fluency, consistency, and relevance (SummEval dataset referenced). For the pairwise/ground-truth rankings used to compute 'true' ranking, reference summaries from the dataset were used to derive true rankings; humans in SummEval provide attribute judgments used for comparison to the LLM judge.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Compared to 'true' rankings (derived from references), Prometheus-2 achieved RBO = 0.806 ± 0.07, MAP-3 = 0.200 ± 0.18, MAP-5 = 0.354 ± 0.28 (Table 3). The paper also reports poor qualitative agreement between Prometheus-2 and human annotations on summarization dimensions (see Figure 15) and describes Prometheus-2 as 'overly pessimistic' across dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using an LLM as judge loses alignment with human judgments in key qualitative dimensions: (1) systematic bias in evaluation (e.g., being overly pessimistic), (2) poorer agreement with reference/human-derived rankings compared to the proposed triplet-ranking methods, (3) inability to reliably reproduce nuanced human preferences across attributes like coherence/fluency/consistency/relevance, and (4) reliance on a single 'trusted' LLM judge which may introduce its own biases.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Prometheus-2's annotations show poor agreement with human annotations from the SummEval dimensions (paper states 'poor agreement' and shows Figure 15). In the CNN/DM pairwise study (50 questions, 10 models) Prometheus-2 yields lower ranking metrics (RBO and MAP) than the paper's FTR/GTR methods (Table 3), indicating degraded ranking fidelity relative to human/reference-based ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>LLM judges are cheaper and can produce metric-free, criterion-driven evaluations (e.g., 'child-friendliness', 'professional tone') and thus are useful where human labels are prohibitive. The paper's triplet-based methods (which do not require a trusted LLM judge) outperform Prometheus-2 on ranking metrics here, suggesting that substituting a single LLM judge with relative inter-model comparisons can mitigate some losses. Also, for certain tasks or simple discrete outputs (e.g., multiple-choice with many options or ensembling/MCA), automated consolidation can perform well.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections: 'Comparison with LLM-as-a-judge' (Results), Appendix C 'LLM as a Judge', Table 3, Figure 15.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ranking Large Language Models without Ground Truth', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9735.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9735.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Known LLM-judge biases (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Positional, knowledge, and formatting biases of LLM-as-a-judge (reported in prior work and cited here)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior work demonstrating that LLM judgments can suffer from positional bias (preference for first answer), knowledge bias (lacking required domain knowledge), and formatting bias (sensitivity to presentation), and notes the additional limitation that choosing a single trusted LLM judge assumes we know which model to trust.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judgelm: Fine-tuned large language models are scalable judges</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General (applies across evaluation tasks including summarization, dialog, generative tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>various LLMs discussed in prior work (JudgeLM and related studies); no single model used in this mention</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Literature-reported LLM-as-judge setups: prompt-based comparisons or direct scoring of outputs; the paper references these prior characterizations rather than running them itself for this statement.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>NA for this mention (this entry summarizes known limitations reported in related work comparing LLM judges to humans).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>This related-work mention does not report a single numeric agreement metric here, but cites Zhu et al. (2023) and other literature documenting biases; the present paper empirically compares to human/reference rankings in other sections.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When substituting LLM judges for humans one risks: positional bias (preference for answers presented first), knowledge bias (judge lacks domain expertise), formatting bias (judgments influenced by surface formatting), and the conceptual risk of assuming a single LLM reflects target human values; these reduce reliability and fidelity to human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper cites Zhu et al. (2023) which demonstrated positional/knowledge/formatting biases; the current paper references these as known limitations of LLM-as-a-judge approaches and motivates the triplet approach to avoid needing a single trusted judge.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>LLM judges can scale cheaply and can be fine-tuned (e.g., JudgeLM/PandaLM) to better reflect human preferences; they may be adequate when task criteria are clear and within the LLM's competence. The paper's own experiments show that an LLM judge can be used (Prometheus-2) but may underperform versus methods that exploit inter-model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Related Work section ('LLM-as-a-judge'), citations to Zhu et al. (2023) and Chiang & Lee (2023); discussion in Introduction motivating triplet approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ranking Large Language Models without Ground Truth', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9735.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9735.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human vs algorithmic ranking (dialog example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison between human annotators and FTR algorithm on dialog (Moral Integrity Corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Practical dialog use case where human annotators selected best responses from four fine-tuned variants; the FTR method (using entailment probability and ROUGE) recovered best and worst models but swapped two middle-ranked models compared to the three human annotators, illustrating differences between machine-ranking approaches and human subjective judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Dialogue (Moral Integrity Corpus; moral/ethical dialogue responses)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>N/A (FTR used entailment probability + ROUGE as evaluation function; no external LLM judge was used for ranking in this case)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>FTR used entailment probabilities (DeBERTa-derived NLI probabilities) and ROUGE scores as the triplet-evaluation function; comparisons performed across all triplets for 4 models and 100 prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Three human annotators (external to paper) judged 4 model responses for each of 100 prompts by choosing the single 'best' response (no further attribute rubric provided).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Ranking comparison: FTR output ranking [M1, M3, M2, PT] vs human annotators' aggregate ranking [M1, M2, M3, PT]. The paper reports that best and worst were matched; middle two were flipped and 'quite close' in performance (no numeric agreement metric reported for this specific dialog comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>This example illustrates that algorithmic judges (or algorithmic ranking methods) can disagree with humans on subjective middle cases; what is lost when replacing humans includes nuanced subjective preference and annotator-specific moral judgments that are hard to capture with automated scoring functions (entailment/ROUGE).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>For the Moral Integrity dialog set, although the automated FTR method correctly found the top and bottom models, it ranked M3 ahead of M2 whereas human annotators ranked M2 above M3 — a divergence attributed to the closeness of the two models and the subjective nature of human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The algorithm matched humans on the most important extremes (best and worst), suggesting automated methods may suffice for coarse-grained selection; paper also notes human annotator judgments are subjective, so some disagreement may reflect annotator variability rather than algorithmic failure.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Results section 'Dialog' and Appendix A.2 (dialog experiment description).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ranking Large Language Models without Ground Truth', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judgelm: Fine-tuned large language models are scalable judges <em>(Rating: 2)</em></li>
                <li>Prometheus 2: An open source language model specialized in evaluating other language models <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Summeval: Re-evaluating summarization evaluation <em>(Rating: 2)</em></li>
                <li>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9735",
    "paper_id": "paper-267898009",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Prometheus-2 vs Humans (summarization)",
            "name_full": "Prometheus-2 LLM evaluator compared to human annotations on summarization",
            "brief_description": "Empirical comparison in this paper between a state-of-the-art LLM evaluator (Prometheus-2) used as an automatic judge and human annotations/ground-truth-derived rankings on CNN/DM summarization (and SummEval human judgments); shows lower agreement and systematic differences (e.g., overly pessimistic judgments) when using the LLM as judge.",
            "citation_title": "Prometheus 2: An open source language model specialized in evaluating other language models",
            "mention_or_use": "use",
            "task_domain": "Summarization (CNN/DailyMail; comparison to SummEval human annotations)",
            "llm_judge_model": "Prometheus-2",
            "llm_judge_setup": "Pairwise comparisons: Prometheus-2 was prompted with two model summaries and a rubric (prompt template provided) and asked to produce feedback and declare a winner ('Which is the better response?'). Pairwise judgements were collected for all pairs of LLMs on a sample of 50 CNN/DM questions for a set of 10 models; win rates were aggregated into a ranking.",
            "human_evaluation_setup": "Human annotations from the SummEval-style protocol: humans evaluate summaries along attributes such as coherence, fluency, consistency, and relevance (SummEval dataset referenced). For the pairwise/ground-truth rankings used to compute 'true' ranking, reference summaries from the dataset were used to derive true rankings; humans in SummEval provide attribute judgments used for comparison to the LLM judge.",
            "agreement_metric": "Compared to 'true' rankings (derived from references), Prometheus-2 achieved RBO = 0.806 ± 0.07, MAP-3 = 0.200 ± 0.18, MAP-5 = 0.354 ± 0.28 (Table 3). The paper also reports poor qualitative agreement between Prometheus-2 and human annotations on summarization dimensions (see Figure 15) and describes Prometheus-2 as 'overly pessimistic' across dimensions.",
            "losses_identified": "Using an LLM as judge loses alignment with human judgments in key qualitative dimensions: (1) systematic bias in evaluation (e.g., being overly pessimistic), (2) poorer agreement with reference/human-derived rankings compared to the proposed triplet-ranking methods, (3) inability to reliably reproduce nuanced human preferences across attributes like coherence/fluency/consistency/relevance, and (4) reliance on a single 'trusted' LLM judge which may introduce its own biases.",
            "examples_of_loss": "Prometheus-2's annotations show poor agreement with human annotations from the SummEval dimensions (paper states 'poor agreement' and shows Figure 15). In the CNN/DM pairwise study (50 questions, 10 models) Prometheus-2 yields lower ranking metrics (RBO and MAP) than the paper's FTR/GTR methods (Table 3), indicating degraded ranking fidelity relative to human/reference-based ranking.",
            "counterexamples_or_caveats": "LLM judges are cheaper and can produce metric-free, criterion-driven evaluations (e.g., 'child-friendliness', 'professional tone') and thus are useful where human labels are prohibitive. The paper's triplet-based methods (which do not require a trusted LLM judge) outperform Prometheus-2 on ranking metrics here, suggesting that substituting a single LLM judge with relative inter-model comparisons can mitigate some losses. Also, for certain tasks or simple discrete outputs (e.g., multiple-choice with many options or ensembling/MCA), automated consolidation can perform well.",
            "paper_reference": "Sections: 'Comparison with LLM-as-a-judge' (Results), Appendix C 'LLM as a Judge', Table 3, Figure 15.",
            "uuid": "e9735.0",
            "source_info": {
                "paper_title": "Ranking Large Language Models without Ground Truth",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Known LLM-judge biases (related work)",
            "name_full": "Positional, knowledge, and formatting biases of LLM-as-a-judge (reported in prior work and cited here)",
            "brief_description": "The paper cites prior work demonstrating that LLM judgments can suffer from positional bias (preference for first answer), knowledge bias (lacking required domain knowledge), and formatting bias (sensitivity to presentation), and notes the additional limitation that choosing a single trusted LLM judge assumes we know which model to trust.",
            "citation_title": "Judgelm: Fine-tuned large language models are scalable judges",
            "mention_or_use": "mention",
            "task_domain": "General (applies across evaluation tasks including summarization, dialog, generative tasks)",
            "llm_judge_model": "various LLMs discussed in prior work (JudgeLM and related studies); no single model used in this mention",
            "llm_judge_setup": "Literature-reported LLM-as-judge setups: prompt-based comparisons or direct scoring of outputs; the paper references these prior characterizations rather than running them itself for this statement.",
            "human_evaluation_setup": "NA for this mention (this entry summarizes known limitations reported in related work comparing LLM judges to humans).",
            "agreement_metric": "This related-work mention does not report a single numeric agreement metric here, but cites Zhu et al. (2023) and other literature documenting biases; the present paper empirically compares to human/reference rankings in other sections.",
            "losses_identified": "When substituting LLM judges for humans one risks: positional bias (preference for answers presented first), knowledge bias (judge lacks domain expertise), formatting bias (judgments influenced by surface formatting), and the conceptual risk of assuming a single LLM reflects target human values; these reduce reliability and fidelity to human preferences.",
            "examples_of_loss": "Paper cites Zhu et al. (2023) which demonstrated positional/knowledge/formatting biases; the current paper references these as known limitations of LLM-as-a-judge approaches and motivates the triplet approach to avoid needing a single trusted judge.",
            "counterexamples_or_caveats": "LLM judges can scale cheaply and can be fine-tuned (e.g., JudgeLM/PandaLM) to better reflect human preferences; they may be adequate when task criteria are clear and within the LLM's competence. The paper's own experiments show that an LLM judge can be used (Prometheus-2) but may underperform versus methods that exploit inter-model comparisons.",
            "paper_reference": "Related Work section ('LLM-as-a-judge'), citations to Zhu et al. (2023) and Chiang & Lee (2023); discussion in Introduction motivating triplet approach.",
            "uuid": "e9735.1",
            "source_info": {
                "paper_title": "Ranking Large Language Models without Ground Truth",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Human vs algorithmic ranking (dialog example)",
            "name_full": "Comparison between human annotators and FTR algorithm on dialog (Moral Integrity Corpus)",
            "brief_description": "Practical dialog use case where human annotators selected best responses from four fine-tuned variants; the FTR method (using entailment probability and ROUGE) recovered best and worst models but swapped two middle-ranked models compared to the three human annotators, illustrating differences between machine-ranking approaches and human subjective judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Dialogue (Moral Integrity Corpus; moral/ethical dialogue responses)",
            "llm_judge_model": "N/A (FTR used entailment probability + ROUGE as evaluation function; no external LLM judge was used for ranking in this case)",
            "llm_judge_setup": "FTR used entailment probabilities (DeBERTa-derived NLI probabilities) and ROUGE scores as the triplet-evaluation function; comparisons performed across all triplets for 4 models and 100 prompts.",
            "human_evaluation_setup": "Three human annotators (external to paper) judged 4 model responses for each of 100 prompts by choosing the single 'best' response (no further attribute rubric provided).",
            "agreement_metric": "Ranking comparison: FTR output ranking [M1, M3, M2, PT] vs human annotators' aggregate ranking [M1, M2, M3, PT]. The paper reports that best and worst were matched; middle two were flipped and 'quite close' in performance (no numeric agreement metric reported for this specific dialog comparison).",
            "losses_identified": "This example illustrates that algorithmic judges (or algorithmic ranking methods) can disagree with humans on subjective middle cases; what is lost when replacing humans includes nuanced subjective preference and annotator-specific moral judgments that are hard to capture with automated scoring functions (entailment/ROUGE).",
            "examples_of_loss": "For the Moral Integrity dialog set, although the automated FTR method correctly found the top and bottom models, it ranked M3 ahead of M2 whereas human annotators ranked M2 above M3 — a divergence attributed to the closeness of the two models and the subjective nature of human judgments.",
            "counterexamples_or_caveats": "The algorithm matched humans on the most important extremes (best and worst), suggesting automated methods may suffice for coarse-grained selection; paper also notes human annotator judgments are subjective, so some disagreement may reflect annotator variability rather than algorithmic failure.",
            "paper_reference": "Results section 'Dialog' and Appendix A.2 (dialog experiment description).",
            "uuid": "e9735.2",
            "source_info": {
                "paper_title": "Ranking Large Language Models without Ground Truth",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judgelm: Fine-tuned large language models are scalable judges",
            "rating": 2,
            "sanitized_title": "judgelm_finetuned_large_language_models_are_scalable_judges"
        },
        {
            "paper_title": "Prometheus 2: An open source language model specialized in evaluating other language models",
            "rating": 2,
            "sanitized_title": "prometheus_2_an_open_source_language_model_specialized_in_evaluating_other_language_models"
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Summeval: Re-evaluating summarization evaluation",
            "rating": 2,
            "sanitized_title": "summeval_reevaluating_summarization_evaluation"
        },
        {
            "paper_title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization",
            "rating": 1,
            "sanitized_title": "pandalm_an_automatic_evaluation_benchmark_for_llm_instruction_tuning_optimization"
        }
    ],
    "cost": 0.012933,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Ranking Large Language Models without Ground Truth</p>
<p>Amit Dhurandhar 
IBM Research
NYUSA</p>
<p>Rahul Nair 
IBM Research
Ireland</p>
<p>Moninder Singh 
IBM Research
NYUSA</p>
<p>Elizabeth Daly 
IBM Research
Ireland</p>
<p>Karthikeyan Natesan Ramamurthy 
IBM Research
NYUSA</p>
<p>Ranking Large Language Models without Ground Truth
20886480A2457C0C94D6DF23EB90F298
Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact.Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable.In this paper, we provide a novel perspective where, given a dataset of prompts (viz.questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses.Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability.We also analyze our idea and provide sufficient conditions for it to succeed.Applying this idea repeatedly, we propose two methods to rank LLMs.In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover close to true rankings without reference data.This points to a viable low-resource mechanism for practical use 1 .* Equal contribution 1 Code available at https://huggingface.co/spaces/ ibm/llm-rank-themselves.</p>
<p>Introduction</p>
<p>Recent advancement in LLM capabilities have resulted in a significant challenge for assessing or measuring these capabilities.To benchmark LLM performance, we need a set of input prompts, outputs from the LLMs for these prompts, and a metric that measures how good the LLM performance is on its own or in comparison with other models.</p>
<p>A common strategy in benchmarking is to use datasets that come with both input prompts and reference ("ground truth") responses in which case automated metrics can assess the model responses by comparing them with reference responses.These The three models M1, M2 and M3 have accuracies of 80%, 60% and 40% respectively based on their responses to five prompts (green are correct responses and red incorrect) when compared with the ground truth which is unknown to us.Our triplet approach ranks M3 as the worst model, since it is ranked as such by both M1 (only two answers match) and M2 (only one answer matches).This core idea (with slight variations) can be applied repeatedly to rank an arbitrary number of models as described by the algorithms in Section 3. benchmarks have been shown to be sensitive to minor perturbations (Zhu et al., 2023) and the growing capabilities of LLMs can quickly render a static benchmark out-of-date (Laskar et al., 2023)."Ground truth" labels are more readily available in classification and multi-choice question answering but the problem of collecting labels is more challenging in the context of generative tasks.Additionally, research has also shown benchmarks which target certain metrics can be a poor proxy for assessing an LLM's performance (Ethayarajh and Jurafsky, 2020).</p>
<p>For these reasons, the idea of LLM-as-a-judge is starting to be employed in several scenarios in order to serve as a more accurate proxy for human preferences, which cannot be captured easily by simple metrics.However, LLM-as-a-judge may suffer biases (Zhu et al., 2023) and also assume the pre-selected choice of a trusted LLM to serve as the judge.This paper aims to address the scenarios under which we cannot determine which LLMs can arXiv:2402.14860v4[cs.CL] 10 Jun 2024 be trusted as judges.</p>
<p>We consider the setup where we have n LLMs and a dataset of prompts, which could be questions like in Q&amp;A datasets or instructions such as in summarization or in other extractive/generative tasks.Our goal then is to rank these n LLMs in the order of their performance on the chosen task.A priori, we do not assume anything about the quality of the models nor do we assume access to "ground truth" or reference responses that can act as the golden standard.</p>
<p>The core idea of our method, which can be leveraged to design algorithms that rank models in practice, is illustrated in Figure 1.We consider three models at a time and let each one judge the rest.Based on the cumulative judgements, we decide which is the worst model for that round.Note that if we consider only two models, we will not be able to decide the worst model in the round with much trust, since there is no a priori assumption on model quality.The triplet of models idea stems from the real-life intuition that an expert in an area should be able distinguish between a knowledgeable person and a novice.The knowledgeable person should also be able to rank the novice lower than the expert.Hence, both the expert and the knowledgeable person will rank the novice lower than the other and this consensus will help us identify the novice.This core idea is a part of both the greedy and full ranking methods we propose in this paper (Algorithms 1 and 2).Other strategies may also be possible that stem from this idea.Being a judge in Figure 1 does not necessarily imply only prompting the judge to choose between two responses, but one could also use NLP metrics (viz.ROUGE, BERT Score, NLI models) to find closeness of the responses of other models to the judge's responses, and pick the closest response as the winner.In contrast to our approach, in recommender systems, items are recommended to users based on some notion of user preference, whereas here the LLMs (equivalently considered as items) rank each other without any other intervention or availability of partial ratings.</p>
<p>As a part of this work, we analyze the triplet approach and provide conditions for it to succeed.We also discuss the time complexities of the proposed methods.We show that using summarization, multiple choice question-answering, and dialog as applications, our methods retrieve rankings reliably without any reference data.We exceedingly see the benefit of this approach in summarization and dialog, that is tasks where the responses are long texts, rather than single tokens such as in multiple choice, as other strategies such as ensembling of responses could be employed in the latter case.</p>
<p>Related Work</p>
<p>Benchmarking Benchmarking is heavily relied upon by the community to assess the performance of LLMs.It is viewed as one of the most important problems that need to be addressed with urgency, but it is also recognized that there are no "onesize-fits-all" solutions to this problem.In addition to performance benchmarking, there can be trust and safety benchmarking along different dimensions such as bias, toxicity, social stigma, stereotype, privacy, adversarial robustness, social norms, and machine ethics.There are also comprehensive benchmarks such as HELM (Liang et al., 2023) that include both performance and safety measures.The benchmarking results are conveniently hosted in locations such as the Open LLM Leaderboard (Beeching et al., 2023;Gao et al., 2023), HELM leaderboard (Liang et al., 2023), TrustLLM leaderboard (Sun et al., 2024), and LLM Safety leaderboard (Wang et al., 2023a).</p>
<p>For tasks with binary labels such as classification, or multiple choice question answering (MCQA), usually the metric used is accuracy.(Ye et al., 2024) discuss the necessity for incorporating uncertainty when measuring an LLM's performance.For generative tasks, the metrics used can be based on comparison of the generated text to a reference text such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), ROUGE-N (Lin and Hovy, 2003), METEOR (Banerjee and Lavie, 2005), and BERT score (Zhang et al., 2020).Natural Language Inferencing (NLI) benchmarks require human level understanding in order to assess given a premise statement whether it supports (entailment), contradicts or has no relationship (neutral) in relation to a hypothesis statement (Dagan et al., 2013;Bowman et al., 2015).Libraries such as Unitxt (Bandel et al., 2024) combine these metrics and many others in a customizable manner.Computing accuracy or a related measure on human judgements or preferences is also a very common approach for benchmarking.</p>
<p>Human preference of model responses along the dimensions of helpfulness and harmlessness are collected in the HH-RLHF dataset (Bai et al., 2022).Another significant dataset with quality labels and human preferences of model responses is OpenAssistant Conversations (Köpf et al., 2023).Human preference data for model responses to 80 multi-turn questions was collected in the MT-Bench dataset (Zheng et al., 2023).</p>
<p>Model Selection There are benchmarking deployments in the wild such as Chatbot Arena2 (Zheng et al., 2023) where humans provide the input prompts as well as provide their preference based on the output of two models.Chatbot Arena uses the Elo rating system (Elo, 1967), popularly used for rating Chess players, to rank LLMs.While collecting the input prompts to create a dataset is time-consuming, it is even more challenging to collect human labels, judgements, or preferences, particularly in cases where the deployment of models need to be made domain-specific.The number of applications and use cases may be so varied, and the human expertise need may be so high, that it will be too expensive to collect such data.</p>
<p>LLM-as-a-judge Trusted LLMs have been used as a proxy for human preferences and feedback (Chiang and Lee, 2023).Collecting human preferences are costly, however it has been shown metrics associated with benchmarks are not rich enough to capture the many and nuanced aspects that quantify what represents a good output a person might prefer.PandaLM (Wang et al., 2023b) is trained on human-annotations in order to evaluate model performance on criteria difficult to capture in existing benchmarks reflecting human preferences in order to select a model out of several candidates.Other applications include acting as substitute for a traditional measurement of a user defined criteria.Instead of creating a traditional classification or regression model, the practitioner can specify the custom criteria in free text 3 .More generally, LLM-as-a-judge has also been used to rank model performance (Chiang et al., 2023).</p>
<p>While more scalable than human-evaluations, leveraging an LLM-as-a-judge has some limitations.Zhu et al. (2023) demonstrated that LLM judgments may suffer from positional bias (e.g.prefers the first answer), knowledge bias (e.g.lacks the knowledge required) and formatting bias (e.g. has a preference for how the judgement is presented to the LLM).Another limitation is the assumption of one true judge that reflects the preferences and values the solution requires.In the case of seeking feedback on answers that appear natural we can imagine many LLMs are capable of this task, however if the task is specialised or domain dependant the question of which LLM can act as the judge may be unknown.There exist many models that have been specialised in specific domains such as law (Chalkidis et al., 2020;Shaghaghian et al., 2020), finance (Wu et al., 2023) and medicine (Thirunavukarasu et al., 2023).</p>
<p>Methods</p>
<p>We now propose two methods based on the triplet idea to rank a given set of models when no groundtruth is available for the given dataset.The first one is a more efficient greedy approach, while the latter considers all triplets and performs an evolving weighted majority based ranking.</p>
<p>Greedy Triplet Ranking</p>
<p>Given a set of n models to rank, a dataset of prompts (with no responses) and an evaluation function that compares the model responses with the judge model, the Greedy Triplet Ranking (GTR) method in Algorithm 1 outputs a ranking of these n models by incrementally relegating the worst model in a triplet.In particular, starting at an arbitrary triplet it identifies the worst model, removes it from the triplet, then adds the next model in the set.It does this over all models, where at the end we would have the top two models (i.e.first time the for loop is executed).Now with the remaining models it repeats this process again finding the top two in the remaining set.This continues until we have less than three models.One of the top two from the first run is (randomly) selected as the best4 and this model is used to resolve the ordering of the pairs of top models that are outputted at the end of the subsequent for loop runs.</p>
<p>In this way all the models get ranked at the end, where in each triplet comparison we greedily relegate the worst model to the following run.</p>
<p>Full Triplet Ranking</p>
<p>For Full Triplet Ranking (FTR) too we assume we are given a set of n models, a dataset of prompts and an evaluation function that compares models based on the third being the judge (see Algorithm Algorithm 1 Greedy Triplet Ranking (GTR).Input: M set of (≥ 3) models to rank, f evaluation function and dataset D of prompts (viz. questions, instructions, etc.).
Initialize: R ← ϕ # Current ranked list of models while |M | ≥ 3 do T = {M 1 , M 2 , M 3 } # Triplet to begin for i = 3 to |M | do T ← T ∪ M i W ← Using f (T, D
) compare pairs of models with the third being the judge and output the model voted as the worst by the other two.In case of tie output
M i . T ← T \ W M ← M \ T # Remove top two models as they will be added to R if R == ϕ then R ← list(T ) # Amongst top two models randomly pick one as the best. else T ← T ∪ R 1 W ← Using f (T, D) and R 1 as judge out- put the worse model. R ← append (T \ W, W ) to R if |M | == 1 then R ← append M to R else if |M | == 2 then T ← M ∪ R 1 W ← Using f (T, D) and R 1 as judge output the worse model. R ← append (T \ W, W ) to R Output: R
2).The difference from GTR is that we consider all triplet of models here and we compute a reputation score for each model that is proportional to the number of (triplet) comparisons that they come on top.This score is then used in the next iteration to determine how much importance to give to a judge in the triplet comparisons, which in turn determines the reputation of other models in that iteration.This process continues until the reputations do not change by much.The final reputations are then used to rank the models.</p>
<p>Analysis</p>
<p>We now analyze our triplet approach, which also provides intuition of when the approach is likely to work well in practice.We then analyze the time complexity of GTR and FTR.</p>
<p>Algorithm 2 Full Triplet Ranking (FTR).</p>
<p>Input: M set of (≥ 3) models to rank, g triplet evaluation function, dataset D of prompts and ϵ a small constant.Evaluate all triplets on D (i.e.M i -vs-M j using
M k ) y ijk = g({M i , M j , M k }, D), ∀i, j, k ∈ M . Initialize: Model reputation score r k = 1.0 ∀k ∈ M . while True do m ij = 1 |M | k y ijk r k ∀M i , M j ∈ M # Weighted preference matrix. z ij = 1 if m ij ≥ m ji ∀M i , M j ∈ M and 0 otherwise # Majority vote. r ′ i = 1 |M |−1 j z ij ∀M i ∈ M # New reputa- tion score. δ = k |r k − r ′ k | # Convergence condition. if δ ≤ ϵ then Break else r k = r ′ k # Update reputation Sort M using reputation score r k for ranking R. Output: R</p>
<p>Conditions for the triplet approach to succeed</p>
<p>Given a triplet of models (M i , M j , M k ) and their corresponding (fractional) accuracies (a i , a j , a k ) on a task, where w.l.o.g.assume 1 ≥ a i &gt; a j &gt; a k ≥ 0 we want to analyze sufficient conditions under which model M k will be voted as the worse model by both M i and M j as judges.We analyze sufficient conditions since, given simply the ordered accuracies there are no specific necessary conditions on the accuracies for M k to be voted the worst as it could happen for all possible values.</p>
<p>First we analyze the case where none of the models will agree on incorrect responses to input prompts.In other words, no two models will have the same incorrect response on an input.If they are incorrect they will be incorrect in different ways.</p>
<p>Lemma 1.Given a triplet of models (M i , M j , M k ), where their accuracies (a i , a j , a k ) satisfy 1 ≥ a i &gt; a j &gt; a k ≥ 0 with no two models agreeing upon incorrect responses, then a k &lt; a i + a j − 1 will result in M k being (correctly) voted as the worse model by both M i and M j as judges.</p>
<p>Proof.We want to get the least overlap between correct responses of M i and M j .This will happen when M j is correct on all the responses that M i is incorrect on, which is 1 − a i fraction.Note that M j can also be correct on all the responses that M i is correct on.The overlap then between M i and M j of correct responses is a
j − (1 − a i ) = a j + a i − 1.
Hence, if a k &lt; a j + a i − 1 then both M i and M j will rate M k to be worse than the other even if M k overlaps on all correct responses with M i and/or M j .</p>
<p>Note that the above is a sufficient condition so in practice our approach can work even if the above condition is not met as is seen in the experiments.This is especially true when a i + a j ≤ 1, where in practice we might still be able to identify M k as the worst, although the sufficient condition would require a k &lt; 0.</p>
<p>If we allow two models to agree on up to m fraction of the incorrect responses then the above result can be extended as follows:</p>
<p>Theorem 1.Given the setup in Lemma 1, but where two models may agree on up to m fraction of the incorrect responses, then a k &lt; a i +a j −1−m will result in M k being (correctly) voted as the worse model by both M i and M j as judges.</p>
<p>Proof.From Lemma 1 we know the minimum overlap between M i and M j can be a i + a j − 1.However, M k could agree on m fraction of incorrect responses with either M i or M j making its maximum possible agreement with M i or M j to be a k + m.Hence, if M k is to be voted as the worst
a k + m &lt; a i + a j − 1.
As such, we would expect m to be small for LLMs, as they are typically used for generative tasks where the generations/responses can be multiple sentences or paragraphs (viz.summarization, writing articles, etc.) leading to a diverse set of possible responses and hence many different ways in which they may be incorrect.In other words, there can be a plethora of ways in which they may respond incorrectly when they make mistakes and hence, overlap between incorrect responses for different LLMs is likely to be small, especially when they are from different model families, for many applications.</p>
<p>Although the sufficient conditions above might be a bit strict they provide the intuition, also seen in practice, that there needs to be some gap in performance between the best and the worst models for our schemes to work.This seems to be reasonable to have in practice, since if all models are of similar accuracy, then all rankings can be considered as equally good/bad.</p>
<p>We now prove that if the sufficient conditions in Theorem 1 are met, then our GTR algorithm will output the correct ranking where the ranking of the top two models may be flipped.</p>
<p>Proposition 1.Given a set M of models (|M | ≥ 3) with a strict (accuracy) ordering on some dataset with the condition on accuracies in Theorem 1 holding for all triplet of models in M , then the GTR algorithm will output the correct ranking, where the order of the top two models may be flipped with 1 2 probability.</p>
<p>Proof.In the (inner) for loop of the GTR algorithm we will always correctly identify the top two models amongst those considered present in M at that iteration of looping.This is because all triplet comparisons will correctly identify the worst model given Theorem 1 as we iterate through the for loop leaving just the two best models of that iteration at the end.Ordering these top two models when R ̸ = ϕ can be done by R 1 correctly again because of Theorem 1.However, when R = ϕ we cannot resolve the ordering of the (overall) top two models and so we randomly pick one with probability 1 2 .Moreover, in the first looping of the for loop all models undergo a triplet comparison at least once.Outside the while loop when M = 1 we have the worst model as it has lost in all rounds of triplet comparisons.When M = 2 we can again pick R 1 as the judge and because of Theorem 1 we should be able to correctly order these last models as well.</p>
<p>The FTR algorithm is harder to analyze because it weights the models in each iteration based on the extent to which they were up voted and is a more sophisticated heuristic whose analysis we leave for future work.</p>
<p>Time Complexity</p>
<p>The for loop in the GTR algorithm identifies the top two models every time it is invoked and removes them from M .Thus, if there were n total models in M initially we do O(n) comparisons in the for loop.Since the size of M reduces by a constant (2) every time we run the for loop the total time complexity of GTR is O(n 2 ).</p>
<p>FTR on the other hand performs all triplet comparisons and hence is O(n 3 ).</p>
<p>We test our methods on three cases: summarization task, multiple-choice, and dialog.As a basis of comparison we devise a new method based on the 'most common answer' (MCA).The MCA ranker parses outputs from all LLMs to determine the most common output which is then treated as the reference to rank the models.For contexts such as (single token) multiple choice Q&amp;A, this is possibly the preferred method as it is easy to aggregate responses and ensembling in such contexts can have state-of-the-art performance (Mienye and Sun, 2022).</p>
<p>To measure effectiveness of the methods, we compare the estimated rankings with 'true' rankings derived using reference data.We use two metrics: Rank-biased Overlap (RBO) (Webber et al., 2010) and Mean Average Precision@k (MAP-k) to quantify how well the estimated rankings reflect the 'true' rankings.Both measures span the unit interval, with 1.0 denoting perfect agreement.</p>
<p>Summarization: We test on two datasets: the CNN-Daily Mail (CNN/DM) summarization dataset (Hermann et al., 2015;Nallapati et al., 2016) and XSUM (Narayan et al., 2018).In both cases we gather responses from 40 LLMs for 3000 instances from HELM (Liang et al., 2023).Our evaluation functions, i.e. f in GTR and g in FTR, employ ROUGE scores which are widely used for summarization tasks and in HELM.In particular, we consider the f-measure of bigrams.For the 'most-common answer', we use the top-k bigrams from all model responses and consider this to be the reference answer (k = 256 in our experiments).For this case, we also compare with LLM-as-ajudge approach using Prometheus-2 (Kim et al., 2024), which is a state-of-the-art LLM evaluator or judge model.True ranking, against which our estimated ranking is compared, is inferred by comparing model outputs to the reference response in the dataset.</p>
<p>Multiple-choice:</p>
<p>We simulate responses from models with known accuracy in a multiple choice setting.Datasets are constructed for several cases by varying the number of models, model accuracy, number of prompts and number of possible answers.Table 5 shows a sample.Since answers are discrete, the evaluation functions (i.e.f in GTR and g in FTR) use the equality operator to determine if a response is correct.We also experi-mented with a noisy equality operator, where the outcome of the evaluation is flipped with a known probability.This simulates a realistic case where comparisons are imperfect.</p>
<p>Dialog: In our third case, we tackle a practical need of an internal team comparing several finetuned variants that optimize for multiple objectives.For this task, they gathered responses from four model variants for 100 prompts from the Moral Integrity Corpus (Ziems et al., 2022).The corpus contains annotations of chatbot logs that describe their latent values and moral statements.These responses were passed on to three human annotators who were tasked with determining the 'best' response from the four models.Annotators provided a single preference per response.</p>
<p>We summarize the main empirical findings next with Appendix A providing more details on experimental setup and results.</p>
<p>Results</p>
<p>Summarization: Overall, both GTR and FTR outperform the MCA as shown in Table 1.The methods improve in recovering ranking as the size of the prompt dataset increases.In other words, both methods benefit from a larger number of prompts to make accurate evaluations.</p>
<p>We experimented with the number of models to rank by sampling from the catalogue of 40 models.Additional sampling experiments, where we considered mode performance, are reported in the appendix.Figure 2 show rankings to be reliably recovered for most cases (recall an RBO value of 1.0 implies perfect agreement with true rankings).Quality does degrade as the number of models to be ranked increases.</p>
<p>The number of triplet evaluations are shown in Figure 3.The greedy approach requires far fewer evaluations to estimate rankings as compared to the full method.This is not a concern when evaluations are cheap (e.g.ROUGE scores), but can be prohibitive when they are not.For example, our experiments leveraging entailment probabilities using DeBerta (He et al., 2020) were scuttled due to expensive evaluations.In such cases, the greedy method is preferred.</p>
<p>Additional details on experiments are shown in Appendix A where we show the impact of prompt dataset size (larger datasets aid evaluations particularly for FTR).Additionally, we experiment with various sampling strategies to study the effect of model quality differences in ranking.We show empirically that models with discernible quality differences are easier to rank.We also note that the MCA is not the most natural approach in such contexts where the responses are free form, although based on our design of using bi-grams it was easily compatible with existing metrics like ROUGE.Multiple-choice: The efficacy of our methods for multiple choice responses is summarized in Figure 5. Interestingly, all three methods perform very poorly when the number of possible responses are low.To see why this is the case, consider a triplet evaluation with yes/no responses.A weak model that erroneously outputs 'no' will judge another model that responds with 'no' as better.The judge's response is highly correlated with weak models' outputs.In other words, as there are few failure modes and low variance in wrong answers, weak models tend to be promoted.Therefore, triplet evaluations are susceptible to cases where there are few possible outcomes.This validates the theory in Section 3.3, where we mentioned that we need m (overlap of incorrect responses) to be small for our idea to be effective.</p>
<p>With increase in the size of the response set MCA, as expected, is highly performant as it ensembles responses from all the evaluated models to come up with an answer.For discrete responses that are easy to consolidate, such as single token responses in multiple choice, the most common answer is also likely to be the right answer.So this serves as a good proxy even when reference data is not available.Nonetheless, our triplet methods, especially FTR, still are quite competitive even in this case showcasing the generality of contexts in which they can be used.</p>
<p>Since the data in this case is simulated, we tested the impact of model performance on ranking recovery.Table 2 shows that even when the correctness of outcomes are pure chance (i.e.50%) for the best of the models being ranked, the RBO metric is at 0.832 for FTR and 0.723 for GTR which indicate good agreement with true rankings.</p>
<p>We investigate the role of noise in the evaluation function.In multiple choice, this comparison is exact as it uses an equality operator.This is substituted with a noisy equality operator, which randomly flips the outcome with a noise probability p.This allows us to evaluate the robustness of our methods in the face of noisy evaluationswhich is a more realistic setting in generative cases.Figure 4 shows the RBO metric as a function of noise for various methods.Both FTR and GTR are much more robust to noise at low and medium levels as compared to MCA.FTR does best here with its dynamic weighted voting making it less susceptible to noise.MCA on the other hand starts to degrade rapidly as noise increases.Additional plots are presented in Figure 14 in the appendix.This suggests our algorithms have broader applicability, including for single token classification type tasks.</p>
<p>Dialog: Our last use case is qualitative and ground in a practical application of assessing amongst several fine-tuned variants.Specifically, from a pre-trained model (P T ), several reward functions were specified that aimed to fine tune for multiple moral objectives.Three variants (M 1 , M 2 , and M 3 ) were LLMs created by fine-tuning for each reward function using RLHF.Outputs of each model for 100 prompts were recorded and scored by human annotators.The annotator task was simply to choose the 'best' response, without qualifying any specific dimension for evaluation.Three annotators unrelated to this paper specified their preferences.We evaluated the FTR method using entailment probability and ROUGE scores as the basis for the evaluation function g.We ran only FTR here, since there were only four models and so efficiency wasn't a factor.Our method ranked the models [M 1 , M 3 , M 2 , P T ] as compared to the human annotators [M 1 , M 2 , M 3 , P T ].Thus, we were able to rank the best and the worst models correctly.The middle two were quite close in performance from what we heard so flipping them was probably not that damaging.</p>
<p>Comparison with LLM-as-a-judge Next, we compare our approach to the growing LLM-as-ajudge literature (Zheng et al., 2024).In this setting, language models themselves are used as evaluators.LLM judges used in this was offer lower cost of annotations/labels compared to human annotations and can make metric-free evaluations, for instance evaluating responses that meet criteria such as 'child-friendliness' or 'professional tone'.Prometheus (Kim et al., 2023) and its subsequent version (Kim et al., 2024) are two such specialized LLM evaluators.</p>
<p>We leverage Prometheus-2 to compare LLM outputs in a pairwise fashion.As such, we prompt it to compare the results of two summaries from two LLMs for the CNN/DM summarization dataset.This is done for all pairs of LLMs and for a sample of 50 questions in the benchmark.For each evaluation, the LLM judge (i.e.Prometheus-2) declares a winning LLM.We aggregate win rates across the benchmark and derive a LLM ranking based on win rates.We consider a (random) sample of 10 LLMs from the benchmark.Our task criteria is simply 'Which is the better response?'.</p>
<p>Table 3 shows the mean and standard deviation of ranking metrics compared to this approach.Our methods out-perform the LLM judge across all metrics.Additional details are included in the Appendix C.</p>
<p>Discussion</p>
<p>Given the proliferation of LLMs, and claims of superiority made by the developers, there needs to be a trustworthy mechanism to evaluate them.LLM leaderboards are extremely beneficial for this purpose.However, it takes a lot of effort to collect reference data or judgements or preferences about the model responses, which only multiplies when we consider various domains and tasks which can   also lead to benchmarks becoming obsolete.Our proposed approaches can be seen as a first pass to substantially reduce this effort without imposing the need to have a trustworthy model for evaluation as demanded by LLM-as-a-judge approaches.Moreover, in addition to simply obtaining a ranking, the relative values of the reputation score for different LLMs in FTR could potentially be used to also gauge the performance gap between them.</p>
<p>There are many interesting avenues to explore in the future.First, how we can incorporate and benefit from additional information such as a partial ordering between LLMs, which is reasonable to assume when some LLMs belong to a model family.The additional information could also be in the form of a few ground truth labels for the task at hand.In this case, there may be a wider set of possible methods such as those based on uncertainty or possibly even Bayesian approaches.Second, rather than triplets what if we consider larger sets to compare in each round.Of course, for FTR the number of comparisons might scale exponentially with the set size so more efficient variants might have to be thought of.Here it is possible that the variance in ranking the worse models might increase as a larger set of judges will produce their own ranking.Moreover, asking each judge to rank a larger set of models might be more error prone.Nonetheless, the bias-variance trade-off between different set sizes might be interesting to explore.Third, more efficient variants than GTR may be worth exploring.For instance, in GTR the top two models at the end of the first round may be used to rank the rest resulting in a linear time variant.Also methods from matrix completion could be explored where we obtain a few partial orderings running GTR or FTR for a few iterations.Fourth, using a threshold in the triplet comparisons to determine a winner may reduce the possibility of selecting a winner based on random chance and further improve the ranking.Finally, our approach might find use in other contexts where, consolidation of responses into a single response is not easy such as: i) when evaluating non-expert human annotators on descriptive tasks.ii) In accelerated drug discovery type of applications where different algorithms might recommend different molecular structures that may be hard to consolidate or iii) even in multi-agent systems where a protocol is needed to decide which agent to follow in a specific context.</p>
<p>Limitations</p>
<p>We tested our proposed method only on English language corpuses.This could have introduced bias in the choices we made in implementations.Further, in the use case with Dialog data, the moral judgements of human annotators are subjective and hence the original rankings of the models may vary based on the annotators.These two limitations also hold for a vast majority of human preference benchmarks.</p>
<p>Our ranking will be subpar if there is high correlation amongst incorrect answers, which can happen in multiple choice with few options as is also seen in the experiments.This is why we recommend using our method for more complex generative tasks where the set of possible responses is large.</p>
<p>In addition, ROUGE is an imperfect metric even though it is widely used.We call upon the community to work on more sophisticated metrics that can be easily adapted based on the context.Finally, if the models that are used in ranking are all subpar in performance, our ranking could be incorrect, but in this case may be even a correct ranking has limited utility.</p>
<p>Ethics Statement</p>
<p>Our work has the potential to improve the trustworthiness of LLMs by making larger scale evaluations possible with reduced human effort, since we do not need access to a reference dataset.Lack of substantial evaluations is one of the blockers for trustworthy adoption of LLMs, and it could be eased by our approach.In addition, this could be beneficial when the human labor involved for creating reference data or preferences requires looking at malicious content which could lead to psychological harm for the humans involved.It is however also true, that the ranking provided by our method should only be considered as an initial estimate and human oversight is necessary to ensure that the quality of models deemed good are sufficiently good for the application at hand.</p>
<p>A Experiment Details</p>
<p>A.1 Summarization</p>
<p>We use summarization scenario with CNN/Daily Mail and XSUM and collect instance runs from HELM v0.2.3.Specifically, we use HELM APIs to gather the per instance responses of all benchmarked models (40 in all).For each summarization prompt, HELM runs three trials using various perturbations on the prompt.The resulting dataset consists of 1000 summarization queries, each run with 3 perturbations, resulting in 3000 prompt-response pairs for each of the 40 models.Each row in this dataset also contains the reference output used to compute the true rankings.</p>
<p>For our experiments, we take n samples from the set of models, n = [5,6,7,8,9,10,15] in our experiments.For each n we conduct 10 trials.Three sampling strategies are used.</p>
<p>• Random: n models are sampled at random.</p>
<p>• Spread: We first compute true rankings of all 40 models using same evaluation function g and the reference output.We then use systematic sampling to selected n models such that model performance is 'spread' across the list.Specifically, we first generate from U [0, 40/n] and then select every 40/nth model from the ordered set of models.This provides a sample of models where performance is varied.</p>
<p>• Close: We order all models based on true ranking (similar to the case above).We define a window size w and draw a sample s from U [w, 40 − w].We then sample n samples from a subset in the window
U [s − w, s + w].
This results in a sample of models with similar performance.</p>
<p>All results in the main paper are reported based on the random sampling strategy.Figures 9 and 10 show the assessment when the sampling strategy changes.</p>
<p>For each case, we vary the number of summarization prompts to be [100,500,1000,3000].The three methods FTR, GTR and the baseline are run on each combination of run configurations for 10 trials each.As evaluation metrics, we compute Rank Biased Overlap (RBO) with p = 0.95 and Mean Average Precision @ k, with k = 3, 5, 10.As the evaluation function, f in GTR and g in FTR, we use the ROUGE-2 fmeasure with is a common summarization metric and also used in HELM.</p>
<p>To complete the number of evaluations for CNN/DM, Figure 6 shows the number of evaluations for the XSUM summarization task, showing similar scaling.Figure 7 shows the impact of prompt size with both FTR and GTR benefitting from a larger corpus of prompts to evaluate over.</p>
<p>The different sampling strategies yielded different performance characteristics as shown in Figure 9.When model performance is similar (sampling = close), estimating ranking is more challenging as it is harder to distinguish between model responses.All test methods appear to have similar performance in this case.On the other hand, when model perfomances are dissimilar (sampling = spread), rankings are recovered with more efficacy.The FTR model here performs very well.In the random case, the GTR and FTR methods outperform the most-common-answer method considerably.</p>
<p>Figure 11 shows sample bump charts that visualize rank changes between the 'true' (based on reference data) and the estimated ranks from the proposed methods.Figure 11a shows perfect agreement between estimated and true rankings.In Figure 11b rankings are not exactly recovered, but the clusters of good and bad models are persisted, even if the exact rankings of them within the clusters are not correct.This is also seen in Figure 11c and  11d.</p>
<p>A.2 Multiple Choice</p>
<p>For simulating this data, we generate a matrix of responses, with each row denoting a questions and each column denoting a model (of known accuracy).Each answer takes on a discrete value depending on the number of possible answers.We consider different sizes of prompts, i.e. [100, 500], number of possible answers ranging from 2 to 50, and models of varying accuracy.The accuracy is set to be between 90 and 10 and experiments are run with various combinations to simulate model performance differences.As evaluation function, g in FTR and f in GTR, we consider the equality operator.In addition we also experimented with a noisy equality operator, that randomly flipped the outcome of equality.</p>
<p>The intent of the experiments was to stress test our ranking methods.Figure 12 plots the full set of results over all combinations when the evaluations are based on equality.Generally, the ensembling method of MCA performs well with FTR showing comparable performance.See also Figure 13 for similar trends in the MAP-5 metric.</p>
<p>When the number of possible answers is low, the triplet evaluations face difficulty in disambiguation.For all methods, when the possible outcomes are 2 for example, RBO values are low.Weak models in these cases unite to promote the wrong answers.</p>
<p>Generally, rankings can be recovered reasonably well (provided there are sufficient possible answers) even if the best model performance is at 50%.</p>
<p>On noisy equality experiments, where the triplet evaluations are randomly perturbed with a known probability, we see that efficacy of our methods decline with increasing noise in Figure 14.FTR and even GTR are more robust than MCA at lowmedium levels of noise.This is shown for some specific sizes of instances, the patterns are similar for other cases as well.</p>
<p>A.3 Models used</p>
<p>Table 4 shows the list of models used in the HELM experiments for summarization.</p>
<p>A.4 Synthetic Data Generation</p>
<p>Table 5 shows an example of synthetic data generated for the multiple choice setting.We parameter- ize the generation on model accuracy, number of possible answers, and the number of questions in the dataset.</p>
<p>B Most Common Answer</p>
<p>We now present additional details on the Most-Common Answer (MCA) method that we use as a baseline.When the context is multiple choice, the concept of the most common answer is straight forward.You assign the label with the majority vote from all the LLMs being tested as the true label.For instance, in a multiple choice setting with 4 options A, B, C, D, if from five different models you gather responses for a specific question as A, A, B, C, D, then A would be considered as the true label.</p>
<p>In generative settings, such as for summarization, what constitutes a 'common' answer involves more work.To establish an MCA baseline, we first generate all n-grams from all model responses.We then use the most frequent n-grams across all responses upto a specific size, and consider that to be the 'true' response.Specific answers from the models are then evaluated against this to generate the closest response.This is illustrated using a simple example.Consider three models' responses to the question What is the capital of Canada? in Table 6.From all the responses, we compute frequencies of all bigrams to get 'ta': 3, 'nt': 2, 'Ot': 2, 'tt': 2, 'aw': 2, 'wa': 2,'To': 1,'or': 1,'ro': 1,'on': 1,'to': 1,'a,': 1,', ': 1,' O': 1,'On': 1,'ar': 1,'ri': 1,'io': 1.From this, we select the top-k bigrams to constitute the most-common answer.In this example we assume the top five, but in the summarization experiments we consider k to be 256.We can now compute similarity measures of each model response relative to this most-common answer.Rouge-2 scores, as an example are shown in Table 6 indicating that M3 has the best response.</p>
<p>C LLM as a Judge</p>
<p>We prompt Prometheus-2 (Kim et al., 2024) to compare the results of two summaries from the CNN/DM summarization dataset.This is done for all pairs of LLMs and for a sample of 50 questions in the benchmark.For each evaluation, the LLM Judge declares a winning LLM.Our task criteria is simply 'Which is the better response?'.Generally, human annotators judging summarization quality do so along specific attributes such as coherence, fluency, consistency, relevance, and (Fabbri et al., 2021).We additionally compared Prometheus with human annotations along these dimensions and find poor agreement.For this task, the LLM Judge is overly pessimistic across all dimensions, see Figure 15.</p>
<p>Next we considered pairwise evaluations, specifically, we prompted Prometheus-2 with the following prompt template:</p>
<p>"""###Task Description: An instruction (might include an Input inside it), a response to evaluate, and a score rubric representing a evaluation criteria are given.1. Write a detailed feedback that assess the quality of two responses strictly based on the given score rubric, not evaluating in general.</p>
<p>Figure 1 :
1
Figure 1: We see the intuition behind the triplet approach.</p>
<p>Figure 2 :
2
Figure 2: Evaluation metrics on summarization for two datasets: CNN/DM (top) and XSUM (bottom), RBO (left) and MAP-5 (right), as a function of number of models being ranked (note x-axis is not linear).</p>
<p>Figure 3 :
3
Figure 3: Number of triplet evaluations for CNN/DM dataset.(log y-scale).</p>
<p>Figure 4 :
4
Figure4: Quality of rankings recovered as a function of noise in the evaluation function for different methods.Four sets of 10 LLMs with each set having a maximum LLM accuracy of 30%, 50%, 70% or 90% are considered, where the number of questions is 50.We see the robustness of the proposed methods (GTR and FTR) at low to medium levels of noise in such a setup.</p>
<p>Figure 5 :
5
Figure 5: Evaluation metrics on multiple-choice, RBO (left) and MAP-5 (right), when ranking 25 models where the accuracy of the best performing model is 50%.</p>
<p>Figure 6 :
6
Figure 6: Triplet evaluations for XSUM summarization task.</p>
<p>Figure 7 :
7
Figure 7: Summarization -CNN/DM dataset: Evaluation measures as a function of prompt dataset size.RBO (top), MAP-3 (middle), MAP-5 (bottom).</p>
<p>Figure 8 :
8
Figure 8: Summarization -XSUM dataset: Evaluation measures as a function of prompt dataset size.RBO (top), MAP-3 (middle), MAP-5 (bottom).</p>
<p>Figure 9 :
9
Figure 9: Summarization -CNN/DM: Evaluation measures as a function of sampling.RBO (top), MAP-3 (middle), MAP-5 (bottom).</p>
<p>2 .
2
After writing a feedback, choose a better response between Response 1 and Response 2. You should refer to the score rubric.3. The output format should look as follows: "Feedback: (write a feedback for criteria) [RESULT] (1 or 2)" 4. Please do not generate any other opening, closing, and explanations.###Instruction: {instruction} ###Response 1: {response_1} ###Response 2: {response_2} ###Score Rubric: {rubric}</p>
<p>Figure 10 :
10
Figure 10: Summarization -XSUM: Evaluation measures as a function of sampling.RBO (top), MAP-3 (middle), MAP-5 (bottom).</p>
<p>Figure 12 :
12
Figure 12: Multiple choice: Full version of Figure 5 showing summary of RBO for all runs.</p>
<p>Figure 13 :
13
Figure 13: Multiple choice: MAP-5 for all for all runs.</p>
<p>Figure 14 :
14
Figure14: Impact of noisy evaluations on multiple choice data for selected cases.</p>
<p>Figure 15 :
15
Figure 15: Comparison of Human annotations with LLM Judge (Prometheus) annotations based on the SummEval dataset (Fabbri et al., 2021).</p>
<p>Table 1 :
1
Mean and standard deviation of Rank-biased overlap (RBO) and Mean Average Precision at 5 (MAP-5) for varying sizes of prompts used for the summarization task.Computed over 10 trials.Best (mean) result in bold.
RBOMAP-5SizeMCAFTRGTRMCAFTRGTRCNN/1000.808±0.07 0.866±0.07 0.867±0.07 0.445±0.33 0.677±0.29 0.664±0.28DM5000.802±0.07 0.881±0.05 0.882±0.05 0.430±0.33 0.804±0.22 0.750±0.221000 0.804±0.07 0.893±0.06 0.884±0.06 0.425±0.33 0.815±0.23 0.763±0.233000 0.805±0.08 0.889±0.05 0.885±0.07 0.460±0.33 0.782±0.26 0.746±0.26XSUM 1000.803±0.07 0.878±0.06 0.860±0.06 0.433±0.32 0.744±0.24 0.723±0.265000.799±0.07 0.884±0.06 0.871±0.07 0.420±0.32 0.758±0.25 0.716±0.261000 0.790±0.07 0.888±0.06 0.877±0.06 0.374±0.33 0.785±0.27 0.739±0.253000 0.788±0.08 0.879±0.06 0.871±0.06 0.371±0.34 0.761±0.26 0.731±0.26</p>
<p>Table 2 :
2
Mean and standard deviation of Rank-biased overlap (RBO) and Mean Average Precision at 5 (MAP-5) for varying accuracy of the best performing model for multiple-choice.Computed over 5 trials.Best (mean) result in bold.
Acc. bestRBOMAP-5modelMCAFTRGTRMCAFTRGTR300.668±0.26 0.694±0.26 0.622±0.21 0.378±0.39 0.433±0.41 0.262±0.28500.818±0.23 0.832±0.22 0.723±0.19 0.663±0.39 0.698±0.38 0.452±0.33700.927±0.16 0.927±0.15 0.833±0.14 0.866±0.29 0.866±0.27 0.671±0.30900.980±0.09 0.981±0.08 0.919±0.09 0.965±0.17 0.971±0.15 0.855±0.20</p>
<p>Table 3 :
3
Comparison with state-of-the-art Prometheus-2 LLM-judge using pairwise evaluation on CNN/DM summarization task with 10 models.Best (mean) result in bold.
AlgorithmRBOMAP-3MAP-5FTR0.835±0.04 0.200±0.14 0.523±0.16GTR0.841±0.06 0.300±0.26 0.448±0.23Prometheus-2 0.806±0.07 0.200±0.18 0.354±0.28MCA0.792±0.05 0.144±0.16 0.296±0.26</p>
<p>Table 4 :
4
Models used in summarization experiments
GT M1 M2 M3 M4 M5Q0000070Q1262221Q2111107Q3666466Q4551555</p>
<p>Table 5 :
5
Sample generated dataset with model accuracies in the range 50 − 80%, with 10 possible answers per question.GT denotes the ground truth, i.e. true label, and M1-5 denote simulated model responses.
Model ResponseRouge-2 w.r.t. MCAM1Toronto0.117M2Ottawa, Ontario0.480M3Ottawa0.500MCA ta: 3, nt: 2, ot: 2,tt: 2, aw: 2-</p>
<p>Table 6 :
6
Illustration of model responses and mostcommon answer (MCA) for the question: 'What is the capital of Canada?'</p>
<p>https://chat.lmsys.org/
https://python.langchain.com/docs/guides/ evaluation/string/criteria_eval_chain
One could also use ranking from the final triplet comparison to determine the best.
AcknowledgementsWe thank Kush Varshney for initial discussions as well as encouraging the appropriate collaborations.We thank Djallel Bouneffouf for sharing data and human annotations on the dialog use case.
Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan2022</p>
<p>Unitxt: Flexible, shareable and reusable data preparation and evaluation for generative ai. Elron Bandel, Yotam Perlitz, Elad Venezian, Roni Friedman-Melamed, Ofir Arviv, Matan Orbach, Shachar Don-Yehyia, Dafna Sheinwald, Ariel Gera, Leshem Choshen, Michal Shmueli-Scheuer, Yoav Katz, 2024</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, Thomas Wolf, Open llm leaderboard. 2023</p>
<p>Gabor Samuel R Bowman, Christopher Angeli, Christopher D Potts, Manning, arXiv:1508.05326A large annotated corpus for learning natural language inference. 2015arXiv preprint</p>
<p>Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. Ilias Chalkidis, Manos Fergadiotis, arXiv:2010.02559Legal-bert: The muppets straight out of law school. 2020arXiv preprint</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2305.019372023arXiv preprint</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, 2023. April 202314</p>
<p>Recognizing Textual Entailment: Models and Applications. Synthesis Lectures on Human Language Technologies. Ido Dagan, Dan Roth, Mark Sammons, Fabio Massimo Zanzotto, 10.2200/S00509ED1V01Y201305HLT0232013Morgan &amp; Claypool Publishers</p>
<p>The proposed uscf rating system, its development, theory, and applications. E Arpad, Elo, Chess Life. 2281967</p>
<p>Utility is in the eye of the user: A critique of nlp leaderboards. Kawin Ethayarajh, Dan Jurafsky, arXiv:2009.138882020arXiv preprint</p>
<p>Summeval: Re-evaluating summarization evaluation. Wojciech Alexander R Fabbri, Bryan Kryściński, Caiming Mc-Cann, Richard Xiong, Dragomir Socher, Radev, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, Mcdonell, 10.5281/zenodo.10256836Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite. Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf; Ben Wang, Kevin Wangand Andy Zou. 2023. A framework for few-shot language model evaluation</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, arXiv:2006.036542020arXiv preprint</p>
<p>Teaching machines to read and comprehend. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Advances in neural information processing systems. 201528</p>
<p>Prometheus: Inducing finegrained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo, 2023</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, arXiv:2405.015352024arXiv preprint</p>
<p>Openassistant conversations -democratizing large language model alignment. Andreas Köpf, Yannic Kilcher, Sotiris Dimitri Von Rütte, Anagnostidis, Rui Zhi, Keith Tam, Abdullah Stevens, Barhoum, Minh Duc, Oliver Nguyen, Richárd Stanley, Nagyfi, E S Shahul, Sameer Suri, David Alexandrovich Glushkov, Arnav Varma Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, Alexander Julian, Mattick , Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang, arXiv:2305.18486A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. 2023arXiv preprint</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana Acosta-Navas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Wang Jue, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda, Transactions on Machine Learning Research. 2023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Automatic evaluation of summaries using n-gram co-occurrence statistics. Chin-Yew Lin, Eduard Hovy, 10.3115/1073445.1073465Proceedings of the 2003 Conference of the North American Chapter. the Association for Computational Linguistics on Human Language Technology. the 2003 Conference of the North American ChapterUSA. Association for Computational Linguistics20031</p>
<p>A survey of ensemble learning: Concepts, algorithms, applications, and prospects. Ibomoiye Domor, Mienye , Yanxia Sun, IEEE Access. 102022</p>
<p>Abstractive text summarization using sequence-to-sequence RNNs and beyond. Ramesh Nallapati, Bowen Zhou, Caglar Cicero Dos Santos, Bing Gulcehre, Xiang, 10.18653/v1/K16-1028Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning. the 20th SIGNLL Conference on Computational Natural Language LearningBerlin, GermanyAssociation for Computational Linguistics2016</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Customizing contextualized language models for legal document reviews. Shohreh Shaghaghian, Luna Yue Feng, Borna Jafarpour, Nicolai Pogrebnyakov, 2020 IEEE International Conference on Big Data (Big Data). IEEE2020</p>
<p>Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, arXiv:2401.05561Trustllm: Trustworthiness in large language models. 2024arXiv preprint</p>
<p>Large language models in medicine. Arun James Thirunavukarasu, Darren Shu, Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, Daniel Shu, Wei Ting, Nature medicine. 2982023</p>
<p>Decodingtrust: A comprehensive assessment of trustworthiness in GPT models. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023a</p>
<p>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, arXiv:2306.050872023barXiv preprint</p>
<p>A similarity measure for indefinite rankings. William Webber, Alistair Moffat, Justin Zobel, ACM Transactions on Information Systems (TOIS). 2842010</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek F Wong, Emine Yilmaz, Shuming Shi, Zhaopeng Tu, arXiv:2401.12794Benchmarking llms via uncertainty quantification. 2024arXiv preprint</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 202436</p>
<p>Judgelm: Fine-tuned large language models are scalable judges. Lianghui Zhu, Xinggang Wang, Xinlong Wang, Caleb Ziems, Jane A Yu, Yi-Chia Wang, Alon Halevy, Diyi Yang, arXiv:2310.17631arXiv:2204.03021The moral integrity corpus: A benchmark for ethical dialogue systems. 2023. 2022arXiv preprint</p>
<p>Model 1 AlephAlpha_luminous-base. </p>
<p>AlephAlpha_luminous-extended. </p>
<p>AlephAlpha_luminous-supreme 4 ai21_j1-grande 5 ai21_j1-grande-v2-beta 6 ai21_j1-jumbo 7 ai21_j1-large 8 ai21_j2-grande 9 ai21_j2-jumbo 10 ai21_j2-large 11 anthropic_stanford-online-all-v4-s3 12 cohere_command-medium-beta 13 cohere_command-xlarge-beta. 3b-v1 38 together_yalm 39 writer_palmyra-instruct-30 40 writer_palmyra-x14 cohere_large-20220720 15 cohere_medium-20220720 16 cohere_medium-20221108 17 cohere_small-20220720 18 cohere_xlarge-20220609 19 cohere_xlarge-20221108 20 microsoft_TNLGv2_530B 21together_bloom 33 together_gpt-j-6b 34 together_gpt-neox-20b 35 together_opt-175b 36 together_opt-66b 37 together_redpajama-incite-base-</p>            </div>
        </div>

    </div>
</body>
</html>
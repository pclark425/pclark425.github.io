<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9724 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9724</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9724</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-271891943</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.08781v1.pdf" target="_blank">Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions</a></p>
                <p><strong>Paper Abstract:</strong> LLMs-as-a-judge is a recently popularized method which replaces human judgements in task evaluation (Zheng et al. 2024) with automatic evaluation using LLMs. Due to widespread use of RLHF (Reinforcement Learning from Human Feedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have strong alignment with human preferences when prompted for a quality judgement, such as the coherence of a text. While this seems beneficial, it is not clear whether the assessments by an LLM-as-a-judge constitute only an evaluation based on the instructions in the prompts, or reflect its preference for high-quality data similar to its fine-tune data. To investigate how much influence prompting the LLMs-as-a-judge has on the alignment of AI judgements to human judgements, we analyze prompts with increasing levels of instructions about the target quality of an evaluation, for several LLMs-as-a-judge. Further, we compare to a prompt-free method using model perplexity as a quality measure instead. We aggregate a taxonomy of quality criteria commonly used across state-of-the-art evaluations with LLMs and provide this as a rigorous benchmark of models as judges. Overall, we show that the LLMs-as-a-judge benefit only little from highly detailed instructions in prompts and that perplexity can sometimes align better with human judgements than prompting, especially on textual quality.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9724.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9724.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity_vs_Prompting_Textual</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model Perplexity versus Prompted LLM-as-a-Judge for Textual Quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper compares model perplexity (no prompt) to prompted LLM judgements and finds that perplexity often correlates better with human judgements on content/textual quality (e.g., fluency, structural text quality) and can outperform generic prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Summarization and general textual generation (e.g., news summaries, story generation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various (GPT-4, Llama3 70b/8b, Mistral, Phi3, Prometheus-2); comparisons aggregated across models</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Four settings tested: (1) Perplexity scoring (no prompt) where solution scored by model perplexity given task description; (2) Generic quality prompt (simple 1–5 Likert); (3) Criteria-specific prompt (name of criterion only); (4) Full rubric prompt (criterion definition and rubric for each score).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations from eight open benchmark datasets (e.g., SummEval, Hanna, InstruSumm) providing per-example human scores for multiple criteria; paper uses those human labels as reference (reference-free LLM evaluation; original annotator counts not enumerated in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation between LLM-generated scores (or perplexity) and human annotations; reported examples include: perplexity Pearson correlation 0.51 for textual content metrics vs 0.44 for prompted LLM judgement (abstract / results).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When replacing humans with prompted LLM-as-a-judge for textual quality, one might lose the benefit of simple, prompt-robust signal: paradoxically, prompting does not always improve alignment and can degrade performance relative to a prompt-free measure (perplexity) — i.e., prompt-based judgements can be noisier or less aligned for structural/textual metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>SummEval fluency: human annotations correlate much better with model perplexity than with any prompting method (paper states fluency agreement much higher with perplexity). The paper also reports perplexity outperforming prompting on SummEval and Flask average correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>For some non-textual, task-specific criteria (e.g., sentiment/aspect extraction in OpinSummEval), prompting (with rubric) is necessary and perplexity underperforms; powerful models (GPT-4) still produce strong prompted judgements and prompting can help on some specific subjective criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Abstract; Dataset level results; Details on content-based criteria results; Results (perplexity vs prompting comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9724.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9724.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt_Rubric_Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Increasing Prompt Instruction (Full Rubric) on LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper systematically measures how adding more detailed rubric information to prompts affects alignment with human judgements and finds only small aggregate improvements (≈4% Pearson) across criteria, with heterogeneous effects by model and criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multiple NLG tasks and evaluation criteria across benchmarks (content, relevance, integrity, engagement)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various (GPT-4, Llama3 70b/8b, Mistral, Phi3, Prometheus-2)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Comparison across 4 prompting levels: none (perplexity), generic prompt, criterion-name prompt, and full-rubric prompt with definitions and score exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations from benchmark datasets with provided annotation guidelines; for full-rubric prompting the paper extracts benchmark rubric text and injects it into prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation; aggregate improvement from minimal to full rubric is reported as small (about 4% increase aggregated across taxonomy).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs-as-a-judge with full rubric often does not recover the nuanced human judgement advantage — additional instruction yields only marginal gains, meaning detailed human rubric reasoning and contextual interpretation are not fully captured by LLM prompting; in some cases prompting even reduces alignment (rubric can harm performance).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Phi3's performance decreased when full rubric details were provided compared to simpler prompts — Phi3's prior knowledge aligned better with human annotators than when given extensive rubric text. Generally, most large/midsize models saw only small changes when full rubric added.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Smaller models (Llama3 8b, Mistral) sometimes benefit from full rubric information. GPT-4 shows only marginal improvement from criteria-specific to full-rubric prompting, indicating high prior alignment with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Introduction; Evaluating LLMs-as-a-judge (prompt settings); Model level results; Conclusion</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9724.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9724.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Integrity_Failure</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Degradation on Integrity/Logical Correctness when Using LLMs-as-a-Judge (except strongest models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For integrity-based criteria (logical correctness, task-specific correctness), many LLM-as-judges fail to match human judgements — especially in assigning low/bad scores — unless the judge model itself is capable of solving the task (GPT-4 performs markedly better).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Logical and mathematical reasoning tasks (e.g., FLASK, Roscoe datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, Llama3-70b, Phi3, others (paper highlights GPT-4 vs Llama3/Phi3)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompted judgements (various prompt levels) and perplexity comparisons on task solutions; integrity criteria judged reference-free against task description and solution.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labels for integrity/logical correctness from Flask and Roscoe benchmark datasets; humans annotated correctness and missing steps etc.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation; reported: GPT-4 Pearson = 0.68 on logical correctness vs Llama3-70b = 0.34 and Phi3 = 0.33 (Results - Details on integrity-based criteria).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Most LLM-as-a-judge models (except top-tier models) systematically fail to detect incorrect logical reasoning and tend to over-score incorrect responses (grade bad examples highly), losing the human ability to discriminate logical correctness — i.e., inability to assign low scores and to reliably detect errors.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Violin plots show that only GPT-4 generates lower scores for 'bad' solutions; other models predominantly give high scores, grading logically incorrect responses as 'very good' (Results, Fig. 6).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>GPT-4 substantially outperforms other models and is capable of producing judgements aligned with humans on integrity; the paper hypothesizes judges need to be able to solve the task themselves to judge correctly (citing Lin et al. 2024). Also, providing more detailed rubric information improves judgement for some complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Details on integrity-based criteria results; Results (Fig. 6); Model level results; Conclusion</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9724.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9724.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Engagement_Subjectivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Loss of Subjective Nuance in Engagement-based Criteria</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Engagement-based criteria (e.g., empathy, surprise) are subjective and show higher annotation variance; LLM-as-a-judge outputs are more variable across models and often cluster, losing nuanced distributional agreement with diverse human judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Dialogue and creative story generation (Hanna, TheNextChapter, TopicalChat datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4, Phi3, Llama3-8b, others</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompted evaluation (multiple prompt levels) of engagement-related criteria; reported distribution of generated scores (violin plots).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations for engagement/empathy from datasets like Hanna and TopicalChat; human scores show high variance and lower average scores for these subjective criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation reported for empathy: GPT-4 = 0.32, Phi3 = 0.31, Llama3-8b = 0.30; models' outputs have higher variance and often cluster (Results - engagement details, Fig. 5).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM judges lose the diverse subjective nuance found in human annotations: models often cluster around a limited set of scores and produce high-variance outputs that do not reflect the broad human judgement distribution; thus, subtle subjective aspects of engagement are degraded.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Empathy (Hanna dataset): human annotations show significant variation; Phi3 generates scores with higher variability, while most models cluster around a single score—indicating mismatch in capturing subjective diversity (Results, Fig. 5).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Phi3 sometimes outperforms other models on engagement due to producing higher variability; full rubric information can help for unconventional engagement criteria; overall differences between models on engagement are smaller than on other criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Details on engagement-based criteria results; Criteria level analysis; Results (Fig. 5)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9724.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9724.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dataset_and_Model_Biases</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dataset Effects and Model Preference Biases When Using LLMs-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes dataset-dependent behaviors and biases: perplexity aligns strongly on simple text tasks but not on complex multi-aspect tasks; some models show preference or misalignment due to finetuning/training data (e.g., propensity to prefer own-style outputs or be inhibited re: harmfulness).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Wide set of NLG tasks across 8 benchmark datasets (SummEval, OpinSummEval, TheNextChapter, Flask, Roscoe, Hanna, TopicalChat, InstruSumm)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various (Llama3, Mistral, Phi3, Prometheus-2, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompted evaluations and perplexity scoring across datasets; per-dataset correlations analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations from each dataset used as ground truth; paper uses dataset-provided annotation guidelines when available.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pearson correlation per-dataset and per-criteria; examples: perplexity often higher than prompting on SummEval and Flask; model-specific numbers given in tables and text.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLM judges can conflate preference for data similar to their fine-tune/training set with true adherence to evaluation instructions — e.g., LLMs may favor outputs resembling their own training distribution, miss dataset-specific aspects, or be overly influenced by fine-tuning (e.g., harmlessness judged by perplexity shows 100% agreement in one case, likely reflecting strong fine-tuning). Dataset-level averages can be misleading if dominated by text-related criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Prometheus-2: prompting settings 2 and 3 returned poor results; only perplexity and full rubric aligned due to its fine-tuning. Flask dataset: high average perplexity correlation may hide per-criterion variance; SummEval: perplexity outperforms prompting on textual criteria, suggesting loss of nuance when using prompted judges. Harmlessness on Flask: perplexity had 100% agreement with human annotations, possibly reflecting fine-tuning rather than true judgement capability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Some datasets and criteria (e.g., TheNextChapter, complex creative/story related criteria) benefit from full rubric prompting. Larger models tend to perform better on relevance and integrity; smaller models may improve with rubrics. The paper explicitly warns dataset-level aggregates can mislead and recommends per-criteria analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Introduction; Dataset level results; Details on content/relevance/integrity sections; Dataset level analysis; Results</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions", 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>NLG Evaluation using GPT-4 with Better Human Alignment <em>(Rating: 2)</em></li>
                <li>LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores <em>(Rating: 2)</em></li>
                <li>Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models <em>(Rating: 2)</em></li>
                <li>FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets <em>(Rating: 2)</em></li>
                <li>ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning <em>(Rating: 1)</em></li>
                <li>Judging LLM-as-a-judge with MT-bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9724",
    "paper_id": "paper-271891943",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Perplexity_vs_Prompting_Textual",
            "name_full": "Model Perplexity versus Prompted LLM-as-a-Judge for Textual Quality",
            "brief_description": "The paper compares model perplexity (no prompt) to prompted LLM judgements and finds that perplexity often correlates better with human judgements on content/textual quality (e.g., fluency, structural text quality) and can outperform generic prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Summarization and general textual generation (e.g., news summaries, story generation)",
            "llm_judge_model": "Various (GPT-4, Llama3 70b/8b, Mistral, Phi3, Prometheus-2); comparisons aggregated across models",
            "llm_judge_setup": "Four settings tested: (1) Perplexity scoring (no prompt) where solution scored by model perplexity given task description; (2) Generic quality prompt (simple 1–5 Likert); (3) Criteria-specific prompt (name of criterion only); (4) Full rubric prompt (criterion definition and rubric for each score).",
            "human_evaluation_setup": "Human annotations from eight open benchmark datasets (e.g., SummEval, Hanna, InstruSumm) providing per-example human scores for multiple criteria; paper uses those human labels as reference (reference-free LLM evaluation; original annotator counts not enumerated in paper).",
            "agreement_metric": "Pearson correlation between LLM-generated scores (or perplexity) and human annotations; reported examples include: perplexity Pearson correlation 0.51 for textual content metrics vs 0.44 for prompted LLM judgement (abstract / results).",
            "losses_identified": "When replacing humans with prompted LLM-as-a-judge for textual quality, one might lose the benefit of simple, prompt-robust signal: paradoxically, prompting does not always improve alignment and can degrade performance relative to a prompt-free measure (perplexity) — i.e., prompt-based judgements can be noisier or less aligned for structural/textual metrics.",
            "examples_of_loss": "SummEval fluency: human annotations correlate much better with model perplexity than with any prompting method (paper states fluency agreement much higher with perplexity). The paper also reports perplexity outperforming prompting on SummEval and Flask average correlations.",
            "counterexamples_or_caveats": "For some non-textual, task-specific criteria (e.g., sentiment/aspect extraction in OpinSummEval), prompting (with rubric) is necessary and perplexity underperforms; powerful models (GPT-4) still produce strong prompted judgements and prompting can help on some specific subjective criteria.",
            "paper_reference": "Abstract; Dataset level results; Details on content-based criteria results; Results (perplexity vs prompting comparisons)",
            "uuid": "e9724.0",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Prompt_Rubric_Effect",
            "name_full": "Effect of Increasing Prompt Instruction (Full Rubric) on LLM-as-a-Judge",
            "brief_description": "The paper systematically measures how adding more detailed rubric information to prompts affects alignment with human judgements and finds only small aggregate improvements (≈4% Pearson) across criteria, with heterogeneous effects by model and criterion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Multiple NLG tasks and evaluation criteria across benchmarks (content, relevance, integrity, engagement)",
            "llm_judge_model": "Various (GPT-4, Llama3 70b/8b, Mistral, Phi3, Prometheus-2)",
            "llm_judge_setup": "Comparison across 4 prompting levels: none (perplexity), generic prompt, criterion-name prompt, and full-rubric prompt with definitions and score exemplars.",
            "human_evaluation_setup": "Human annotations from benchmark datasets with provided annotation guidelines; for full-rubric prompting the paper extracts benchmark rubric text and injects it into prompts.",
            "agreement_metric": "Pearson correlation; aggregate improvement from minimal to full rubric is reported as small (about 4% increase aggregated across taxonomy).",
            "losses_identified": "Using LLMs-as-a-judge with full rubric often does not recover the nuanced human judgement advantage — additional instruction yields only marginal gains, meaning detailed human rubric reasoning and contextual interpretation are not fully captured by LLM prompting; in some cases prompting even reduces alignment (rubric can harm performance).",
            "examples_of_loss": "Phi3's performance decreased when full rubric details were provided compared to simpler prompts — Phi3's prior knowledge aligned better with human annotators than when given extensive rubric text. Generally, most large/midsize models saw only small changes when full rubric added.",
            "counterexamples_or_caveats": "Smaller models (Llama3 8b, Mistral) sometimes benefit from full rubric information. GPT-4 shows only marginal improvement from criteria-specific to full-rubric prompting, indicating high prior alignment with humans.",
            "paper_reference": "Introduction; Evaluating LLMs-as-a-judge (prompt settings); Model level results; Conclusion",
            "uuid": "e9724.1",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Integrity_Failure",
            "name_full": "Degradation on Integrity/Logical Correctness when Using LLMs-as-a-Judge (except strongest models)",
            "brief_description": "For integrity-based criteria (logical correctness, task-specific correctness), many LLM-as-judges fail to match human judgements — especially in assigning low/bad scores — unless the judge model itself is capable of solving the task (GPT-4 performs markedly better).",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Logical and mathematical reasoning tasks (e.g., FLASK, Roscoe datasets)",
            "llm_judge_model": "GPT-4, Llama3-70b, Phi3, others (paper highlights GPT-4 vs Llama3/Phi3)",
            "llm_judge_setup": "Prompted judgements (various prompt levels) and perplexity comparisons on task solutions; integrity criteria judged reference-free against task description and solution.",
            "human_evaluation_setup": "Human labels for integrity/logical correctness from Flask and Roscoe benchmark datasets; humans annotated correctness and missing steps etc.",
            "agreement_metric": "Pearson correlation; reported: GPT-4 Pearson = 0.68 on logical correctness vs Llama3-70b = 0.34 and Phi3 = 0.33 (Results - Details on integrity-based criteria).",
            "losses_identified": "Most LLM-as-a-judge models (except top-tier models) systematically fail to detect incorrect logical reasoning and tend to over-score incorrect responses (grade bad examples highly), losing the human ability to discriminate logical correctness — i.e., inability to assign low scores and to reliably detect errors.",
            "examples_of_loss": "Violin plots show that only GPT-4 generates lower scores for 'bad' solutions; other models predominantly give high scores, grading logically incorrect responses as 'very good' (Results, Fig. 6).",
            "counterexamples_or_caveats": "GPT-4 substantially outperforms other models and is capable of producing judgements aligned with humans on integrity; the paper hypothesizes judges need to be able to solve the task themselves to judge correctly (citing Lin et al. 2024). Also, providing more detailed rubric information improves judgement for some complex reasoning tasks.",
            "paper_reference": "Details on integrity-based criteria results; Results (Fig. 6); Model level results; Conclusion",
            "uuid": "e9724.2",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Engagement_Subjectivity",
            "name_full": "Loss of Subjective Nuance in Engagement-based Criteria",
            "brief_description": "Engagement-based criteria (e.g., empathy, surprise) are subjective and show higher annotation variance; LLM-as-a-judge outputs are more variable across models and often cluster, losing nuanced distributional agreement with diverse human judgements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Dialogue and creative story generation (Hanna, TheNextChapter, TopicalChat datasets)",
            "llm_judge_model": "GPT-4, Phi3, Llama3-8b, others",
            "llm_judge_setup": "Prompted evaluation (multiple prompt levels) of engagement-related criteria; reported distribution of generated scores (violin plots).",
            "human_evaluation_setup": "Human annotations for engagement/empathy from datasets like Hanna and TopicalChat; human scores show high variance and lower average scores for these subjective criteria.",
            "agreement_metric": "Pearson correlation reported for empathy: GPT-4 = 0.32, Phi3 = 0.31, Llama3-8b = 0.30; models' outputs have higher variance and often cluster (Results - engagement details, Fig. 5).",
            "losses_identified": "LLM judges lose the diverse subjective nuance found in human annotations: models often cluster around a limited set of scores and produce high-variance outputs that do not reflect the broad human judgement distribution; thus, subtle subjective aspects of engagement are degraded.",
            "examples_of_loss": "Empathy (Hanna dataset): human annotations show significant variation; Phi3 generates scores with higher variability, while most models cluster around a single score—indicating mismatch in capturing subjective diversity (Results, Fig. 5).",
            "counterexamples_or_caveats": "Phi3 sometimes outperforms other models on engagement due to producing higher variability; full rubric information can help for unconventional engagement criteria; overall differences between models on engagement are smaller than on other criteria.",
            "paper_reference": "Details on engagement-based criteria results; Criteria level analysis; Results (Fig. 5)",
            "uuid": "e9724.3",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Dataset_and_Model_Biases",
            "name_full": "Dataset Effects and Model Preference Biases When Using LLMs-as-a-Judge",
            "brief_description": "The paper notes dataset-dependent behaviors and biases: perplexity aligns strongly on simple text tasks but not on complex multi-aspect tasks; some models show preference or misalignment due to finetuning/training data (e.g., propensity to prefer own-style outputs or be inhibited re: harmfulness).",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Wide set of NLG tasks across 8 benchmark datasets (SummEval, OpinSummEval, TheNextChapter, Flask, Roscoe, Hanna, TopicalChat, InstruSumm)",
            "llm_judge_model": "Various (Llama3, Mistral, Phi3, Prometheus-2, GPT-4)",
            "llm_judge_setup": "Prompted evaluations and perplexity scoring across datasets; per-dataset correlations analyzed.",
            "human_evaluation_setup": "Human annotations from each dataset used as ground truth; paper uses dataset-provided annotation guidelines when available.",
            "agreement_metric": "Pearson correlation per-dataset and per-criteria; examples: perplexity often higher than prompting on SummEval and Flask; model-specific numbers given in tables and text.",
            "losses_identified": "Using LLM judges can conflate preference for data similar to their fine-tune/training set with true adherence to evaluation instructions — e.g., LLMs may favor outputs resembling their own training distribution, miss dataset-specific aspects, or be overly influenced by fine-tuning (e.g., harmlessness judged by perplexity shows 100% agreement in one case, likely reflecting strong fine-tuning). Dataset-level averages can be misleading if dominated by text-related criteria.",
            "examples_of_loss": "Prometheus-2: prompting settings 2 and 3 returned poor results; only perplexity and full rubric aligned due to its fine-tuning. Flask dataset: high average perplexity correlation may hide per-criterion variance; SummEval: perplexity outperforms prompting on textual criteria, suggesting loss of nuance when using prompted judges. Harmlessness on Flask: perplexity had 100% agreement with human annotations, possibly reflecting fine-tuning rather than true judgement capability.",
            "counterexamples_or_caveats": "Some datasets and criteria (e.g., TheNextChapter, complex creative/story related criteria) benefit from full rubric prompting. Larger models tend to perform better on relevance and integrity; smaller models may improve with rubrics. The paper explicitly warns dataset-level aggregates can mislead and recommends per-criteria analysis.",
            "paper_reference": "Introduction; Dataset level results; Details on content/relevance/integrity sections; Dataset level analysis; Results",
            "uuid": "e9724.4",
            "source_info": {
                "paper_title": "Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "NLG Evaluation using GPT-4 with Better Human Alignment",
            "rating": 2,
            "sanitized_title": "nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores",
            "rating": 2,
            "sanitized_title": "llms_as_narcissistic_evaluators_when_ego_inflates_evaluation_scores"
        },
        {
            "paper_title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
            "rating": 2,
            "sanitized_title": "perplexed_by_perplexity_perplexitybased_data_pruning_with_small_reference_models"
        },
        {
            "paper_title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
            "rating": 2,
            "sanitized_title": "flask_finegrained_language_model_evaluation_based_on_alignment_skill_sets"
        },
        {
            "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
            "rating": 1,
            "sanitized_title": "roscoe_a_suite_of_metrics_for_scoring_stepbystep_reasoning"
        },
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and Chatbot Arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
            "rating": 1,
            "sanitized_title": "exploring_the_use_of_large_language_models_for_referencefree_text_quality_evaluation_an_empirical_study"
        }
    ],
    "cost": 0.012206999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions
16 Aug 2024</p>
<p>Bhuvanashree Murugadoss 
Christian Poelitz cpoelitz@microsoft.com 
Ian Drosos 
Vu Le 
Microsoft Redmond</p>
<p>Nick Mckenna 
Carina Suzana Negreanu 
Chris Parnin 
Microsoft Redmond</p>
<p>Advait Sarkar 
Microsoft Research </p>
<p>Microsoft Research Cambridge
UK</p>
<p>Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions
16 Aug 2024A7F8E37D6ECFF9380C47F93F61914ED2arXiv:2408.08781v1[cs.AI]
LLMs-as-a-judge is a recently popularized method which replaces human judgements in task evaluation(Zheng et al. 2024) with automatic evaluation using LLMs.Due to widespread use of RLHF (Reinforcement Learning from Human Feedback), state-of-the-art LLMs like GPT4 and Llama3 are expected to have strong alignment with human preferences when prompted for a quality judgement, such as the coherence of a text.While this seems beneficial, it is not clear whether the assessments by an LLM-as-a-judge constitute only an evaluation based on the instructions in the prompts, or reflect its preference for high-quality data similar to its finetune data.To investigate how much influence prompting the LLMs-as-a-judge has on the alignment of AI judgements to human judgements, we analyze prompts with increasing levels of instructions about the target quality of an evaluation, for several LLMs-as-a-judge.Further, we compare to a promptfree method using model perplexity as a quality measure instead.We aggregate a taxonomy of quality criteria commonly used across state-of-the-art evaluations with LLMs and provide this as a rigorous benchmark of models as judges.Overall, we show that the LLMs-as-a-judge benefit only little from highly detailed instructions in prompts and that perplexity can sometimes align better with human judgements than prompting, especially on textual quality.</p>
<p>Introduction</p>
<p>Recently, new automatic evaluation approaches that rely on LLMs have been proposed on several NLG tasks, such as summarization (Liu et al. 2023b) and machine translation (Kocmi and Federmann 2023).Previous approaches (Siledar et al. 2024) show that for certain situations, such as when assessing textual consistency or fluency, there is high agreement between human judgements and LLM assessments, even without detailed instructions like for example how to assign specific scores.Most of these approaches prompt an LLM to give a judgement as Likert score (Likert 1932) with only simple information about the scale, e.g."give a judgement between 1 (bad) and 5 (good)."More recently, LLM-based evaluations on more fine-grained task-specific criteria (Ye et al. 2024) have also reported high agreement with human judgement, such as assessing the completeness of a solution for a question answering task.</p>
<p>For these evaluations, often more detailed instructions are given about when to assign a specific score, similarly to rubric scoring (Andrade 2005).</p>
<p>While these results are promising for the future of automatic evaluation, it is less clear how the models achieve this agreement, and in general, it is a challenge to identify for any given task, which LLM is most appropriate to evaluate it, and with how much information, respectively how much instructions about the evaluation.Alarmingly, recent results show a clear bias in LLMs preferring their own output over others (Panickssery, Bowman, and Feng 2024), and LLMs' perplexity has emerged as a possible quality criteria for filtering (Ankner et al. 2024) on textual quality.This raises the question of whether some of the results on automatic evaluations with LMs reflect a model's preference for data similar to its own (high quality) fine-tuning data instead of following the provided instructions on how to measure the quality of an answer.Especially for fine-grained evaluations with detailed rubric information, we expect the instructions about when to assign a score to be adhered closely.</p>
<p>In this paper we report our findings when using LLMs-asa-judge (Zheng et al. 2024), where LLMs are used as surrogates for humans judgements to evaluate several NLG and LLM-based tasks, in skill-specific settings (e.g.completeness) and skill-unspecific (e.g.textual coherence).We show the annotations for many of these quality criteria in state-ofthe-art benchmarks have a high correlation with the perplexity of the LLMs, often higher than prompting the LLM for a score.We identify which evaluation settings can benefit the most from more detailed prompting and for which settings simple generic prompts, or just using models' perplexity as quality score, suffice.</p>
<p>In detail, we make the following three main contributions: 1.We propose a novel taxonomy of qualitative evaluation criteria useful for assessing the competence of automatic evaluation methods by LLMs-as-a-judge.Our taxonomy consists of 4 evaluation categories (Content, Relevance, Integrity, and Engagement) which encapsulate 34 metrics as tested by 8 distinct state-of-the-art benchmark datasets.2. We systematically evaluate the effectiveness of LLMsas-a-judge using the taxonomy with several major LLM families including GPT4, Llama3, Mistral, and Phi3 across 4 levels of increasing prompt instruction.We find that, aggregated across the taxonomy, increasing instruction by including more granular evaluation rubrics only somewhat improves the Pearson correlation of models with human judgements, by only as much as 4%.However, some individual metrics may benefit.3. We evaluate the potential of simple model perplexity as an alternative automatic evaluation to LLMs-as-a-judge.While perplexity often outperforms minimal prompting in terms of correlation with human judgements when detailed rubrics are not available, textual content-related metrics are the closest aligned category.For these metrics perplexity achieves a Pearson correlation of 0.51 in contrast to 0.44 when prompting the LLM-as-a-judge, suggesting it is the better choice for simple scenarios.</p>
<p>Related Work</p>
<p>LLMs as evaluators for general NLG (Liu et al. 2023b), as well as for knowledge and problem-solving tasks (Ye et al. 2024) have been widely studied recently (Chiang and Lee 2023;Li et al. 2024;Gao et al. 2024).Most previous approaches, either perform pair-wise evaluations (Ji et al. 2023;Chen et al. 2023), measuring the preference of one of two examples for a given criterion, or perform direct assessments for a single given example and a evaluation criterion (Liu et al. 2023b;Ye et al. 2024).Additionally, they distinguish between reference-free evaluations, where the LLM is presented only an example and the criterion for evaluation, and reference-based evaluations with given annotated examples of different qualities or ground truth for each example.Generally, previous works use LLMs as evaluators by using simple prompting strategies (Siledar et al. 2024), only few fine-tuned models are available (Kim et al. 2023(Kim et al. , 2024) ) for measuring for specific quality criteria.Most other finetuning approaches concentrate on scenario-specific quality feedbacks (Li et al. 2023;Wang et al. 2023) or on specific use-cases (McAleese et al. 2024).</p>
<p>Recently, there are several approaches (Liu, Moosavi, and Lin 2024;Liu et al. 2024Liu et al. , 2023c;;Stureborg, Alikaniotis, and Suhara 2024) reporting biases and mismatches with human annotations, but our work is the first to study whether the models' perplexity can be a better surrogate for quality then prompting the corresponding model and whether instructions in the prompts are impacting the results across a number of different LLMs-as-a-judge.</p>
<p>Evaluating LLMs-as-a-judge</p>
<p>In this section we give a short definition of LLMs-as-a-judge and automatic evaluation using AI.We define different settings of prompting the LLMs-as-a-judge to measure the impact on the alignment of LLM judgments with human judgements.We also evaluate a prompt-free metric using simple model perplexity.This alternative approach requires no prompt engineering and transparently measures alignment with training data without bias from a prompt, so it is a compelling alternative for evaluation.Finally, we introduce a new taxonomy, aggregating the quality criteria most frequently used in state-of-the-art benchmarks for automatic evaluation with LLMs.We categorize these into 4 groups representing the major aspects of evaluating AI generated responses.</p>
<p>LLMs-as-a-judge</p>
<p>As LLM-as-a-judge we refer to the definition introduced by (Zheng et al. 2024) as potential replacement for human annotations by prompting an LLM for a judgement of an AI assistant response.We concentrate on judging textual examples only e.g., AI generated summaries for news articles (Fabbri et al. 2021) or step-by-step solution to mathematical reasoning questions (Golovneva et al. 2023).We phrase the task to judge an AI generated response as the following: Given a task A and an AI generated solution B, judge the quality of the solution B considering only the task A. In contrast to other previous approaches, we perform a reference-free evaluation where we do not provide a possible correct reference solution.We solely rely on the models' ability to judge the solution given only the task.</p>
<p>To measure the impact of prompting the LLM-as-a-judge, we study the LLMs' performance in 4 different settings:</p>
<ol>
<li>
<p>Perplexity: We score each task solution by its perplexity under the corresponding LLM, given only the task description.This approach is unbiased by prompts, so it transparently measures alignment with model training data, providing a good comparison and alternative to prompt-based approaches.</p>
</li>
<li>
<p>Generic quality prompt: We prompt each LLM-as-ajudge with a basic instruction to measure the quality of the task solution, but give no specific criteria or instructions.In this case, we rely solely on the models' prior knowledge about the quality for the task solution from the examples generated.</p>
</li>
<li>
<p>Criteria specific prompt: We prompt each LLM-as-ajudge with an instruction to measure the quality for a specific criteria e.g., coherence.We only provide the name of the criteria, not a definition.We rely on the models' prior knowledge of the specific quality criteria only.</p>
</li>
</ol>
<p>Full rubric prompt:</p>
<p>We prompt each LLM-as-a-judge with an instruction to measure the quality for a specific quality criterion, together with a definition of the criterion and instructions when to assign each rubric score e.g., "Score 1: Incoherent text with many logical flaws."</p>
<p>We evaluate different LLMs-as-a-judge under the above settings on several different benchmarks (as described in the next subsection).We use the criteria as specified in the corresponding annotation guidelines from the benchmark datasets.For setting 4, we use all available annotation guidelines with information about the criteria and when to assign each score.We extract this information directly from benchmarks into a full rubric containing information about the criteria and the scores.For our experiments, we structure the settings from least instructive (Perplexity / no prompting) to most instructive (Full rubric information with instructions when to assign a score).In Fig. 1 we show the different setting of prompting for an example quality criterion.</p>
<p>Generic prompt # Task to evaluate</p>
<p>Your tasks is to evaluate the interaction between a user and an AI assistant.I want you to evaluate the assistant's response.Evaluate the quality of the response.The AI responses are for: {task description}.</p>
<h1 example="example">Sample to evaluate</h1>
<h1>Instructions</h1>
<p>Evaluate the quality of the response from the sample and return a score between 1 (bad) and 5 (very good) as: ## Score: [Number] 3. Criteria specific prompt # Task to evaluate Your tasks is to evaluate the interaction between a user and an AI assistant.I want you to evaluate the assistant's response.Evaluate the response for the given rubric below.Use the rubric to guide you evaluating and base all your evaluation decision on the rubric.The AI responses are for: {task description}.</p>
<h1 example="example">Sample to evaluate</h1>
<h1>Rubrics Logicality</h1>
<h1>Instructions</h1>
<p>Evaluate the quality of the response from the sample and return a score between 1 (bad) and 5 (very good) as: ## Score: [Number] 4. Full rubric prompt # Task to evaluate Your tasks is to evaluate the interaction between a user and an AI assistant.I want you to evaluate the assistant's response.Evaluate the response for the given rubric below.Use the rubric to guide you evaluating and base all your evaluation decision on the rubric.The AI responses are for: {task description}.</p>
<h1>Sample to evaluate {example} # Rubrics Logicality: Measure how much the story obeys your commonsense.Score 1: The story is full of absurd things.Score 2: The story has one or two things make sense, but generally very absurd.Score 3: The story roughly makes sense.Score 4: The story largely makes sense, except one or two things.Score 5: The story totally complies with commonsense.</h1>
<h1>Instructions</h1>
<p>Evaluate the quality of the response from the sample and return a score between 1 and 5 as: ## Score: [Number] Figure 1: Our prompting settings.We measure how much influence the information about the actual evaluation has for model performance as LLM-as-a-judge.For setting 1, perplexity, we don't prompt the models but calculate the models' perplexity for the task solution in the example instead.The example prompts shown above are used for the LLMs-as-a-judge to measure the quality for the criterion logicality as defined in for the benchmark dataset TheNextChapter.</p>
<p>Datasets</p>
<p>We use 8 different open-source benchmark datasets commonly used for LLM-based evaluations with human annotations for several evaluation criteria per task.The datasets contain task which span several aspects from coarse-grained NLG-quality evaluations, to fine-grained very task specific evaluations with detailed information about how to score examples.</p>
<p>Firstly, we leverage two of the most prominently used datasets for coarse-grained NLG-quality evaluations: The SummEval (Fabbri et al. 2021) dataset contains news article summaries generated by different models together with human annotations for 4 different quality criteria e.g., fluency; and the TopicalChat (Gopalakrishnan et al. 2019) dataset contains human conversations over 8 different topics annotated by humans for 5 different quality criteria e.g., engagement.Further, we use two more challenging benchmark datasets for coarse-grained NLG-evaluations: the OpinSummEval (Shen and Wan 2023) dataset is a opinion summarization dataset, which consists of review summaries annotated for aspects, opinions and sentiments; the InstruSumm (Liu et al. 2023a) dataset, consists of news article summaries following specific instructions with human annotations for content specific quality-criteria e.g., amount of missing information.</p>
<p>Second, we use two benchmark datasets for more finegrained NLG evaluations: the Hanna (Chhun et al. 2022) dataset and the TheNextChapter (Xie, Cohn, and Lau 2023) dataset contain creative stories generated for a given initial user prompt.Each story is annotated by humans for NLG and style based criteria e.g., coherence, but also for more unconventional criteria like surprise.Finally, we use two task-specific evaluation benchmark datasets with quality-criteria depending in task solution quality: Roscoe (Golovneva et al. 2023) is a collection of datasets of reasoning tasks, together with GPT3 generated with stepby-step solutions.The human annotations cover coarsegrained task specific evaluation criteria like "missing step"; the Flask (Ye et al. 2024) dataset contains several knowledge and problem solving tasks with LLM generated solutions.The human annotations cover more fine-grained taskspecific criteria like completeness and factuality.Most criteria need an understanding of the solution e.g., completeness.</p>
<p>Criteria taxonomy</p>
<p>We introduce a simple taxonomy of quality evaluation criteria based on current state-of-the-art benchmark datasets and quality criteria commonly used for automatic evaluations by LLMs.We define 4 groups of quality criteria, relevant for automatic evaluation:</p>
<ol>
<li>Content-based criteria: Measure how well the solution is presented to the user, for example, whether a news article summary is fluent.We assign each of the criteria used in the benchmark datasets above to these 4 groups (Fig. 2).For content-based criteria, we are interested in how to measure the quality of the content as it is presented to the user.This includes mainly criteria of the textual quality of the solution.For example, the criterion fluency is used for measuring the quality of the summaries in the SummEval dataset and hence is a content-based criteria.The engagement-based criteria, combine criteria of how the AI generated solution engages with the user.This includes for example the empathy criterion used to measure the quality of the generated stories in the Hanna dataset.The remaining two groups concentrate on more task specific evaluation criteria.Integrity-based criteria measure the coherence of task solution and whether it makes sense logically.For example the criterion logical correctness, used for measuring the quality of (mathematical) resonsning or coding task solutions in the Flask dataset, is a integrity-based criterion.Finally, relevance-based criteria measure the direct relevance of a task solution to the actual task.This includes for example the criterion relevance, used for measuring the connections of task solutions and initial task in several of the benchmark datasets e.g., TheNextChapter dataset.The separation of the quality criteria classes are not a 100% perfect and there are overlaps, for example content-based criteria like readability, can also be seen as engagement-based, since less legible solution are also less engaging.</li>
</ol>
<p>Model Selection for LLM-as-a-judge</p>
<p>To</p>
<p>Results</p>
<p>In this section, we present the main results of the evaluations using the different LLMs-as-a-judge under the different settings of prompting.Analogous to previous work (Liu et al. 2023b), to measure the quality of the evaluations we calculate the Pearson correlation of the generated scores by the LLMs-as-a-judge, respectively the perplexity values, and the human annotations given for each quality criteria from the benchmark dataset.We split the results section into model level, dataset level and criteria level results.In the model level results subsection, we present the results comparing the different LLMs under the setting of prompting, averaging over all criteria; the dataset level results subsection presents the results when we compare the different datasets under the setting of prompting average over all criteria; the criteria level results subsection presents the results when we compare models and setting of prompting under the different groups of criteria.Finally, we present the results of a detailed analysis for each group of criteria from the introduced taxonomy.</p>
<p>Model level results</p>
<p>There is only small effect adding full rubric information.</p>
<p>Providing the LLMs-as-a-judge with more detailed rubric information of the quality criteria, generally has only small influence on evaluation performance for the large and midsize models, and might even be disadvantageous in certain situations (see Tab.1).For instance, Phi3's performance decreases when complete rubric details are provided compared to simple prompts which only mention the criterion name in the prompt.Here, Phi3's prior knowledge about evaluating the criteria has higher agreement with human annotators compared to when using full rubric information.Only the smaller Llama3 8b and Mistral models see improvements when given comprehensive rubric information for assessment.Among the open models, Llama3, both the 70b and 8b versions, perform best.Meanwhile, Mistral and Prometheus-2 do not show improvements when the LLM is prompted, with models' perplexity having higher correlation then the generated scores.For Prometheus-2, we only report perplexity and full rubric information in the prompts since this aligns with the fine-tuning data for this model and both setting 2 and 3 did return very poor results.GPT4 performs best among all models.As may be expected, prompting GPT4-as-a-judge, even for a generic quality judgement, results in the highest performance in terms of agreement with human annotations compared to all other models tested.Further, GPT4's judgements do only improve marginally from prompting setting 3 to 4, indicating that GPT4's prior knowledge about evaluating does already agree with the human judgements to a high degree without the need to add more detailed rubric information about the evaluation.</p>
<p>Dataset level results</p>
<p>Perplexity correlates with text quality criteria.We observe (see Tab. 2) that the quality criteria from datasets with simple textual content creation tasks e.g., summarization in the SummEval dataset or story generation in the Hanna dataset, show high agreement with models' perplexity compared to simple prompting (setting 2 and 3).For more complex NLG tasks, which depend on several aspects and multiple possible steps, the human annotation correlate less strongly with perplexity compared prompting the LLMsas-a-judge with more information.For example the opinion summary evaluations from the OpinSummEval dataset uses criteria which depend on sentiment identification and extractions of the key aspects, in these cases prompting the LLM seems necessary.</p>
<p>Full rubric information helps for non-default textual quality evaluations.Unusual textual quality evaluation tasks which measure the quality beyond simple textual criteria like fluency, can benefit from more full rubric information about the the evaluation task.For example, we observe that for the TheNextChapter dataset, full rubric information in the prompts to the LLMs-as-a-judge leads to judgements with the highest correlations with human annotations.Compared to other datasets for textual quality evaluation, these two datasets contain much more complex texts e.g., creative stories with non-default quality criteria like relatedness which is difficult to estimate without additional information by an LLM.Furthermore, evaluating more complex tasks which include more than text quality, like the logical reasoning tasks, benefit also from more detailed rubric information in the prompts.The logical and mathematical reasoning tasks in the Roscoe datasets for example do benefit from more information to effectively judge as shown by the higher correlations with the human judgements compared to prompting with less information or using perplexity.</p>
<p>Dataset level analysis can be misleading.Models' perplexity on both the Flask dataset and the SummEval dataset, outperforms simple prompting in aligning to human judgements.While the quality criteria in the SummEval dataset primarily focuses on textual quality where we expect perplexity to perform well for example, the Flask dataset consist a variety of different quality criteria which make it difficult to generalise and the average correlations values might be biased the high values on the text related criteria.In the next subsection, we investigate this issue by using the previously introduced taxonomy to analyze results on a percriteria class basis rather than average results per dataset.</p>
<p>Criteria level analysis</p>
<p>Content-based quality criteria correlate the most with perplexity.When evaluating quality with a focus on textual context, perplexity seems a viable alternative to prompting LLMs-as-a-judge.We observe that on average the agreement with human annotations is more then 20% higher when using models' perplexity to judge the quality compared to prompting (Fig. 3).Further, there are only small differences between using a simple generic quality prompt for evaluation compared to all other settings of prompting, showing that models' prior knowledge generates judgements with high agreement with human judgements on textual quality.</p>
<p>Engagement-based quality criteria benefit the most from full rubric information.These criteria are unconventional as they assess the likelihood of a user feeling personally engaged, as opposed to merely evaluating straightforward text quality.Access to full rubric information can help judging with directives, particularly when the evaluation is more unusual and different from the text quality alone.</p>
<p>Often, there is only little improvement of adding full rubric information for most criteria groups.Except for the engagement-based criteria, there is only limited effect on adding full rubrics information to the prompts.Further, simple prompts for generic quality judgements results in similar correlation values with the human annotations than detailed information for content and relevance based evaluation crite-  ria.This confirms again that advanced models' prior knowledge of text quality or relevance already has a high agreement with human judgements.GPT4 clearly outperforms all other models.GPT-4 particularly outperforms in relevance and integrity quality criteria, surpassing other models on these criteria (Fig. 4).Although Phi3 matches GPT-4's performance in engagement based criteria, it generally performs less consistent with human annotations across other evaluative measures; Mitral's performance falls short in criteria associated with relevance and integrity; LLamA3-70b exhibits a marginally improved performance over Phi3 when it comes to content and relevance-based criteria.</p>
<p>Details on content-based criteria results</p>
<p>Drilling down the content-based evaluations criteria, we observe that perplexity outperforms prompting mainly on structural text quality criteria.For example, human annotations for fluency from the SummEval dataset have much higher agreement with perplexity compared to all prompting methods.This quality criterion judges grammar, spelling, and sentence structure for example.On the other hand, evaluating for more complex, specific and subjective contentbased criteria like naturalness, which measures how natural the task response sounds, benefits from more instructions in the prompts for the LLMs-as-a-judge.Here, instructions to judge how much the task solution resembles a human answer improves agreement with human judgements.</p>
<p>Notably, we observe that model perplexity has 100% agreement with the human annotations for harmlessness on Flask datasets.This might reflect the strong influence fine-tuning has in inhibiting the generation of harmful content (Dubey et al. 2024).Conversely, prompting for measuring harmfulness performs much lower until we provide full rubric information in the prompts.</p>
<p>Details on engagement-based criteria results</p>
<p>Engagement-based criteria are more challenging to judge since they are often subjective.We observe that there are fewer performance differences between the models compared to the results on the other criteria e.g., GPT4 judgements have an agreement (by Pearson correlation) of 0.32, Phi3 of 0.31 and Llama3 8b of 0.3.The overall performance of all models on these criteria is lower and the scores generated by the models have higher variance compared to other criteria.Further, human annotations for these criteria have on average lower scores with higher variance.For example, for the criterion empathy from the Hanna dataset (Fig. 5), human annotations show a significant degree of variation, and Phi3 notably generates scores with higher variability,   unlike most other models that tend to cluster around a singular score.Hence, the distribution in engagement-based criteria may explain why Phi3 outperforms other models.</p>
<p>Details on relevance-based criteria results</p>
<p>For relevance-based quality criteria, we assume that the models need robust information about the problem to measure whether the information in the task solution is relevant, and estimate to what degree.Models' perplexity, but also generic prompts seem not sufficient for evaluation since relevance is more specific to certain aspects of the task solution.</p>
<p>Still, we also observe that including a full rubric doesn't always appear necessary; instead the size of the models seem more important.For the criterion groundedness from the TopicalChat dataset for example (Tab.4), we identify a clear trend of increasing agreement with the human judgements with larger models as LLM-as-a-judge.</p>
<p>Details on integrity-based criteria results</p>
<p>Similar to the relevance-based quality criteria, we assume that to measure the quality for integrity-based criteria the LLMs-as-a-judge need to have an understanding of the task, but also the ability to solve the task itself.We hypothesize that, for task-specific evaluations, the underlying LLM-asa-judge actually needs to be able to solve the task itself to apply the correct score.As reported in (Lin et al. 2024), LLMs' ability to critic a task solution correlates with its ability to solve the task.We exemplify this by the evaluations for the integritybased evaluation criterion on the criterion logical correctness from the Flask dataset.This criterion reflects the correctness elements which are reflected in human annotations, which are more clearly scored as high (for logically correct) or low (for logically incorrect) scores.To select the appropriate scores, the LLM-as-a-judges need to know what is correct and what is wrong.Here, GPT-4 significantly outperforms all other models by a wide margin with a Pearson correlation of 0.68 with the human judgements in contrast to 0.34 for Llama3 70b and 0.33 for Phi3, for example.</p>
<p>To illustrate this, we plot the generated scores of the LLMs-as-a-judge (Fig. 6) and the human annotations.We observe that only GPT4 is able to generate lower scores to judge a task solution as "bad."All other models predominantly give high scores, consistently grading bad logically incorrect responses as "very good."</p>
<p>Conclusion</p>
<p>In this paper, we investigate how increasing levels of prompting impact the automatic evaluations made by LLMsas-a-judge in measuring the quality of AI-generated text.We introduce a new taxonomy of quality criteria, summarizing commonly used criteria in automatic evaluations with LLMs into four broad categories: Content, Relevance, Integrity, and Engagement.We systematically evaluated several LLMs, including GPT-4, Llama-3, and others, across all settings of prompting to determine if more detailed instructions enhance the LLMs' alignment with human judgements.Key findings include:</p>
<p>• Detailed quality criteria information might not be necessary in the most powerful models; for instance, GPT-4 shows a high level of agreement with human judgements even without detailed instruction.• Simple perplexity values are very effective at estimating textual quality, often outperforming the results of prompting the LLMs-as-a-judge with basic instructions.</p>
<p>• Judging task-specific quality criteria like relevance or logical correctness requires more capable, larger models, aligning with previous research on the necessary model capabilities for critiquing (Lin et al. 2024).</p>
<p>Figure 2 :
2
Figure 2: Taxonomy of quality criteria summarizing current state-of-the-art benchmark datasets and criteria used for automatic evaluations with LLMs.We group all 34 quality criteria as defined in the 8 different benchmark datasets into 4 groups: Content-based, Engagement-based, Integritybased, Relevance-based criteria.</p>
<p>understand how model size and finetuning affect performance across the different quality criteria and settings of prompting, we test several current LLMs: GPT4-Turbo (OpenAI et al. 2024)-0125 as large closed-model baseline; Llama3 70b (Touvron et al. 2023; Dubey et al. 2024) as a medium size open-model; Llama3 8b, Mistral-v0.3 (Jiang et al. 2023) as small open-models, Phi3 (Abdin et al. 2024)-Medium-128k as fine-tuned model for reasoning, and Prometheus-2 (Kim et al. 2024) as fine-tuned models for evaluation tasks 1 .</p>
<p>Figure 3 :
3
Figure 3: Radar chart of average Pearson correlations for the quality criteria groups for each of different settings of prompting (1 -Perplexity / No Prompting, 2 -Generic prompt, 3 -Specific prompt, 4 -Full rubric) over all models.</p>
<p>Figure 4 :
4
Figure 4: Radar chart of average Pearson correlations for the quality criteria groups for each of the different LLMs-as-ajudge over all setting of prompting.</p>
<p>Figure 5 :
5
Figure 5: Violin plot of the generated scores by the LLMsas-a-judge for the engagement-based criterion empathy, together with the corresponding human annotations.</p>
<p>Figure 6 :
6
Figure 6: Violin plot of the generated scores by the LLMsas-a-judge for integrity-based criterion logical correctness from the Flask dataset, together with human annotations.</p>
<p>Table 3
3: Pearson correlations of scores generated by differ-ent LLMs-as-a-judge with human annotations for content-based evaluation criteria, from the benchmark dataset for re-spective settings (1 -Perplexity (No Prompt), 2 -Genericprompt, 3 -Specific prompt, 4 -Full rubric)</p>
<p>Table 4 :
4
Pearson correlations of the scores generated by different LLMs-as-a-judge with the human annotations for
groundedness for the different setting (1 -Perplexity / NoPrompting, 2 -Generic prompt, 3 -Specific prompt, 4 -Full rubric)
. Engagement-based criteria: Measure how engaging the solution is, for example, whether a generated story contains an element of surprise.
3. Integrity-based criteria: Measure how consistent and logical coherent the solution is, for example, whether a math solution is correct.
4.Relevance-based criteria: Measure how relevant the solution is for the given task, for example, whether a legal advice answer contains irrelevant information.
In all experiments, we generate
responses with a temperature of 0.3 and use the average of all generated scores, similar to(Liu et al. 2023b).</p>
<p>. M Abdin, S A Jacobs, A A Awan, J Aneja, A Awadallah, H Awadalla, N Bach, A Bahree, A Bakhtiari, J Bao, H Behl, A Benhaim, M Bilenko, J Bjorck, S Bubeck, Q Cai, M Cai, C C T Mendes, W Chen, V Chaudhary, D Chen, D Chen, Y.-C Chen, Y.-L Chen, P Chopra, X Dai, A D Giorno, G De Rosa, M Dixon, R Eldan, V Fragoso, D Iter, M Gao, M Gao, J Gao, A Garg, A Goswami, S Gunasekar, E Haider, J Hao, R J Hewett, J Huynh, M Javaheripi, X Jin, P Kauffmann, N Karampatziakis, D Kim, M Khademi, L Kurilenko, J R Lee, Y T Lee, Y Li, Y Li, C Liang, L Liden, C Liu, M Liu, W Liu, E Lin, Z Lin, C Luo, P Madan, M Mazzola, A Mitra, H Modi, A Nguyen, B Norick, B Patra, D Perez-Becker, T Portet, R Pryzant, H Qin, M Radmilac, C Rosset, S Roy, O Ruwase, O Saarikivi, A Saied, A Salim, M Santacroce, S Shah, N Shang, H Sharma, S Shukla, X Song, M Tanaka, A Tupini, X Wang, L Wang, C Wang, Y Wang, R Ward, G Wang, P Witte, H Wu, M Wyatt, B Xiao, C Xu, J Xu, W Xu, S Yadav, F Yang, J Yang, Z Yang, Y Yang, D Yu, L Yuan, C Zhang, C Zhang, J Zhang, L L Zhang, Y Zhang, Y Zhang, Y Zhang, arXiv:2404.14219and Zhou, X. 2024. Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone</p>
<p>Teaching With Rubrics: The Good, the Bad, and the Ugly. H G Andrade, College Teaching. 5312005</p>
<p>Z Ankner, C Blakeney, K Sreenivasan, M Marion, M L Leavitt, M Paul, arXiv:2405.20541Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models. 2024</p>
<p>Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study. Y Chen, R Wang, H Jiang, S Shi, R Xu, arXiv:2304.007232023</p>
<p>Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation. C Chhun, P Colombo, F M Suchanek, C Clavel, N Calzolari, C.-R Huang, H Kim, J Pustejovsky, L Wanner, K.-S Choi, P.-M Ryu, H.-H Chen, L Donatelli, H Ji, S Kurohashi, P Paggio, N Xue, S Kim, Y Hahm, He, Proceedings of the 29th International Conference on Computational Linguistics. T K Lee, E Santus, F Bond, S.-H Na, the 29th International Conference on Computational LinguisticsGyeongju2022Republic of Korea: International Committee on Computational Linguistics</p>
<p>Can Large Language Models Be an Alternative to Human Evaluations. C.-H Chiang, H.-Y Lee, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>. Canada ; Toronto, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, A Goyal, A Hartshorn, A Yang, A Mitra, A Sravankumar, A Korenev, A Hinsvark, A Rao, A Zhang, A Rodriguez, A Gregerson, A Spataru, B Roziere, B Biron, B Tang, B Chern, C Caucheteux, C Nayak, C Bi, C Marra, C Mcconnell, C Keller, C Touret, C Wu, C Wong, C C Ferrer, C Nikolaidis, D Allonsius, D Song, D Pintz, D Livshits, D Esiobu, D Choudhary, D Mahajan, D Garcia-Olano, D Perino, D Hupkes, E Lakomkin, E Albadawy, E Lobanova, E Dinan, E M Smith, F Radenovic, F Zhang, G Synnaeve, G Lee, G L Anderson, G Nail, G Mialon, G Pang, G Cucurell, H Nguyen, H Korevaar, H Xu, H Touvron, I Zarov, I A Ibarra, I Kloumann, I Misra, I Evtimov, J Copet, J Lee, J Geffert, J Vranes, J Park, J Mahadeokar, J Shah, J Van Der Linde, J Billock, J Hong, J Lee, J Fu, J Chi, J Huang, J Liu, J Wang, J Yu, J Bitton, J Spisak, J Park, J Rocca, J Johnstun, J Saxe, J Jia, K V Alwala, K Upasani, K Plawiak, K Li, K Heafield, K Stone, K El-Arini, K Iyer, K Malik, K Chiu, K Bhalla, L Rantala-Yeary, L Van Der Maaten, L Chen, L Tan, L Jenkins, L Martin, L Madaan, L Malo, L Blecher, L Landzaat, L De Oliveira, M Muzzi, M Pasupuleti, M Singh, M Paluri, M Kardas, M Oldham, M Rita, M Pavlova, M Kambadur, M Lewis, M Si, M K Singh, M Hassan, N Goyal, N Torabi, N Bashlykov, N Bogoychev, N Chatterji, O Duchenne, O ¸elebi, P Alrassy, P Zhang, P Li, P Vasic, P Weng, P Bhargava, P Dubal, P Krishnan, P S Koura, P Xu, Q He, Q Dong, R Srinivasan, R Ganapathy, R Calderer, R S Cabral, R Stojnic, R Raileanu, R Girdhar, R Patel, R Sauvestre, R Polidoro, R Sumbaly, R Taylor, R Silva, R Hou, R Wang, S Hosseini, S Chennabasappa, S Singh, S Bell, S S Kim, S Edunov, S Nie, S Narang, S Raparthy, S Shen, S Wan, S Bhosale, S Zhang, S Vandenhende, S Batra, S Whitman, S Sootla, S Collot, S Gururangan, S Borodinsky, T Herman, T Fowler, T Sheasha, T Georgiou, T Scialom, T Speckbacher, T Mihaylov, T Xiao, U Karn, V Goswami, V Gupta, V Ramanathan, V Kerkez, V Gonguet, V Do, V Vogeti, V Petrovic, W Chu, W Xiong, W Fu, W Meers, X Martinet, X Wang, X E Tan, X Xie, X Jia, X Wang, Y Goldschlag, Y Gaur, Y Babaei, Y Wen, Y Song, Y Zhang, Y Li, Y Mao, Z D Coudert, Z Yan, ; A Chen, A Lupu, A Alvarado, A Caples, A Gu, A Ho, A Poulton, A Ryan, A Ramchandani, A Franco, A Saraf, A Chowdhury, A Gabriel, A Bharambe, A Eisenman, A Yazdan, B James, B Maurer, B Leonhardi, B Huang, B Loyd, B D Paola, B Paranjape, B Liu, B Wu, B Ni, B Hancock, B Wasti, B Spence, C Mejia, C Wang, C Kim, C Zhou, C Hu, C Chu, C.-H Cai, C Tindal, C Feichtenhofer, C Civin, D Beaty, D Kreymer, D Li, D Wyatt, D Adkins, D Xu, D Testuggine, D David, D Parikh, D Liskovich, D Foss, D Wang, D Le, D Holland, D , Papakipos, Z.Singh, A.Grattafiori, A.Jain, A.Kelsey, A.Shajnfeld, A.Gangidi, A.Victoria, A.Goldstand, A.Menon, A.Sharma, A.Boesenberg, A.Vaughan, A.Baevski, A.Feinstein, A.Kallet, A.Sangani, A.Yunus,Association for Computational LinguisticsStojkovic, B; Montalvo, B; Parker, CDowling, E.; Jamil, E.; Montgomery, E.; Presani, E.; Hahn, E.; Wood, E.; Brinkman, E.; Arcaute, E.; Dunbar, E.; Smothers, E.; Sun, F.; Kreuk, F.; Tian, F.; Ozgenel, F.; Caggioni, F.; Guzmán, F.; Kanayet, F.; Seide, F.; Florez, G. M.; Schwarz, G.; Badeer, G.; Swee, G.; Halpern, G.; Thattai, G.; Herman, G.; Sizov, G.; Guangyi; Zhang</p>
<p>. G Lakshminarayanan, H Shojanazeri, H Zou, H Wang, H Zha, H Habeeb, H Rudolph, H Suk, H Aspegren, H Goldman, I Molybog, I Tufanov, I.-E Veliche, I Gat, J Weissman, J Geboski, J Kohli, J Asher, J.-B Gaya, J Marcus, J Tang, J Chan, J Zhen, J Reizenstein, J Teboul, J Zhong, J Jin, J Yang, J Cummings, J Carvill, J Shepard, J Mcphie, J Torres, J Ginsburg, J Wang, K Wu, U , K H Saxena, K Prasad, K Khandelwal, K Zand, K Matosich, K Veeraraghavan, K Michelena, K Li, K Huang, K Chawla, K Lakhotia, K Huang, K Chen, L Garg, L Silva, L Bell, L Zhang, L Guo, L Yu, L Moshkovich, L Wehrstedt, L Khabsa, M Avalani, M Bhatt, M Tsimpoukelli, M Mankus, M Hasson, M Lennie, M Reso, M Groshev, M Naumov, M Lathi, M Keneally, M Seltzer, M L Valko, M Restrepo, M Patel, M Vyatskov, M Samvelyan, M Clark, M Macey, M Wang, M Hermoso, M J Metanat, M Rastegari, M Bansal, M Santhanam, N Parks, N White, N Bawa, N Singhal, N Egebo, N Usunier, N Laptev, N P Dong, N Zhang, N Cheng, N Chernoguz, O Hart, O Salpekar, O Kalinli, O Kent, P Parekh, P Saab, P Balaji, P Rittner, P Bontrager, P Roux, P Dollar, P Zvyagina, P Ratanchandani, P Yuvraj, P Liang, Q Alao, R Rodriguez, R Ayub, R Murthy, R Nayani, R Mitra, R Li, R Hogan, R Battey, R Wang, R Maheswari, R Howes, R Rinott, R Bondu, S J Datta, S Chugh, S Hunt, S Dhillon, S Sidorov, S Pan, S Verma, S Yamamoto, S Ramaswamy, S Lindsay, S Lindsay, S Feng, S Lin, S Zha, S C Shankar, S Zhang, S Zhang, S Wang, S Agarwal, S Sajuyigbe, S Chintala, S Max, S Chen, S Kehoe, S Satterfield, S Govindaprasad, S Gupta, S Cho, S Virk, S Subramanian, S Choudhury, S Goldman, S Remez, T Glaser, T Best, T Kohler, T Robinson, T Li, T Zhang, T Matthews, T Chou, T Shaked, T Vontimitta, V Ajayi, V Montanez, V Mohan, V Kumar, V S Mangla, V Ionescu, V Poenaru, V Mihailescu, V T Ivanov, V Li, W Wang, W Jiang, W Bouaziz, W Constable, W Tang, X Wang, X Wu, X Wang, X Xia, X Wu, X Gao, X Chen, Y Hu, Y Jia, Y Qi, Y Li, Y Zhang, Y Zhang, Y Adi, Y Nam, Y Yu, Wang , </p>
<p>Y Hao, Y Qian, Y He, Z Rait, Z Devito, Z Rosnbrick, Z Wen, Z Yang, Z Zhao, arXiv:2407.21783The Llama 3 Herd of Models. 2024</p>
<p>A R Fabbri, W Kryściński, B Mccann, C Xiong, R Socher, D Radev, arXiv:2007.12626SummEval: Re-evaluating Summarization Evaluation. 2021</p>
<p>M Gao, X Hu, J Ruan, X Pu, X Wan, arXiv:2402.01383LLMbased NLG Evaluation: Current Status and Challenges. 2024</p>
<p>Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations. O Golovneva, M Chen, S Poff, M Corredor, L Zettlemoyer, M Fazel-Zarandi, A Celikyilmaz, K Gopalakrishnan, B Hedayatnia, Q Chen, A Gottardi, S Kwatra, A Venkatesh, R Gabriel, D Hakkani-Tür, arXiv:2212.07919ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. 2023. 2019. 2019Proc. Interspeech</p>
<p>Exploring ChatGPT's Ability to Rank Content: A. Y Ji, Y Gong, Y Peng, C Ni, P Sun, D Pan, B Ma, X Li, arXiv:2303.07610Preliminary Study on Consistency with Human Preferences. 2023</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D De Las Casas, F Bressand, G Lengyel, G Lample, L Saulnier, L R Lavaud, M.-A Lachaux, P Stock, T L Scao, T Lavril, T Wang, T Lacroix, W E Sayed, S Kim, J Shin, Y Cho, J Jang, S Longpre, H Lee, S Yun, S Shin, S Kim, J Thorne, arXiv:2310.06825arXiv:2302.14520Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models. 2023. 2023. 2024arXiv preprintPrometheus: Inducing Fine-grained Evaluation Capability in Language Models. and Federmann, C. 2023. Large Language Models Are State-of-the-Art Evaluators of Translation Quality</p>
<p>J Li, S Sun, W Yuan, R.-Z Fan, H Zhao, P Liu, arXiv:2310.05470Generative Judge for Evaluating Alignment. 2023</p>
<p>Z Li, X Xu, T Shen, C Xu, J.-C Gu, C Tao, arXiv:2401.07103Leveraging Large Language Models for NLG Evaluation: A Survey. 2024</p>
<p>A technique for the measurement of attitudes. Archives of psychology. R Likert, Z Lin, Z Gou, T Liang, R Luo, H Liu, Y Yang, arXiv:2402.14809CriticBench: Benchmarking LLMs for Critique-Correct Reasoning. 1932. 2024</p>
<p>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization. Y Liu, A R Fabbri, J Chen, Y Zhao, S Han, S Joty, P Liu, D Radev, C.-S Wu, A Cohan, arXiv:2311.091842023a</p>
<p>Y Liu, D Iter, Y Xu, S Wang, R Xu, C 2023b Zhu, G-Eval, arXiv:2303.16634NLG Evaluation using GPT-4 with Better Human Alignment. </p>
<p>Y Liu, N S Moosavi, C Lin, arXiv:2311.09766LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores. 2024</p>
<p>Y Liu, T Yang, S Huang, Z Zhang, H Huang, F Wei, W Deng, F Sun, Q Zhang, arXiv:2309.13308Calibrating LLM-Based Evaluator. 2023c</p>
<p>Y Liu, H Zhou, Z Guo, E Shareghi, I Vulić, A Korhonen, N Collier, arXiv:2403.16950Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators. 2024</p>
<p>N Mcaleese, R M Pokorny, J F C Uribe, E Nitishinskaya, M Trebacz, J Leike, arXiv:2407.00215LLM Critics Help Catch LLM Bugs. OpenAI2024</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, R Avila, I Babuschkin, S Balaji, V Balcom, P Baltescu, H Bao, M Bavarian, J Belgum, I Bello, J Berdine, G Bernadett-Shapiro, C Berner, L Bogdonoff, O Boiko, M Boyd, A.-L Brakman, G Brockman, T Brooks, M Brundage, K Button, T Cai, R Campbell, A Cann, B Carey, C Carlson, R Carmichael, B Chan, C Chang, F Chantzis, D Chen, S Chen, R Chen, J Chen, M Chen, B Chess, C Cho, C Chu, H W Chung, D Cummings, J Currier, Y Dai, C Decareaux, T Degry, N Deutsch, D Deville, A Dhar, D Dohan, S Dowling, S Dunning, A Ecoffet, A Eleti, T Eloundou, D Farhi, L Fedus, N Felix, S P Fishman, J Forte, I Fulford, L Gao, E Georges, C Gibson, V Goel, T Gogineni, G Goh, R Gontijo-Lopes, J Gordon, M Grafstein, S Gray, R Greene, J Gross, S S Gu, Y Guo, C Hallacy, J Han, J Harris, Y He, M Heaton, J Heidecke, C Hesse, A Hickey, W Hickey, P Hoeschele, B Houghton, K Hsu, S Hu, X Hu, J Huizinga, S Jain, S Jain, J Jang, Jiang, A.; Jiang, R.; Jin, H.; Jin, D.; Jomoto, S.; Jonn, B.; Jun, H.; Kaftan, T.; Łukasz Kaiser</p>
<p>A Kamali, I Kanitscheider, N S Keskar, T Khan, L Kilpatrick, J W Kim, C Kim, Y Kim, J H Kirchner, J Kiros, M Knight, D Kokotajlo, A Łukasz Kondraciuk; Kondrich, A Konstantinidis, K Kosic, G Krueger, V Kuo, M Lampe, I Lan, T Lee, J Leike, J Leung, D Levy, C M Li, R Lim, M Lin, S Lin, M Litwin, T Lopez, R Lowe, P Lue, A Makanju, K Malfacini, S Manning, T Markov, Y Markovski, B Martin, K Mayer, A Mayne, B Mcgrew, S M Mckinney, C Mcleavey, P Mcmillan, J Mcneil, D Medina, A Mehta, J Menick, L Metz, A Mishchenko, P Mishkin, V Monaco, E Morikawa, D Mossing, T Mu, M Murati, O Murk, D Mély, A Nair, R Nakano, R Nayak, A Neelakantan, R Ngo, H Noh, L Ouyang, C O'keefe, J Pachocki, A Paino, J Palermo, A Pantuliano, G Parascandolo, J Parish, E Parparita, A Passos, M Pavlov, A Peng, A Perelman, F De Avila Belbute Peres, M Petrov, H P De Oliveira Pinto, M Michael; Pokorny; Pokrass, V H Pong, T Powell, A Power, B Power, E Proehl, R Puri, A Radford, J Rae, A Ramesh, C Raymond, F Real, K Rimbach, C Ross, B Rotsted, H Roussez, N Ryder, M Saltarelli, T Sanders, S Santurkar, G Sastry, H Schmidt, D Schnurr, J Schulman, D Selsam, K Sheppard, T Sherbakov, J Shieh, S Shoker, P Shyam, S Sidor, E Sigler, M Simens, J Sitkin, K Slama, I Sohl, B Sokolowsky, Y Song, N Staudacher, F P Such, N Summers, I Sutskever, J Tang, N Tezak, M B Thompson, P Tillet, A Tootoonchian, E Tseng, P Tuggle, N Turley, J Tworek, J F C Uribe, A Vallone, A Vijayvergiya, C Voss, C Wainwright, J J Wang, A Wang, B Wang, J Ward, J Wei, C Weinmann, A Welihinda, P Welinder, J Weng, L Weng, M Wiethoff, D Willner, C Winter, S Wolrich, H Wong, L Workman, S Wu, J Wu, M Wu, K Xiao, T Xu, S Yoo, K Yu, Q Yuan, W Zaremba, R Zellers, C Zhang, M Zhang, S Zhao, T Zheng, J Zhuang, W Zhuk, B Zoph, Y Shen, X Wan, arXiv:2303.08774arXiv:2402.11683OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. Bowman, S. R.; and Feng2024. 2023Technical ReportLLM Evaluators Recognize and Favor Their Own Generations. Nath, S.; Muddu, S. S. R. R.; Rangaraju, R.; Nath, S.; Bhattacharyya, P.; Banerjee, S.; Patil, A.; Singh, S. S.; Chelliah, M.; and Garera, N. 2024. One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation</p>
<p>Large Language Models are Inconsistent and Biased Evaluators. R Stureborg, D Alikaniotis, Y Suhara, arXiv:2405.017242024</p>
<p>LLaMA: Open and Efficient Foundation Language Models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, T Wang, P Yu, X E Tan, S O'brien, R Pasunuru, J Dwivedi-Yu, O Golovneva, L Zettlemoyer, M Fazel-Zarandi, A Celikyilmaz, Z Xie, T Cohn, J H Lau, S Ye, D Kim, S Kim, H Hwang, S Kim, Y Jo, J Thorne, J Kim, arXiv:2302.13971arXiv:2307.10928The Next Chapter: A Study of Large Language Models in Storytelling. 2023. 2023. 2023Shepherd: A Critic for Language Model Generation. and Seo, M. 2024. FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets</p>
<p>Judging LLM-as-ajudge with MT-bench and Chatbot Arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2024</p>            </div>
        </div>

    </div>
</body>
</html>
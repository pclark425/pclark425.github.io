<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7525 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7525</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7525</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc" target="_blank">Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the Code Usage Frequency of GPT-4 Code Interpreter is explored, and a novel and effective prompting method, explicit \uline{c}ode-based \ULine{s}elf-\uline {v}erification~(CSV), is proposed to further boost the mathematical reasoning potential of GPN.</p>
                <p><strong>Paper Abstract:</strong> Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \uline{c}ode-based \uline{s}elf-\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \textbf{(53.9\% $\to$ 84.3\%)}.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7525.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7525.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4-Code (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Code Interpreter (GPT4-Code)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool-enabled variant of GPT-4 that can generate and execute Python code during reasoning; used in this paper as a text-based simulator to solve challenging math word problems by interleaving natural-language reasoning and executable code, and by using code to self-verify and refine solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Code Interpreter (GPT4-Code)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Instruction-tuned GPT-4 variant with an integrated code-execution tool (code-interpreter); closed-source / API-accessed</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (math word problems: intermediate algebra, precalculus, geometry, number theory, counting & probability, prealgebra, algebra)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve math word problems by producing step-by-step natural language reasoning and executable Python code, executing that code to compute intermediate results and verify the final answer (i.e., simulate the mathematical solution process via text and code).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot prompts with variants controlling code usage: Prompt 1 (no code), Prompt 2 (single code block, PAL-like), Basic Prompt (no constraint => multiple short code steps), Explicit Code-based Self-Verification (CSV) zero-shot prompt that asks the model to verify answers via code; sampling multiple reasoning paths and applying verification-guided weighted majority voting (self-consistency style) with verification states {True, Uncertain, False} and weights w_T > w_U > w_F; k=16 sampled paths reported for the main MATH voting run.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match answer accuracy (%) on MATH benchmark; additional verification evaluation via confusion matrix, precision and recall for the verification classifier (treating 'True' as positive).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>MATH overall exact-match: GPT4-Code zero-shot 69.69%; GPT4-Code + CSV 73.54%; GPT4-Code + CSV + verification-guided weighted majority voting (k=16) 84.32%. (Per-subject figures provided in paper, e.g., Number Theory 77.2% -> 94.1% with CSV+Voting.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Baselines reported in paper: GPT-4 (no code) 42.20% overall; GPT-3.5 34.12%; GPT-4 (Chain-of-Thought) 50.36%; GPT-4 (PHP / progressive-hint prompting) 53.90%. The base GPT4-Code (no CSV/voting) used as the experimental baseline: 69.69%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Code Usage Frequency (number of code executions per solution) — positively correlated with accuracy', 'Self-debugging enabled by incremental code execution and correction after runtime or logic errors', 'Prompt constraint on code usage (no-code, single-code, unrestricted multi-step code)', 'Explicit code-based verification vs natural-language verification (code-based verification performs better)', 'Number of sampled reasoning paths (k) for majority/self-consistency voting', 'Verification state weighting (w_T, w_U, w_F) in verification-guided weighted majority voting', 'Problem difficulty level (harder problems show larger gains from code use)', 'Problem subject/modality (e.g., geometry requires multi-modality; gains smaller)', 'Python library usage (e.g., heavy use of sympy) correlates with computation styles']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Zero-shot prompts; CSV prompt text requests 'solve using code interpreter step by step, and please verify your answer using code interpreter.' For MATH voting run: k=16 sampled paths; verification states classified as True/False/Uncertain; voting weights set with w_T > w_U > w_F (exact numeric settings tuned in ablation). Temperature / sampling temperature not specified in paper; execution environment: OpenAI GPT-4 Code Interpreter (with Python packages available such as sympy, numpy, math).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Analysis focused on a single closed-source model (GPT4-Code); geometry shows small improvement (likely because of multimodal requirements); natural-language self-verification can degrade performance relative to code-based verification; verification may produce False or Uncertain states (limiting how many solutions are 'verified True'); method depends on an executable code environment and is not yet evaluated broadly on other LLMs or model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7525.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7525.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4-Code (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Code Interpreter (GPT4-Code)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same GPT-4 Code Interpreter used as a text-based simulator to solve grade-school arithmetic word problems (GSM8K) by generating and executing code and applying code-based self-verification plus weighted voting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Code Interpreter (GPT4-Code)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Instruction-tuned, tool-enabled code interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (primary-school/grade-school arithmetic and reasoning problems)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Compute numeric solutions to arithmetic word problems by producing executable Python code for calculations and verifying results with CSV, optionally aggregating multiple sampled solutions via verification-guided weighted majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot CSV prompting combined with sampling k reasoning paths and verification-guided weighted majority voting; reported GSM8K voting run used k=5.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match answer accuracy (%) on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>GSM8K: GPT4-Code 92.9% (single-run); GPT4-Code + CSV + Voting (k=5) 97.0% (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Baselines shown for comparison: GPT-3.5 (5-shot) 57.1%; GPT-4 (5-shot CoT) 92.0%; GPT-4 (PHP) 96.5%; GPT-4 (Model selection) 96.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['CSV (code-based verification) improves correctness over base GPT4-Code', 'Number of sampled paths (k) — voting reduces required k vs other methods', 'Code Usage Frequency (more frequent short code steps improves reliability for computation-heavy problems)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>CSV + Voting with k=5 sampled paths for GSM8K; zero-shot CSV prompt; execution using GPT-4 Code Interpreter and available Python libraries; numeric hyperparameters of voting weights tuned in ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Dependence on code-execution environment; paper reports strong performance on GSM8K but comparisons include few-shot/5-shot baselines so direct protocol differences should be considered.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7525.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7525.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4-Code (MMLU-Math/STEM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Code Interpreter (GPT4-Code)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applied GPT4-Code as a text-based simulator for higher-level academic math and STEM multiple-choice/probing tasks (MMLU math and STEM subsets), using CSV and verification-guided voting to improve zero-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Code Interpreter (GPT4-Code)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Instruction-tuned, tool-enabled code interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (MMLU-Math) and broader STEM (MMLU-STEM categories: physics, engineering-adjacent topics, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Answer domain knowledge and reasoning questions in math/STEM by generating reasoning and executable code to compute/verify answers, with CSV-based verification and voting for final answer selection.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot CSV prompting; sampling multiple reasoning paths and applying verification-guided weighted majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Multiple-choice accuracy (%) on MMLU benchmarks (Math subset and STEM aggregate).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>MMLU-Math: GPT4-Code zero-shot 87.5%; GPT4-Code + CSV + Voting 89.2%. MMLU-STEM: GPT4-Code 86.8%; GPT4-Code + CSV + Voting 87.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Baselines listed: Chinchilla (5-shot) 35.7% on MMLU-Math, Galactica (5-shot) 41.3%; open-source baselines on STEM: LLaMA 2 (5-shot) 58.0%, OpenLLM 70.6%; GPT-4 (zero-shot) for STEM: 82.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['CSV and verification-guided voting provide modest gains over base GPT4-Code for MMLU tasks', 'Domain-specific difficulty and breadth (some MMLU subdomains like virology and certain engineering/humanities subcategories show weaker performance)', 'Code Usage Frequency and verification precision affect effectiveness']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Zero-shot CSV prompting; verification-guided weighted majority voting; reported as zero-shot evaluations; exact sampling k for MMLU runs not always specified for every reported figure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Performance varies by subdomain; paper notes notably low scores in some fields (e.g., virology < 60%) indicating domain gaps; method evaluated primarily on math/STEM subsets and may not generalize uniformly across all academic domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7525.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7525.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (baseline mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (non-code variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The (non-code-interpreter) GPT-4 model is reported in the paper as a baseline for math reasoning; it lacks integrated code execution and shows substantially lower accuracy on math benchmarks versus GPT4-Code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Instruction-tuned base language model (no integrated code-execution in the reported runs)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics (used as baseline for math word problem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Solve math word problems using natural-language chain-of-thought style reasoning without executing code.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Zero-shot / Chain-of-Thought (CoT) prompting and progressive-hint prompting (PHP) used as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match accuracy (%) on MATH and GSM8K benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>MATH: GPT-4 (no code) 42.20%; GPT-4 (CoT) 50.36%; GPT-4 (PHP) 53.90%. GSM8K: GPT-4 (5-shot CoT) 92.0% (reported in comparison table).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Absence of executable code capability limits computational precision and self-debugging', 'Prompting style (CoT, PHP) affects performance but lags behind code-executing variants']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Reported as baselines; specific sampling or temperature settings not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower arithmetic/calculation fidelity compared to code-execution approaches; struggles more on complex/high-difficulty math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PAL: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Let's verify step by step <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7525",
    "paper_id": "paper-1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "GPT4-Code (MATH)",
            "name_full": "GPT-4 Code Interpreter (GPT4-Code)",
            "brief_description": "A tool-enabled variant of GPT-4 that can generate and execute Python code during reasoning; used in this paper as a text-based simulator to solve challenging math word problems by interleaving natural-language reasoning and executable code, and by using code to self-verify and refine solutions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 Code Interpreter (GPT4-Code)",
            "model_size": null,
            "model_type": "Instruction-tuned GPT-4 variant with an integrated code-execution tool (code-interpreter); closed-source / API-accessed",
            "scientific_domain": "Mathematics (math word problems: intermediate algebra, precalculus, geometry, number theory, counting & probability, prealgebra, algebra)",
            "simulation_task_description": "Solve math word problems by producing step-by-step natural language reasoning and executable Python code, executing that code to compute intermediate results and verify the final answer (i.e., simulate the mathematical solution process via text and code).",
            "prompting_strategy": "Zero-shot prompts with variants controlling code usage: Prompt 1 (no code), Prompt 2 (single code block, PAL-like), Basic Prompt (no constraint =&gt; multiple short code steps), Explicit Code-based Self-Verification (CSV) zero-shot prompt that asks the model to verify answers via code; sampling multiple reasoning paths and applying verification-guided weighted majority voting (self-consistency style) with verification states {True, Uncertain, False} and weights w_T &gt; w_U &gt; w_F; k=16 sampled paths reported for the main MATH voting run.",
            "evaluation_metric": "Exact-match answer accuracy (%) on MATH benchmark; additional verification evaluation via confusion matrix, precision and recall for the verification classifier (treating 'True' as positive).",
            "reported_accuracy": "MATH overall exact-match: GPT4-Code zero-shot 69.69%; GPT4-Code + CSV 73.54%; GPT4-Code + CSV + verification-guided weighted majority voting (k=16) 84.32%. (Per-subject figures provided in paper, e.g., Number Theory 77.2% -&gt; 94.1% with CSV+Voting.)",
            "baseline_accuracy": "Baselines reported in paper: GPT-4 (no code) 42.20% overall; GPT-3.5 34.12%; GPT-4 (Chain-of-Thought) 50.36%; GPT-4 (PHP / progressive-hint prompting) 53.90%. The base GPT4-Code (no CSV/voting) used as the experimental baseline: 69.69%.",
            "factors_reported": [
                "Code Usage Frequency (number of code executions per solution) — positively correlated with accuracy",
                "Self-debugging enabled by incremental code execution and correction after runtime or logic errors",
                "Prompt constraint on code usage (no-code, single-code, unrestricted multi-step code)",
                "Explicit code-based verification vs natural-language verification (code-based verification performs better)",
                "Number of sampled reasoning paths (k) for majority/self-consistency voting",
                "Verification state weighting (w_T, w_U, w_F) in verification-guided weighted majority voting",
                "Problem difficulty level (harder problems show larger gains from code use)",
                "Problem subject/modality (e.g., geometry requires multi-modality; gains smaller)",
                "Python library usage (e.g., heavy use of sympy) correlates with computation styles"
            ],
            "experimental_conditions": "Zero-shot prompts; CSV prompt text requests 'solve using code interpreter step by step, and please verify your answer using code interpreter.' For MATH voting run: k=16 sampled paths; verification states classified as True/False/Uncertain; voting weights set with w_T &gt; w_U &gt; w_F (exact numeric settings tuned in ablation). Temperature / sampling temperature not specified in paper; execution environment: OpenAI GPT-4 Code Interpreter (with Python packages available such as sympy, numpy, math).",
            "limitations_or_failure_modes": "Analysis focused on a single closed-source model (GPT4-Code); geometry shows small improvement (likely because of multimodal requirements); natural-language self-verification can degrade performance relative to code-based verification; verification may produce False or Uncertain states (limiting how many solutions are 'verified True'); method depends on an executable code environment and is not yet evaluated broadly on other LLMs or model sizes.",
            "uuid": "e7525.0",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT4-Code (GSM8K)",
            "name_full": "GPT-4 Code Interpreter (GPT4-Code)",
            "brief_description": "Same GPT-4 Code Interpreter used as a text-based simulator to solve grade-school arithmetic word problems (GSM8K) by generating and executing code and applying code-based self-verification plus weighted voting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 Code Interpreter (GPT4-Code)",
            "model_size": null,
            "model_type": "Instruction-tuned, tool-enabled code interpreter",
            "scientific_domain": "Mathematics (primary-school/grade-school arithmetic and reasoning problems)",
            "simulation_task_description": "Compute numeric solutions to arithmetic word problems by producing executable Python code for calculations and verifying results with CSV, optionally aggregating multiple sampled solutions via verification-guided weighted majority voting.",
            "prompting_strategy": "Zero-shot CSV prompting combined with sampling k reasoning paths and verification-guided weighted majority voting; reported GSM8K voting run used k=5.",
            "evaluation_metric": "Exact-match answer accuracy (%) on GSM8K.",
            "reported_accuracy": "GSM8K: GPT4-Code 92.9% (single-run); GPT4-Code + CSV + Voting (k=5) 97.0% (reported).",
            "baseline_accuracy": "Baselines shown for comparison: GPT-3.5 (5-shot) 57.1%; GPT-4 (5-shot CoT) 92.0%; GPT-4 (PHP) 96.5%; GPT-4 (Model selection) 96.8%.",
            "factors_reported": [
                "CSV (code-based verification) improves correctness over base GPT4-Code",
                "Number of sampled paths (k) — voting reduces required k vs other methods",
                "Code Usage Frequency (more frequent short code steps improves reliability for computation-heavy problems)"
            ],
            "experimental_conditions": "CSV + Voting with k=5 sampled paths for GSM8K; zero-shot CSV prompt; execution using GPT-4 Code Interpreter and available Python libraries; numeric hyperparameters of voting weights tuned in ablation.",
            "limitations_or_failure_modes": "Dependence on code-execution environment; paper reports strong performance on GSM8K but comparisons include few-shot/5-shot baselines so direct protocol differences should be considered.",
            "uuid": "e7525.1",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT4-Code (MMLU-Math/STEM)",
            "name_full": "GPT-4 Code Interpreter (GPT4-Code)",
            "brief_description": "Applied GPT4-Code as a text-based simulator for higher-level academic math and STEM multiple-choice/probing tasks (MMLU math and STEM subsets), using CSV and verification-guided voting to improve zero-shot performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 Code Interpreter (GPT4-Code)",
            "model_size": null,
            "model_type": "Instruction-tuned, tool-enabled code interpreter",
            "scientific_domain": "Mathematics (MMLU-Math) and broader STEM (MMLU-STEM categories: physics, engineering-adjacent topics, etc.)",
            "simulation_task_description": "Answer domain knowledge and reasoning questions in math/STEM by generating reasoning and executable code to compute/verify answers, with CSV-based verification and voting for final answer selection.",
            "prompting_strategy": "Zero-shot CSV prompting; sampling multiple reasoning paths and applying verification-guided weighted majority voting.",
            "evaluation_metric": "Multiple-choice accuracy (%) on MMLU benchmarks (Math subset and STEM aggregate).",
            "reported_accuracy": "MMLU-Math: GPT4-Code zero-shot 87.5%; GPT4-Code + CSV + Voting 89.2%. MMLU-STEM: GPT4-Code 86.8%; GPT4-Code + CSV + Voting 87.0%.",
            "baseline_accuracy": "Baselines listed: Chinchilla (5-shot) 35.7% on MMLU-Math, Galactica (5-shot) 41.3%; open-source baselines on STEM: LLaMA 2 (5-shot) 58.0%, OpenLLM 70.6%; GPT-4 (zero-shot) for STEM: 82.7%.",
            "factors_reported": [
                "CSV and verification-guided voting provide modest gains over base GPT4-Code for MMLU tasks",
                "Domain-specific difficulty and breadth (some MMLU subdomains like virology and certain engineering/humanities subcategories show weaker performance)",
                "Code Usage Frequency and verification precision affect effectiveness"
            ],
            "experimental_conditions": "Zero-shot CSV prompting; verification-guided weighted majority voting; reported as zero-shot evaluations; exact sampling k for MMLU runs not always specified for every reported figure.",
            "limitations_or_failure_modes": "Performance varies by subdomain; paper notes notably low scores in some fields (e.g., virology &lt; 60%) indicating domain gaps; method evaluated primarily on math/STEM subsets and may not generalize uniformly across all academic domains.",
            "uuid": "e7525.2",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-4 (baseline mentions)",
            "name_full": "GPT-4 (non-code variant)",
            "brief_description": "The (non-code-interpreter) GPT-4 model is reported in the paper as a baseline for math reasoning; it lacks integrated code execution and shows substantially lower accuracy on math benchmarks versus GPT4-Code.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_type": "Instruction-tuned base language model (no integrated code-execution in the reported runs)",
            "scientific_domain": "Mathematics (used as baseline for math word problem solving)",
            "simulation_task_description": "Solve math word problems using natural-language chain-of-thought style reasoning without executing code.",
            "prompting_strategy": "Zero-shot / Chain-of-Thought (CoT) prompting and progressive-hint prompting (PHP) used as baselines.",
            "evaluation_metric": "Exact-match accuracy (%) on MATH and GSM8K benchmarks.",
            "reported_accuracy": "MATH: GPT-4 (no code) 42.20%; GPT-4 (CoT) 50.36%; GPT-4 (PHP) 53.90%. GSM8K: GPT-4 (5-shot CoT) 92.0% (reported in comparison table).",
            "baseline_accuracy": null,
            "factors_reported": [
                "Absence of executable code capability limits computational precision and self-debugging",
                "Prompting style (CoT, PHP) affects performance but lags behind code-executing variants"
            ],
            "experimental_conditions": "Reported as baselines; specific sampling or temperature settings not provided in the paper.",
            "limitations_or_failure_modes": "Lower arithmetic/calculation fidelity compared to code-execution approaches; struggles more on complex/high-difficulty math problems.",
            "uuid": "e7525.3",
            "source_info": {
                "paper_title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 2
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1
        }
    ],
    "cost": 0.01377925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Solving Challenging Math Word Problems Using GPT-4 CODe Interpreter With Code-Based Self-Verification</h1>
<p>Aojun Zhou ${ }^{1^{<em>}}$ Ke Wang ${ }^{2^{</em>}}$ Zimu Lu ${ }^{3^{<em>}}$ Weikang Shi ${ }^{4^{</em>}}$ Sichun Luo ${ }^{5^{<em>}}$ Zipeng Qin ${ }^{1}$<br>Shaoqing Lu ${ }^{6}$ Anya Jia ${ }^{7}$ Linqi Song ${ }^{5}$ Mingjie Zhan ${ }^{1^{</em>}}$ Hongsheng Li ${ }^{1^{3}}$<br>${ }^{1}$ Multimedia Laboratory (MMLab), The Chinese University of Hong Kong<br>${ }^{2}$ Nanjing University ${ }^{3}$ University of Science and Technology of China<br>${ }^{4}$ Tsinghua University ${ }^{5}$ City University of Hong Kong<br>${ }^{6}$ Changsha University of Science and Technology ${ }^{7}$ Tufts University<br>{aojunzhou, wangk.gm, sichunluo2, zmjdll}@gmail.com<br>luzimu@mail.ustc.edu.cn shiwk20@mails.tsinghua.edu.cn<br>linqi.song@cityu.edu.hk hsli@ee.cuhk.edu.hk</p>
<h4>Abstract</h4>
<p>Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the Code Usage Frequency of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit code-based self-verification (CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as "False", the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset ( $\mathbf{5 3 . 9 \%} \rightarrow \mathbf{8 4 . 3 \%}$ ).</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) (Brown et al., 2020; OpenAI, 2023; Anil et al., 2023) have shown impressive success in various tasks, such as common sense understanding and code generation. However, they still fall short in mathematical reasoning, often producing nonsensical or inaccurate content and struggling with complex calculations. Previous attempts to tackle these challenges include the Chain-of-Thought (CoT) (Wei et al., 2022) framework, which enhances LLMs' logical reasoning abilities by generating intermediate steps in their reasoning process. Additionally, PAL (Gao et al., 2023) introduces a novel approach by using the Python programming interpreter to improve computational accuracy.
In recent advancements, OpenAI has unveiled an improved version of GPT-4, namely the GPT-4 Code Interpreter ${ }^{12}$ or GPT4-Code, which is proficient at providing logical natural language reason-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ing, alongside step-by-step Python code. Notably, it can generate and execute code incrementally, and subsequently present the executed code's output back to the LLM. The addition of code generation and execution to natural language outputs has shown promising results in solving mathematical reasoning problems. Our initial experiments show that GPT4-Code achieved an impressive zero-shot accuracy of $69.7 \%$ on the challenging MATH dataset (Hendrycks et al., 2021), marking a significant improvement of $27.5 \%$ over GPT-4's performance ( $42.2 \%$ ).</p>
<p>While GPT4-Code has demonstrated proficiency in solving math problems, there has been a notable absence of systematic analysis focusing on understanding and further enhancing its mathematical problem-solving abilities. A key distinction between GPT4-Code and its predecessor, GPT-4, lies in GPT4-Code's ability to automatically generate and execute code. Therefore, this paper presents pilot experiments that investigate GPT4-Code's code generation and execution mechanism using specific code-constrained prompts. The analysis reveals that GPT4-Code's strong performance is not solely due to its code generation and execution abilities, but also its capacity to adjust its problemsolving strategies based on feedback from code execution-a process we term self-debugging (illustrated in Tab. 7 and Tab. 8). Due to the fact that code generation evolves its reasoning step-by-step and performs self-debugging after code execution errors, there is an increased frequency of code usage. Hence, we introduce the concept of Code Usage Frequency to differentiate these unique prompting strategies to quantitatively analyze the impact of code-constrained prompts on GPT4Code for mathematical problem-solving.</p>
<p>The step-by-step code generation and self-debugging mechanisms highlight the critical role of code in mathematical problem-solving. Nevertheless, the self-debugging mechanism only verifies each step of the generated code while lacks the verification of the reasoning steps and the final answer, which has been demonstrated to be of vital importance to the math problem-solving abilities of LLMs (Cobbe et al., 2021; Lightman et al., 2023; Weng et al., 2023).</p>
<p>We therefore ask the question: can we fully exploit the code generation and self-debugging mechanisms in GPT4-code, so that it can automatically verify and correct its solutions, without extra assistance from other models or users?</p>
<p>To answer this question, we propose a simple yet effective prompting technique termed the explicit code-based self-verification (CSV), which guides GPT4-Code to generate additional code that verifies the answer and adjusts the reasoning steps if there's a flaw in reasoning. Unlike previous methods that rely on external language models for verification (Lightman et al., 2023; Cobbe et al., 2021), our approach leverages GPT4-Code's inherent strengths. This approach offers two key benefits: (1) When the verification indicates an answer is False, GPT4-Code can rectify its prior solution and provide an improved alternative. (2) Solutions verified as True tend to be more reliable, akin to human problem-solving. However, even if a solution is self-verified as False, we do not directly abandon it. Instead, we propose a weighted majority voting strategy that incorporates the code-based solution verification results, as opposed to relying exclusively on the frequency of answers. We assign different weights to the solutions according to their verification states, reflecting the solutions' varying levels of reliability. In alignment with the Code Usage Frequency analysis from our pilot experiments, our explicit code-based self-verification prompt boosts GPT4-Code's accuracy in mathematical problem-solving with increased code usage.</p>
<p>Empirical study demonstrates the effectiveness of our proposed framework on the MATH, GSM8K, and MMLU-Math datasets using GPT4-Code. Our approach achieves an impressive accuracy of $\mathbf{8 4 . 3 2 \%}$ on the MATH dataset, greatly outperforming the base GPT4-Code and previous state-of-the-art methods. Additionally, we are making our experimental data on the MMLU-Math and MATH datasets publicly available, enabling result replication and facilitating fine-tuning of open-source LLM models (e.g., LLaMA 2 (Touvron et al., 2023)) to further enhance mathematical problemsolving capabilities with the assistance of code generation.</p>
<p>This paper's main contributions can be summarized in three key aspects:</p>
<ul>
<li>This study provides the first systematic analysis of code generation, execution, and selfdebugging's role in mathematical problem-solving. Our findings reveal that GPT4-Code's impressive mathematical problem-solving proficiency is primarily attributed to its step-bystep code generation and dynamic solution refinement based on code execution outcomes.</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example from the 277th number theory problem in the MATH dataset: (a) Prompt 1 generates intermediate text-only reasoning steps. (b) Prompt 2 generates intermediate text reasoning steps and post-hoc Python code, akin to PAL (Gao et al., 2023). (c) Basic prompt that set no constrain on code usage. Details of the three prompts are presented in Sec. 3.1. (For more examples, see Tab. 9 and Tab. 10.)</p>
<ul>
<li>We introduce the innovative explicit code-based self-verification (CSV) prompt, which leverages GPT4-Code's advanced code generation mechanism. This prompt guides the model to verify the answer and then reevaluate its solution with code. CSV not only extends the verification to the logic behind problem-solving but also improves the efficacy of the majority voting method by integrating the verification states.</li>
<li>Additionally, we have contributed to the LLM community by creating two new instructionfollowing datasets: MATH-code and MMLU-Math-code. These datasets are designed to enhance the mathematical reasoning capabilities of open-source models.</li>
</ul>
<h1>2 RELATED WORK</h1>
<p>Chain-of-Thought Reasoning. The Chain-of-Thought (CoT) prompting approach proposed by Wei et al. (2022) is a notable contribution that showcases the multi-step reasoning capabilities of LLMs. By simply adding "Let's think step by step" before questions, Kojima et al. (2022) implements Zero-shot-CoT, which can serve as a strong zero-shot baseline. Further research extends the reasoning</p>
<p>capabilities of CoT by applying majority voting to improve self-consistency (Wang et al., 2023), choosing few-shot examples and output chains with more complex reasoning steps (Fu et al., 2022), breaking down the problem to simpler sub-problems (Zhou et al., 2023), or even expanding Chain-of-thought to Tree-of-Thoughts (Yao et al., 2023). Similar to Zero-shot-CoT, our method apply "step by step"-like prompts to regularize GPT4-Code's use of code without the careful design of step-bystep few-shot examples. Additionally, We enhance majority voting to verification-guided weighted majority voting, leveraging the results of CSV as voting weights.
Solving Math Problems with Code. Large language models have been found to be less accurate in performing arithmetic calculations, such as addition, subtraction, multiplication, etc (Cobbe et al., 2021; Lewkowycz et al., 2022; Gao et al., 2023; Lu et al., 2022). Consequently, previous works have attempted to solve math problems with the assistance of code. The GSM8K dataset (Cobbe et al., 2021) uses calculation annotations to extract all arithmetic calculations solved by an external calculator: the Python eval function. To further leverage the role of code in LLMs, Program-Aided Language model (PAL) (Gao et al., 2023) as well as Program of Thoughts (PoT) (Chen et al., 2022) interpret the math problems as Python codes and execute the codes with an external Python interpreter to obtain the answer. Although they can get more accurate answers than some non-code methods, many generated codes have execution errors or get wrong answers due to the lack of verification mechanism. Our approach not only utilizes the ability of GPT4-Code to generate multi-step codes and refine codes that fail to run, but also uses CSV to enhance the reliability and accuracy of the answers.</p>
<p>Self-Verification. Human problem solving is not always a one-time success, but rather requires iterative thinking, verification, and refinement. Unlike previous studies that train an additional verifier to verify the correctness of final answers (Cobbe et al., 2021) or intermediate steps (Lightman et al., 2023; Li et al., 2023), Weng et al. (2023) showed the self-verification abilities of LLMs by generating multiple answers and ranking them by self-verification scores. Furthermore, SELF-REFINE proposed by Madaan et al. (2023) iteratively refines its output through self-generated feedback. Unlike these self-verification methods that require LLMs to give verification feedback in natural language, our method applies generated codes to verify the answers and votes on different answers based on the verification results, thus improving the accuracy of the verification and making full use of the information in the verification process.</p>
<h1>3 Method</h1>
<p>We first conduct a pilot experiment with GPT4-Code on the challenging MATH dataset (Hendrycks et al., 2021). Remarkably, it achieves an accuracy of $69.7 \%$, significantly surpassing the previous state-of-the-art performance of $53.9 \%$ (Zheng et al., 2023). Encouraged by the compelling performance of GPT4-Code, we strive to systematically explore and analyze its underlying code mechanisms. In Sec. 3.1, we illustrate, via our code-constrained prompts design, that GPT4-Code's robust performance in solving math problems derives not only from its ability to generate accurate step-by-step code, but also from its self-debugging mechanism. In Sec. 3.2, we aim to leverage GPT4-Code's self-debugging strengths to further improve its mathematical problem-solving ability.</p>
<h3>3.1 Pilot Experiments on analyzing Code Usage of GPT4-Code</h3>
<p>To explore the impact of code usage on GPT4-Code's math problem-solving capabilities, we adopt a straightforward approach by constraining GPT4-Code's interaction with code through thoughtfully constructed prompts. Specifically, we introduce two code-constrained prompts and the basic prompt for comparison:</p>
<ul>
<li>Prompt 1: No code usage is allowed: In response to this prompt, GPT4-Code is prohibited from incorporating code into its solutions. This prompts GPT4-Code to rely solely on Natural Language (NL) reasoning chain, resembling solutions in the CoT framework (Wei et al., 2022). The resulting sequence of reasoning steps is depicted as $\mathbf{C}_{\mathrm{NL}}$, with an example given in Fig. 1 (a).</li>
<li>Prompt 2: Code can be used only once: In this prompt setting, GPT4-Code is permitted to employ code exclusively within a single code block to generate the solution, mirroring</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance on MATH dataset of different levels by applying different prompts to adjust the frequency of code usage. (a) Comparison of overall accuracy between the 4 prompts. (b) Code usage frequency is in proportion to accuracy in all five levels and this phenomenon is especially apparent when the problems are relatively complicated (i.e. with higher level).
the PAL approach introduced by Gao et al. (2023). We denote this sequence as $\mathbf{C}_{\mathrm{SL}}$, representing a series of Symbolic Language (SL), such as Python. An example is shown in Fig. 1 (b).</p>
<ul>
<li>Basic Prompt: GPT4-Code is prompted to tackle the problem without any restrictions on code usage. This prompt leads to GPT4-Code's typical functioning pattern, which can be denoted as $\mathbf{C}=\left(\left(\mathbf{c 1}<em _mathrm_SL="\mathrm{SL">{\mathrm{NL}}, \mathbf{c 1}</em>}}\right),\left(\mathbf{c 2<em _mathrm_SL="\mathrm{SL">{\mathrm{NL}}, \mathbf{c 2}</em>\right), \ldots\right)$, representing a sequential list of reasoning steps, each consisted of both natural language and Python code, with an example shown in Fig. 1 (c).}</li>
</ul>
<p>Apart from the specific example in Fig. 1, we introduce Code Usage Frequency to record the number of the code execution for different prompts. The results of the experiments using these prompts are shown in Fig. 2 (b). This figure illustrates a positive correlation between the better performance of GPT4-Code and the higher Code Usage Frequency. More specifically,</p>
<p>Prompt 1 v.s. Prompt 2: Prompt 1 results in almost negligible code usage, while Prompt 2 results in approximately 1 time's code usage. Prompt 2 yields an accuracy gain of 6.9 percent over Prompt 1. This suggests that the Python code chains $\mathbf{C}<em L="L" _mathbf_N="\mathbf{N">{\mathbf{S L}}$, can improve computational capability more than the natural language chains $\mathbf{C}</em>$. This observation is consistent with the findings in previous Python-based prompting methods (Gao et al., 2023; Chen et al., 2022). However, employing code only once comes with an inherent drawback - the model lacks the ability to self-debugging when the code output triggers an error or produces an implausible outcome.}</p>
<p>Prompt 2 v.s. Basic Prompt: The Basic Prompt consistently produces solutions that entail multiple instances of code usage, resulting in a large Code Usage Frequency. Additionally, the Basic Prompt exhibits notably enhanced accuracy. These improvements in Code Usage Frequency and accuracy might be attributable to two unique advantages: (1) Generating code in brief and frequent segments, divided among natural language reasoning steps, tends to result in higher accuracy. (2) The model possesses the capability to evaluate the results of code execution and make corrections to solution steps if the outcomes contain bugs or are deemed illogical, as illustrated in Tab. 7 and Tab. 8.</p>
<p>From these observations, it is plausible to enhance and build upon the favorable attributes of GPT4Code, to further improve its precision in tackling math problems.</p>
<h1>3.2 Explicit Code-based Self-Verification Prompting</h1>
<p>Inspired by the observations on Code Usage Frequency analysis, we seek to harness the capabilities of GPT4-Code. These capabilities include the model's aptitude for generating accurate code, evaluating the outcomes of code execution, and automatically adjusting reasoning steps of solutions when</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Question from the 712th intermediate algebra problem in the MATH dataset. (a) Without selfverification, the model generates a wrong answer. (b) With self-verification, the model corrects the error and generates the correct answer. The CSV prompt: To solve the problem using code interpreter step by step, and please verify your answer using code interpreter.
needed. However, despite these advantages, GPT4-Code currently falls short in assuring solution correctness. Consequently, our objective is to utilize these strengths to augment solution verification.</p>
<p>To achieve this objective, we propose the technique termed as explicit code-based self-verification (CSV). This method prompts GPT4-Code to explicitly validate its answer through code generation. By implementing this prompt, we introduce an extra verification stage to the solution $\mathbf{C}$, referred to as $\mathbf{V}$. The verification outcome $\mathbf{V}$ can be classified as True, False, or Uncertain. An Uncertain classification indicates that GPT4-Code encountered difficulties in identifying an effective method for answer verification, thereby abstaining from delivering a definitive verification result. Leveraging GPT4-Code's inherent autonomous capabilities, we can formulate the proposed prompting as follows:</p>
<p>$$
\mathbf{C} \rightarrow \mathbf{V}=\left{\begin{array}{ll}
\text { True } &amp; \rightarrow \text { final answer } \
\text { False } &amp; \rightarrow \mathbf{C}_{\text {new }} \rightarrow \mathbf{V} \rightarrow \cdots \rightarrow \text { True } \rightarrow \text { final answer } \
\text { Uncertain } &amp; \rightarrow \text { final answer }
\end{array}\right.
$$</p>
<p>An example is presented in Fig. 3 (b). Incorporated with CSV, the model becomes capable of using code to verify answers, then reviewing and adjusting how it arrived at the solution if the verification result is False, aiming at obtaining the correct answer. Upon refining and correcting the initial solution, we anticipate a notable increase in accuracy. It is worth noting that both the verification and rectification stages are code-based. This inevitably results in increased Code Usage Frequency, akin to the aforementioned analysis, which will be further demonstrated in subsequent experiments.</p>
<p>We perform experiments with CSV, and these results can be found in Fig. 2. The experiment here is conducted with GPT4-Code on MATH (Hendrycks et al., 2021). In Fig. 2 (b), the accuracy achieved with our proposed CSV prompt consistently surpasses that of the Basic Prompt across all designated difficulty levels ${ }^{3}$. Meanwhile, the Code Usage Frequency receives a clear increase.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Before the advent of GPT4-Code, prior frameworks (Lightman et al., 2023; Cobbe et al., 2021) depended on an external LLM to use natural language for verification and well-designed few-shot example prompts. In contrast, our approach simplifies the process by relying solely on a straightforward prompt for GPT4-Code, all in a zero-shot manner. This enables GPT4-Code to autonomously verify and independently rectify its solutions using the advanced code execution mechanism, thereby eliminating the need for customized few-shot examples.</p>
<p>Given that CSV can effectively verify problem-solving answers, we can naturally integrate the verification states into majority voting, akin to the methodology embraced in self-consistency CoT (Wang et al., 2023). Answers deemed True through verification are generally more trustworthy, reflecting the problem-solving approach seen in human cognition (Newell \&amp; Simon, 1972; Wang \&amp; Chiew, 2010). This improved reliability can be leveraged in the widely-used majority voting process. To exploit this insight, we introduce verification-guided weighted majority voting, which assigns different weights to the states of the verification process.</p>
<p>In practice, it sometimes occurs that once an answer is confirmed as False, no additional verification is conducted, yielding a False verification state. We allocate corresponding weights these states of True, Uncertain, False: $w_{\mathrm{T}}, w_{\mathrm{U}}$, and $w_{\mathrm{F}}$, respectively.</p>
<p>Similar to the Self-consistency with CoT (CoT-SC) (Wang et al., 2023) in Fig. 4 (a)(ii), our framework can sample $k$ paths. For simplicity, we extract pairs of final answers and their corresponding verification results from $k$ solutions, represented as $\left(v^{i}, a^{i}\right), i=1,2, \ldots, k$, where $v^{i}$ and $a^{i}$ denote the $i$-th final answer and final verification result, respectively.</p>
<p>So the voting score for each candidate answer $a$ can be expressed as:</p>
<p>$$
\operatorname{Score}(a)=\sum_{\left{v^{i}\right}} w_{v}\left(#\left{i \mid a^{i}=a \text { and } v^{i}=v\right}\right), \quad v \in{\text { True, Uncertain, False }}
$$</p>
<p>Here, $a$ represents a candidate answer, $v$ denotes the state of verification, and $w_{v}$ is an element from the set $\left{w_{\mathrm{T}}, w_{\mathrm{U}}, w_{\mathrm{F}}\right}$. Each $w_{v}$ signifies the degree of confidence associated with its corresponding verification state.</p>
<p>Finally, we select the answer with the highest score from all candidate answers,</p>
<p>$$
\text { Output }=\underset{a}{\arg \max } \operatorname{Score}(a)
$$</p>
<p>where $\operatorname{Score}(a)$ refers to the score of answer $a$ according to Eq. 1.
It should be noted that when $w_{v}=1$ for all $w_{v} \in\left{w_{\mathrm{T}}, w_{\mathrm{U}}, w_{\mathrm{F}}\right}$, Eq. 1 becomes equivalent to the naive majority voting employed in Self-Consistency with CoT (CoT-SC) (Wang et al., 2023). Typically, we set $w_{\mathrm{T}}&gt;w_{\mathrm{U}}&gt;w_{\mathrm{F}}$, which means that an answer verified true has a greater confidence than the one with an uncertain state of verification, while an answer verified false has the lowest degree of confidence. An example of the calculation process within verification-guided weighted majority voting is illustrated in Fig. 4.</p>
<h1>4 EXPERIMENTS</h1>
<h3>4.1 PERFORMANCE ON MATH</h3>
<p>The MATH dataset (Hendrycks et al., 2021) is recognized as the most challenging math word problem dataset, as also highlighted by Chen et al. (Chen et al., 2023). Most of our experiments and the corresponding analyses are performed on the MATH benchmark. Tab. 1 compares the performance of the GPT4-Code against other models. GPT4-Code reaches 69.69\% on MATH (Hendrycks et al., 2020), largely surpassing the previous state of art result ( $53.90 \%$ ), which shows that GPT4-Code exhibits strong abilities in solving math problems and is used as our baseline for ablation study. On top of GPT4-Code, our method further improves its accuracy, raising the result to $73.54 \%$ after adding explicit code-based self-verification, and $84.32 \%$ after adding both explicit code-based selfverification and verification-guided weighted majority voting (the number of sampled paths is 16).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) Illustration of the Naive majority voting (Wang et al., 2023) and our Verification-guided weighted majority voting. The full pipeline of the proposed Verification-guided Weighted Majority Voting framework. We use the model to generate several different solutions. Then we detect the self-verification state of each solution, and classify them into three states: True, Uncertain, and False. According to the state of the verification, we assign each solution a different weight, and use the classified result to vote the score of each possible answer.</p>
<p>Table 1: Accuracy (\%) on MATH dataset. VW-voting is the abbreviation for the verification-guided weighted majority voting. (Overall: The results across various MATH subtopics (Hendrycks et al., 2021))</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Code-based <br> Verification</th>
<th style="text-align: center;">VW- <br> Voting</th>
<th style="text-align: center;">Intermediate <br> Algebra</th>
<th style="text-align: center;">Precalculus</th>
<th style="text-align: center;">Geometry</th>
<th style="text-align: center;">Number <br> Theory</th>
<th style="text-align: center;">Counting \&amp; <br> Probability</th>
<th style="text-align: center;">PreAlgebra</th>
<th style="text-align: center;">Algebra</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.20</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">34.12</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (CoT)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">50.36</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (PHP)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">53.90</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">69.69</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code + CSV</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">73.54</td>
</tr>
<tr>
<td style="text-align: left;">Improvement</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+6.5$</td>
<td style="text-align: center;">$+2.4$</td>
<td style="text-align: center;">$+8.6$</td>
<td style="text-align: center;">$+7.6$</td>
<td style="text-align: center;">$+6.7$</td>
<td style="text-align: center;">$+8.2$</td>
<td style="text-align: center;">$+3.3$</td>
<td style="text-align: center;">$+3.85$</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code + CSV + Voting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">84.32</td>
</tr>
<tr>
<td style="text-align: left;">Improvement</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$+24.3$</td>
<td style="text-align: center;">$+16.3$</td>
<td style="text-align: center;">$+11.5$</td>
<td style="text-align: center;">$+16.9$</td>
<td style="text-align: center;">$+18.4$</td>
<td style="text-align: center;">$+5.3$</td>
<td style="text-align: center;">$+12.0$</td>
<td style="text-align: center;">$+14.63$</td>
</tr>
</tbody>
</table>
<p>Note that this astonishingly high result is based on the strong abilities of the base model GPT4-Code, and our method amplifies its good qualities of GPT4-Code, with the ability to verify solutions.</p>
<p>Note that although adding Code-based Self-verification can improve the performance of every individual subject, the extent of improvement varies from subject to subject, from $7.6 \%$ to only $0.6 \%$. In particular, the Geometry problem only has an increased accuracy of $0.6 \%$, even though the original GPT4-Code accuracy is only $54.0 \%$, which is low among the subjects. This discrepancy may be attributed to the fact that solving geometry problems often requires multi-modality (Chen et al., 2023), a concept beyond the scope of this paper.</p>
<h1>4.2 PERFORMANCE ON OTHER DATASETS</h1>
<p>In addition to the challenging MATH dataset, we have also performed our method on other reasoning datasets such as GSM8K (Cobbe et al., 2021), MMLU-Math, and MMLU-STEM (Hendrycks et al., 2020). The corresponding results can be viewed in Tab. 2 and Tab. 3. When integrated on top of GPT-</p>
<p>Table 2: Performance on GSM8K dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Sampled paths</th>
<th style="text-align: center;">Accuracy( $\%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5 (5-shot)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">57.1</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (5-shot CoT)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">92.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (PHP)</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">96.5</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (Model selection)</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">96.8</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">92.9</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code + CSV + Voting</td>
<td style="text-align: center;">$\mathbf{5}$</td>
<td style="text-align: center;">$\mathbf{9 7 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Performances on MMLU dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Accuracy( $\%$</th>
<th style="text-align: center;">Few-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chinchilla (Hoffmann et al., 2022)</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">5-shot</td>
</tr>
<tr>
<td style="text-align: left;">Galactica (Taylor et al., 2022)</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">5-shot</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">zero-shot</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code + CSV + Voting</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">$\mathbf{8 9 . 2}$</td>
<td style="text-align: center;">zero-shot</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA 2</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">5-shot</td>
</tr>
<tr>
<td style="text-align: left;">OpenLLM</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">5-shot</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">zero-shot</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">zero-shot</td>
</tr>
<tr>
<td style="text-align: left;">GPT4-Code + CSV + Voting</td>
<td style="text-align: center;">STEM</td>
<td style="text-align: center;">$\mathbf{8 7 . 0}$</td>
<td style="text-align: center;">zero-shot</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The four points on each curve correspond to results using Prompt 1, Prompt 2, Basic Prompt and Code-based Self-verification Prompt, respectively. (a) The accuracy of different levels at various code usage frequencies. (b) The accuracy of different subjects at various code usage frequencies.</p>
<p>4-code, our method outperforms other methods in the competition, achieving state-of-the-art results across all datasets. Other subjects in MMLU benchmarks are provided in Fig. 8. A comparative analysis of our results with those of previous state-of-the-art techniques and open-source models is also provided.</p>
<p>Tab. 2 illustrates that verification-guided majority voting is an effective framework to reduce the number of sampled paths, compared to GPT-4 with model selection (Zhao et al., 2023) and PHP (Zheng et al., 2023).</p>
<p>Tab. 3 presents a comparison of our model's performance with existing models (Hoffmann et al., 2022; Taylor et al., 2022) on the MMLU-Math dataset and with state-of-the-art open-sourced models ${ }^{4}$ on MMLU-STEM. The open-source models remain significantly outpaced by their closedsource counterparts. To address this gap, we have made the dataset and will make it publicly available in the near future. Our intention is to facilitate the fine-tuning of open-source LLMs. For example, the open-source model LLaMA 2 (Touvron et al., 2023) can potentially utilize this data to further bolster its math reasoning capabilities.</p>
<h1>4.3 CODE USAGE FREQUENCY OF PROPOSED PROMPTS</h1>
<p>Analogous to the approach taken in Sec. 3.1, we gather data to elucidate the correlation between accuracy and Code Usage Frequency across various dimensions - prompts (proposed CSV prompt as well as prompts used in pilot experiments), subjects, and difficulty levels. As shown in Fig. 5, the model's behavior is in accordance with our expectations when adding the code-based prompts. Each line in Fig. 5 has an obvious trend of going upwards, proving that the increase of Code Usage Frequency induces a general improvement in accuracy. The performance gain when using more code is more obvious in the higher difficulty levels, while in lower levels, the performance gain is not very prominent, as shown in Fig. 5 (a). Also, the Code Usage Frequency increases steadily with</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 4: Comparison Self-verification with/without explicit code-based prompt (Overall:The results across various MATH subtopics (Hendrycks et al., 2021))</p>
<table>
<thead>
<tr>
<th>GPT4-Code</th>
<th>Verification Method</th>
<th>Intermediate Algebra</th>
<th>Precalculus</th>
<th>Geometry</th>
<th>Number Theory</th>
<th>Counting &amp; Probability</th>
<th>PreAlgebra</th>
<th>Algebra</th>
<th>Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>without Verification</td>
<td>50.1</td>
<td>51.5</td>
<td>53.4</td>
<td>77.2</td>
<td>70.6</td>
<td>86.3</td>
<td>83.6</td>
<td>69.69</td>
</tr>
<tr>
<td></td>
<td>Nature Language</td>
<td>52.6</td>
<td>48.7</td>
<td>50.8</td>
<td>79.9</td>
<td>72.5</td>
<td>83.1</td>
<td>82.6</td>
<td>69.29</td>
</tr>
<tr>
<td></td>
<td>Code-based</td>
<td>+2.5</td>
<td>-2.8</td>
<td>-2.6</td>
<td>+2.7</td>
<td>+1.9</td>
<td>-3.2</td>
<td>-1.0</td>
<td>-0.40</td>
</tr>
<tr>
<td></td>
<td></td>
<td>56.6</td>
<td>53.9</td>
<td>54.0</td>
<td>85.6</td>
<td>77.3</td>
<td>86.5</td>
<td>86.9</td>
<td>73.54</td>
</tr>
<tr>
<td></td>
<td></td>
<td>+6.5</td>
<td>+2.4</td>
<td>+0.6</td>
<td>+8.4</td>
<td>+6.7</td>
<td>+0.2</td>
<td>+3.3</td>
<td>+3.85</td>
</tr>
</tbody>
</table>
<p>the increase of difficulty levels. This shows that the harder math problems require more frequent code usage, which implies that invoking code multiple times might be an important reason why GPT4-Code have such an advantage in solving difficult math problems. There is a similar trend in Fig. 5 (b).</p>
<h1>4.4 Ablation Study and Discussion</h1>
<p>Comparisons between Natural Language and Code-based Self-Verification: to underscore the significance of code in the self-verification stage, we employed a distinct natural language selfverification prompt. In this approach, GPT4-Code is directed to verify the solution through natural language instead of relying on code-based verification, as presented in Tab. 4. The accuracy achieved with this method was slightly lower than that of the Basic Prompt. Moreover, we observed a decline in accuracy for 4 of the 7 subtopics, indicating that relying solely on natural language verification can not only compromise accuracy but also negatively impact performance. In contrast, code-based verification enhances accuracy across all 7 subtopics when compared to the Basic Prompt.</p>
<p>Analysis of Verification-guided Weighted Majority Voting: we initially compiled the confusion matrix (TP/TN/FP/FN), capturing solutions with self-verification that matches the True and False states mentioned in Eq. 1 from five distinct sampled paths. The details of the confusion matrix are presented in Appendix A.1.1. From this data, we computed Precision, Recall, and Accuracy. (Solutions in the True state are seen as positive.) The results are presented in Fig. 6. In comparison to Accuracy, we observed numerical enhancements of $22.3 \%$ and $5.6 \%$ in the average Precision and Recall, respectively. In particular, the average Precision registered at $95.88 \%$. This implies that the Accuracy has the potential to become much higher, if more solutions reach the verified True state before giving the final answer.</p>
<p>Hyper-parameters ablation in Verification-guided Weighted Majority Voting: we also performed ablation studies on the hyper-parameter $w_{v} \in\left{w_{\mathrm{T}}, w_{\mathrm{U}}, w_{\mathrm{F}}\right}$ in Eq. 1. When the hyperparameter setting satisfied $w_{\mathrm{T}}&gt;w_{\mathrm{U}} \geq w_{\mathrm{F}}$, the performance of the verification-guided weighted majority voting consistently surpassed that of the naive majority voting methods across all sampled paths. In contrast, when we set the hyper-parameter $\left(w_{\mathrm{T}}=0.5, w_{\mathrm{U}}=0.5, w_{\mathrm{F}}=1\right)$, the performance under this configuration was worse than the naive majority voting. Therefore, our proposed method, verification-guided weighted majority voting, is easy to tune and robust.</p>
<h2>5 CONCLUSION AND LIMITATION</h2>
<p>In this paper, we begin with pilot experiments on GPT4-Code to explore how its use of code impacts its performance in mathematical reasoning. By analyzing Code Usage Frequency and accuracy, we determine that GPT4-Code's skill in solving math problems can be largely attributed to its ability to generate and execute code, as well as its effectiveness in adjusting and rectifying solutions when confronted with implausible execution outputs. Expanding on this understanding, we introduce the ideas of explicit code-based self-verification and verification-guided weighted majority voting, with the goal of enhancing GPT4-Code's mathematical capabilities.
However, there are limitations in our work that we plan to explore further in the future. Firstly, our analysis and improvements are currently focused on GPT4-Code, which is somewhat restrictive. We aim to apply the methods to other LLMs. Secondly, our explicit code-based self-verification and verification-guided weighted majority voting technique could potentially create more accurate datasets. These datasets would include detailed step-by-step code-based solution generation and</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: (a). shows the precision, recall, and accuracy on different reasoning paths. (b). shows the accuracy in response to the number of sampled reasoning paths when the weight is set to different values.
code-based validation, which could help improve open-source LLMs like LLaMA 2 (Touvron et al., 2023) and enhance their mathematical abilities. Although we haven't yet investigated this approach, we leave it for future work.</p>
<h1>REFERENCES</h1>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2022.</p>
<p>Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: A theorem-driven question answering dataset, 2023.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ArXiv, abs/2009.03300, 2020. URL https://api.semanticscholar.org/CorpusID: 221516475 .</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pp. 22199-22213, 2022.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 53155333, 2023.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.
A. Newell and H.A. Simon. Human Problem Solving. ACS symposium series. Prentice-Hall, 1972. ISBN 9780134454030. URL https://books.google.com.hk/books?id= h03uAAAAMAAJ.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw.</p>
<p>Yingxu Wang and Vincent Chiew. On the cognitive process of human problem solving. Cognitive Systems Research, 11(1):81-92, 2010. ISSN 1389-0417.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview. net/forum?id=_VjQ1MeSB_J.</p>
<p>Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification, 2023.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv, abs/2305.10601, 2023.</p>
<p>Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with large language models for reasoning. arXiv preprint arXiv:2305.14333, 2023.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2023.</p>
<h1>APPENDIX</h1>
<p>This Appendix contains two sections. The first section provides the experiment details, including the detailed experiment results on MATH and MMLU datasets. The second section presents some examples of GPT4-Code.</p>
<h2>A EXPERIMENT DETAILS</h2>
<h2>A. 1 DETAILED EXPERIMENT RESULT ON MATH DATASET</h2>
<h2>A.1.1 Confusion Matrix</h2>
<p>A confusion matrix is a specific table layout that allows visualization of the performance of an algorithm. It's particularly useful for classification problems, and we utilize it to analyze the performance of our verification process.</p>
<p>The matrix itself is a two-dimensional grid, $2 \times 2$, for the binary classification of verification results. Each row of the matrix represents the instances in a predicted class, which is determined by the verification results given by the language model, while each column represents the instances in an actual class, which is determined by the actual correctness of the answer given by the model. Tab. 5 shows how the matrix looks for our verification process:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Table 5: Confusion Matrix of Verification</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Answer Correct</td>
<td style="text-align: center;">Answer Wrong</td>
</tr>
<tr>
<td style="text-align: left;">Verification True</td>
<td style="text-align: center;">TP</td>
<td style="text-align: center;">FP</td>
</tr>
<tr>
<td style="text-align: left;">Verification False</td>
<td style="text-align: center;">FN</td>
<td style="text-align: center;">TN</td>
</tr>
</tbody>
</table>
<p>Here's what the four terms mean:</p>
<ul>
<li>True Positive (TP): The cases in which the model's verification result is 'True', and the answer is actually correct.</li>
<li>True Negative (TN): The cases in which the model's verification result is 'False', and the answer is actually wrong.</li>
<li>False Positive (FP): The cases in which the model's verification result is 'True', but the answer is actually wrong.</li>
<li>False Negative (FN): The cases in which the model's verification result is 'False', but the answer is actually correct.</li>
</ul>
<p>This matrix is very helpful for measuring more than just straightforward accuracy, based on which Precision and Recall are two important metrics. They are defined in Eq. 3 and their meanings are as follows:</p>
<ul>
<li>Precision is the fraction of relevant instances among the retrieved instances. It is a measure of the accuracy of the classifier when it predicts the positive class.</li>
<li>Recall is the fraction of the total amount of relevant instances that were actually retrieved. It is a measure of the ability of a classifier to find all the positive instances.</li>
</ul>
<p>$$
\text { Precision }=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}, \text { Recall }=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}
$$</p>
<p>In other words, precision answers the question "What proportion of verified TRUE answers was actually correct?", while recall answers "What proportion of actual correct answers was verified TRUE?" Given its meaning, verification-guided voting is bounded to be effective when the precision of verification is high.</p>
<h1>A.1.2 Python Package Usage analysis</h1>
<p>Tab. 6 outlines the usage of various Python packages in our experiments. Among them, we found that the sympy package is utilized most frequently, highlighting its central role in the computational tasks performed.</p>
<p>Table 6: Python package usage frequency on MATH dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Package</th>
<th style="text-align: center;">sympy</th>
<th style="text-align: center;">numpy</th>
<th style="text-align: center;">math</th>
<th style="text-align: center;">fractions</th>
<th style="text-align: center;">itertools</th>
<th style="text-align: center;">cmath</th>
<th style="text-align: center;">scipy</th>
<th style="text-align: center;">matplotlib</th>
<th style="text-align: center;">functools</th>
<th style="text-align: center;">collections</th>
<th style="text-align: center;">statistics</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">0.4168</td>
<td style="text-align: center;">0.0284</td>
<td style="text-align: center;">0.1590</td>
<td style="text-align: center;">0.0094</td>
<td style="text-align: center;">0.0034</td>
<td style="text-align: center;">0.0034</td>
<td style="text-align: center;">0.0016</td>
<td style="text-align: center;">0.0010</td>
<td style="text-align: center;">0.0004</td>
<td style="text-align: center;">0.0004</td>
<td style="text-align: center;">0.0002</td>
</tr>
<tr>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">0.3907</td>
<td style="text-align: center;">0.0241</td>
<td style="text-align: center;">0.1638</td>
<td style="text-align: center;">0.0110</td>
<td style="text-align: center;">0.0029</td>
<td style="text-align: center;">0.0026</td>
<td style="text-align: center;">0.0009</td>
<td style="text-align: center;">0.0003</td>
<td style="text-align: center;">0.0003</td>
<td style="text-align: center;">0.0006</td>
<td style="text-align: center;">0.0003</td>
</tr>
<tr>
<td style="text-align: center;">correct per code</td>
<td style="text-align: center;">0.3323</td>
<td style="text-align: center;">0.0205</td>
<td style="text-align: center;">0.1393</td>
<td style="text-align: center;">0.0094</td>
<td style="text-align: center;">0.0025</td>
<td style="text-align: center;">0.0022</td>
<td style="text-align: center;">0.0007</td>
<td style="text-align: center;">0.0003</td>
<td style="text-align: center;">0.0003</td>
<td style="text-align: center;">0.0005</td>
<td style="text-align: center;">0.0003</td>
</tr>
<tr>
<td style="text-align: center;">Wrong</td>
<td style="text-align: center;">0.4724</td>
<td style="text-align: center;">0.0383</td>
<td style="text-align: center;">0.1493</td>
<td style="text-align: center;">0.0058</td>
<td style="text-align: center;">0.0045</td>
<td style="text-align: center;">0.0052</td>
<td style="text-align: center;">0.0032</td>
<td style="text-align: center;">0.0026</td>
<td style="text-align: center;">0.0007</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">wrong per code</td>
<td style="text-align: center;">0.3194</td>
<td style="text-align: center;">0.0259</td>
<td style="text-align: center;">0.1009</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.0031</td>
<td style="text-align: center;">0.0035</td>
<td style="text-align: center;">0.0022</td>
<td style="text-align: center;">0.0018</td>
<td style="text-align: center;">0.0004</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">c/w per code</td>
<td style="text-align: center;">$104 \%$</td>
<td style="text-align: center;">$79 \%$</td>
<td style="text-align: center;">$138 \%$</td>
<td style="text-align: center;">$238 \%$</td>
<td style="text-align: center;">$80 \%$</td>
<td style="text-align: center;">$63 \%$</td>
<td style="text-align: center;">$34 \%$</td>
<td style="text-align: center;">$14 \%$</td>
<td style="text-align: center;">$57 \%$</td>
<td style="text-align: center;">NaN</td>
<td style="text-align: center;">NaN</td>
</tr>
</tbody>
</table>
<h2>A. 2 Detailed EXPERIMENT Result on MMLU DATASET</h2>
<p>Fig. 8 illustrates that GPT4-Code performs relatively poorly in certain domains, such as engineering and the humanities, with a particularly marked deficiency in virology, where it achieves a score of less than $60 \%$. These observations delineate specific areas that call for further investigation and refinement, thus outlining the direction for future improvements in the model.</p>
<h2>B EXAMPLES</h2>
<p>In this section, we provide more examples.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: GPT4-Code performance on MMLU dataset on: (a) subsubcategory, (b) category, (c) subcategory</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Model performance on MMLU dataset on: (a) subsubcategory, (b) category, (c) subcategory</p>
<p>Math Problem: $0 . \overline{54}$ as a fraction in lowest terms.
Sure, we can solve this problem by setting up an equation, solving it, and simplifying the fraction.
Let's call $0 . \overline{54}$ as $x$.
The decimal $0 . \overline{54}$ means that the digits " 54 " repeat indefinitely. So, we can multiply $x$ by 100 (since " 54 " is a two-digit number) to get another equation.
This gives us $100 x=54 . \overline{54}$.
By subtracting the original equation from this new one, we eliminate the repeating decimal: $100 x-x=54 . \overline{54}-0 . \overline{54}$.
Let's perform these steps in the next Python cell.</p>
<div class="codehilite"><pre><span></span><code># define the symbol
x = symbols(&#39;x&#39;)
# define the equations
eq1 = Eq(x, 0.54545454545454545454545454545454) # approximating the
        repeating decimal
</code></pre></div>

<p>. . . . .</p>
<h1>RESULT</h1>
<p>TypeError Traceback (most recent call last)
・.....
Apologies for the mistake. We can't directly subtract two equations in Python. Instead, we should subtract the right-hand sides and left-hand sides of the equations separately to get a new equation. Let's correct that.</p>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="nx">subtract</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">right</span><span class="w"> </span><span class="nx">hand</span><span class="w"> </span><span class="nx">sides</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">left</span><span class="w"> </span><span class="nx">hand</span><span class="w"> </span><span class="nx">sides</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">equations</span>
<span class="w">    </span><span class="nx">separately</span>
<span class="nx">rhs</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">simplify</span><span class="p">(</span><span class="nx">eq2</span><span class="p">.</span><span class="nx">rhs</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">eq1</span><span class="p">.</span><span class="nx">rhs</span><span class="p">)</span>
<span class="nx">lhs</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">simplify</span><span class="p">(</span><span class="nx">eq2</span><span class="p">.</span><span class="nx">lhs</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">eq1</span><span class="p">.</span><span class="nx">lhs</span><span class="p">)</span>
</code></pre></div>

<p>. . . . .</p>
<h2>RESULT</h2>
<p>Eq(99*x, 54.0)
After subtraction, we get the equation $99 x=54$.
Next, we can solve for $x$ to express $0 . \overline{54}$ as a fraction.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> solve for x
solution = solve(new_eq, x)
</code></pre></div>

<p>solution[0]</p>
<h2>RESULT</h2>
<h2>0.545454545454545</h2>
<p>The solution for $x$ is approximately 0.545454545454545 , but we need to express it as a fraction. Let's convert this decimal into a fraction.</p>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="nx">convert</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">decimal</span><span class="w"> </span><span class="nx">solution</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">fraction</span>
<span class="nx">fraction</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">nsimplify</span><span class="p">(</span><span class="nx">solution</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>

<p>fraction</p>
<h2>RESULT</h2>
<p>6/11
The decimal $0 . \overline{54}$ can be expressed as the fraction $\frac{6}{11}$.</p>
<h2>Answer: $\frac{6}{11}$</h2>
<p>Table 7: An example in which GPT4-Code self-debugs when the code execution emits an error.</p>
<p>Math Problem: A bus comes by Jerry's bus stop every 20 minutes starting at exactly 5:13 a.m. If Jerry shows up at exactly 8:35 a.m., how many minutes will he have to wait for the next bus?
Step 1: Calculate the total minutes from 5:13 a.m. to 8:35 a.m.
Total minutes from 5:13 a.m. to 8:35 a.m. is $180+22$ minutes.
Step 2: Calculate how many buses have come by during this time.
Step 3: Calculate the time when the last bus before 8:35 a.m. came by.</p>
<ul>
<li>The time when the last bus came by is $5: 13$ a.m. $+20 \times\lfloor n\rfloor$ minutes.</li>
</ul>
<p>Step 4: Calculate the waiting time.
Now, let's calculate each step.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Step 1: Calculate the total minutes from 5:13 a.m. to 8:35 a.m.
total_minutes = 180 + 22
</code></pre></div>

<p>・.. .</p>
<h1>RESULT</h1>
<h2>-311</h2>
<p>It seems that there's a mistake in the calculations. The waiting time cannot be negative. This error might have occurred in Step 3, where I calculated the time of the last bus in minutes past midnight, but then in Step 4, I subtracted this from the total time from 5:13 a.m. to 8:35 a.m. I should correct Step 4 to calculate the time until the next bus, which is either 0 if a bus has just arrived at 8:35 a.m., or 20 minutes minus the time since the last bus if a bus arrived before 8:35 a.m.</p>
<p>Let's correct this.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Corrected Step 4: Calculate the time until the next bus
time_since_last_bus = total_minutes - last_bus_time
</code></pre></div>

<p>・.. .</p>
<h2>RESULT</h2>
<p>331
There still seems to be a mistake in the calculations. The waiting time calculated is significantly more than 20 minutes, which is not possible given that the bus comes every 20 minutes.
Looking back at the calculations, it seems that the error is in Step 3 when calculating the time the last bus came. The last bus time should be calculated from 5:13 a.m., not from midnight. Let's correct this.</p>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Corrected Step 3: Calculate the time when the last bus before 8:35
    a.m. came by.
last_bus_time_corrected = 5 <span class="gs">* 60 + 13 + num_buses *</span> 20
</code></pre></div>

<p>・.. .</p>
<h2>RESULT</h2>
<p>331
It seems there is still an error. Let's try a different approach.</p>
<ol>
<li>Calculate the total minutes that have passed since the first bus at 5:13 a.m. until 8:35 a.m.</li>
<li>Since the buses come every 20 minutes, calculate how many complete cycles of 20 minutes have passed in the calculated total minutes.</li>
<li>The time Jerry will have to wait is the remainder of the total minutes divided by 20 (which is the time since the last bus) subtracted from 20 (which is the bus cycle).
This approach should give us the correct waiting time. Let's calculate it.</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Calculate total minutes from 5:13 a.m. to 8:35 a.m.
total_minutes = 3 * 60 + 22 # 3 hours and 22 minutes
</code></pre></div>

<p>・.. .</p>
<h2>RESULT</h2>
<p>18
So, Jerry will have to wait for 18 minutes for the next bus.</p>
<h2>Answer: 18</h2>
<p>Table 8: An example in which GPT4-Code adjusts its solution based on the result of code execution.</p>
<p>Math Problem: Solve for $x: 3^{2 x}+19=10^{x}$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">code 0</th>
<th style="text-align: center;">code 1</th>
<th style="text-align: center;">code more</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">This is a difficult equation to solve and there's no simple analytical solution using elementary functions. In real world scenarios, such equations are typically solved numerically. For example, using a graphing calculator, a computer, or a mathematical software. <br> However, if you want an approximation of the solution, it is advisable to use numerical methods such as the bisection method, Newton's method, or others. <br> If you would like me to provide an algorithm to find the solution numerically, let me know.</td>
<td style="text-align: center;">Answer: <br> The equation you're asking to solve, $3^{2 x}+19=10^{x}$, cannot be solved algebraically due to the nature of the functions and operations involved. Therefore, we'll need to use numerical methods such as Newton's method or Bisection method for root finding. <br> from scipy.optimize import fsolve import numpy as np <br> # Defining the function <br> def func (x) : <br> return $3 * *(2 * x)+$ $19-10 * * x$ <br> # Initial guess for $x$, it can be anything, 1.0 is chosen arbitrarily here $\mathrm{x} 0=1.0$ <br> # Using fsolve to find the root <br> root $=$ fsolve(func, x0 ) <br> $\mathrm{x}=$ root [0] <br> x <br> RESULT <br> 1.001831315399663 <br> So the answer is 1.00 .</td>
<td style="text-align: center;">Rewrite the equation as: $3^{2 x}-10+19=0$ <br> Let's denote $3^{x}=a$, and $10^{x}=b$ <br> so $a^{2}-b+19=0$ <br> now we have two equations: <br> 1. $a=3^{x}$ <br> 2. $b=10^{x}$ Let's solve this with Python. <br> from scipy.optimize import root <br> import numpy as np <br> # Define the system of equations <br> def equations (vars) : $\mathrm{x}, \mathrm{a}, \mathrm{b}=$ vars <br> eq1 $=\mathrm{a}-3 * * \mathrm{x}$ <br> eq2 $=\mathrm{b}-10 * * \mathrm{x}$ <br> eq3 $=\mathrm{a} * * 2-\mathrm{b}+$ <br> 19 <br> return [eq1, eq2, <br> eq3] <br> $\qquad$ <br> RESULT: <br> 0.0 <br> $3^{2.0}+19=10^{0}$ <br> The left-hand side becomes $1+19=20$ and the right-hand side becomes 1. Thus, the original equation does not hold true for $x=0$. <br> Let's try a different initial guess for the solution to see if that helps. <br> # Adjust initial guess for $x, a, b$ <br> $x 0=[1,3,10]$ <br> # Solve system of equations <br> solution $=$ root ( equations, x0) <br> # Extract solution <br> $\mathrm{x}=$ solution.x[0] <br> x <br> RESULT: <br> 2.0000000000004463 <br> We know that $3^{2.2}=3^{4}=81$ and $10^{2}=100$. The left-hand side becomes 100 and the righthand side is also 100 . Thus, the original equation holds true for $x=2$.</td>
</tr>
</tbody>
</table>
<h1>Answer: 2</h1>
<p>Table 9: An example in which no restriction on code use succeeds while not using code and using code only once fails.</p>
<p>Math Problem: What is the sum of the digits in the terminating decimal representation of the fraction $\frac{4321}{5^{7} \cdot 2^{8}}$ ?</p>
<h2>code 0</h2>
<p>The given fraction is</p>
<p>$$
\frac{4321}{5^{7} \cdot 2^{8}}
$$</p>
<p>To simplify this fraction, we can notice that $5^{7} \cdot 2^{8}=10^{7}$, which would mean the fraction is equivalent to $4321 \times 10^{-7}$, i.e., the number 4321 with the decimal point moved 7 places to the left. This would result in the number 0.0004321 .</p>
<p>To get the sum of the digits in the terminating decimal representation of this number, we just need to add up the digits in 4321, since the zeros do not contribute to the sum.
So, the sum is $4+3+2+1=10$. Therefore, the answer is 10</p>
<h2>code 1</h2>
<p>Answer:
First, we need to simplify the fraction.
Given that $4321=2^{5} \cdot 3^{3} \cdot$ $5^{0} \cdot 7^{0} \cdot 11^{0} \cdot 13^{1}$, we can simplify the fraction $\frac{4321}{5^{7} \cdot 2^{8}}$ to $\frac{2^{5} \cdot 3^{3} \cdot 5^{0} \cdot 7^{0} \cdot 11^{0} \cdot 13^{1}}{2^{3} \cdot 2^{8}}=3^{3} \cdot 13^{1} \cdot$ $2^{-3} \cdot 5^{-7}$.
Next, we need to compute the decimal representation and sum the digits.
# Compute the decimal representation of the fraction
value $=(3 * * 3 * 13 * * 1)$
/ (2<strong>3 * 5</strong>7)
# Convert to string and replace the decimal point
str_value = str(value) .replace('.','')
# Compute the sum of the digits
digit_sum = sum(int( digit) for digit in str_value)
digit_sum
RESULT:
18</p>
<h2>code more</h2>
<p>Answer:
The denominator can be rewritten as $10^{\min (7,8)} \times$ $5^{\max (0,7-8)} \times 2^{\max (0,8-7)}$, where $\min (a, b)$ is the minimum of $a$ and $b$, and $\max (a, b)$ is the maximum of $a$ and $b$. Now let's calculate the value of $\min (7,8), \max (0,7-8)$, and $\max (0,8-7)$.
min_ab = min(a, b)
max_0_a_minus_b = max $(0, a-b)$
max_0_b_minus_a = max $(0, b-a)$
min_ab,
max_0_a_minus_b,
max_0_b_minus_a
RESULT:
$(7,0,1)$
Now, let's simplify the fraction $\frac{4321}{10^{7} \times 5^{7} \times 2^{7}}$.
# define the numerator and denominator
numerator $=4321$
denominator $=10 * * 7 *$ $5 * * 0 * 2 * * 1$
. . . . . .</p>
<h2>RESULT:</h2>
<p>0.00021605</p>
<p>We exclude the zeros before the first non-zero digit and the decimal point as they do not contribute to the sum.
Thus, the sum of the digits is the sum of $2,1,6,0$, and 5 . Let's calculate it.
# digits in the fraction
digits $=[2,1,6,0$, 5]
# calculate the sum of the digits
sum_of_digits = sum( digits)
sum_of_digits</p>
<h2>RESULT:</h2>
<p>14
So, the final answer is: 14</p>
<p>Answer: 14
Table 10: An example in which no restriction on code use succeeds while not using code and using code only once fails.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
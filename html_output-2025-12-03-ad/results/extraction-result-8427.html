<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8427 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8427</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8427</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-a22f3398ea865426c89ee66f4824ec626e56a864</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a22f3398ea865426c89ee66f4824ec626e56a864" target="_blank">RET-LLM: Towards a General Read-Write Memory for Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> RET-LLM is proposed, a novel framework that equips LLMs with a general write-read memory unit, allowing them to extract, store, and recall knowledge from the text as needed for task performance.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have significantly advanced the field of natural language processing (NLP) through their extensive parameters and comprehensive data utilization. However, existing LLMs lack a dedicated memory unit, limiting their ability to explicitly store and retrieve knowledge for various tasks. In this paper, we propose RET-LLM a novel framework that equips LLMs with a general write-read memory unit, allowing them to extract, store, and recall knowledge from the text as needed for task performance. Inspired by Davidsonian semantics theory, we extract and save knowledge in the form of triplets. The memory unit is designed to be scalable, aggregatable, updatable, and interpretable. Through qualitative evaluations, we demonstrate the superiority of our proposed framework over baseline approaches in question answering tasks. Moreover, our framework exhibits robust performance in handling temporal-based question answering tasks, showcasing its ability to effectively manage time-dependent information.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8427.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8427.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retentive Large Language Model (RET-LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that endows an instruction-tuned LLM with an external, editable read-write memory storing extracted triplets <t1, relation, t2> plus their vector representations and accessed via a text-based memory API generated by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based agent (controller + finetuned LLM + memory) where the LLM is fine-tuned to emit text-form API calls ([MEM_WRITE], [MEM_READ]) to store and retrieve structured triplet facts into/from an external memory; the controller executes these calls and returns results to the LLM to produce answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B (finetuned with LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following Alpaca-7B used as base model; finetuned with LoRA to generate memory API calls and to perform information extraction, lookup, and fact-based answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthetic triplet QA / qualitative question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>The model is trained/evaluated on synthetic population/corporation/relationship triplets: it must (1) extract and write triplets from statements, (2) retrieve relevant triplets to answer questions (single- and multi-answer queries), and (3) handle temporal updates by modifying memory entries.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / information extraction + retrieval-augmented QA</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external read-write triplet memory (structured relational store) with vectorized entries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Triplet table (three-column store) where each text field has its average LLM embedding stored in an LSH table; LLM emits textual API calls ([MEM_WRITE] and [MEM_READ]) which the controller executes to write/read entries. Retrieval attempts exact text match first, then fuzzy semantic match via LSH on mean embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Extracted triplets ⟨t1, relation, t3⟩ as text plus mean vector representations (h_AVG for each t_i) stored in LSH for fuzzy lookup.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Exact string matching fallback to LSH-based nearest-neighbor (semantic) lookup on stored mean embeddings; returns all matching triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No quantitative ablations reported; qualitative comparison shows Alpaca-7B zero-shot (no memory usage) fails on example QA despite having required context, whereas RET-LLM answers correctly after writing/reading memory; no numeric metrics or systematic ablations provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit read-write memory that stores structured triplets and is invoked by the LLM via API calls can enable correct QA answers in examples where a comparable LLM with the same context fails; memory affords interpretability, aggregatability across documents, and easy temporal updates of facts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Only qualitative examples presented (no numeric evaluation); training/evaluation on synthetic triplets limits generalization claims; no ablation studies or quantitative comparisons; implementation used parameter-efficient LoRA and a single-GPU setup, so large-scale behavior not evaluated; memory retrieval depends on LSH and average embeddings which may have limitations not explored here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RET-LLM: Towards a General Read-Write Memory for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8427.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8427.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemLLM: Finetuning LLMs to use an explicit read-write memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as an evolved and thoroughly evaluated methodology related to RET-LLM that finetunes LLMs to use explicit read-write memories (cited as Modarressi et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memllm: Finetuning llms to use an explicit read-write memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemLLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An approach (cited by the authors as the evolved & thoroughly evaluated successor/concurrent work) that finetunes LLMs to interact with an explicit read-write memory; specifics are not given in this paper beyond the citation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit read-write memory (as indicated by the title)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper referenced as an evolved and thoroughly evaluated version of the concept introduced here; details deferred to that work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RET-LLM: Towards a General Read-Write Memory for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8427.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8427.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorizing Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorizing Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer architecture that stores key-value pairs extracted from transformer layers into a memory and retrieves relevant pairs to augment generation, enabling attention over longer context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorizing transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memorizing Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A transformer variant that saves (Key, Value) pairs from layer activations into a memory store and retrieves relevant pairs to add to the generation context, permitting longer-range memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based model augmented with an external memory of K/V pairs (as described in the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Designed to improve long-context attention and retrieval of memorized pieces for generation tasks; specific benchmarks not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>memory-augmented generation / long-context modeling</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>trainable key-value memory extracted from transformer layers</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store (Key, Value) pairs from transformer layers in an external memory and retrieve relevant pairs to add to current context during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Key-value pairs derived from transformer layer activations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieve relevant K/V pairs based on current context and append them to context for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Cited as a related work; no ablation details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an approach that enables attention over longer documents by storing and retrieving K/V pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in detail in this paper; cited for contrast (RET-LLM does not change LLM architecture).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RET-LLM: Towards a General Read-Write Memory for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8427.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8427.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Train-with-Memory (Zhong et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training Language Models with Memory Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method involving trainable memory units introduced during training to augment language models' capabilities by optimizing memory units along with model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training language models with memory augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Train-with-Memory (Zhong et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Approach that augments LMs with trainable memory units that are optimized during training to store and retrieve information.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>memory-augmented language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>trainable memory units (integrated during training)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Introduce trainable memory units that are updated/optimized during model training to serve as an external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Not specified here (paper cites the approach generally).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Not specified in this paper's text beyond being a retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as prior work that degrades the role of memory to retrieval of documents for context augmentation; no new ablation presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as prior art that augments LMs with trainable memory units; RET-LLM contrasts by not modifying LLM architecture and storing extracted facts instead.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Characterized here as approaches that often degrade memory to document retrieval added to context, lacking some desired properties (scalability, interpretability, aggregatability) emphasized by RET-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RET-LLM: Towards a General Read-Write Memory for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8427.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8427.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework where agents maintain a dynamic, natural-language memory of their experiences (observations and reflections) and retrieve them to guide behavior; the cited work integrates memory within the agent and uses LLMs as an external planning tool.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that store and dynamically retrieve a record of experiences in natural language; the memory is part of the agent and the LLM is used as an external planner rather than controlling memory contents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>simulated agent behavior / interactive simulation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents interact in a simulated environment, storing observations and reflections in a memory and using them to plan or generate behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>interactive simulation / agent planning with memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>agent-internal natural-language episodic memory (observation/reflection logs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store past observations and reflections in natural language; dynamically retrieve relevant memories to inform planning and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Natural-language records of experiences (observations and reflections).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Dynamic retrieval of relevant past experiences conditioned on current context/plan.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Paper contrasted with RET-LLM: in generative-agents memory is inherent to the agent and LLM acts externally, whereas RET-LLM gives the LLM direct control over what to store/read.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as demonstrating storage and retrieval of agent experiences; difference highlighted is who controls memory content (agent vs LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Described limitation: LLM in Park et al.'s framework lacks control over what gets stored/retrieved, unlike RET-LLM's LLM-driven memory API approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RET-LLM: Towards a General Read-Write Memory for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8427.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8427.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cheng et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language model with plug-in knowldge memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned approach that encodes documents, stores their representations, and retrieves relevant documents to augment the LLM's context during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language model with plug-in knowldge memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Cheng et al. plug-in knowledge memory</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A method that encodes each document, stores encodings, and retrieves relevant documents based on current context to add to generation context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented generation / document retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>document encoding store / retrieval-augmented context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Encode documents, save representations, and retrieve relevant documents for the current context to augment LLM inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Document encodings (vector representations).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieve relevant document encodings based on query context; add retrieved documents to the context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Cited among prior work; RET-LLM contrasts by storing extracted, aggregated triplet facts rather than whole documents.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Characterized as earlier methods that store document representations and retrieve documents to augment context, which may lack the aggregatability/interpretabiliy desired by RET-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not detailed beyond the paper's characterization that document-level retrieval can be less scalable/interpretable for certain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RET-LLM: Towards a General Read-Write Memory for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8427.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8427.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Time-aware LMs (Dhingra et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Time-aware language models as temporal knowledge bases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work curating a dataset to differentiate temporal vs non-temporal facts and proposing training LMs on temporally annotated data to improve temporal awareness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Time-aware language models as temporal knowledge bases</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Time-aware LMs (Dhingra et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An approach focused on temporal facts: curates temporally annotated datasets and trains models to better represent time-dependent information.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>temporal fact awareness / temporal QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Differentiate and correctly answer temporal vs non-temporal factual questions; improve models' temporal knowledge handling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>temporal question answering / temporal knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Cited as related work on temporal facts; RET-LLM claims to handle temporal updates via its updatable memory instead of retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Highlights temporal knowledge is a distinct challenge; RET-LLM positions updatable memory as an alternative to retraining/editing model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RET-LLM: Towards a General Read-Write Memory for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8427.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8427.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method that trains LMs to generate API calls to external tools; cited for similarity in teaching an LLM to use an external tool (here, the memory module).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Approach teaching LMs to use external tools by generating API calls; cited as conceptually similar to teaching an LLM to use the memory API, though Toolformer targets tool use more broadly (calculators, APIs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>tool use via API generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned to draw analogy: RET-LLM teaches LLM to call a memory API similar to how Toolformer teaches LMs to call external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used to justify the design choice of text-based API calls generated by the LLM to interact with an external tool (memory).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Toolformer itself is not evaluated here; no memory-specific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RET-LLM: Towards a General Read-Write Memory for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memorizing transformers <em>(Rating: 2)</em></li>
                <li>Training language models with memory augmentation <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Memllm: Finetuning llms to use an explicit read-write memory. <em>(Rating: 2)</em></li>
                <li>Language model with plug-in knowldge memory <em>(Rating: 1)</em></li>
                <li>Time-aware language models as temporal knowledge bases <em>(Rating: 1)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8427",
    "paper_id": "paper-a22f3398ea865426c89ee66f4824ec626e56a864",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "RET-LLM",
            "name_full": "Retentive Large Language Model (RET-LLM)",
            "brief_description": "A framework that endows an instruction-tuned LLM with an external, editable read-write memory storing extracted triplets &lt;t1, relation, t2&gt; plus their vector representations and accessed via a text-based memory API generated by the LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RET-LLM",
            "agent_description": "An LLM-based agent (controller + finetuned LLM + memory) where the LLM is fine-tuned to emit text-form API calls ([MEM_WRITE], [MEM_READ]) to store and retrieve structured triplet facts into/from an external memory; the controller executes these calls and returns results to the LLM to produce answers.",
            "model_name": "Alpaca-7B (finetuned with LoRA)",
            "model_description": "Instruction-following Alpaca-7B used as base model; finetuned with LoRA to generate memory API calls and to perform information extraction, lookup, and fact-based answer generation.",
            "task_name": "Synthetic triplet QA / qualitative question answering",
            "task_description": "The model is trained/evaluated on synthetic population/corporation/relationship triplets: it must (1) extract and write triplets from statements, (2) retrieve relevant triplets to answer questions (single- and multi-answer queries), and (3) handle temporal updates by modifying memory entries.",
            "task_type": "question answering / information extraction + retrieval-augmented QA",
            "memory_used": true,
            "memory_type": "external read-write triplet memory (structured relational store) with vectorized entries",
            "memory_mechanism": "Triplet table (three-column store) where each text field has its average LLM embedding stored in an LSH table; LLM emits textual API calls ([MEM_WRITE] and [MEM_READ]) which the controller executes to write/read entries. Retrieval attempts exact text match first, then fuzzy semantic match via LSH on mean embeddings.",
            "memory_representation": "Extracted triplets ⟨t1, relation, t3⟩ as text plus mean vector representations (h_AVG for each t_i) stored in LSH for fuzzy lookup.",
            "memory_retrieval_method": "Exact string matching fallback to LSH-based nearest-neighbor (semantic) lookup on stored mean embeddings; returns all matching triplets.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No quantitative ablations reported; qualitative comparison shows Alpaca-7B zero-shot (no memory usage) fails on example QA despite having required context, whereas RET-LLM answers correctly after writing/reading memory; no numeric metrics or systematic ablations provided.",
            "key_findings": "Explicit read-write memory that stores structured triplets and is invoked by the LLM via API calls can enable correct QA answers in examples where a comparable LLM with the same context fails; memory affords interpretability, aggregatability across documents, and easy temporal updates of facts.",
            "limitations_or_challenges": "Only qualitative examples presented (no numeric evaluation); training/evaluation on synthetic triplets limits generalization claims; no ablation studies or quantitative comparisons; implementation used parameter-efficient LoRA and a single-GPU setup, so large-scale behavior not evaluated; memory retrieval depends on LSH and average embeddings which may have limitations not explored here.",
            "uuid": "e8427.0",
            "source_info": {
                "paper_title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MemLLM",
            "name_full": "MemLLM: Finetuning LLMs to use an explicit read-write memory",
            "brief_description": "Referenced as an evolved and thoroughly evaluated methodology related to RET-LLM that finetunes LLMs to use explicit read-write memories (cited as Modarressi et al., 2024).",
            "citation_title": "Memllm: Finetuning llms to use an explicit read-write memory.",
            "mention_or_use": "mention",
            "agent_name": "MemLLM",
            "agent_description": "An approach (cited by the authors as the evolved & thoroughly evaluated successor/concurrent work) that finetunes LLMs to interact with an explicit read-write memory; specifics are not given in this paper beyond the citation.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "memory_used": true,
            "memory_type": "explicit read-write memory (as indicated by the title)",
            "memory_mechanism": null,
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": null,
            "key_findings": "Paper referenced as an evolved and thoroughly evaluated version of the concept introduced here; details deferred to that work.",
            "limitations_or_challenges": null,
            "uuid": "e8427.1",
            "source_info": {
                "paper_title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Memorizing Transformer",
            "name_full": "Memorizing Transformer",
            "brief_description": "A transformer architecture that stores key-value pairs extracted from transformer layers into a memory and retrieves relevant pairs to augment generation, enabling attention over longer context.",
            "citation_title": "Memorizing transformers",
            "mention_or_use": "mention",
            "agent_name": "Memorizing Transformer",
            "agent_description": "A transformer variant that saves (Key, Value) pairs from layer activations into a memory store and retrieves relevant pairs to add to the generation context, permitting longer-range memorization.",
            "model_name": null,
            "model_description": "Transformer-based model augmented with an external memory of K/V pairs (as described in the cited work).",
            "task_name": null,
            "task_description": "Designed to improve long-context attention and retrieval of memorized pieces for generation tasks; specific benchmarks not discussed in this paper.",
            "task_type": "memory-augmented generation / long-context modeling",
            "memory_used": true,
            "memory_type": "trainable key-value memory extracted from transformer layers",
            "memory_mechanism": "Store (Key, Value) pairs from transformer layers in an external memory and retrieve relevant pairs to add to current context during generation.",
            "memory_representation": "Key-value pairs derived from transformer layer activations.",
            "memory_retrieval_method": "Retrieve relevant K/V pairs based on current context and append them to context for generation.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Cited as a related work; no ablation details provided in this paper.",
            "key_findings": "Cited as an approach that enables attention over longer documents by storing and retrieving K/V pairs.",
            "limitations_or_challenges": "Not discussed in detail in this paper; cited for contrast (RET-LLM does not change LLM architecture).",
            "uuid": "e8427.2",
            "source_info": {
                "paper_title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Train-with-Memory (Zhong et al.)",
            "name_full": "Training Language Models with Memory Augmentation",
            "brief_description": "A method involving trainable memory units introduced during training to augment language models' capabilities by optimizing memory units along with model weights.",
            "citation_title": "Training language models with memory augmentation",
            "mention_or_use": "mention",
            "agent_name": "Train-with-Memory (Zhong et al.)",
            "agent_description": "Approach that augments LMs with trainable memory units that are optimized during training to store and retrieve information.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": "memory-augmented language modeling",
            "memory_used": true,
            "memory_type": "trainable memory units (integrated during training)",
            "memory_mechanism": "Introduce trainable memory units that are updated/optimized during model training to serve as an external memory.",
            "memory_representation": "Not specified here (paper cites the approach generally).",
            "memory_retrieval_method": "Not specified in this paper's text beyond being a retrieval augmentation.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as prior work that degrades the role of memory to retrieval of documents for context augmentation; no new ablation presented here.",
            "key_findings": "Cited as prior art that augments LMs with trainable memory units; RET-LLM contrasts by not modifying LLM architecture and storing extracted facts instead.",
            "limitations_or_challenges": "Characterized here as approaches that often degrade memory to document retrieval added to context, lacking some desired properties (scalability, interpretability, aggregatability) emphasized by RET-LLM.",
            "uuid": "e8427.3",
            "source_info": {
                "paper_title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative agents: Interactive simulacra of human behavior",
            "brief_description": "A framework where agents maintain a dynamic, natural-language memory of their experiences (observations and reflections) and retrieve them to guide behavior; the cited work integrates memory within the agent and uses LLMs as an external planning tool.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (Park et al.)",
            "agent_description": "Agents that store and dynamically retrieve a record of experiences in natural language; the memory is part of the agent and the LLM is used as an external planner rather than controlling memory contents.",
            "model_name": null,
            "model_description": null,
            "task_name": "simulated agent behavior / interactive simulation",
            "task_description": "Agents interact in a simulated environment, storing observations and reflections in a memory and using them to plan or generate behaviors.",
            "task_type": "interactive simulation / agent planning with memory",
            "memory_used": true,
            "memory_type": "agent-internal natural-language episodic memory (observation/reflection logs)",
            "memory_mechanism": "Store past observations and reflections in natural language; dynamically retrieve relevant memories to inform planning and actions.",
            "memory_representation": "Natural-language records of experiences (observations and reflections).",
            "memory_retrieval_method": "Dynamic retrieval of relevant past experiences conditioned on current context/plan.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Paper contrasted with RET-LLM: in generative-agents memory is inherent to the agent and LLM acts externally, whereas RET-LLM gives the LLM direct control over what to store/read.",
            "key_findings": "Cited as demonstrating storage and retrieval of agent experiences; difference highlighted is who controls memory content (agent vs LLM).",
            "limitations_or_challenges": "Described limitation: LLM in Park et al.'s framework lacks control over what gets stored/retrieved, unlike RET-LLM's LLM-driven memory API approach.",
            "uuid": "e8427.4",
            "source_info": {
                "paper_title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Cheng et al.",
            "name_full": "Language model with plug-in knowldge memory",
            "brief_description": "Mentioned approach that encodes documents, stores their representations, and retrieves relevant documents to augment the LLM's context during generation.",
            "citation_title": "Language model with plug-in knowldge memory",
            "mention_or_use": "mention",
            "agent_name": "Cheng et al. plug-in knowledge memory",
            "agent_description": "A method that encodes each document, stores encodings, and retrieves relevant documents based on current context to add to generation context.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": "retrieval-augmented generation / document retrieval",
            "memory_used": true,
            "memory_type": "document encoding store / retrieval-augmented context",
            "memory_mechanism": "Encode documents, save representations, and retrieve relevant documents for the current context to augment LLM inputs.",
            "memory_representation": "Document encodings (vector representations).",
            "memory_retrieval_method": "Retrieve relevant document encodings based on query context; add retrieved documents to the context.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Cited among prior work; RET-LLM contrasts by storing extracted, aggregated triplet facts rather than whole documents.",
            "key_findings": "Characterized as earlier methods that store document representations and retrieve documents to augment context, which may lack the aggregatability/interpretabiliy desired by RET-LLM.",
            "limitations_or_challenges": "Not detailed beyond the paper's characterization that document-level retrieval can be less scalable/interpretable for certain tasks.",
            "uuid": "e8427.5",
            "source_info": {
                "paper_title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Time-aware LMs (Dhingra et al.)",
            "name_full": "Time-aware language models as temporal knowledge bases",
            "brief_description": "Work curating a dataset to differentiate temporal vs non-temporal facts and proposing training LMs on temporally annotated data to improve temporal awareness.",
            "citation_title": "Time-aware language models as temporal knowledge bases",
            "mention_or_use": "mention",
            "agent_name": "Time-aware LMs (Dhingra et al.)",
            "agent_description": "An approach focused on temporal facts: curates temporally annotated datasets and trains models to better represent time-dependent information.",
            "model_name": null,
            "model_description": null,
            "task_name": "temporal fact awareness / temporal QA",
            "task_description": "Differentiate and correctly answer temporal vs non-temporal factual questions; improve models' temporal knowledge handling.",
            "task_type": "temporal question answering / temporal knowledge",
            "memory_used": null,
            "memory_type": null,
            "memory_mechanism": null,
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Cited as related work on temporal facts; RET-LLM claims to handle temporal updates via its updatable memory instead of retraining.",
            "key_findings": "Highlights temporal knowledge is a distinct challenge; RET-LLM positions updatable memory as an alternative to retraining/editing model parameters.",
            "limitations_or_challenges": "Not detailed here.",
            "uuid": "e8427.6",
            "source_info": {
                "paper_title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Toolformer",
            "name_full": "Toolformer: Language models can teach themselves to use tools",
            "brief_description": "Method that trains LMs to generate API calls to external tools; cited for similarity in teaching an LLM to use an external tool (here, the memory module).",
            "citation_title": "Toolformer: Language models can teach themselves to use tools",
            "mention_or_use": "mention",
            "agent_name": "Toolformer",
            "agent_description": "Approach teaching LMs to use external tools by generating API calls; cited as conceptually similar to teaching an LLM to use the memory API, though Toolformer targets tool use more broadly (calculators, APIs).",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": "tool use via API generation",
            "memory_used": null,
            "memory_type": null,
            "memory_mechanism": null,
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned to draw analogy: RET-LLM teaches LLM to call a memory API similar to how Toolformer teaches LMs to call external tools.",
            "key_findings": "Used to justify the design choice of text-based API calls generated by the LLM to interact with an external tool (memory).",
            "limitations_or_challenges": "Toolformer itself is not evaluated here; no memory-specific claims.",
            "uuid": "e8427.7",
            "source_info": {
                "paper_title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memorizing transformers",
            "rating": 2
        },
        {
            "paper_title": "Training language models with memory augmentation",
            "rating": 2
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2
        },
        {
            "paper_title": "Memllm: Finetuning llms to use an explicit read-write memory.",
            "rating": 2
        },
        {
            "paper_title": "Language model with plug-in knowldge memory",
            "rating": 1
        },
        {
            "paper_title": "Time-aware language models as temporal knowledge bases",
            "rating": 1
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1
        }
    ],
    "cost": 0.013920749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RET-LLM: Towards a General Read-Write Memory for Large Language Models</h1>
<p>Note: This concept paper outlines an initial methodology, now evolved and thoroughly evaluated in MemLLM. ${ }^{\dagger}$<br>Ali Modarressi ${ }^{1,2 <em>}$ Ayyoob Imani ${ }^{1,2 </em>}$ Mohsen Fayyaz ${ }^{3}$ Hinrich Schütze ${ }^{1,2}$<br>${ }^{1}$ Center for Information and Language Processing, LMU Munich, Germany<br>${ }^{2}$ Munich Center for Machine Learning, Germany ${ }^{3}$ Microsoft, Berlin, Germany<br>{amodaresi, ayyoob}@cis.lmu.de</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have significantly advanced the field of natural language processing (NLP) through their extensive parameters and comprehensive data utilization. However, existing LLMs lack a dedicated memory unit, limiting their ability to explicitly store and retrieve knowledge for various tasks. In this paper, we propose RET-LLM a novel framework that equips LLMs with a general write-read memory unit, allowing them to extract, store, and recall knowledge from the text as needed for task performance. Inspired by Davidsonian semantics theory, we extract and save knowledge in the form of triplets. The memory unit is designed to be scalable, aggregatable, updatable, and interpretable. Through qualitative evaluations, we demonstrate the superiority of our proposed framework over baseline approaches in question answering tasks. Moreover, our framework exhibits robust performance in handling temporal-based question answering tasks, showcasing its ability to effectively manage time-dependent information.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have significantly advanced the field of natural language processing (NLP) in recent years (Bubeck et al., 2023; Chowdhery et al., 2022; Touvron et al., 2023). With their vast parameter count and access to extensive data, LLMs have demonstrated remarkable accuracy across various tasks. However, current state-of-the-art LLMs lack a dedicated memory unit. Instead, they are trained to predict words based on context, encoding knowledge implicitly in their parameters, which differs from the ideal memory function.</p>
<p>An ideal memory unit should possess certain characteristics. Firstly, it should allow for read and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of RET-LLM. A user could prompt with (A): an informative sentence and our approach stores potent information from it inside the memory or (B): a question where previously saved information should be utilized to generate a valid answer.
write operations, enabling the language model to interact with stored knowledge. Scalability is also crucial, as the memory unit should accommodate the consistently evolving nature of knowledge. Furthermore, the memory unit should not be limited to textual documents alone; it should be capable of acquiring knowledge from diverse sources such as database systems. Interpretabilty is desired, granting insight into the specific knowledge required by the LLM to solve a given task. Lastly, the information stored in the memory unit should be aggregatable, enabling the model to combine related information across multiple documents. For instance an LLM should be able to list all cities of a country mentioned in multiple documents.</p>
<p>Previous attempts to incorporate memory into LLMs have fallen short in capturing the complete range of memory characteristics. For example, (Zhong et al., 2022; Wu et al., 2022) and (Cheng et al.) degrade the memory as the ability to retrieve relevant documents for a given query context, and adding them to the context when generating answers. Park et al. (2023) merely stores and retrieves previous observations and reflections of a generative agent in a simulated environment.</p>
<p>To address these limitations, we introduce RETLLM, (Retentive LLM) a solution that endows</p>
<p>LLMs with a scalable, updatable, interpretable, and aggregatable memory module. Our proposal involves equipping language models with a memory module, which allows them to extract knowledge from text and save it for future reference. When faced with a task, the LLM can query the memory module for additional information to support its response. The memory module supports updates and can incorporate information from non-textual sources such as SQL and no-SQL databases and spreadsheets. Furthermore, it enables aggregation of various pieces of information related to a particular concept scattered in a huge document or within multiple documents.</p>
<p>Figure 1 shows the architecture of RET-LLM. It comprises three components: an LLM, a controller, and a memory unit. We employ Alpaca Taori et al. (2023), a recently released instruction-tuned language model (LLM), and design a fine-tuning process to enable it to acquire the following abilities: information extraction, information lookup, and fact-based answer generation.</p>
<p>Information extraction entails the identification and extraction of triplets in the form of <concept1, relationship, concept2> from informative sentences. The information lookup task involves querying the memory unit to acquire additional information concerning a given concept and its associated relationships when confronted with tasks necessitating further information. Lastly, fact-based answer generation involves generating a final answer based on the retrieved information. The triplet-based storage approach draws inspiration from the theoretical framework of Davidsonian semantics (Davidson, 1967), which provides a foundation for representing concepts described in sentences using a tripletlike structure of <event, subject, object>.</p>
<p>The memory module stores the triplets and their vector representations. During retrieval, it first searches for an exact match of the query text and resorts to a fuzzy search based on vector representations if no exact match is found. For efficient fuzzy search and retrieval, we employ LSH-based hashing of vector representations. The controller acts as an interface, automating interactions between users, the LLM, and the memory module, ensuring a seamless interaction experience with an intelligent chat system.</p>
<p>Our proposed approach offers several advantages over previous methods. It enables LLMs to explicitly store and retrieve knowledge, which is crucial
for real-world NLP applications. By incorporating explicit knowledge storage and retrieval, we gain better understanding of the workings of these models and the knowledge they rely on to solve tasks. The use of an external memory unit separate from the LLM ensures scalability and easy modification of stored information. The fuzzy search technique enables efficient retrieval of relevant information, even in the absence of exact matches. Storing information in triplets facilitates the generation of precise and comprehensive solutions, particularly when data aggregation is necessary. Lastly, the memory module allows for easy incorporation of information from diverse sources and accommodates changing facts over time.</p>
<p>Over a qualitative evaluation using question answering examples, we demonstrate cases where a comparable LLM such as Alpaca-7B fails to return a correct answer. We show that this shortcoming occurs while the model has access to all the information required for generating a valid answer. However, in our proposed approach after storing the extractable knowledge from the context, the RET-LLM shows its capability in answering a question without the need of reinputting the context. We also demonstrate that RET-LLM could handle temporal based QA examples. Since it is equipped with a modifiable memory which could handle temporal facts.</p>
<h2>2 Related Works</h2>
<p>Prior works in the field have explored incorporating relevant context into large language models (LLMs) by retrieving and adding relevant documents to the task's context. Zhong et al. (2022) propose training LLMs with memory augmentation by introducing trainable memory units that are optimized during the training process. Wu et al. (2022) presents the Memorizing Transformer, which can attend to longer documents during inference. This approach stores (Key, Value) pairs, extracted from a transformer layer, in a memory and retrieves relevant pairs to add them to the current context during generation. (Cheng et al.) encode each documents, save them, and retrieve relevant documents based on the current context. In contrast to these approaches, our method offers improved scalability as we do not modify the architecture of the LLM. Instead, we suggest extracting and saving information from documents, allowing for the aggregation of extracted information from multiple sources.</p>
<p>This enables us to provide more relevant and concise retrieved information that is closely aligned with the specific question being addressed.</p>
<p>Park et al. (2023) utilizes an LLM within a generative agent framework to facilitate the storage and dynamic retrieval of a comprehensive record of the agent's experiences using natural language. However, there exists a fundamental distinction between their architecture and ours. In Park's framework, the memory component is an inherent part of the agent itself, while the LLM serves as an external tool employed solely for planning the agent's behaviors. Consequently, the LLM lacks control over the specific content to be stored and retrieved within the agent's memory.</p>
<p>Dhingra et al. (2022) contribute to the field by curating a dataset specifically designed to differentiate between temporal and non-temporal facts. They propose training language models on temporally annotated data to enhance their temporal awareness. This work aligns with our research focus on addressing temporal information challenges. However, in our proposed solution, we address these challenges by introducing an updatable memory module.</p>
<p>Schick et al. (2023) present a methodology that empowers LLMs to leverage external tools by generating API calls to access additional functionalities, such as using a calculator for task execution. Our work shares similarities with their approach in terms of teaching the LLM to utilize an external tool. However, it should be noted that our focus lies on incorporating a more intricate and influential tool, namely the memory module, which has the potential to significantly impact the LLM's output.</p>
<h2>3 Approach</h2>
<p>We aim to design a RET-LLM where the user can perform two actions: (1): Provide one or a series of informative statements where the RET-LLM should be able to memorize the containing information. Previous methods perform this task by either training/fine-tuning the LLM over the provided document or creating a vector representation for the document and storing the representation. (2): Asking related questions which the RET-LLM would answer based on the stored memory. All these actions should function in a seamless setting where the user should only interact in natural language.</p>
<p>Our RET-LLM is constituted by three main com-
ponents: (1) Controller, (2): Fine-tuned LLM \&amp; (3): Memory. As shown in Figure 1, the controller moderates the flow of information between the user, the LLM and the memory. The LLM acts as a processing unit, where it receives the texts passed by the controller and figures where it needs to invoke a memory call or not. Since the LLM operates with text, inspired by Schick et al. (2023), we standardized the memory calls by implementing a text-based API schema. Therefore the LLM could generate memory API calls and the controller could apply the LLM API calls to the memory. In our setting, the memory stores data in triplets by using a three-columned table. This is based on the theoretical framework of Davidsonian semantics (Davidson, 1967), where concepts described in sentences could be stored in a structure of <first argument, relation, second argument>.</p>
<p>In the following we describe RET-LLM in more detail. The memory-API, how we finetune the LLM to become capable of these calls and the memory structure.</p>
<h3>3.1 Memory Structure</h3>
<p>Each triplet defines a relationship between two arguments with the following format: $\left\langle t_{1}, t_{2}, t_{3}\right\rangle$ where $t_{1}$ is the first argument, $t_{2}$ is the relation and $t_{3}$ is the second argument in the relationship. For instance in the sentence: "Mark Zuckerberg is the CEO of Meta Inc." the informative triplet that could be extracted is: (Mark Zuckerberg, CEO, Meta Inc.).</p>
<p>To store these triplets we use a three-columned table where each column is associated with each part of the triplet. Alongside saving the texts, we store the average representations so that the memory could also handle queries which have semantically similar words. If the memory module fails to find the exact text in the table, it checks for similar texts by comparing the vector representation of the query text with vector representations of text peices already stored in the dataset. Therefore for every $t_{i}$ the mean representation retrieved by the $\operatorname{LLM}\left(\boldsymbol{h}<em i="i">{A V G}\left(t</em>\right)\right)$ is stored in a Locality-Sensitive Hashing (LSH) table. The reason of utilizing LSH is to reduce the computation required for finding similar representations. Without a hash table for a given query representation, the distances to all of the stored representations should be computed which would be a computationally-expensive task.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Memory-Write scenario: (1) Controller passes the input to the LLM (2) which generates the appropiate memory write call. (3) The controller gives the data (and their average represntations) to the memory to be stored.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Memory-Read scenario: (1) Controller passes the question to the LLM (2) which generates the appropiate memory read call. (3) The controller apply the query on the memory with the given search terms from the LLM. (4) The memory returns the query results which are (5) forwarded back to the LLM. (6) The LLM generates the answer to the question using the query results and (7) the answer would be returned back to the user.</p>
<p>Figure 2: A visualization of the process in both read- and write-based inputs.</p>
<p>Handling Memory Queries. In a memory query, one or two of the triplet parameters should be provided as input:</p>
<p>$$
\mathcal{Q} \in\left{\left\langle q_{1}\right\rangle,\left\langle q_{2}\right\rangle,\left\langle q_{3}\right\rangle,\left\langle q_{1}, q_{2}\right\rangle,\left\langle q_{1}, q_{3}\right\rangle,\left\langle q_{2}, q_{3}\right\rangle\right}
$$</p>
<p>Where $q_{i}$ is the search term for the $i$-th parameter in the stored tuples. Before retrieving the query results, each search term is checked For a given $\mathcal{Q}$, first the memory checks whether the search terms $\left(q_{i}\right)$ have an exact match in the storage table. If $q_{i}$ does not exist in the stored terms, we use its average representation $\boldsymbol{h}<em i="i">{A V G}\left(q</em>}\right)$ and the LSH table for an alternative term $\left(\hat{q<em i="i">{i}\right)$ that has an exact match in out memory table. Possibly, the LSH table may not find an alternative term for the given representation, therefore the query would not have a result: $\mathcal{Q} \rightarrow \emptyset$. In any case (exact match or similar match), the query might have multiple matches in the data table $\left(q</em>\right)$. In this case all resulting triplets would be returned as the query output.}=t_{i</p>
<h3>3.2 Memory-API \&amp; Dataflow</h3>
<p>To enable communication between the memory and the LLM, we design an API schema for memory read and write functions. This API allows the controller to understand when the LLM is calling the memory and what parameters should be passed. Based on the triplets discussed in the previous section, the two memory calls are as the following:</p>
<ul>
<li>[MEM_WRITE $\left{t_{1} \gg t_{2} \gg t_{3}\right}$ ]: This structure is for storing a triplet $\left\langle t_{1}, t_{2}, t_{3}\right\rangle$. Depending on the prompt, multiple write calls could be sequentially generated by the LLM to store multiple triplets extracted from a text.</li>
<li>[MEM_READ $\left{_\right}<em -="-">{-} \gg</em>$ atleast one of them should be filled with the search terms. Based on the query results from the memory, one or a list of triplets could be returned as shown in the highlighted segment.}\right}:\left{t_{1} \gg t_{2} \gg t_{3}\right} ; \ldots$ ] : In a memory read, as shown in the API, there are three placeholders that based on $\mathcal{Q</li>
</ul>
<p>Figure 2 demonstrates how RET-LLM operates using the memory-API. Depending on the input given by the user, RET-LLM either have to read or write information from or to the memory. If the user prompt an informative statement (or ideally a full document), it would be memory write scenario. On the other hand, by having a question in the input, we consider this to be a memory read case. In both cases the user input is the first input to RET-LLM that is passed on to the LLM.</p>
<p>Based on the given input the LLM infers and generates the relevant API call. With a memory write case, after the API call is generated the controller detects it and invoke a memory storation function with the given parameters. The memory receives the data in a triplet format and stores it for future usage. If a memory read call is generated by the LLM, the controller also detects it and pauses the model's sequence generation for the memory retrieval. It uses the parameters given inside the read call as the query terms and passes them to the memory. The memory lists all stored triplets that feature the given search terms (or a semantically similar version of them according to $\S 3.1$ ) and return the results back to the controller. Using the API discussed in the beginning of this section, the read results are listed after the call so that the LLM could use them to produce a naturally sounded answer. After the answer is produced it is returned back to the user.</p>
<p>As the controller is in between of the user and the LLM, it could hide the whole memory-API schema. This would make the user feel an end-to-end simple language modeling experience without knowing the memory functionality behind the scene.</p>
<h3>3.3 Finetuning the LLM</h3>
<p>In this part we discuss how the LLM is finetuned to be capable of generating memory-API calls. In the end the LLM should be capable of detecting which type of memory call (read or write) it should provoke based on the input. As stated in Section 3.2, the LLM's input may have one of the two previously discussed structures depending on the memory function. Therefore the LLM should be able to generate and handle this API to store or read the relevant information. To this end, we develop a synthetic dataset to train the LLM. The synthetic task is to learn the relationships of the discussed people with the respective corporations. Based on the stored information, RET-LLM should be
capable of answering any questions regarding the people, the corporations or the relationships.</p>
<p>We use a set of firstname and lastnames to generate a synthetic population, called $\mathcal{P}$. Each person from this population per $\in \mathcal{P}$ could have only one relationship from the following list: $r e l \in \mathcal{R}={$ employment, manager, investor, founder, customer $}$ with an organization $\operatorname{org} \in$ $\mathcal{O}$. Where $\mathcal{O}$ is a set of corporation names. Hence, each triplet would be as: $(p e r, r e l, o r g)$. For instance: $\langle$ Dominick Alphonso, employment, BMW $\rangle .{ }^{1}$ Based on this triplet we can build three triplet-specific questions:</p>
<ul>
<li>$\mathcal{Q}=\langle$ per $\rangle$, e.g. "Who is Dominick Alphonso?"</li>
<li>$\mathcal{Q}=\langle$ per, org $\rangle$, e.g. "How Dominick Alphonso is related to BMW?"</li>
<li>$\mathcal{Q}=\langle$ per, rel $\rangle$, e.g. "Dominick Alphonso is employed by which company?"
and the answer to all above should be "Dominick Alphonso is employed by BMW.". Alongside these questions three other types of questions could be asked that could be relevant to multiple triplets:</li>
<li>$\mathcal{Q}=\langle r e l\rangle$, e.g. "Who are the employees?"</li>
<li>$\mathcal{Q}=\langle o r g\rangle$, e.g. "Who are related to BMW?"</li>
<li>$\mathcal{Q}=\langle r e l, o r g\rangle$, e.g. "Who are employed by BMW?"</li>
</ul>
<p>Unlike the first three, each of these questions could have multiple persons related to the answer. For each of these questions we expect the model answer the questions without any extra information (e.g. stating the corporation of employment when its not asked). To create a training data instance from these questions based on the memory-API, we use the templates stated in Table 1. During finetuning the Question, API query (with the MEM_READ command), API Response and the answer are concatenated as the data input for the LLM. However, the langauge modeling loss is only applied to the API query and Answer sections. Since these two segments are the text sequences that the LLM is expected to generate based on the other two segments (Question \&amp; API Response) that are provided by the controller.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Query Type</th>
<th>Question</th>
<th>API Query</th>
<th>API Response</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\langle per\rangle$</td>
<td>Who is per?</td>
<td>${p e r * *\rangle$ :</td>
<td>${p e r * r e 1 * o r g}$</td>
<td>per is rel to org.</td>
</tr>
<tr>
<td>$\langle per,org\rangle$</td>
<td>How per is related to org?</td>
<td>${p e r * * o r g}$ :</td>
<td>${p e r * r e 1 * o r g}$</td>
<td>per is rel to org.</td>
</tr>
<tr>
<td>$\langle per,rel\rangle$</td>
<td>per is rel which company?</td>
<td>${p e r * r e 1 *\rangle$ :</td>
<td>${p e r * r e 1 * o r g}$</td>
<td>per is rel to org.</td>
</tr>
<tr>
<td>$\langle org\rangle$</td>
<td>Who are related to org?</td>
<td>${* * o r g}$ :</td>
<td>${\mathrm{per}<em 1="1">{1} * \mathrm{rel}</em>} * \mathrm{org}} ;\left{\mathrm{per<em 2="2">{2} * \mathrm{rel}</em>\right} ; \ldots$} * \mathrm{org</td>
<td>${\mathrm{per}<em 2="2">{1}, \mathrm{per}</em>, \ldots}$ is/are related to org.</td>
</tr>
<tr>
<td>$\langle rel\rangle$</td>
<td>Who are the rel?</td>
<td>${<em> \mathrm{rel} </em>\rangle$ :</td>
<td>$\left{\mathrm{per}<em 1="1">{1} * \mathrm{rel}<em>{</em>} \mathrm{org}</em>}\right} ;\left{\mathrm{per<em 2="2">{2} * \mathrm{rel}<em>{</em>} \mathrm{org}</em>\right} ; \ldots$</td>
<td>${\mathrm{per}<em 2="2">{1}, \mathrm{per}</em>, \ldots}$ is/are rel.</td>
</tr>
<tr>
<td>$\langle org, rel\rangle$</td>
<td>Who are rel org?</td>
<td>${* \mathrm{rel} * \mathrm{org}}$ :</td>
<td>$\left{\mathrm{per}<em 2="2">{1} * \mathrm{rel}<em>{</em>} \mathrm{org}\right} ;\left{\mathrm{per}</em>} * \mathrm{rel<em>{</em>} \mathrm{org}\right} ; \ldots$</td>
<td>$\left{\mathrm{per}<em 2="2">{1}, \mathrm{per}</em>, \ldots\right}$ is/are rel to org.</td>
</tr>
</tbody>
</table>
<p>Table 1: Memory read data examples for finetuning. The first three types of questions are based on a single triplet therefore the API response would be only one triplet. However the second three may have multiple relevant tiplets stored in the memory as shown in their API-Resonse. Thus, the answer should combine the triplets data into a single sentence. $\left[\mathrm{per}<em 2="2">{1}, \mathrm{per}</em>, \ldots\right]$ is the placeholder of the names written sequentially in a natural way. For instance: "Dirk Alosa, Ty Baumkirchner, and Vera Bayless"</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Triplet(s)</th>
<th style="text-align: left;">Statement</th>
<th style="text-align: left;">API Write Call(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\left[\left(\mathrm{per}<em 2="2">{1}, \mathrm{rel}, \mathrm{org}\right),\left(\mathrm{per}</em>\right), \ldots\right]$}, \mathrm{rel}, \mathrm{org</td>
<td style="text-align: left;">$\left[\mathrm{per}<em 2="2">{1}, \mathrm{per}</em>, \ldots\right]$ is/are rel to org.</td>
<td style="text-align: left;">$\left[\mathrm{MEM} _\right.$WRITE $\left(\mathrm{per}<em 1="1">{1} * \mathrm{rel}</em>} * \mathrm{org}\right) \mathrm{][MEM} _\right.$WRITE $\left(\mathrm{per<em 2="2">{2} * \mathrm{rel}</em> \ldots$} * \mathrm{org}\right) \mathrm{]</td>
</tr>
</tbody>
</table>
<p>Table 2: Memory write data example structure for finetuning.</p>
<p>As we also need informative examples where have MEM_WRITE calls, we use a similar strategy by using the population, organizations and relations that were previously defined $(\mathcal{P}, \mathcal{Q}, \mathcal{R})$. Based on the memory-API, in a memory write scenario the RET-LLM receives a sentence which here contains a relationship information and then the LLM should generate the corresponding memory write calls. In our dataset we opted to build examples where it states about multiple people whom have the same relationship with the same company: $\left(\mathrm{per}_{i}, \mathrm{rel}, \mathrm{org}\right)$. The template for the memory write data examples are shown in Table 2. Similar to the question-based examples, the statement and the API call are concatenated to form the full input sequence. Also the loss function is applied only to the API segment, since the first part is provided by the controller.</p>
<p>We opted to use the Instruction-following Alpaca-7B model (Taori et al., 2023) as a base model for our finetuning. To execute the training in a resource limited setup, we use low-rank adaptation (LoRA) (Hu et al., 2022). ${ }^{2}$ This parameter efficient measure allows us to finetune the base model on a single A6000 48GB GPU.</p>
<h2>4 Qualitative Results</h2>
<p>In this part, we present the internal process and final output on multiple evaluation examples. These examples were generated with the same procedure stated in $\S 3.3$. First to demonstrate the importance of our approach, we provide the same example to</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>our base model (Alpaca-7B) in a zero-shot setting. The input would be a short instruction for the task, the informative sentences from the example and in the end is the question. As shown in Figure 3, the zero-shot result from the instruction tuned model is clearly incorrect. While the model does have all the information in its context, its still produces an incorrect response.</p>
<p>In thie same example, the RET-LLM first stores the extracted triplets from the examples into the memory. After storing the extracted relationships, the RET-LLM could respond to the same question even without having the information in the input. With the help of the memory-API and the memory itself, the relevant triplet is found. The LLM manages to answer correctly after appending the query result to the memory call.</p>
<p>One potential use cases of our approach is in answering questions that have a temporal context. For example, the presidency of the United States undergoes a change every 4 to 8 years. A normal PLM model answers the question about the presidency based on its own training data. While model retraining or parameter editing has its own challenges, our approach could provide an easy and interpretable solution for this issue (Figure 4).</p>
<h2>5 Conclusion \&amp; Future Work</h2>
<p>In this work, we introduced a RET-LLM capable of storing information and retrieving it in further use. With a triplet based memory structure, information are stored in relationships between two arguments with a known relation. The memory could be utilized via a memory-API which is generated</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: An example that has an incorrect result in a zero-shot setting and a correct one in our approach. Note that in the zero-shot setting the model has direct access to the information required for answering the question in its input and still end up with an incorrect answer. However, in our approach each of the user prompts could be given to the RET-LLM in separately. Another example is mentioned in the appendix Figure 5.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Asking a question which requires temporal context usually leads to an outdated answer as shown here with Alpaca. However, in our RET-LLM with the aid of a modifiable memory, these questions could be answered by simply providing a updated memory entry.</p>
<p>by a finetuned LLM. Using a controller, all components could communicate with each other and the user would interact with the controller being unbeknown of the behind process. We have shown that the LLM generates the proper API calls in some question answering examples without having the information in its input context. As this work is still under development, in our next revision we will add a more in-detail empirical evaluation, preferrably on a real dataset. We also seek to improve our finetuning method to a more generalized setting so that it could be capable of working with more types of informative relations.</p>
<h2>References</h2>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Xin Cheng, Yankai Lin, Dongyan Zhao, and Rui Yan. Language model with plug-in knowldge memory.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.</p>
<p>Donald Davidson. 1967. The logical form of action sentences, reprinted in d. davidson (1980) essays on actions and events.</p>
<p>Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2022. Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics, 10:257273.</p>
<p>Edward J Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.</p>
<p>Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. 2024. Memllm: Finetuning llms to use an explicit read-write memory. arXiv preprint arXiv:2404.11672.</p>
<p>Joon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.</p>
<p>Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. Memorizing transformers. arXiv preprint arXiv:2203.08913.</p>
<p>Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5657-5673, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<h2>A Extra Evaluation Example</h2>
<h1>Evaluation Example #2 (Zero-Shot Setting - Alpaca-7B):</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<h2>Evaluation Example #2:</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 5: Another evaluation example that has an incorrect result in a zero-shot setting and a correct one in our approach.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The code for finetuning a llama-based model using LoRA is available at: github.com/tloen/alpaca-lora&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
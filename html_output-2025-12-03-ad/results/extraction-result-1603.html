<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1603 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1603</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1603</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-252118966</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.03736v1.pdf" target="_blank">Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Knowledge-Driven Program Synthesis (KDPS) as a variant of the program synthesis task that requires the agent to solve a sequence of program synthesis problems. In KDPS, the agent should use knowledge from the earlier problems to solve the later ones. We propose a novel method based on PushGP to solve the KDPS problem, which takes subprograms as knowledge. The proposed method extracts subprograms from the solution of previously solved problems by the Even Partitioning (EP) method and uses these subprograms to solve the upcoming programming task using Adaptive Replacement Mutation (ARM). We call this method $\text{PushGP}+\text{EP}+\text{ARM}$. With $\text{PushGP}+\text{EP}+\text{ARM}$, no human effort is required in the knowledge extraction and utilization processes. We compare the proposed method with PushGP, as well as a method using subprograms manually extracted by a human. Our $\text{PushGP}+\text{EP}+\text{ARM}$ achieves better train error, success count, and faster convergence than PushGP. Additionally, we demonstrate the superiority of PushGP+EP+ARM when consecutively solving a sequence of six program synthesis problems.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1603.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1603.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PushGP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Push Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A genetic programming system that evolves programs written in the Turing-complete Push language using population-based search, selection (lexicase), and mutation operators; programs are sequences of Push instructions executed by multiple typed stacks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autoconstructive evolution: Push, pushgp, and pushpop</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PushGP</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PushGP evolves populations of candidate programs represented as sequences of Push instructions. Execution follows Push semantics (multiple typed stacks). Search is driven by selection (lexicase selection in this paper) and variation operators (uniform mutation by addition and deletion — UMAD — and Adaptive Replacement Mutation when an archive is available). Fitness is measured on I/O examples (minimize aggregate error across cases). The implementation used is based on Pyshgp and Helmuth et al.'s PushGP variants (down-sampled lexicase selection, UMAD).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Primarily uniform mutation by addition and deletion (UMAD): randomly add or delete instructions with configured rates (in experiments addition rate = 0.09, deletion rate = 0.0826). In configurations using knowledge, Adaptive Replacement Mutation (ARM) may be applied instead of UMAD with probability r_arm (0.1); ARM replaces a random partition of a parent with a subprogram from an archive (see ARM entry for full details).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Task error computed as sum of ||p(in_i)-out_i|| across N I/O examples (Equation (1)); comparator for ordering individuals uses count of solved cases (count of zeros in the error vector). A 'successful run' is one that passes all I/O cases on both training and testing sets.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Across experiments PushGP+EP+ARM achieved lower training error and higher test success counts than plain PushGP (e.g., in Experiment II Order 2 on CSL: PushGP+EP+ARM achieved 16 successful runs vs PushGP's 8), but many of the higher success counts were not statistically significant under Fisher's exact test.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>Not explicitly quantified; paper discusses negative transfer and possible overfitting when archives are used inappropriately, indicating a qualitative tradeoff where added knowledge (from archives) can speed convergence but harm generalization/executability on some problems.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>program synthesis (PS), Knowledge-Driven Program Synthesis (KDPS)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>PushGP (original without archives), PushGP+HP+ARM (using human-extracted subprograms)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PushGP augmented with automatic archive extraction (EP) and ARM yields significantly lower training error, faster convergence, and often higher test success counts vs baseline PushGP; automatic archives perform comparably but slightly worse than human-partitioned archives in some measures. Larger archives and irrelevant archived subprograms can cause negative transfer and reduced effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1603.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1603.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Replacement Mutation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mutation operator that replaces a randomly chosen segment of a parent program with a subprogram drawn from a subprogram archive, using an adaptive selection scheme that favors subprograms which previously improved fitness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Incorporating sub-programs as knowledge in program synthesis by pushgp and adaptive replacement mutation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Adaptive Replacement Mutation (ARM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ARM integrates an external archive of subprograms into PushGP's variation. For a parent program (length l1) and a chosen subprogram (length l2), ARM replaces a random partition of the parent of length l2 with the subprogram; if l1 < l2 the whole parent is replaced. ARM maintains a quality counter Q_k per subprogram k, incremented when replacement yields an improved individual; subprograms are selected either uniformly at random or by proportional selection according to Q_k (p(k) = Q_k / ΣQ). Configuration parameters used: r_arm (probability to attempt ARM) = 0.1, r_prop (probability to use proportional selection) = 0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Replacement Mutation (RM): choose subprogram k from archive K (via uniform or proportional selection), pick a random contiguous partition of the parent matching subprogram length, and substitute; if subprogram contains inputs unavailable in current problem, map/replace those inputs with a random valid input for current problem. Adaptive mechanism updates Q_k whenever RM produces an offspring with strictly better fitness than the parent.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Same fitness / executability as PushGP: count of I/O cases solved and aggregate error. ARM judges improvement by comparing offspring and parent via their error vectors; Q_k increments when offspring is better.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>ARM combined with EP (PushGP+EP+ARM) reduces training error and speeds convergence versus plain PushGP; in multiple experiments this led to higher test success counts (though many differences not statistically significant). No global numeric novelty/diversity metrics reported. Specific numeric example: in one ordering (Experiment II, Order 2) CSL problem, PushGP+EP+ARM had 16 successful runs vs PushGP's 8.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>Paper reports that a larger subprogram archive can make it harder for ARM's adaptive selection to find helpful subprograms (reducing effectiveness) and that archives from unrelated tasks may cause negative transfer — qualitatively a tradeoff between leveraging knowledge (speed) and preserving generalizable executability.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>program synthesis (PS), KDPS</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>UMAD-only mutation (PushGP), PushGP+HP+ARM (human-partitioned subprogram archive)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ARM enables automatic reuse of subprograms to accelerate convergence and improve training error; its proportional adaptive selection (based on Q_k) preferentially reuses subprograms that have empirically improved parents. Effectiveness depends on archive quality and size—poor or large archives can induce negative transfer and slower/brittle performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1603.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1603.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Replacement Mutation (component of ARM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concrete mutation operator that replaces a contiguous partition of a parent program with a chosen subprogram from an archive; part of the ARM mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Replacement Mutation (RM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RM takes a parent program (length l1) and a subprogram (length l2) and replaces a randomly chosen contiguous partition of the parent of length l2 with the subprogram; if the subprogram is longer than the parent (l2 > l1), the parent is entirely replaced. RM is deterministic given the chosen partition and subprogram, and included as the core variation step within ARM.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Partition replacement: choose a partition uniformly at random from the parent matching the length of the chosen subprogram and substitute. If subprogram refers to input variables not present in the target task, those inputs are substituted with random valid inputs from the current task.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Improvement judged by comparing offspring fitness to parent (error vector / number of solved I/O cases); RM increments subprogram quality counters in ARM only when immediate improvement occurs.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>RM serves as the variation operator that enabled observed faster convergence and lower training error when combined with archive selection (as part of ARM). No separate numerical evaluation solely for RM is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>RM can inject larger functional blocks (subprograms) into parents which can rapidly change behavior (increasing novelty) but risks breaking generalization if subprograms are irrelevant to the current task (reducing executability); the paper discusses such negative transfer qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>UMAD mutation (addition/deletion)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Replacement of partitions with archived subprograms can accelerate search and produce better training performance, but effectiveness relies on archive relevance and adaptive selection to avoid repeated use of unhelpful subprograms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1603.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1603.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UMAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform Mutation by Addition and Deletion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple mutation operator for PushGP that randomly adds or deletes instructions uniformly throughout a program, with specified per-instruction probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program synthesis using uniform mutation by addition and deletion</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Uniform Mutation by Addition and Deletion (UMAD)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>UMAD selects mutation sites uniformly and either inserts an instruction (from instruction set) or deletes an existing instruction according to configured probabilities. In this paper UMAD is the baseline mutation operator used by PushGP when ARM is not applied.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Addition: insert a randomly chosen instruction at a random position with probability (experimentally set to 0.09). Deletion: remove a randomly selected instruction with probability (experimentally set to 0.0826).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Same program fitness metrics as PushGP (aggregate I/O error; number of solved cases). UMAD is fallback mutation when ARM does not improve offspring.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Used as the alternative mutation when ARM either is not selected or fails to improve; baseline PushGP using UMAD showed slower convergence and higher training error than PushGP+EP+ARM in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>UMAD produces smaller local edits compared to RM (lower immediate novelty but less disruptive risk), which can lead to slower exploration but potentially more stable refinement — the paper shows UMAD-only runs converge slower than ARM-augmented runs.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>ARM (replacement with stored subprograms)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>UMAD alone is a weaker variation operator in these experiments than ARM augmented with automatically-extracted subprograms; UMAD remains important as fallback to ensure local search and prevent dependence on archive subprograms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1603.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1603.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Even Partitioning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic subprogram extraction method that divides a simplified solution program into n contiguous parts of (nearly) equal length to create subprograms for a knowledge archive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Even Partitioning (EP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>EP takes a solved program, simplifies it by removing redundant instructions (those without enough arguments), then divides the program into n contiguous partitions of equal or near-equal instruction-length to produce subprograms which are stored in the Subprogram Archive for later reuse via ARM. EP is fully automatic and requires no human labor.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Subprograms are evaluated indirectly by ARM through Q_k counters that count how often a subprogram yields an improved offspring; EP itself has no intrinsic fitness metric.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>EP-generated archives (used with ARM) produced significantly lower training error and faster convergence than PushGP alone and performance comparable (but slightly worse) to human-partitioned archives in experiments; no per-subprogram numeric breakdown is provided except that archives of sizes 10 vs 15/20/25 influenced performance.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>EP is naive and can produce many subprograms; the paper notes that larger archives (e.g., accumulating 25 subprograms) can hinder adaptive selection and cause negative transfer, indicating a tradeoff between archive size (potential novelty source) and executability/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>program synthesis, KDPS (automatic construction of knowledge archives)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human Partitioning (HP) where humans manually split solutions into subprograms</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>EP provides a fully automated way to extract reusable subprograms from solved programs; EP+ARM outperforms baseline PushGP and is competitive with human-created partitions, but suffers when archives grow large or contain irrelevant subprograms (negative transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1603.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1603.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Subprogram Archive (SA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subprogram Archive</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stored collection of subprograms (contiguous subsequences of Push code) extracted from previously solved programs used as a knowledge base for cross-task transfer via ARM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Subprogram Archive (SA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SA contains subprograms extracted (via EP or human partitioning) from previously-solved problems; each archived subprogram k carries an adaptive quality counter Q_k initialized to 0 and updated during ARM-driven searches when the subprogram improves parents. The archive is used as the reservoir of building blocks for Replacement Mutation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Effectiveness of SA items measured indirectly by Q_k counts (number of times a subprogram produced an improved offspring). System-level executability measured by training error and success counts when SA is used with ARM.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Archives constructed automatically via EP and used by ARM improved training performance and convergence relative to no-archive baselines; however, archive size and relevance affect outcomes (larger archives can reduce selection efficiency and cause negative transfer). No exhaustive per-archive numeric data is provided beyond overall success counts and qualitative observations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>Paper documents that increasing archive size (adding five subprograms per solved problem) increased the total archive and made adaptive selection less effective; archives from unrelated tasks cause negative transfer—qualitative evidence of tradeoffs between potential novelty (more building blocks) and executability/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>program synthesis (KDPS)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No-archive PushGP, human-curated archives (PushGP+HP+ARM)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automatically constructed subprogram archives can accelerate synthesis and improve performance, but archive quality and manageability (filtering/adaptation) are critical; naive accumulation of subprograms reduces adaptive selection efficacy and may lead to negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1603.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1603.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lexicase selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lexicase selection (down-sampled variant used)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A selection algorithm that considers test cases in random order and selects individuals that perform best on a subset of cases, promoting specialists and maintaining behavioral diversity; the paper uses lexicase selection (down-sampled variant) in PushGP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explaining and exploiting the advantages of down-sampled lexicase selection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Lexicase selection (down-sampled)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Lexicase selection ranks/selects parents by filtering the population on randomly ordered training cases: retain only individuals with best performance on the current case and continue through cases until one or a few remain. Down-sampled lexicase uses a random subset of cases per selection event to reduce computational cost and increase selection pressure variability.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Selection pressure is based on case-by-case performance (error vector); no dedicated novelty metric is used though lexicase implicitly encourages behavioral diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td>Used as the parent selection mechanism for PushGP in all experiments; contributes to observed differences in convergence but no isolated quantitative effect is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>behavioral diversity (implicit via lexicase selection dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>standard tournament or fitness-proportionate selection (not directly compared here)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Lexicase selection is part of the PushGP configuration used here and supports retaining diverse specialists, complementing ARM's archive-driven variation; the paper uses lexicase but focuses on archive/mutation contributions rather than selection mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Incorporating sub-programs as knowledge in program synthesis by pushgp and adaptive replacement mutation <em>(Rating: 2)</em></li>
                <li>Program synthesis using uniform mutation by addition and deletion <em>(Rating: 2)</em></li>
                <li>Autoconstructive evolution: Push, pushgp, and pushpop <em>(Rating: 2)</em></li>
                <li>Getting a head start on program synthesis with genetic programming <em>(Rating: 1)</em></li>
                <li>Genetic source sensitivity and transfer learning in genetic programming <em>(Rating: 1)</em></li>
                <li>On domain knowledge and novelty to improve program synthesis performance with grammatical evolution <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1603",
    "paper_id": "paper-252118966",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "PushGP",
            "name_full": "Push Genetic Programming",
            "brief_description": "A genetic programming system that evolves programs written in the Turing-complete Push language using population-based search, selection (lexicase), and mutation operators; programs are sequences of Push instructions executed by multiple typed stacks.",
            "citation_title": "Autoconstructive evolution: Push, pushgp, and pushpop",
            "mention_or_use": "use",
            "system_name": "PushGP",
            "system_description": "PushGP evolves populations of candidate programs represented as sequences of Push instructions. Execution follows Push semantics (multiple typed stacks). Search is driven by selection (lexicase selection in this paper) and variation operators (uniform mutation by addition and deletion — UMAD — and Adaptive Replacement Mutation when an archive is available). Fitness is measured on I/O examples (minimize aggregate error across cases). The implementation used is based on Pyshgp and Helmuth et al.'s PushGP variants (down-sampled lexicase selection, UMAD).",
            "input_type": "programs",
            "crossover_operation": null,
            "mutation_operation": "Primarily uniform mutation by addition and deletion (UMAD): randomly add or delete instructions with configured rates (in experiments addition rate = 0.09, deletion rate = 0.0826). In configurations using knowledge, Adaptive Replacement Mutation (ARM) may be applied instead of UMAD with probability r_arm (0.1); ARM replaces a random partition of a parent with a subprogram from an archive (see ARM entry for full details).",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Task error computed as sum of ||p(in_i)-out_i|| across N I/O examples (Equation (1)); comparator for ordering individuals uses count of solved cases (count of zeros in the error vector). A 'successful run' is one that passes all I/O cases on both training and testing sets.",
            "executability_results": "Across experiments PushGP+EP+ARM achieved lower training error and higher test success counts than plain PushGP (e.g., in Experiment II Order 2 on CSL: PushGP+EP+ARM achieved 16 successful runs vs PushGP's 8), but many of the higher success counts were not statistically significant under Fisher's exact test.",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": "Not explicitly quantified; paper discusses negative transfer and possible overfitting when archives are used inappropriately, indicating a qualitative tradeoff where added knowledge (from archives) can speed convergence but harm generalization/executability on some problems.",
            "frontier_characterization": null,
            "benchmark_or_domain": "program synthesis (PS), Knowledge-Driven Program Synthesis (KDPS)",
            "comparison_baseline": "PushGP (original without archives), PushGP+HP+ARM (using human-extracted subprograms)",
            "key_findings": "PushGP augmented with automatic archive extraction (EP) and ARM yields significantly lower training error, faster convergence, and often higher test success counts vs baseline PushGP; automatic archives perform comparably but slightly worse than human-partitioned archives in some measures. Larger archives and irrelevant archived subprograms can cause negative transfer and reduced effectiveness.",
            "uuid": "e1603.0",
            "source_info": {
                "paper_title": "Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "ARM",
            "name_full": "Adaptive Replacement Mutation",
            "brief_description": "A mutation operator that replaces a randomly chosen segment of a parent program with a subprogram drawn from a subprogram archive, using an adaptive selection scheme that favors subprograms which previously improved fitness.",
            "citation_title": "Incorporating sub-programs as knowledge in program synthesis by pushgp and adaptive replacement mutation",
            "mention_or_use": "use",
            "system_name": "Adaptive Replacement Mutation (ARM)",
            "system_description": "ARM integrates an external archive of subprograms into PushGP's variation. For a parent program (length l1) and a chosen subprogram (length l2), ARM replaces a random partition of the parent of length l2 with the subprogram; if l1 &lt; l2 the whole parent is replaced. ARM maintains a quality counter Q_k per subprogram k, incremented when replacement yields an improved individual; subprograms are selected either uniformly at random or by proportional selection according to Q_k (p(k) = Q_k / ΣQ). Configuration parameters used: r_arm (probability to attempt ARM) = 0.1, r_prop (probability to use proportional selection) = 0.5.",
            "input_type": "programs",
            "crossover_operation": null,
            "mutation_operation": "Replacement Mutation (RM): choose subprogram k from archive K (via uniform or proportional selection), pick a random contiguous partition of the parent matching subprogram length, and substitute; if subprogram contains inputs unavailable in current problem, map/replace those inputs with a random valid input for current problem. Adaptive mechanism updates Q_k whenever RM produces an offspring with strictly better fitness than the parent.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Same fitness / executability as PushGP: count of I/O cases solved and aggregate error. ARM judges improvement by comparing offspring and parent via their error vectors; Q_k increments when offspring is better.",
            "executability_results": "ARM combined with EP (PushGP+EP+ARM) reduces training error and speeds convergence versus plain PushGP; in multiple experiments this led to higher test success counts (though many differences not statistically significant). No global numeric novelty/diversity metrics reported. Specific numeric example: in one ordering (Experiment II, Order 2) CSL problem, PushGP+EP+ARM had 16 successful runs vs PushGP's 8.",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": "Paper reports that a larger subprogram archive can make it harder for ARM's adaptive selection to find helpful subprograms (reducing effectiveness) and that archives from unrelated tasks may cause negative transfer — qualitatively a tradeoff between leveraging knowledge (speed) and preserving generalizable executability.",
            "frontier_characterization": null,
            "benchmark_or_domain": "program synthesis (PS), KDPS",
            "comparison_baseline": "UMAD-only mutation (PushGP), PushGP+HP+ARM (human-partitioned subprogram archive)",
            "key_findings": "ARM enables automatic reuse of subprograms to accelerate convergence and improve training error; its proportional adaptive selection (based on Q_k) preferentially reuses subprograms that have empirically improved parents. Effectiveness depends on archive quality and size—poor or large archives can induce negative transfer and slower/brittle performance.",
            "uuid": "e1603.1",
            "source_info": {
                "paper_title": "Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "RM",
            "name_full": "Replacement Mutation (component of ARM)",
            "brief_description": "A concrete mutation operator that replaces a contiguous partition of a parent program with a chosen subprogram from an archive; part of the ARM mechanism.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Replacement Mutation (RM)",
            "system_description": "RM takes a parent program (length l1) and a subprogram (length l2) and replaces a randomly chosen contiguous partition of the parent of length l2 with the subprogram; if the subprogram is longer than the parent (l2 &gt; l1), the parent is entirely replaced. RM is deterministic given the chosen partition and subprogram, and included as the core variation step within ARM.",
            "input_type": "programs",
            "crossover_operation": null,
            "mutation_operation": "Partition replacement: choose a partition uniformly at random from the parent matching the length of the chosen subprogram and substitute. If subprogram refers to input variables not present in the target task, those inputs are substituted with random valid inputs from the current task.",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Improvement judged by comparing offspring fitness to parent (error vector / number of solved I/O cases); RM increments subprogram quality counters in ARM only when immediate improvement occurs.",
            "executability_results": "RM serves as the variation operator that enabled observed faster convergence and lower training error when combined with archive selection (as part of ARM). No separate numerical evaluation solely for RM is provided.",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": "RM can inject larger functional blocks (subprograms) into parents which can rapidly change behavior (increasing novelty) but risks breaking generalization if subprograms are irrelevant to the current task (reducing executability); the paper discusses such negative transfer qualitatively.",
            "frontier_characterization": null,
            "benchmark_or_domain": "program synthesis",
            "comparison_baseline": "UMAD mutation (addition/deletion)",
            "key_findings": "Replacement of partitions with archived subprograms can accelerate search and produce better training performance, but effectiveness relies on archive relevance and adaptive selection to avoid repeated use of unhelpful subprograms.",
            "uuid": "e1603.2",
            "source_info": {
                "paper_title": "Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "UMAD",
            "name_full": "Uniform Mutation by Addition and Deletion",
            "brief_description": "A simple mutation operator for PushGP that randomly adds or deletes instructions uniformly throughout a program, with specified per-instruction probabilities.",
            "citation_title": "Program synthesis using uniform mutation by addition and deletion",
            "mention_or_use": "use",
            "system_name": "Uniform Mutation by Addition and Deletion (UMAD)",
            "system_description": "UMAD selects mutation sites uniformly and either inserts an instruction (from instruction set) or deletes an existing instruction according to configured probabilities. In this paper UMAD is the baseline mutation operator used by PushGP when ARM is not applied.",
            "input_type": "programs",
            "crossover_operation": null,
            "mutation_operation": "Addition: insert a randomly chosen instruction at a random position with probability (experimentally set to 0.09). Deletion: remove a randomly selected instruction with probability (experimentally set to 0.0826).",
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Same program fitness metrics as PushGP (aggregate I/O error; number of solved cases). UMAD is fallback mutation when ARM does not improve offspring.",
            "executability_results": "Used as the alternative mutation when ARM either is not selected or fails to improve; baseline PushGP using UMAD showed slower convergence and higher training error than PushGP+EP+ARM in experiments.",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": "UMAD produces smaller local edits compared to RM (lower immediate novelty but less disruptive risk), which can lead to slower exploration but potentially more stable refinement — the paper shows UMAD-only runs converge slower than ARM-augmented runs.",
            "frontier_characterization": null,
            "benchmark_or_domain": "program synthesis",
            "comparison_baseline": "ARM (replacement with stored subprograms)",
            "key_findings": "UMAD alone is a weaker variation operator in these experiments than ARM augmented with automatically-extracted subprograms; UMAD remains important as fallback to ensure local search and prevent dependence on archive subprograms.",
            "uuid": "e1603.3",
            "source_info": {
                "paper_title": "Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "EP",
            "name_full": "Even Partitioning",
            "brief_description": "An automatic subprogram extraction method that divides a simplified solution program into n contiguous parts of (nearly) equal length to create subprograms for a knowledge archive.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Even Partitioning (EP)",
            "system_description": "EP takes a solved program, simplifies it by removing redundant instructions (those without enough arguments), then divides the program into n contiguous partitions of equal or near-equal instruction-length to produce subprograms which are stored in the Subprogram Archive for later reuse via ARM. EP is fully automatic and requires no human labor.",
            "input_type": "programs",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Subprograms are evaluated indirectly by ARM through Q_k counters that count how often a subprogram yields an improved offspring; EP itself has no intrinsic fitness metric.",
            "executability_results": "EP-generated archives (used with ARM) produced significantly lower training error and faster convergence than PushGP alone and performance comparable (but slightly worse) to human-partitioned archives in experiments; no per-subprogram numeric breakdown is provided except that archives of sizes 10 vs 15/20/25 influenced performance.",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": "EP is naive and can produce many subprograms; the paper notes that larger archives (e.g., accumulating 25 subprograms) can hinder adaptive selection and cause negative transfer, indicating a tradeoff between archive size (potential novelty source) and executability/generalization.",
            "frontier_characterization": null,
            "benchmark_or_domain": "program synthesis, KDPS (automatic construction of knowledge archives)",
            "comparison_baseline": "Human Partitioning (HP) where humans manually split solutions into subprograms",
            "key_findings": "EP provides a fully automated way to extract reusable subprograms from solved programs; EP+ARM outperforms baseline PushGP and is competitive with human-created partitions, but suffers when archives grow large or contain irrelevant subprograms (negative transfer).",
            "uuid": "e1603.4",
            "source_info": {
                "paper_title": "Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Subprogram Archive (SA)",
            "name_full": "Subprogram Archive",
            "brief_description": "A stored collection of subprograms (contiguous subsequences of Push code) extracted from previously solved programs used as a knowledge base for cross-task transfer via ARM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Subprogram Archive (SA)",
            "system_description": "SA contains subprograms extracted (via EP or human partitioning) from previously-solved problems; each archived subprogram k carries an adaptive quality counter Q_k initialized to 0 and updated during ARM-driven searches when the subprogram improves parents. The archive is used as the reservoir of building blocks for Replacement Mutation.",
            "input_type": "programs",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Effectiveness of SA items measured indirectly by Q_k counts (number of times a subprogram produced an improved offspring). System-level executability measured by training error and success counts when SA is used with ARM.",
            "executability_results": "Archives constructed automatically via EP and used by ARM improved training performance and convergence relative to no-archive baselines; however, archive size and relevance affect outcomes (larger archives can reduce selection efficiency and cause negative transfer). No exhaustive per-archive numeric data is provided beyond overall success counts and qualitative observations.",
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": "Paper documents that increasing archive size (adding five subprograms per solved problem) increased the total archive and made adaptive selection less effective; archives from unrelated tasks cause negative transfer—qualitative evidence of tradeoffs between potential novelty (more building blocks) and executability/generalization.",
            "frontier_characterization": null,
            "benchmark_or_domain": "program synthesis (KDPS)",
            "comparison_baseline": "No-archive PushGP, human-curated archives (PushGP+HP+ARM)",
            "key_findings": "Automatically constructed subprogram archives can accelerate synthesis and improve performance, but archive quality and manageability (filtering/adaptation) are critical; naive accumulation of subprograms reduces adaptive selection efficacy and may lead to negative transfer.",
            "uuid": "e1603.5",
            "source_info": {
                "paper_title": "Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Lexicase selection",
            "name_full": "Lexicase selection (down-sampled variant used)",
            "brief_description": "A selection algorithm that considers test cases in random order and selects individuals that perform best on a subset of cases, promoting specialists and maintaining behavioral diversity; the paper uses lexicase selection (down-sampled variant) in PushGP.",
            "citation_title": "Explaining and exploiting the advantages of down-sampled lexicase selection",
            "mention_or_use": "use",
            "system_name": "Lexicase selection (down-sampled)",
            "system_description": "Lexicase selection ranks/selects parents by filtering the population on randomly ordered training cases: retain only individuals with best performance on the current case and continue through cases until one or a few remain. Down-sampled lexicase uses a random subset of cases per selection event to reduce computational cost and increase selection pressure variability.",
            "input_type": "programs",
            "crossover_operation": null,
            "mutation_operation": null,
            "uses_literature": false,
            "uses_code": true,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Selection pressure is based on case-by-case performance (error vector); no dedicated novelty metric is used though lexicase implicitly encourages behavioral diversity.",
            "executability_results": "Used as the parent selection mechanism for PushGP in all experiments; contributes to observed differences in convergence but no isolated quantitative effect is reported in this paper.",
            "diversity_metric": "behavioral diversity (implicit via lexicase selection dynamics)",
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "program synthesis",
            "comparison_baseline": "standard tournament or fitness-proportionate selection (not directly compared here)",
            "key_findings": "Lexicase selection is part of the PushGP configuration used here and supports retaining diverse specialists, complementing ARM's archive-driven variation; the paper uses lexicase but focuses on archive/mutation contributions rather than selection mechanics.",
            "uuid": "e1603.6",
            "source_info": {
                "paper_title": "Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Incorporating sub-programs as knowledge in program synthesis by pushgp and adaptive replacement mutation",
            "rating": 2,
            "sanitized_title": "incorporating_subprograms_as_knowledge_in_program_synthesis_by_pushgp_and_adaptive_replacement_mutation"
        },
        {
            "paper_title": "Program synthesis using uniform mutation by addition and deletion",
            "rating": 2,
            "sanitized_title": "program_synthesis_using_uniform_mutation_by_addition_and_deletion"
        },
        {
            "paper_title": "Autoconstructive evolution: Push, pushgp, and pushpop",
            "rating": 2,
            "sanitized_title": "autoconstructive_evolution_push_pushgp_and_pushpop"
        },
        {
            "paper_title": "Getting a head start on program synthesis with genetic programming",
            "rating": 1,
            "sanitized_title": "getting_a_head_start_on_program_synthesis_with_genetic_programming"
        },
        {
            "paper_title": "Genetic source sensitivity and transfer learning in genetic programming",
            "rating": 1,
            "sanitized_title": "genetic_source_sensitivity_and_transfer_learning_in_genetic_programming"
        },
        {
            "paper_title": "On domain knowledge and novelty to improve program synthesis performance with grammatical evolution",
            "rating": 1,
            "sanitized_title": "on_domain_knowledge_and_novelty_to_improve_program_synthesis_performance_with_grammatical_evolution"
        }
    ],
    "cost": 0.014659249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives
Sep 2022</p>
<p>Yifan He he.yifan.xs@alumni.tsukuba.ac.jp 
University of Tsukuba Tsukuba
Japan</p>
<p>Claus Aranha caranha@cs.tsukuba.ac.jp 
University of Tsukuba Tsukuba
Japan</p>
<p>Tetsuya Sakurai sakurai@cs.tsukuba.ac.jp 
University of Tsukuba Tsukuba
Japan</p>
<p>Knowledge-Driven Program Synthesis via Adaptive Replacement Mutation and Auto-constructed Subprogram Archives
Sep 2022Index Terms-program synthesisPushGPadaptive replace- ment mutationsubprogram archive
We introduce Knowledge-Driven Program Synthesis (KDPS) as a variant of the program synthesis task that requires the agent to solve a sequence of program synthesis problems. In KDPS, the agent should use knowledge from the earlier problems to solve the later ones. We propose a novel method based on PushGP to solve the KDPS problem, which takes subprograms as knowledge. The proposed method extracts subprograms from the solution of previously solved problems by the Even Partitioning (EP) method and uses these subprograms to solve the upcoming programming task using Adaptive Replacement Mutation (ARM). We call this method PushGP+EP+ARM. With PushGP+EP+ARM, no human effort is required in the knowledge extraction and utilization processes. We compare the proposed method with PushGP, as well as a method using subprograms manually extracted by a human. Our PushGP+EP+ARM achieves better train error, success count, and faster convergence than PushGP. Additionally, we demonstrate the superiority of PushGP+EP+ARM when consecutively solving a sequence of six program synthesis problems.</p>
<p>I. INTRODUCTION</p>
<p>Program Synthesis (PS) are techniques that automatically compose computer programs to solve a certain task. PS is useful in fields such as automatic bug fixing, automatic program completion, and low-level code development. PS is a key issue in Artificial General Intelligence [1]. Genetic Programming (GP) [2] is an Evolutionary Algorithm that searches computer programs by selecting and updating a population of program candidates. Some GP variants [3]- [5] can solve problems in a famous PS benchmark suite [6] efficiently. However, the difference between GP and a human programmer is still obvious. As an Evolutionary Algorithm, GP heavily utilizes random sampling; while a human programmer does not write random programs. Humans write programs based on their knowledge, either the domain knowledge about the problem or the programming skills from previous experiences.</p>
<p>Recently, several studies [7]- [11] have attempted to incorporate knowledge in PS, improving the synthesis performance. However, some of these methods have drawbacks in requiring extra information [7], [8] and human efforts [11].</p>
<p>In our prior study, we proposed the Adaptive Replacement Mutation (ARM) [11]. The ARM is a mutation method designed for a well-known GP variant called PushGP [3]. ARM uses subprograms from an archive as knowledge, automatically selecting useful subprograms from the archive according to the search history. Although the ARM provides a way to use existing knowledge from an archive, the archive itself was made by a human. Moreover, it is questionable whether the subprograms written by humans are included in the programs generated by PushGP.</p>
<p>In this study, we focus on the task where an agent is required to solve a sequence of PS problems. The agent should learn knowledge from each problem in the sequence and apply this knowledge to improve its performance in later problems. Ideally, this procedure should not require human interference or extra external information. We call this task the Knowledge-Driven Program Synthesis (KDPS) problem (Section III).</p>
<p>We propose a novel method to solve the KDPS problem based on PushGP [3]. This method takes subprograms as knowledge. The proposed method consecutively solves programming tasks, extracts subprograms from the solution of solved problems, and uses subprograms to solve an upcoming problem. To extract subprograms, we propose Even Partitioning (EP) which divides a solution into several parts with equal lengths. To use these subprograms, we apply ARM [11]. We name our method PushGP+EP+ARM. The details of the proposed methods, including EP and ARM, are given in Section IV.</p>
<p>We analyze the proposed method in two KDPS tasks. The "composite task" (Section V) includes three "composite" PS problems. For each problem, PushGP+EP+ARM prepares the knowledge archive based on the component problems. The "sequential task" (Section VI) has six problems that must be solved in sequence, including the composite and component problems of the previous "composite task". PushGP+EP+ARM updates the knowledge archive at each step of the sequence. PushGP+EP+ARM achieves a better overall success rate and convergence speed in the composite task, and in the later stages of the sequential task, showing that it can create a useful knowledge archive. However, the comparison with a human-curated archive shows that there is still room for improvement.</p>
<p>Our main contributions are as follows. 1) We introduce a new type of task called the KDPS problem. KDPS includes a sequence of single PS problems. The agent is required to solve the single PS problems, extract knowledge, and use it in the later PS problems. 2) We propose EP to extract subprograms from the solution of a solved problem. We propose PushGP+EP+ARM to solve KDPS problems based on EP and our previous work on ARM [11]. 3) We provide our implementation of the proposed method based on PyshGP [12] and experimental scripts in an online repository 1</p>
<p>II. BACKGROUND</p>
<p>A. Program Synthesis PS, also known as automatic programming, focuses on building an intelligent agent that writes computer programs to solve specific tasks with minimal human effort. Programs are sequences of instructions and a task could be described as a set of I/O examples. For instance, a task that adds two integers could be describes as {[in= (1,1), out=2], [in= (1,2), out=3], ...}.</p>
<p>Therefore, PS can be formalized as the optimization problem in (1), which searches a sequence of instructions to minimize the difference between actual and expected program outputs. In (1), p is a program while P instr includes all feasible programs based on the instruction set instr; in i and out i are the i-th I/O example.
min p∈Pinstr Σ N i=1 ||p(in i ) − out i ||(1)</p>
<p>B. PushGP</p>
<p>Since Koza's first work on his tree-based GP [2], several variants of GP [3], [13]- [15] have been applied to solve PS problems. Among these methods, we highlight PushGP [3], [5], [14], which generates programs based on a Turingcomplete language called Push.</p>
<p>Push uses the list to store a program and runs it using multiple stacks of different data types based on the following rules.</p>
<p>1) To execute an instruction, the interpreter pops the required arguments from the corresponding stacks. 2) After executed the instruction, the results are pushed to the corresponding stacks. 3) If the interpreter cannot find enough arguments from the stacks, the instruction will be skipped. Based on the third rule, any sequence of Push instructions can form a valid Push program. Helmuth et al. proposed a variant of PushGP using lexicase selection and uniform mutation by addition and deletion (UMAD) [3]. This variant was further improved by applying down-sampled lexicase selection [5]. 1 https://github.com/Y1fanHE/ssci2022</p>
<p>C. Incorporating knowledge in Program Synthesis</p>
<p>An obvious difference between GP and human programmers is that a human can learn knowledge from his experiences and use knowledge in upcoming tasks. Several recent works have attempted to incorporate knowledge in PS [7]- [11], [16]. Some studies require extra information such as text description [7] and human-written code [8], [11], [16].</p>
<p>Helmuth et al. proposed to transfer the instructions from the solution of other problems to construct the instruction set of PushGP [9]. Wick et al. proposed to use the whole individuals of a problem as a part of the initial population when solving similar problems [10]. Both studies [9], [10] extract knowledge from the solutions of the problems that have been solved by GP.</p>
<p>Recently, we came up with a mutation method that allows using subprograms in a prepared archive with PushGP [11]. Compared with the study by Helmuth [9], a subprogram can capture more information from the solution than a single instruction. Compared to Wick's study [10], the way to use an external archive of knowledge might be more efficient when the number of the problems to transfer is large. However, in our prior study [11], the subprogram archives are extracted by hand from human-written solutions. This step is non-trivial and requires a lot of human effort. Also, considering the case where a GP is able to extract knowledge from programs it wrote in the past, it is questionable whether the source programs would look similar to human written ones.</p>
<p>III. KNOWLEDGE-DRIVEN PROGRAM SYNTHESIS</p>
<p>Human programmers can learn from the programming problems that they have solved before and apply what they have learned to solve the upcoming problems. We suggest that the ability of learning is vital for generating complex programs.</p>
<p>Therefore, we introduce a type of task where an agent is required to solve a sequence of PS problems. Usually, these PS problems are related to each other. When solving a new problem, the agent should use the knowledge learned from previously solved problems. We call this type of task Knowledge-Driven Program Synthesis (KDPS).</p>
<p>The formalization of KDPS is presented in (2). S j is one of the M PS problems in (1) to solve, containing N j pairs of I/O.p j and K j are the solution program and the knowledge that learned from solving S j , respectively. S j is solved by Solve(·) based on the knowledge from the previously solved problems K j−1 ∪ . . . K 0 . K 0 is the initial knowledge before solving the first problem S 1 . By default, K 0 is empty.
        p j , K j ← Solve(S j |K j−1 ∪ · · · ∪ K 0 ) s.t. S = {S 1 , . . . , S M } S j = {(in j 1 , out j 1 ), . . . , (in j Nj , out j Nj )} K 0 = ∅ (2)
To learn knowledge from solving a problem, it is not necessary to use its solution program. However, as an initial step, we assume all the knowledge from solving S j are extracted from its solution programsp j as in (3).
K j ← Extract(p j )(3)</p>
<p>IV. PROPOSED METHOD</p>
<p>To solve KDPS problems, we propose a method based on PushGP [3] to consecutively solve programming tasks, extract knowledge from the solutions, and utilize knowledge in the next problem. This method is entirely automated.</p>
<p>We use subprograms, the sub-sequences of Push instructions, as knowledge. The subprograms hold partial information about the original program. Moreover, any sequence of Push instructions is valid to run. Therefore, we can easily take a subprogram and use it in a different program. Fig. 1 illustrates an example of solving a KDPS problem (a sequence of three PS problems) with our proposed method. Our method solves Problem 1 with PushGP [3] and extracts Subprogram Archive (SA) 1 from its solution (Program 1) with Even Partitioning (EP). This archive is used by Adaptive Replacement Mutation (ARM) [11] when searching for the solution of Problem 2. Similarly, we use SA 1 and SA 2 when solving Problem 3. The details of EP and ARM are provided in the next two subsections.</p>
<p>A. Even Partitioning</p>
<p>EP is a simple method that divides a program into n parts with equal lengths. For example, a program with 15 instructions is divided into subprograms with lengths of (3, 3, 3, 3, 3) if n = 5; the same program is divided into subprograms with lengths of (4, 4, 4, 3) if n = 4.</p>
<p>Before dividing the solution program, a simplification operation is performed to remove the redundant instructions (i.e., instructions without enough arguments to execute). After the dividing step, the subprograms are stored into an archive for the future use.</p>
<p>Algorithm 1 PushGP with Adaptive Replacement Mutation</p>
<p>Require: subprogram archive K where every subprogram k holds a quality metric Q k = 0; 1: P ← initialize population(); 2: while termination criteria is not satisfied do 3: P ′ ← ∅; 4: for i ← 1 to |P | do 5: p ← lexicase selection(P ); 6: if rand() &lt; r arm then 7: if rand() &lt; r prop then 8: k ← proportional selection(K); 9: else 10: k ← random selection(K); 11: p ′ ← replacement mutation(p, k);</p>
<p>12:
if f (p ′ ) ≺ f (p) then 13: Q k ← Q k + 1; 14:
else 15: p ′ ← umad mutation(p); 16:
P ′ ← P ′ ∪ {p ′ }; 17: P ← P ′ ;</p>
<p>B. Adaptive Replacement Mutation</p>
<p>ARM [11] is a mutation method proposed for PushGP, which incorporates a prepared archive of subprograms during the search. ARM contains a Replacement Mutation (RM) and an adaptive strategy.</p>
<p>RM requires a parent candidate (of length l 1 ) from the PushGP population and a subprogram (of length l 2 ) from the prepared archive. RM replaces a random partition (of length l 2 ) of the parent candidate using the subprogram. If l 1 &lt; l 2 , the entire parent is replaced by the subprogram.</p>
<p>The adaptive strategy in ARM is designed to automatically select helpful subprograms when an archive contains both helpful and unhelpful subprograms. The idea is similar to parameter adaptation in many Self-adaptive Evolutionary Algorithms such as JADE [17]. Originally, ARM collects and stores the helpful subprograms that improve the parent candidates by RM into a working archive. This working archive, which consists of helpful subprograms, is used as one of the sources to select subprograms in the later generations.</p>
<p>In this study, we use a slightly different implementation of ARM from the original [11], using proportional selection to select subprograms rather than collecting subprograms in an extra archive. The proportion to select a subprogram depends on how many times that it improves parents.</p>
<p>We provide the pseudo code of PushGP (using lexicase selection and UMAD mutation) with ARM in Algorithm 1. Q k is the count that a subprogram k improves the parents during the search. The probability in proportional selection is computed as in (4).
p(k) = Q k Σ ki∈K Q ki(4)
r arm is the probability to perform ARM and r prop is the probability to perform the proportional selection of subpro-grams. In Line 12 of Algorithm 1, the symbol "≺" means "better than". In our implementation, a solution is better than another if it solves more I/O cases (i.e., contains more "0" in its error vector).</p>
<p>In some cases, the subprograms may include more inputs than the current problem (e.g., a subprogram contains input_3 while the current problem only takes two inputs). We replace the input in the subprograms with a random input of the current problem.</p>
<p>V. EXPERIMENT I: COMPOSITE KDPS TASK</p>
<p>In this experiment, we focus on an intermediate step of the KDPS problem (Fig. 1). That is, to solve a composite problem with our proposed method after solving its sub-problems.</p>
<p>A. Comparison methods</p>
<p>We compare the following three methods.</p>
<p>• PushGP+EP+ARM: PushGP with ARM; the subprogram archives are extracted using EP. • PushGP+HP+ARM: PushGP with ARM; the subprogram archives are extracted by human (HP: human partitioning). • PushGP: the original PushGP [3].</p>
<p>B. Experimental procedures</p>
<p>We use PushGP [3] to solve three problems in PSB1 [6]. They are "small or large" (SL), "compare string lengths" (CSL), and "median" (MD). We take the best and shortest solutions among 25 runs (after 5000 steps of simplification) of the three problems to generate subprogram archives. For PushGP+EP+ARM, We use EP to get five equal-length subprograms for every best and shortest solution automatically. The subprograms used by PushGP+HP+ARM are partitions of the same solutions, however, devised by a human.</p>
<p>We then solve the composite problems of SL, CSL, and MD. When solving a composite problem, PushGP+EP+ARM and PushGP+HP+ARM will use subprogram archives generated from solutions of the corresponded sub-problems by EP and HP, respectively. We compare the three methods on three composite problems. MDSLEN is the composite problem of MD and CSL; SLMD is composed from SL and MD; SLSTR is a composite of SL and CSL.</p>
<p>C. Parameter settings</p>
<p>For all three methods, we use a population size of 1000 and a maximum generation of 300. The UMAD mutation in all three methods is set with addition rate of 0.09 and deletion rate of 0.0826 based on Helmuth's study [3]. For PushGP+EP+ARM and PushGP+HP+ARM, the rate to perform ARM r arm is 0.1 and the rate to perform the proportional selection of subprograms r prop is 0.5 based on the original paper of ARM [11]. For every algorithm, we run 25 repetitions on every problem. Fig. 2 presents the error in the training period of the three methods in 25 runs. The value on a line segment is the p-value of the Wilcoxon rank sum test between two groups. The p-value is marked with an asterisk and red color if the difference between two groups is significant with a 95% family confidence level (i.e., the individual confidence level is computed byŠidák correction 2 ). The proposed PushGP+EP+ARM outperforms the original PushGP with a significant difference in the training error. However, compared with PushGP+HP+ARM, the difference is not statistically significant. Fig. 3 shows the success counts in the test period of the three comparison methods in 25 runs. We count a run as a successful run only when it passes all I/O cases in both training and testing data. In Fig. 3, the number above a bar is the success count; the value on a line segment is the pvalue of Fisher's exact test between two groups. The p-value is marked with an asterisk and red color if the difference between two groups is significant with a 95% family confidence level (i.e., the individual confidence level is computed byŠidák correction 2 ). Compared to PushGP, our PushGP+EP+ARM achieves higher success counts, however, without statistical significance. Compared to the method using human-made subprograms (PushGP+HP+ARM), our proposed method achieves a lower success count on MDSLEN, a higher success count on SLMD, and an equal success count on SLSTR. However, these differences are not significant.</p>
<p>D. Experimental results</p>
<p>We provide a comparison of the best train error by generations in Fig. 4. PushGP+EP+ARM holds a much faster convergence speed compared to PushGP; however, it is slightly slower than PushGP+HP+ARM.</p>
<p>In the case of solving the sub-problems and then the composite problems, PushGP+EP+ARM achieves a better performance in train error, success count, and faster convergence, compared to the original PushGP [3]. However, its performance in both success count and convergence speed is worse than PushGP+HP+ARM without statistical significance.</p>
<p>VI. EXPERIMENT II: SEQUENTIAL KDPS TASK</p>
<p>In this second experiment, we test the entire KDPS process in Fig. 1. That is, using PushGP+ARM+EP to solve a sequence of problems. They are the six problems in the last experiment, namely SL, CSL, MD, MDSLEN, SLMD, and SLSTR. Every time a problem is solved, we extract subprograms from its solution and store them in the archive. This archive will be used when solving the next problem.</p>
<p>A. Comparison methods</p>
<p>• PushGP+EP+ARM: solving a sequence of PS problems with PushGP+EP+ARM in the way as in Fig. 1; every time a problem is solved, the subprograms are extracted by EP from its solution and added to the archive. This archive is used by PushGP+ARM when solving the next problem. • PushGP: solving a sequence of PS problems independently using PushGP [3].</p>
<p>B. Experimental procedures</p>
<p>We solve six problems, namely MD, CSL, SL, MDSLEN, SLMD, and SLSTR. The first three problems are from PSB1 [6]. They do not share any sub-problems. The last three problems are the composite problems of MD, CSL, and SL (Section V). Any pair of the three composite problems share a sub-problem.</p>
<p>For PushGP+EP+ARM, we run a procedure as in Fig. 1. We solve the first problem (MD) with the original PushGP (i.e., PushGP+ARM with an empty archive) and the rest problems with PushGP+ARM. We initialize an empty subprogram archive when solving the first problem. For every problem, we run 25 repetitions and take the best and shortest program. We use EP to extract five subprograms from the best and shortest programs. These subprograms are stored in the archive that we initialized before and used in solving the later problems by PushGP+ARM. For PushGP, we solve the six problems independently in the same order with PushGP+EP+ARM. However, no subprograms are stored and used.</p>
<p>We provide results of solving the six problems in two different orders. Order 1 solves simple problems at first and later harder ones while Order 2 is a reverse order of Order 1.</p>
<p>• Order 1: MD→CSL→SL→MDSLEN→SLMD→SLSTR • Order 2: SLSTR→SLMD→MDSLEN→SL→CSL→MD We use the same parameter settings as in Section V. We present the results and the statistical test in a similar manner as in Section V.</p>
<p>C. Experimental results of Order 1</p>
<p>In Fig. 5, PushGP+EP+ARM holds a significantly lower train error than PushGP on the three composite problems, while the difference on the rest three problems is not significant. Fig. 6 shows the test success count of PushGP+EP+ARM and PushGP. The test success count of PushGP+EP+ARM is lower than PushGP on CSL and SL but higher than PushGP on MSDLEN, SLMD, and SLSTR. All these difference is not statistically significant through Fisher's exact test. On MD, the difference between the two methods is very small since it is solved by two equivalent methods. Fig. 7 provides the best error in the population by generations of both methods. It is obvious that PushGP+EP+ARM converges faster than PushGP on all problems except MD.</p>
<p>D. Experimental results of Order 2</p>
<p>In Fig. 8, PushGP+EP+ARM holds a lower train error than PushGP on all problems without statistical significance. Fig. 9 shows the test success count of PushGP+EP+ARM and PushGP. The test success count of PushGP+EP+ARM is higher than PushGP on on most of the problems except MD. Especially on CSL, PushGP+EP+ARM gets 16 success while PushGP only gets 8. However, all these difference is not statistically significant through Fisher's exact test. Fig. 10 provides the best error in the population by generations of both methods. It is obvious that PushGP+EP+ARM converges faster than PushGP on all problems except SLMD.</p>
<p>Therefore, when solving a sequence of problems, PushGP+EP+ARM achieves a better optimization performance (i.e., train error and convergence speed). This performance finally leads to a higher test success count, however, without statistical significance.</p>
<p>E. Discussion</p>
<p>We find that the PushGP+EP+ARM in Section V is better than the PushGP+EP+ARM in Section VI with Order 1, in terms of test success on the three composite problems. Though their algorithms are the same, they have at least two differences.</p>
<p>1) In Section VI, PushGP+EP+ARM adds five subprograms to the archive after solving one problem. Therefore, the size of the archive is 15, 20, and 25 when solving MDSLEN, SLMD, and SLSTR, respectively. However, in Section V, PushGP+EP+ARM uses archives with 10 subprograms (five for one sub-problem). A larger subprogram archive makes it harder to select helpful subprograms by the adaptive strategy in Algorithm 1.</p>
<p>2) CSL and SL are solved in different conditions in the two experiments. In Section VI, CSL is solved with subprograms from MD; SL is solved with subprograms from MD and CSL. However, no subprogram is used when solving MD, CSL, and SL in Section V (i.e., they are solved by the original PushGP [3]). Moreover, MD, CSL, and SL do not share the same sub-problems. Therefore, solving CSL and SL with PushGP+EP+ARM is not as good as with the original PushGP (as shown in the first three subfigures of Fig. 6). Thus, the subprograms extracted in Section VI is not as good as in Section V. These subprograms further influence the performance of PushGP+EP+ARM in Section VI in solving the later problems MDSLEN, SLMD, and SLSTR. This issue is called "negative transfer". In Section VI (problems in Order 2), we find that PushGP+EP+ARM holds a lower success count on MD. However, on MD, the training error of all runs with PushGP+EP+ARM is 0 (Fig. 8). Moreover, Fig. 10 shows PushGP+EP+ARM converges much faster than PushGP. This observation may indicate an over-fitting issue with the proposed method.</p>
<p>VII. CONCLUSIONS</p>
<p>In this study, we introduced a problem called Knowledge-Driven Program Synthesis (KDPS) problem. KDPS requires an agent to solve a sequence of related PS problems. To solve KDPS, we proposed a method based on PushGP [3]. This method consecutively solves programming tasks, extracts subprograms from the solutions as knowledge, and uses these subprograms to solve the next problem. To extract subprograms from the solution of a solved problem, we proposed the Even Partitioning (EP) method; to use these subprograms, we applied Adaptive Replacement Mutation (ARM) [11].</p>
<p>We compared our proposed method (PushGP+EP+ARM) with the original PushGP [3] and a method extracting subprograms by humans (PushGP+HP+ARM). Our PushGP+EP+ARM achieved a significantly better train error, success count, and convergence speed than PushGP. The performance of our proposed method is slightly worse than PushGP+HP+ARM. We further compared our PushGP+EP+ARM with the original PushGP in solving a sequence of problems. Our method achieved a better train error and convergence speed. Our PushGP+EP+ARM also holds a higher test success count, however, without statistical significance.</p>
<p>The current method to automatically construct the subprogram archive is rather naive. We would like to improve this method in our future work. In the discussion section in Section VI, PushGP+EP+ARM has limitations in dealing with the growing subprogram archive after solving more problems. Moreover, PushGP+EP+ARM also suffers from the "negative transfer" of the subprograms from the unrelated problems. A part of our future work is to fix these issues. Methods such as a more efficient adaptation or filtering strategy on subprograms are promising to solve the two limitations. Additionally, the strategy of extracting and using knowledge could be applied to problems other than program synthesis, such as training soft robotics [18].  . Experiment II, Order 2: Test success count in 25 runs. The number above a bar is the success count. The value on a line segment is the pvalue of Fisher's exact test between two groups. The p-value is marked with an asterisk and red color if the difference between two groups is significant. PushGP+EP+ARM holds a higher success count than PushGP on SLSTR, SLMD, MDSLEN, SL, and CSL; however, without statistical significance. </p>
<p>Fig. 1 .
1An example of solving Knowledge-Driven Program Synthesis with the proposed method. The method solves problems with PushGP, extract subprograms with Even Partitioning (EP), and uses subprograms with Adaptive Replacement Mutation (ARM).</p>
<p>•
Median String Length (MDSLEN): given 3 strings, print the median of their lengths. • Small or Large Median (SLMD): given 4 integers a, b, c, d, print "small" if median(a,b,c) &lt; d and "large" if median(a,b,c) &gt; d (and nothing if median(a,b,c) = d). • Small or Large String (SLSTR): given a string n, print "small" if len(n) &lt; 100 and "large" if len(n) ≥ 200 (and nothing if 100 ≤ len(n) &lt; 200).</p>
<p>Fig. 2 .Fig. 3 .Fig. 4 .
234Experiment I: Train error in 25 runs. The value on a line segment is the p-value of Wilcoxon rank sum test between two groups. The p-value is marked with an asterisk and red color if the difference between two groups is significant. PushGP+ARM+EP holds a significantly lower train error than PushGP and a comparable train error compared to PushGP+ARM+HP. Experiment I: Test success count in 25 runs. The number above a bar is the success count. The value on a line segment is the p-value of Fisher's exact test between two groups. The p-value is marked with an asterisk and red color if the difference between two groups is significant. PushGP+ARM+EP holds higher success counts than PushGP but without statistical significance. Experiment I: Average of the best train error in the population by generations. PushGP+ARM+EP converges much faster than PushGP but slightly slower (SLSTR) or at the similar speed (MDSLEN and SLMD) compared to PushGP+ARM+HP.</p>
<p>Fig. 5 .Fig. 6 .Fig. 7 .
567Experiment II, Order 1: Train error in 25 runs. The value on a line segment is the p-value of Wilcoxon rank sum test between two groups. The p-value is marked with an asterisk and red color if the difference between two groups is significant. PushGP+EP+ARM holds a significantly lower train error than PushGP on MDSLEN, SLMD, and SLSTR. Experiment II, Order 1: Test success count in 25 runs. The number above a bar is the success count. The value on a line segment is the pvalue of Fisher's exact test between two groups. The p-value is marked with an asterisk and red color if the difference between two groups is significant. PushGP+EP+ARM holds a higher success count than PushGP on MD, MDSLEN, SLMD, and SLSTR; however, without statistical significance. Experiment II, Order 1: Average of the best train error in the population by generations. PushGP+EP+ARM converges much faster than PushGP on the five problems except MD.</p>
<p>Fig. 8 .Fig. 9
89Experiment II, Order 2: Train error in 25 runs. The value on a line segment is the p-value of Wilcoxon rank sum test between two groups. The p-value is marked with an asterisk and red color if the difference between two groups is significant. PushGP+EP+ARM holds a lower train error than PushGP on all the problems without statistical significance.</p>
<p>Fig. 10 .
10Experiment II, Order 2: Average of the best train error in the population by generations. PushGP+EP+ARM converges much faster than PushGP on the five problems except SLMD.
© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
The individual confidence levels in Section V and Section VI are 99.4% and 99.1%, respectively.</p>
<p>Automatic programming: The open issue. M O&apos;neill, L Spector, Genetic Programming and Evolvable Machines. 21M. O'Neill and L. Spector, "Automatic programming: The open issue?" Genetic Programming and Evolvable Machines, vol. 21, no. 1, pp. 251- 262, 2020.</p>
<p>On the programming of computers by means of natural selection. J Koza, Genetic programming. J. Koza, "On the programming of computers by means of natural selection," Genetic programming, 1992.</p>
<p>Program synthesis using uniform mutation by addition and deletion. T Helmuth, N F Mcphee, L Spector, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation ConferenceT. Helmuth, N. F. McPhee, and L. Spector, "Program synthesis using uniform mutation by addition and deletion," in Proceedings of the Genetic and Evolutionary Computation Conference, 2018, pp. 1127- 1134.</p>
<p>Improving genetic programming with novel exploration-exploitation control. J Kelly, E Hemberg, U.-M O&apos;reilly, European Conference on Genetic Programming. SpringerJ. Kelly, E. Hemberg, and U.-M. O'Reilly, "Improving genetic pro- gramming with novel exploration-exploitation control," in European Conference on Genetic Programming. Springer, 2019, pp. 64-80.</p>
<p>Explaining and exploiting the advantages of down-sampled lexicase selection. T Helmuth, L Spector, ALIFE 2020: The 2020 Conference on Artificial Life. MIT PressT. Helmuth and L. Spector, "Explaining and exploiting the advantages of down-sampled lexicase selection," in ALIFE 2020: The 2020 Conference on Artificial Life. MIT Press, 2020, pp. 341-349.</p>
<p>General program synthesis benchmark suite. T Helmuth, L Spector, Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation. the 2015 Annual Conference on Genetic and Evolutionary ComputationT. Helmuth and L. Spector, "General program synthesis benchmark suite," in Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, 2015, pp. 1039-1046.</p>
<p>On domain knowledge and novelty to improve program synthesis performance with grammatical evolution. E Hemberg, J Kelly, U.-M O&apos;reilly, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation ConferenceE. Hemberg, J. Kelly, and U.-M. O'Reilly, "On domain knowledge and novelty to improve program synthesis performance with grammatical evolution," in Proceedings of the Genetic and Evolutionary Computation Conference, 2019, pp. 1039-1046.</p>
<p>Teaching gp to program like a human software developer: using perplexity pressure to guide program synthesis approaches. D Sobania, F Rothlauf, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation ConferenceD. Sobania and F. Rothlauf, "Teaching gp to program like a human software developer: using perplexity pressure to guide program synthesis approaches," in Proceedings of the Genetic and Evolutionary Computa- tion Conference, 2019, pp. 1065-1074.</p>
<p>Genetic source sensitivity and transfer learning in genetic programming. T Helmuth, E Pantridge, G Woolson, L Spector, ALIFE 2020: The 2020 Conference on Artificial Life. MIT PressT. Helmuth, E. Pantridge, G. Woolson, and L. Spector, "Genetic source sensitivity and transfer learning in genetic programming," in ALIFE 2020: The 2020 Conference on Artificial Life. MIT Press, 2020, pp. 303-311.</p>
<p>Getting a head start on program synthesis with genetic programming. J Wick, E Hemberg, U.-M O&apos;reilly, European Conference on Genetic Programming. SpringerPart of EvoStarJ. Wick, E. Hemberg, and U.-M. O'Reilly, "Getting a head start on program synthesis with genetic programming," in European Conference on Genetic Programming (Part of EvoStar). Springer, 2021, pp. 263- 279.</p>
<p>Incorporating sub-programs as knowledge in program synthesis by pushgp and adaptive replacement mutation. Y He, C Aranha, T Sakurai, Proceedings of the Genetic and Evolutionary Computation Conference Companion. the Genetic and Evolutionary Computation Conference CompanionY. He, C. Aranha, and T. Sakurai, "Incorporating sub-programs as knowledge in program synthesis by pushgp and adaptive replacement mutation," in Proceedings of the Genetic and Evolutionary Computation Conference Companion, 2022.</p>
<p>Pyshgp: Pushgp in python. E Pantridge, L Spector, Proceedings of the Genetic and Evolutionary Computation Conference Companion. the Genetic and Evolutionary Computation Conference CompanionE. Pantridge and L. Spector, "Pyshgp: Pushgp in python," in Proceedings of the Genetic and Evolutionary Computation Conference Companion, 2017, pp. 1255-1262.</p>
<p>Grammatical evolution. M O&apos;neill, C Ryan, IEEE Transactions on Evolutionary Computation. 54M. O'Neill and C. Ryan, "Grammatical evolution," IEEE Transactions on Evolutionary Computation, vol. 5, no. 4, pp. 349-358, 2001.</p>
<p>Autoconstructive evolution: Push, pushgp, and pushpop. L Spector, Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-2001). the Genetic and Evolutionary Computation Conference (GECCO-2001)137L. Spector, "Autoconstructive evolution: Push, pushgp, and pushpop," in Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-2001), vol. 137, 2001.</p>
<p>Extending program synthesis grammars for grammar-guided genetic programming. S Forstenlechner, D Fagan, M Nicolau, M O&apos;neill, International Conference on Parallel Problem Solving from Nature. SpringerS. Forstenlechner, D. Fagan, M. Nicolau, and M. O'Neill, "Extending program synthesis grammars for grammar-guided genetic programming," in International Conference on Parallel Problem Solving from Nature. Springer, 2018, pp. 197-208.</p>
<p>Evolution through large models. J Lehman, J Gordon, S Jain, K Ndousse, C Yeh, K O Stanley, arXiv:2206.08896arXiv preprintJ. Lehman, J. Gordon, S. Jain, K. Ndousse, C. Yeh, and K. O. Stanley, "Evolution through large models," arXiv preprint arXiv:2206.08896, 2022.</p>
<p>Jade: adaptive differential evolution with optional external archive. J Zhang, A C Sanderson, IEEE Transactions on evolutionary computation. 135J. Zhang and A. C. Sanderson, "Jade: adaptive differential evolution with optional external archive," IEEE Transactions on evolutionary computation, vol. 13, no. 5, pp. 945-958, 2009.</p>
<p>Evolving modular soft robots without explicit inter-module communication using local selfattention. F Pigozzi, Y Tang, E Medvet, D Ha, arXiv:2204.06481arXiv preprintF. Pigozzi, Y. Tang, E. Medvet, and D. Ha, "Evolving modular soft robots without explicit inter-module communication using local self- attention," arXiv preprint arXiv:2204.06481, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
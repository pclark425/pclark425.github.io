<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8245 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8245</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8245</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-268667163</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.14982v2.pdf" target="_blank">MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts</a></p>
                <p><strong>Paper Abstract:</strong> Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 - which provides a dataset of puzzles for testing natural language understanding. We employ large language models (LLMs) to solve this task through several prompting techniques. Zero-shot and few-shot prompting generate reasonably good results when tested with proprietary LLMs, compared to the open-source models. We obtain further improved results with chain-of-thought prompting, an iterative prompting method that breaks down the reasoning process step-by-step. We obtain our best results by utilizing an ensemble of chain-of-thought prompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle subtask. The strong performance of prompted LLMs demonstrates their capability for complex reasoning when provided with a decomposition of the thought process. Our work sheds light on how step-wise explanatory prompts can unlock more of the knowledge encoded in the parameters of large models.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8245.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8245.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large language model from OpenAI accessed via API; used as the top-performing model in this work and evaluated across zero-shot, few-shot, chain-of-thought, and ensemble-of-CoT prompting conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary OpenAI large transformer model (GPT-4 family variant) accessed via API; described in the paper as the most capable model tested and able to leverage in-context chain-of-thought prompts and ensemble voting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot prompting', 'chain-of-thought prompting (CoT)', 'ensemble of chain-of-thought prompts (majority voting across diverse CoTs)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Zero-shot: present puzzle and choices with a role assignment and task description (no examples). Few-shot: include 4 solved training examples before a test puzzle. Chain-of-thought (CoT): provide worked-out step-by-step reasoning examples (CoT) and then prompt the model to produce a reasoning chain and answer; implemented at 2-shot, 4-shot, and 8-shot CoT. Ensemble of CoT: run multiple CoT promptings across different random subsets of training questions (8 random questions per prompt, repeated 5 times) and aggregate predictions via majority voting to increase robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared zero-shot, few-shot, and CoT prompting across 0, 2, 4, and 8 shots. For diversity, constructed an ensemble by prompting components on different random training-question subsets (8 different random questions selected per prompt, repeated 5 times) and aggregated outputs by majority vote to combine diverse CoT runs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>BrainTeaser (SemEval-2024 Task 9) — multiple-choice lateral-thinking puzzles with two subtasks: sentence puzzles (627) and word puzzles (492); human baseline 0.91.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Best reported: eight-shot chain-of-thought prompting combined with ensemble ([E]) achieved test set scores: Sentence = 0.93, Word = 0.95. Summary: zero-shot lagged; performance improved with more shots and with CoT; ensemble gave further improvement and robustness. (Human baseline: 0.91 both.)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Chain-of-thought prompting significantly boosted performance relative to zero-shot and basic few-shot; increasing the number of in-context examples improved accuracy; ensemble voting across diverse CoT prompt sets reduced sensitivity to spurious patterns and noise. The authors also note that experimenting with several different CoT variants yielded mostly similar results, motivating the ensemble to increase empirical confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT prompting and larger numbers of shots unlock latent reasoning ability in GPT4 Turbo; an ensemble of varied CoT prompts (majority voting) further improves robustness and yields the best performance, surpassing human baseline on the evaluated BrainTeaser subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8245.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8245.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 2.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 2.1 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large language model from Anthropic accessed via API; evaluated with zero-shot, few-shot, chain-of-thought prompting and ensemble-of-CoT, showing clear gains from CoT and ensembles though trailing GPT4 Turbo on some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 2.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary Anthropic large language model accessed via API; used in experiments to compare prompting strategies (zero/few-shot, CoT) and ensemble aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot prompting', 'chain-of-thought prompting (CoT)', 'ensemble of chain-of-thought prompts (majority voting)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Zero-shot: single-puzzle prompts with role/task description. Few-shot: include solved examples in-context. Chain-of-thought: supply step-by-step reasoning exemplars (2-, 4-, 8-shot) and prompt model to generate reasoning then answer. Ensemble of CoT: run multiple CoT prompts over different random training subsets (8 random questions per prompt, repeated 5 times) and aggregate by majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Same comparative setup as other models: zero/few-shot vs CoT at increasing shot counts; ensemble created by prompting with different random training-question subsets and majority-voting predictions to introduce diversity across CoT runs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>BrainTeaser (SemEval-2024 Task 9) — sentence and word multiple-choice lateral-thinking puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Best reported: eight-shot chain-of-thought prompting with ensemble achieved test set scores: Sentence = 0.86, Word = 0.95. Overall Claude 2.1 improved with CoT and more shots; its Word score matched GPT4 Turbo at 0.95 in the best condition while Sentence lagged behind GPT4 Turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Chain-of-thought prompting and increased shots improved Claude 2.1's performance; ensemble aggregation boosted robustness. The paper notes model-specific differences and suggests model-specific prompt adjustments may matter.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT prompting and ensembles significantly improve Claude 2.1's puzzle-solving; model-specific tuning can influence outcomes, and ensembles mitigate issues from single-model spurious pattern latch-ons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8245.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8245.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral (open-source Mixtral of experts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source large language model evaluated as a lower-performing baseline in the paper; benefits from few-shot and chain-of-thought prompting and from ensemble aggregation but remains behind proprietary models in absolute performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mixtral of experts.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source model (referred to as Mixtral of experts in the paper) used for comparison; described as state-of-the-art among open models in several reasoning tasks but inferior to proprietary models on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['zero-shot prompting', 'few-shot prompting', 'chain-of-thought prompting (CoT)', 'ensemble of chain-of-thought prompts (majority voting)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Zero-shot and few-shot prompting similar to other models; CoT implemented with step-by-step exemplars at 2-, 4-, and 8-shot. Ensemble: multiple CoT prompt runs over random training-question subsets with majority voting for final prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Same comparative setup as other models: evaluated across 0–8 shots, compared regular vs CoT prompting; used an ensemble of CoT runs (8 random training questions per prompt, repeated 5 times) to combine diverse reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>BrainTeaser (SemEval-2024 Task 9), sentence and word puzzle subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Best reported: eight-shot chain-of-thought prompting with ensemble achieved test set scores: Sentence = 0.88, Word = 0.82. Performance improves steadily with more shots and CoT, but Mixtral remained below GPT4 Turbo and Claude 2.1 on some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Mixtral benefits from CoT and more in-context examples similarly to proprietary models, but absolute accuracy is lower; ensemble helps but cannot fully close the gap with stronger proprietary models.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Open-source models like Mixtral improve with CoT prompting and ensemble aggregation, showing the generality of the approach, though proprietary models achieved higher absolute performance on BrainTeaser.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
                <li>BRAINTEASER: Lateral thinking puzzles for large language models. <em>(Rating: 2)</em></li>
                <li>Semeval-2024 task 9: Brainteaser: A novel task defying common sense. <em>(Rating: 2)</em></li>
                <li>Tree of uncertain thoughts reasoning for large language models. <em>(Rating: 1)</em></li>
                <li>Mixtral of experts. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8245",
    "paper_id": "paper-268667163",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "GPT4 Turbo",
            "name_full": "GPT-4 Turbo (OpenAI)",
            "brief_description": "A proprietary large language model from OpenAI accessed via API; used as the top-performing model in this work and evaluated across zero-shot, few-shot, chain-of-thought, and ensemble-of-CoT prompting conditions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT4 Turbo",
            "model_description": "Proprietary OpenAI large transformer model (GPT-4 family variant) accessed via API; described in the paper as the most capable model tested and able to leverage in-context chain-of-thought prompts and ensemble voting.",
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot prompting",
                "chain-of-thought prompting (CoT)",
                "ensemble of chain-of-thought prompts (majority voting across diverse CoTs)"
            ],
            "reasoning_methods_description": "Zero-shot: present puzzle and choices with a role assignment and task description (no examples). Few-shot: include 4 solved training examples before a test puzzle. Chain-of-thought (CoT): provide worked-out step-by-step reasoning examples (CoT) and then prompt the model to produce a reasoning chain and answer; implemented at 2-shot, 4-shot, and 8-shot CoT. Ensemble of CoT: run multiple CoT promptings across different random subsets of training questions (8 random questions per prompt, repeated 5 times) and aggregate predictions via majority voting to increase robustness.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared zero-shot, few-shot, and CoT prompting across 0, 2, 4, and 8 shots. For diversity, constructed an ensemble by prompting components on different random training-question subsets (8 different random questions selected per prompt, repeated 5 times) and aggregated outputs by majority vote to combine diverse CoT runs.",
            "task_or_benchmark": "BrainTeaser (SemEval-2024 Task 9) — multiple-choice lateral-thinking puzzles with two subtasks: sentence puzzles (627) and word puzzles (492); human baseline 0.91.",
            "performance_results": "Best reported: eight-shot chain-of-thought prompting combined with ensemble ([E]) achieved test set scores: Sentence = 0.93, Word = 0.95. Summary: zero-shot lagged; performance improved with more shots and with CoT; ensemble gave further improvement and robustness. (Human baseline: 0.91 both.)",
            "qualitative_findings": "Chain-of-thought prompting significantly boosted performance relative to zero-shot and basic few-shot; increasing the number of in-context examples improved accuracy; ensemble voting across diverse CoT prompt sets reduced sensitivity to spurious patterns and noise. The authors also note that experimenting with several different CoT variants yielded mostly similar results, motivating the ensemble to increase empirical confidence.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "CoT prompting and larger numbers of shots unlock latent reasoning ability in GPT4 Turbo; an ensemble of varied CoT prompts (majority voting) further improves robustness and yields the best performance, surpassing human baseline on the evaluated BrainTeaser subtasks.",
            "uuid": "e8245.0",
            "source_info": {
                "paper_title": "MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Claude 2.1",
            "name_full": "Claude 2.1 (Anthropic)",
            "brief_description": "A proprietary large language model from Anthropic accessed via API; evaluated with zero-shot, few-shot, chain-of-thought prompting and ensemble-of-CoT, showing clear gains from CoT and ensembles though trailing GPT4 Turbo on some metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude 2.1",
            "model_description": "Proprietary Anthropic large language model accessed via API; used in experiments to compare prompting strategies (zero/few-shot, CoT) and ensemble aggregation.",
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot prompting",
                "chain-of-thought prompting (CoT)",
                "ensemble of chain-of-thought prompts (majority voting)"
            ],
            "reasoning_methods_description": "Zero-shot: single-puzzle prompts with role/task description. Few-shot: include solved examples in-context. Chain-of-thought: supply step-by-step reasoning exemplars (2-, 4-, 8-shot) and prompt model to generate reasoning then answer. Ensemble of CoT: run multiple CoT prompts over different random training subsets (8 random questions per prompt, repeated 5 times) and aggregate by majority vote.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Same comparative setup as other models: zero/few-shot vs CoT at increasing shot counts; ensemble created by prompting with different random training-question subsets and majority-voting predictions to introduce diversity across CoT runs.",
            "task_or_benchmark": "BrainTeaser (SemEval-2024 Task 9) — sentence and word multiple-choice lateral-thinking puzzles.",
            "performance_results": "Best reported: eight-shot chain-of-thought prompting with ensemble achieved test set scores: Sentence = 0.86, Word = 0.95. Overall Claude 2.1 improved with CoT and more shots; its Word score matched GPT4 Turbo at 0.95 in the best condition while Sentence lagged behind GPT4 Turbo.",
            "qualitative_findings": "Chain-of-thought prompting and increased shots improved Claude 2.1's performance; ensemble aggregation boosted robustness. The paper notes model-specific differences and suggests model-specific prompt adjustments may matter.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "CoT prompting and ensembles significantly improve Claude 2.1's puzzle-solving; model-specific tuning can influence outcomes, and ensembles mitigate issues from single-model spurious pattern latch-ons.",
            "uuid": "e8245.1",
            "source_info": {
                "paper_title": "MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Mixtral",
            "name_full": "Mixtral (open-source Mixtral of experts)",
            "brief_description": "An open-source large language model evaluated as a lower-performing baseline in the paper; benefits from few-shot and chain-of-thought prompting and from ensemble aggregation but remains behind proprietary models in absolute performance.",
            "citation_title": "Mixtral of experts.",
            "mention_or_use": "use",
            "model_name": "Mixtral",
            "model_description": "Open-source model (referred to as Mixtral of experts in the paper) used for comparison; described as state-of-the-art among open models in several reasoning tasks but inferior to proprietary models on this dataset.",
            "reasoning_methods": [
                "zero-shot prompting",
                "few-shot prompting",
                "chain-of-thought prompting (CoT)",
                "ensemble of chain-of-thought prompts (majority voting)"
            ],
            "reasoning_methods_description": "Zero-shot and few-shot prompting similar to other models; CoT implemented with step-by-step exemplars at 2-, 4-, and 8-shot. Ensemble: multiple CoT prompt runs over random training-question subsets with majority voting for final prediction.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Same comparative setup as other models: evaluated across 0–8 shots, compared regular vs CoT prompting; used an ensemble of CoT runs (8 random training questions per prompt, repeated 5 times) to combine diverse reasoning outputs.",
            "task_or_benchmark": "BrainTeaser (SemEval-2024 Task 9), sentence and word puzzle subtasks.",
            "performance_results": "Best reported: eight-shot chain-of-thought prompting with ensemble achieved test set scores: Sentence = 0.88, Word = 0.82. Performance improves steadily with more shots and CoT, but Mixtral remained below GPT4 Turbo and Claude 2.1 on some metrics.",
            "qualitative_findings": "Mixtral benefits from CoT and more in-context examples similarly to proprietary models, but absolute accuracy is lower; ensemble helps but cannot fully close the gap with stronger proprietary models.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Open-source models like Mixtral improve with CoT prompting and ensemble aggregation, showing the generality of the approach, though proprietary models achieved higher absolute performance on BrainTeaser.",
            "uuid": "e8245.2",
            "source_info": {
                "paper_title": "MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "BRAINTEASER: Lateral thinking puzzles for large language models.",
            "rating": 2,
            "sanitized_title": "brainteaser_lateral_thinking_puzzles_for_large_language_models"
        },
        {
            "paper_title": "Semeval-2024 task 9: Brainteaser: A novel task defying common sense.",
            "rating": 2,
            "sanitized_title": "semeval2024_task_9_brainteaser_a_novel_task_defying_common_sense"
        },
        {
            "paper_title": "Tree of uncertain thoughts reasoning for large language models.",
            "rating": 1,
            "sanitized_title": "tree_of_uncertain_thoughts_reasoning_for_large_language_models"
        },
        {
            "paper_title": "Mixtral of experts.",
            "rating": 1,
            "sanitized_title": "mixtral_of_experts"
        }
    ],
    "cost": 0.008990749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thought Prompts</p>
<p>MdNishat Raihan mraihan2@gmu.edu 
George Mason University
USA</p>
<p>Dhiman Goswami 
George Mason University
USA</p>
<p>Al Nahian 
George Mason University
USA</p>
<p>Bin Emran 
George Mason University
USA</p>
<p>Sadiya Sayara 
George Mason University
USA</p>
<p>Chowdhury Puspo 
George Mason University
USA</p>
<p>Amrita Ganguly 
George Mason University
USA</p>
<p>Marcos Zampieri 
George Mason University
USA</p>
<p>MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thought Prompts
D01CED17D8EA51B0E982833630E6FF2E
This paper presents the MasonTigers' submission to the SemEval-2024 Task 9 which provides a dataset of puzzles for testing natural language understanding.We employ large language models (LLMs) to solve this task through several prompting techniques.We show that zero-shot and few-shot prompting with proprietary LLMs outperform open-source models.Results are further improved with chain-ofthought prompting.We obtain our best results by utilizing an ensemble of chain-of thought prompts, ranking 2 nd in the word puzzle subtask and 13 th in the sentence puzzle sub-task.</p>
<p>Introduction</p>
<p>In recent years, LLMs have achieved impressive performance on several question answering and language understanding tasks when provided with appropriate prompting (Brown et al., 2020).However, complex reasoning abilities often present a challenge for these models.SemEval-2024 Task 9 (Jiang et al., 2024b) introduces a novel dataset called BrainTeaser (Jiang et al., 2023) which includes a set of complex puzzles and brainteasers.Such tasks involve solving word and sentence puzzles, which require multi-step inference and deduction.The dataset covers a diverse range of puzzle types including sequences, analogies, classification, mathematical reasoning, inferences about implicit relationships, and more.Solutions frequently demand a chained application of knowledge and logic across multiple steps to uncover insights or concepts not directly stated in the problem description.</p>
<p>Solving these elaborate reasoning problems is a challenging scenario for NLP systems.We explore whether and how LLMs can succeed on this task.We employ proprietary models such as GPT-4 (OpenAI, 2023) and Claude 2.1 (Anthropic, 2023) through APIs.These models have shown promising few-shot reasoning ability.We also use Mixtral (Jiang et al., 2024a), an open-source LLM that shows state-of-the-results in several language reasoning tasks.The prompting paradigm involves providing models with natural language descriptions that encode the reasoning process step-bystep (Liu et al., 2021).We implement various prompting approaches for mapping puzzles to conditional text and systematically transforming reasoning into explanation chains.Our core method, chain-of-thought prompting (Wei et al., 2022), iteratively breaks down the deduction into simplified logical steps.</p>
<p>Experiments reveal that while zero-shot performance lags due to a lack of grounding, multi-step prompts can unlock substantial reasoning ability in models.Performance improves with more steps and more specificity in the prompt.While introducing few-shot prompting generates good results, we observed that models do significantly better with chain-of-thought prompting.We experiment with several chains of thought and achieve mostly similar results with each attempt.To make a more empirically confident guess towards solving the puzzles we adopt an ensemble of these chains based on majority voting.Our approach achieves competitive performance, ranking 2nd on the word puzzle subtask and 13th on sentence puzzles.</p>
<p>Related Work</p>
<p>LLMs have been widely used for complex and challenging language processing tasks recently (Raihan et al., 2023a,b;Goswami et al., 2023).They have shown good reasoning abilities in several tasks.The task of solving puzzles and the BrainTeaser dataset (Jiang et al., 2023) represent both a novel task and a novel dataset respectively.Similarly to their multiple choice questions (MCQs) approach, a few datasets like MathQA (Austin et al., 2021), have been compiled.However, these are intended for specific tasks in which domain knowledge is usually enough thus they not requiring deep reason-arXiv:2403.14982v2[cs.CL] 3 Apr 2024 ing.A similar work is done by Saeedi et al. (2020) where they investigate a task that combines natural language understanding and commonsense reasoning.They present deep learning architectures for distinguishing between sensible and nonsensical statements.</p>
<p>Pun detection by (Zou and Lu, 2019) is a puzzlelike activity that is similar to BrainTeaser.It presents a method for joint pun detection and localization utilizing a sequence labeling perspective.This highlights the complexity of language comprehension, especially in detecting subtle wordplay.Another dataset, LatEval is curated by Huang et al. (2023) that delves further into lateral thinking and commonsense reasoning, highlighting the challenges faced by language models in tasks requiring unconventional thinking and creativity.Zhou et al. (2023) presents ROME, a dataset designed to assess vision-language models' capacity to reason beyond intuitive understanding, highlighting the shortcomings of existing models in understanding events that defy common sense.</p>
<p>In the field of reasoning task, a chain-of-thought (Wei et al., 2022) implies a logical sequence of connected ideas, fostering coherence and depth in responses.On the other hand, a tree-of-thought suggests branching out into various related ideas, offering a more comprehensive exploration of a topic.While few-shot prompting is effective for some tasks by providing examples to guide the model, it may have limitations in capturing the complexity of nuanced conversations.The optimal choice may involve a hybrid approach, where a few-shot prompt sets the initial (Yao et al., 2023) context, and the model subsequently follows a chain or tree of thought to generate more contextually rich and coherent responses useful for reasoning tasks.</p>
<p>Tan (2023) shows the performance of LLM's on the reasoning of arithmetic word problems.It states that higher degrees of realization are associated with better overall accuracy on arithmetic problems.And chain-of-thought is really helpful in this aspect as it covers a variety of prompts to strengthen the reasoning.Similarly, Mo and Xin (2023) presents a new reasoning framework for large language models by addressing a gap in prior tree-based reasoning methods which overlooked inherent uncertainties in intermediate decision points made by models.Overall, the key innovation is leveraging uncertainty estimation locally within the models during tree reasoning to enable more precise problem-solving and reasoning.</p>
<p>The BrainTeaser Dataset</p>
<p>The BrainTeaser dataset (Jiang et al., 2023), introduced with the task (Jiang et al., 2024b) is a question-answering benchmark designed to evaluate models' ability for lateral thinking, i.e., to defy default commonsense associations and reason creatively.The dataset contains 1,100 multiplechoice questions divided into two sub-tasks -627 sentence-based puzzles relying on narrative context and common phrases and 492 puzzles focused on the literal form and letters of words.</p>
<p>For a fair comparison with human performance, the dataset also provides a separate human evaluation set with 102 randomly sampled questions.Each question in BrainTeaser has one correct answer and three distractor choices, including the option "none of the above".To prevent memorization of training data, the dataset also contains semantically and contextually reconstructed variants for every question while preserving the original reasoning process and answers.The key statistics of the dataset are shown in During the SemEval-2024 Task 9 development phase, a total of 240 prompts (120 for both sentence and word puzzles) are provided.During the test phase, a total of 216 prompts (120 for sentence and 96 for word puzzles) are provided.</p>
<p>Experiments</p>
<p>In our experiments, we focus on several prompting strategies by employing three state-of-the-art models including proprietary models like GPT-4 (OpenAI, 2023) and Claude 2.1 (Anthropic, 2023) (accessed via API key) and one open-source model -Mixtral (Jiang et al., 2024a).</p>
<p>Zero-Shot Prompting</p>
<p>We start with zero-shot prompting by assigning the AI a role, describing the task, and giving it one puzzle at a time, as shown in Figure 1.</p>
<p>Few-Shot Prompting</p>
<p>In order to give the LLMs more context we integrate more examples and design prompts for fewshot prompting.We include 4 solved puzzles from the train set and then attach one puzzle from the test set each time we prompt the models.We also use some tags for better extracting the generated answers, as shown in Figure 2.</p>
<p>Chain-of-Thought</p>
<p>To guide the models toward better reasoning -we experiment with chain-of-thought prompting.We give the model the puzzle, and the potential answers and work with every example one-by-one in order to choose the most reasonable one.Like the original CoT approach (Wei et al., 2022), we do not assign any role or explain the task -just pose the question, the CoT, and the answer (see Figure 3).We do this as 2-shot, 4-shot, and 8-shot for all three models.</p>
<p>Ensemble of Chain-of-Thought Prompts</p>
<p>To assess model performance, an ensemble approach is utilized with chain-of-thought prompting to make more confident guesses regarding the correct answers.Specifically, majority voting is done across an ensemble of models prompted by different question groups.For each prompt, 8 different random questions are selected from the BrainTeaser training set -repeated 5 times in total.Finally, the predictions are aggregated through voting to output the overall ensemble prediction.</p>
<p>This ensemble methodology with chain-ofthought prompting helps improve robustness to out- The brother of a beggar passed away, but the deceased had no brothers.How is that possible?Choices: 1.The beggar was the man's sister.2. The man is angry for his brother being a beggar and cut ties with him.3. The bagger's brother is a murderer.</p>
<ol>
<li>None of the above.Correct Answer: <ans5> ?</ans5> Choose the most suitable answer.Thanks.lier examples and noise compared to using a single model.By prompting the ensemble components on different random question subsets, diversity is promoted to capture a more holistic representation of the overall data distribution.The voting also helps cancel out issues with single models latching onto spurious patterns.Overall, the ensemble approach with multiple chain-of-thought prompt sets provides a robust assessment strategy suited for the open-ended nature and diversity of the BrainTeaser puzzles.</li>
</ol>
<p>Results</p>
<p>We analyze the performance of the three models -including GPT4 Turbo, Claude 2.1, and Mixtral.These models are tested with different types of prompts -regular and chain-of-thought, and with a varying number of examples, known as shots, ranging from zero to eight.Additionally, an ensem-ble method is applied to the eight-shot chain-ofthought prompting to see if it can further improve the models' performance.The results, shown in Table 2, reveal how the models performed under each condition.A human baseline with scores of 0.91 for both Sentence and Word puzzles in the test set is provided by the task organizers for comparison purposes.</p>
<p>GPT4 Turbo shows the best performance, especially with chain-of-thought prompting and an increasing number of shots.The model performs best with the eight-shot chain-of-thought prompting combined with the ensemble method ([E]), reaching the highest Sentence and Word scores of 0.93 and 0.95 in the test set, respectively.This shows that chain-of-thought prompting and the ensemble method significantly improve the model's understanding and output.Claude 2.1 also improves with chain-of-thought prompting and more shots.Its best scores were with the eight-shot chain-ofthought with the ensemble, achieving Sentence and Word scores of 0.86 and 0.95 in the test set, respectively.The asterisk (*) mark in Table 2 denotes our submission during the test phase.Even though Mixtral's performance is inferior to the performance of the other two models, it consistently gets better with more shots and chain-of-thought prompting.Mixtral delivered best results with the eight-shot chain-of-thought and the ensemble technique, with Sentence and Word scores of 0.88 and 0.82 in the test set, respectively.</p>
<p>Finally, the results highlight the effectiveness of chain-of-thought prompting in boosting the performance of LLMs.This approach, especially when combined with more examples and the ensemble method, greatly improves models' abilities to process and generate more accurate responses.GPT4 Turbo's top performance is likely due to its advanced design, which makes the most of these strategies.On the other hand, Claude 2.1's results point to the importance of model-specific adjustments.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we presented MasonTigers' approach to SemEval-2024 Task 9 on solving puzzles using LLMs.We explored various prompting strategies to guide the models, including zero-shot, few-shot, and chain-of-thought prompting.Our key method involved iteratively breaking down reasoning into simplified logical steps to decompose the complex</p>
<p>You are a helpful AI assistant.You are given the task of solving a sentence/word puzzle.+ Definition: It's a Multiple Choice Question paired with 4 potential answers.Choose the most suitable one.+ Question: What part of London is in France?Choices: 1.The letter N. 2. The letter O. 3. The letter L. 4. None of the above.Choose the most suitable answer.Thanks.</p>
<p>Figure 1 :
1
Figure 1: Sample structure for Zero-Shot Prompting.</p>
<p>Figure 2 :
2
Figure 2: Sample structure for Few-Shot Prompting.</p>
<p>Figure 3 :
3
Figure 3: Sample structure for Chain-of-Thought Prompting (8-shot).</p>
<p>Table 1 .
1
Sentence Word
Number of puzzles627492Avg. tokens (prompt)34.8810.65Avg. tokens (choices)9.113.0</p>
<p>Table 1 :
1
Key statistics of the BrainTeaser dataset in the sentence and word puzzle sub-task.</p>
<p>AcknowledgmentsWe would like to thank the shared task organizers for proposing this interesting competition and for providing participants with the BrainTeaser dataset.deduction process.Our experiments revealed promising results.While zero-shot performance was limited, providing explanatory prompts substantially improved the models' reasoning abilities.Performance increased with more prompt specificity and steps.Our best results came from an ensemble approach applying majority voting across multiple chain-of-thought prompts.Ultimately, our system achieved competitive rankings on the leaderboard, placing 2 nd in the word puzzle sub-task and 13 th on sentence puzzles.The strong capability unlocked through guided prompting highlights these models' latent reasoning potential when given a structured thought process.Our work sheds light on how explanatory chains can elicit more of the knowledge within large language model parameters.A few key limitations remain to be addressed in future work.First, constructing effective prompts requires extensive human effort and insight -automating this prompting process could improve scalability.Additionally, performance still lags behind human levels, indicating that there is room for advancement.Architectural constraints related to long-term memory and reasoning likely need to be overcome.Finally, our approach focused narrowly on the given puzzles rather than teaching broader inferential skills -developing more generalizable reasoning through prompts is an open challenge.
Claude 2.1: Updates and improvements. Anthropic, 2023</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, arXiv:2108.07732Program synthesis with large language models. 2021arXiv preprint</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Subbiah, Advances in neural information processing systems. 2020</p>
<p>OffMix-3L: A novel code-mixed test dataset in bangla-english-hindi for offensive language identification. Dhiman Goswami, Md Nishat Raihan, Antara Mahmud, Proceedings of SocialNLP (AACL). SocialNLP (AACL)2023Antonios Anastasopoulos, and Marcos Zampieri</p>
<p>Lateval: An interactive llms evaluation benchmark with incomplete information from lateral thinking puzzles. Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong Zhang, Hai-Tao Zheng, arXiv:2308.108552023arXiv preprint</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024aarXiv preprint</p>
<p>Semeval-2024 task 9: Brainteaser: A novel task defying common sense. Yifan Jiang, Filip Ilievski, Kaixin Ma, Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024). the 18th International Workshop on Semantic Evaluation (SemEval-2024)Mexico City, MexicoAssociation for Computational Linguistics2024b. 1996-2010</p>
<p>BRAINTEASER: Lateral thinking puzzles for large language models. Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati, Proceedings of EMNLP. EMNLP2023</p>
<p>What makes good in-context examples for gpt-3?. Peng Liu, Nicholas Lourie, Sean Welleck, Julia Hockenmaier, Proceedings of EMNLP. EMNLP2021</p>
<p>Tree of uncertain thoughts reasoning for large language models. Shentong Mo, Miao Xin, arXiv:2309.076942023arXiv preprint</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Antonios Anastasopoulos, and Marcos Zampieri. 2023a. SentMix-3L: A novel code-mixed test dataset in bangla-english-hindi for sentiment analysis. Md Nishat Raihan, Dhiman Goswami, Antara Mahmud, Proceedings of SEALP (AACL). SEALP (AACL)</p>
<p>Offensive language identification in transliterated and code-mixed bangla. Md Nishat Raihan, Umma Hani Tanmoy, Anika Binte Islam, Proceedings of BLP (EMNLP). BLP (EMNLP)2023b</p>
<p>Sirwe Saeedi, Aliakbar Panahi, Seyran Saeedi, Alvis C Fong, arXiv:2006.01205Cs-nlp team at semeval-2020 task 4: Evaluation of state-of-the-art nlp deep learning architectures on commonsense reasoning task. 2020arXiv preprint</p>
<p>Causal abstraction for chain-ofthought reasoning in arithmetic word problems. Tan Juanhe, Proceedings of BlackboxNLP (EMNLP). BlackboxNLP (EMNLP)2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Advances in Neural Information Processing Systems. 2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, arXiv:2305.106012023arXiv preprint</p>
<p>Rome: Evaluating pre-trained vision-language models on reasoning beyond visual common sense. Kankan Zhou, Eason Lai, Wei Bin, Au Yeong, Kyriakos Mouratidis, Jing Jiang, arXiv:2310.193012023arXiv preprint</p>
<p>Joint detection and location of English puns. Yanyan Zou, Wei Lu, Proceedings of NAACL. NAACL2019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4761 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4761</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4761</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-258947120</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.16334v1.pdf" target="_blank">OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities</a></p>
                <p><strong>Paper Abstract:</strong> In most current research, large language models (LLMs) are able to perform reasoning tasks by generating chains of thought through the guidance of specific prompts. However, there still exists a significant discrepancy between their capability in solving complex reasoning problems and that of humans. At present, most approaches focus on chains of thought (COT) and tool use, without considering the adoption and application of human cognitive frameworks. It is well-known that when confronting complex reasoning challenges, humans typically employ various cognitive abilities, and necessitate interaction with all aspects of tools, knowledge, and the external environment information to accomplish intricate tasks. This paper introduces a novel intelligent framework, referred to as OlaGPT. OlaGPT carefully studied a cognitive architecture framework, and propose to simulate certain aspects of human cognition. The framework involves approximating different cognitive modules, including attention, memory, reasoning, learning, and corresponding scheduling and decision-making mechanisms. Inspired by the active learning mechanism of human beings, it proposes a learning unit to record previous mistakes and expert opinions, and dynamically refer to them to strengthen their ability to solve similar problems. The paper also outlines common effective reasoning frameworks for human problem-solving and designs Chain-of-Thought (COT) templates accordingly. A comprehensive decision-making mechanism is also proposed to maximize model accuracy. The efficacy of OlaGPT has been stringently evaluated on multiple reasoning datasets, and the experimental outcomes reveal that OlaGPT surpasses state-of-the-art benchmarks, demonstrating its superior performance. Our implementation of OlaGPT is available on GitHub: \url{https://github.com/oladata-team/OlaGPT}.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4761.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4761.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OlaGPT (multi-template ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OlaGPT multi-thinking-template ensemble with voting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed system that runs multiple distinct Chain-of-Thought (CoT) reasoning templates (analogy, decomposition, plan, step, etc.) in parallel and aggregates answers via voting (regex majority or LLM-based voting), optionally using an Active Learning 'notes' library and retrieval strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-turbo: the chat-oriented OpenAI large language model used as the base LLM for all experiments in this paper (no model size reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Multi-template Chain-of-Thought ensemble with voting (OlaGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Constructs multiple distinct CoT templates that encode different human-like thinking styles (Analogy, Decomposition/DT/DST, Plan, Step/Sequential), executes each template as a separate 'agent' on the same question, and aggregates outputs using voting (regex majority or an LLM-based vote). Diversity arises from using different prompting templates / thinking strategies and optionally different retrieved example notes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AQuA (algebra word problems) and E-KAR (analogical reasoning, Chinese)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>AQuA: large algebraic QA-with-rationales dataset; E-KAR: interpretable analogical reasoning benchmark (Chinese version used).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>AQuA: OlaGPT (regex-vote) reported accuracies across retrieval modes with values including 0.5945, 0.6496, 0.6732, 0.6772; OlaGPT (llm-vote) reported up to 0.7047 in the best configuration. E-KAR (Chinese): OlaGPT (regex-vote) values include 0.4209, 0.4597, 0.4567, 0.4716; OlaGPT (llm-vote) values include 0.4000, 0.4418, 0.4358, 0.4507 (see Table 3 and Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Self-Consistency (SC) and Auto-CoT baselines (see paper): SC is reported to improve over the GPT-3.5-turbo baseline; OlaGPT surpasses SC in the reported experiments (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Running multiple distinct reasoning templates and aggregating their outputs via voting substantially improves accuracy over single-template or zero-shot baselines; best results achieved when combining (retrieval + notes) and using LLM-vote in some configurations. Ensemble diversity (more templates) increases the potential upper bound of performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Regex majority voting can fail when a majority of templates produce the same incorrect answer (paper notes example where 2 correct vs 3 incorrect would force regex-vote to the incorrect majority); LLM-vote may sometimes underperform the regex supremum (paper reports LLM-vote is on average lower than the regex supremum by ~0.0325 on AQuA and ~0.0485 on E-KAR).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4761.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4761.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency decoding for Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/ensemble strategy that samples multiple CoT reasoning paths from the same prompt (stochastic sampling) and marginalizes/selects the most consistent final answer across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (as evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as a decoding strategy on top of GPT-3.5-turbo to sample diverse reasoning chains and choose the most consistent answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency (sample multiple reasoning paths and vote)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generates multiple stochastic CoT traces for the same prompt (via sampling) and selects the majority/most-consistent final answer across traces (reduces variance across sampled chains).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AQuA and E-KAR (as used in paper baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks used for OlaGPT comparisons (mathematical and analogical reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that SC outperforms the raw GPT-3.5-turbo baseline (qualitative statement); detailed numeric SC values are reported in the paper's Table 3 for configurations on AQuA and E-KAR (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to GPT-3.5-turbo zero-shot (SC > turbo) and to OlaGPT (OlaGPT > SC) as reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using multiple sampled reasoning paths (self-consistency) improves performance over a single deterministic chain; however, OlaGPT's approach of ensembling different thinking templates with voting outperforms SC in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although SC improves over the base model, the paper reports that the multi-template diversity + Active Learning + voting (OlaGPT) yields higher accuracy than SC on the two evaluated datasets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4761.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4761.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Chain-of-Thought prompting methods (Auto-CoT / Auto-generated CoT demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automatic procedures that generate CoT exemplar demonstrations (e.g., by sampling diverse questions or reasoning chains) to construct in-context few-shot prompts without manual annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Chain of Thought Prompting in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (as used in paper baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-CoT methods are applied using the same base LLM to assemble demonstrations automatically; specific model size not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Auto-CoT (automatic CoT exemplar generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Automatically samples or constructs diverse question/CoT exemplars to include in few-shot prompts, encouraging diverse reasoning exemplars in-context rather than manual demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AQuA (reported in paper comparisons) and other reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks used for multi-step reasoning evaluation; AQuA is highlighted in the comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports Auto-CoT baseline values in Table 3 (e.g., one Auto-CoT configuration shows 0.5748 on AQuA in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared with GPT-3.5-turbo baseline and with OlaGPT: Auto-CoT improves over zero-shot baseline but OlaGPT outperforms Auto-CoT in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automatic prompt construction (Auto-CoT) that increases template / example diversity helps, but explicitly designing multiple distinct human-inspired thinking templates and voting (OlaGPT) produced stronger gains on the evaluated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Auto-CoT helps relative to zero-shot, but still trails the multi-template/voting approach in the reported experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4761.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4761.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-template DT (Decomposition Thinking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decomposition Thinking (DT) Chain-of-Thought template</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single CoT prompting template that decomposes a complex problem into subproblems and solves them step-by-step (used as one of OlaGPT's individual thinking templates).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as a single-template CoT run of GPT-3.5-turbo in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Decomposition Chain-of-Thought (single-template)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Uses one fixed decomposition prompt to force the model to break problems into sub-questions and solve each sequentially; executed as a single reasoning template (no ensemble diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AQuA and E-KAR</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical word problems (AQuA) and analogical reasoning (E-KAR) as in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>AQuA: DT reported accuracies across retrieval/mode variants in Table 3 roughly in the range 0.5591–0.5945 (55.91%–59.45%). E-KAR (Chinese): DT reported in Table 3 in the range ≈0.3612–0.4269 (36.12%–42.69%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to other single templates (ST, DST, PT) and to ensemble (OlaGPT), DT performs well among single templates (paper notes DT and ST relatively strong) but is outperformed by the multi-template ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decomposition (DT) is one of the more effective single templates on math tasks (AQuA); however, combining multiple templates and applying voting yields higher accuracy than any single template alone.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although DT performs well, its outputs are not fully consistent across examples and combining it with other templates in an ensemble further improves results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4761.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4761.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-template ST (Step Thinking / 'Let's think step by step')</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Step Thinking (ST) Chain-of-Thought template</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single CoT template implementing step-by-step reasoning (explicit 'let's think step by step' style prompting), used as one of the reasoning templates in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as a single fixed prompt encouraging stepwise decomposition and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Step-by-step Chain-of-Thought (single-template)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>A fixed prompt instructing the model to produce a stepwise solution chain; no variation in template during a run (hence 'similar').</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AQuA and E-KAR</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above: algebraic reasoning and analogical reasoning benchmarks used in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>AQuA: ST reported accuracies across modes roughly in the range 0.5197–0.6102 (51.97%–61.02%). E-KAR (Chinese): ST reported values roughly 0.3373–0.4388 (33.73%–43.88%) depending on retrieval strategy (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared to DT and other single templates: ST and DT are among the best single-template performers for reasoning-type questions; ensemble methods still outperform ST alone.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Step-by-step prompting is effective (especially on reasoning problems), but combining multiple distinct thinking templates and voting gives additional gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Performance of ST varies by dataset and retrieval strategy; ST is not uniformly superior in all configurations and benefits from ensemble aggregation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4761.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4761.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other single templates (DST, PT, AT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decomposition variant (DST), Plan Thinking (PT), Analogical Thinking (AT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Additional fixed CoT templates used by the authors: DST (another decomposition prompt), PT (planning-style prompt), and AT (analogy-specific template) — each executed as a single reasoning template in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as single-template prompts tailored to different thinking styles (analogy, plan, decomposition variants).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Single-template CoT variants: DST, PT, AT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Each is a single fixed Chain-of-Thought prompt encoding a specific reasoning style: DST = stepwise decomposition, PT = make a plan then execute, AT = analogical induction and mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AQuA and E-KAR</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>DST/PT mainly applied to AQuA (math); AT applied to analogical E-KAR tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Representative values from Table 3: DST on AQuA ≈ 0.5079–0.5984 depending on mode; PT on AQuA ≈ 0.5512–0.5827; AT reported for E-KAR with accuracies in the paper's table ≈0.3821–0.3910 depending on mode. (See Table 3 for mode-by-mode values.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compared against other single templates and OlaGPT ensemble: per-dataset effectiveness varies (e.g., AT relevant for analogical tasks), but ensemble voting across templates generally yields higher robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Different single templates perform differently across tasks — AT benefits analogy tasks and DST/PT/DT benefit math tasks; no single template uniformly dominates, motivating the ensemble approach.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>A single template sometimes gives lower consistency/accuracy on some datasets; effectiveness depends on task-template match and retrieval/notes strategies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4761.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4761.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo zero-shot baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (zero-shot / base LLM baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The base chat-capable large language model used as the starting point for all comparisons; runs zero-shot or with CoT prompting as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-oriented large language model from OpenAI used as the base LLM in all experiments; paper sets temperature=0 for deterministic runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot / single deterministic CoT (single reasoning path)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>No ensemble diversity: either zero-shot answers or a single deterministic chain-of-thought prompt per question.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AQuA and E-KAR (benchmarks used as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical word problems (AQuA) and analogical reasoning (E-KAR); baseline performance reported in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported zero-shot baseline on AQuA: 0.3228 accuracy (Table 3). The paper states SC and various template/ensemble methods improve over this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Many methods (SC, Auto-CoT, single CoT templates, and OlaGPT ensemble) are compared to this baseline; ensemble/diverse methods provide consistent improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Deterministic single-run zero-shot baseline is much weaker than methods that introduce reasoning-path diversity (SC, Auto-CoT) or multiple distinct prompting templates plus voting (OlaGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>None reported where zero-shot single-run outperforms the diverse/ensemble approaches on the evaluated tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models. <em>(Rating: 2)</em></li>
                <li>Automatic Chain of Thought Prompting in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. <em>(Rating: 1)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4761",
    "paper_id": "paper-258947120",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "OlaGPT (multi-template ensemble)",
            "name_full": "OlaGPT multi-thinking-template ensemble with voting",
            "brief_description": "The paper's proposed system that runs multiple distinct Chain-of-Thought (CoT) reasoning templates (analogy, decomposition, plan, step, etc.) in parallel and aggregates answers via voting (regex majority or LLM-based voting), optionally using an Active Learning 'notes' library and retrieval strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "GPT-3.5-turbo: the chat-oriented OpenAI large language model used as the base LLM for all experiments in this paper (no model size reported in the paper).",
            "reasoning_method_name": "Multi-template Chain-of-Thought ensemble with voting (OlaGPT)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Constructs multiple distinct CoT templates that encode different human-like thinking styles (Analogy, Decomposition/DT/DST, Plan, Step/Sequential), executes each template as a separate 'agent' on the same question, and aggregates outputs using voting (regex majority or an LLM-based vote). Diversity arises from using different prompting templates / thinking strategies and optionally different retrieved example notes.",
            "task_name": "AQuA (algebra word problems) and E-KAR (analogical reasoning, Chinese)",
            "task_description": "AQuA: large algebraic QA-with-rationales dataset; E-KAR: interpretable analogical reasoning benchmark (Chinese version used).",
            "performance": "AQuA: OlaGPT (regex-vote) reported accuracies across retrieval modes with values including 0.5945, 0.6496, 0.6732, 0.6772; OlaGPT (llm-vote) reported up to 0.7047 in the best configuration. E-KAR (Chinese): OlaGPT (regex-vote) values include 0.4209, 0.4597, 0.4567, 0.4716; OlaGPT (llm-vote) values include 0.4000, 0.4418, 0.4358, 0.4507 (see Table 3 and Table 8).",
            "comparison_with_other_method": true,
            "performance_other_method": "Self-Consistency (SC) and Auto-CoT baselines (see paper): SC is reported to improve over the GPT-3.5-turbo baseline; OlaGPT surpasses SC in the reported experiments (see Table 3).",
            "key_findings": "Running multiple distinct reasoning templates and aggregating their outputs via voting substantially improves accuracy over single-template or zero-shot baselines; best results achieved when combining (retrieval + notes) and using LLM-vote in some configurations. Ensemble diversity (more templates) increases the potential upper bound of performance.",
            "counter_examples_or_negative_results": "Regex majority voting can fail when a majority of templates produce the same incorrect answer (paper notes example where 2 correct vs 3 incorrect would force regex-vote to the incorrect majority); LLM-vote may sometimes underperform the regex supremum (paper reports LLM-vote is on average lower than the regex supremum by ~0.0325 on AQuA and ~0.0485 on E-KAR).",
            "uuid": "e4761.0"
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-Consistency decoding for Chain-of-Thought",
            "brief_description": "A decoding/ensemble strategy that samples multiple CoT reasoning paths from the same prompt (stochastic sampling) and marginalizes/selects the most consistent final answer across samples.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (as evaluated in this paper)",
            "model_description": "Applied as a decoding strategy on top of GPT-3.5-turbo to sample diverse reasoning chains and choose the most consistent answer.",
            "reasoning_method_name": "Self-Consistency (sample multiple reasoning paths and vote)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generates multiple stochastic CoT traces for the same prompt (via sampling) and selects the majority/most-consistent final answer across traces (reduces variance across sampled chains).",
            "task_name": "AQuA and E-KAR (as used in paper baselines)",
            "task_description": "Same benchmarks used for OlaGPT comparisons (mathematical and analogical reasoning).",
            "performance": "Paper reports that SC outperforms the raw GPT-3.5-turbo baseline (qualitative statement); detailed numeric SC values are reported in the paper's Table 3 for configurations on AQuA and E-KAR (see Table 3).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to GPT-3.5-turbo zero-shot (SC &gt; turbo) and to OlaGPT (OlaGPT &gt; SC) as reported in the paper.",
            "key_findings": "Using multiple sampled reasoning paths (self-consistency) improves performance over a single deterministic chain; however, OlaGPT's approach of ensembling different thinking templates with voting outperforms SC in the experiments reported.",
            "counter_examples_or_negative_results": "Although SC improves over the base model, the paper reports that the multi-template diversity + Active Learning + voting (OlaGPT) yields higher accuracy than SC on the two evaluated datasets.",
            "uuid": "e4761.1"
        },
        {
            "name_short": "Auto-CoT",
            "name_full": "Automatic Chain-of-Thought prompting methods (Auto-CoT / Auto-generated CoT demonstrations)",
            "brief_description": "Automatic procedures that generate CoT exemplar demonstrations (e.g., by sampling diverse questions or reasoning chains) to construct in-context few-shot prompts without manual annotation.",
            "citation_title": "Automatic Chain of Thought Prompting in Large Language Models.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (as used in paper baselines)",
            "model_description": "Auto-CoT methods are applied using the same base LLM to assemble demonstrations automatically; specific model size not provided in this paper.",
            "reasoning_method_name": "Auto-CoT (automatic CoT exemplar generation)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Automatically samples or constructs diverse question/CoT exemplars to include in few-shot prompts, encouraging diverse reasoning exemplars in-context rather than manual demonstrations.",
            "task_name": "AQuA (reported in paper comparisons) and other reasoning benchmarks",
            "task_description": "Benchmarks used for multi-step reasoning evaluation; AQuA is highlighted in the comparisons.",
            "performance": "Paper reports Auto-CoT baseline values in Table 3 (e.g., one Auto-CoT configuration shows 0.5748 on AQuA in Table 3).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared with GPT-3.5-turbo baseline and with OlaGPT: Auto-CoT improves over zero-shot baseline but OlaGPT outperforms Auto-CoT in the paper's experiments.",
            "key_findings": "Automatic prompt construction (Auto-CoT) that increases template / example diversity helps, but explicitly designing multiple distinct human-inspired thinking templates and voting (OlaGPT) produced stronger gains on the evaluated datasets.",
            "counter_examples_or_negative_results": "Auto-CoT helps relative to zero-shot, but still trails the multi-template/voting approach in the reported experiments.",
            "uuid": "e4761.2"
        },
        {
            "name_short": "Single-template DT (Decomposition Thinking)",
            "name_full": "Decomposition Thinking (DT) Chain-of-Thought template",
            "brief_description": "A single CoT prompting template that decomposes a complex problem into subproblems and solves them step-by-step (used as one of OlaGPT's individual thinking templates).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Applied as a single-template CoT run of GPT-3.5-turbo in experiments.",
            "reasoning_method_name": "Decomposition Chain-of-Thought (single-template)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Uses one fixed decomposition prompt to force the model to break problems into sub-questions and solve each sequentially; executed as a single reasoning template (no ensemble diversity).",
            "task_name": "AQuA and E-KAR",
            "task_description": "Mathematical word problems (AQuA) and analogical reasoning (E-KAR) as in the paper's experiments.",
            "performance": "AQuA: DT reported accuracies across retrieval/mode variants in Table 3 roughly in the range 0.5591–0.5945 (55.91%–59.45%). E-KAR (Chinese): DT reported in Table 3 in the range ≈0.3612–0.4269 (36.12%–42.69%).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to other single templates (ST, DST, PT) and to ensemble (OlaGPT), DT performs well among single templates (paper notes DT and ST relatively strong) but is outperformed by the multi-template ensemble.",
            "key_findings": "Decomposition (DT) is one of the more effective single templates on math tasks (AQuA); however, combining multiple templates and applying voting yields higher accuracy than any single template alone.",
            "counter_examples_or_negative_results": "Although DT performs well, its outputs are not fully consistent across examples and combining it with other templates in an ensemble further improves results.",
            "uuid": "e4761.3"
        },
        {
            "name_short": "Single-template ST (Step Thinking / 'Let's think step by step')",
            "name_full": "Step Thinking (ST) Chain-of-Thought template",
            "brief_description": "A single CoT template implementing step-by-step reasoning (explicit 'let's think step by step' style prompting), used as one of the reasoning templates in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Used as a single fixed prompt encouraging stepwise decomposition and reasoning.",
            "reasoning_method_name": "Step-by-step Chain-of-Thought (single-template)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "A fixed prompt instructing the model to produce a stepwise solution chain; no variation in template during a run (hence 'similar').",
            "task_name": "AQuA and E-KAR",
            "task_description": "As above: algebraic reasoning and analogical reasoning benchmarks used in paper.",
            "performance": "AQuA: ST reported accuracies across modes roughly in the range 0.5197–0.6102 (51.97%–61.02%). E-KAR (Chinese): ST reported values roughly 0.3373–0.4388 (33.73%–43.88%) depending on retrieval strategy (see Table 3).",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared to DT and other single templates: ST and DT are among the best single-template performers for reasoning-type questions; ensemble methods still outperform ST alone.",
            "key_findings": "Step-by-step prompting is effective (especially on reasoning problems), but combining multiple distinct thinking templates and voting gives additional gains.",
            "counter_examples_or_negative_results": "Performance of ST varies by dataset and retrieval strategy; ST is not uniformly superior in all configurations and benefits from ensemble aggregation.",
            "uuid": "e4761.4"
        },
        {
            "name_short": "Other single templates (DST, PT, AT)",
            "name_full": "Decomposition variant (DST), Plan Thinking (PT), Analogical Thinking (AT)",
            "brief_description": "Additional fixed CoT templates used by the authors: DST (another decomposition prompt), PT (planning-style prompt), and AT (analogy-specific template) — each executed as a single reasoning template in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Applied as single-template prompts tailored to different thinking styles (analogy, plan, decomposition variants).",
            "reasoning_method_name": "Single-template CoT variants: DST, PT, AT",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Each is a single fixed Chain-of-Thought prompt encoding a specific reasoning style: DST = stepwise decomposition, PT = make a plan then execute, AT = analogical induction and mapping.",
            "task_name": "AQuA and E-KAR",
            "task_description": "DST/PT mainly applied to AQuA (math); AT applied to analogical E-KAR tasks.",
            "performance": "Representative values from Table 3: DST on AQuA ≈ 0.5079–0.5984 depending on mode; PT on AQuA ≈ 0.5512–0.5827; AT reported for E-KAR with accuracies in the paper's table ≈0.3821–0.3910 depending on mode. (See Table 3 for mode-by-mode values.)",
            "comparison_with_other_method": true,
            "performance_other_method": "Compared against other single templates and OlaGPT ensemble: per-dataset effectiveness varies (e.g., AT relevant for analogical tasks), but ensemble voting across templates generally yields higher robustness and accuracy.",
            "key_findings": "Different single templates perform differently across tasks — AT benefits analogy tasks and DST/PT/DT benefit math tasks; no single template uniformly dominates, motivating the ensemble approach.",
            "counter_examples_or_negative_results": "A single template sometimes gives lower consistency/accuracy on some datasets; effectiveness depends on task-template match and retrieval/notes strategies.",
            "uuid": "e4761.5"
        },
        {
            "name_short": "GPT-3.5-turbo zero-shot baseline",
            "name_full": "GPT-3.5-turbo (zero-shot / base LLM baseline in experiments)",
            "brief_description": "The base chat-capable large language model used as the starting point for all comparisons; runs zero-shot or with CoT prompting as baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Chat-oriented large language model from OpenAI used as the base LLM in all experiments; paper sets temperature=0 for deterministic runs.",
            "reasoning_method_name": "Zero-shot / single deterministic CoT (single reasoning path)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "No ensemble diversity: either zero-shot answers or a single deterministic chain-of-thought prompt per question.",
            "task_name": "AQuA and E-KAR (benchmarks used as baselines)",
            "task_description": "Mathematical word problems (AQuA) and analogical reasoning (E-KAR); baseline performance reported in Table 3.",
            "performance": "Reported zero-shot baseline on AQuA: 0.3228 accuracy (Table 3). The paper states SC and various template/ensemble methods improve over this baseline.",
            "comparison_with_other_method": true,
            "performance_other_method": "Many methods (SC, Auto-CoT, single CoT templates, and OlaGPT ensemble) are compared to this baseline; ensemble/diverse methods provide consistent improvements.",
            "key_findings": "Deterministic single-run zero-shot baseline is much weaker than methods that introduce reasoning-path diversity (SC, Auto-CoT) or multiple distinct prompting templates plus voting (OlaGPT).",
            "counter_examples_or_negative_results": "None reported where zero-shot single-run outperforms the diverse/ensemble approaches on the evaluated tasks.",
            "uuid": "e4761.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Automatic Chain of Thought Prompting in Large Language Models.",
            "rating": 2,
            "sanitized_title": "automatic_chain_of_thought_prompting_in_large_language_models"
        },
        {
            "paper_title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data.",
            "rating": 1,
            "sanitized_title": "automatic_prompt_augmentation_and_selection_with_chainofthought_from_labeled_data"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.0196115,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities</p>
<p>Yuanzhen Xie 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Tao Xie 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Mingxiong Lin 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Wentao Wei 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Chenglin Li 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Beibei Kong 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Lei Chen 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Chengxiang Zhuo 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Bo Hu 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Zang Li 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities</p>
<p>In most current research, large language models (LLMs) are able to perform reasoning tasks by generating chains of thought through the guidance of specific prompts. However, there still exists a significant discrepancy between their capability in solving complex reasoning problems and that of humans. At present, most approaches focus on chains of thought (COT) and tool use, without considering the adoption and application of human cognitive frameworks. It is well-known that when confronting complex reasoning challenges, humans typically employ various cognitive abilities, and necessitate interaction with all aspects of tools, knowledge, and the external environment information to accomplish intricate tasks. This paper introduces a novel intelligent framework, referred to as OlaGPT. OlaGPT carefully studied a cognitive architecture framework, and propose to simulate certain aspects of human cognition. The framework involves approximating different cognitive modules, including attention, memory, reasoning, learning, and corresponding scheduling and decision-making mechanisms. Inspired by the active learning mechanism of human beings, it proposes a learning unit to record previous mistakes and expert opinions, and dynamically refer to them to strengthen their ability to solve similar problems. The paper also outlines common effective reasoning frameworks for human problem-solving and designs Chain-of-Thought (COT) templates accordingly. A comprehensive decision-making mechanism is also proposed to maximize model accuracy. The efficacy of OlaGPT has been stringently evaluated on multiple reasoning datasets, and the experimental outcomes reveal that OlaGPT surpasses state-of-the-art benchmarks, demonstrating its superior performance. Our implementation of OlaGPT is available on GitHub: https://github.com/oladata-team/OlaGPT.</p>
<p>INTRODUCTION</p>
<p>In the past few years, large language models (LLMs) have developed the ability to process contextual information and generate fluent human language. As we encounter their outputs that sound natural and confident, we quickly assume that they have acquired the long-awaited thinking abilities, such as reasoning, communication, or collaboration, which are highly complex human skills. However, after in-depth understanding of LLM, we find that this reproduction based on high-probability language patterns is still far from the artificial general intelligence we expected. The most obvious gaps include the following: one is that LLMs in some cases produce content that is meaningless or deviates from human value preferences, or even dangerous suggestions with high confidence; secondly, the knowledge of LLMs is limited to the concepts and facts explicitly encountered in their training data. As a result, when faced with more complex problems, LLMs struggle to truly emulate human intelligence by understanding the ever-changing environment, collecting existing knowledge or tools, reflecting on historical lessons, decomposing problems, and using the thinking patterns that humans have summed up in the long-term evolution (such as Analogy, Inductive Reasoning and Deductive Reasoning, etc.) to effectively solve task. One way to solve the first problem is to introduce Reinforcement Learning from Human Feedback (RLHF), which has been recently implemented in ChatGPT [3]. It attempts to explicitly encode human expression preferences into the training process: experts will be asked to rank the answers given by the model according to human common sense and ethical requirements. Obviously, this method is still an idea based on selection or injecting constraints, which can avoid the generation of toxic information to a certain extent, but still cannot reduce the gap with human reasoning ability.</p>
<p>Naturally, inspired by the attempts to use a combination of data weights and bias to mimic how neurons work, we hope to draw on the cognitive model of the human brain and the thinking models developed in the long-term evolution process more comprehensively, and design corresponding system components to endow these cognitive structures or thinking processes to LLM, so as to approximately align the reasoning processes of humans and LLMs, and expect LLMs to be able to solve complex problems more effectively. Some recent works have tried to partially solve some problems, such as using vector databases to store and retrieve external domain knowledge in real time, hoping to improve the memory of LLMs and the ability to capture real-time knowledge; other works such as langchain 1 and toolfomer [27] are designed to be able to leverage available tools, etc. However, the process of mimicking the human brain to deal with problems still faces many systematic challenges:</p>
<p>Challenge 1: How to systematically imitate and encode the main modules in the human cognitive framework, and at the same time schedule the modules according to the general human reasoning patterns in a realizable way. As mentioned before, existing works [27,34] do not comprehensively attempt to align human and LLM reasoning pipelines.</p>
<p>Challenge 2: How to motivate LLMs to perform active learning like humans, that is, learn and evolve from historical mistakes or expert solutions to difficult problems? Encoding the corrected answers by retraining model might be feasible, but it is obviously costly and inflexible. The common in-context learning [3,6,23,36] is more to explain instructions or patterns in a few-shot way. The large model still lacks a human-like thinking framework for wrongly answered questions or historical lessons, such as "reflection-memorizing-reference-reasoning" mental model. Challenge 3. How can a LLM flexibly be able to leverage the diverse thinking patterns that human beings have evolved so as to improve its reasoning performance? It is hard to adapt to various problems by designing a fixed and general thinking model. Just like human beings usually choose different thinking methods flexibly when facing different types of problems, such as analogical reasoning, deductive reasoning and so on.</p>
<p>In order to solve challenge 1, we carefully studied humans' cognitive architecture framework [16], designed some functions to approximate these thinking modules, such as understanding of intention, memory and comprehensive decision-making, etc., and designed the corresponding scheduling mechanism. To address Challenge 2, we propose a concept called "difficult question notes" with the aim of recording cases where the model frequently answers incorrectly. Notes for difficult cases are collected either through manual corrections or by following prompts until a self-correction is made. When answering questions, LLMs can dynamically review the note pool, identify solutions for similar problems, and use them as a point of reference. For the third challenge, we summarize the most effective reasoning frameworks for humans in problem solving, and design corresponding Chain-Of-Thought templates accordingly. We also designed a comprehensive decision-making mechanism to summarize the answers given by each COT reasoning, so as to maximize the accuracy of the model. The main contributions of this work can be summarized as follows:</p>
<p>• As far as we know, this is the first work that attempts to systematically enhance LLMs' problem-solving abilities by learning from a human cognitive processing framework. Results show that this alignment approach can boost the LLMs' performance in many aspects, such as understanding of intention, accuracy of knowledge, correctness of reasoning, etc. • This work tries to summarize various methods of human reasoning into Chain-of-Thought (CoT) templates, so as to maximize the LLMs' reasoning effect in different scenarios. The article also innovatively designs an efficient active learning mechanism and vote mechanism to improve the accuracy and robustness of solving complex cases. • We conduct comprehensive experiments on two datasets and evaluate each module of the proposed method. The experimental results demonstrate that OlaGPT outperforms stateof-the-art baselines, indicating its superior performance.</p>
<p>RELATED WORK 2.1 Augmented Language Model</p>
<p>Augmented language model usually refers to the enhancement of a large language model's reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules. Chain of Thought. Among prompt engineering methods, Chainof-Thought (CoT) prompting [32] is a popular technique that does not require fine-tuning model parameters. It is particularly effective in improving the model's performance in complex reasoning questions by simply changing the input. [6] references uncertaintybased active learning to mine most uncertain questions, which are then manually annotated and iteratively selected to stimulate reasoning ability as much as possible. To reduce the cost of manual annotation, a fully automated pipeline named Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought) is proposed in [28], which uses a variance-reduced policy gradient strategy to estimate the significance of each example in LLM. Another automatic CoT prompting method: Auto-CoT [36] samples questions with diversity and generates reasoning chains to construct prompt. Additionally, a solution is proposed in [30] to improve the results by using a voting strategy to select the most consistent answer output based on the results generated from different reasoning paths. Automatic Reasoning and Tool-use (ART) [23] uses frozen LLMs to automatically generate intermediate reasoning steps as a program. This framework selects demonstrations of multi-step reasoning and tool use from a task library. Self-Taught Reasoner (STaR) [35] relies on a simple loop: if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers. Few-shot prompting struggles as task complexity increases. Consequently, many recent works employ in-context learning to decompose complex problems into sub-problems and effectively teach these sub-problems via separate prompts, such as SeqZero [33], Decomposed Prompting [13].</p>
<p>Tool Use. Although recent LLMs are able to correctly decompose many problems, they are still prone to errors when dealing with performing complex arithmetics. Program-Aided Language models (PAL) [8] decompose symbolic reasoning, mathematical reasoning, or algorithmic tasks into intermediate steps along with python code for each step. Similarly, [7] prompts Codex [5] to generate executable code-based solutions to university-level problems. furthermore, [27] introduces Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.</p>
<p>Our work aims to enhance the performance of large models by drawing inspiration from human-like problem-solving capabilities, with a comparison to some related works presented in Table 1. </p>
<p>Cognitive Architecture</p>
<p>Cognitive architecture is a subset of general artificial intelligence research that began in the 1950s with the ultimate goal of modeling the human mind, bringing us closer to building human-level artificial intelligence. An early approach to cognitive architectures consisted of production systems, which used condition-action rules to represent and perform reasoning [22]. One notable cognitive architecture resulting from this line of research is SOAR, which combined problem-solving, learning, and knowledge representation within a unified system [19]. SOAR has evolved into a comprehensive framework for modeling various cognitive processes such as decision-making, planning, and natural language understanding [18]. Another influential cognitive architecture is ACT-R (Adaptive Control of Thought-Rational), which emphasizes symbolic processing and focuses on memory processes [2]. ACT-R has been applied to extensive cognitive tasks, such as problem-solving, language comprehension, and learning [1]. DUAL (Distributed Unit for Assembling Learning) is a hybrid cognitive architecture that strives to balance symbolic processing and connectionist approaches [15]. By employing a central executive agent and multiple collateral units, DUAL manages a diverse range of cognitive tasks. The Sigma architecture [26] is composed of several key components, such as perception, working memory, long-term memory, production memory, subgoals, decision network, and learning mechanism. [16] highlights the core cognitive abilities of these architectures and their practical applications in various domains. This paper will follow the framework of establishing an analogy between LLM and Cognitive Architectures.</p>
<p>METHOD</p>
<p>To address the challenges mentioned, we present a novel framework denoted as OlaGPT, which is a human-like problem-solving framework to empower LLMs.</p>
<p>Overview</p>
<p>Our approach draws on the theory of cognitive architecture [16], which suggests that the core capabilities of the cognitive framework include Attention, Memory, Learning, Reasoning, Action OlaGPT enhances the user's question intention; the second step is to select multiple thinking templates, tools, wrong question notes, and factual knowledge that may be used; In the third step, the previously obtained wrong notes(used as examples), factual knowledge(pre-knowledge), thinking(guidance) are combined to complete the construction of multiple template agents, and then execute these agents to obtain preliminary answers; The fourth step is to vote on the answers to get the final answer. Selection 2 . We fine-tuned this framework according to the needs of implementation and proposed a process suitable for LLM to solve complex problems. It includes six modules: the Intention Enhance module (corresponding to Attention), Memory module (Memory), Active Learning module (Learning), Reasoning module (Reasoning), Controller module (Action Selection), and Voting module.</p>
<p>The functional profile of each module is as follows:</p>
<p>Intention Enhance. According to [16], attention is an important component of human cognition, as it facilitates the identification of pertinent information and filters out irrelevant data. Similarly, we design corresponding attention modules for LLMs, namely Intention Enhance, aiming to extract the most relevant information and establish stronger associations between user input and the model's linguistic patterns.</p>
<p>Memory. The Memory module serves a crucial function in storing information in various libraries. Recent studies [21,27] have highlighted the limitations of current LLMs in comprehending the latest factual data. In light of this issue, we have designed the Memory module to focus on consolidating the existing knowledge that the model has not yet solidified and storing it in external libraries as long-term memory. During querying, the module's retrieval function can extract relevant knowledge from these libraries. There are four types of memory libraries involved in our paper: facts, tools, notes, and thinking.</p>
<p>Learning. The capability to learn is essential for humans to continuously improve their performance. Essentially, all forms of learning depend on experience. In particular, we found one way to quickly improve LLMs' reasoning ability is to let them learn from the mistakes it has made before. Firstly, we identify problems that cannot be resolved by LLMs. Next, we record the insights and explanations provided by experts in the notes library. Finally, we select relevant notes to facilitate LLMs' learning and enable them to handle similar questions more effectively.</p>
<p>Reasoning. The Reasoning module is designed to create multiple agents based on the human reasoning process, thereby stimulating the potential thought capacity of LLMs to effectively solve reasoning problems. The module incorporates various thinking templates that reference specific thinking types, such as lateral, sequential, critical, and integrative thinking, to facilitate reasoning tasks.</p>
<p>Controller. The Controller module is designed to handle relevant action selection, corresponding to the Action selection discussed in [16]. Specifically, the action selection in this paper involves the internal planning of the model for tasks such as selecting certain modules to execute and choosing from libraries of facts, tools, notes, and thinking.</p>
<p>Voting. The Voting module enables collective decision-making by leveraging the strengths of multiple thinking templates. As different thinking templates may be better suited for different types of questions, we have designed the Voting module to facilitate ensemble calibration among multiple thinking templates. The module is responsible for generating the best answer by employing various voting strategies to improve performance.</p>
<p>After introducing each module, we begin with an outline of the intelligent simulator with human-like problem-solving abilities (OlaGPT) followed by a more detailed description of each component. As depicted in Fig 2, once the user inputs a query, the Intention Enhance module generates a more understandable QA format for the LLMs. The Controller module then retrieves tools, notes, factual knowledge, and multiple thinking templates based on the user's intention. The relevant tools are integrated into the retrieved thinking templates, while notes and factual knowledge serve as supplementary information. (In our implementation, we build an index for the tool library and dynamically retrieve the relevant tools for each template from the index. It is worth noting that this feature has been implemented in the code. However, due to the lack of suitable tools in the current datasets, we did not utilize this feature in the experiment.)</p>
<p>Obviously, utilizing a singular cognitive framework is insufficient for addressing the diverse range of complex problems effectively. Thus, it is essential to adopt a variety of thinking template in order to derive more holistic and precise solutions, which is widely employed in model ensembles to reduce the variance in model outputs. More specifically, we execute multiple templates simultaneously and then employ various voting strategies in the Voting module to get the final answer.</p>
<p>Currently, LLMs have not yet reached a level of performance where they can surpass experts in various fields simultaneously. Nonetheless, by amalgamating the versatility of LLMs and the proficiency of specialists, it is possible to attain superior outcomes. As depicted in Fig 2, human feedback can be integrated into multiple aspects of the proposed framework. These aspects may include incorporating human feedback tools during the implementation of the thinking template, curating the notes library, and making selections in the Controller module. Such an approach ensures that the overall performance is enhanced through the combined expertise of humans and the framework itself. Intuitively, the inclusion of human experts to express clearly ambiguous content can notably improve the performance of LLMs when tackling intricate and multifaceted problems.</p>
<p>By leveraging the insights and expertise of human domain experts, LLMs can better understand the nuances of complex problems and generate more relevant and contextually appropriate responses. It is worth noting that the human feedback sub-module can be disabled in our design to achieve end-to-end reasoning. In this experiment, in order to reduce the cost of labor and resource, we choose the end-to-end approach for the experiment.</p>
<p>The specific content of each module is described in detail below.</p>
<p>Intention Enhance Module</p>
<p>Intention Enhance Module can be regarded as an optimized converter from user expression habits to model expression habits. A more suitable intent enhancement statement is designed for LLMs. Specifically, we get the type of questions by LLMs through specific prompts (see Table 5 in Appendix) in advance and then restructure the way the question is asked. As Fig 3 shows, the sentence--"Now give you the XX(question type produced by LLM model, the prompt see Table 5 in appendix) question and choices:" is added at the beginning of the question. To facilitate the analysis and processing of the results, the sentence--"The answer must end with JSON format: Answer: one of options[A,B,C,D,E]. ") is added at the end of the content. Additionally, we are currently trying to build an automated intention enhance module, set up a seed dataset and related instructions, and call the GPT interface to generate a batch of training data. Using the open-source LLaMA model [29] and Lora [10] technology, fine-tuning is performed using the data generated by the instruction. Intended to implement a module that automates user input enhancements. This module is still under development and experimentation, and relevant experiment results will be released in the future if there is an effect.</p>
<p>Memory Module</p>
<p>The memory module mainly stores relevant knowledge and dialogues. We use the memory function provided by langchain for short-term memory, and long-term memory is implemented by a Faiss-based vector database [11]. There are four main categories of knowledge in our approach: facts, tools, notes, and thinking library. We briefly describe these knowledge libraries as follows:</p>
<p>Facts library. Facts are real-world information like common sense and other knowledge that everyone accepts.</p>
<p>Tools library. In order to solve some defects of LLMs, a tool library that contains search engines, calculators and Wikipedia libraries, is introduced to assist the LLMs to complete some work without fine-tuning. The input and output of the tool should be in text format.</p>
<p>Notes library. Notes mainly record some hard cases and their problem-solving steps. Examples of Notes can be found in Fig 3. Thinking library. Thinking library mainly stores human problemsolving thinking templates written by experts that can be humans or models.</p>
<p>Active Learning Module</p>
<p>Intuitively, it is known that large language models (LLMs) have limitations in some specific tasks. The Active Learning module has been developed to identify challenging cases and provide expert answers. By doing so, LLMs can use expert answers as a reference when encountering similar tasks in the future. In essence, this module aims to facilitate the active learning process of LLMs, enabling them to acquire knowledge on the types of questions they typically struggle with.</p>
<p>Learn from mistakes. A common approach to improving the performance of language models in specific scenarios is to retrain them with annotated answers [24]. However, with the emergence of LLMs, this method is not practical as it requires enormous resources and cannot be updated in real-time. Drawing inspiration from how humans use notes to record their mistakes, we propose to introduce the note library for models to correct stubborn mistakes. When the LLM encounters a difficult question, it can dynamically refer to the note library to find similar problems and their solutions as references as shown in Fig 4. This approach can quickly improve the LLM's problem-solving abilities through prompts engineering. The specific notes format can be seen in Fig 3. The creation of of notes. When constructing notes library, it is recommended to first have LLMs answer a batch of questions repeatedly as shown in Fig 4. The questions that LLMs consistently answer incorrectly can then be recorded. Subsequently, experts can ensure the question types, detailed problem-solving steps, and general problem-solving ideas to form the notes library. Although the notes can be written by either experts or LLMs, it is generally preferable to have experts write the notes first and then have LLMs refine them. The final dataset format is processed as json: {"question":"x", "answer":"x", "error_reason":"x", "model_expert":"x", "explanation":"x", llm_task_type :"x"}. The type of task is generated with LLMs by specific prompt as shown in Table 5. The strategies of retrieving notes. In the Controller module, there are various ways to implement note retrieval. One intuitive approach is to find the most similar question type as an example reference, which has been found to be highly beneficial for LLMs. However, the number of questions of the same type is often too large to be used as additional knowledge input to LLMs (exceeding the number of tokens). Thus we have implemented multiple strategies, including Random Selection, Dual Retrieval, and Combine, denoting as , and respectively. indicates the method that randomly selects questions from the notes library.</p>
<p>represents a dual retrieval strategy that first retrieves the most similar question type using LLM and then retrieves the top-n relevant notes from the notes of such question type. However, text similarity does not always indicate question similarity. Hence, for , we use a random retrieval method to increase diversity in the second retrieval stage of . Last but not least, we use zero-shot to identify the zero-shot strategy, which means disabling the Active Learning module from the framework.</p>
<p>Reasoning Module</p>
<p>The diverse cognitive processes that humans possess are of importance for accurate problem-solving and achieving excellence in various tasks. To fully leverage the model's reasoning ability, we have designed multiple human reasoning approaches in the Reasoning module to assist the model in reasoning. These approaches are designed based on four thinking types, including Analogical, Sequential, Critical, and Synthesis thinking, which show in Fig 5. The following introduces some thinking principles and implementation methods.</p>
<p>• Lateral Thinking : Lateral Thinking is a creative problemsolving approach that involves looking at situations from unconventional perspectives. It includes techniques such as challenging assumptions, seeking alternative solutions with analogy, and embracing ambiguity.</p>
<p>-Analogical Thinking: Analogy Thinking is a cognitive process that involves finding similarities between two  distinct entities or concepts in order to gain a deeper understanding of a problem or situation, which is one of the most effective tools to generate innovative ideas [14]. The prompts can be seen in Table 6. • Sequential Thinking: Sequential Thinking (also named Linear Thinking) is a problem-solving approach that involves breaking down complex tasks or problems into smaller, more manageable parts and addressing them in a logical, step-bystep manner [9].</p>
<p>-Decomposition Thinking: Decomposition Thinking involves breaking down complex problems into smaller subproblems and guiding task decomposition.</p>
<p>-Plan Thinking: Plan Thinking involves creating a detailed roadmap or strategy for achieving a specific goal or completing a task.</p>
<p>-Step Thinking:</p>
<p>Step Thinking is a problem-solving approach that involves breaking down complex tasks into smaller, manageable steps, allowing for more efficient and organized solutions. • Critical Thinking: Critical thinking is the process of objectively analyzing and evaluating information to form reasoned judgments. It includes the component skills of analyzing arguments, making inferences using inductive or deductive reasoning, judging or evaluating, and making decisions or solving problems [17].</p>
<p>-Verification Thinking: In real-life problem-solving scenarios, humans often employ one approach to solve a problem and then use another approach or seek the input of another person to verify the solution and improve its accuracy. This collaborative and iterative process enables more reliable and well-rounded outcomes. Based on this observation, we designed the verification thinking, which involves using an LLM to generate an output and then allowing the same model to provide feedback on its own output from multiple perspectives. The model can subsequently refine its previously generated output based on this feedback. It has not been used in experiments so far. • Integrative Thinking: Integrative Thinking involves combining multiple approaches in various ways, which seems to be the core component in models of adult cognitive development [12]. In our specific implementation, we guide the model to use a variety of thought approaches in a free DIY(Do It Yourself) mode.</p>
<p>As of yet, it has not been utilized in any experiments.</p>
<p>Collectively, these thinking methods are designed to enable models to reason more efficiently and provide more accurate answers. The thinking introduced above is not exhaustive, and the common ones include Concrete Thinking, Abstract Thinking, Vertical Thinking, etc. In actual use, different types of thinking templates can be designed according to the situation. In this module, appropriate thinking templates can be incorporated based on the specific requirements of different tasks. Alternatively, LLMs can generate new templates by combining and selecting from an array of existing thinking templates. This flexibility allows the model to adapt and engage more effectively with the diverse challenges it encounters. In the experiment, the paper explores analogy (Lateral Thinking), decomposition (Sequential Thinking), plan (Sequential Thinking), and step (Sequential Thinking) thinking.</p>
<p>Controller Module</p>
<p>In the Controller module, relevant facts, tools, notes, and thinking are first retrieved and matched (as shown in the second step of Fig  2). The retrieved content is subsequently integrated into a template agent, requiring the LLMs to furnish a response under a single template in an asynchronous manner (as shown in the third step of Fig  2). Just as humans may struggle to identify all relevant information at the outset of reasoning, it is difficult to expect LLMs to do so. Therefore, dynamic retrieval is implemented based on the user's question and intermediate reasoning progress.</p>
<p>As mentioned earlier, the Faiss method [11] has been employed to create embedding indices for all four libraries, with retrieval strategies differing among them. Specifically, information retrieval serves as a tool that can be accessed at any time during the LLM model's problem decomposition and reasoning process from external knowledge bases. In terms of tool selection, we provide a list of tools, enabling the LLM to make real-time choices during the reasoning process. The retrieval strategy for notes differs from the others, as we have designed the three methods described in Section 3.4. As for the retrieval of thinking, we design prompts to allow the LLM for judging and selecting relevant thinking templates. But currently, we use all thinking templates together to perform a comprehensive analysis in the experiment.</p>
<p>Voting Module</p>
<p>Following the third step depicted in Fig 2, we acquire the answers of various thinking templates through LLMs. Intuitively, distinct thinking templates may be more suitable for different types of questions. Therefore, instead of relying on a single template, we adopt a voting mechanism to aggregate the answers from multiple templates and improve the final performance of our model. This approach also enhances the robustness of the model's results, as it takes into consideration the variability in the efficacy of different templates when applied to distinct questions. There are several voting methods available: 1) vote by LLM: Instruct the LLM model to choose the most consistent answer among several given options by providing an output answer along with a rationale through prompts. 2) vote by regex: Extract the answer by regex expression to get the majority vote.</p>
<p>EXPERIMENTAL</p>
<p>We conducted a series of experiments on a range of public datasets and compared the proposed OlaGPT with existing approaches. Our findings indicate that our approach consistently outperforms the baselines on every dataset considered, demonstrating its robustness and effectiveness.</p>
<p>Experimental Setup</p>
<p>Our experiments are designed to address the followings:</p>
<p>• RQ1. How does OlaGPT perform compared to the state-ofthe-art baselines? • RQ2. How effective are sub-modules in the overall model design? • RQ3. How do hyperparameters affect experimental results?</p>
<p>DataSets. To evaluate the proposed framework in a more comprehensive manner, we utilize several publicly available datasets for experimentation. AQuA [20] (Algebra QA with Rationales) is a dataset comprising approximately 100,000 algebraic word problems and 254 test questions. For our labeled training set, we randomly sample 200 training problems. E-KAR [4] is the first interpretable knowledge-intensive analogical reasoning dataset consisting of 1,655 (Chinese) and 1,251 (English) questions from the Chinese Civil Service Exam. In our experiment, we utilize the Chinese version to explore the performance with the Chinese language.</p>
<p>The statistical results of the dataset are presented in Table 2. We identified the questions that the large model answered incorrectly 3-5 times from the training set as its error books. However, it is too expensive to achieve this on the training set of AQuA due to its large size. To address this issue, we utilized Sentence-BERT [25] for embedding and then clustered the training set into 20 groups using K-means. Finally, 211 questions were randomly selected from each group based on their weights.</p>
<p>Evaluation Metrics. The current metric for measurement is the accuracy rate, and most of the questions are in multiple-choice format. The answer is considered correct only when it exactly matches the provided answer options. To compute the model's accuracy rate, we initially use regular expressions to match answers and subsequently perform manual inspection and correction of the assessed results.</p>
<p>The use of thinking templates. In the main experiment, we utilized five thinking templates and the original base model (GPT-3.5-turbo). The prompts of each thinking template can be found in Table 6. Specifically, E-KAR employed all six thinking templates, including origin, Analogy Thinking (AT), Decomposition Thinking 1 (DT), Decomposition Thinking 2 (DST), Plan Thinking (PT), and Sequential Thinking (ST). For AQuA, we used the other five templates excluding AT.</p>
<p>Baselines. We select conventionally and recently published</p>
<p>Augmented LLM baselines for model comparison, which can be briefly categorized into three groups: (1) base LLM model (GPT-3.5turbo); (2) Augmented LLM based on prompt engineering (Auto-CoT); (3) Augmented LLM based on process optimization (SC). To ensure a fair comparison, all baseline methods use the same intention enhancement or voting mechanism.</p>
<p>• GPT-3.5-turbo: A base LLM model that builds upon the GPT-3 architecture and incorporates additional training data and techniques to improve performance on natural language processing tasks. It achieves state-of-the-art results on several benchmark datasets, demonstrating its effectiveness in language modeling and downstream tasks. The temperature is set to 0 in the experiment. • Auto-CoT [36] An automatic CoT (Chain of Thought) prompting method. This approach involves sampling diverse questions and generating reasoning chains to construct demonstrations. • SC [31] A new decoding strategy called self-consistency for chain-of-thought prompting. It samples diverse reasoning paths and selects the most consistent answer by marginalizing the sampled paths. This approach acknowledges that complex reasoning problems have multiple correct ways of thought.</p>
<p>The Model Performance Comparison</p>
<p>In order to evaluate the effectiveness of our augmented LLM framework for inference tasks, we conducted comprehensive experimental comparisons on two types of inference datasets. The results of the experiments are summarized in Table 3. The best performance is indicated in bold, and "Improv" indicates the improvement over the best baseline. Our experiments yielded several findings:</p>
<p>• The performance of SC is better than GPT-3.5-turbo, suggesting that employing ensemble methods to a certain extent can indeed contribute to enhancing the effectiveness of largescale models. • The performance of our approach surpasses that of SC (SC adopts the same majority voting extracted by employing regular expressions as our method), which, to a certain extent, demonstrates the effectiveness of our thinking template strategy. The answers of different thinking templates exhibit considerable variation, and conducting voting at different thinking templates ultimately yields better results than simply running multiple rounds and voting. • The effects of different thinking templates are different, as shown in Table 3. Relatively speaking, ST and DT have better effects. This can be attributed to the fact that this kind of stepby-step solution may be more suitable for reasoning-type questions. • The results presented in Table 3 demonstrate that the Active Learning module yields significantly better performance compared to the zero-shot approach. Specifically, the random, retrieval, and combine columns exhibit superior performance. These findings suggest that incorporating challenging cases as a note library is a viable strategy. • Different Retrieval schemes work differently on different datasets. Overall, the Combine strategy works better. • Our method is significantly better than other solutions, thanks to the rational design of the overall framework, the specific reasons are as follows: 1) The effective design of the Active Learning module; 2) The thinking template has achieved the adaptation of different models as expected, and the results under different thinking templates are different; 3) The Controller module plays a good control role and selects the content that better matches the required content; 4) The way of ensembles with different thinking templates designed by the Voting module is effective.</p>
<p>Ablation Study</p>
<p>To investigate the impact of each sub-module, we conducted an ablation analysis on our framework.</p>
<p>the Performance of Active Learning module. In order to verify the effectiveness of the Active Learning module, we compare whether to use this module or not. The results shown in Fig  6 indicate that the notes' performance surpasses that of the zeroshot. Taking the similarly weak topics of large models as hints can significantly improve performance. The underlying rationale is straightforward: humans learn through accumulating experiences, especially incorrect ones. By providing LLM with the correct problem-solving process for similar problems and allowing it to understand and imitate the process, we can enable the LLM to learn from past mistakes.</p>
<p>the Performance of different retrieval strategies in Controller module. For different notes retrieval strategies, we also conducted experiments and the experimental results are shown in Fig 6 and Fig 9. From the results of different _ ℎ _ values in the table 6, we can see that: the strategy has the best effect, Random is the second one, and the worst one is zero-shot. Intuitively, finding the most similar question type as an example reference has the greatest gain for LLMs. Due to the large scale of the notes and the large number of questions depending on the type of questions, it is unrealistic to introduce them all as few shots. Both and strategies perform a second search after retrieving the question type. The strategy uses similarity search the second time and retrieval uses random search. The strategy is better than because random retrieval introduces greater diversity. Compared with random retrieval, similarity retrieval cannot necessarily satisfy the similarity of topic types, only the similarity in characters. This conclusion is similar to that of Auto-CoT. The most essential thing is to find the most relevant questions.</p>
<p>The best result of E-KAR (chinese) is the strategy. There are several reasons here: 1) The types of questions are not detailed enough. Because of this kind of analogical reasoning questions, the type of questions generated by the model is almost an analogy question, unlike the question types of math questions that can be more detailed. 2) Templates are not customizable. Except for AT for analogy thinking, other thinking templates are made for mathematics questions.</p>
<p>the Performance of Different Thoughts in Reasoning module. This section mainly discusses the effect of different thinking templates on different tasks. From Table 3, it is evident that different thinking templates exhibit varying effects across distinct datasets and retrieval strategies. However, all of them surpass the turbo's results, thereby validating the notion presented in the article that a single template cannot effectively address multiple tasks. Currently, the top three strategies yielding superior results on AQuA are ST-retrieval, DST-retrieval, and DT-combine. For E-KAR, the more effective strategies include ST-combine, DT-retrieval, and DST-random.</p>
<p>After posterior analysis, the final correct rate of some templates has little difference as shown in the table 3, but their consistency is not high. For this very reason, designing a voting module can help select as many correct answers as possible. We have also examined the possible maximum accuracy that can be achieved by different voting strategies. Please refer to Table 8 in the appendix.  c=2  66  56  45  61  42  45  44  40  c=3  95  71  79  73  94  94  91  92  c=4  63  48  63  61  102  82  81  86  c=5  29  78  67  57  64  74  71  75  c=6  ----16 16 16 16 the Performance of Voting module. We explored different voting strategies and the experimental situation of voting under different template numbers. In this experiment, if it is not specified, the regex results will be used for discussion.</p>
<p>• As shown in Fig 7, the performance of the model after voting is significantly improved compared to that of individual thinking templates, which also validates the effectiveness of the voting approach. Different thinking templates are appropriate for different types of questions, and the voting strategy can consolidate the strengths of different thinking templates to further enhance the framework's performance. We consider all correct results as the upper bound and all incorrect results as the lower bound with the regex-vote (majority vote by regex expression) method. The specific values are presented in Table 8 in the Appendix. • As shown in Fig 6, the accuracy rate increases with the growth in the number of templates. As the number of templates increases, the variety of answer combinations generated by different templates also expands, thereby offering the voting module a broader potential upper limit for performance improvement.</p>
<p>• The regex-vote strategy can ensure the majority voting mechanism, while the llm-vote strategy relies on the explanation of the candidate's answers, which may have a larger degree of uncertainty. Consider an example with five templates, where there are 2 correct and 3 incorrect answers. The regex-vote strategy will always select the incorrect. The llm-vote strategy still has the possibility of selecting the correct answer, based on the different confidence of weights on different answers. Consequently, the potential accuracy upper bound of the llm-vote strategy may be even higher.</p>
<p>In summary, we can observe the improvement of the overall recommendation performance whenever we incrementally add a new module or feature on top of the previous model, which illustrates the effectiveness of the Active Learning, Controller, Thought, and Voting modules.</p>
<p>Hyperparameter Study</p>
<p>In this subsection, we investigate the model's hyperparameters.</p>
<p>To determine the optimal number of notes to use as the example reference, we conduct an experiment on the number of examples. Taking the experimental results of the AQuA dataset under the ST template as an example, the outcomes are displayed in Fig 9. It can be found that the optimal value for the number of notes varies among different retrieval strategies. The strategy exhibits consistent improvement, while the and strategies first increase and then decrease. When more sample questions are included, additional noise may be introduced. Thus, it is important to find the appropriate number of examples, neither too few nor too many, making a trade-off.</p>
<p>CONCLUSION</p>
<p>This paper designs an enhanced LLM cognition framework (OlaGPT), aiming to solve difficult reasoning problems with human-like problemsolving abilities. Specifically, referring to the theory of human cognition, OlaGPT proposes to approximate cognitive modules, such as attention (for entention enhancement), Memory, Learning, Reasoning, action selection (Controller) and decision making. The user's query undergoes refinement through the Intention Enhancement module, achieving a more precise expression. Then, the Controller module controls and utilizes this expression to select the required library content, and completes the filling of multiple thinking templates. After acquiring the results of asynchronously executing multiple reasoning templates, a more robust effect is achieved using the Voting module. We conduct experiments on two real inference datasets and show that the OlaGPT method outperforms existing methods in answering inference questions. In addition, we also demonstrate the effectiveness of the design of each part in the model. Most of the modules are designed to be pluggable, and the required modules can be determined according to the needs of different scenarios.</p>
<p>In the follow-up work, we will continue to optimize and improve the functions of each sub-module, and it is expected to complete an easy-to-use enhanced large-scale model framework with human thinking ability. First, more diverse datasets and baselines will be added for experimental testing. In addition, we will continue to optimize the design of each sub-module and conduct more experiments to testify our ideas.         </p>
<p>A APPENDIX A.1 Prompts</p>
<p>In this section, we mainly put the related Prompts design in the experiment, including the prompts of generating question types as shown in Table 5 and thinking templates as shown in Table 6.</p>
<p>A.2 Cognitive Architecture</p>
<p>The framework structure of this paper mainly refers to the article [16], and the key modules listed in the article will be described in detail below.</p>
<p>Attention. The attention section of the essay primarily focuses on the selective mechanisms utilized by cognitive architectures for prioritizing relevant information and filtering out extraneous data. The main idea is to explore how attentional processes help individuals efficiently allocate cognitive resources to specific stimuli or tasks, as well as to discuss the techniques and models applied to modulate attention in various contexts. In LLM, the primary objective of attention is to interpret the input prompt and discern the intent underlying the words.</p>
<p>Action Selection. In the action selection section, the main idea revolves around examining the decision-making mechanisms employed by cognitive architectures to choose appropriate actions in response to external stimuli or internal states. This part covers key computational models, methods, and algorithmic logic responsible for determining and selecting goal-directed actions based on the available information and environmental context.</p>
<p>Memory. The memory section of the essay explores the concepts and models related to the storage and retrieval of information within cognitive architectures. The main idea is to investigate the fundamental features of short-term (or working) and long-term memory, as well as to present the mechanisms by which cognitive systems encode, maintain, and retrieve important information and experiences. We focus on reinforcing the existing knowledge that the model has not yet firmly established. Memory stores the consolidated information in external libraries, effectively functioning as long-term memory for the model.</p>
<p>Learning. The learning section delves into the processes that help cognitive architectures acquire new information, adapt to new situations, and generalize from previous experiences. The main idea is to examine the different learning paradigms, such as supervised, unsupervised, and reinforcement learning, and their applications in equipping cognitive architectures with the ability to modify and improve their knowledge structures, representations, and decisionmaking processes. By updating the few-shot content in the prompt, this task can be easily accomplished. This paper achieves learning in the Large Language Model (LLM) by updating the note library, allowing the model to acquire new knowledge and adapt to new information.</p>
<p>Reasoning. The reasoning section of the essay emphasizes the cognitive processes underlying problem-solving, decision-making, and inference within cognitive architectures. The main idea is to present various approaches and models that demonstrate logical and probabilistic reasoning, as well as to discuss the mechanisms for generating predictions, explanations, and strategies based on available information and knowledge. The module incorporates various templates that enable the model to approach problem-solving situations more effectively and generate well-structured solutions.</p>
<p>A.3 Examples</p>
<p>This section mainly introduces the specific execution text samples. In the Fig 10 and 11, we give the full-text content in different datasets using the strategy to retrieve the notes when asking questions about LLMs(gpt-turbo-3.5) with the DT thinking template.</p>
<p>A.4 Templates analysis</p>
<p>The performance of each template is presented in Table 3. According to Table 7, there are fluctuations in each template when executed on different datasets, which suggests that utilizing model ensembling may lead to improved performance. Consequently, this work employs a voting mechanism to capitalize on the strengths of various templates while addressing their individual limitations.</p>
<p>On the AQuA dataset, the best-performing template is DT, which demonstrates the highest average accuracy, reaching 0.5807. In the E-KAR Chinese dataset, the top-performing template is ST, achieving an accuracy of 0.4015. These results highlight the effectiveness of using different templates tailored to the specificities of each dataset in order to maximize performance.</p>
<p>A.5 Vote analysis</p>
<p>In this study, we investigate voting methods in model ensembles, employing two distinct approaches for fusing the results derived from different models. The first approach entails extracting candidate answers using regular expressions and selecting the most frequently occurring answer as the ensemble's output. The second approach feeds the predicted outputs and their analyses from diverse models into GPT-3.5-turbo, which subsequently generates the final answer.</p>
<p>Considering the voting method utilizing regular expressions for answer extraction, a supremum and infimum inherently exist. To clarify this, the study introduces two metrics evaluating the answers produced by various models for identical questions: accuracy and incorrect consistency. Accuracy represents the proportion of models delivering the correct answer, while incorrect consistency refers to the proportion associated with the most frequently recurring answer, excluding the correct one.</p>
<p>The supremum corresponds to the proportion of questions where accuracy either surpasses or equals consistency. In contrast, the infimum signifies the proportion where accuracy exceeds consistency. The GPT-3.5-turbo-based voting method results reveal an average increase of 0.0561 and 0.0366 in comparison to the infimum derived through regular expression answer extraction for the AQuA and E-KAR (Chinese) datasets, respectively. When compared to the supremum, accuracy remains an average of 0.0325 lower for the AQuA dataset and 0.0485 lower for the E-KAR (Chinese) dataset. Consequently, the outcomes acquired through LLM voting exhibit a higher degree of robustness.</p>
<p>Furthermore, the heatmap shown in Fig 12 demonstrates that answers derived from different templates tend to cluster, indicating Table 5: the prompts of generating question type.</p>
<p>E-KAR datasets:</p>
<p>You are the examiner of the Chinese Civil Service Examination, and you need to judge the specific question types of the following analogy questions and don't give an explanation. Question: {question} Answer: The output must only be in a strict JSON format: "task_type": "question type".</p>
<p>Math datasets:</p>
<p>As a mathematics professor, you need to judge the type of the following question and don't give an explanation Question: {question} Answer: The output must only be in a strict JSON format: "task_type": "question type". Table 6: the main prompts of some thinking templates.</p>
<p>Analogical Thinking (AT): For the problem of analogical reasoning, it is completed in three steps. First conduct an inductive analysis of the given sample data, considering the similarity of the relationship between words; Next, judge whether the sample to be selected is satisfied; Finally check the validity of the mapping and explain if the mapping is correct.</p>
<p>Decomposition Thinking: 1) DT: The following questions can be disassembled into multiple sub-questions to solve, the steps and answers of each sub-question are given, and finally the answer to the following question is given. 2) DST: Disassemble the following complex problems to solve them step by step</p>
<p>Plan Thinking (PT):</p>
<p>Think carefully about the problem to be solved and make a detailed plan to solve it.</p>
<p>Step Thinking (ST): Let's think step by step.     </p>
<p>Figure 1 :
1The overall structure of the OlaGPT model.</p>
<p>Figure 2 :
2The overall flow chart of the OlaGPT model. First,</p>
<p>Figure 3 :
3The notes are combined as examples. The tem-plates_prefix is template-specific content.</p>
<p>Figure 4 :
4The overall of the Active Learning module.</p>
<p>Figure 5 :
5The example of some thinking.</p>
<p>Figure 6 :
6The performance under the different number of templates and notes retrieval modes.</p>
<p>Figure 7 :
7The performance before and after voting.</p>
<p>[</p>
<p>Figure 8 :
8The performance before and after voting.</p>
<p>Figure 9 :
9The performance of ST under the different number of different notes retrieval examples.</p>
<p>Figure 10 :
10one example contents for AQuA.</p>
<p>Figure 11 :
11one example contents for E-KAR.</p>
<p>Figure 12 :
12The Accuracy and Incorrect Consistency of AQuA and E-KAR (Chinese).</p>
<p>Table 1 :
1Comparing OlaGPT with related approaches.Features 
CoT Auto-CoT Toolformer OlaGPT 
Multi-step reasoning ✓ 
✓ 
✓ 
Limited supervision 
✓ 
✓ 
✓ 
Tool use 
✓ 
✓ 
Extendable libraries 
✓ 
Cross-task transfer 
✓ 
✓ 
✓ 
Human feedback 
✓ 
✓ 
Active learning 
✓ </p>
<p>Table 2 :
2Statistics of Datasets.Domain 
Datasets #testing #training #error books </p>
<p>mathematical reasoning AQuA 
254 
97467 
81 </p>
<p>analogical reasoning 
E-KAR 
335 
1155 
632 </p>
<p>Table 3 :
3Performance comparison of different methods on two datasets.Type 
Datasets 
AQuA 
E-KAR (Chinese) 
_ 
_ 
zero-shot 
zero-shot </p>
<p>baselines </p>
<p>turbo 
0.3228 
0.5236 
0.5315 
0.5039 
0.3762 
0.3731 
0.403 
0.3701 
auto_cot 
-
0.5748 
-
-
-
0.3791 
-
-
sc 
0.3189 
0.6142 
0.5394 
0.5906 
0.3761 
0.4179 
0.4119 
0.3851 </p>
<p>thinking templates </p>
<h2>AT</h2>
<h2>-</h2>
<p>-
0.3851 
0.3821 
0.3910 
0.3881 
DT 
0.5591 
0.5866 
0.5827 
0.5945 
0.3612 
0.4119 
0.4269 
0.3881 
DST 
0.5079 
0.5236 
0.5984 
0.5945 
0.3552 
0.4030 
0.3761 
0.4000 
PT 
0.5512 
0.5472 
0.5827 
0.5512 
0.3851 
0.3552 
0.4119 
0.4060 
ST 
0.5197 
0.5787 
0.6102 
0.5669 
0.3373 
0.4179 
0.4119 
0.4388 </p>
<p>Our Framework 
OlaGPT-regex-vote 
0.5945 
0.6496 
0.6732 
0.6772 
0.4209 
0.4597 
0.4567 
0.4716 
OlaGPT-llm-vote 
0.5984 
0.6417 
0.6654 
0.7047 
0.4000 
0.4418 
0.4358 
0.4507 
%Improv. 
85.38% 
5.76% 
24.81% 
19.32% 
11.82% 
10.00% 
10.88% 
22.46% </p>
<p>Table 4 :
4Consistency analysis of the thinking templates.#consistency 
AQuA 
E-KAR (Chinese) 
zero-shot 
zero-shot </p>
<p>Table 7 :
7Precision analysis of the thinking templates.AQuAE-KAR (Chinese) zero-shot random retrieval combine zero-shot random retrieval combine that these templates produce analogous judgments for the same questions.Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009Range 
0.0512 
0.0630 
0.0275 
0.0433 
0.0478 
0.0627 
0.0508 
0.0507 
Mean 
0.5345 
0.5590 
0.5935 
0.5768 
0.3648 
0.3940 
0.4036 
0.4042 </p>
<p>Table 8 :
8Explore the high accuracy of the theory.Combination 
AQuA 
E-KAR (Chinese) 
zero-shot 
zero-shot </p>
<p>regex-upper 
0.6457 
0.6614 
0.7047 
0.7283 
0.4537 
0.5015 
0.4866 
0.4806 
regex-lower 
0.5315 
0.5906 
0.6614 
0.6024 
0.3552 
0.4179 
0.4030 
0.4060 
llm-vote 
0.5984 
0.6417 
0.6654 
0.7047 
0.4000 
0.4418 
0.4358 
0.4507 
reg-vote 
0.5945 
0.6496 
0.6732 
0.6772 
0.4209 
0.4597 
0.4567 
0.4716 </p>
<p>https://python.langchain.com/en/latest/modules/agents/how_to_guides.html arXiv:2305.16334v1 [cs.CL] 23 May 2023
Relevant content is described in Appendix A.2.</p>
<p>How can the human mind occur in the physical universe?. John R Anderson, Oxford University PressJohn R Anderson. 2009. How can the human mind occur in the physical universe? Oxford University Press.</p>
<p>The atomic components of thought. R John, Christian J Anderson, Lebiere, Psychology PressJohn R Anderson and Christian J Lebiere. 2014. The atomic components of thought. Psychology Press.</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.</p>
<p>E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning. Jiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao Li, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua Xiao, Hao Zhou, Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational LinguisticsJiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao Li, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua Xiao, and Hao Zhou. 2022. E-KAR: A Benchmark for Ratio- nalizing Natural Language Analogical Reasoning. In Findings of the Association for Computational Linguistics: ACL 2022. Association for Computational Linguistics, Dublin, Ireland, 3941-3955. https://aclanthology.org/2022.findings-acl.311</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, arXiv:2107.03374Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec RadfordDave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang; Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlishcs.LGMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo- tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shan- tanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. arXiv:2107.03374 [cs.LG]</p>
<p>Active Prompting with Chain-of-Thought for Large Language Models. Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang, arXiv:2302.12246arXiv preprintShizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023. Active Prompting with Chain-of-Thought for Large Language Models. arXiv preprint arXiv:2302.12246 (2023).</p>
<p>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, Gilbert Strang, 10.1073/pnas.2123433119Proceedings of the National Academy of Sciences. 11932Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, and Gilbert Strang. 2022. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences 119, 32 (aug 2022). https: //doi.org/10.1073/pnas.2123433119</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2211.10435PAL: Program-aided Language Models. cs.CLLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. PAL: Program-aided Language Models. arXiv:2211.10435 [cs.CL]</p>
<p>Linking linear/nonlinear thinking style balance and managerial ethical decision-making. Kevin Groves, Charles Vance, Yongsun Paik, Journal of Business Ethics. 80Kevin Groves, Charles Vance, and Yongsun Paik. 2008. Linking linear/nonlinear thinking style balance and managerial ethical decision-making. Journal of Busi- ness Ethics 80 (2008), 305-325.</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. arXiv preprintEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).</p>
<p>Billion-scale similarity search with gpus. Jeff Johnson, Matthijs Douze, Hervé Jégou, IEEE Transactions on Big Data. 7Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535-547.</p>
<p>Integrative thinking is the key: An evaluation of current research into the development of adult thinking. Eeva Kallio, Theory &amp; Psychology. 21Eeva Kallio. 2011. Integrative thinking is the key: An evaluation of current research into the development of adult thinking. Theory &amp; Psychology 21, 6 (2011), 785-801.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, arXiv:2210.02406Peter Clark, and Ashish Sabharwal. 2023. Decomposed Prompting: A Modular Approach for Solving Complex Tasks. cs.CLTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Pe- ter Clark, and Ashish Sabharwal. 2023. Decomposed Prompting: A Modular Approach for Solving Complex Tasks. arXiv:2210.02406 [cs.CL]</p>
<p>Analogical thinking for generation of innovative ideas: An exploratory study of influential factors. Eunyoung Kim, Hideyuki Horii, Interdisciplinary Journal of Information, Knowledge, and Management. 11201Eunyoung Kim and Hideyuki Horii. 2016. Analogical thinking for generation of innovative ideas: An exploratory study of influential factors. Interdisciplinary Journal of Information, Knowledge, and Management 11 (2016), 201.</p>
<p>A hybrid model of reasoning by analogy. Boicho Kokinov, Advances in connectionist and neural computation theory. 2Boicho Kokinov. 1994. A hybrid model of reasoning by analogy. Advances in connectionist and neural computation theory 2 (1994), 247-318.</p>
<p>40 years of cognitive architectures: core cognitive abilities and practical applications. Iuliia Kotseruba, John K Tsotsos, Artificial Intelligence Review. 53Iuliia Kotseruba and John K Tsotsos. 2020. 40 years of cognitive architectures: core cognitive abilities and practical applications. Artificial Intelligence Review 53, 1 (2020), 17-94.</p>
<p>Critical thinking: A literature review. R Emily, Lai, Pearson's Research Reports. 6Emily R Lai. 2011. Critical thinking: A literature review. Pearson's Research Reports 6, 1 (2011), 40-41.</p>
<p>The Soar cognitive architecture. E John, Laird, MIT pressJohn E Laird. 2019. The Soar cognitive architecture. MIT press.</p>
<p>Soar: An architecture for general intelligence. E John, Allen Laird, Paul S Newell, Rosenbloom, Artificial intelligence. 33John E Laird, Allen Newell, and Paul S Rosenbloom. 1987. Soar: An architecture for general intelligence. Artificial intelligence 33, 1 (1987), 1-64.</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, arXiv:1705.04146arXiv preprintWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146 (2017).</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprintNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019).</p>
<p>The Sigma cognitive architecture and system: Towards functionally elegant grand unification. Abram Paul S Rosenbloom, Volkan Demski, Ustun, Journal of Artificial General Intelligence. 71Paul S Rosenbloom, Abram Demski, and Volkan Ustun. 2016. The Sigma cognitive architecture and system: Towards functionally elegant grand unification. Journal of Artificial General Intelligence 7, 1 (2016), 1.</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.04761arXiv preprintTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Lan- guage models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 (2023).</p>
<p>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. Kashun Shum, Shizhe Diao, Tong Zhang, arXiv:2302.12822cs.CLKaShun Shum, Shizhe Diao, and Tong Zhang. 2023. Automatic Prompt Aug- mentation and Selection with Chain-of-Thought from Labeled Data. (2023). arXiv:2302.12822 [cs.CL]</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Naman Baptiste Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.11171cs.CLXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. (2023). arXiv:2203.11171 [cs.CL]</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.11171arXiv preprintXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903cs.CLJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. (2023). arXiv:2201.11903 [cs.CL]</p>
<p>SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models. Jingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, Diyi Yang, arXiv:2205.07381cs.CLJingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, and Diyi Yang. 2022. SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models. (2022). arXiv:2205.07381 [cs.CL]</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. arXiv preprintShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022).</p>
<p>STaR: Bootstrapping Reasoning With Reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, arXiv:2203.14465cs.LGEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. STaR: Boot- strapping Reasoning With Reasoning. (2022). arXiv:2203.14465 [cs.LG]</p>
<p>Automatic Chain of Thought Prompting in Large Language Models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.03493cs.CLZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Auto- matic Chain of Thought Prompting in Large Language Models. (2022). arXiv:2210.03493 [cs.CL]</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1873 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1873</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1873</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-36.html">extraction-schema-36</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</div>
                <p><strong>Paper ID:</strong> paper-280969033</p>
                <p><strong>Paper Title:</strong> Explainable Artificial Intelligence: A Perspective on Drug Discovery</p>
                <p><strong>Paper Abstract:</strong> The convergence of artificial intelligence (AI) and drug discovery is accelerating the pace of therapeutic target identification, refining of drug candidates, and streamlining processes from laboratory research to clinical applications. Despite these promising advances, the inherent opacity of AI-driven models, especially deep-learning (DL) models, poses a significant “black-box" problem, limiting interpretability and acceptance within the pharmaceutical researchers. Explainable artificial intelligence (XAI) has emerged as a crucial solution for enhancing transparency, trust, and reliability by clarifying the decision-making mechanisms that underpin AI predictions. This review systematically investigates the principles and methodologies underpinning XAI, highlighting various XAI tools, models, and frameworks explicitly designed for drug-discovery tasks. XAI applications in healthcare are explored with an in-depth discussion on the potential role in accelerating the drug-discovery processes, such as molecular modeling, therapeutic target identification, Absorption, Distribution, Metabolism, and Excretion (ADME) prediction, clinical trial design, personalized medicine, and molecular property prediction. Furthermore, this article critically examines how XAI approaches effectively address the black-box nature of AI models, bridging the gap between computational predictions and practical pharmaceutical applications. Finally, we discuss the challenges in deploying XAI methodologies, focusing on critical research directions to improve transparency and interpretability in AI-driven drug discovery. This review emphasizes the importance of researchers staying current on evolving XAI technologies to realize their transformative potential in fully improving the efficiency, reliability, and clinical impact of drug-discovery pipelines.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1873.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1873.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COVID-Net</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COVID-Net: A tailored deep convolutional neural network for COVID-19 detection from chest X-ray images</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep convolutional neural network trained on chest X-ray images to detect COVID-19; the model's internal decision-making was audited with GSInquire to ensure reliance on clinically relevant image regions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>COVID-Net (deep convolutional neural network)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>healthcare / medical imaging (disease detection)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>predicted COVID-19 class labels on chest X-ray dataset (model accuracy / sensitivity on labeled data)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML model (deep CNN)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>A deep convolutional neural network trained on labeled chest X-ray images to output a COVID-19 positive/negative classification; GSInquire was used post-hoc to audit the network by identifying influential internal features and mapping them to image regions.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric_name</strong></td>
                            <td>clinical/labeled chest X-ray diagnosis (dataset labels, e.g., radiologist-confirmed COVID-19 status)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_validation_method</strong></td>
                            <td>Evaluation of model predictions against labeled chest X-ray dataset (comparison to clinical/dataset labels obtained from curated cases). No wet-lab or prospective clinical trial validation is described in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Accuracy = 93.3%; Sensitivity = 91.1% (reported on the labeled chest X-ray dataset after auditing with GSInquire)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Same as above — performance reported with respect to dataset ground-truth labels (Accuracy = 93.3%; Sensitivity = 91.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_gap_measurement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performed</strong></td>
                            <td>computational + dataset label comparison (model evaluated on labeled chest X-ray dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>number_predictions_made</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_experimentally_validated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extrapolation_distance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>established application of deep-learning in medical imaging (well-established CNN methods applied to COVID-19 detection; XAI auditing is relatively newer)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_failure_modes</strong></td>
                            <td>Paper highlights risk of models relying on spurious correlations/artifacts in images; GSInquire auditing was used specifically to detect and prevent reliance on clinically irrelevant patterns (i.e., image artifacts) which represent a failure mode where proxy predictions do not reflect true pathology.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>single-stage: computational model training and evaluation against labeled dataset (no in vitro/in vivo/clinical prospective cascade reported in this review)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>image artifacts and spurious correlations in radiographs can create misleading proxy signals; clinical relevance of localized image regions must be verified to reduce proxy-to-ground-truth divergence.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1873.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1873.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Brain-age classifier (Alzheimer's)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chronological and brain age-based model for Alzheimer's disease classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model trained on MRI scans (IXI, ADNI, AIBL datasets) that leverages chronological and brain age features to classify Alzheimer’s disease, reporting sex-stratified accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Classification-Biased Apparent Brain Age for the Prediction of Alzheimer's Disease.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Brain-age-based Alzheimer's classifier (MRI-based ML model)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>medical imaging / neurology (Alzheimer's disease classification)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>predicted Alzheimer's disease class labels derived from MRI-based brain-age features (classification accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML model (MRI feature-based classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Model trained on MRI scans (datasets: IXI, ADNI, AIBL) using features derived from brain morphology and brain-age estimations to predict Alzheimer’s disease status; performance evaluated on the aggregated dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric_name</strong></td>
                            <td>clinical diagnosis labels for Alzheimer's disease in the MRI datasets (dataset ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_validation_method</strong></td>
                            <td>Comparison of model predictions to dataset clinical labels derived from the ADNI/AIBL/IXI cohorts (retrospective dataset evaluation). No prospective experimental or wet-lab validation is described in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Reported classification accuracy: 88% for females and 92% for males (on the aggregated MRI datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Same numbers reported against the dataset clinical labels (88% female; 92% male).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_gap_measurement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performed</strong></td>
                            <td>computational + dataset label comparison (retrospective evaluation on existing MRI cohorts)</td>
                        </tr>
                        <tr>
                            <td><strong>number_predictions_made</strong></td>
                            <td>Predictions made on dataset of 1901 participants (dataset size reported in review).</td>
                        </tr>
                        <tr>
                            <td><strong>number_experimentally_validated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_novelty</strong></td>
                            <td>outperformed the existing ML approaches</td>
                        </tr>
                        <tr>
                            <td><strong>extrapolation_distance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging ML approaches for neuroimaging-based disease classification (models trained on established public MRI cohorts but still developing in clinical translation)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_failure_modes</strong></td>
                            <td>Not specifically detailed in the review for this model; general concerns for MRI-based ML include morphological variability, dataset bias, and demographic confounders that can cause proxy-to-ground-truth divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>single-stage retrospective evaluation (computational on labeled MRI cohorts; no prospective clinical validation reported)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Qualitative statement: the model 'outperformed the existing ML approaches' (no quantitative baseline numbers provided in review).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>morphological semantics of MRI (brain structure variability) and demographic factors can influence model performance and generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1873.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1873.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational or proxy metrics to make predictions or discoveries, and how those predictions compare to experimental or ground-truth validation results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold 3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold 3 (deep-learning protein structure prediction with confidence scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art deep-learning system for protein structure prediction that provides per-region confidence scores to indicate uncertain areas of predicted structures, aiding downstream structure-based drug design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with AlphaFold.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold 3</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>structural biology / protein engineering / drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>predicted 3D protein structures and per-residue confidence scores (proxy for experimental structure accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML model (deep neural network for structure prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Deep-learning model that predicts protein 3D coordinates from sequence; includes confidence scoring (per-region uncertainty estimates) to flag predicted regions with low reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric_name</strong></td>
                            <td>experimentally determined protein structures (e.g., X-ray crystallography, cryo-EM) used as the ultimate ground truth for structure accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_validation_method</strong></td>
                            <td>Not detailed in this review text; generally would be comparison to solved experimental structures (RMSD/structural alignment metrics), but the review only states AlphaFold 3 includes confidence scoring and achieves high accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Described qualitatively as 'high accuracy' in the review; no numerical performance metrics (e.g., RMSD or GDT_TS) are provided in this review text.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explicit_gap_measurement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performed</strong></td>
                            <td>computational prediction with confidence scoring (review does not report quantitative experimental comparisons in this article)</td>
                        </tr>
                        <tr>
                            <td><strong>number_predictions_made</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_experimentally_validated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extrapolation_distance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_bias_correction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established, high-performing ML model for protein structure prediction (transformative in structural biology according to the review)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_failure_modes</strong></td>
                            <td>Review notes AlphaFold 3 provides confidence scoring to identify uncertain areas — implicit failure modes are low-confidence regions where predictions may diverge from experimental structures (e.g., flexible loops, disordered regions, or multi-protein assemblies).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Confidence scoring provided, but no calibration statistics or quantitative calibration against experimental gaps are reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>computational prediction stage with built-in uncertainty flags; review does not describe downstream experimental validation cascade for AlphaFold outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>protein flexibility, disordered regions, multi-protein complexes, and protein-ligand induced conformational changes are domain factors that affect proxy (predicted structure) vs experimental structure agreement.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images. <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with AlphaFold. <em>(Rating: 2)</em></li>
                <li>Classification-Biased Apparent Brain Age for the Prediction of Alzheimer's Disease. <em>(Rating: 2)</em></li>
                <li>Interpretation of Compound Activity Predictions from Complex Machine Learning Models Using Local Approximations and Shapley Values <em>(Rating: 2)</em></li>
                <li>Identification of active compounds as novel dipeptidyl peptidase-4 inhibitors through machine learning and structure-based molecular docking simulations. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1873",
    "paper_id": "paper-280969033",
    "extraction_schema_id": "extraction-schema-36",
    "extracted_data": [
        {
            "name_short": "COVID-Net",
            "name_full": "COVID-Net: A tailored deep convolutional neural network for COVID-19 detection from chest X-ray images",
            "brief_description": "A deep convolutional neural network trained on chest X-ray images to detect COVID-19; the model's internal decision-making was audited with GSInquire to ensure reliance on clinically relevant image regions.",
            "citation_title": "COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images.",
            "mention_or_use": "mention",
            "system_name": "COVID-Net (deep convolutional neural network)",
            "domain": "healthcare / medical imaging (disease detection)",
            "proxy_metric_name": "predicted COVID-19 class labels on chest X-ray dataset (model accuracy / sensitivity on labeled data)",
            "proxy_metric_type": "data-driven ML model (deep CNN)",
            "proxy_metric_description": "A deep convolutional neural network trained on labeled chest X-ray images to output a COVID-19 positive/negative classification; GSInquire was used post-hoc to audit the network by identifying influential internal features and mapping them to image regions.",
            "ground_truth_metric_name": "clinical/labeled chest X-ray diagnosis (dataset labels, e.g., radiologist-confirmed COVID-19 status)",
            "ground_truth_validation_method": "Evaluation of model predictions against labeled chest X-ray dataset (comparison to clinical/dataset labels obtained from curated cases). No wet-lab or prospective clinical trial validation is described in the review.",
            "proxy_performance": "Accuracy = 93.3%; Sensitivity = 91.1% (reported on the labeled chest X-ray dataset after auditing with GSInquire)",
            "ground_truth_performance": "Same as above — performance reported with respect to dataset ground-truth labels (Accuracy = 93.3%; Sensitivity = 91.1%).",
            "explicit_gap_measurement": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "has_both_proxy_and_ground_truth": true,
            "validation_performed": "computational + dataset label comparison (model evaluated on labeled chest X-ray dataset)",
            "number_predictions_made": null,
            "number_experimentally_validated": null,
            "discovery_novelty": null,
            "extrapolation_distance": null,
            "proxy_bias_correction": false,
            "proxy_bias_correction_method": null,
            "validation_cost_time": null,
            "domain_maturity": "established application of deep-learning in medical imaging (well-established CNN methods applied to COVID-19 detection; XAI auditing is relatively newer)",
            "proxy_failure_modes": "Paper highlights risk of models relying on spurious correlations/artifacts in images; GSInquire auditing was used specifically to detect and prevent reliance on clinically irrelevant patterns (i.e., image artifacts) which represent a failure mode where proxy predictions do not reflect true pathology.",
            "uncertainty_quantification": null,
            "uncertainty_calibration": null,
            "multiple_proxy_types": false,
            "validation_cascade": "single-stage: computational model training and evaluation against labeled dataset (no in vitro/in vivo/clinical prospective cascade reported in this review)",
            "comparison_to_baseline": null,
            "domain_specific_factors": "image artifacts and spurious correlations in radiographs can create misleading proxy signals; clinical relevance of localized image regions must be verified to reduce proxy-to-ground-truth divergence.",
            "uuid": "e1873.0"
        },
        {
            "name_short": "Brain-age classifier (Alzheimer's)",
            "name_full": "Chronological and brain age-based model for Alzheimer's disease classification",
            "brief_description": "A model trained on MRI scans (IXI, ADNI, AIBL datasets) that leverages chronological and brain age features to classify Alzheimer’s disease, reporting sex-stratified accuracies.",
            "citation_title": "Classification-Biased Apparent Brain Age for the Prediction of Alzheimer's Disease.",
            "mention_or_use": "mention",
            "system_name": "Brain-age-based Alzheimer's classifier (MRI-based ML model)",
            "domain": "medical imaging / neurology (Alzheimer's disease classification)",
            "proxy_metric_name": "predicted Alzheimer's disease class labels derived from MRI-based brain-age features (classification accuracy)",
            "proxy_metric_type": "data-driven ML model (MRI feature-based classifier)",
            "proxy_metric_description": "Model trained on MRI scans (datasets: IXI, ADNI, AIBL) using features derived from brain morphology and brain-age estimations to predict Alzheimer’s disease status; performance evaluated on the aggregated dataset.",
            "ground_truth_metric_name": "clinical diagnosis labels for Alzheimer's disease in the MRI datasets (dataset ground truth)",
            "ground_truth_validation_method": "Comparison of model predictions to dataset clinical labels derived from the ADNI/AIBL/IXI cohorts (retrospective dataset evaluation). No prospective experimental or wet-lab validation is described in the review.",
            "proxy_performance": "Reported classification accuracy: 88% for females and 92% for males (on the aggregated MRI datasets).",
            "ground_truth_performance": "Same numbers reported against the dataset clinical labels (88% female; 92% male).",
            "explicit_gap_measurement": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "has_both_proxy_and_ground_truth": true,
            "validation_performed": "computational + dataset label comparison (retrospective evaluation on existing MRI cohorts)",
            "number_predictions_made": "Predictions made on dataset of 1901 participants (dataset size reported in review).",
            "number_experimentally_validated": null,
            "discovery_novelty": "outperformed the existing ML approaches",
            "extrapolation_distance": null,
            "proxy_bias_correction": null,
            "proxy_bias_correction_method": null,
            "validation_cost_time": null,
            "domain_maturity": "emerging ML approaches for neuroimaging-based disease classification (models trained on established public MRI cohorts but still developing in clinical translation)",
            "proxy_failure_modes": "Not specifically detailed in the review for this model; general concerns for MRI-based ML include morphological variability, dataset bias, and demographic confounders that can cause proxy-to-ground-truth divergence.",
            "uncertainty_quantification": null,
            "uncertainty_calibration": null,
            "multiple_proxy_types": null,
            "validation_cascade": "single-stage retrospective evaluation (computational on labeled MRI cohorts; no prospective clinical validation reported)",
            "comparison_to_baseline": "Qualitative statement: the model 'outperformed the existing ML approaches' (no quantitative baseline numbers provided in review).",
            "domain_specific_factors": "morphological semantics of MRI (brain structure variability) and demographic factors can influence model performance and generalization.",
            "uuid": "e1873.1"
        },
        {
            "name_short": "AlphaFold 3",
            "name_full": "AlphaFold 3 (deep-learning protein structure prediction with confidence scoring)",
            "brief_description": "A state-of-the-art deep-learning system for protein structure prediction that provides per-region confidence scores to indicate uncertain areas of predicted structures, aiding downstream structure-based drug design.",
            "citation_title": "Highly accurate protein structure prediction with AlphaFold.",
            "mention_or_use": "mention",
            "system_name": "AlphaFold 3",
            "domain": "structural biology / protein engineering / drug discovery",
            "proxy_metric_name": "predicted 3D protein structures and per-residue confidence scores (proxy for experimental structure accuracy)",
            "proxy_metric_type": "data-driven ML model (deep neural network for structure prediction)",
            "proxy_metric_description": "Deep-learning model that predicts protein 3D coordinates from sequence; includes confidence scoring (per-region uncertainty estimates) to flag predicted regions with low reliability.",
            "ground_truth_metric_name": "experimentally determined protein structures (e.g., X-ray crystallography, cryo-EM) used as the ultimate ground truth for structure accuracy",
            "ground_truth_validation_method": "Not detailed in this review text; generally would be comparison to solved experimental structures (RMSD/structural alignment metrics), but the review only states AlphaFold 3 includes confidence scoring and achieves high accuracy.",
            "proxy_performance": "Described qualitatively as 'high accuracy' in the review; no numerical performance metrics (e.g., RMSD or GDT_TS) are provided in this review text.",
            "ground_truth_performance": null,
            "explicit_gap_measurement": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "has_both_proxy_and_ground_truth": false,
            "validation_performed": "computational prediction with confidence scoring (review does not report quantitative experimental comparisons in this article)",
            "number_predictions_made": null,
            "number_experimentally_validated": null,
            "discovery_novelty": null,
            "extrapolation_distance": null,
            "proxy_bias_correction": null,
            "proxy_bias_correction_method": null,
            "validation_cost_time": null,
            "domain_maturity": "well-established, high-performing ML model for protein structure prediction (transformative in structural biology according to the review)",
            "proxy_failure_modes": "Review notes AlphaFold 3 provides confidence scoring to identify uncertain areas — implicit failure modes are low-confidence regions where predictions may diverge from experimental structures (e.g., flexible loops, disordered regions, or multi-protein assemblies).",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Confidence scoring provided, but no calibration statistics or quantitative calibration against experimental gaps are reported in this review.",
            "multiple_proxy_types": null,
            "validation_cascade": "computational prediction stage with built-in uncertainty flags; review does not describe downstream experimental validation cascade for AlphaFold outputs.",
            "comparison_to_baseline": null,
            "domain_specific_factors": "protein flexibility, disordered regions, multi-protein complexes, and protein-ligand induced conformational changes are domain factors that affect proxy (predicted structure) vs experimental structure agreement.",
            "uuid": "e1873.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images.",
            "rating": 2
        },
        {
            "paper_title": "Highly accurate protein structure prediction with AlphaFold.",
            "rating": 2
        },
        {
            "paper_title": "Classification-Biased Apparent Brain Age for the Prediction of Alzheimer's Disease.",
            "rating": 2
        },
        {
            "paper_title": "Interpretation of Compound Activity Predictions from Complex Machine Learning Models Using Local Approximations and Shapley Values",
            "rating": 2
        },
        {
            "paper_title": "Identification of active compounds as novel dipeptidyl peptidase-4 inhibitors through machine learning and structure-based molecular docking simulations.",
            "rating": 1
        }
    ],
    "cost": 0.0180825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Explainable Artificial Intelligence: A Perspective on Drug Discovery
27 August 2025</p>
<p>Yazdan Ahmad Qadri yazdan@yu.ac.kr 0000-0001-5708-1532
School of Computer Science and Engineering
Yeungnam University
38541Gyeongsan-siRepublic of Korea</p>
<p>Sibhghatulla Shaikh 0000-0002-7489-2393
Department of Medical Biotechnology
Yeungnam University
38541Gyeongsan-siRepublic of Korea</p>
<p>Research Institute of Cell Culture
Yeungnam University
38541Gyeongsan-siRepublic of Korea</p>
<p>Khurshid Ahmad k.ahmad@qu.edu.sa 0000-0002-1095-8445
Department of Health Informatics
College of Applied Medical Sciences
Qassim University
51452BuraydahSaudi Arabia</p>
<p>Inho Choi inhochoi@ynu.ac.kr 0000-0002-0884-5994
Department of Medical Biotechnology
Yeungnam University
38541Gyeongsan-siRepublic of Korea</p>
<p>Research Institute of Cell Culture
Yeungnam University
38541Gyeongsan-siRepublic of Korea</p>
<p>Sung Won Kim 0000-0001-8454-6980
School of Computer Science and Engineering
Yeungnam University
38541Gyeongsan-siRepublic of Korea</p>
<p>Athansios V Vasilakos thanos.vasilakos@uia.no 0000-0003-1902-9877
Department of Information and Communication Technology
University of Agder
4879GrimstadNorway</p>
<p>Explainable Artificial Intelligence: A Perspective on Drug Discovery
27 August 2025887E472CFC5AC911DF1B1790191A1B4210.3390/pharmaceutics17091119Received: 24 June 2025 Revised: 5 August 2025 Accepted: 26 August 2025artificial intelligenceexplainable artificial intelligencedrug discoverymolecular modelingtherapeutic innovationpersonalized medicine United States Food and Drug Administration ADMET: Absorption, Distribution, Metabolism, Excretion, and Toxicity IND: Investigational New Drug
The convergence of artificial intelligence (AI) and drug discovery is accelerating the pace of therapeutic target identification, refining of drug candidates, and streamlining processes from laboratory research to clinical applications.Despite these promising advances, the inherent opacity of AI-driven models, especially deep-learning (DL) models, poses a significant "black-box" problem, limiting interpretability and acceptance within the pharmaceutical researchers.Explainable artificial intelligence (XAI) has emerged as a crucial solution for enhancing transparency, trust, and reliability by clarifying the decision-making mechanisms that underpin AI predictions.This review systematically investigates the principles and methodologies underpinning XAI, highlighting various XAI tools, models, and frameworks explicitly designed for drug-discovery tasks.XAI applications in healthcare are explored with an in-depth discussion on the potential role in accelerating the drug-discovery processes, such as molecular modeling, therapeutic target identification, Absorption, Distribution, Metabolism, and Excretion (ADME) prediction, clinical trial design, personalized medicine, and molecular property prediction.Furthermore, this article critically examines how XAI approaches effectively address the black-box nature of AI models, bridging the gap between computational predictions and practical pharmaceutical applications.Finally, we discuss the challenges in deploying XAI methodologies, focusing on critical research directions to improve transparency and interpretability in AI-driven drug discovery.This review emphasizes the importance of researchers staying current on evolving XAI technologies to realize their transformative potential in fully improving the efficiency, reliability, and clinical impact of drug-discovery pipelines.</p>
<p>Introduction</p>
<p>The Human Genome Project (HGP), completed in 2003, was a feat of human scientific ambition that took 13 years to understand the human genetic makeup.This process involved sequencing the human DNA to understand the genetic makeup [1].HGP led to new research offshoots, enabling the understanding of the genetic causes of diseases and possible interventions.Therefore, shedding light on the functional significance of the genes has led to an economic impact of $800 billion.The emergence of novel diseases and persistent disorders has led to an increase in the use of pharmaceutical agents in daily life in recent years [2].Furthermore, due to modern lifestyles that expose individuals to harmful pollutants and microorganisms, there is a pressing need for innovative and more reliable pharmacotherapeutic interventions.Consequently, rapid progress in drug discovery and development is unavoidable, and it is reasonable to expect the emergence of significant pharmaceutical solutions in a short time span.Presently, numerous research and development institutions, including government and commercial institutions, heavily rely on the expertise of pharmaceutical professionals for the development of these interventions.</p>
<p>The process of drug discovery evolved from the 1980s, when chemists began developing chemical compounds that specifically target distinct molecular entities such as receptors, enzymes, and ion channels.Structure-based drug development gained prominence in the 1990s, particularly in identifying lead compounds for drug development.Several technological developments, including high-throughput synthesis, genomics, structural biology, and computational chemistry, were brought into the drug-development process in the 2020s [3][4][5][6].Given that the biological activity of a therapeutic molecule depends on its three-dimensional structure, medicinal chemistry plays a pivotal role in drug discovery.Thus, during the early stages of drug development, it is crucial to understand a drug's chemical properties through structure-activity relationship studies [7].The lead optimization phase is a critical stage in the drug-discovery pipeline, wherein promising molecules identified during the hit-to-lead stage are systematically modified to improve their efficacy, selectivity, and drug-like properties.This process aims to enhance the therapeutic potential of the lead compounds while minimizing undesirable characteristics such as toxicity or poor bioavailability.During this phase, candidate compounds undergo a series of in vitro assays to assess their potency, physicochemical characteristics, and absorption, distribution, metabolism, excretion, and toxicity (ADMET) profiles.Following this, preclinical in vivo studies are conducted to investigate the pharmacokinetic and pharmacodynamic properties of the selected molecules.Pharmacokinetics examines a drug's kinetics, which are primarily influenced by the body's ADME processes.In contrast, pharmacodynamics quantifies the drug's impact on the body, including various dynamics such as biomarker response, cytokine release, tumor progression, and other related factors [8].Various physicochemical properties, such as molecular weight, lipophilicity, and permeability, influence the pharmacokinetic behavior of a drug [9].Moreover, the drug's exposure and, consequently, its efficacy can be affected by the complex physiology of the body [10].The data obtained during the research process is combined into a translational approach to predict a clinically appropriate and effective dosage and regimen that ensures safety [11,12].Predicting clinical efficacy solely based on a compound's intrinsic properties or its behavior in preclinical in vivo studies can be challenging.However, a drug's ability to achieve a safe and effective exposure level is generally considered the primary determinant of its efficacy.The timeline and process of developing a new drug are illustrated in Figure 1.Typically, creating a new pharmaceutical drug takes about 12 to 15 years in the United States and requires continuous monitoring after its general rollout [13,14]. .Timeline of the conventional drug-discovery process.A typical drug undergoes five major phases.The process begins with target identification and validation, where disease-associated biological targets are identified and confirmed.This is followed by hit and lead discovery, in which compounds that interact with the target are identified and optimized for potency and selectivity.The preclinical phase involves in vitro and in vivo studies to assess the compound's safety, efficacy, pharmacokinetics, and pharmacodynamics.If successful, the drug enters clinical trials, conducted in three phases, each involving an increasing number of human participants to evaluate safety, dosage, and therapeutic efficacy.Finally, the post-marketing surveillance phase involves continuous monitoring of the drug's long-term safety and effectiveness in the broader population.</p>
<p>Artificial intelligence (AI) is a data-driven system that uses advanced tools and networks to mimic human intelligence [15].The integration of AI in healthcare encompasses disease prediction and detection, genetic analysis and gene editing, drug discovery, radiography, and personalized medicine [16].AI models demonstrate high accuracy and efficiency [17].AI algorithms of varying complexity perform diverse functions at various levels of healthcare applications [18].Neural networks (NN), such as convolutional neural networks (CNNs), have demonstrated a high degree of accuracy in biomedical image analysis [19].In contrast, recurrent neural networks (RNNs) are adept at identifying anomalies in time-series biomedical data [20].State-of-the-art large language models (LLMs) have revolutionized diagnosis, genomics, drug discovery, and personalized medicine [21,22].Although these AI models yield highly accurate results, the basis for their reasoning is obscured by the highly complex mathematical processes that underpin these models.As of 2024, the United States Food and Drug Administration (FDA) had approved 950 artificial intelligence/machine-learning (AI/ML)-enabled devices for disease diagnosis [23].The challenges in using AI for determining prognosis and developing treatment plans have slowed progress due to safety concerns.Therefore, clinical decisions must be founded on well-established principles.Although the conclusion is accurate, flawed reasoning is unacceptable, especially in safety-critical applications such as healthcare.</p>
<p>The vast chemical space, estimated to encompass over 10 60 potential molecules, offers a rich foundation for the discovery of novel drug candidates [24].However, screening through such a large candidate list using rudimentary methods can significantly impede the drug-development process, making it time-consuming and financially burdensome.However, using AI-based methods has the potential to overcome these limitations as illustrated in Figure 2 [25].AI can identify hit and lead compounds, enabling faster drug-target validation and optimization of drug structure design [26].Incorporating large datasets into AI models has the potential to reduce the risk associated with introducing a new molecular entity, eliminating the need for extensive experimentation.Researchers can achieve an automated and more efficient screening and selection strategy by incorporating in-silico AI models, which differ from a 'trial-and-error' approach that relies solely on expert intuition.This paradigm increases the number of screened compounds while decreasing the screening times.While various efforts have been reported for the early phases of the drug-development pipeline, such as target identification and hit finding, the potential relevance of these techniques in the later stages of the process remains unclear.The use of AI tools is thought to significantly reduce the experimental burden and timelines currently required for characterizing drug response in vitro and in vivo [27].The foundation of the outcomes of these models is uncertain due to the "black-box" nature of the AI models.Explainable AI (XAI) bridges the gap between the outcomes of an AI model and the underlying reasoning behind those outcomes.XAI techniques can establish a foundation for trusting the reliability of models that assist in the drug design pipeline.XAI techniques address these challenges by identifying which molecular features or descriptors contribute most significantly to a given prediction, or by estimating the marginal contribution of each feature to the output, or highlighting specific substructures that are strongly associated with a predicted outcome.These insights enable researchers to rationally prioritize or modify molecular scaffolds, improve candidate selection, and enhance lead optimization.Moreover, XAI can potentially enhance regulatory compliance and build confidence in AI-driven pipelines by offering human-interpretable explanations for model predictions, such as poor absorption, high distribution volume, metabolic instability, or toxicity during the ADMET prediction.With the adoption of multi-modalities, from SMILES strings and molecular graphs to transcriptomics and imaging data, XAI provides a necessary layer of transparency, enabling the deployment of AI not only as predictive tools but rather as a reliable decision support system.The state-of-the-art AI models can potentially accelerate the drug-discovery process, particularly in the initial two stages.The published literature reviews have summarized the role of XAI in improving the drug-discovery pipeline.The two widely accepted explainability methods are the SHapley Additive exPlanations (SHAP) and the Local Interpretable Model-agnostic Explanations (LIME).The authors of [28] explore the role of SHAP and its variants in enhancing transparency in AI-driven drug-discovery processes.The authors outline the regulatory and practical importance of interpretability, emphasizing that explainability improves trust and reduces the downstream costs associated with opaque models.Their review outlines technical and regulatory challenges and future directions for XAI.Ding et. al. [29] systematically evaluates the literature on XAI applications in chemical and drug research, encompassing traditional Chinese medicine domains.However, this work predominantly relies on quantitative metrics without deep qualitative insights into the practical efficacy or impact of specific XAI techniques.In [30], authors deliver a structured taxonomy tailored specifically for medicinal chemistry, advocating for essential visualization and interactive methodologies.They outline clear guidelines for effectively integrating XAI into chemical research.The main limitation is that the recommendations primarily focus on structural visualization, rather than performance metrics or quantitative evaluations.Jiménez-Luna et al. [31] focus on the challenges associated with interpreting deep-learning (DL) models in drug discovery.The authors detail various feature attribution methods and gradient-based approaches to enhance the interpretability of models.They underscore that interpretability significantly impacts the practical application of DL, particularly when accuracy must be balanced with human comprehensibility and regulatory acceptance.Vo et al. [32] review XAI methodologies for predicting drug-drug interactions (DDIs).Given the clinical importance and high-risk nature of drug-drug interactions, the authors emphasize the necessity of transparent AI predictions to ensure reliability and clinical acceptance.It comprehensively categorizes ML/DL models, identifying gaps and limitations, and suggests pathways to strengthen model transparency and reliability.A comprehensive survey [33] covers various XAI frameworks and their applications, including target identification, compound design, and toxicity prediction.The authors identify key limitations, such as the interpretability versus performance tradeoff, and provide future research directions to guide the effective integration of XAI into drug-discovery processes.Although their work offers a clear understanding of XAI in drug discovery, their discussion is presented from an AI-centric standpoint.Therefore, the existing literature for health science researchers is limited to brief reviews on this research domain.</p>
<p>Discovery and Development</p>
<p>Pre-clinical Development
Clinical
To address the existing literature gap for biomedical science researchers, this comprehensive review examines the role of XAI in enhancing the interpretability and transparency of AI-driven drug-discovery methods.It summarizes the key XAI tools, models, and their applications in molecular modeling, target identification, molecular property prediction, clinical trial design, and personalized medicine.The review investigates how XAI addresses the opacity issues of traditional AI models, identifies current implementation challenges, and outlines key future research avenues for effectively incorporating XAI into pharmaceutical research.This article is organized as follows.Section 2 introduces XAI, its basic concepts, and its types.Section 3 elucidates the role of XAI in healthcare.Section 4 delves into the role of XAI in the drug-discovery process, while Section 5 discusses in detail the impact XAI has on the drug-discovery pipeline.The challenges and future research directions are outlined in Section 6.The discussion is concluded in Section 7.</p>
<p>Explainable AI</p>
<p>Explainable AI "explains" the output of an AI model.XAI constitutes a set of processes that explain the intent and reasoning for the output generated by an AI model.XAI elucidates the process and logical reasoning used by an AI model to arrive at a conclusion.Ensuring the accuracy along with safety in operation is crucial in critical applications such as autonomous vehicles, healthcare, and industrial Internet of Things (I-IoT) [34].Data-driven decision systems in critical applications should be both trustworthy and interpretable.Interpretability elucidates the inner workings and explains how an AI model makes a decision.While explainability takes into account all the interpretable factors that contribute to an AI model's decision and allows the user to understand why the model made a particular decision [35].The AI models can be classified into three categories based on their explainability: white-box, gray-box, and black-box models [34,36].The white-box models are self-interpretable.Users can interpret the working logic of models, such as those in linear regression and decision trees.Still, there is a significant tradeoff in accuracy, as they assume the data to be linear or sub-linear, which is contrary to real-world data [37].Additionally, self-interpretable models are not highly scalable and therefore do not meet the requirements for critical applications.The gray-box models aim to strike a balance between accuracy and interpretability.The gray-box models can support vital applications as they offer a level of interpretability by allowing analysis of the model's inner workings and a higher accuracy [38].However, the powerful AI models powering high-end applications are highly complex, making them difficult to interpret.Their ambiguous decision system makes them inappropriate for critical applications.However, for these black-box AI models with high obscurity, XAI tools can ensure trustworthiness [39].The various types of AI models classified based on their interpretability are illustrated in Figure 3. XAI methods can be broadly classified into two groups, based on intrinsically interpretable models and post-hoc models [40].The former category consists of models that are inherently easy to comprehend, while the latter requires a set of specialized methods to explain the model decisions [41].The general outline of the XAI classification is illustrated in Figure 4.The following discussion follows the structure outlined in this figure.</p>
<p>Intrinsically Interpretable Models</p>
<p>Intrinsically interpretable models are designed in such a way that humans can readily understand their structure, parameters, and decision-making processes.They provide interpretations organically due to their structure, and there is no need for extra post-hoc approaches for interpretability [42].These models are significant in critical domains as healthcare, finance, and law, where understanding the underlying causes for a certain decision is equally important to model accuracy.Inherently interpretable models can provide stakeholders with unparalleled insights into decision-making, enabling trust, transparency, and accountability to flourish.The following features define intrinsically interpretable models.(a) Simplicity, as they rely on uncomplicated mathematical models, including linear systems, rule sets, or tree ensembles.(b) Transparency, as each action or decision made by this model can be delineated and elucidated.(c) Feature importance clarifies the contribution of each feature to the final prediction.These models typically possess fewer parameters than black-box models, making them more comprehensible and interpretable [43].The intrinsically interpretable models can be classified into the following categories: 2.1.1.Linear Models Linear models represent the simplest type of intrinsically interpretable models, where the output is the result of a linear combination of the input features.These models assume a linear relationship between input features and target variables.Therefore, the impact of an input variable on the output is directly interpreted by its coefficient [44].Linear Regression models calculate a continuous dependent variable from a set of predictor variables.The model, in turn, fits a linear equation to the empirical data.Each coefficient provides a clear interpretation of how a one-unit change in a feature influences the target variable, assuming all other features are held constant [45].Logistic regression is a classification technique used for binary problems in a manner quite similar to linear regression.It generates a model to compute the probability that an instance belongs to a specific class.In the logistic regression model, this output is transformed through a sigmoid function that ranges between 0 and 1.In logistic regression, the coefficients signify the log-odds of a one-unit change in the respective feature.Although still not as intuitive as the coefficients from linear regression, directly interpreting the output with probabilities and odds ratios does it [46].</p>
<p>The linear models are simple, as they are directly interpretable and can be trained easily to perform accurately using a moderately sized dataset.The use of regularization can also help improve explainability in linear models.Regularization techniques help reduce overfitting by adding a penalty to the model's loss function, encouraging simpler weights [47].This reduces the influence of less important input variables by moving the variable coefficients to near zero, improving model interpretability.Regularization also enhances generalization, making the model more robust to unseen data.However, it requires careful hyperparameter tuning to determine the optimal strength of the penalty.Additionally, regularization is particularly effective in high-dimensional data, where it helps mitigate the risk of overfitting due to the large number of input variables.On the other hand, regularization can also overly simplify models if the regularization parameter is too large, causing the model to miss important patterns.It also assumes equal importance for all input variables, which may not hold true, so incorporating domain knowledge can enhance performance.Regularization may be computationally expensive, especially for high-dimensional data, making it less suitable for real-time applications [48].Additionally, it presumes that data are linearly separable and independent and identically distributed (IID), which may not apply to complex datasets like time-series or spatial data, requiring more advanced models [49].</p>
<p>Generalized Additive Models (GAMs) extend traditional linear models by allowing the relationship between each feature and the target variable to be modeled with smooth, nonlinear functions, while still maintaining an additive structure.Unlike linear models, which assume a straight-line relationship between features and the target, GAMs are more flexible, as they can capture complex, nonlinear relationships [50].However, despite this flexibility, GAMs remain interpretable because each feature's contribution to the prediction remains independent of the contributions of the others.This structure enables the visualization of the effect of each feature, often in the form of individual plots for each constituent function.The advantages of GAMs lie in their balance of flexibility and interpretability [51].They can model nonlinear patterns in the data without sacrificing transparency, as the additive nature of the model ensures that each feature's influence is clear and separable from the others.This makes GAMs particularly useful for applications where both accuracy and interpretability are essential.However, a key limitation of GAMs is that they are restricted to additive relationships between features and the target, meaning they cannot model interactions between features.Consequently, while GAMs are powerful in capturing individual feature effects, they may be less suitable for datasets where feature interactions play a critical role [52].</p>
<p>Decision Tree</p>
<p>A decision tree is a hierarchical ML model that partitions data into subsets based on feature values, using a series of if-then rules to make decisions [53].The model is structured with three primary components: the root node, representing the initial feature used for data splitting; internal nodes, indicating subsequent decision points; and leaf nodes, which provide the final prediction or classification.This structure allows for high interpretability, as each path from the root to a leaf can be easily understood as a sequence of decision rules.One of the key advantages of decision trees is their transparency and ease of interpretation.Additionally, they do not require feature scaling, allowing them to work effectively with unprocessed data [54].Furthermore, decision trees are capable of modeling complex, nonlinear relationships, making them versatile in capturing a wide range of diverse patterns.However, these models have limitations, such as their susceptibility to overfitting, particularly when the trees are deep, which can lead to the capture of noise rather than meaningful patterns in the data.Decision trees are also known for their instability, as small changes in the input data can result in significant alterations to the tree structure, making them sensitive to data variations [55].</p>
<p>Similar to a decision tree, rule-based association is an ML method that identifies relationships or patterns between variables in large datasets through the use of if-then rules.The explicit association rules make them particularly useful in applications such as market basket analysis, where relationships between items can be easily extracted and interpreted.Rule-based models are inherently interpretable, as users can assess the relevance and validity of each rule and modify them, if necessary, based on domain knowledge [56].This interpretability is crucial for applications in sensitive fields, such as healthcare and finance, where understanding the rationale behind decisions is essential for ensuring fairness, accountability, and trustworthiness in AI systems.</p>
<p>In healthcare, decision trees and rule-based models are employed for medical diagnoses due to their transparent decision-making processes, such as diagnosing diseases based on symptoms and test results.In finance, linear models such as logistic regression are used for credit scoring to predict default risk, while rule-based models aid in fraud detection.In the legal domain, rule-based models and decision trees are utilized in risk assessments, including determining parole eligibility and predicting recidivism.These models' interpretability makes them valuable in fields where transparency is crucial.</p>
<p>Post-Hoc Explainability</p>
<p>Models that are not inherently interpretable require additional tools to enable a human to understand them.Post hoc methods, applied after a model is trained, aim to explain the decisions of a complex, already trained model.These methods are often indispensable when using black-box models, as the internal working mechanisms of the model are too complex to be understood without additional aids.Examples of post-hoc interpretative methods include backpropagation-based methods within neural networks, which quantify the features at the input, and model-agnostic approaches such as LIME and SHAP.These provide approximate decision processes for black-box models and, in this manner, may be used to provide insight into the interpretation of different inputs about predictive outputs [57].While these post hoc interpretations are indispensable in the study of blackbox models, they suffer from issues of complexity and domain specificity.In addition to identifying essential input features, these methods can often only roughly approximate the kind of relationships that exist between features and outputs in many domains, excluding image and text analysis [41].</p>
<p>Model-Agnostic XAI</p>
<p>These techniques are used to explain the output of machine-learning models, without regard for the underlying AI model.They are model-agnostic because they do not depend on the architecture or inner workings of the model, which allows them to be applied to any machine-learning model.The primary motive behind model-agnostic methods is to provide interpretability to predictions or insights into how the model arrives at a decision [58].Since the complexity of the underlying AI can vary, a surrogate model that is inherently interpretable can be used to explain the model's decisions.A surrogate model is an interpretable model, such as a decision tree or linear model, used to approximate the predictions of a more complex "black-box" model.By analyzing the surrogate, insights can be gained into how the black-box model makes decisions.The advantage of this approach is that it provides a global understanding of the complex model's behavior.However, it may not fully capture the behavior of the original model, particularly in highly nonlinear problems.Surrogate models are often used to obtain high-level explanations of complex models, such as in DL applications in healthcare.</p>
<p>LIME</p>
<p>LIME is a widely used method that provides interpretability for individual predictions of any ML model, irrespective of its complexity.LIME is particularly useful for complex, black-box models such as deep neural networks and ensemble methods, which often produce accurate predictions but are difficult to interpret.The core idea behind LIME is to generate a local surrogate model, typically a simpler and more interpretable model like linear regression or decision trees, to approximate the behavior of the black-box model in the neighborhood of a specific instance.This surrogate model allows for detailed local explanations, making the black-box model's predictions more transparent on a case-by-case basis [59].To explain an individual prediction, LIME first samples data points around the instance in question by perturbing the input features and generating new examples similar to the original instance.Perturbation involves modifying the input data slightly to see how the black-box model's predictions change.By observing these changes, LIME can understand how sensitive the model's prediction is to individual features.It then feeds these perturbed instances into the black-box model to obtain corresponding predictions.LIME assigns more weight to perturbed instances that are closer to the original instance and fits a simple interpretable model to this weighted data, approximating the blackbox model's decision-making process in that local region.The surrogate model thus provides insights into the contribution of each feature toward the prediction, allowing users to understand which features were most influential in the model's decision for that particular instance.LIME's primary advantage is its model-agnostic nature, meaning it can be applied to any machine-learning model, regardless of complexity.This makes it particularly effective in explaining nonlinear models [60].It also provides detailed explanations for individual predictions, which is crucial in high-stakes decision-making areas such as healthcare, finance, and legal systems.However, LIME is not without limitations.One notable drawback is its instability; small changes in data can lead to different explanations for similar instances, especially when the black-box model's decision boundary is highly nonlinear.Furthermore, the local explanations provided by LIME may not generalize well to the entire model, limiting the scope of the explanations to the neighborhood around the instance being explained.LIME can also be computationally intensive as it requires generating numerous perturbed samples and running predictions for each, which can be challenging for large datasets or complex models.</p>
<p>Despite these limitations, LIME has demonstrated broad applicability in various fields.In credit scoring, for instance, LIME can explain why a loan application was approved or denied, providing users with detailed reasons based on their financial features.Similarly, in healthcare, LIME has been used to explain diagnostic models, helping clinicians understand which patient features contributed most to a given prediction.LIME has also been applied to image classification tasks, where it can highlight the specific parts of an image that were most influential in the model's decision.In summary, LIME offers a flexible, interpretable solution for understanding the behavior of complex machine-learning models at the local level, although it is essential to consider its limitations in terms of stability and generalizability.</p>
<p>Shapley Additive Explanations</p>
<p>SHAP is a method rooted in cooperative game theory that provides a comprehensive and theoretically sound framework for explaining individual predictions by quantifying the contribution of each feature.It assigns each feature an importance value, known as the SHAP value, which represents the feature's contribution to the model's prediction.SHAP values offer a unified measure of feature importance by considering the contribution of each feature in the context of all possible feature subsets.This ensures that the assigned SHAP values accurately reflect each feature's role in the prediction [61].</p>
<p>The primary advantage of SHAP is its solid theoretical foundation, which guarantees consistency in attributing feature importance at local (individual prediction) and global (overall model behavior) levels.SHAP values ensure additivity, meaning that the sum of all feature contributions equals the model's prediction, which provides a clear and interpretable breakdown of how each feature influences the outcome.However, the method's robustness comes with the disadvantage of high computational cost, particularly when applied to large and complex models, due to the need to compute contributions across all possible feature subsets.</p>
<p>SHAP operates by calculating the contribution of each feature through the lens of Shapley values, a concept from cooperative game theory.Shapley values represent a fair allocation method, originally designed to distribute payouts among players based on their contribution to a coalition's total value.In machine learning, each feature is considered a "player," and the model's prediction is the total "payout."SHAP values ensure that each feature's contribution is fairly evaluated by averaging the marginal contribution of the feature across all possible combinations of features.This approach provides a comprehensive and equitable distribution of the prediction value among the input features.SHAP delivers a robust and interpretable method for understanding the behavior of complex models, ensuring fairness and transparency in decision-making processes [62].</p>
<p>Partial Dependence Plots</p>
<p>Partial Dependence Plots (PDPs) illustrate the marginal effect of one or two features on a model's prediction by varying the target feature(s) while keeping other features constant.This approach helps visualize how changes in a specific feature impact the predicted outcome, providing an intuitive and straightforward means of interpreting feature influence.However, PDPs assume feature independence, which may not hold in real-world scenarios where features are often correlated.This can lead to misleading interpretations, particularly in complex models.However, Accumulated Local Effects (ALE) plots offer an alternative to PDPs by addressing the limitations related to feature dependencies.ALE plots estimate the local effect of a feature on the model's predictions by computing the changes in the prediction within small intervals of the target feature and then accumulating these effects over the feature's range [43].</p>
<p>Model-Specific XAI</p>
<p>Model-specific XAI methods are tailored to leverage the internal structures and characteristics of specific types of machine-learning models to provide detailed and contextsensitive interpretations.These methods are inherently linked to the particular architecture or operational principles of the models they are designed for, enabling a deeper and more precise understanding of model behavior than generic, model-agnostic approaches [63].This category of XAI techniques is particularly relevant for complex models such as deep neural networks, where interpretability is crucial for understanding the decision-making process, building trust, and ensuring compliance in critical applications like healthcare, finance, and autonomous systems [64].Some of the major model-specific XAI methods are described as follows.</p>
<p>Attention</p>
<p>Attention mechanisms were developed to address the challenges traditional neural network architectures posed in processing long data sequences, particularly within natural language processing (NLP) tasks [65].Initially introduced for machine translation, attention mechanisms enable models to focus selectively on different parts of the input sequence when generating each segment of the output.This approach effectively mitigates the limitations of RNNs, which rely on fixed-size context vectors that struggle with long-term dependencies.The core functionality of attention mechanisms lies in the computation of attention weights, which quantify the relevance of each input element to a specific output element.In sequence-to-sequence models, these weights are derived by measuring the similarity between the current state of the decoder and each state of the encoder.The resulting attention weights are then used to generate a weighted sum of the encoder states, forming a context vector that guides the model's current prediction [37].Various forms of attention mechanisms exist, including self-attention, where each element in the input sequence attends to all others, a method particularly effective in models like Transformers that have set new benchmarks in NLP.Soft attention assigns differentiable weights to all input elements, while hard attention selects a single element in a non-differentiable manner, often requiring reinforcement learning for optimization.Attention mechanisms enhance interpretability by highlighting which parts of the input data are most influential in the model's predictions, typically visualized through attention heatmaps.This transparency facilitates a clearer understanding of the decision-making process.Consequently, attention mechanisms are widely utilized in NLP applications such as machine translation, text summarization, and question answering.They are also employed in computer vision tasks, where they enable models to concentrate on specific regions of an image, thereby improving performance in tasks such as object detection and image captioning.</p>
<p>Saliency Maps</p>
<p>Saliency maps are a widely used visualization technique designed to interpret the decision-making process of DL models, particularly CNNs.They identify and highlight the regions of an input, such as specific pixels in an image, that are most influential in driving the model's predictions.The theoretical basis for saliency maps lies in the observation that the gradient of a model's output with respect to its input features can indicate the sensitivity of the prediction to changes in those features.In essence, these gradients can reveal which parts of the input the model considers most significant when making a prediction [37].</p>
<p>Generating a saliency map involves computing the gradient of the model's output score for a specific class with respect to each input pixel.This gradient is then visualized as a heatmap, where the magnitude of the gradient at each pixel denotes its importance to the prediction.A higher gradient value suggests that minor alterations in that pixel would lead to a substantial change in the model's output, thereby identifying the critical regions of the input that the model relies on.This method provides an intuitive understanding of the model's focus and decision-making process, particularly in complex image recognition tasks.Several variations of saliency maps offer different perspectives on feature importance.</p>
<p>1.</p>
<p>Vanilla Saliency Maps: These use the absolute value of the gradient of the output class with respect to each input pixel, providing a basic visualization of feature relevance.Guided Backpropagation enhances this approach by allowing only the gradients that positively influence the target class to flow back, thus filtering out irrelevant information and offering a more refined view of feature importance.Integrated Gradients further refine the attribution process by calculating the cumulative gradient as the input transitions from a baseline to the actual input, resulting in a more stable and comprehensive measure of feature contribution.Gradient saliency methods constitute a category of XAI techniques that utilize the gradients of a model's output with respect to its input features to determine the contribution of each feature to the model's predictions.These methods are grounded in the principle that the gradient of the output with respect to the input can indicate how sensitive the model's prediction is to small changes in the input variables.By analyzing these gradients, one can infer which features are most influential in driving the model's decisions [37].The operational process of gradient saliency methods involves computing the derivative of the model's output with respect to each input feature, resulting in a gradient vector.This vector captures the direction and magnitude of change in the prediction for infinitesimal variations in each feature.The gradients are then used to generate visualizations or attribution scores that highlight the relative importance of the input features.There are several notable gradient-based attribution techniques, each tailored to provide unique insights into model behavior: 2.</p>
<p>Gradient Saliency Maps: These use the raw gradients to generate a visual representation of feature importance.The saliency map indicates which input features, such as pixels in an image or words in a text, have the most significant impact on the model's prediction.This visualization allows for a straightforward interpretation of the model's focus and decision-making process.</p>
<p>•</p>
<p>Class Activation Mapping (CAM) and Gradient-weighted Class Activation</p>
<p>Mapping (Grad-CAM): CAM and Grad-CAM extend the concept of saliency maps by integrating class-specific gradient information with spatial feature maps from convolutional layers.CAM works by leveraging the linear relationship between convolutional feature maps and the output layer in CNNs with global average pooling (GAP).Specifically, it computes the weighted sum of the feature maps in the last convolutional layer using the weights from the output layer corresponding to a particular class [66].This yields a coarse localization map that indicates the most discriminative regions used by the model for a given prediction.Grad-CAM computes the gradient of the class score with respect to the feature maps of a target convolutional layer, and then performs a GAP on these gradients to obtain importance weights for each feature map.Grad-CAM utilizes the gradients of any target concept flowing into the final convolutional layer to produce a localization map, making it compatible with a variety of CNN-based models without architectural modifications [67].By combining the spatial awareness of CNNs with gradient information, Grad-CAM provides a more interpretable and class-discriminative visualization, which is particularly valuable for complex image-based models [37].</p>
<p>•</p>
<p>Deep-Learning Important FeaTures (DeepLIFT): DeepLIFT assigns contribution scores to each input feature by comparing the network's output to a baseline or reference output.Unlike simple gradient methods, DeepLIFT propagates these differences backward through the network, providing a more stable and interpretable measure of feature importance.This approach addresses some limitations of gradient-based methods, such as zero gradients in saturated regions of activation functions, thereby offering a more comprehensive view of feature contributions [37].</p>
<p>The primary advantage of gradient-based attribution methods is their ability to provide both local, instance-specific, and global, model-wide interpretability, making them versatile tools for understanding complex models.Gradient-based methods have broad applicability across various domains.In computer vision, they are employed to visualize feature importance in image classification, object detection, and segmentation tasks, providing insights into which parts of an image contribute most to the model's predictions.In NLP, they help identify the significance of individual words or phrases in tasks such as text classification and sentiment analysis, facilitating a deeper understanding of how models process linguistic information.In the healthcare sector, gradient-based methods are employed to evaluate the impact of clinical variables on model predictions, facilitating medical diagnosis and prognosis by identifying the factors that most significantly influence the model's decision-making process.Overall, gradient saliency methods are powerful tools for elucidating the inner workings of complex machine-learning models, offering interpretable explanations that can enhance trust, transparency, and accountability in high-stakes applications.</p>
<p>We summarize the key features of post-hoc methods discussed in this section in Table 1.</p>
<p>XAI in Healthcare</p>
<p>On average, global healthcare expenses per capita are increasing due to longer life expectancy.Thus, it increases the burden on those suffering from chronic diseases.Therefore, questions about the long-term viability of current healthcare systems are growing.AI has the potential to help address these issues by improving care quality and cost effectiveness [68].However, because of the potential fatal consequences of inaccurate predictions by an AI model, these models must be transparent and explainable.Clinicians must understand the AI decision-making processes to develop trust and enable adoption.Thus, healthcare decision-makers must be reliable, accurate, and transparent in their actions.To overcome this challenge, research efforts are ongoing to make ML and DL models interpretable [69].AI systems should provide clinicians with explicit explanations of their results, such as highlighting crucial aspects that influence diagnostic decisions in disease identification [70].</p>
<p>To elucidate the link between microbial communities and phenotypes, the SHAP method was used, which interprets model predictions depending on the contribution of each feature [71].Positive SHAP values suggest characteristics that support the projected outcome.Dopaminergic imaging modalities, such as SPECT DaTscan, have been investigated for early diagnosis of Parkinson's disease [72], with the LIME algorithm used to classify cases and provide interpretable explanations.XAI has also been used to diagnose acute critical illnesses.An early warning score system uses SHAP to explain predictions based on Electronic Health Record (EHR) data [73].Furthermore, XAI approaches have been investigated in Glioblastoma diagnosis, with models using fluid-attenuation inversion recovery data validated for multiform classification and LIME used to assess local feature significance in test samples [70].</p>
<p>An explainable computer-assisted approach for lung cancer diagnosis has been presented, which uses the LIME method to generate natural language explanations from important features [74].An ensemble clustering-based XAI model for traumatic brain injury diagnosis improved interpretability by combining expert knowledge and automated analysis [75].COVID-NET, a model for COVID-19 detection using chest X-rays, obtained 93.3% accuracy and 91.1% sensitivity after interpreting its data using GSInquire, which audits the network's internal decision-making by identifying the most influential internal features and mapping them to specific regions in chest X-ray images.This ensures the model bases its COVID-19 predictions on clinically relevant patterns rather than spurious correlations or artifacts [76].Additionally, an interpretable ML model has been constructed to predict post-stroke hospital discharge disposition [77].</p>
<p>XAI-enabled classification models for COVID-19 have been presented to produce accurate predictions and credible explanations [78].The model utilizes 380 positive and 424 negative CT volumes, aiding radiologists in localizing lesions and enhancing diagnostic insight.Early detection of sepsis is crucial, as delays can lead to irreparable organ damage and higher mortality, which is addressed by analyzing health information from the Cardiology Challenge 2019 [79].An XAI model based on 168 hourly characteristics was developed, utilizing a gradient boosting model (XGBoost) with K-fold cross-validation to predict sepsis risk and provide interpretable results in the ICU setting.A study used brain MRI scans from 1901 participants from the IXI, ADNI, and AIBL datasets to classify Alzheimer's Disease by training a model on chronological and brain age data [80].This model outperformed the existing ML approaches, with 88% accuracy for females and 92% for males.It can support both regression and classification tasks while preserving the morphological semantics of the input space and assigning feature scores to quantify the contribution of each region to the final result.Table 2 tabulates the XAI tools used in recent proposals to understand the outcomes of AI-based disease detection networks.</p>
<p>XAI Tool Modality Applications Reference</p>
<p>CAM Bone X-ray</p>
<p>The model was intended to estimate knee damage severity and pain level based on X-ray images.</p>
<p>[81]</p>
<p>CAM Lung Ultrasound and X-ray</p>
<p>The model uses three types of lung ultrasound images and VGG-16 and VGGCAM networks to classify three pneumonia subtypes.</p>
<p>[82]</p>
<p>CAM Breast X-ray</p>
<p>A globally aware multiple instance classifier (GMIC) was proposed, which uses CAM to find the most informative regions by combining local and global data.</p>
<p>[83]</p>
<p>CAM</p>
<p>Lung CT It trains the DRE-Net model on data from both healthy and COVID-19 patients.[84] Grad-CAM Lung CT A deep feature fusion method was proposed, with higher performance compared to a single CNN.</p>
<p>[85]</p>
<p>Grad-CAM Chest Ultrasound</p>
<p>A semi-supervised model integrating an attention mechanism and disentanglement was proposed, with Grad-CAM used to improve explainability.</p>
<p>[86]</p>
<p>Grad-CAM Colonoscopy It uses DenseNet121 to predict the presence of ulcerative colitis in patients.[87] Grad-CAM Chest CT A neighboring-aware graph neural network was suggested for COVID-19 detection based on chest CT images.[88] Table 2. Cont.</p>
<p>XAI Tool Modality Applications Reference</p>
<p>Grad-CAM and LIME Lung X-ray and CT The study examines five deep-learning models and uses a visualization technique to interpret NASNetLarge.[89] Attention Breast X-ray</p>
<p>The study uses the A³Net model with triple-attention learning to diagnose 14 chest illnesses.</p>
<p>[90] SHAP EHR It proposes a predicted length-of-stay strategy to solve imbalanced EHR datasets.</p>
<p>[91]</p>
<p>SHAP</p>
<p>Lung CT It introduces a model for predicting mutations in individuals with non-small cell lung cancer.[92] LIME and SHAP Chest X-ray It provides a single pipeline to improve CNN explainability using several XAI approaches.</p>
<p>[93]</p>
<p>XAI in Drug Discovery</p>
<p>In biological systems, there are intricate layers of regulation.These layers encompass dynamic interactions among genes, proteins, signaling networks, and metabolic pathways.Therapeutic targeting and drug response prediction are challenging due to the inherent variability of diseases, particularly cancer, neurodegeneration, and metabolic disorders.Despite the strong predictive capabilities of AI and ML methodologies when working with large datasets, their opaque and black-box nature frequently limits the biological interpretability of their results.XAI is an essential tool that allows researchers to understand the reasoning behind a model's specific prediction by connecting these decisions to biologically significant variables.This interpretability is crucial for enhancing trust and reproducibility, as well as for developing new hypotheses based on mechanisms that will inform future phases of drug discovery.</p>
<p>Recent advancements in explainable and interpretable AI have markedly improved the reliability and acceptance of AI models in drug discovery and healthcare.Numerous newly established frameworks illustrate the effective integration of XAI principles into drug-target interaction (DTI) prediction and molecular property modeling.DeFuseDTI and DTRE utilize advanced DL architectures in conjunction with feature attribution methods to enhance the precision and interpretability of DTI predictions, thereby facilitating more informed therapeutic decisions.ARGENT further refines this methodology by integrating attention mechanisms and interpretable embeddings, enabling researchers to correlate model predictions with distinct biological or chemical characteristics.DCGAN-DTA utilizes generative adversarial networks to predict drug-target affinity, ensuring transparency via interpretable outputs.These models underscore the growing emphasis on integrating XAI into complex predictive systems, highlighting the importance of transparency, trust, and actionable insights in modern drug-development processes.</p>
<p>XAI Tools Enabling Interpretability in Drug Discovery</p>
<p>In recent years, there has been an increase in the number of XAI tools designed to elucidate the predictions generated by complex models in drug discovery.The following tools offer case-specific interpretability, including structure-activity modeling, toxicity prediction, and molecular property analysis.A plethora of XAI tools has significantly enhanced the interpretability of complex models in drug discovery.SHAP utilizes game theory to assess the contribution of individual input features and has been extensively applied in models such as random forests, support vector machines (SVMs), and deep neural networks to identify molecular substructures affecting compound activity in Quantitative Structure-Activity Relationship (QSAR) studies [61,94].LIME creates simple surrogate models tailored to specific predictions, enabling chemists to understand the crucial structural elements that influence a compound's expected activity or toxicity [57].Combined with attention mechanisms, Graph Neural Networks (GNNs) effectively recognize critical atoms and bonds in molecular graphs, facilitating optimization guided by substructures.Integrated Gradients and DeepLIFT provide gradient-based attributions that are vital in omics-driven research, pinpointing genes or features that impact drug response classifications.Furthermore, Chemprop, a framework for predicting molecular properties, has been integrated with SHAP to clarify ADMET predictions by linking pharmacokinetic properties with specific atomic and structural features, thereby enabling informed lead optimization.Drug repositioning can be facilitated by identifying and elucidating biologically plausible compound-disease associations using GraphIX, which combines GNN with SHAP-like methods [95].InstructMol is a multimodal model that employs natural language prompts and molecular structures to create new compounds [96].It achieves this by ensuring that textual and chemical features align in an interpretable manner, enabling rationale-driven molecule design.AlphaFold 3 includes confidence scoring to identify uncertain areas in predicted protein structures [97].This makes structural drug design more reliable.Furthermore, platforms like PandaOmics and ID4 utilize explainable analytics and visualization components to assist with target discovery, disease mechanisms, and lead prioritization, enhancing transparency in AI-driven pharmaceutical processes [98].Table 3 lists the XAI tools used in the current drug-discovery processes.</p>
<p>Impact of XAI on Drug Discovery</p>
<p>The development of AI and ML has transformed drug research.Transparency and interpretability become increasingly crucial as the complexity of these models grows.XAI solves this issue by providing a better understanding of the predictions provided by ML algorithms.</p>
<p>Data Analysis</p>
<p>XAI algorithms facilitate the analysis of large and diverse datasets containing chemical, biological, and clinical information to find novel drug targets, predict medication efficacy and toxicity, and improve drug design [100].Advanced computational approaches and ML algorithms are utilized in XAI drug discovery to process and evaluate large datasets from multiple sources, such as molecular structures, biochemical tests, high-throughput screening (HTS), and preclinical and clinical trials [33].</p>
<p>AI and ML models have shown promising outcomes in areas such as lead optimization, virtual screening, chemical design, and medication repurposing [101][102][103][104].As these models evolve, they have the potential to significantly increase drug-discovery success rates while reducing time and costs.However, their predictive capacities frequently lack interpretability, making it difficult for academics, clinicians, and regulatory authorities to trust and validate the results.Without insights into model decision-making, it is difficult to evaluate and prioritize targets or compounds.XAI addresses this issue by providing clear explanations of model predictions [28], which increases trust, enables the detection of biases or inaccuracies, and facilitates a deeper understanding of model behavior [33].</p>
<p>Molecular Property Prediction</p>
<p>XAI can optimize lead compounds to enhance effectiveness, pharmacokinetics, and drug-like features, resulting in the development of more effective medications with fewer adverse effects [105].XAI in drug development improves the transparency and accountability of AI models, which are critical for lead optimization and toxicity prediction [106].This increases trust in AI-generated outcomes, encouraging their use in the pharmaceutical industry.XAI also identifies and mitigates biases, resulting in fair and accurate predictions, which are critical for avoiding the development of ineffective or harmful medications [31].Various XAI investigations have focused on unraveling molecular substructures using the gathered data in drug discovery.The authors in [94] utilize SHAP to interpret key characteristics and substructures for predicting chemical activity.Jiménez-Luna et al. [107] also used integrated gradient attribution to highlight key chemical characteristics and structural aspects in graph neural network models.</p>
<p>Personalized Medicine</p>
<p>XAI algorithms help to analyze patient data and predict individual responses to treatments, allowing for the development of personalized and effective medications [108].In drug research, XAI facilitates personalized medicine by utilizing AI to analyze large datasets for evidence-based decision-making, drug repurposing, and real-time monitoring [109].XAI methodologies, such as SHAP, LIME, and attention mechanisms, help researchers understand the molecular or biological features that influence predictions, allowing them to correlate model outputs with domain expertise and refine compound design decisions.</p>
<p>Unraveling Drug-Drug and Drug-Target Interactions</p>
<p>Drug-drug interactions (DDIs) are common in polypharmacy, when the effects of one drug might influence the actions of another in a combined therapy regimen.Ideally, such interactions produce synergistic effects as well as therapeutic advantages.However, in the treatment of multiple diseases, adverse drug events that result in toxicity or reduced efficacy may occur, thereby increasing patient morbidity and death [110,111].The current growth in the approval of new medications and indications has increased the possibility of DDIs [112,113].While wet-lab investigations to verify DDIs are time-consuming and resource-intensive, rendering them unsuitable for routine use, AI models have been used to predict DDIs better [114][115][116].Efforts have been made to improve drug database models to aid clinical decision-making.Effective DDI management is critical for maintaining pharmacovigilance and patient safety.The application of XAI in predicting DDIs has recently been extensively reviewed elsewhere [116].</p>
<p>Biomedical experiments to investigate DTIs are resource-intensive.To reduce costs and time, ML algorithms have been used to predict these interactions.The abundance of drug and target data, advances in computing technology, and the distinct capabilities of multiple ML algorithms have made them the primary tools for predicting drug-target interactions.This prediction approach aids in screening out inappropriate compounds, which is an important stage in novel drug development [117].Modeling cellular networks in cancer using AI provides a quantitative framework for investigating the association between network properties and disease, allowing the identification of potential new anticancer targets and drugs [118][119][120].The use of XAI in identifying novel anticancer targets, the ideas underlying common algorithms, and its applications in biological investigation have recently been reviewed elsewhere [121].</p>
<p>Facilitating Drug Repositioning and Combination Therapy</p>
<p>Drug repositioning entails identifying new therapeutic applications for FDA-approved drugs.This strategy focuses on assessing the efficacy of existing drugs or those under development in various pathological conditions [122,123].Since 1995, new drug approvals have been declining due to the traditional drug-development procedure, which is costly and time-consuming.Hence, drug repositioning has emerged as a potential alternative, using XAI to expedite drug discovery while lowering costs and risks [122].The significant benefits of this strategy include knowledge of drug pharmacokinetics and toxicity, as well as the low cost of implementation, which benefits low-to middle-income nations where traditional therapies may be too expensive [124].</p>
<p>Drug repositioning strategies combine computational and experimental techniques to uncover new therapeutic applications for current drugs [125,126].ML, network analysis, and NLP are three critical computing methodologies [127].These methods are classified as disease-centric, drug-centric, or combinations of both [128].Disease-centric techniques identify new applications for drugs by grouping diseases based on phenotypic commonalities, molecular markers, and genetic variants [129,130].Drug-centric techniques seek similarities in molecular action between drugs to identify new potential applications [131].Combination techniques integrate both strategies by creating drug-drug and diseasedisease similarity networks, assigning drugs based on meta-path scores, and predicting disease-drug relationships by correlating disease expression patterns with genes affected by drugs [132,133].</p>
<p>Clinical Trial Design</p>
<p>XAI enhances clinical trial design by identifying appropriate patient demographics, predicting trial success, and detecting possible adverse effects.This enables a more accurate assessment of the safety and efficacy of novel medications in humans [134].XAI can also aid in predictive modeling, patient selection, and safety precautions during drug development.</p>
<p>Ethics and Regulatory Implications</p>
<p>Lack of transparency in AI systems raises significant ethical concerns, particularly in healthcare and drug development, where decisions must be both interpretable and justifiable.Recent frameworks emphasize the importance of fairness, accountability, and human oversight in the deployment of AI.XAI contributes to these goals by exposing decision logic, identifying potential biases, and facilitating more transparent communication with regulatory bodies, clinicians, and interdisciplinary teams.</p>
<p>Key Challenges and Future Research Directions in XAI for Drug Discovery</p>
<p>Key Challenges</p>
<p>XAI is rapidly becoming a crucial element in AI-assisted drug discovery.It promotes informed decision-making in drug screening, biomarker identification, clinical trial design, and personalized medicine by enhancing model transparency, interpretability, and reliability.In clinical and biomedical settings, XAI enhances interpretability for healthcare practitioners, facilitates bias detection, improves patient communication, promotes ethical adherence, and ensures regulatory compliance.Nonetheless, despite its potential, several significant challenges must be addressed to harness XAI's capabilities in drug discovery to their maximum.</p>
<p>Data Limitations</p>
<p>To discover significant patterns, XAI models require large, high-quality datasets with diverse and varied sample spaces.However, many drug-discovery datasets are limited, incomplete, or biased, compromising model performance and interpretability.Innovative technologies, such as data augmentation, synthetic data production, and transfer learning, will be crucial in overcoming data scarcity and enhancing generalizability.</p>
<p>Complexity and Interpretability Tradeoff</p>
<p>Highly accurate models, particularly deep NNs, often operate as black boxes, offering little insight into their decision-making processes.In contrast, interpretable models may lack the predictive effectiveness necessary for complex biomedical applications.Striking a balance in this tradeoff presents a significant challenge.Developing hybrid XAI frameworks that combine predictive power with intuitive interpretability is a viable strategy and a challenge for widespread adoption.</p>
<p>Ethical and Bias Concerns</p>
<p>It is crucial to thoroughly assess the ethical implications of XAI models, particularly in terms of bias and fairness across different demographic groups.Predictions based on biased training data may exacerbate existing health disparities.The responsible use of AI in drug discovery requires rigorous validation procedures and models that are created with fairness in mind.</p>
<p>Regulatory Compliance</p>
<p>To be used in clinical or regulatory settings, XAI systems must provide clear, scientifically relevant explanations that align with expert knowledge.Regulatory bodies seek models that can be comprehended to evaluate their safety and reliability.The current application of XAI requires further development to produce understandable results that meet high safety and regulatory standards.</p>
<p>Future Research Directions</p>
<p>Multimodal Data Integration and Augmentation</p>
<p>Drug responses depend on multiple factors, including genetic variations, protein expression, metabolic pathways, and clinical phenotypes.Consequently, future research should prioritize the integration of multimodal data-including genomics, proteomics, transcriptomics, metabolomics, and real-world clinical data-to build comprehensive and context-aware models of drug action.Integrating gene expression profiles with chemical structure data significantly enhances the performance of drug sensitivity prediction models, while also improving biological plausibility and interpretability [135].Other studies demonstrate how multimodal fusion not only enhances predictive performance but also provides mechanistic insights into drug action [136].However, aligning heterogeneous data types remains a significant challenge due to differences in data scale, format, and biological context.To address this, research should also explore data augmentation strategies such as generative modeling, cross-modal embeddings, and transfer learning to enrich underrepresented data domains and improve generalization.By advancing data integration and augmentation methodologies, XAI frameworks can evolve into more resilient and biologically grounded systems, ultimately supporting safer and more personalized therapeutic development.</p>
<p>Next-Generation XAI Frameworks</p>
<p>The complex biochemical interactions and the effects of pharmaceuticals on various targets necessitate the development of innovative XAI models and frameworks.Future research should focus on models that integrate GNNs with attention-based architectures to model and interpret complex biochemical interactions accurately.GNNs are well-suited for representing molecular structures, while attention mechanisms can highlight the most dominant molecular substructures that contribute to biological activity, binding affinity, or toxicity [31,137,138].By leveraging these techniques, XAI models can provide intuitive and interpretable explanations that align with pharmacological principles, offering clarity in identifying functional groups responsible for specific pharmacological effects and highlighting structural alerts linked to adverse outcomes.When combined with cross-modal attention between molecular and protein representations, these models could also clarify drug-target binding mechanisms.Incorporating domain knowledge, such as known reaction rules, toxicophore databases, or protein-ligand interaction motifs, further enhances the biological plausibility of the explanations.This direction enables the creation of transparent, mechanism-aware AI systems that not only predict outcomes but also generate actionable hypotheses, supporting critical decision-making in hit-to-lead optimization, multitarget drug design, and safety profiling.</p>
<p>Experimental Validation and Hybrid Models</p>
<p>The integration of XAI with experimental methodologies, including molecular dynamics simulations (MDS) and high-throughput screening, can facilitate the confirmation and enhancement of computational predictions.Research efforts should focus on integrating XAI with MDS and HTS to validate and refine predictions generated by the AI models.Attention maps and feature attributions used in [138] can be used to highlight critical substructures involved in drug-target interactions.These predictions can then be evaluated using MDS to test the stability and conformational dynamics of the predicted binding modes, thereby offering physicochemical validation of model outputs.Similarly, XAIguided compound prioritization can inform HTS experiments by narrowing the chemical search space to biologically plausible candidates, enhancing hit rates and reducing false positives [137].Experimental feedback from such validation efforts can be reintegrated into training datasets to fine-tune model weights and improve generalizability, establishing a feedback loop between computation and experimentation.Furthermore, XAI can support drug repurposing by identifying alternative binding sites or off-target effects, which may then be verified through in vitro assays or biochemical profiling.This hybrid approach not only augments model performance but also advances the interpretability and scientific validity of AI-driven drug discovery, enabling the generation of testable hypotheses that are both biologically plausible and experimentally verifiable.</p>
<p>Collaborative Open Platforms</p>
<p>The MELLODDY project is a large-scale federated learning (FL) initiative in which several pharmaceutical companies collaboratively train advanced AI models without explicitly sharing their proprietary data.It leverages over 2.6 billion activity records from 21 million molecules across 40,000 assays.This enables improved predictive modeling for drug discovery while preserving data privacy and intellectual property rights.The MELLODDY project serves as a benchmark for collaborative ecosystems that facilitate the exchange of data, models, and tools, thereby expediting the development and validation of XAI frameworks.Open research platforms can enhance reproducibility, transparency, and regulatory compliance among stakeholders.</p>
<p>Ethical-by-Design Frameworks</p>
<p>Incorporating ethical considerations into the design of XAI systems is crucial for ensuring safety across diverse demographic groups.To ensure the ethical utilization of AI in healthcare and pharmaceutical development, it is imperative to integrate fairness constraints, safeguard data privacy, and promote stakeholder accountability.Fairness constraints are a primary consideration and critical in drug-discovery applications involving patient data or population-specific models, as algorithmic bias can lead to unequal access to treatment or inaccurate predictions across demographic subgroups.Racial bias in healthcare algorithms can significantly impact treatment prioritization, underscoring the need to incorporate fairness-aware modeling techniques into biomedical AI pipelines [139].Similarly, safeguarding data privacy through methods such as FL can enable large-scale collaboration without compromising sensitive information.Moreover, the development of XAI systems must be coupled with mechanisms for stakeholder accountability, ensuring that domain experts, data custodians, and AI developers are collectively responsible for model decisions and their consequences.Future research must therefore prioritize the co-design of XAI systems with ethics experts, clinicians, and regulatory bodies to create frameworks that not only explain model behavior but also align with broader safety and ethical values.This ethical-by-design approach is foundational to building trustworthy AI systems that can be safely and equitably deployed in the pharmaceutical and healthcare sectors.</p>
<p>Conclusions</p>
<p>As AI transforms drug research, the incorporation of explainability has become a fundamental requirement rather than an ancillary attribute.XAI reconciles predicted accuracy with scientific confidence by providing openness, accountability, and biological interpretability in AI models.This study highlights the growing importance of XAI tools and frameworks, which clarify the reasoning behind complex predictions, allowing researchers to make more informed, ethical, and practical decisions when designing and developing novel therapies.The future of drug development relies on integrating advanced AI models with strong interpretability, robust ethical protections, and interdisciplinary collaboration.Confronting existing limitations, such as data integrity, model complexity, and regulatory requirements, while adopting emerging technical breakthroughs, will ensure that AI tech-nologies are both effective and trustworthy in clinical contexts.Progressing this research area necessitates a purposeful transition to next-generation XAI research emphasizing transparency, inclusion, and fairness.XAI can expedite drug-development timeframes, mitigate risks, and facilitate more tailored and accountable therapeutic approaches.The meticulous implementation of this approach will characterize the forthcoming epoch of pharmaceutical research, whereby data-driven discovery is both insightful and comprehensible.</p>
<p>Figure 2 .
2
Figure 2. AI-supported drug-development pipeline.AI can potentially accelerate the progress of each stage, from target identification to post-market surveillance.This is achieved by enabling faster compound screening, predictive modeling, clinical trial optimization, and safety monitoring, thus improving efficiency and reducing development timelines.</p>
<p>Figure 3 .
3
Figure 3. Types of AI models classified according to their interpretability.There is a tradeoff between the interpretability and accuracy of the AI model.</p>
<p>Figure 4 .
4
Figure 4. Outline of classification of XAI models.</p>
<p>Table 1 .
1
Comparison of the key post-hoc XAI techniques used in drug discovery.
TechniqueBasic Working PrincipleInput TypeRequirementsUses cooperative gameSHAPtheory. Assigns each feature an importanceTabular, molecular descriptors, genomic dataHighvalue for a prediction</p>
<p>Table 1 .
1
Cont.
TechniqueBasic Working PrincipleInput TypeRequirementsPerturbs input locally andLIMEfits a simple interpretable model to approximateTabular, image, textModeratethe predictionShows average predictedPartial Dependence Plotsoutcome as a function of one or two features,TabularLow to moderatemarginalizing othersAttentionAllocates weights to input elements, indicating their contributionSequences like SMILES, molecular graphsModerateSaliency MapsComputes gradients of the output with respect to input featuresImage, 2D/3D molecular structuresModerateMeasures the sensitivity ofGradient Saliencyoutput to small perturbations in input byText, image, sequenceModeratecomputing gradients</p>
<p>Table 2 .
2
Summary of XAI models used for healthcare applications.</p>
<p>Table 3 .
3
Summary of XAI models used in identifying interactions for the development of drugs.
Tool/PlatformDescriptionApplications in Drug DiscoveryReferenceInterpreting ML predictions in QSARSHAPA model-agnostic method that assigns each feature an importance value for a particular predictionand SAR studies, identifying key transparency in model-guided molecular features influencing compound activity, and increasing[61,94]drug designExplains the predictions of anyUnderstanding model decisions inLIMEclassifier by approximating it locallycompound activity prediction and[60]with an interpretable modeltoxicity assessmentsDeepLIFTAttributes importance scores to each input feature by comparing the activation to a reference activationInterpreting DL models in genomics and proteomics data analysis[37]Integrated GradientsAssigns feature importance by integrating gradients of the model's output with respect to the inputsExplaining deep neural networks in molecular property prediction[99]Predicts protein structures and theirAccelerating target identification andAlphaFold 3interactions with high accuracyunderstanding protein-ligand[97]using AIinteractions.A graph-based XAI framework forIdentifying potential new uses forGraphIXdrug repositioning usingexisting drugs by analyzing[95]biopharmaceutical networksbiological networksIntegrates molecular graph data andEnhances the foundation for XAI inInstructMolSMILES sequences with natural language by fine-tuning adrug discovery by aligning molecular structures with natural language[96]pretrained LLMthrough instruction tuningPandaOmicsAn AI-driven platform for target discovery and biomarker identificationDiscovering novel therapeutic targets and biomarkers in various diseases[98]
Funding: This work is funded by the Norwegian Research Council under the grant name SecureIoT.Conflicts of Interest:The authors declare no conflicts of interest.AbbreviationsThe following abbreviations are used in this manuscript:AI
Human Genome Project: Big science transforms biology and medicine. L Hood, L Rowen, The, Genome Med. 2013, 5, 79</p>
<p>Developing therapeutic approaches for twenty-first-century emerging infectious viral diseases. R M Meganck, R S Baric, 10.1038/s41591-021-01282-0Nat. Med. 272021</p>
<p>The role of the medicinal chemist in drug discovery-then and now. J G Lombardino, J A Lowe, 10.1038/nrd1523Nat. Rev. Drug Discov. 32004</p>
<p>Mss51 protein inhibition serves as a novel target for type 2 diabetes: A molecular docking and simulation study. S Ali, K Ahmad, S Shaikh, H J Chun, I Choi, E J Lee, 10.1080/07391102.2023.2223652J. Biomol. Struct. Dyn. 422024</p>
<p>Identification and Evaluation of Traditional Chinese Medicine Natural Compounds as Potential Myostatin Inhibitors: An In Silico Approach. S Ali, K Ahmad, S Shaikh, J H Lim, H J Chun, S S Ahmad, E J Lee, I Choi, Molecules. 43032022</p>
<p>Computational Identification of Dithymoquinone as a Potential Inhibitor of Myostatin and Regulator of Muscle Mass. S S Ahmad, K Ahmad, E J Lee, S Shaikh, I Choi, 10.3390/molecules26175407Molecules. 2654072021</p>
<p>Drug Discovery and the Medicinal Chemist. B E Maryanoff, 10.4155/fmc.09.2Future Med. Chem. 12009</p>
<p>Pharmacokinetic and Pharmacodynamic Properties of Drug Delivery Systems. P M Glassman, V R Muzykantov, 10.1124/jpet.119.257113J. Pharmacol. Exp. Ther. 3702019</p>
<p>Virtual Insights into Natural Compounds as Potential 5α-Reductase Type II Inhibitors: A Structure-Based Screening and Molecular Dynamics Simulation Study. S Shaikh, S Ali, J H Lim, K Ahmad, K S Han, E J Lee, I Choi, 10.3390/life13112152Life. 132023. 2152</p>
<p>PK/PD models in antibacterial development. T Velkov, P J Bergen, J Lora-Tamayo, C B Landersdorfer, J Li, 10.1016/j.mib.2013.06.010Curr. Opin. Microbiol. 162013</p>
<p>Lessons learned from the fate of AstraZeneca's drug pipeline: A five-dimensional framework. D Cook, D Brown, R Alexander, R March, P Morgan, G Satterthwaite, M N Pangalos, 10.1038/nrd4309Nat. Rev. Drug Discov. 132014</p>
<p>Impact of a five-dimensional framework on R&amp;D productivity at AstraZeneca. P Morgan, D G Brown, S Lennard, M J Anderton, J C Barrett, U Eriksson, M Fidock, B Hamrén, A Johnson, R E March, Nat. Rev. Drug Discov. 172018</p>
<p>Drug discovery and development: Introduction to the general public and patient groups. N Singh, P Vayer, S Tanwar, J L Poyet, K Tsaioun, B O Villoutreix, 10.3389/fddsv.2023.1201419Front. Drug Discov. 2023, 3, 1201419. [CrossRef</p>
<p>Omics"-Informed Drug and Biomarker Discovery: Opportunities, Challenges and Future Perspectives. H Matthews, J Hanison, N Nirmalan, 10.3390/proteomes4030028Proteomes. 2016, 4, 28</p>
<p>Artificial intelligence in drug discovery and development. D Paul, G Sanap, S Shenoy, D Kalyane, K Kalia, R K Tekade, 10.1016/j.drudis.2020.10.010Drug Discov. Today. 262021</p>
<p>. A Bhardwaj, S Kishore, D K Pandey, 10.3390/life12091430Artificial Intelligence in Biological Sciences. Life. 122022. 1430</p>
<p>. E Lawrence, A El-Shazly, S Seal, C K Joshi, P Liò, S Singh, A Bender, P Sormanni, M Greenig, 10.48550/arXiv.2403.04106arXiv:2403.04106Understanding Biology in the Age of Artificial Intelligence. arXiv. 2024</p>
<p>Artificial intelligence in healthcare: Past, present and future. Stroke Vasc. F Jiang, Y Jiang, H Zhi, Y Dong, H Li, S Ma, Y Wang, Q Dong, H Shen, Y Wang, 10.1136/svn-2017-000101Neurol. 22017</p>
<p>Convolutional neural networks for medical image analysis: State-of-the-art, comparisons, improvement and perspectives. H Yu, L T Yang, Q Zhang, D Armstrong, M J Deen, 10.1016/j.neucom.2020.04.157Neurocomputing. 4442021</p>
<p>Applying Recurrent Neural Networks for Anomaly Detection in Electrocardiogram Sensor Data. A Minic, L Jovanovic, N Bacanin, C Stoean, M Zivkovic, P Spalevic, A Petrovic, M Dobrojevic, R Stoean, 10.3390/s23249878Sensors. 232023</p>
<p>A survey of large language models for healthcare: From data, technology, and applications to accountability and ethics. K He, R Mao, Q Lin, Y Ruan, X Lan, M Feng, E Cambria, 10.1016/j.inffus.2025.102963Inf. Fusion. 1182025. 102963</p>
<p>Large Language Models in Genomics-A Perspective on Personalized Medicine. S Ali, Y A Qadri, K Ahmad, Z Lin, M F Leung, S W Kim, A V Vasilakos, T Zhou, 10.3390/bioengineering12050440202512440</p>
<p>AI/ML)-Enabled Medical Devices. Artificial Intelligence and Machine Learning. 2025. May 2025</p>
<p>Artificial intelligence in drug development: Present status and future prospects. K K Mak, M R Pichika, 10.1016/j.drudis.2018.11.014Drug Discov. Today. 242019</p>
<p>Artificial intelligence in drug development. K Zhang, X Yang, Y Wang, Y Yu, N Huang, G Li, X Li, J Wu, S Yang, 10.1038/s41591-024-03434-4Nat. Med. 312025</p>
<p>Artificial intelligence in drug discovery. M A Sellwood, M Ahmed, M H S Segler, N Brown, 10.4155/fmc-2018-0212Future Med. Chem. 102018</p>
<p>Machine Learning guided early drug discovery of small molecules. N Pillai, A Dasgupta, S Sudsakorn, J Fretland, P D Mavroudis, 10.1016/j.drudis.2022.03.017Drug Discov. Today. 272022</p>
<p>Explainability and White Box in Drug Discovery. K K Kırbo Ga, S Abbasi, E Küçüksille, Chem. Biol. Drug Des. 1012023</p>
<p>Explainable Artificial Intelligence in the Field of Drug Research. Q Ding, R Yao, Y Bai, L Da, Y Wang, R Xiang, X Jiang, F Zhai, 10.2147/DDDT.S525171Drug Des. Dev. Ther. 192025</p>
<p>Explainable artificial intelligence: A taxonomy and guidelines for its application to drug discovery. I Ponzoni, L Capoferri, P A B Reis, J D Holliday, A Bender, 10.1002/wcms.1681WIREs Comput. Mol. Sci. 132023. e1681</p>
<p>Drug discovery with explainable artificial intelligence. J Jiménez-Luna, F Grisoni, G Schneider, 10.1038/s42256-020-00236-4Nat. Mach. Intell. 22020</p>
<p>On the Road to Explainable AI in Drug-Drug Interactions Prediction: A Systematic Review. T Vo, N Nguyen, Q Kha, N Le, 10.1016/j.csbj.2022.04.021Comput. Struct. Biotechnol. J. 202022</p>
<p>Explainable Artificial Intelligence for Drug Discovery and Development-A Comprehensive Survey. R Alizadehsani, S S Oyelere, S Hussain, S K Jagatheesaperumal, R R Calixto, M Rahouti, 10.1109/ACCESS.2024.3373195IEEE Access. 122024</p>
<p>Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence. S Ali, T Abuhmed, S El-Sappagh, K Muhammad, J M Alonso-Moral, R Confalonieri, R Guidotti, J Del Ser, N Díaz-Rodríguez, F Herrera, 10.1016/j.inffus.2023.101805Inf. Fusion. 992023. 101805</p>
<p>Methods for interpreting and understanding deep neural networks. G Montavon, W Samek, K R Müller, 10.1016/j.dsp.2017.10.011Digit. Signal Process. 732018</p>
<p>Notions of explainability and evaluation approaches for explainable artificial intelligence. G Vilone, L Longo, 10.1016/j.inffus.2021.05.009Inf. Fusion. 762021</p>
<p>A Das, P Rad, arXiv:2006.11371Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey. 2020</p>
<p>An Explainable Self-Labeling Grey-Box Model. B Seddik, D Ahlem, C Hocine, 10.1109/PAIS56586.2022.9946912Proceedings of the 2022 4th International Conference on Pattern Analysis and Intelligent Systems (PAIS). the 2022 4th International Conference on Pattern Analysis and Intelligent Systems (PAIS)Oum El Bouaghi, Algeria12-13 October 2022</p>
<p>Interpreting Black-Box Models. V Hassija, V Chamola, A Mahapatra, A Singal, D Goel, K Huang, S Scardapane, I Spinelli, M Mahmud, A Hussain, 10.1007/s12559-023-10179-8A Review on Explainable Artificial Intelligence. Cogn. Comput. 162024</p>
<p>Explainable AI (XAI): Core ideas, techniques and solutions. R Dwivedi, D Dave, H Naik, S Singhal, O Rana, P Patel, B Qian, Z Wen, T Shah, G Morgan, 10.1145/3561048ACM ComputSurv. 2023, 55, 194. [CrossRef</p>
<p>Interpretability of machine learning-based prediction models in healthcare. G Stiglic, P Kocbek, N Fijacko, M Zitnik, K Verbert, L Cilar, 10.1002/widm.1379Rev. Data Min. Knowl. Discov. 2020, 10, e1379Wiley Interdiscip</p>
<p>Interpretable Machine Learning: A brief survey from the predictive maintenance perspective. S Vollert, M Atzmueller, A Theissler, 10.1109/ETFA45728.2021.9613467Proceedings of the 2021 IEEE 26th International Conference on Emerging Technologies and Factory Automation (ETFA). the 2021 IEEE 26th International Conference on Emerging Technologies and Factory Automation (ETFA)Online7-10 September 2021</p>
<p>A Survey on Explainable Artificial Intelligence Techniques and Challenges. A Hanif, X Zhang, S Wood, 10.1109/EDOCW52865.2021.00036Proceedings of the 2021 IEEE 25th International Enterprise Distributed Object Computing Conference Workshops (EDOCW). the 2021 IEEE 25th International Enterprise Distributed Object Computing Conference Workshops (EDOCW)Gold Coast, AustraliaOctober 2021</p>
<p>A M Salih, Y Wang, 10.48550/arXiv.2407.12177arXiv:2407.12177Are Linear Regression Models White Box and Interpretable? arXiv 2024. </p>
<p>Experimental Analysis of Methods Used to Solve Linear Regression Models. M Abu-Faraj, A Al-Hyari, Z A Alqadi, 10.32604/cmc.2022.027364Comput. Mater. Contin. 722022</p>
<p>Linear regression. T M H Hope, 10.1016/B978-0-12-815739-8.00004-3Machine Learning: Methods and Applications to Brain Disorders. A Mechelli, S Vieira, London, UKAcademic Press2020</p>
<p>A comprehensive survey on regularization strategies in machine learning. Y Tian, Y Zhang, Inf. Fusion. 802022</p>
<p>Regularized target encoding outperforms traditional methods in supervised machine learning with high cardinality features. F Pargent, F Pfisterer, J Thomas, B Bischl, 10.1007/s00180-022-01207-6Comput. Stat. 372022</p>
<p>Deep Learning. I Goodfellow, Y Bengio, A Courville, 2016MIT PressCambridge, MA, USA</p>
<p>Post-hoc vs ante-hoc explanations: XAI design guidelines for data scientists. C O Retzlaff, A Angerschmid, A Saranti, D Schneeberger, R Röttger, H Müller, A Holzinger, 10.1016/j.cogsys.2024.101243Cogn. Syst. Res. 861012432024</p>
<p>R Agarwal, L Melnick, N Frosst, X Zhang, B Lengerich, R Caruana, G Hinton, arXiv:2004.13912Neural Additive Models: Interpretable Machine Learning with Neural Nets. 2020</p>
<p>Inference and computation with generalized additive models and their extensions. TEST. S N Wood, 10.1007/s11749-020-00711-5202029</p>
<p>. F Oviedo, J Lavista Ferres, T Buonassisi, K T Butler, Explainable Interpretable, 10.1021/accountsmr.1c00244Machine Learning for Materials Science and Chemistry. Accounts Mater. Res. 2022</p>
<p>Explainable Artificial Intelligence (XAI) in Biomedicine: Making AI Decisions Trustworthy for Physicians and Patients. J Lötsch, D Kringel, A Ultsch, 10.3390/biomedinformatics2010001BioMedInformatics. 2022</p>
<p>. Y Izza, A Ignatiev, Marques-Silva, 10.48550/arXiv.2010.11034arXiv:2010.11034J. On Explaining Decision Trees. arXiv. 2020</p>
<p>Towards consistency of rule-based explainer and black box model: fusion of rule induction and XAI-based feature importance. M Kozielski, M Sikora, Ł Wawrowski, 10.1016/j.knosys.2025.113092arXiv:2407.145432024</p>
<p>. M Cesarini, L Malandri, F Pallucchini, A Seveso, F Xing, 10.1007/s12559-024-10325-wExplainable AI for Text Classification: Lessons from a Comprehensive Evaluation of Post Hoc Methods. Cogn. Comput. 162024</p>
<p>Explainable AI with Python. L Gianfagna, A Di Cecco, 10.1007/978-3-030-68640-62021SpringerCham, Switzerland</p>
<p>J Dieber, S Kirrane, 10.48550/arXiv.2012.00093arXiv:2012.00093Why model why? Assessing the strengths and limitations of LIME. 2020</p>
<p>Why Should I Trust You? Explaining the Predictions of Any Classifier. M T Ribeiro, S Singh, C Guestrin, 10.1145/2939672.2939778Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16). the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16)San Francisco, CA, USAAugust 2016</p>
<p>S M Lundberg, S I Lee, 10.48550/arXiv.1705.07874arXiv:1705.07874A Unified Approach to Interpreting Model Predictions. 2017</p>
<p>. A M Salih, Z Raisi-Estabragh, I Boscolo Galazzo, P Radeva, S E Petersen, K Lekadir, G Menegaz, 10.1002/aisy.202400304A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME. Adv. Intell. Syst. 2024, 7, 2400304. [CrossRef</p>
<p>A Review of Taxonomies of Explainable Artificial Intelligence (XAI) Methods. T Speith, Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22)Seoul, Republic of KoreaJune 2022</p>
<p>Beyond Explaining: Opportunities and Challenges of XAI-Based Model Improvement. L Weber, S Lapuschkin, A Binder, W Samek, 10.1016/j.inffus.2022.11.013Inf. Fusion. 922023</p>
<p>Attention Mechanisms in Deep Learning: Towards Explainable Artificial Intelligence. N E H Dehimi, Z Tolba, 10.1109/PAIS62114.2024.10541203Proceedings of the 2024 6th International Conference on Pattern Analysis and Intelligent Systems (PAIS). the 2024 6th International Conference on Pattern Analysis and Intelligent Systems (PAIS)Oum El Bouaghi, AlgeriaApril 2024</p>
<p>Learning Deep Features for Discriminative Localization. B Zhou, A Khosla, A Lapedriza, A Oliva, A Torralba, 10.48550/arXiv.1512.04150arXiv:1512.041502015</p>
<p>Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, 10.1007/s11263-019-01228-7Int. J. Comput. Vis. 1282020</p>
<p>From Bit to Bedside: A Practical Framework for Artificial Intelligence Product Development in Healthcare. D Higgins, V I Madai, 10.1002/aisy.202000052Adv. Intell. Syst. 22020. 2000052</p>
<p>Designing Interpretable ML System to Enhance Trust in Healthcare: A Systematic Review to Proposed Responsible Clinician-AI-Collaboration Framework. E Nasarian, R Alizadehsani, U R Acharya, K L Tsui, 10.1016/j.inffus.2024.102412Inf. Fusion. 1082024. 102412</p>
<p>M Rucco, G Viticchi, L Falsetti, 10.3390/math8050770Towards Personalized Diagnosis of Glioblastoma in Fluid-Attenuated Inversion Recovery (FLAIR) by Topological Interpretable Machine Learning. Mathematics. 20208770</p>
<p>Explainable AI reveals changes in skin microbiome composition linked to phenotypic differences. A P Carrieri, N Haiminen, S Maudsley-Barton, L J Gardiner, B Murphy, A E Mayes, S Paterson, S Grimshaw, M Winn, C Shand, 10.1038/s41598-021-83922-6Sci. Rep. 1145652021</p>
<p>An Explainable Machine Learning Model for Early Detection of Parkinson's Disease using LIME on DaTSCAN Imagery. P R Magesh, R D Myloth, R J Tom, 10.1016/j.compbiomed.2020.104041Comput. Biol. Med. 1262020. 104041</p>
<p>Explainable artificial intelligence model to predict acute critical illness from electronic health records. S M Lauritsen, M Kristensen, M V Olsen, M S Larsen, K M Lauritsen, M J Jørgensen, J Lange, B Thiesson, 10.1038/s41467-020-17431-xNat. Commun. 1138522020</p>
<p>The natural language explanation algorithms for the lung cancer computer-aided diagnosis system. A Meldo, L Utkin, M Kovalev, E Kasimov, 10.1016/j.artmed.2020.101952Artif. Intell. Med. 1082020. 101952</p>
<p>An Explainable and Statistically Validated Ensemble Clustering Model Applied to the Identification of Traumatic Brain Injury Subgroups. D Yeboah, L Steinmeister, D B Hier, B Hadi, D C Wunsch, G R Olbricht, T Obafemi-Ajayi, 10.1109/ACCESS.2020.3027453IEEE Access. 82020</p>
<p>COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images. L Wang, Z Q Lin, A Wong, 10.1038/s41598-020-76550-zSci. Rep. 102020. 19549</p>
<p>Predicting Post-stroke Hospital Discharge Disposition Using Interpretable Machine Learning Approaches. L Yao, A R Syed, M H Rahman, M M Rahman, R E Foraker, I Banerjee, 10.1109/BigData47090.2019.9006592Proceedings of the 2019 IEEE International Conference on Big Data (Big Data). the 2019 IEEE International Conference on Big Data (Big Data)Angeles, CA, USA; Piscataway, NJ, USA9-12 December 2019. 2019IEEE</p>
<p>Q Ye, J Xia, G Yang, 10.48550/arXiv.2104.14506arXiv:2104.14506Explainable AI For COVID-19 CT Classifiers: An Initial Comparison Study. arXiv 2021. </p>
<p>Early Prediction of Sepsis From Clinical Data: The PhysioNet/Computing in Cardiology Challenge. M A Reyna, C S Josef, R Jeter, S P Shashikumar, M B Westover, S Nemati, G D Clifford, A Sharma, 10.1097/CCM.0000000000004145Crit. Care Med. 482020</p>
<p>Classification-Biased Apparent Brain Age for the Prediction of Alzheimer's Disease. A Varzandian, M A S Razo, M R Sanders, A Atmakuru, G D Fatta, 10.3389/fnins.2021.673120Front. Neurosci. 152021</p>
<p>An algorithmic approach to reducing unexplained pain disparities in underserved populations. E Pierson, D M Cutler, J Leskovec, S Mullainathan, Z Obermeyer, 10.1038/s41591-020-01192-7Nat. Med. 272021</p>
<p>. J Born, N Wiedemann, M Cossio, C Buhre, G Brändle, K Leidermann, J Goulet, A Aujayeb, M Moor, B Rieck, 10.3390/app11020672Accelerating Detection of Lung Pathologies with Explainable Ultrasound Image Analysis. Appl. Sci. 2021672</p>
<p>An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization. Y Shen, N Wu, J Phang, J Park, K Liu, S Tyagi, L Heacock, S G Kim, L Moy, K Cho, 10.1016/j.media.2020.101908Med. Image Anal. 682021. 101908</p>
<p>Deep Learning Enables Accurate Diagnosis of Novel Coronavirus (COVID-19) with CT Images. Y Song, S Zheng, L Li, X Zhang, X Zhang, Z Huang, J Chen, R Wang, H Zhao, Y Zha, 10.1109/TCBB.2021.3065361IEEE/ACM Trans. Comput. Biol. Bioinform. 182021</p>
<p>COVID-19 classification by FGCNet with deep feature fusion from graph convolutional network and convolutional neural network. S H Wang, V V Govindaraj, J M Górriz, X Zhang, Y D Zhang, 10.1016/j.inffus.2020.10.004Inf. Fusion. 672021</p>
<p>Joint localization and classification of breast masses on ultrasound images using an auxiliary attention-based framework. Z Fan, P Gong, S Tang, C U Lee, X Zhang, P Song, S Chen, H Li, 10.1016/j.media.2023.102960Med. Image Anal. 902023. 102960</p>
<p>Artificial intelligence enabled automated diagnosis and grading of ulcerative colitis endoscopy images. R T Sutton, O R Zaïane, R Goebel, D C Baumgart, 10.1038/s41598-022-06726-2Sci. Rep. 122022. 2748</p>
<p>Classification of COVID-19 based on neighboring aware representation from deep graph neural network. S Lu, Z Zhu, J M Gorriz, S H Wang, Y D Zhang, Nagnn, 10.1002/int.22686Int. J. Intell. Syst. 372022</p>
<p>Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray images using fine-tuned deep neural networks. N S Punn, S Agarwal, 10.1007/s10489-020-01900-3Appl. Intell. 512021</p>
<p>Triple attention learning for classification of 14 thoracic diseases using chest radiography. H Wang, S Wang, Z Qin, Y Zhang, R Li, Y Xia, 10.1016/j.media.2020.101846Med. Image Anal. 672021. 101846</p>
<p>An explainable machine learning framework for lung cancer hospital length of stay prediction. B Alsinglawi, O Alshari, M Alorjani, O Mubin, F Alnajjar, M Novoa, O Darwish, 10.1038/s41598-021-04608-7Sci. Rep. 126072022</p>
<p>Machine Learning-Based Radiomics Signatures for EGFR and KRAS Mutations Prediction in Non-Small-Cell Lung Cancer. N Q K Le, Q H Kha, V H Nguyen, Y C Chen, S J Cheng, C Y Chen, 10.3390/ijms22179254Int. J. Mol. Sci. 2292542021</p>
<p>LISA: Enhance the explainability of medical images unifying current XAI techniques. S H P Abeyagunasekera, Y Perera, K Chamara, U Kaushalya, P Sumathipala, O Senaweera, 10.1109/I2CT54291.2022.9824840Proceedings of the 2022 IEEE 7th International Conference for Convergence in Technology (I2CT). the 2022 IEEE 7th International Conference for Convergence in Technology (I2CT)Pune, IndiaApril 2022</p>
<p>Interpretation of Compound Activity Predictions from Complex Machine Learning Models Using Local Approximations and Shapley Values. R Rodríguez-Pérez, J Bajorath, 10.1021/acs.jmedchem.9b01101J. Med. Chem. 632020</p>
<p>GraphIX: Graph-based In silico XAI (explainable artificial intelligence) for drug repositioning from biopharmaceutical network. A Takagi, M Kamada, E Hamatani, R Kojima, Y Okuno, arXiv:2212.107882022</p>
<p>H Cao, Z Liu, X Lu, Y Yao, Y Li, Instructmol, arXiv:2311.16208v2Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery. arXiv 2024. </p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, 10.1038/s41586-021-03819-2Nature. 5962021</p>
<p>PandaOmics: An AI-Driven Platform for Therapeutic Target and Biomarker Discovery. P Kamya, I V Ozerov, F W Pun, K Tretina, T Fokina, S Chen, V Naumov, X Long, S Lin, M Korzinkin, 10.1021/acs.jcim.3c01619J. Chem. Inf. Model. 642024</p>
<p>Axiomatic Attribution for Deep Networks. M Sundararajan, A Taly, Q Yan, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, AustraliaAugust 2017</p>
<p>. D Precup, Y W Teh, 2017New York, NY, USA</p>
<p>Interpretation of structure-activity relationships in real-world drug design data sets using explainable artificial intelligence. T Harren, H Matter, G Hessler, M Rarey, C Grebner, 10.1021/acs.jcim.1c01263J. Chem. Inf. Model. 622022</p>
<p>Structure-Based Virtual Screening: From Classical to Artificial Intelligence. E H B Maia, L H M De Souza, R T De Souza, A D Andricopulo, 10.3389/fchem.2020.00343Front. Chem. 83432020</p>
<p>Rethinking Drug Design in the Artificial Intelligence Era. P Schneider, W P Walters, A T Plowright, N Sieroka, J Listgarten, R A Goodnow, Jr, J Fisher, J M Jansen, J S Duca, T S Rush, 10.1038/s41573-019-0050-3Nat. Rev. Drug Discov. 192020</p>
<p>Drug Repurposing Strategy (DRS): Emerging Approach to Identify Potential Therapeutics for Treatment of Novel Coronavirus Infection. B M Sahoo, B V V R Kumar, J Sruti, M K Mahapatra, B K Banik, P Borah, 10.3389/fmolb.2021.628144Front. Mol. Biosci. 2021, 8, 628144</p>
<p>Identification of active compounds as novel dipeptidyl peptidase-4 inhibitors through machine learning and structure-based molecular docking simulations. S Ali, S Shaikh, K Ahmad, I Choi, 10.1080/07391102.2023.2292299J. Biomol. Struct. Dyn. 432025</p>
<p>. Danishuddin, </p>
<p>A decade of machine learning-based predictive models for human pharmacokinetics: Advances and challenges. V Kumar, M Faheem, K W Lee, 10.1016/j.drudis.2021.09.013Drug Discov. Today. 272022</p>
<p>Quantitative evaluation of explainable graph neural networks for molecular property prediction. J Rao, S Zheng, Y Lu, Y Yang, 10.1016/j.patter.2022.100628Patterns 2022, 3, 100628. [CrossRef</p>
<p>Coloring molecules with explainable artificial intelligence for preclinical relevance assessment. J Jiménez-Luna, M Skalic, N Weskamp, G Schneider, 10.1021/acs.jcim.0c01344J. Chem. Inf. Model. 612021</p>
<p>The fourth scientific discovery paradigm for precision medicine and healthcare: Challenges ahead. Precis. L Shen, J Bai, W Jiao, B Shen, 10.1093/pcmedi/pbab007Clin. Med. 42021</p>
<p>Neuro-Symbolic XAI: Application to Drug Repurposing for Rare Diseases. M Drancé, Database Systems for Advanced Applications. </p>
<p>. A Bhattacharya, J Lee Mong Li, D Agrawal, P K Reddy, M Mohania, A Mondal, V Goyal, Uday Kiran, R.2022Springer International PublishingCham, Switzerland</p>
<p>Frequency and nature of drug-drug interactions in the intensive care unit. M Askari, S Eslami, M Louws, P C Wierenga, D A Dongelmans, R A Kuiper, A Abu-Hanna, 10.1002/pds.3415Pharmacoepidemiol. Drug Saf. 222013</p>
<p>Drug-Drug Interactions in Elderly Patients with Potentially Inappropriate Medications in Primary Care, Nursing Home and Hospital Settings: A Systematic Review and a Preliminary Study. M Bories, G Bouzillé, M Cuggia, P Le Corre, 10.3390/pharmaceutics13020266Pharmaceutics. 132021</p>
<p>Evaluation of three brands of drug interaction software for use in intensive care units. A M M Reis, S H D B Cassiani, 10.1007/s11096-010-9445-2Pharm. World Sci. 322010</p>
<p>Evaluation of frequently used drug interaction screening programs. P Vonbach, A Dubied, S Krähenbühl, J H Beer, 10.1007/s11096-008-9191-xPharm. World Sci. 302008</p>
<p>Machine learning-based prediction of drug-drug interactions by integrating drug phenotypic, therapeutic, chemical, and genomic properties. F Cheng, Z Zhao, 10.1136/amiajnl-2013-002512J. Am. Med. Inform. Assoc. 212014</p>
<p>Deep learning improves prediction of drug-drug and drug-food interactions. J Y Ryu, H U Kim, S Y Lee, 10.1073/pnas.1803294115Proc. Natl. Acad. Sci. Natl. Acad. SciUSA2018115</p>
<p>Similarity-based modeling in large-scale prediction of drug-drug interactions. S Vilar, E Uriarte, L Santana, T Lorberbaum, G Hripcsak, C Friedman, N P Tatonetti, 10.1038/nprot.2014.151Nat. Protoc. 92014</p>
<p>Application of Machine Learning for Drug-Target Interaction Prediction. L Xu, X Ru, R Song, Front. Genet. 126801172021</p>
<p>Network approaches and applications in biology. T Ideker, R Nussinov, 10.1371/journal.pcbi.1005771PLoS Comput. Biol. 132017. e1005771</p>
<p>MiR-205-5p and miR-342-3p cooperate in the repression of the E2F1 transcription factor in the context of anticancer chemotherapy resistance. X Lai, S K Gupta, U Schmitz, S Marquardt, S Knoll, A Spitschak, O Wolkenhauer, B M Pützer, J Vera, 10.7150/thno.19904Theranostics. 82018</p>
<p>Systems biology-based investigation of cooperating microRNAs as monotherapy or adjuvant therapy in cancer. X Lai, M Eberhardt, U Schmitz, J Vera, 10.1093/nar/gkz638Nucleic Acids Res. 472019</p>
<p>Artificial intelligence in cancer target identification and drug discovery. Y You, X Lai, Y Pan, H Zheng, J Vera, S Liu, S Deng, L Zhang, 10.1038/s41392-022-00994-0Signal Transduct. Target. Ther. 72022</p>
<p>A novel computational approach for drug repurposing using systems biology. A Peyvandipour, N Saberian, A Shafi, M Donato, S Drȃghici, 10.1093/bioinformatics/bty133Bioinformatics. 342018</p>
<p>Drug-repositioning opportunities for cancer therapy: Novel molecular targets for known compounds. R Würth, S Thellung, A Bajetto, M Mazzanti, T Florio, F Barbieri, 10.1016/j.drudis.2015.09.017Drug Discov. Today. 212016</p>
<p>Repurposing drugs in your medicine cabinet: Untapped opportunities for cancer therapy?. P Pantziarka, G Bouche, L Meheus, V Sukhatme, V P Sukhatme, 10.2217/fon.14.244Future Oncol. 112015</p>
<p>A review of computational drug repurposing. K Park, 10.12793/tcp.2019.27.2.59Transl. Clin. Pharmacol. 272019</p>
<p>Explainable Artificial Intelligence in High-Throughput Drug Repositioning for Subgroup Stratifications with Interventionable Potential. Z Al-Taie, D Liu, J B Mitchem, C Papageorgiou, J T Kaifi, W C Warren, C R Shyu, 10.1016/j.jbi.2021.103792J. Biomed. Inform. 1182021. 103792</p>
<p>Review of Drug Repositioning Approaches and Resources. H Xue, J Li, H Xie, Y Wang, Int. J. Biol. Sci. 142018</p>
<p>Heter-LP: A heterogeneous label propagation algorithm and its application in drug repositioning. M Lotfi Shahreza, N Ghadiri, S R Mousavi, J Varshosaz, J R Green, 10.1016/j.jbi.2017.03.006J. Biomed. Inform. 682017</p>
<p>PhenoPredict: A disease phenome-wide drug repositioning approach towards schizophrenia drug discovery. R Xu, Q Wang, 10.1016/j.jbi.2015.06.027J. Biomed. Inform. 562015</p>
<p>A genomics-based systems approach towards drug repositioning for rheumatoid arthritis. R Xu, Q Wang, 10.1186/s12864-016-2910-0BMC Genom. 172016</p>
<p>The Connectivity Map: A new tool for biomedical research. J Lamb, 10.1038/nrc2044Nat. Rev. Cancer. 72007</p>
<p>DeSigN: Connecting gene expression with therapeutics for drug repurposing and development. B K B Lee, K H Tiong, J K Chang, C S Liew, Z A Abdul Rahman, A C Tan, T F Khang, S C Cheong, 10.1186/s12864-016-3260-72017BMC Genom18</p>
<p>Computational drug repositioning using meta-path-based semantic network analysis. Z Tian, Z Teng, S Cheng, M Guo, 10.1186/s12918-018-0658-7BMC Syst. Biol. 122018</p>
<p>Extending the nested model for user-centric XAI: A design study on GNN-based drug repurposing. Q Wang, K Huang, P Chandak, M Zitnik, N Gehlenborg, 10.1109/TVCG.2022.3209435IEEE Trans. Vis. Comput. Graph. 292023</p>
<p>EFMSDTI: Drug-target interaction prediction based on an efficient fusion of multi-source data. B Zhang, Z Huang, H Zheng, W Li, Z Liu, Y Zhang, Q Huang, X Liu, H Jiang, Q Liu, 10.3389/fphar.2022.1009996Front. Pharmacol. 132022. 1009996</p>
<p>A Multimodal Data Fusion-Based Deep Learning Approach for Drug-Drug Interaction Prediction. A Huang, X Xie, X Wang, S Peng, 10.1007/978-3-031-23198-8_25Bioinformatics Research and Applications. Lecture Notes in Computer Science. Cham, SwitzerlandSpringer202313760</p>
<p>Mitigating Molecular Aggregation in Drug Discovery with Predictive Insights from Explainable AI. H Sturm, J Teufel, K Isfeld, P Friederich, R Davis, 10.1002/ange.202503259Angew. Chem. Int. Ed. 1372025. e202503259</p>
<p>Generating Explanations for Graph Neural Networks. R Ying, D Bourgeois, J You, M Zitnik, J Leskovec, Gnnexplainer, Proceedings of the Advances in Neural Information Processing Systems (NeurIPS). the Advances in Neural Information Processing Systems (NeurIPS)Vancouver, BC, Canada8-14 December 201932</p>
<p>Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations. Z Obermeyer, B Powers, C Vogeli, S Mullainathan, 10.1126/science.aax2342Science. 3662019</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
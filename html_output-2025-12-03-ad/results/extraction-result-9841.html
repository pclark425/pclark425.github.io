<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9841 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9841</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9841</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-268248855</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.02574v1.pdf" target="_blank">ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary</a></p>
                <p><strong>Paper Abstract:</strong> The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9841.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9841.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatCite: LLM agent with human workflow guidance for comparative literature summary</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent introduced in this paper that mimics human literature-review workflow: it extracts key elements from each reference then iteratively builds a comparative, organized related-work/literature-summary using a Reflective Incremental Generator and an LLM-based reflective evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-1106) as decoder; GPT-4 (gpt-4-turbo-preview) used for evaluation and baselines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses off-the-shelf OpenAI LLMs as generation and evaluation components — GPT-3.5 (16k context variant) as the primary generation/decoder for ChatCite due to cost, and GPT-4 (128k-preview) as a stronger baseline and as the automatic evaluator. No additional model training: the approach is a prompting/agent pipeline that orchestrates LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>NudtRwG-Citation dataset: curated set of academic target papers and their annotated reference papers in computer science; each sample includes a target paper (missing related-work), a gold related-work section, and annotated reference papers (authors/years). The paper's experiments used this test set of widely-cited CS papers.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>50</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Generate a comprehensive comparative literature summary / related-work section for a given target paper D using a provided set of reference papers R and a description of the proposed work; guidance emphasizes comparative analysis, organization, and retention of key elements.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>A two-stage, human-workflow guided prompting pipeline: (1) Key Element Extractor: concatenate seven guiding questions (Q_e) with each paper's content and prompt an LLM to extract structured key elements for each reference and for the proposed work; store elements in memory. (2) Reflective Incremental Generator: iteratively process each reference paper, using a Comparative Summarizer to expand prior candidate summaries with the new paper's key elements (generate n_s candidates per candidate), then apply a Reflective Evaluator (LLM-based voting repeated n_v times) to score and select top n_c candidates; repeat breadth-first style until all references processed and select highest-scoring summary. Uses few-shot/zero-shot prompts and comparative-guidance strings; no fine-tuning; relies on LLM sampling + voting to stabilize outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative literature summary / related-work section (narrative synthesis organized by themes and comparative points, with citation markers).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>A draft related-work paragraph that organizes referenced works into themes, compares methods/datasets/limitations vs. the target paper, and cites references inline (e.g., '[Reference 11]'): (excerpt) 'The field of machine translation has witnessed remarkable progress in addressing data scarcity and enhancing translation quality.... The 11th reference paper by Rie Kubota Ando and Tong Zhang (2005) focuses on learning predictive structures from multiple tasks and unlabeled data...' (full generated examples are included in the paper's appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic lexical-overlap metrics (ROUGE-1/2/L F1) and an LLM-based multidimensional evaluator 'G-Score' (GPT-4 used to score six dimensions and vote among candidates); human evaluation by domain researchers on a sample set using the same six dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Qualitatively, ChatCite produced literature summaries that human evaluators and the GPT-4-based G-Score preferred over direct LLM baselines and the reproduced LitLLM baseline; ChatCite outperformed other LLM-based approaches on the paper's LLM-based quality metrics and human preference judgments. ROUGE scores were comparable but in some cases slightly lower than GPT-4 zero-shot; however G-Score and human preference favored ChatCite for comparative quality and organization.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Mimics human workflow to retain key elements, yields more organized and comparative summaries, uses iterative reflective selection to improve stability, requires no additional model training (only prompting/orchestration), demonstrably preferred by human evaluators and LLM-based evaluator over simple CoT or direct summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Experiments limited to a 50-paper CS dataset (domain limited); ChatCite focuses solely on the summarization stage (does not handle retrieval/filtering); used GPT-3.5 as the main decoder for cost reasons which imposes context/window constraints; evaluation relies on LLM-based metrics (possible evaluator bias); outputs can still be random/unstable and require human review/editing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Information omission when underlying summarization of individual papers loses key elements; instability in generated paragraphs without reflective mechanism (mitigated but not eliminated); dependence on the quality of extracted key elements and on LLM sampling—errors in extraction propagate; domain-generalization untested beyond CS dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9841.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9841.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-Score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>G-Score: LLM-based multidimensional automatic evaluation metric for literature summaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation procedure introduced in this paper, inspired by G-Eval, that uses an LLM (GPT-4) to score generated literature summaries on six human-grounded dimensions and vote to select best outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-turbo-preview) as the evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses GPT-4 to simultaneously score multiple candidate generated summaries in a single conversation on six dimensions (Consistency, Coherence, Comparative, Integrity, Fluency, Cite Accuracy) on scales 1–5 and to vote/select the best summary; designed for fairness by scoring multiple models' outputs together.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Generated candidate summaries from ChatCite and baseline LLMs for each target paper in the NudtRwG-Citation dataset (50 academic CS papers), and corresponding gold related-work sections for reference where needed.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>50</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Automatic multidimensional quality assessment of literature-summary outputs (not a topic query per se).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not a distillation method; an evaluation pipeline: prompt GPT-4 with explicit scoring instructions for the six dimensions, have it score candidates and vote for top choices; aggregate scores to produce a G-Score and preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Numeric scores per dimension (1–5), aggregated G-Score and ranked preference among model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>For a given candidate summary, GPT-4 returns: Consistency: 4; Coherence: 4; Comparative: 5; Integrity: 4; Fluency: 5; Cite Accuracy: 4; Overall G-Score aggregated accordingly and used to rank candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared alignment between G-Score rankings and human evaluator preferences on sampled summaries; also used alongside ROUGE metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported alignment between G-Score and human judgments (Figure 4 in the paper): G-Score distributions aligned well with human preference distributions across the six dimensions, suggesting consistency with human evaluation; used to show ChatCite's superiority in quality metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Multidimensional, human-grounded evaluation tailored to literature summaries; operationalizes LLM-based evaluation to approximate human judgments; can score and rank multiple candidates in a single conversation for relative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>LLM-based evaluators may have biases (toward LLM-generated text), rely on the evaluator LLM's own limitations, and the paper notes 'room for improvements' in automatic evaluation accuracy; potential circularity if the same family of models is used both to generate and evaluate.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Potential evaluator bias leading to overrating LLM-produced texts, reduced reliability on edge cases requiring domain expertise beyond LLM knowledge, and sensitivity to prompt wording/instruction framing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9841.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9841.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM: A toolkit for scientific literature review (Agarwal et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed toolkit (cited and used as a baseline in this paper) that applies Retrieval-Augmented Generation (RAG), specialized prompting, and instructive techniques to produce literature-review style outputs while attempting to mitigate hallucination and recency issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litllm: A toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>In the paper's reproduction of the baseline, LitLLM was used with GPT-4 as the decoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LitLLM (as described by its authors) integrates retrieval (RAG) to ground generation in source documents and uses carefully designed prompts/instructions to control hallucination; in ChatCite experiments the authors reproduced LitLLM's CoT-style prompt and used GPT-4 as the decoder for best-case performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not detailed in this paper for LitLLM; for baseline comparison the same NudtRwG-Citation test set (50 CS papers) was used to produce related-work summaries with the LitLLM prompting approach.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>50</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Generate literature-review / related-work summaries using retrieval-augmented prompts and instructive techniques to reduce hallucination and include recent research.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) combined with specialized prompting and chain-of-thought style guidance (as reported in LitLLM); in ChatCite reproduction the LitLLM CoT prompt was used with GPT-4 as decoder to generate related-work baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature summaries / related-work drafts (narrative synthesis grounded via retrieved passages).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>In the reproduction, LitLLM produced related-work sections similar to GPT-4 zero-shot outputs but structured according to its RAG and prompt design (examples appear in the appendix of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated with ROUGE metrics and the G-Score (GPT-4 evaluator), and included in human preference comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LitLLM with GPT-4 yielded performance similar to GPT-4 zero-shot in ROUGE but was significantly lower than ChatCite in the paper's LLM-based G-Score and human preference metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Addresses hallucination and recency by grounding generation with retrieval; uses instructive techniques to improve factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>In this paper's experiments, LitLLM outputs were less preferred than ChatCite's human-workflow guided outputs; may still suffer from readability/organizational shortcomings when using simple CoT guidance alone; exact implementation and retrieval indexes influence results but were not reproduced in full detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When reproduced via CoT prompts with GPT-4, outputs lacked the comparative organization and stability achieved by ChatCite's workflow; potential for hallucination if retrieval is incomplete or retrieval prompts are weak.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9841.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9841.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-1106)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-3.5 variant with extended context (16k) used as the primary generation/decoder for ChatCite experiments due to cost considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-1106)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A production OpenAI LLM used via API; in experiments it was run with a 16k context window and used for generation steps (Key Element extraction and Comparative Summarizer) in the ChatCite pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Operated on the NudtRwG-Citation dataset (50 CS papers) where it was used to summarize individual papers and to iteratively generate candidate related-work paragraphs under ChatCite orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>50</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Prompts to extract key elements from individual papers and guides for incremental comparative summarization for the target paper's related work.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompting in two-step flows (summarize each article, then generate related-work using those summaries) due to context-window constraints; incorporated few-shot examples in some settings and used ChatCite's Key Element prompts and Comparative Summarizer prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Draft related-work sections and intermediate extracted key elements.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Two-step approach: [p_s] 'Summarize the current article, preserving as much information as possible. Content:{content}' then [p_g] 'Generate the related work section based on the given target paper summary and its references summary...'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Outputs evaluated with ROUGE and GPT-4-based G-Score and human judgments as part of overall experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-3.5 served as a cost-effective generator; ChatCite built on GPT-3.5 achieved strong results. GPT-3.5 required two-step summarization because of context limits, which can increase risk of information omission relative to models with larger windows.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Lower cost; adequate for orchestrated multi-step pipelines; supports extended context (16k) enabling larger inputs than default models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Context-window constraints required two-step pipelines causing potential information loss; generally weaker than GPT-4 in zero-shot ROUGE metrics; sensitivity to few-shot examples and prompt engineering can affect output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Two-step summarization pipeline can drop key elements between intermediate summaries; more stochasticity in outputs without strong reflective selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9841.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9841.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-turbo-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 preview (128K context) used both as a high-performing baseline for direct related-work generation and as the LLM evaluator for G-Score in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-turbo-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A more capable OpenAI LLM with a 128k context window used as a decoder baseline for zero-shot and few-shot conditions and as the LLM evaluator (G-Score).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to the same NudtRwG-Citation dataset (50 CS papers) for zero-shot and few-shot related-work generation and to evaluate candidate summaries via G-Score.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>50</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Zero-shot and few-shot prompts to generate related-work sections directly from target and reference contents; also used to score summaries across six dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Direct prompting (zero-shot and few-shot) including Chain-of-Thought style guidance in reproduced baselines; used as a high-capacity generator and as an LLM-based evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated related-work / literature summaries; numeric evaluation scores and rankings when used as evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Direct related-work generation prompts were used: '[p_g] Generate the related work section based on the given target paper summary and its references summary...'; GPT-4 outputs served as baseline summaries and as input to G-Score evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>ROUGE metrics and G-Score (GPT-4 used as evaluator in the pipeline), plus human evaluation comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4 achieved the best ROUGE under zero-shot in the experiments, but performed poorly in few-shot settings (sensitivity to examples). Despite high ROUGE, GPT-4 zero-shot outputs were still less preferred on LLM-based quality metrics/human preference compared to ChatCite's workflow-guided outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>High capability/generation quality, large context window enabling direct generation over larger inputs, strong ROUGE performance in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Few-shot sensitivity causing poor few-shot outcomes in the experiments; stochastic behavior leading to less reliable comparative/organizational structure when run as a black-box generator; cost and access constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Poor few-shot performance observed (examples influence leading to irrelevant/erroneous summaries); direct CoT-driven generation lacked comparative organization and stability compared to ChatCite pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9841.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9841.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that asks LLMs to produce intermediate reasoning steps (a 'chain of thought') before producing final answers; discussed as common prior guidance for literature-summary tasks but noted to be insufficient for comparative, organized literature reviews when used alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Technique applied to LLMs (e.g., GPT-3.5/GPT-4) in prior work and baselines</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting method (Wei et al., 2023) that elicits step-by-step reasoning from LLMs by asking them to produce intermediate thoughts; prior literature-review systems used simple CoT guidance for retrieval/filtering and summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Used in prior literature-review work referenced in the paper (e.g., LitLLM reproduction and other LLM-based literature review studies), typically applied to sets of retrieved papers; not a single corpus defined here.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Used to guide LLMs to produce intermediate reasoning steps when generating summaries or making retrieval/filtering decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-level technique: instruct model to output chains of reasoning and then final summary; in this paper CoT was used in baselines but criticized as insufficient for comprehensive comparative literature summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Intermediate reasoning steps and final generated summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Prompts that encourage the model to 'think step by step' before answering, leading to multi-step generated text prior to final summary.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Referenced work uses standard summarization metrics and human evaluation; in this paper CoT-based outputs were compared via ROUGE and G-Score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The paper reports that simple CoT guidance often produces discrete per-paper summaries lacking comparative analysis and organization; ChatCite's human-workflow approach outperformed CoT-only baselines on multi-dimensional quality metrics and human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Can improve complex reasoning and help LLMs plan generation in some tasks; widely used and easy to apply as a prompt technique.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>When applied naively to literature summarization, CoT tends to produce per-paper discrete summaries without organized comparative synthesis; stochastic nature and length limitations of LLMs reduce its reliability for multi-document comparative summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Direct CoT generation frequently omitted key elements and failed to provide integrated comparative organization across multiple papers; two-step CoT pipelines risk losing information due to intermediate summarization steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Litllm: A toolkit for scientific literature review <em>(Rating: 2)</em></li>
                <li>G-eval: Nlg evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Automatic related work section generation: experiments in scientific document abstracting <em>(Rating: 1)</em></li>
                <li>Automatic generation of citation texts in scholarly papers: A pilot study <em>(Rating: 1)</em></li>
                <li>The role of chatgpt in scientific communication: writing better scientific review articles <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9841",
    "paper_id": "paper-268248855",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "ChatCite",
            "name_full": "ChatCite: LLM agent with human workflow guidance for comparative literature summary",
            "brief_description": "An LLM-based agent introduced in this paper that mimics human literature-review workflow: it extracts key elements from each reference then iteratively builds a comparative, organized related-work/literature-summary using a Reflective Incremental Generator and an LLM-based reflective evaluator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-1106) as decoder; GPT-4 (gpt-4-turbo-preview) used for evaluation and baselines",
            "model_description": "Uses off-the-shelf OpenAI LLMs as generation and evaluation components — GPT-3.5 (16k context variant) as the primary generation/decoder for ChatCite due to cost, and GPT-4 (128k-preview) as a stronger baseline and as the automatic evaluator. No additional model training: the approach is a prompting/agent pipeline that orchestrates LLM calls.",
            "model_size": null,
            "input_corpus_description": "NudtRwG-Citation dataset: curated set of academic target papers and their annotated reference papers in computer science; each sample includes a target paper (missing related-work), a gold related-work section, and annotated reference papers (authors/years). The paper's experiments used this test set of widely-cited CS papers.",
            "input_corpus_size": 50,
            "topic_query_description": "Generate a comprehensive comparative literature summary / related-work section for a given target paper D using a provided set of reference papers R and a description of the proposed work; guidance emphasizes comparative analysis, organization, and retention of key elements.",
            "distillation_method": "A two-stage, human-workflow guided prompting pipeline: (1) Key Element Extractor: concatenate seven guiding questions (Q_e) with each paper's content and prompt an LLM to extract structured key elements for each reference and for the proposed work; store elements in memory. (2) Reflective Incremental Generator: iteratively process each reference paper, using a Comparative Summarizer to expand prior candidate summaries with the new paper's key elements (generate n_s candidates per candidate), then apply a Reflective Evaluator (LLM-based voting repeated n_v times) to score and select top n_c candidates; repeat breadth-first style until all references processed and select highest-scoring summary. Uses few-shot/zero-shot prompts and comparative-guidance strings; no fine-tuning; relies on LLM sampling + voting to stabilize outputs.",
            "output_type": "Comparative literature summary / related-work section (narrative synthesis organized by themes and comparative points, with citation markers).",
            "output_example": "A draft related-work paragraph that organizes referenced works into themes, compares methods/datasets/limitations vs. the target paper, and cites references inline (e.g., '[Reference 11]'): (excerpt) 'The field of machine translation has witnessed remarkable progress in addressing data scarcity and enhancing translation quality.... The 11th reference paper by Rie Kubota Ando and Tong Zhang (2005) focuses on learning predictive structures from multiple tasks and unlabeled data...' (full generated examples are included in the paper's appendix).",
            "evaluation_method": "Automatic lexical-overlap metrics (ROUGE-1/2/L F1) and an LLM-based multidimensional evaluator 'G-Score' (GPT-4 used to score six dimensions and vote among candidates); human evaluation by domain researchers on a sample set using the same six dimensions.",
            "evaluation_results": "Qualitatively, ChatCite produced literature summaries that human evaluators and the GPT-4-based G-Score preferred over direct LLM baselines and the reproduced LitLLM baseline; ChatCite outperformed other LLM-based approaches on the paper's LLM-based quality metrics and human preference judgments. ROUGE scores were comparable but in some cases slightly lower than GPT-4 zero-shot; however G-Score and human preference favored ChatCite for comparative quality and organization.",
            "strengths": "Mimics human workflow to retain key elements, yields more organized and comparative summaries, uses iterative reflective selection to improve stability, requires no additional model training (only prompting/orchestration), demonstrably preferred by human evaluators and LLM-based evaluator over simple CoT or direct summarization.",
            "limitations": "Experiments limited to a 50-paper CS dataset (domain limited); ChatCite focuses solely on the summarization stage (does not handle retrieval/filtering); used GPT-3.5 as the main decoder for cost reasons which imposes context/window constraints; evaluation relies on LLM-based metrics (possible evaluator bias); outputs can still be random/unstable and require human review/editing.",
            "failure_cases": "Information omission when underlying summarization of individual papers loses key elements; instability in generated paragraphs without reflective mechanism (mitigated but not eliminated); dependence on the quality of extracted key elements and on LLM sampling—errors in extraction propagate; domain-generalization untested beyond CS dataset.",
            "uuid": "e9841.0",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "G-Score",
            "name_full": "G-Score: LLM-based multidimensional automatic evaluation metric for literature summaries",
            "brief_description": "An automatic evaluation procedure introduced in this paper, inspired by G-Eval, that uses an LLM (GPT-4) to score generated literature summaries on six human-grounded dimensions and vote to select best outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-turbo-preview) as the evaluator",
            "model_description": "Uses GPT-4 to simultaneously score multiple candidate generated summaries in a single conversation on six dimensions (Consistency, Coherence, Comparative, Integrity, Fluency, Cite Accuracy) on scales 1–5 and to vote/select the best summary; designed for fairness by scoring multiple models' outputs together.",
            "model_size": null,
            "input_corpus_description": "Generated candidate summaries from ChatCite and baseline LLMs for each target paper in the NudtRwG-Citation dataset (50 academic CS papers), and corresponding gold related-work sections for reference where needed.",
            "input_corpus_size": 50,
            "topic_query_description": "Automatic multidimensional quality assessment of literature-summary outputs (not a topic query per se).",
            "distillation_method": "Not a distillation method; an evaluation pipeline: prompt GPT-4 with explicit scoring instructions for the six dimensions, have it score candidates and vote for top choices; aggregate scores to produce a G-Score and preferences.",
            "output_type": "Numeric scores per dimension (1–5), aggregated G-Score and ranked preference among model outputs.",
            "output_example": "For a given candidate summary, GPT-4 returns: Consistency: 4; Coherence: 4; Comparative: 5; Integrity: 4; Fluency: 5; Cite Accuracy: 4; Overall G-Score aggregated accordingly and used to rank candidates.",
            "evaluation_method": "Compared alignment between G-Score rankings and human evaluator preferences on sampled summaries; also used alongside ROUGE metrics.",
            "evaluation_results": "Reported alignment between G-Score and human judgments (Figure 4 in the paper): G-Score distributions aligned well with human preference distributions across the six dimensions, suggesting consistency with human evaluation; used to show ChatCite's superiority in quality metrics.",
            "strengths": "Multidimensional, human-grounded evaluation tailored to literature summaries; operationalizes LLM-based evaluation to approximate human judgments; can score and rank multiple candidates in a single conversation for relative comparisons.",
            "limitations": "LLM-based evaluators may have biases (toward LLM-generated text), rely on the evaluator LLM's own limitations, and the paper notes 'room for improvements' in automatic evaluation accuracy; potential circularity if the same family of models is used both to generate and evaluate.",
            "failure_cases": "Potential evaluator bias leading to overrating LLM-produced texts, reduced reliability on edge cases requiring domain expertise beyond LLM knowledge, and sensitivity to prompt wording/instruction framing.",
            "uuid": "e9841.1",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LitLLM (baseline)",
            "name_full": "LitLLM: A toolkit for scientific literature review (Agarwal et al., 2024)",
            "brief_description": "A recently proposed toolkit (cited and used as a baseline in this paper) that applies Retrieval-Augmented Generation (RAG), specialized prompting, and instructive techniques to produce literature-review style outputs while attempting to mitigate hallucination and recency issues.",
            "citation_title": "Litllm: A toolkit for scientific literature review",
            "mention_or_use": "use",
            "model_name": "In the paper's reproduction of the baseline, LitLLM was used with GPT-4 as the decoder",
            "model_description": "LitLLM (as described by its authors) integrates retrieval (RAG) to ground generation in source documents and uses carefully designed prompts/instructions to control hallucination; in ChatCite experiments the authors reproduced LitLLM's CoT-style prompt and used GPT-4 as the decoder for best-case performance.",
            "model_size": null,
            "input_corpus_description": "Not detailed in this paper for LitLLM; for baseline comparison the same NudtRwG-Citation test set (50 CS papers) was used to produce related-work summaries with the LitLLM prompting approach.",
            "input_corpus_size": 50,
            "topic_query_description": "Generate literature-review / related-work summaries using retrieval-augmented prompts and instructive techniques to reduce hallucination and include recent research.",
            "distillation_method": "Retrieval-Augmented Generation (RAG) combined with specialized prompting and chain-of-thought style guidance (as reported in LitLLM); in ChatCite reproduction the LitLLM CoT prompt was used with GPT-4 as decoder to generate related-work baselines.",
            "output_type": "Literature summaries / related-work drafts (narrative synthesis grounded via retrieved passages).",
            "output_example": "In the reproduction, LitLLM produced related-work sections similar to GPT-4 zero-shot outputs but structured according to its RAG and prompt design (examples appear in the appendix of the paper).",
            "evaluation_method": "Evaluated with ROUGE metrics and the G-Score (GPT-4 evaluator), and included in human preference comparisons.",
            "evaluation_results": "LitLLM with GPT-4 yielded performance similar to GPT-4 zero-shot in ROUGE but was significantly lower than ChatCite in the paper's LLM-based G-Score and human preference metrics.",
            "strengths": "Addresses hallucination and recency by grounding generation with retrieval; uses instructive techniques to improve factuality.",
            "limitations": "In this paper's experiments, LitLLM outputs were less preferred than ChatCite's human-workflow guided outputs; may still suffer from readability/organizational shortcomings when using simple CoT guidance alone; exact implementation and retrieval indexes influence results but were not reproduced in full detail here.",
            "failure_cases": "When reproduced via CoT prompts with GPT-4, outputs lacked the comparative organization and stability achieved by ChatCite's workflow; potential for hallucination if retrieval is incomplete or retrieval prompts are weak.",
            "uuid": "e9841.2",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-3.5 (used)",
            "name_full": "GPT-3.5 (gpt-3.5-turbo-1106)",
            "brief_description": "OpenAI's GPT-3.5 variant with extended context (16k) used as the primary generation/decoder for ChatCite experiments due to cost considerations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-1106)",
            "model_description": "A production OpenAI LLM used via API; in experiments it was run with a 16k context window and used for generation steps (Key Element extraction and Comparative Summarizer) in the ChatCite pipeline.",
            "model_size": null,
            "input_corpus_description": "Operated on the NudtRwG-Citation dataset (50 CS papers) where it was used to summarize individual papers and to iteratively generate candidate related-work paragraphs under ChatCite orchestration.",
            "input_corpus_size": 50,
            "topic_query_description": "Prompts to extract key elements from individual papers and guides for incremental comparative summarization for the target paper's related work.",
            "distillation_method": "Prompting in two-step flows (summarize each article, then generate related-work using those summaries) due to context-window constraints; incorporated few-shot examples in some settings and used ChatCite's Key Element prompts and Comparative Summarizer prompts.",
            "output_type": "Draft related-work sections and intermediate extracted key elements.",
            "output_example": "Two-step approach: [p_s] 'Summarize the current article, preserving as much information as possible. Content:{content}' then [p_g] 'Generate the related work section based on the given target paper summary and its references summary...'.",
            "evaluation_method": "Outputs evaluated with ROUGE and GPT-4-based G-Score and human judgments as part of overall experiments.",
            "evaluation_results": "GPT-3.5 served as a cost-effective generator; ChatCite built on GPT-3.5 achieved strong results. GPT-3.5 required two-step summarization because of context limits, which can increase risk of information omission relative to models with larger windows.",
            "strengths": "Lower cost; adequate for orchestrated multi-step pipelines; supports extended context (16k) enabling larger inputs than default models.",
            "limitations": "Context-window constraints required two-step pipelines causing potential information loss; generally weaker than GPT-4 in zero-shot ROUGE metrics; sensitivity to few-shot examples and prompt engineering can affect output quality.",
            "failure_cases": "Two-step summarization pipeline can drop key elements between intermediate summaries; more stochasticity in outputs without strong reflective selection.",
            "uuid": "e9841.3",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-4 (used)",
            "name_full": "GPT-4 (gpt-4-turbo-preview)",
            "brief_description": "OpenAI's GPT-4 preview (128K context) used both as a high-performing baseline for direct related-work generation and as the LLM evaluator for G-Score in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-turbo-preview)",
            "model_description": "A more capable OpenAI LLM with a 128k context window used as a decoder baseline for zero-shot and few-shot conditions and as the LLM evaluator (G-Score).",
            "model_size": null,
            "input_corpus_description": "Applied to the same NudtRwG-Citation dataset (50 CS papers) for zero-shot and few-shot related-work generation and to evaluate candidate summaries via G-Score.",
            "input_corpus_size": 50,
            "topic_query_description": "Zero-shot and few-shot prompts to generate related-work sections directly from target and reference contents; also used to score summaries across six dimensions.",
            "distillation_method": "Direct prompting (zero-shot and few-shot) including Chain-of-Thought style guidance in reproduced baselines; used as a high-capacity generator and as an LLM-based evaluator.",
            "output_type": "Generated related-work / literature summaries; numeric evaluation scores and rankings when used as evaluator.",
            "output_example": "Direct related-work generation prompts were used: '[p_g] Generate the related work section based on the given target paper summary and its references summary...'; GPT-4 outputs served as baseline summaries and as input to G-Score evaluation.",
            "evaluation_method": "ROUGE metrics and G-Score (GPT-4 used as evaluator in the pipeline), plus human evaluation comparisons.",
            "evaluation_results": "GPT-4 achieved the best ROUGE under zero-shot in the experiments, but performed poorly in few-shot settings (sensitivity to examples). Despite high ROUGE, GPT-4 zero-shot outputs were still less preferred on LLM-based quality metrics/human preference compared to ChatCite's workflow-guided outputs.",
            "strengths": "High capability/generation quality, large context window enabling direct generation over larger inputs, strong ROUGE performance in zero-shot.",
            "limitations": "Few-shot sensitivity causing poor few-shot outcomes in the experiments; stochastic behavior leading to less reliable comparative/organizational structure when run as a black-box generator; cost and access constraints.",
            "failure_cases": "Poor few-shot performance observed (examples influence leading to irrelevant/erroneous summaries); direct CoT-driven generation lacked comparative organization and stability compared to ChatCite pipeline.",
            "uuid": "e9841.4",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "CoT prompting",
            "name_full": "Chain-of-Thought (CoT) prompting",
            "brief_description": "A prompting technique that asks LLMs to produce intermediate reasoning steps (a 'chain of thought') before producing final answers; discussed as common prior guidance for literature-summary tasks but noted to be insufficient for comparative, organized literature reviews when used alone.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "Technique applied to LLMs (e.g., GPT-3.5/GPT-4) in prior work and baselines",
            "model_description": "Prompting method (Wei et al., 2023) that elicits step-by-step reasoning from LLMs by asking them to produce intermediate thoughts; prior literature-review systems used simple CoT guidance for retrieval/filtering and summarization.",
            "model_size": null,
            "input_corpus_description": "Used in prior literature-review work referenced in the paper (e.g., LitLLM reproduction and other LLM-based literature review studies), typically applied to sets of retrieved papers; not a single corpus defined here.",
            "input_corpus_size": null,
            "topic_query_description": "Used to guide LLMs to produce intermediate reasoning steps when generating summaries or making retrieval/filtering decisions.",
            "distillation_method": "Prompt-level technique: instruct model to output chains of reasoning and then final summary; in this paper CoT was used in baselines but criticized as insufficient for comprehensive comparative literature summaries.",
            "output_type": "Intermediate reasoning steps and final generated summaries.",
            "output_example": "Prompts that encourage the model to 'think step by step' before answering, leading to multi-step generated text prior to final summary.",
            "evaluation_method": "Referenced work uses standard summarization metrics and human evaluation; in this paper CoT-based outputs were compared via ROUGE and G-Score.",
            "evaluation_results": "The paper reports that simple CoT guidance often produces discrete per-paper summaries lacking comparative analysis and organization; ChatCite's human-workflow approach outperformed CoT-only baselines on multi-dimensional quality metrics and human preference.",
            "strengths": "Can improve complex reasoning and help LLMs plan generation in some tasks; widely used and easy to apply as a prompt technique.",
            "limitations": "When applied naively to literature summarization, CoT tends to produce per-paper discrete summaries without organized comparative synthesis; stochastic nature and length limitations of LLMs reduce its reliability for multi-document comparative summarization.",
            "failure_cases": "Direct CoT generation frequently omitted key elements and failed to provide integrated comparative organization across multiple papers; two-step CoT pipelines risk losing information due to intermediate summarization steps.",
            "uuid": "e9841.5",
            "source_info": {
                "paper_title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Litllm: A toolkit for scientific literature review",
            "rating": 2
        },
        {
            "paper_title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Automatic related work section generation: experiments in scientific document abstracting",
            "rating": 1
        },
        {
            "paper_title": "Automatic generation of citation texts in scholarly papers: A pilot study",
            "rating": 1
        },
        {
            "paper_title": "The role of chatgpt in scientific communication: writing better scientific review articles",
            "rating": 1
        }
    ],
    "cost": 0.0173605,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary
5 Mar 2024</p>
<p>Yutong Li 
Tsinghua University
BeijingChina</p>
<p>Lu Chen chenlusz@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Suzhou Laboratory
SuzhouChina</p>
<p>Aiwei Liu 
Tsinghua University
BeijingChina</p>
<p>Kai Yu 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
X-LANCE Lab
SJTU AI Institute Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Suzhou Laboratory
SuzhouChina</p>
<p>Lijie Wen 
Tsinghua University
BeijingChina</p>
<p>ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary
5 Mar 2024F78380900CFD74D904ED56562D982163arXiv:2403.02574v1[cs.IR]
The literature review is an indispensable step in the research process.It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works.However, literature summary is challenging and time consuming.The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization.However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary.In this work, we firstly focus on the independent literature summarization step and introduce ChatCite 1 , an LLM agent with human workflow guidance for comparative literature summary.This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism.In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria.The ChatCite agent outperformed other models in various dimensions in the experiments.The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.</p>
<p>Introduction</p>
<p>As the rapid advancement of academic research, scholars must delve into existing literature to understand past studies, recognize future research trends, and find innovative approaches in their fields.Crafting a literature review entails searching for relevant literature and conducting detailed comparative summarization.It typically involves two main steps: literature collection followed by literature summary generation based on the collected sources.However, organizing a high-quality literature review 1 Our code will be released after the review process.</p>
<p>Reference papers set</p>
<p>Proposed work description</p>
<p>Generatedliterature summary</p>
<p>Figure 1: Literature Summary Task Description necessitates scholars to engage in thorough analysis, organization, comparison, and integration of an extensive of related works, which is often a challenging and time-consuming task.</p>
<p>Therefore, Hoang and Kan (2010) have proposed the automatic generation of literature summary.However, machine-generated literature summaries often encounter challenges like information omission, lack of linguistic fluency, and insufficient comparative analysis.In traditional models, summaries generated through extraction and abstraction approach may miss key information due to the limitations of the model, leading to the lack of crucial points or findings of the generated summaries.Some automated systems may lack the ability for in-depth comparative analysis, potentially resulting in literature summaries that lack a comprehensive understanding of the relevant research in the field.</p>
<p>In recent years, with the rapid development of large language models (LLMs) (Radford et al., 2019;Brown et al., 2020), their powerful capabilities in natural language generation tasks have been demonstrated across various tasks, that provides possibilities for handling longer texts and generating comprehensive summaries.Researchers have started exploring how to leverage LLMs to generate automatic literature summaries.Wei et al. (2023) propose a Chain-of-Thought (CoT) prompting method to enhance the ability of large language models to perform complex reasoning.CoT allows LLMs to devise their own plan, resulting in generated text that aligns more closely with human preferences.Recent study by (Huang and Tan, 2023) and Agarwal et al. (2024) on literature review has focused more on how to retrieve relevant papers more accurately and neglected research on literature summarization.They use only simple CoT guidance to generate literature summaries, resulting in a lack of comparative and organizational analysis.Large language models, despite their fluent language generation, struggle to consistently produce comparative literature summaries due to their unpredictable an stochastic nature.The length limitations of these models require a two-step summarization approach, increasing the risk of information omission during abstract generation.</p>
<p>In this work, we focus on the independent literature summarization task, aiming to generate a comprehensive comparative literature summary through a certain collection of literature and a description of the proposed work, as illustrated in Figure 1.To address these challenges mentioned above, our work proposes ChatCite, a LLM-based agent guided by human workflow.Different from simple CoT prompting approach, the agent is designed with the human workflow guidance, rather than formulating the generation process in a blackbox manner, ensuring a more stable generation of higher-quality generic summaries.</p>
<p>Furthermore, quality assessment for generative tasks has always been a challenge.Prior studies on literature summarization have primarily relied on text summarization metrics, such as ROUGE (Lin (2004a)).However, traditional text summary evaluation metrics, like ROUGE, are not sufficient to assess the quality of literature summaries.More comprehensive evaluation criteria covering multiple dimensions are required to ensure that the generated literature summaries truly meet the requirements.Therefore, we combine human studies on literature reviews (Justitia and Wang, 2022) to formulate the evaluation criteria for literature summaries from multiple dimensions2 , and propose an LLM-based automatic evaluation metric, G-Score.Experimental results demonstrate its consistency with human evaluations.</p>
<p>In this paper, we summarize our main contributions of our framework as follows:</p>
<p>• we focus on the independent literature summarization step of literature review, and in-troduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary.</p>
<p>• Based on research on literature summaries, we have developed a multidimensional quality assessment criterion for literature summaries.Additionally, we propose an LLM-based automatic evaluation metric, G-Score, demonstrating results consistent with human preferences.</p>
<p>• The experimental results indicate that</p>
<p>ChatCite outperforms other LLM-based literature summarization methods in all quality dimensions.The literature summaries produced by ChatCite can be directly utilized for drafting literature reviews.</p>
<p>• We demonstrate that LLMs with human workflow guidance, have the ability to effectively perform comprehensive comparative summarization of multiple documents.Therefore, we infer that Large Language Models (LLMs) have the potential to handle more complex inferential summarization tasks.</p>
<p>2 Related Work3</p>
<p>In recent years, there is abundant research on generated literature summaries with the initial proposal made by Hoang and Kan (2010), to automate related work summarization created by a topicrelated work summary based on an extractive approach.To generate citation sentence, Xing et al.</p>
<p>(2020) adopted a multi-source pointer-generator network with cross-attention mechanism, while AbuRa'ed et al. (2020)  Large Language Models (LLMs), such as GPT (Radford et al. (2019), Brown et al. (2020)), have demonstrated their powerful capabilities in natural language generation tasks.The study by Huang and Tan (2023) on the use of AI tools like Chat-GPT in writing scientific review articles reveals the potential benefits and drawbacks of artificial intelligence in academic writing.Building on these insights, Agarwal et al. (2024) introduces the LitLLM toolkit, which overcomes challenges such as generating hallucinated content and overlooking recent research by adopting Retrieval Augmented Generation (RAG) principles, specialized prompting, and instructive techniques.However, these studies only applied a simple Chain of Thought (CoT) to the search and filtering process in literature reviews, resulting in poor readability.By comparison, ChatCite focuses on the independent task of text summarization, aiming to generate higher-quality summaries.</p>
<p>Furthermore, this paper introduced a multidimensional G-Score evaluation metric inspired by the previous attempt to use Large Language Models (LLMs) through chain-of-thought methods to evaluate the quality of natural language generation (NLG) systems (Liu et al. (2023), Goyal et al. (2023)) which is more consistent with human evaluation compared to traditional ROUGE metrics (Lin (2004b)).</p>
<p>ChatCite</p>
<p>The literature review task can be decomposed into two sub tasks: relevant papers retrieval and literature summaries generation.This work focuses on the independent task of literature summary generation.Our task is to generate the literature summary based on the proposed work description D and a certain reference papers set R = {r 1 , r 2 , ..., r n }.Given D and R, our agent generates a literature summary Y = f (D,R).</p>
<p>Diverging from other types of summaries, such as news summaries, the literature summary generated directly by large language models using simple Chain-of-Thought (CoT) guidance in existing work mainly faces the following issues: Key Elements missing: Because of the window limitations of LLMs, generating the complete literature review directly is challenging.Typically, a two-step approach is used involving summarization and literature review generation.However, this process can lead to the loss of key elements during summarization.Even if the entire literature summary can be directly generated, using the entire text may result in mistakes in understanding key elements and the loss of such elements.Lack of Comparative Analysis: Comparative analysis is crucial in literature summary, requiring an analysis on the limitations and advantages of existing research methods, and focusing on differences and similarities in methods, experimental design, dataset usage, and more.Directly using CoT-generated results often lacks comparative analysis.</p>
<p>Lack of Organizational Structure: The literature summary generated solely by CoT tends to be discrete for each paper, lacking classification for similar works and an organized structure for the literature review.</p>
<p>To address these challenges, we have proposed an LLM agent for comparative literature summary with human workflow guidance, ChatCite, consisting two modules: the Key Element Extractor and the Reflective Incremental Generator, as illustrated in Figure 2. In this process, we utilize large language models as both generation and evaluation components, eliminating the need for additional model training and improving the quality of generated text to some extent.</p>
<p>The generation process guided by human workflow is as follows:</p>
<p>1.The proposed work description and reference papers in the reference papers set are initially processed using the Key Element Extractor separately.</p>
<ol>
<li>Iteratively generate literature summaries using reference papers set.In each iteration, use the comparative summarizer to generate a comparative analysis summary.Then, use the reflective evaluator to vote on the generated candidate results, ranking the vote score and retaining the top n c results.Iterate continuously until all reference papers are processed.</li>
</ol>
<p>The final output is selected based on the highest voting score among the generated related work summaries.</p>
<p>In this section, we first elaborate on the specifics of the Key Element Extractor ( §3.1) and the Reflective Iterative Generator module ( §3.2) in detail.</p>
<p>Comparative summarizer</p>
<p>Reflective Evaluator</p>
<p>Given the target paper summary and references, Analyze each choice in detail, then conclude in the last line " The best choice is {s}" , wher e s the integer id of the choice.Choices 1,...;Choice2,...;Choice3,...;...;The best choice is 3.  summary generated with the Reflective Incremental Generator.This process is iteratively repeated until a complete related work summary is generated, and the optimal one is selected as the final result.</p>
<p>Reflective Mechanism</p>
<p>Rank &amp; Select</p>
<p>Key Element Extractor</p>
<p>In order to retain sufficient key element for literature summary, we create seven simple guiding questions based on analysis (Justitia and Wang, 2022) on literature review.We concatenate theses questions and the content required extraction as prompt to instruct LLMs extract the key elements.For each element, a simple question (shown in Figure 2) is set to guide the model in extraction, and these questions are Q e = [q 1 , q 2 , ..., q 7 ] .These questions Q e and paper content C are concatenated to form the key element extraction prompt
P e = [Q e , C]
. Using LLM as extraction decoder to extract key elements and storing them in memory.</p>
<p>Reflective incremental Generator</p>
<p>To overcome the challenges of lacking comparative analysis and organizational structure in literature reviews generated by LLMs, we designed the reflective incremental generator.The generator uses the Comparative Summarizer to continue writing comparative summaries, combining the results from the previous turn and the key elements of the proposed work and reference papers.It then utilizes the reflective evaluator to filter the generated re-sults.This process is interatively applied to each reference paper in the reference papers set until all reference papers are processed.The best result is ultimately retained as the model's generated output.</p>
<p>Comparative Summarizer</p>
<p>For turn i, based on the proposed work key element pro, the key element of the i-th reference paper ref i and comparative summarization guidance sequentially generated summary for each summary s ∈ S i−1 , and generating n s samples each time.
S i = {G(D g , pro, ref i , s, n s ), ∀s ∈ S i−1 }
Here, to enhance the comparability and organization of the generated summaries, comparative summarization guidance are provided: "Considering the relationship between the reference paper and the target paper, as well as existing references in the previously completed related work, while retaining the content of all referenced papers mentioned in the previously completed related work."</p>
<p>Reflective Mechanism</p>
<p>Due to significant uncertainty in text generation tasks, we employ reflective generation to enhance the quality and stability of generated paragraphs.</p>
<p>Here, we use LLMs as Reflective Evaluator to vote n v times on the generated results in each turn and then perform a statistical analysis on the voting results to obtain voting scores E i = E(D e , S ′ i ).Then we sort the scores, and retain the top n c candidates S i = {S t , t ∈ Sort(E i )(1, n c )} .These selected candidates will be used for the next round of incremental generation.This approach helps identify the most promising results, ensures the quality of the generated text, and enhances generation stability.</p>
<p>Reflective Incremental Generator Algorithm</p>
<p>In implementing reflective incremental generation, we drew inspiration from the breadth-first search algorithm for trees (Algorithm 1).
), s ∈ S i−1 } E i ← E(D e , S ′ i ) S i ← {S t , t ∈ Sort(E i )(1, n c )} end for return S argmax i En(i)
notes: G() corresponds to the Comparative Summarizer function described in §3.2.1, and E() corresponds to the Reflective Envaluation function described in §3.2.2.At each step, a collection containing n c most promising generated results is maintained, where the depth of the tree equals the number of documents in the relevant literature collection, S ′ t contains n c * n s results, while S i−1 and S i each contain n c results.</p>
<p>G-Score: LLM-based automatic Evaluation Metrics</p>
<p>The evaluation of generative tasks has always been challenging.Previous research on literature summarization predominantly depended on text summarization metrics, like ROUGE (Lin (2004a)).</p>
<p>However, conventional text summary evaluation metrics such as ROUGE fall short in gauging the quality of literature summaries.It is crucial to adopt more comprehensive evaluation criteria across various dimensions to guarantee that the generated literature summaries align with the necessary standards.Here, inspired by G-Eval (Liu et al., 2023), we attempted to assess it using LLMs.We established six-dimensional metrics for automatic evaluation based on research on literature summaries (Justitia and Wang, 2022).Evaluation Steps.We used Large Language Models (LLMs) to score the six dimensions of generic quality and voted for the best summary from a series of model-generated summaries.Specially, to ensure fairness and consistency in evaluation, we simultaneously scored and voted for the generated results of multiple models in a single conversation.</p>
<p>Evaluation Criterion: Consistency (1-5): Content consistency between the generated summary and the gold summary.The generated summary must not contain content that conflicts with the gold summary.Coherence(1-5): The quality of language coherence in generated summaries, which should not just be a heap of related information.</p>
<p>Comparative (1-5): Assess the extent to whether the generated summary conducts a comparative analysis on references and proposed work.Whether it provides an integrated summary of similar related works.Integrity (1-5): Assess if the summary covers essential elements: research context, reference paper summaries, past research evaluation, contributions, and innovations.Fluency (1-5): Assess the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.Cite Accuracy(1-5): Assess whether the summary correctly cites reference paper in the format '[Reference i]' when mention the reference paper.</p>
<p>Experiment</p>
<p>We validate the capabilities of our proposed ChatCite agent by verifying the following questions: 1) Is the literature summary generated by ChatCite better than that generated directly by LLMs with CoT and other LLM-based literature review approach?2) Do all the modules in the ChatCite contribute to its effectiveness? 3) What specific impact do the modules in the ChatCite framework have on the quality of generated summary?</p>
<p>In this section, we conducted a series of experiments to address these questions.Firstly, we introduced our experimental setup ( §5.1).We compared the performance of existing large language models (LLMs) in directly generating related work under zero-shot and few-shot settings, as well as the bestperforming LLM-based literature review approach ( §5.2).Additionally, we performed ablation analysis on each module in our agent to verify their respective capabilities ( §5.3).Finally, we conducted a human study for a detailed quality assessment of the generated related work summaries ( §5.4).</p>
<p>Experimental Setup</p>
<p>Dataset.We conducted experiments to validate on a paper dataset NudtRwG-Citation dataset (Wang et al., 2020) designed for related work summarization task.This test set includes 50 academic research papers in the field of Computer Science, each data containing the following components: 1) A target paper requiring related work generation without the related work section.2) A ground truth related work section.3) Reference papers of the target paper (annotated with authors and years).</p>
<p>Each paper is well-received in conferences of computational linguistics and natural language processing, with an average citation number reaching 63.59, which indicates these target papers are widely recognized by the academic community.Models.For the LLMs baseline, we employed the GPT-3.5 model (Ouyang et al. (2022)) with a 16k context window (version gpt-3.5-turbo-1106)and the GPT-4.0model (Achiam et al. (2023)) with a 128K context window (gpt-4-turbo-preview).We evaluated their performance under zero-shot and few-shot settings.For the previously bestperforming LLM-based literature review approach, we use the recently proposed approach LitLLM (Agarwal et al., 2024) as the baseline.We reproduce their ability to generate literature summaries according to the CoT prompt mentioned in their paper.To showcase its best performance, we use GPT-4.0 as the decoder for the LitLLM baseline.For our model, due to the high cost of GPT-4.0,we conducted experiment based on GPT-3.5 (version gpt-3.5-turbo-1106)as the decoder for the experiment.For evaluation, we use GPT-4.0 (gpt-4-turbopreview) as decoder.</p>
<p>Implementation.In zero-shot setting, for GPT-3.5 model, due to the limitation of the context window, a two-step approach is used for generation: 1) summarizing and then generating with the prompt [p s ] ="Summarize the current article, preserving as much information as possible.Content:{content}" for summarization.For generating the related work section, we use the prompt [p g ] = "Generate the related work section based on the given target paper summary and its references summary.Read the Target Paper Content: {Target}.References content: {References}".For GPT-4.0 and LitLLM with GPT-4.0,[p g ] is directly used for summarization.</p>
<p>In the few-shot setting, we add the instruction "Follow the writing style of the example but without including any content from the example.{Exam-ples}" to the zero-shot prompt.Evaluation metrics.We utilize both automatic metrics and human evaluations to assess the generic result.We employed traditional automatic metrics for summarization evaluation -the vocabulary overlap measures ROUGE-1/2/L (F1) (Lin (2004b)), our proposed LLM-based evaluation metrics G-Eval, and human evaluation under the same evaluation criterion.</p>
<p>Main Results</p>
<p>We compared the performance of different baseline models on the paper test set (see Table 1).In traditional summary evaluation metrics, such as ROUGE, GPT-4.0 achieved the best results under zero-shot settings.Although ROUGE scores of ChatCite may be slightly lower than GPT-4.0 with zero-shot, its performance in quality metrics generated by LLMs and the preference of LLMs is far superior to results obtained directly from other LLM baselines.</p>
<p>Surprisingly, GPT-4.0 performed poorly in fewshot settings.It is found that influenced by examples in the few-shot, resulting in irrelevant and erroneous summaries after case study.Notably, LitLLM with GPT-4.0 produced outcomes similar to GPT-4.0 in zero-shot but significantly lower than ChatCite.</p>
<p>Therefore, we conclude that "ChatCite performs best among LLM-based literature summarization methods, and the approach following the human workflow guidance is superior to the results obtained by the Chain of Thought (CoT) method."The results are automatically evaluated using ROUGE-1/2/L (F1) and the GPT-4.0evaluator.G-Score represents the total score assessed by the GPT-4.0evaluator, while G-Prf.indicates the model preferences among the five models.
Model ROUGE Metrics G-Score G-Prf. ROUGE-1 ROUGE-2 ROUGE-L (1-5) (%) GPT-3.5 w/</p>
<p>Ablation Analysis</p>
<p>Our Comparative Incremental Generator, with the results of GPT-3.5 w/few-shot used as the baseline for GPT-3.5.</p>
<p>of ChatCite with and without the Reflective Mechanism.However, the overall results of ChatCite are slightly higher, with minimal distribution outliers, suggesting a more stable generation of results.This affirms that the Reflective Mechanism effectively improves the quality and stability of the text generated in ChatCite.</p>
<p>Overall, through ablation experiments on three components, we have demonstrated that "each part of ChatCite framework contributes to the improvement of the quality and stability of the generated results in literature summaries".</p>
<p>Human Study</p>
<p>To conduct a fine-grained analysis on the quality of summary generated by ChatCite and to understand the specific impact of individual modules on summarization, we conducted a human study.Several researchers in the field of computer science, with experience in academic writing, were enlisted to evaluate 10 selected samples using the same set of criteria and choose the better summary.Figure 4 demonstrates the results of G-score metric align with human preferences.Specifically, the method incorporating Key Element Extractor exhibits higher content consistency.Summaries generated with the Comparative Incremental gener- Additionally, Figure 5 shows the extinct human preference of the ChatCite model over the others.</p>
<p>Conclusion</p>
<p>LLMs are powerful tools in generating literature summaries, however, it poses the challenges of information omission, lack of comparative summaries and organizational deficiencies.In ChatCite, the Key Element Extractor contributes to improving content consistency, and the Comparative Incremental Generator effectively enhances the organizational structure, comparative analysis, and citation accuracy of the generated summary.Additionally, the literature summaries generated by ChatCite can be directly used for drafting literature reviews.Our study also demonstrated that the approach following the human workflow guidance is superior to the results obtained by the Chain of Thought (CoT) method.In the future, we hope that our work will further inspire research on complex inferential writing, enabling the full potential of LLMs in open-ended writing tasks.</p>
<p>Limitations</p>
<p>In this work, we focused mainly on the summarization of specific topics based on the selected literatures instead of the collection and the filtering of the literatures themselves.The datasets primarily consist of research articles in the area of computer science and lack research articles from other fields of study to validate our model.Our experimentation used Chat GPT 3.5 as the tool for validating the quality of the generated content and the functionalities of the various components of the agent.We did not explore any additional spec that can influence the result of the GPT3.5 model nor the possibility of using other models as the validation tool.The evaluation of the generated content poses a great challenge.We evaluated the generated results from multiple dimensions using G-Score as the performance metric, but there is still room for improvements over the accuracy of the automatic evaluation process.The generated results exhibit randomness and instability.While our proposed approach demonstrates the effectiveness of the agent, the results have shown further research potential on improving the stability and quality of the output.</p>
<p>Ethics Statement</p>
<p>The dataset we used consists of research articles sourced only from publicly available papers, eliminating concerns about data origin.We employ large language models as generators used and only used for summarizing people's ideas and literature and never on the innovative writing processes of the academic papers.However, if generated literature summaries are to be incorporated into academic paper writing, a review and editing of the generated results should be conducted.This ensures that academic writing content is free from harmful information and plagiarism issues.</p>
<p>We will make our code publicly available to ensure experiment reproducibility.</p>
<p>A Appendix</p>
<p>A.1 An Example of generated results of all the models mentioned Table 3: An Example of literature summary results generated for Paper: BEL: Bagging for Entity Linking Gold literature Summary Statistical machine translation systems often rely on large-scale parallel and monolingual training corpora to generate translations of high quality.Unfortunately, statistical machine translation system often suffers from data sparsity problem due to the fact that phrase tables are extracted from the limited bilingual corpus.Much work has been done to address the data sparsity problem such as the pivot language approach (Wu and Wang,2007;Cohn and Lapata, 2007) and deep learning techniques (Devlin et al., 2014;Gao et al., 2014;Sundermeyer et al., 2014;Liu et al., 2014).On the problem of how to translate one source language to many target languages within one model, few work has been done in statistical machine translation.A related work in SMT is the pivot language approach for statistical machine translation which uses a commonly used language as a "bridge" to generate source-target translation for language pair with few training corpus.Pivot based statistical machine translation is crucial in machine translation for resource-poor language pairs, such as Spanish to Chinese.Considering the problem of translating one source language to many target languages, pivot based SMT approaches does work well given a large-scale source language to pivot language bilingual corpus and large-scale pivot language to target languages corpus.However, in reality, language pairs between English and many other target languages may not be large enough, and pivot-based SMT sometimes fails to handle this problem.Our approach handles one to many target language translation in a different way that we directly learn an end to multi-end translation system that does not need a pivot language based on the idea of neural machine translation.Neural Machine translation is a emerging new field in machine translation, proposed by several work recently (Kalchbrenner and Blunsom, 2013;Sutskever et al., 2014;Bahdanau et al., 2014), aiming at end-to-end machine translation without phrase table extraction and language model training.Different from traditional statistical machine translation, neural machine translation encodes a variable-length source sentence with a recurrent neural network into a fixed-length vector representation and decodes it with another recurrent neural network from a fixed-length vector into variable-length target sentence.A typical model is the RNN encoder-decoder approach proposed by Bahdanau et al. ( 2014), which utilizes a bidirectional recurrent neural network to compress the source sentence information and fits the conditional probability of words in target languages with a recurrent manner.Moreover, soft alignment parameters are considered in this model.As a specific example model in this paper, we adopt a RNN encoder-decoder neural machine translation model for multi-task learning, though all neural network based model can be adapted in our framework.In the natural language processing field, a1724 notable work related with multi-task learning was proposed by Collobert et al. ( 2011) which shared common representation for input words and solve different traditional NLP tasks such as part-of-Speech tagging, name entity recognition and semantic role labeling within one framework, where the convolutional neural network model was used.Hatori et al. (2012) proposed to jointly train word segmentation, POS tagging and dependency parsing, which can also be seen as a multi-task learning approach.Similar idea has also been proposed by Li et al. (2014) in Chinese dependency parsing.Most of multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005) where they jointly trained models and shared center parameters in NLP tasks.Researchers have also explored similar approaches (Sennrich et al., 2013;Cui et al., 2013) in statistical machine translation which are often refered as domain adaption.Our work explores the possibility of machine translation under the multitask framework by using the recurrent neural networks.To the best of our knowledge, this is thefirst trial of end to end machine translation under multi-task learning framework.</p>
<p>ChatCite with GPT-3.5 Related Work</p>
<p>The field of machine translation has witnessed remarkable progress in addressing data scarcity and enhancing translation quality.Early approaches such as the pivot language approach proposed by Wu and Wang (2007)  ] presents a joint model for word segmentation, POS tagging, and dependency parsing for Chinese, demonstrating significant improvements in accuracies of the three tasks over baseline models.The study's positive results in Chinese segmentation accuracies and the effectiveness of the proposed joint model are highly relevant to the advancements in multi-task learning frameworks for machine translation, providing further insights into joint model approaches and their potential for improving translation quality.The 11th reference paper by Rie Kubota Ando and Tong Zhang (2005) focuses on learning predictive structures from multiple tasks and unlabeled data.It attempts to improve the performance of supervised learning algorithms by including unlabeled data and investigates a closely related problem that leads to a novel approach to semi-supervised learning.The paper introduces a general framework for structural learning and proposes algorithms for structural learning, demonstrating the effectiveness of the proposed algorithms in semi-supervised learning.In a similar vein, the proposed multi-task learning framework in the target paper aims to improve the translation quality and address data scarcity by leveraging multiple tasks and large-scale parallel training corpora.By integrating insights from Rie Kubota Ando and Tong Zhang (2005) [Reference 11] with existing literature, the target paper aligns with the broader trend of leveraging multiple tasks and unlabeled data to enhance machine learning models.The contributions of both the target paper and the referenced works collectively enrich the understanding of addressing data scarcity, enhancing translation quality, and advancing the capabilities of machine translation models, laying the foundation for the multi-task learning framework proposed in the target paper.These contributions provide valuable insights into the advancements in machine translation, emphasizing contributions to the research field and predicting potential future directions.</p>
<p>ChatCite without Key Elements Extractor Related Work</p>
<p>The field of machine translation has witnessed significant advancements in overcoming challenges related to limited training resources and enhancing translation quality across various language pairs.Notably, approaches such as the pivot language method, multi-parallel corpora, and the triangulation method have been explored to address scenarios with limited training data [Radford et al. (2019)[Reference 5]].Additionally, the development of neural network joint models (NNJM) [Sutskever, Vinyals, and Le (2014)[Reference 8]] and the integration of bidirectional recurrent neural networks have shown promise in improving translation quality, particularly for language pairs with limited resources.In alignment with these developments, the target paper focuses on the translation of sentences from a source language to multiple target languages using a multi-task learning framework inspired by neural machine translation.This approach not only demonstrates substantial progress in machine translation, especially for languages with limited training resources but also introduces a neural network joint model (NNJM) for statistical machine translation, aligning with the innovative approaches presented in the referenced studies and offering promise for enhancing translation quality for language pairs with limited resources.Moreover, Ïncremental joint approach to word segmentation, POS tagging, and dependency parsing in Chinese [Reference 10] proposes a joint model for word segmentation, POS tagging, and dependency parsing in Chinese, showing improvements particularly for out-of-vocabulary words.This study contributes to a comprehensive understanding of the methods used to address challenges in machine translation and natural language processing.Expanding on these foundations, the 11-th reference paper introduces a framework for learning predictive structures from multiple tasks and unlabeled data.This framework presents a general approach for formulating and analyzing the structural learning problem in the context of learning with unlabeled data, demonstrating its relevance to the challenges in machine translation.The proposed algorithm for joint empirical risk minimization is particularly notable, as it addresses the problem of learning predictive structures on hypothesis spaces from multiple learning tasks, aligning with the multi-task learning framework proposed in the target paper.Furthermore, the experiments presented in the 11-th reference show significant performance improvements over supervised methods in various tasks, indicating the effectiveness of the proposed framework in semi-supervised learning settings.In summary, the integrated literature on machine translation and natural language processing contributes valuable insights for the development of machine translation systems, offering innovative approaches to enhance translation quality, particularly for language pairs with limited resources.The introduction of the framework for learning predictive structures from multiple tasks and unlabeled data adds a new dimension to the existing research landscape, emphasizing the potential for semi-supervised learning to address challenges in machine translation and natural language processing.These advancements collectively pave the way for potential future research directions in the field of machine translation.</p>
<p>ChatCite without Reflective Incremental Generator</p>
<p>Related Work</p>
<p>The related work section provides an overview of the literature relevant to the target paper's research topic, which aims to address the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages.The related work encompasses various studies in the field of machine translation, natural language processing, and neural network models.</p>
<p>[1] Hua Wu and Haifeng Wang (2007) proposed a pivot language approach for phrase-based statistical machine translation, which addresses the translation problem for language pairs with scarce resources by using a pivot language and making use of large bilingual corpora without language-dependent resources or deep linguistic processing.The study demonstrated the effectiveness of the pivot language method for translation on language pairs with a small bilingual corpus.</p>
<p>[2] Trevor Cohn and Mirella Lapata ( 2007) introduced the method of triangulation for translation modeling, which translates from a source to a target language via an intermediate third language, to exploit multi-parallel corpora for training and improve the coverage and quality of phrase-based statistical machine translation.The research focused on addressing the issue of poor performance of current phrase-based SMT systems when using small training sets.</p>
<p>[3] Jacob Devlin et al. ( 2014) formulated a neural network joint model (NNJM) for machine translation, along with techniques to overcome the high cost of using NNLM-style models in MT decoding.The study demonstrated significant improvements in machine translation performance using the proposed NNJM and its variations.</p>
<p>[4] Jianfeng Gao et al. ( 2014) introduced the Continuous-space Phrase Translation Model (CPTM) to address the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations.The study demonstrated substantial improvement over the baseline system with a statistically significant margin.</p>
<p>[5] Martin Sundermeyer et al. ( 2014) explored the effectiveness of recurrent neural networks in translation modeling, specifically focusing on word-based and phrase-based approaches, as well as the inclusion of bidirectional architectures.The research demonstrated improvements over strong baselines in translation modeling.</p>
<p>[6] Zhenghua Li et al. ( 2011) proposed a joint optimization approach for Chinese POS tagging and dependency parsing, showcasing significant improvements in parsing accuracy.The study addressed the issue of error propagation in parsing accuracy due to using automatic POS tags instead of gold ones.</p>
<p>[7] Nal Kalchbrenner and Phil Blunsom (2013) introduced Recurrent Continuous Translation Models (RCTMs), demonstrating their translation capabilities and low perplexities with respect to reference translations.The study highlighted the sensitivity of RCTMs to word order, syntax, and meaning despite lacking alignments.</p>
<p>[8] Ilya Sutskever et al. ( 2014) presented a general end-to-end approach to sequence learning using Deep Neural Networks (DNNs) for mapping sequences to sequences, showcasing the outperformance of the proposed LSTM-based approach over a mature SMT system.</p>
<p>[9] Ronan Collobert et al. (2011) proposed a unified neural network architecture and learning algorithm for various natural language processing tasks, demonstrating the effectiveness of leveraging large unlabeled datasets and multitask supervised training to improve performance.</p>
<p>[10] Jun Hatori et al. ( 2012) introduced an incremental joint approach to word segmentation, POS tagging, and dependency parsing for Chinese, showcasing substantial improvements in the accuracies of the three tasks over baseline models.</p>
<p>[11] Rie Kubota Ando and Tong Zhang (2005) proposed a framework for learning predictive structures from multiple tasks and unlabeled data, demonstrating significant performance improvements over supervised methods in various tasks through structural learning.These studies collectively contribute to the field of machine translation, natural language processing, and neural network models, providing insights and advancements that are relevant to the target paper's research on learning a machine translation model for translating sentences from one source language to multiple target languages.The innovations and findings from these studies serve as valuable background and insights for the target research, highlighting the significance of the related work to the target study and potential future research directions in the field.</p>
<p>GPT-3.5 with zero shot Related Work Section: The proposed multi-task learning framework for translating sentences from one source language to multiple target languages using a machine translation model builds upon and is related to several key works in the field of machine translation and natural language processing. 1. Pivot language approach for phrase-based statistical machine translation The article Pivot language approach for phrase-based statistical machine translationproposes a method for phrase-based statistical machine translation using a pivot language to address translation problems for language pairs with scarce resources.This method is related to the challenge of translating between languages with limited parallel data, which is also addressed in the target paper.The pivot language approach demonstrates the effectiveness of utilizing large bilingual corpora from a pivot language to improve translation quality, especially for language pairs with limited training data.2. Machine translation by triangulation: Making effective use of multi-parallel corpora The article discusses the method of triangulation, which involves translating from a source to a target language via an intermediate third language, aiming to improve machine translation systems by exploiting multi-parallel corpora.This work is relevant to the target paper as it addresses the issue of improving translation quality using a wider range of parallel corpora, particularly with small training sets and for translating between low-density language pairs.3. Fast and robust neural network joint models for statistical machine translation The article presents a novel neural network joint model (NNJM) for statistical machine translation, which is related to the use of neural network models in machine translation.The NNJM augments a neural network language model with a source context window and is purely lexicalized, demonstrating significant improvements in BLEU scores.This work is relevant to the target paper's exploration of neural machine translation and the effectiveness of neural network-based models in improving translation quality.</p>
<p>Learning continuous phrase representations for translation modeling</p>
<p>The article introduces a novel phrase translation model, the Continuous-space Phrase Translation Model (CPTM), which addresses the sparsity problem in estimating phrase translation probabilities.This work is related to the target paper's exploration of continuous representations and their use in improving translation accuracy.The CPTM significantly improves the accuracy of a state-of-the-art phrase-based SMT system, demonstrating its effectiveness in improving translation quality.</p>
<p>Translation modeling with bidirectional recurrent neural networks</p>
<p>The article presents two different translation models using recurrent neural networks, demonstrating their capability to improve machine translation performance.This work is related to the target paper in showcasing the effectiveness of recurrent neural networks in improving translation quality, particularly with the use of bidirectional recurrent models.6. Joint optimization for Chinese POS tagging and dependency parsing The article discusses the importance of joint optimization for Chinese POS tagging and dependency parsing, proposing a unique model to optimize both tasks.This work is related to the target paper's exploration of joint learning frameworks and their impact on improving parsing accuracy, demonstrating the potential of joint optimization for improving translation quality.</p>
<p>Recurrent continuous translation models</p>
<p>The article introduces a class of probabilistic continuous translation models called Recurrent Continuous Translation Models (RCTMs), which are purely based on continuous representations for words, phrases, and sentences.This work is related to the target paper's exploration of continuous representations and their potential in estimating translation quality, highlighting the effectiveness of RCTMs in capturing syntactic and semantic information.8. Sequence to sequence learning with neural networks The article discusses the use of Deep Neural Networks (DNNs) for sequence to sequence learning, particularly in the context of machine translation.This work is related to the target paper's exploration of neural network-based models and their effectiveness in improving translation performance, demonstrating the potential of sequence to sequence learning with neural networks for various sequence to sequence problems.9. Incremental joint approach to word segmentation, POS tagging, and dependency parsing in Chinese The article proposes a joint model for word segmentation, POS tagging, and dependency parsing in Chinese, demonstrating significant improvements in accuracies for all three tasks over baseline models.This work is related to the target paper's exploration of joint learning frameworks and their impact on improving translation quality, particularly in the context of joint modeling for multiple language translation tasks.10.A framework for learning predictive structures from multiple tasks and unlabeled data The article presents a framework for learning predictive structures from multiple tasks and unlabeled data, addressing the concept of semi-supervised learning and the problem of learning predictive structures on hypothesis spaces from multiple learning tasks.This work is related to the target paper's exploration of multi-task learning frameworks and their effectiveness in improving translation quality, particularly in the context of learning from multiple tasks and unlabeled data.The related work presented here demonstrates the diverse approaches and techniques employed in the field of machine translation and natural language processing, providing valuable insights and methods that contribute to the advancement of multi-task learning frameworks and neural machine translation models for improving translation quality.</p>
<p>GPT-3.5 with few shot Related Work Section: The related work section will include a review of existing literature and studies that are relevant to the topic of multi-task learning for machine translation and related methods for improving translation quality.The section will discuss the following papers: 1. Pivot language approach for phrase-based statistical machine translation -This study proposes a new method for phrase-based statistical machine translation using a pivot language to address translation problems for language pairs with scarce resources.The method demonstrates the use of large bilingual corpora from a pivot language to improve translation quality for language pairs with limited training data.2. Machine translation by triangulation: Making effective use of multi-parallel corpora -The article discusses a method for improving machine translation systems by exploiting multi-parallel corpora.The proposed method, called triangulation, involves translating from a source to a target language via an intermediate third language, allowing the use of a wider range of parallel corpora for training.3. Fast and robust neural network joint models for statistical machine translation -This article discusses the development of a novel neural network joint model (NNJM) for statistical machine translation (SMT).The NNJM augments a neural network language model (NNLM) with a source context window and demonstrates significant improvements in translation quality.4. Learning continuous phrase representations for translation modeling -The study presents a novel phrase translation model for statistical machine translation (SMT) that addresses the sparsity problem in estimating phrase translation probabilities.The model utilizes continuous phrase representations to improve the accuracy of a state-of-the-art phrase-based SMT system. 5. Translation modeling with bidirectional recurrent neural networks -The article introduces two different translation models using recurrent neural networks and demonstrates their effectiveness in improving machine translation performance on various tasks.6. Recurrent continuous translation models -This study introduces a class of probabilistic continuous translation models called Recurrent Continuous Translation Models (RCTMs) and demonstrates their potential in capturing syntactic and semantic information and estimating translation quality.7. Sequence to sequence learning with neural networks -The article discusses the use of Deep Neural Networks (DNNs) for sequence to sequence learning and demonstrates the effectiveness of this approach on an English to French translation task.8. Ïncremental joint approach to word segmentation, POS tagging, and dependency parsing in Chinese -The article proposes a joint model for word segmentation, POS tagging, and dependency parsing in Chinese and demonstrates significant improvements in accuracies for all three tasks over baseline models.9. Ä framework for learning predictive structures from multiple tasks and unlabeled data -The study presents a framework for learning predictive structures from multiple tasks and unlabeled data, demonstrating significant performance improvements over supervised methods in various text categorization, named entity chunking, part-of-speech tagging, and hand-written digit image classification tasks.</p>
<p>GPT-4.0 with zero shot</p>
<p>Based on the given information, here is a related work section for the target paper: Related Work The problem of learning from multiple tasks and utilizing unlabeled data has been an area of significant research interest in machine learning and natural language processing (NLP).The concept of semi-supervised learning, which involves the use of both labeled and unlabeled data, has been explored in various studies with the aim of improving the performance of supervised learning algorithms.Vapnik introduced the notion of transductive inference, which is a form of semi-supervised learning, although its effectiveness has been debated (Vapnik, 1998;Zhang and Oles, 2000).Co-training, another semi-supervised approach, was proposed by Blum and Mitchell (1998) and is related to the bootstrap method used in NLP applications (Yarowsky, 1995).However, the effectiveness of co-training can be limited by the assumptions made by the method and the potential introduction of noise into the labels (Pierce and Cardie, 2001).The idea of learning predictive structures from multiple tasks, also known as structural learning or multi-task learning, has been discussed in the literature.Ando and Zhang (2005) presented a framework for structural learning where multiple tasks are used to discover a shared low-dimensional predictive structure.This approach is based on the principle that a good hypothesis space should have a small approximation error and a small estimation error.The authors proposed an algorithm for structural learning that finds a common low-dimensional feature space shared by multiple problems.Their method was shown to be effective in semi-supervised learning settings, improving the performance of various NLP tasks.In the context of NLP, the joint approach to tasks such as word segmentation, POS tagging, and dependency parsing has been explored.Hatori et al. (2012) proposed an incremental joint model for Chinese POS tagging and dependency parsing, demonstrating improvements over baseline models.The work by Kubota Ando and Zhang (2005) extends this idea to a joint model that simultaneously processes word segmentation, POS tagging, and dependency parsing, capturing global interactions among these tasks.Their model employs an efficient character-based decoding method that combines features from state-of-the-art segmentation, POS tagging, and dependency parsing models.The use of external resources, such as dictionaries, has also been considered in joint models to enhance performance.Wang et al. (2011) used semi-supervised methods with large auto-analyzed data to improve Chinese word segmentation and POS tagging.The incorporation of syntactic information has been shown to improve the segmentation of out-of-vocabulary words, which is a common challenge in languages like Chinese where transliterated words often include characters used as common or function words.Overall, the related work demonstrates the potential of joint and semi-supervised approaches in leveraging multiple tasks and unlabeled data to improve the performance of machine learning models in NLP tasks.The target paper builds upon these concepts by proposing a unified framework that avoids task-specific engineering and learns internal representations from vast amounts of unlabeled data, achieving good performance across various NLP tasks.</p>
<p>GPT-4.0 with few shot</p>
<p>The examples provided discuss the use of Large Language Models (LLMs) as proxies for human behaviors and the systematic study of LLMs' personalities based on established personality trait theories.Here's a summary of the key points from the examples: 1. LLMs Mimicking Human Behaviors: -Recent studies have shown that LLMs can mimic human behaviors, including reasoning and cognitive abilities, as well as participate in social science experiments.</p>
<p>-These studies are mostly empirical and based on case-by-case observations.2. Systematic Study of LLMs' Personalities: -The authors propose a systematic and quantitative approach to study LLMs' behaviors by evaluating and inducing personality traits in LLMs.</p>
<p>-This approach does not require supervised fine-tuning or human evaluation of generated utterances.</p>
<p>Personality Trait Theories:</p>
<p>-The Big Five and Sixteen Personality Factors (16PF) are two prominent theories that provide consistent descriptions of individual differences.</p>
<p>-Psychometric tests based on these theories, such as NEO-PI-R, are effective in assessing personality traits and have shown that human differences can be categorized into continuous factor dimensions. 4. Correlation Between Personality and Behavior: -Empirical studies have confirmed a strong correlation between personality traits and real-world human behaviors.</p>
<p>-Specifically, there is a significant link between the Big Five traits and language use, suggesting that personality influences how people communicate.The examples highlight the potential of LLMs to simulate complex human-like behaviors and the importance of understanding the personality traits of these models for various applications.</p>
<p>LitLLM with GPT-4.0The Unified Neural Network Model for NLP Tasks Natural Language Processing (NLP) tasks such as part-of-speech (POS) tagging, chunking, named entity recognition (NER), and semantic role labeling (SRL) are essential for understanding and generating human language.Traditional approaches to these tasks often involve task-specific feature engineering, which can be labor-intensive and may not generalize well to new tasks or datasets.To address these issues, researchers have proposed unified neural network models that can learn to perform multiple NLP tasks simultaneously without extensive feature engineering.These models leverage large amounts of both labeled and unlabeled data to learn useful representations for words, phrases, and sentences that capture syntactic and semantic information.One such model is the Long Short-Term Memory (LSTM) network, which has been shown to be effective for sequence-to-sequence learning tasks like machine translation.LSTMs can process input sequences of variable lengths and learn long-range dependencies, making them suitable for NLP tasks.They can be trained end-to-end with backpropagation and can learn from both supervised and unsupervised data.Another approach is the use of semi-supervised learning methods, which combine labeled and unlabeled data to improve model performance.For example, structural learning algorithms can learn predictive structures from multiple tasks and use them to enhance the performance of individual tasks.This can be particularly useful when there is a large amount of unlabeled data available.The success of these unified neural network models demonstrates the potential of using deep learning techniques for NLP tasks.By avoiding task-specific engineering and leveraging large datasets, these models can achieve state-of-the-art performance on a variety of benchmarks, making them valuable tools for researchers and practitioners in the field of NLP."</p>
<p>A.2 Related work draft for this paper generated by ChatCite with GPT-4.0Table 4: We show the related work summary using this paper and our collection of related work papers generated by the ChatCite model based on .It can be observed that the citations in the paper are consistent with those in the collection, the content is accurate at the same time.We utilized this summary as a draft and, completed our related work section after some minor modifications.</p>
<p>Generated literature summary for this paper In the evolving landscape of automated scholarly writing, particularly in the domain of literature review summarization, the proposed ChatCite framework within our target paper represents a significant advancement, aiming to harness the capabilities of Large Language Models (LLMs) to generate high-quality related work summaries.The foundational work in this field began with hierarchical keyword structures for extractive summarization [Reference 1], and has progressed through various iterations including multi-source pointer-generator networks [Reference 2], and sophisticated models like Recent studies have concentrated on the unsupervised and few-shot learning capabilities of LLMs, notably GPT-3, establishing a solid groundwork for AI application in scholarly communication .The exploration of AI tools such as ChatGPT by Huang, Jingshan, and Ming Tan (2023) [Reference 10] in crafting scientific review articles, sheds light on both the potential benefits and drawbacks of AI in academic writing, such as enhancement in speed and quality versus risks of inaccuracies and bias.Building on these insights, the LitLLM toolkit suggested by Shubham Agarwal, Issam H. Laradji et al. (2024) [Reference 11] brings novel approaches to overcome challenges like the generation of hallucinated content and the overlooking of recent studies by employing Retrieval Augmented Generation (RAG) principles, specialized prompting, and instructive techniques.This showcases a significant alignment with our target paper's objectives, particularly in mitigating hallucinations and enhancing summarization accuracy through the ChatCite framework.The latest contribution in this domain is presented by Yang Liu, Dan Iter, et al. (2023) [Reference 12] through the G-EVAL framework, which introduces a methodology for evaluating the quality of texts generated by natural language generation (NLG) systems using LLMs, leveraging chain-of-thoughts (CoT) and a form-filling paradigm.This approach, which significantly outperforms previous methods in evaluating the creativity and diversity required in text generation tasks, aligns closely with the goals of the ChatCite framework, especially in its pursuit of generating high-quality literature summaries.The G-EVAL framework not only surpasses all previous state-of-the-art evaluators on the SummEval and Topical-Chat benchmarks but also underscores the importance of instruction sensitivity and the benefits of a CoT approach, which could potentially inform future enhancements in the ChatCite framework [Reference 12].By evaluating the quality of NLG outputs, [Reference 12] addresses the limitations identified in prior works, including the potential bias of LLM-based evaluators towards LLM-generated texts, and calls for further research to understand the behavior of LLM-based evaluators fully.This contribution is particularly relevant to our target paper, which aims to enhance the quality and stability of literature review summarizations generated by LLMs.Thus, integrating insights from G-EVAL [Reference 12] can provide a more nuanced understanding of evaluating the effectiveness of automated scholarly writing tools, highlighting a potential area for future development within the scope of the ChatCite framework.In summary, the development of automated scholarly writing, particularly in literature review summarization, has progressed through key innovations in LLM capabilities, methodological enhancements, and now, through improved evaluation frameworks.This journey from extractive summarization to sophisticated LLM-based frameworks underscores a collaborative advancement in the field, with each contribution building upon the last to address emerging challenges and achieve greater accuracy, efficiency, and relevance in automated scholarly writing.The ChatCite framework, supported by insights from references [References 1-12], continues this trajectory by proposing innovative solutions to the challenges in literature review summarization, setting a platform for further exploration and innovation in the field.</p>
<p>Figure 2 :
2
Figure2: The ChatCite consists of two modules, the Key Element Extractor and the Reflective Incremental Generator.The agent mimicking human workflow generates literature summary utilizing the Key Element Extractor to process the proposed work description and reference paper in Reference Papers Set.It then iteratively generates literature summaries using each paper in the Reference Papers Set, proposed work key elements and previous summary generated with the Reflective Incremental Generator.This process is iteratively repeated until a complete related work summary is generated, and the optimal one is selected as the final result.</p>
<p>Figure 4 :
4
Figure 4: Human Evaluation vs. G-Score on six dimensions of the generic summary quality.The scoring results of the G-Score model is aligned with the distribution of human evaluations.</p>
<p>[Reference 1], and the triangulation method introduced by Cohn and Lapata (2007) [Reference 2], focused on improving phrase-based statistical machine translation, particularly for language pairs with scarce resources.Additionally, Devlin et al. (2014) [Reference 3] contributed by introducing a neural network joint model (NNJM) for machine translation, offering valuable insights into the formulation of a novel neural network joint model and techniques for enhancing machine translation performance.Moreover, Gao et al. (2014) [Reference 4] enriched the understanding of addressing data scarcity and improving translation quality by introducing a Continuous-space Phrase Translation Model (CPTM) to project source and target phrases into continuous-valued vector representations, complementing the approaches of previous researchers.Furthermore, Sundermeyer et al. (2014) [Reference 5] expanded the scope of machine translation by investigating the effectiveness of recurrent neural networks in translation modeling, contributing to the broader landscape of machine translation research.In the context of the target paper, which proposed a multi-task learning framework for machine translation, the works by Zhenghua Li et al. (2011) [Reference 6] and Nal Kalchbrenner and Phil Blunsom (2013) [Reference 7] proved highly relevant.Zhenghua Li et al. (2011) proposed joint optimization for Chinese POS tagging and dependency parsing, offering valuable insights into joint optimization techniques.Similarly, Nal Kalchbrenner and Phil Blunsom (2013) [Reference 7] introduced recurrent continuous translation models (RCTMs), providing a new perspective in the domain of machine translation and contributing to the advancement of purely continuous sentence-level translation models.The work by Ilya Sutskever, Oriol Vinyals, and Quoc V. Le (2014) [Reference 8] on sequence to sequence learning with neural networks is also highly relevant to the target paper's objectives.The paper addresses the challenge of mapping sequences to sequences using Deep Neural Networks (DNNs) and proposes a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure, aligning with the multi-task learning framework proposed in the target paper.Additionally, the 9th reference paper by Ronan Collobert et al. (2011) [Reference 9] addresses the effectiveness of leveraging large unlabeled datasets and multitask supervised training to improve performance, aligning with the multi-task learning framework proposed in the target paper.The recently explored Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese by Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun (2012) [Reference 10</p>
<p>the ARWG system [Reference 3], BACO [Reference 4], and the Relation-aware Related work Generator (RRG) [Reference 5].The introduction of contrastive learning to improve summarization quality [Reference 6], and automatic citation sentence generation methods [Reference 7], have further refined the capabilities of LLMs in this space.</p>
<p>Turn One Turn Two Turn Three Trun Three Reflective Incremental Generator
푟� 1푟� 2푟� 2Proposedwork discripition푟� 3푟� 3푟� 3푟� 3푟� 4푟� 4푟� 4푟� 4푟� 4푟� 4Choice 1Choice 2Choice 3Choice 4Choice 5Choice 6.........ReferencePapersSetChoice 3Choice 4Choice 1Choice 6Choice 2Choice 5</p>
<p>Table 1 :
1
Main Results:
zero shot26.016.1124.023.41022.21GPT-3.5 w/few shot25.846.0123.553.596810.80GPT-4 w/zero shot30.028.0327.973.507626.40GPT-4 w/few shot15.521.7814.201.66210.21LitLLM w/GPT-427.086.0724.943.544824.51ChatCite25.306.3623.134.064235.86</p>
<p>Table 2 :
2
proposed framework can be decomposed into two components: the Key Element Extractor and the Reflective Incremental Generator.The Reflective Incremental Generator comprises two key points: the Comparative Incremental Generation and the Reflective Mechanism.Therefore, we will analyze the three part separately.Key Element Extractor.To validate the effectiveness of the Key Element Extractor, we chose ChatCite without the Key Element Extractor as a comparison.The ChatCite without Key Element Extractor used the baseline summary prompt [p s ] to directly summarize the article and then use Reflective Incremental Generator generate the literature summary.In Table2, comparing the results of ChatCite without Key Element Extractor and ChatCite, we can observe that ChatCite performs better in all dimensions of ROUGE metrics and the metrics generated by the LLM based evaluator.Therefore, it indicates that the Topic Extractor module plays an effective role in literature summarization.Comparative Incremental Mechanism.To validate the effectiveness of the Comparative Incremental Mechanism, we choose ChatCite without Comparative Incremental Mechanism as comparison, following the few-shot baseline prompt [p s ] and few-shot examples as prompts to directly generate literature summaries from the text after standard summarization.Considering controlling variables for the incremental mechanism, we also incorporated CoT writing instructions into the method to ensure that the experimental results are not influenced by the writing instructions.In Table 2, when comparing ChatCite with and without the Comparative Incremental Mechanism, the results indicate that ChatCite achieves higher ROUGE metrics and LLM-based evaluation metrics compared to ChatCite without the Comparative Incremental Mechanism.This suggests that the Comparative Incremental Mechanism significantly contributes to the effectiveness of literature summarization in the ChatCite framework.Ablation Results: This table presents the ablation results on the model's Key Element Extractor and
ChatCite -w/o Reflective54321Scores5Consistency Coherence ComparativeIntegrity ChatCiteFluency Cite_Accuracy Overall4321Consistency Coherence ComparativeIntegrityFluency Cite_Accuracy OverallEvaluation DimensionsFigure 3: Ablation Study on the Reflective Mechanism.The upper and lower whiskers represent the overallrange of the data, while the box displays the distributionof the middle 50% of the dataset, with a line inside thebox representing the median of the data. Data pointsoutside the boxplot are considered outliers, indicatingdata points that significantly deviate from the box andwhiskers. It can be observed that ChatCite performsmore stable across all dimensions.
Reflective Mechanism.In conclusion, we analyzed the reflective mechanism's impact.G-Scores for various dimensions were assessed based on multiple results from ChatCite, both with and without the Reflective Mechanism.The boxplot results in Figure3show similarities between the outcome</p>
<p>Six evaluation dimensions are: Consistency, Coherence, Comparative, Integrity, Fluency, Cite Accuracy.
Our related work utilizes summaries generated by ChatCite with GPT-4 as a draft, with minimal refinement. The information is comprehensive with minimal errors. The generated results organize the literature and include comparative analysis. The generated results are presented in the appendix (
Table 4).
Please extr act the following key elements fr om the content:
Automatic related work section generation: experiments in scientific document abstracting. Ahmed Abura'ed, Horacio Saggion, Alexander Shvets, Àlex Bravo, 10.1007/s11192-020-03630-22020125</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Litllm: A toolkit for scientific literature review. Shubham Agarwal, Issam H Laradji, Laurent Charlin, Christopher Pal, 2024</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</p>
<p>Capturing relations between scientific papers: An abstractive model for related work section generation. Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, Rui Yan, 10.18653/v1/2021.acl-long.473Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Target-aware abstractive related work generation with contrastive learning. Xiuying Chen, Hind Alamro, Li Mingzhe, Shen Gao, Rui Yan, Xin Gao, Xiangliang Zhang, Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval2022</p>
<p>BACO: A background knowledge-and content-based framework for citing sentence generation. Yubin Ge, Ly Dinh, Xiaofeng Liu, Jinsong Su, Ziyao Lu, Ante Wang, Jana Diesner, 10.18653/v1/2021.acl-long.116Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>News summarization and evaluation in the era of. Tanya Goyal, Junyi , Jessy Li, Greg Durrett, 20233</p>
<p>Towards automated related work summarization20. Cong Duy, Vu Hoang, Min-Yen Kan, Coling 2010: Posters. Beijing, China2010. 2010Organizing Committee</p>
<p>The role of chatgpt in scientific communication: writing better scientific review articles. Jingshan Huang, Ming Tan, American Journal of Cancer Research. 13411482023</p>
<p>Automatic related work section in scientific article: Research trends and future directions. Army Justitia, Hei-Chia Wang, 2022 International Seminar on Intelligent Technology and Its Applications (ISITIA). IEEE2022</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Annual Meeting of the Association for Computational Linguistics. 2004a</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004b</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-eval: Nlg evaluation using gpt-4 with better human alignment. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>Toc-rwg: Explore the combination of topic model and citation information for automatic related work generation. Pancheng Wang, Shasha Li, Haifang Zhou, Jintao Tang, Ting Wang, IEEE Access. 82020</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Automatic generation of citation texts in scholarly papers: A pilot study. Xinyu Xing, Xiaosheng Fan, Xiaojun Wan, Annual Meeting of the Association for Computational Linguistics. 2020</p>            </div>
        </div>

    </div>
</body>
</html>
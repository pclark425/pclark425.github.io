<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5063 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5063</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5063</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-03eb382e04cca8cca743f7799070869954f1402a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/03eb382e04cca8cca743f7799070869954f1402a" target="_blank">CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> This work presents a diagnostic dataset that tests a range of visual reasoning abilities and uses this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.</p>
                <p><strong>Paper Abstract:</strong> When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover short-comings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5063.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5063.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM (question-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-based question encoder (question-only baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent neural network (LSTM) that encodes the question text and predicts answers without using image input; used as a bias-only baseline to measure question-conditional biases and short-term memory capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (question-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Word embeddings followed by a word-level LSTM (one or two layers, 512 or 1024 units per layer as tuned in experiments). The final LSTM hidden state is passed to a multi-layer perceptron classifier predicting answers. No visual input is provided (models only the question distribution). Trained with Adam; no parameter counts reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CLEVR visual reasoning questions (diagnostic spatial reasoning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Synthetic visual question-answering tasks requiring attribute identification, counting, attribute comparison, and spatial relations (left/right/in front/behind). Questions are represented as functional programs executed on scene graphs; tasks test spatial reasoning, short-term memory (comparing attributes), and compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Language-only statistical modeling of question-conditioned answer distributions using LSTM encoding and MLP classification; serves as a baseline that can exploit question-conditional biases rather than visual/spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>By design LSTM has no access to image information and therefore cannot perform visual/spatial reasoning; used to reveal question-conditional biases (e.g., LSTM achieves ~60% on some existence questions due to bias). The paper uses LSTM performance as negative evidence that a model is not using spatial reasoning when it outperforms or matches LSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accuracies (examples): for attribute-query types LSTM attains chance-like rates consistent with the attribute cardinalities (e.g., ~12.5% for color queries reflecting 8 colors); for counting LSTM ~42%; for existence questions ~60% (indicating bias); for attribute-comparison tasks LSTM ~50% (chance for yes/no). Exact per-question-type values reported in the analysis sections.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cannot use image information or perform true spatial reasoning; it captures question-conditional biases and thus performs poorly on tasks requiring vision or short-term memory of visual attributes; its performance can misleadingly be high when dataset biases exist.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a baseline against image-aware models (CNN+LSTM, attention models). LSTM exposes dataset biases and provides a lower-bound that image-enabled models must beat to demonstrate use of visual/spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5063.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5063.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN+LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CNN image features with LSTM question encoder and MLP answer predictor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard multimodal VQA model that encodes images with a pretrained CNN and questions with an LSTM, concatenates features, and predicts answers with an MLP classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CNN+LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Image features extracted from a ResNet-101 pretrained on ImageNet (final average pooling layer giving 2048-D features, images resized to 224×224). Questions encoded by LSTM (512 or 1024 units). Image and question features concatenated and passed to an MLP (ReLU, dropout) to predict answer distribution. Trained with Adam; networks and hyperparameters tuned on validation set. No total parameter counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CLEVR visual reasoning questions (spatial reasoning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same as above: visual questions that require object recognition, spatial relation reasoning (left/right/in front/behind defined relative to camera projection), counting, attribute comparison, and compositional chains of reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Early-fusion multimodal approach: CNN visual features concatenated with LSTM question representation, followed by MLP answer classification. No explicit spatial attention or memory mechanism in this variant.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performance differences vs. LSTM-only baseline indicate use of visual information for some tasks (e.g., better than LSTM on many query and existence questions). However analyses show limited ability to learn true relational spatial semantics: models often rely on absolute image positions and fail on questions requiring reasoning over multiple spatial relations or same-attribute comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported relative performance: CNN+LSTM typically outperforms LSTM-only and Q-type on many tasks but underperforms attention-enhanced variants; for counting and integer comparison it performs near chance for some subtasks (paper gives aggregated numbers showing modest gains over baselines but specific per-task numbers vary in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles with tasks requiring short-term memory (attribute comparison), long reasoning chains, and learning true semantics of spatial relations; limited by pooled global CNN features (2048-D) without spatial locality in this variant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to CNN+LSTM+SA (spatial attention), CNN+LSTM typically underperforms; compared to LSTM-only it often improves but still fails on relational and compositional tasks highlighted by CLEVR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5063.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5063.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN+LSTM+SA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CNN+LSTM with soft spatial attention (stacked attention style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An image-question attention model that uses spatial attention over convolutional feature maps conditioned on the question, enabling the model to focus on image regions relevant to subparts of the question.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CNN+LSTM+SA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Image features extracted from conv4 of ResNet-101 producing 14×14×1024 feature maps; question encoded by LSTM. One or more rounds of soft spatial attention (question-guided) are applied to produce attended visual features which are combined and fed to an MLP for answer prediction. Training used Adam; conv features were not finetuned.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CLEVR visual reasoning questions (spatial reasoning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>See above: tasks test spatial relations and compositional reasoning on 3D-arranged synthetic scenes with annotated object attributes and scene graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Question-conditioned soft spatial attention over convolutional feature map (iterative attention rounds) to localize objects and attributes required to answer questions; effectively an attention-based iterative reasoning mechanism but without explicit symbolic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>CNN+LSTM+SA substantially outperforms other models on attribute-query tasks and shows ability to localize target objects (e.g., much higher accuracy on query-attribute questions). However analyses show that despite attention, it often relies on absolute image positions rather than learning true relational semantics: performance drops significantly when questions require non-absolute (relational) spatial reasoning and when absolute-position shortcuts are excluded. Also fails at attribute-comparison requiring attending to two objects simultaneously (accuracy ~50%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative reported numbers: query spatial vs same-attribute queries: 93% vs 78% (showing gap); counting accuracy ~52% (low absolute performance); attribute-comparison accuracy ~50% (chance); compositional generalization (Condition A->A vs A->B) on query-color for cubes/cylinders drops from 85% to 51%; query-material shows smaller drop (90% -> 81%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to learn full semantics of spatial relations (relies on absolute spatial cues), cannot reliably perform attribute comparisons requiring short-term memory (attending to multiple objects), brittle on long reasoning chains (effective question size correlates with error), and poor compositional generalization to novel attribute combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperforms CNN+LSTM and CNN+BoW on many query tasks and counting, showing benefits of attention; nonetheless, it still performs far below ideal for tasks requiring multi-step relational reasoning and memory, and falls short of human-level performance (human baseline collected but specific aggregated numbers not fully reported in the text).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5063.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5063.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN+BoW</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CNN image features with bag-of-words question encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple baseline that averages pretrained word vectors for the question (BoW) and concatenates with CNN image features for answer prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CNN+BoW</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Question encoded by averaging word vectors (Google-News word2vec embeddings, not fine-tuned). Image encoded by ResNet-101 pooled features (2048-D). Concatenate and pass to MLP classifier. Trained with Adam; used as a strong but simple baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CLEVR visual reasoning questions (spatial reasoning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>CLEVR diagnostic VQA tasks requiring spatial and compositional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Shallow language encoding (BoW) combined with global CNN visual features; no sequence modeling, no attention or explicit relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Performs poorly on tasks that require discriminating multiple object sets described by phrases (e.g., integer comparison), because BoW mixes the describing words and loses structure — evidence that structured question encoding matters for relational/spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In integer-comparison tasks CNN+BoW performs no better than chance (paper states BoW mixes words making discrimination impossible), and generally underperforms LSTM and attention models on relational questions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>BoW loses word order and structure critical for multi-object references and relational queries; fails on tasks requiring distinguishing two described sets (integer comparison) and longer compositional questions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared unfavorably to LSTM-based encoders and attention models; serves as evidence that richer question encoders are needed for spatial/compositional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5063.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5063.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN+LSTM+MCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CNN+LSTM with compact multimodal bilinear pooling (MCB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal pooling approach that combines CNN visual features and LSTM question features using compact bilinear pooling to better fuse modalities before answer prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CNN+LSTM+MCB</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Images encoded with ResNet-101 pooled features (2048-D), questions encoded by LSTM; features combined using compact multimodal bilinear pooling (MCB) and fed to an MLP classifier. Implementational details follow cited MCB work; features were not finetuned.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CLEVR visual reasoning questions (spatial reasoning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>As above: synthetic scenes and compositional questions requiring spatial relations and attribute reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Improved multimodal feature fusion (bilinear pooling) intended to capture richer interactions between visual and textual features than simple concatenation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>MCB-based fusion improves some question types in standard VQA benchmarks in prior work, but on CLEVR the paper reports that CNN+LSTM+MCB performs on par with CNN+LSTM for counting and other tasks, indicating that better fusion alone does not solve relational/spatial reasoning deficits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper states CNN+LSTM(+MCB) performs on par with LSTM for counting (no strong gains) and underperforms attention variants; no exhaustive per-task numeric table is given in the text beyond plotted figures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>MCB fusion does not address the core limitations in relational reasoning, short-term memory, or learning spatial relation semantics highlighted by CLEVR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to attention-enabled models (CNN+LSTM+SA) which outperform it on many spatial and attribute tasks; MCB gives little advantage for the compositional and relational challenges of CLEVR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5063.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5063.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic module networks / Neural module networks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Module Networks / Neural Module Networks (compositional neural modules)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that composes small differentiable neural modules into a custom network layout per question, aligning network architecture to the compositional structure of the question.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dynamic Module Networks / Neural Module Networks</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frameworks that parse a question to produce a layout of neural modules (e.g., 'find', 'filter', 'and', 'describe') and dynamically assemble them into task-specific networks; prior work provides module designs and layout heuristics. In CLEVR experiments the authors attempted to apply dynamic module networks but found parsing/layout heuristics tuned for VQA did not generalize to CLEVR's longer, more complex questions.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CLEVR visual reasoning questions (compositional spatial reasoning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>CLEVR's synthetic, functionally-structured questions (represented as functional programs) that require compositional reasoning over scene graphs, including spatial relations and same-attribute relations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Assembles differentiable modules into a question-specific neural architecture using dependency parsing and heuristic layout fragment construction, then ranks candidate layouts with an MLP; aims to mirror the functional-program decomposition of CLEVR questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>The method is explicitly designed for compositional reasoning and would map to CLEVR's functional programs, but empirical evidence in this paper shows limitations: on CLEVR a large fraction of questions (28.9% on a 10k sample) could not be parsed into fragments and fell back to a default architecture, indicating the approach failed to robustly capture CLEVR's compositional spatial questions out-of-the-box.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Operational statistic reported: dynamic module networks used default fallback architecture for 28.9% of random CLEVR questions (vs 7.8% on sampled VQA questions), indicating parsing/layout failures; no aggregated accuracy numbers are reported for successful layouts in CLEVR in this paper because the method did not work out-of-the-box.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Parsing heuristics for layout generation did not generalize to CLEVR's longer, more complex and diverse functional programs; high fallback rate to default networks; indicates brittle NLP-to-layout heuristics and the need for more robust parsing or learned layout generation for complex spatial/compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively to other models: whereas attention models provided some benefit, module-based approaches require robust parsing to realize their potential; the paper suggests module-network methods are promising but that existing parsing heuristics must be improved for CLEVR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to compose neural networks for question answering <em>(Rating: 2)</em></li>
                <li>Neural module networks <em>(Rating: 2)</em></li>
                <li>Stacked attention networks for image question answering <em>(Rating: 2)</em></li>
                <li>Dynamic memory networks for visual and textual question answering <em>(Rating: 2)</em></li>
                <li>Multimodal compact bilinear pooling for visual question answering and visual grounding <em>(Rating: 2)</em></li>
                <li>VQA: Visual question answering <em>(Rating: 1)</em></li>
                <li>Are you talking to a machine? Dataset and methods for multilingual image question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5063",
    "paper_id": "paper-03eb382e04cca8cca743f7799070869954f1402a",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "LSTM (question-only)",
            "name_full": "LSTM-based question encoder (question-only baseline)",
            "brief_description": "A recurrent neural network (LSTM) that encodes the question text and predicts answers without using image input; used as a bias-only baseline to measure question-conditional biases and short-term memory capabilities.",
            "citation_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
            "mention_or_use": "use",
            "model_name": "LSTM (question-only)",
            "model_description": "Word embeddings followed by a word-level LSTM (one or two layers, 512 or 1024 units per layer as tuned in experiments). The final LSTM hidden state is passed to a multi-layer perceptron classifier predicting answers. No visual input is provided (models only the question distribution). Trained with Adam; no parameter counts reported in the paper.",
            "puzzle_name": "CLEVR visual reasoning questions (diagnostic spatial reasoning benchmark)",
            "puzzle_description": "Synthetic visual question-answering tasks requiring attribute identification, counting, attribute comparison, and spatial relations (left/right/in front/behind). Questions are represented as functional programs executed on scene graphs; tasks test spatial reasoning, short-term memory (comparing attributes), and compositional generalization.",
            "mechanism_or_strategy": "Language-only statistical modeling of question-conditioned answer distributions using LSTM encoding and MLP classification; serves as a baseline that can exploit question-conditional biases rather than visual/spatial reasoning.",
            "evidence_of_spatial_reasoning": "By design LSTM has no access to image information and therefore cannot perform visual/spatial reasoning; used to reveal question-conditional biases (e.g., LSTM achieves ~60% on some existence questions due to bias). The paper uses LSTM performance as negative evidence that a model is not using spatial reasoning when it outperforms or matches LSTM.",
            "performance_metrics": "Reported accuracies (examples): for attribute-query types LSTM attains chance-like rates consistent with the attribute cardinalities (e.g., ~12.5% for color queries reflecting 8 colors); for counting LSTM ~42%; for existence questions ~60% (indicating bias); for attribute-comparison tasks LSTM ~50% (chance for yes/no). Exact per-question-type values reported in the analysis sections.",
            "limitations_or_failure_cases": "Cannot use image information or perform true spatial reasoning; it captures question-conditional biases and thus performs poorly on tasks requiring vision or short-term memory of visual attributes; its performance can misleadingly be high when dataset biases exist.",
            "comparison_baseline": "Used as a baseline against image-aware models (CNN+LSTM, attention models). LSTM exposes dataset biases and provides a lower-bound that image-enabled models must beat to demonstrate use of visual/spatial reasoning.",
            "uuid": "e5063.0",
            "source_info": {
                "paper_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "CNN+LSTM",
            "name_full": "CNN image features with LSTM question encoder and MLP answer predictor",
            "brief_description": "A standard multimodal VQA model that encodes images with a pretrained CNN and questions with an LSTM, concatenates features, and predicts answers with an MLP classifier.",
            "citation_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
            "mention_or_use": "use",
            "model_name": "CNN+LSTM",
            "model_description": "Image features extracted from a ResNet-101 pretrained on ImageNet (final average pooling layer giving 2048-D features, images resized to 224×224). Questions encoded by LSTM (512 or 1024 units). Image and question features concatenated and passed to an MLP (ReLU, dropout) to predict answer distribution. Trained with Adam; networks and hyperparameters tuned on validation set. No total parameter counts provided.",
            "puzzle_name": "CLEVR visual reasoning questions (spatial reasoning benchmark)",
            "puzzle_description": "Same as above: visual questions that require object recognition, spatial relation reasoning (left/right/in front/behind defined relative to camera projection), counting, attribute comparison, and compositional chains of reasoning.",
            "mechanism_or_strategy": "Early-fusion multimodal approach: CNN visual features concatenated with LSTM question representation, followed by MLP answer classification. No explicit spatial attention or memory mechanism in this variant.",
            "evidence_of_spatial_reasoning": "Performance differences vs. LSTM-only baseline indicate use of visual information for some tasks (e.g., better than LSTM on many query and existence questions). However analyses show limited ability to learn true relational spatial semantics: models often rely on absolute image positions and fail on questions requiring reasoning over multiple spatial relations or same-attribute comparisons.",
            "performance_metrics": "Reported relative performance: CNN+LSTM typically outperforms LSTM-only and Q-type on many tasks but underperforms attention-enhanced variants; for counting and integer comparison it performs near chance for some subtasks (paper gives aggregated numbers showing modest gains over baselines but specific per-task numbers vary in figures).",
            "limitations_or_failure_cases": "Struggles with tasks requiring short-term memory (attribute comparison), long reasoning chains, and learning true semantics of spatial relations; limited by pooled global CNN features (2048-D) without spatial locality in this variant.",
            "comparison_baseline": "Compared to CNN+LSTM+SA (spatial attention), CNN+LSTM typically underperforms; compared to LSTM-only it often improves but still fails on relational and compositional tasks highlighted by CLEVR.",
            "uuid": "e5063.1",
            "source_info": {
                "paper_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "CNN+LSTM+SA",
            "name_full": "CNN+LSTM with soft spatial attention (stacked attention style)",
            "brief_description": "An image-question attention model that uses spatial attention over convolutional feature maps conditioned on the question, enabling the model to focus on image regions relevant to subparts of the question.",
            "citation_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
            "mention_or_use": "use",
            "model_name": "CNN+LSTM+SA",
            "model_description": "Image features extracted from conv4 of ResNet-101 producing 14×14×1024 feature maps; question encoded by LSTM. One or more rounds of soft spatial attention (question-guided) are applied to produce attended visual features which are combined and fed to an MLP for answer prediction. Training used Adam; conv features were not finetuned.",
            "puzzle_name": "CLEVR visual reasoning questions (spatial reasoning benchmark)",
            "puzzle_description": "See above: tasks test spatial relations and compositional reasoning on 3D-arranged synthetic scenes with annotated object attributes and scene graphs.",
            "mechanism_or_strategy": "Question-conditioned soft spatial attention over convolutional feature map (iterative attention rounds) to localize objects and attributes required to answer questions; effectively an attention-based iterative reasoning mechanism but without explicit symbolic memory.",
            "evidence_of_spatial_reasoning": "CNN+LSTM+SA substantially outperforms other models on attribute-query tasks and shows ability to localize target objects (e.g., much higher accuracy on query-attribute questions). However analyses show that despite attention, it often relies on absolute image positions rather than learning true relational semantics: performance drops significantly when questions require non-absolute (relational) spatial reasoning and when absolute-position shortcuts are excluded. Also fails at attribute-comparison requiring attending to two objects simultaneously (accuracy ~50%).",
            "performance_metrics": "Representative reported numbers: query spatial vs same-attribute queries: 93% vs 78% (showing gap); counting accuracy ~52% (low absolute performance); attribute-comparison accuracy ~50% (chance); compositional generalization (Condition A-&gt;A vs A-&gt;B) on query-color for cubes/cylinders drops from 85% to 51%; query-material shows smaller drop (90% -&gt; 81%).",
            "limitations_or_failure_cases": "Fails to learn full semantics of spatial relations (relies on absolute spatial cues), cannot reliably perform attribute comparisons requiring short-term memory (attending to multiple objects), brittle on long reasoning chains (effective question size correlates with error), and poor compositional generalization to novel attribute combinations.",
            "comparison_baseline": "Outperforms CNN+LSTM and CNN+BoW on many query tasks and counting, showing benefits of attention; nonetheless, it still performs far below ideal for tasks requiring multi-step relational reasoning and memory, and falls short of human-level performance (human baseline collected but specific aggregated numbers not fully reported in the text).",
            "uuid": "e5063.2",
            "source_info": {
                "paper_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "CNN+BoW",
            "name_full": "CNN image features with bag-of-words question encoding",
            "brief_description": "A simple baseline that averages pretrained word vectors for the question (BoW) and concatenates with CNN image features for answer prediction.",
            "citation_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
            "mention_or_use": "use",
            "model_name": "CNN+BoW",
            "model_description": "Question encoded by averaging word vectors (Google-News word2vec embeddings, not fine-tuned). Image encoded by ResNet-101 pooled features (2048-D). Concatenate and pass to MLP classifier. Trained with Adam; used as a strong but simple baseline.",
            "puzzle_name": "CLEVR visual reasoning questions (spatial reasoning benchmark)",
            "puzzle_description": "CLEVR diagnostic VQA tasks requiring spatial and compositional reasoning.",
            "mechanism_or_strategy": "Shallow language encoding (BoW) combined with global CNN visual features; no sequence modeling, no attention or explicit relational reasoning.",
            "evidence_of_spatial_reasoning": "Performs poorly on tasks that require discriminating multiple object sets described by phrases (e.g., integer comparison), because BoW mixes the describing words and loses structure — evidence that structured question encoding matters for relational/spatial tasks.",
            "performance_metrics": "In integer-comparison tasks CNN+BoW performs no better than chance (paper states BoW mixes words making discrimination impossible), and generally underperforms LSTM and attention models on relational questions.",
            "limitations_or_failure_cases": "BoW loses word order and structure critical for multi-object references and relational queries; fails on tasks requiring distinguishing two described sets (integer comparison) and longer compositional questions.",
            "comparison_baseline": "Compared unfavorably to LSTM-based encoders and attention models; serves as evidence that richer question encoders are needed for spatial/compositional reasoning.",
            "uuid": "e5063.3",
            "source_info": {
                "paper_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "CNN+LSTM+MCB",
            "name_full": "CNN+LSTM with compact multimodal bilinear pooling (MCB)",
            "brief_description": "A multimodal pooling approach that combines CNN visual features and LSTM question features using compact bilinear pooling to better fuse modalities before answer prediction.",
            "citation_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
            "mention_or_use": "use",
            "model_name": "CNN+LSTM+MCB",
            "model_description": "Images encoded with ResNet-101 pooled features (2048-D), questions encoded by LSTM; features combined using compact multimodal bilinear pooling (MCB) and fed to an MLP classifier. Implementational details follow cited MCB work; features were not finetuned.",
            "puzzle_name": "CLEVR visual reasoning questions (spatial reasoning benchmark)",
            "puzzle_description": "As above: synthetic scenes and compositional questions requiring spatial relations and attribute reasoning.",
            "mechanism_or_strategy": "Improved multimodal feature fusion (bilinear pooling) intended to capture richer interactions between visual and textual features than simple concatenation.",
            "evidence_of_spatial_reasoning": "MCB-based fusion improves some question types in standard VQA benchmarks in prior work, but on CLEVR the paper reports that CNN+LSTM+MCB performs on par with CNN+LSTM for counting and other tasks, indicating that better fusion alone does not solve relational/spatial reasoning deficits.",
            "performance_metrics": "Paper states CNN+LSTM(+MCB) performs on par with LSTM for counting (no strong gains) and underperforms attention variants; no exhaustive per-task numeric table is given in the text beyond plotted figures.",
            "limitations_or_failure_cases": "MCB fusion does not address the core limitations in relational reasoning, short-term memory, or learning spatial relation semantics highlighted by CLEVR.",
            "comparison_baseline": "Compared to attention-enabled models (CNN+LSTM+SA) which outperform it on many spatial and attribute tasks; MCB gives little advantage for the compositional and relational challenges of CLEVR.",
            "uuid": "e5063.4",
            "source_info": {
                "paper_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "Dynamic module networks / Neural module networks",
            "name_full": "Dynamic Module Networks / Neural Module Networks (compositional neural modules)",
            "brief_description": "An approach that composes small differentiable neural modules into a custom network layout per question, aligning network architecture to the compositional structure of the question.",
            "citation_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
            "mention_or_use": "use",
            "model_name": "Dynamic Module Networks / Neural Module Networks",
            "model_description": "Frameworks that parse a question to produce a layout of neural modules (e.g., 'find', 'filter', 'and', 'describe') and dynamically assemble them into task-specific networks; prior work provides module designs and layout heuristics. In CLEVR experiments the authors attempted to apply dynamic module networks but found parsing/layout heuristics tuned for VQA did not generalize to CLEVR's longer, more complex questions.",
            "puzzle_name": "CLEVR visual reasoning questions (compositional spatial reasoning benchmark)",
            "puzzle_description": "CLEVR's synthetic, functionally-structured questions (represented as functional programs) that require compositional reasoning over scene graphs, including spatial relations and same-attribute relations.",
            "mechanism_or_strategy": "Assembles differentiable modules into a question-specific neural architecture using dependency parsing and heuristic layout fragment construction, then ranks candidate layouts with an MLP; aims to mirror the functional-program decomposition of CLEVR questions.",
            "evidence_of_spatial_reasoning": "The method is explicitly designed for compositional reasoning and would map to CLEVR's functional programs, but empirical evidence in this paper shows limitations: on CLEVR a large fraction of questions (28.9% on a 10k sample) could not be parsed into fragments and fell back to a default architecture, indicating the approach failed to robustly capture CLEVR's compositional spatial questions out-of-the-box.",
            "performance_metrics": "Operational statistic reported: dynamic module networks used default fallback architecture for 28.9% of random CLEVR questions (vs 7.8% on sampled VQA questions), indicating parsing/layout failures; no aggregated accuracy numbers are reported for successful layouts in CLEVR in this paper because the method did not work out-of-the-box.",
            "limitations_or_failure_cases": "Parsing heuristics for layout generation did not generalize to CLEVR's longer, more complex and diverse functional programs; high fallback rate to default networks; indicates brittle NLP-to-layout heuristics and the need for more robust parsing or learned layout generation for complex spatial/compositional tasks.",
            "comparison_baseline": "Compared qualitatively to other models: whereas attention models provided some benefit, module-based approaches require robust parsing to realize their potential; the paper suggests module-network methods are promising but that existing parsing heuristics must be improved for CLEVR.",
            "uuid": "e5063.5",
            "source_info": {
                "paper_title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                "publication_date_yy_mm": "2016-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to compose neural networks for question answering",
            "rating": 2
        },
        {
            "paper_title": "Neural module networks",
            "rating": 2
        },
        {
            "paper_title": "Stacked attention networks for image question answering",
            "rating": 2
        },
        {
            "paper_title": "Dynamic memory networks for visual and textual question answering",
            "rating": 2
        },
        {
            "paper_title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
            "rating": 2
        },
        {
            "paper_title": "VQA: Visual question answering",
            "rating": 1
        },
        {
            "paper_title": "Are you talking to a machine? Dataset and methods for multilingual image question answering",
            "rating": 1
        }
    ],
    "cost": 0.015168,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</h1>
<p>Justin Johnson ${ }^{1,2 *}$<br>Li Fei-Fei ${ }^{1}$<br>Bharath Hariharan ${ }^{2}$<br>C. Lawrence Zitnick ${ }^{2}$<br>Laurens van der Maaten ${ }^{2}$<br>Ross Girshick ${ }^{2}$<br>${ }^{1}$ Stanford University<br>${ }^{2}$ Facebook AI Research</p>
<h4>Abstract</h4>
<p>When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.</p>
<h2>1. Introduction</h2>
<p>A long-standing goal of artificial intelligence research is to develop systems that can reason and answer questions about visual information. Recently, several datasets have been introduced to study this problem [4, 10, 21, 26, 32, 46, 49]. Each of these Visual Question Answering (VQA) datasets contains challenging natural language questions about images. Correctly answering these questions requires perceptual abilities such as recognizing objects, attributes, and spatial relationships as well as higher-level skills such as counting, performing logical inference, making comparisons, or leveraging commonsense world knowledge [31]. Numerous methods have attacked these problems [2, 3, 9, 24, 44], but many show only marginal improvements over strong baselines [4, 16, 48]. Unfortunately, our ability to understand the limitations of these methods is impeded by the inherent complexity of the VQA task. Are methods hampered by failures in recognition, poor reasoning, lack of commonsense knowledge, or something else?</p>
<p>The difficulty of understanding a system's competences</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Q: Are there an equal number of large things and metal spheres? Q: What size is the cylinder that is left of the brown metal thing that is left of the big sphere? Q: There is a sphere with the same size as the metal cube; is it made of the same material as the small red sphere? Q: How many objects are either small cylinders or metal things? Figure 1. A sample image and questions from CLEVR. Questions test aspects of visual reasoning such as attribute identification, counting, comparison, multiple attention, and logical operations.
is exemplified by Clever Hans, a 1900s era horse who appeared to be able to answer arithmetic questions. Careful observation revealed that Hans was correctly "answering" questions by reacting to cues read off his human observers [30]. Statistical learning systems, like those used for VQA, may develop similar "cheating" approaches to superficially "solve" tasks without learning the underlying reasoning processes [35, 36]. For instance, a statistical learner may correctly answer the question "What covers the ground?" not because it understands the scene but because biased datasets often ask questions about the ground when it is snow-covered [1, 47]. How can we determine whether a system is capable of sophisticated reasoning and not just exploiting biases of the world, similar to Clever Hans?</p>
<p>In this paper we propose a diagnostic dataset for studying the ability of VQA systems to perform visual reasoning. We refer to this dataset as the Compositional Language and Elementary Visual Reasoning diagnostics dataset (CLEVR; pronounced as clever in homage to Hans). CLEVR contains 100k rendered images and about one million automaticallygenerated questions, of which 853 k are unique. It has chal-</p>
<p>lenging images and questions that test visual reasoning abilities such as counting, comparing, logical reasoning, and storing information in memory, as illustrated in Figure 1.</p>
<p>We designed CLEVR with the explicit goal of enabling detailed analysis of visual reasoning. Our images depict simple 3D shapes; this simplifies recognition and allows us to focus on reasoning skills. We ensure that the information in each image is complete and exclusive so that external information sources, such as commonsense knowledge, cannot increase the chance of correctly answering questions. We minimize question-conditional bias via rejection sampling within families of related questions, and avoid degenerate questions that are seemingly complex but contain simple shortcuts to the correct answer. Finally, we use structured ground-truth representations for both images and questions: images are annotated with ground-truth object positions and attributes, and questions are represented as functional programs that can be executed to answer the question (see Section 3). These representations facilitate indepth analyses not possible with traditional VQA datasets.</p>
<p>These design choices also mean that while images in CLEVR may be visually simple, its questions are complex and require a range of reasoning skills. For instance, factorized representations may be required to generalize to unseen combinations of objects and attributes. Tasks such as counting or comparing may require short-term memory [15] or attending to specific objects [24, 44]. Questions that combine multiple subtasks in diverse ways may require compositional systems $[2,3]$ to answer.</p>
<p>We use CLEVR to analyze a suite of VQA models and discover weaknesses that are not widely known. For example, we find that current state-of-the-art VQA models struggle on tasks requiring short term memory, such as comparing the attributes of objects, or compositional reasoning, such as recognizing novel attribute combinations. These observations point to novel avenues for further research.</p>
<p>Finally, we stress that accuracy on CLEVR is not an end goal in itself: a hand-crafted system with explicit knowledge of the CLEVR universe might work well, but will not generalize to real-world settings. Therefore CLEVR should be used in conjunction with other VQA datasets in order to study the reasoning abilities of general VQA systems.</p>
<p>The CLEVR dataset, as well as code for generating new images and questions, will be made publicly available.</p>
<h2>2. Related Work</h2>
<p>In recent years, a range of benchmarks for visual understanding have been proposed, including datasets for image captioning [7, 8, 23, 45], referring to objects [19], relational graph prediction [21], and visual Turing tests [12, 27]. CLEVR, our diagnostic dataset, is most closely related to benchmarks for visual question answering [4, 10, 21, 26, $32,37,46,49]$, as it involves answering natural-language
questions about images. The two main differences between CLEVR and other VQA datasets are that: (1) CLEVR minimizes biases of prior VQA datasets that can be used by learning systems to answer questions correctly without visual reasoning and (2) CLEVR's synthetic nature and detailed annotations facilitate in-depth analyses of reasoning abilities that are impossible with existing datasets.</p>
<p>Prior work has attempted to mitigate biases in VQA datasets in simple cases such as yes/no questions [12, 47], but it is difficult to apply such bias-reduction approaches to more complex questions without a high-quality semantic representation of both questions and answers. In CLEVR, this semantic representation is provided by the functional program underlying each image-question pair, and biases are largely eliminated via sampling. Winograd schemas [22] are another approach for controlling bias in question answering: these questions are carefully designed to be ambiguous based on syntax alone and require commonsense knowledge. Unfortunately this approach does not scale gracefully: the first phase of the 2016 Winograd Schema Challenge consists of just 60 hand-designed questions. CLEVR is also related to the bAbI question answering tasks [38] in that it aims to diagnose a set of clearly defined competences of a system, but CLEVR focuses on visual reasoning whereas bAbI is purely textual.</p>
<p>We are also not the first to consider synthetic data for studying (visual) reasoning. SHRDLU performed simple, interactive visual reasoning with the goal of moving specific objects in the visual scene [40]; this study was one of the first to demonstrate the brittleness of manually programmed semantic understanding. The pioneering DAQUAR dataset [28] contains both synthetic and human-written questions, but they only generate 420 synthetic questions using eight text templates. VQA [4] contains 150,000 natural-language questions about abstract scenes [50], but these questions do not control for questionconditional bias and are not equipped with functional program representations. CLEVR is similar in spirit to the SHAPES dataset [3], but it is more complex and varied both in terms of visual content and question variety and complexity: SHAPES contains 15,616 total questions with just 244 unique questions while CLEVR contains nearly a million questions of which 853,554 are unique.</p>
<h2>3. The CLEVR Diagnostic Dataset</h2>
<p>CLEVR provides a dataset that requires complex reasoning to solve and that can be used to conduct rich diagnostics to better understand the visual reasoning capabilities of VQA systems. This requires tight control over the dataset, which we achieve by using synthetic images and automatically generated questions. The images have associated ground-truth object locations and attributes, and the questions have an associated machine-readable form. These</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. A field guide to the CLEVR universe. <strong>Left:</strong> Shapes, attributes, and spatial relationships. <strong>Center:</strong> Examples of questions and their associated functional programs. <strong>Right:</strong> Catalog of basic functions used to build questions. See Section 3 for details.</p>
<p>ground-truth structures allow us to analyze models based on, for example: question type, question topology (chain <em>vs</em>. tree), question length, and various forms of relationships between objects. Figure 2 gives a brief overview of the main components of CLEVR, which we describe in detail below.</p>
<p><strong>Objects and relationships.</strong> The CLEVR universe contains three object shapes (cube, sphere, and cylinder) that come in two absolute sizes (small and large), two materials (shiny "metal" and matte "rubber"), and eight colors. Objects are spatially related via four relationships: "left", "right", "behind", and "in front". The semantics of these prepositions are complex and depend not only on relative object positions but also on camera viewpoint and context. We found that generating questions that invoke spatial relationships with semantic accord was difficult. Instead we rely on a simple and unambiguous definition: projecting the camera viewpoint vector onto the ground plane defines the "behind" vector, and one object is behind another if its ground-plane position is further along the "behind" vector. The other relationships are similarly defined. Figure 2 (left) illustrates the objects, attributes, and spatial relationships in CLEVR. The CLEVR universe also includes one non-spatial relationship type that we refer to as the <em>same-attribute relation</em>. Two objects are in this relationship if they have equal attribute values for a specified attribute.</p>
<p><strong>Scene representation.</strong> Scenes are represented as collections of objects annotated with shape, size, color, material, and position on the ground-plane. A scene can also be represented by a <em>scene graph</em> [17, 21], where nodes are objects annotated with attributes and edges connect spatially related objects. A scene graph contains all ground-truth information for an image and could be used to replace the vision component of a VQA system with <em>perfect sight</em>.</p>
<p><strong>Image generation.</strong> CLEVR images are generated by randomly sampling a scene graph and rendering it using Blender [6]. Every scene contains between three and ten objects with random shapes, sizes, materials, colors, and positions. When placing objects we ensure that no objects intersect, that all objects are at least partially visible, and that there are small horizontal and vertical margins between the image-plane centers of each pair of objects; this helps reduce ambiguity in spatial relationships. In each image the positions of the lights and camera are randomly jittered.</p>
<p><strong>Question representation.</strong> Each question in CLEVR is associated with a <em>functional program</em> that can be <em>executed</em> on an image's scene graph, yielding the answer to the question. Functional programs are built from simple basic functions that correspond to elementary operations of visual reasoning such as <em>querying</em> object attributes, <em>counting</em> sets of objects, or <em>comparing</em> values. As shown in Figure 2, complex questions can be represented by compositions of these simple building blocks. Full details about each basic function can be found in the supplementary material.</p>
<p>As we will see in Section 4, representing questions as functional programs enables rich analysis that would be impossible with natural-language questions. A question's functional program tells us exactly which reasoning abilities are required to solve it, allowing us to compare performance on questions requiring different types of reasoning.</p>
<p>We categorize questions by question type, defined by the outermost function in the question's program; for example the questions in Figure 2 have types query-color and exist. Figure 3 shows the number of questions of each type.</p>
<p>Question families. We must overcome several key challenges to generate a VQA dataset using functional programs. Functional building blocks can be used to construct an infinite number of possible functional programs, and we must decide which program structures to consider. We also need a method for converting functional programs to natural language in a way that minimizes question-conditional bias. We solve these problems using question families.</p>
<p>A question family contains a template for constructing functional programs and several text templates providing multiple ways of expressing these programs in natural language. For example, the question "How many red things are there?" can be formed by instantiating the text template "How many $<C><M>$ things are there?", binding the parameters $\langle C\rangle$ and $\langle M\rangle$ (with types "color" and "material") to the values red and nil. The functional program count(filter_color(red, scene())) for this question can be formed by instantiating the associated program template
count(filter_color( $\langle C\rangle$, filter_material( $\langle M\rangle$, scene( $)$ ))) with the same values, using the convention that functions taking a nil input are removed after instantiation.</p>
<p>CLEVR contains a total of 90 question families, each with a single program template and an average of four text templates. Text templates were generated by manually writing one or two templates per family and then crowdsourcing question rewrites. To further increase language diversity we use a set of synonyms for each shape, color, and material. With up to 19 parameters per template, a small number of families can generate a huge number of unique questions; Figure 3 shows that of the nearly one million questions in CLEVR, more than 853 k are unique. CLEVR can easily be extended by adding new question families.</p>
<p>Question generation. Generating a question for an image is conceptually simple: we choose a question family, select values for each of its template parameters, execute the resulting program on the image's scene graph to find the answer, and use one of the text templates from the question family to generate the final natural-language question.</p>
<p>However, many combinations of values give rise to questions which are either ill-posed or degenerate. The question "What color is the cube to the right of the sphere?" would be ill-posed if there were many cubes right of the sphere, or degenerate if there were only one cube in the scene since the reference to the sphere would then be unnecessary. Avoiding such ill-posed and degenerate questions is critical to ensure the correctness and complexity of our questions.</p>
<p>A naïve solution is to randomly sample combinations of values and reject those which lead to ill-posed or degenerate
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Top: Statistics for CLEVR; the majority of questions are unique and few questions from the val and test sets appear in the training set. Bottom left: Comparison of question lengths for different VQA datasets; CLEVR questions are generally much longer. Bottom right: Distribution of question types in CLEVR.
questions. However, the number of possible configurations for a question family is exponential in its number of parameters, and most of them are undesirable. This makes bruteforce search intractable for our complex question families.</p>
<p>Instead, we employ a depth-first search to find valid values for instantiating question families. At each step of the search, we use ground-truth scene information to prune large swaths of the search space which are guaranteed to produce undesirable questions; for example we need not entertain questions of the form "What color is the $<S>$ to the $<R>$ of the sphere" for scenes that do not contain spheres.</p>
<p>Finally, we use rejection sampling to produce an approximately uniform answer distribution for each question family; this helps minimize question-conditional bias since all questions from the same family share linguistic structure.</p>
<h2>4. VQA Systems on CLEVR</h2>
<h3>4.1. Models</h3>
<p>VQA models typically represent images with features from pretrained CNNs and use word embeddings or recurrent networks to represent questions and/or answers. Models may train recurrent networks for answer generation [10, 28, 41], multiclass classifiers over common answers [4, 24, 25, 32, 48, 49], or binary classifiers on image-question-answer triples [9, 16, 33]. Many methods incorporate attention over the image [9, 33, 44, 49, 43] or question [24]. Some methods incorporate memory [42] or dynamic network architectures $[2,3]$.</p>
<p>Experimenting with all methods is logistically challenging, so we reproduced a representative subset of methods: baselines that do not look at the image (Q-type mode, LSTM), a simple baseline (CNN+BoW) that performs near state-of-the-art [16, 48], and more sophisticated methods</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Accuracy per question type of the six VQA methods on the CLEVR dataset (higher is better). Figure best viewed in color.</p>
<p>using recurrent networks (CNN+LSTM), sophisticated feature pooling (CNN+LSTM+MCB), and spatial attention (CNN+LSTM+SA).^{1} These are described in detail below.</p>
<p><strong>Q-type mode:</strong> Similar to the "per Q-type prior" method in [4], this baseline predicts the most frequent training-set answer for each question's type.</p>
<p><strong>LSTM:</strong> Similar to "LSTM Q" in [4], the question is processed with learned word embeddings followed by a word-level LSTM [15]. The final LSTM hidden state is passed to a multi-layer perceptron (MLP) that predicts a distribution over answers. This method uses no image information so it can only model question-conditional bias.</p>
<p><strong>CNN+BoW:</strong> Following [48], the question is encoded by averaging word vectors for each word in the question and the image is encoded using features from a convolutional network (CNN). The question and image features are concatenated and passed to a MLP which predicts a distribution over answers. We use word vectors trained on the Google-News corpus [29]; these are not fine-tuned during training.</p>
<p><strong>CNN+LSTM:</strong> As above, images and questions are encoded using CNN features and final LSTM hidden states, respectively. These features are concatenated and passed to an MLP that predicts an answer distribution.</p>
<p><strong>CNN+LSTM+MCB:</strong> Images and questions are encoded as above, but instead of concatenation, their features are pooled using compact multimodal pooling (MCB) [9, 11].</p>
<p><strong>CNN+LSTM+SA:</strong> Again, the question and image are encoded using a CNN and LSTM, respectively. Following [44], these representations are combined using one or more rounds of soft spatial attention and the final answer distribution is predicted with an MLP.</p>
<p><strong>Human:</strong> We used Mechanical Turk to collect human responses for 5500 random questions from the test set, taking a majority vote among three workers for each question.</p>
<p><strong>Implementation details.</strong> Our CNNs are ResNet-101 models pretrained on ImageNet [14] that are not finetuned; images are resized to 224×224 prior to feature extraction.</p>
<p>CNN+LSTM+SA extracts features from the last layer of the conv4 stage, giving 14×14×1024-dimensional features. All other methods extract features from the final average pooling layer, giving 2048-dimensional features. LSTMs use one or two layers with 512 or 1024 units per layer. MLPs use ReLU functions and dropout [34]; they have one or two hidden layers with between 1024 and 8192 units per layer. All models are trained using Adam [20].</p>
<p><strong>Experimental protocol.</strong> CLEVR is split into train, validation, and test sets (see Figure 3). We tuned hyperparameters (learning rate, dropout, word vector size, number and size of LSTM and MLP layers) independently per model based on the validation error. All experiments were designed on the validation set; after finalizing the design we ran each model once on the test set. <em>All experimental findings generalized from the validation set to the test set.</em></p>
<h3>4.2. Analysis by Question Type</h3>
<p>We can use the program representation of questions to analyze model performance on different forms of reasoning. We first evaluate performance on each question type, defined as the outermost function in the program. Figure 4 shows results and detailed findings are discussed below.</p>
<p><strong>Querying attributes:</strong> Query questions ask about an attribute of a particular object (<em>e.g. "What color is the thing right of the red sphere?"</em>). The CLEVR world has two sizes, eight colors, two materials, and three shapes. On questions asking about these different attributes, Q-type mode and LSTM obtain accuracies close to 50%, 12.5%, 50%, and 33.3% respectively, showing that the dataset has minimal question-conditional bias for these questions. CNN+LSTM+SA substantially outperforms all other models on these questions; its attention mechanism may help it focus on the target object and identify its attributes.</p>
<p><strong>Comparing attributes:</strong> Attribute comparison questions ask whether two objects have the same value for some attribute (<em>e.g. "Is the cube the same size as the sphere?"</em>). The only valid answers are "yes" and "no". Q-Type mode and LSTM achieve accuracies close to 50%, confirming there is no dataset bias for these questions. Unlike attribute-query</p>
<p>^{1}We performed initial experiments with dynamic module networks [2] but its parsing heuristics did not generalize to the complex questions in CLEVR so it did not work out-of-the-box; see supplementary material.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Accuracy on questions with a single spatial relationship vs. a single same-attribute relationship. For query and count questions, models generally perform worse on questions with sameattribute relationships. Results on exist questions are mixed.
questions, attribute-comparison questions require a limited form of memory: models must identify the attributes of two objects and keep them in memory to compare them. Interestingly, none of the models are able to do so: all models have an accuracy of approximately $50 \%$. This is also true for the CNN+LSTM+SA model, suggesting that its attention mechanism is not capable of attending to two objects at once to compare them. This illustrates how CLEVR can reveal limitations of models and motivate follow-up research, e.g., augmenting attention models with explicit memory.</p>
<p>Existence: Existence questions ask whether a certain type of object is present (e.g., "Are there any cubes to the right of the red thing?"). The $50 \%$ accuracy of QType mode shows that both answers are a priori equally likely, but the LSTM result of $60 \%$ does suggest a questionconditional bias. There may be correlations between question length and answer: questions with more filtering operations (e.g., "large red cube" vs. "red cube") may be more likely to have "no" as the answer. Such biases may be present even with uniform answer distributions per question family, since questions from the same family may have different numbers of filtering functions. CNN+LSTM(+SA) outperforms LSTM, but its performance is still quite low.</p>
<p>Counting: Counting questions ask for the number of objects fulfilling some conditions (e.g. "How many red cubes are there?"); valid answers range from zero to ten. Images have three and ten objects and counting questions refer to subsets of objects, so ensuring a uniform answer distribution is very challenging; our rejection sampler therefore pushes towards a uniform distribution for these questions rather than enforcing it as a hard constraint. This results in a question-conditional bias, reflected in the $35 \%$ and $42 \%$ accuracies achieved by Q-type mode and LSTM. $\mathrm{CNN}+\mathrm{LSTM}(+\mathrm{MCB})$ performs on par with LSTM, suggesting that CNN features contain little information relevant to counting. CNN+LSTM+SA performs slightly better, but at $52 \%$ its absolute performance is low.</p>
<p>Integer comparison: Integer comparison questions ask which of two object sets is larger (e.g. "Are there fewer cubes than red things?"); this requires counting, memory,
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Accuracy on questions with two spatial relationships, broken down by question topology: chain-structured questions vs. tree-structured questions joined with a logical AND operator.
and comparing integer quantities. The answer distribution is unbiased (see Q-Type mode) but a set's size may correlate with the length of its description, explaining the gap between LSTM and Q-type mode. CNN+BoW performs no better than chance: BoW mixes the words describing each set, making it impossible for the learner to discriminate between them. CNN+LSTM+SA outperforms LSTM on "less" and "more" questions, but no model outperforms LSTM on "equal" questions. Most models perform better on "less" than "more" due to asymmetric question families.</p>
<h3>4.3. Analysis by Relationship Type</h3>
<p>CLEVR questions contain two types of relationships: spatial and same-attribute (see Section 3). We can compare the relative difficulty of these two types by comparing model performance on questions with a single spatial relationship and questions with a single same-attribute relationship; results are shown in Figure 5. On query-attribute and counting questions we see that same-attribute questions are generally more difficult; the gap between CNN+LSTM+SA on spatial and same-relate query questions is particularly large ( $93 \%$ vs. $78 \%$ ). Same-attribute relationships may require a model to keep attributes of one object "in memory" for comparison, suggesting again that models augmented with explicit memory may perform better on these questions.</p>
<h3>4.4. Analysis by Question Topology</h3>
<p>We next evaluate model performance on different question topologies: chain-structured questions vs. treestructured questions with two branches joined by a logical AND (see Figure 2). In Figure 6, we compare performance on chain-structured questions with two spatial relationships vs. tree-structured questions with one relationship along each branch. On query questions, CNN+LSTM+SA shows a large gap between chain and tree questions ( $92 \%$ vs. $74 \%$ ); on count questions, CNN+LSTM+SA slightly outperforms LSTM on chain questions ( $55 \%$ vs. $49 \%$ ) but no method outperforms LSTM on tree questions. Tree questions may be more difficult since they require models to perform two subtasks in parallel before fusing their results.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Top: Many questions can be answered correctly without correctly solving all subtasks. For a given question and scene we can prune functions from the question's program to generate an effective question which is shorter but gives the same answer. Bottom: Accuracy on query questions vs. actual and effective question size. Accuracy decreases with effective question size but not with actual size. Shaded area shows a $95 \%$ confidence interval.</p>
<h3>4.5. Effect of Question Size</h3>
<p>Intuitively, longer questions should be harder since they involve more reasoning steps. We define a question's size to be the number of functions in its program, and in Figure 7 (bottom left) we show accuracy on query-attribute questions as a function of question size. ${ }^{2}$ Surprisingly accuracy appears unrelated to question size.</p>
<p>However, many questions can be correctly answered even when some subtasks are not solved correctly. For example, the question in Figure 7 (top) can be answered correctly without identifying the correct large blue cylinder, because all large objects left of a cylinder are cylinders.</p>
<p>To quantify this effect, we define the effective question of an image-question pair: we prune functions from the question's program to find the smallest program that, when executed on the scene graph for the question's image, gives the same answer as the original question. ${ }^{3}$ A question's effective size is the size of its effective question. Questions whose effective size is smaller than their actual size need not be degenerate. The question in Figure 7 is not degenerate because the entire question is needed to resolve its object references (there are two blue cylinders and two rubber cylinders), but it has a small effective size since it can be correctly answered without resolving those references.</p>
<p>In Figure 7 (bottom), we show accuracy on query questions as a function of effective question size. The error rate of all models increases with effective question size, suggesting that models struggle with long reasoning chains.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Top: Some questions can be correctly answered using absolute definitions for spatial relationships; for example in this image there is only one purple cube in the bottom half of the image. Bottom: Accuracy of each model on chain-structured questions as a function of the number of spatial relationships in the question, separated by question type. Top row shows all chainstructured questions; bottom row excludes questions that can be correctly answered using absolute spatial reasoning.</p>
<h3>4.6. Spatial Reasoning</h3>
<p>We expect that questions with more spatial relationships should be more challenging since they require longer chains of reasoning. The top set of plots in Figure 8 shows accuracy on chain-structured questions with different numbers of relationships. ${ }^{4}$ Across all three question types, CNN+LSTM+SA shows a significant drop in accuracy for questions with one or more spatial relationship; other models are largely unaffected by spatial relationships.</p>
<p>Spatial relationships force models to reason about objects' relative positions. However, as shown in Figure 8, some questions can be answered using absolute spatial reasoning. In this question the purple cube can be found by simply looking in the bottom half of the image; reasoning about its position relative to the metal sphere is unnecessary.</p>
<p>Questions only requiring absolute spatial reasoning can be identified by modifying the semantics of spatial relationship functions in their programs: instead of returning sets of objects related to the input object, they ignore their input object and return the set of objects in the half of the image corresponding to the relationship. A question only requires absolute spatial reasoning if executing its program with these modified semantics does not change its answer.</p>
<p>The bottommost plots of Figure 8 show accuracy on chain-structured questions with different number of relationships, excluding questions that can be answered</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. In Condition $A$ all cubes are gray, blue, brown, or yellow and all cylinders are red, green, purple, or cyan; in Condition $B$ color palettes are swapped. We train models in Condition A and test in both conditions to assess their generalization performance. We show accuracy on "query color" and "query material" questions, separating questions by shape of the object being queried.
with absolute spatial reasoning. On query questions, CNN+LSTM+SA performs significantly worse when absolute spatial reasoning is excluded; on count questions no model outperforms LSTM, and on exist questions no model outperforms Q-type mode. These results suggest that models have not learned the semantics of spatial relationships.</p>
<h3>4.7. Compositional Generalization</h3>
<p>Practical VQA systems should perform well on images and questions that contain novel combinations of attributes not seen during training. To do so models might need to learn disentangled representations for attributes, for example learning separate representations for color and shape instead of memorizing all possible color/shape combinations.</p>
<p>We can use CLEVR to test the ability of VQA models to perform such compositional generalization. We synthesize two new versions of CLEVR: in Condition $A$ all cubes are gray, blue, brown, or yellow and all cylinders are red, green, purple, or cyan; in Condition $B$ these shapes swap color palettes. Both conditions contain spheres of all eight colors.</p>
<p>We retrain models on Condition A and compare their performance when testing on Condition $\mathrm{A}(\mathrm{A} \rightarrow \mathrm{A})$ and testing on Condition B $(\mathrm{A} \rightarrow \mathrm{B})$. In Figure 9 we show accuracy on query-color and query-material questions, separating questions asking about spheres (which are the same in A and B) and cubes/cylinders (which change from A to B).</p>
<p>Between $\mathrm{A} \rightarrow \mathrm{A}$ and $\mathrm{A} \rightarrow \mathrm{B}$, all models perform about the same when asked about the color of spheres, but perform much worse when asked about the color of cubes or cylinders; CNN+LSTM+SA drops from $85 \%$ to $51 \%$. Models seem to learn strong biases about the colors of objects and cannot overcome these biases when conditions change.</p>
<p>When asked about the material of cubes and cylinders, CNN+LSTM+SA shows a smaller gap between $\mathrm{A} \rightarrow \mathrm{A}$ and
$\mathrm{A} \rightarrow \mathrm{B}(90 \%$ vs $81 \%$ ); other models show no gap. Having seen metal cubes and red metal objects during training, models can understand the material of red metal cubes.</p>
<h2>5. Discussion and Future Work</h2>
<p>This paper has introduced CLEVR, a dataset designed to aid in diagnostic evaluation of visual question answering (VQA) systems by minimizing dataset bias and providing rich ground-truth representations for both images and questions. Our experiments demonstrate that CLEVR facilitates in-depth analysis not possible with other VQA datasets: our question representations allow us to slice the dataset along different axes (question type, relationship type, question topology, etc.), and comparing performance along these different axes allows us to better understand the reasoning capabilities of VQA systems. Our analysis has revealed several key shortcomings of current VQA systems:</p>
<ul>
<li>Short-term memory: All systems we tested performed poorly in situations requiring short-term memory, including attribute comparison and integer equality questions (Section 4.2), same-attribute relationships (Section 4.3), and tree-structured questions (Section 4.4). Attribute comparison questions are of particular interest, since models can successfully identity attributes of objects but struggle to compare attributes.</li>
<li>Long reasoning chains: Systems struggle to answer questions requiring long chains of nontrivial reasoning, including questions with large effective sizes (Section 4.5) and count and existence questions with many spatial relationships (Section 4.6).</li>
<li>Spatial Relationships: Models fail to learn the true semantics of spatial relationships, instead relying on absolute image position (Section 4.6).</li>
<li>Disentangled Representations: By training and testing models on different data distributions (Section 4.7) we argue that models do not learn representations that properly disentangle object attributes; they seem to learn strong biases from the training data and cannot overcome these biases when conditions change.
Our study also shows cases where current VQA systems are successful. In particular, spatial attention [44] allows models to focus on objects and identify their attributes even on questions requiring multiple steps of reasoning.</li>
</ul>
<p>These observations present clear avenues for future work on VQA. We plan to use CLEVR to study models with explicit short-term memory, facilitating comparisons between values $[13,18,39,42]$; explore approaches that encourage learning disentangled representations [5]; and investigate methods that compile custom network architectures for different patterns of reasoning [2, 3]. We hope that diagnostic datasets like CLEVR will help guide future research in VQA and enable rapid progress on this important task.</p>
<h2>Supplementary Material</h2>
<h2>A. Basic Functions</h2>
<p>As described in Section 3 and shown in Figure 2, each question in CLEVR is associated with a functional program built from a set of basic functions. In this section we detail the semantics of these basic functional building blocks.</p>
<p>Data Types. Our basic functional building blocks operate on values of the following types:</p>
<ul>
<li>Object: A single object in the scene.</li>
<li>ObjectSet: A set of zero or more objects in the scene.</li>
<li>Integer: An integer between 0 and 10 (inclusive).</li>
<li>Boolean: Either yes or no.</li>
<li>Value types:</li>
<li>Size: One of large or small.</li>
<li>Color: One of gray, red, blue, green, brown, purple, cyan, or yellow.</li>
<li>Shape: One of cube, sphere, or cylinder.</li>
<li>Material: One of rubber or metal.</li>
<li>Relation: One of left, right, in front, or behind.</li>
</ul>
<p>Basic Functions. The functional program representations of questions are built from the following set of basic building blocks. Each of these functions takes the image's scene graph as an additional implicit input.</p>
<ul>
<li>scene ( $\emptyset \rightarrow$ ObjectSet $)$</li>
</ul>
<p>Returns the set of all objects in the scene.</p>
<ul>
<li>unique (ObjectSet $\rightarrow$ Object)</li>
</ul>
<p>If the input is a singleton set, then return it as a standalone Object; otherwise raise an exception and flag the question as ill-posed (See Section 3).</p>
<ul>
<li>relate (Object $\times$ Relation $\rightarrow$ ObjectSet)</li>
</ul>
<p>Return all objects in the scene that have the specified spatial relation to the input object. For example if the input object is a red cube and the input relation is left, then return the set of all objects in the scene that are left of the red cube.</p>
<ul>
<li>count (ObjectSet $\rightarrow$ Integer)</li>
</ul>
<p>Returns the size of the input set.</p>
<ul>
<li>exist (ObjectSet $\rightarrow$ Boolean)</li>
</ul>
<p>Returns yes if the input set is nonempty and no if it is empty.</p>
<ul>
<li>
<p>Filtering functions: These functions filter the input objects by some attribute, returning the subset of input objects that match the input attribute. For example calling filter_size with the first input small will return the set of all small objects in the second input.</p>
</li>
<li>
<p>filter_size (ObjectSet $\times$ Size $\rightarrow$ ObjectSet)</p>
</li>
<li>filter_color (ObjectSet $\times$ Color $\rightarrow$ ObjectSet)</li>
<li>filter_material (ObjectSet $\times$ Material $\rightarrow$ ObjectSet)</li>
<li>filter_shape (ObjectSet $\times$ Shape $\rightarrow$ ObjectSet)</li>
<li>Query functions: These functions return the specified attribute of the input object; for example calling query_color on a red object returns red.</li>
<li>query_size (Object $\rightarrow$ Size)</li>
<li>query_color (Object $\rightarrow$ Color)</li>
<li>query_material (Object $\rightarrow$ Material)</li>
<li>query_shape (Object $\rightarrow$ Shape)</li>
</ul>
<h2>- Logical operators:</h2>
<ul>
<li>AND (ObjectSet $\times$ ObjectSet $\rightarrow$ ObjectSet)</li>
</ul>
<p>Returns the intersection of the two input sets.</p>
<ul>
<li>OR (ObjectSet $\times$ ObjectSet $\rightarrow$ ObjectSet)</li>
</ul>
<p>Returns the union of the two input sets.</p>
<ul>
<li>Same-attribute relations: These functions return the set of objects that have the same attribute value as the input object, not including the input object. For example calling same_shape on a cube returns the set of all cubes in the scene, excluding the query cube.</li>
<li>same_size (Object $\rightarrow$ ObjectSet)</li>
<li>same_color (Object $\rightarrow$ ObjectSet)</li>
<li>same_material (Object $\rightarrow$ ObjectSet)</li>
<li>same_shape (Object $\rightarrow$ ObjectSet)</li>
<li>Integer comparison: Checks whether the two integer inputs are equal, or whether the first is less than or greater than the second, returning either yes or no.</li>
<li>equal_integer (Integer $\times$ Integer $\rightarrow$ Boolean)</li>
<li>less_than (Integer $\times$ Integer $\rightarrow$ Boolean)</li>
<li>greater_than (Integer $\times$ Integer $\rightarrow$ Boolean)</li>
<li>Attribute comparison: These functions return yes if their inputs are equal and no if they are not equal.</li>
<li>equal_size (Size $\times$ Size $\rightarrow$ Boolean)</li>
<li>equal_material (Material $\times$ Material $\rightarrow$ Boolean)</li>
<li>equal_color (Color $\times$ Color $\rightarrow$ Boolean)</li>
<li>equal_shape (Shape $\times$ Shape $\rightarrow$ Boolean)</li>
</ul>
<h2>B. Effective Question Size</h2>
<p>In Section 4.5 we note that some questions can be correctly answered without correctly resolving all intermediate object references, and define a question's effective question to quantitatively measure this effect.</p>
<p>For any question we can compute its effective question by pruning functions from the question's program; the effective question is the smallest such pruned program that, when executed on the scene graph for the question's image, gives the same answer as the original question.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. For the above image and the question "What color is the cube behind the cylinder?", the effective question is "What color is the cube?" (see text for details).</p>
<p>Some pruned questions may be ill-posed, meaning that some object references do not refer to a unique object. For example, consider the question "What color is the cube behind the cylinder?"; its associated program is</p>
<div class="codehilite"><pre><span></span><code>query_color(unique(filter_shape(cube,
</code></pre></div>

<div class="codehilite"><pre><span></span><code>relate(behind, unique(filter_shape(cylinder, scene()))))))
</code></pre></div>

<p>Imagine executing this program on the scene shown in Figure 10. The innermost filter_shape gives a set containing the cylinder, the relate returns a set containing just the large cube in the back, the outer filter_shape does nothing, and the query_color returns brown.</p>
<p>This question is not ill-posed (Section 3) because the reference to "the cube" cannot be resolved without the rest of the question; however this question's effective size is less than its actual size because the question can be correctly answered without resolving this object reference correctly.</p>
<p>To compute the effective question, we attempt to prune functions from this program. Starting from the innermost function and working out, whenever we find a function whose input type is Object or ObjectSet, we construct a pruned question by replacing that function's input with a scene function and executing it. The smallest such pruned program that gives the same answer as the original program is the effective question.</p>
<p>Pruned questions may be ill-posed, so we execute them with modified semantics. The output type of the unique function is changed from Object to ObjectSet, and it simply returns its input set. All functions taking an Object input are modified to take an ObjectSet input instead by mapping the original function over its input set and flattening the resulting set; thus the relate functions return the set of objects in the scene that have the specified relationship with any of the input objects, and the query functions return sets of values rather than single values.</p>
<p>Therefore for this example question we consider the fol-
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Accuracy on query questions vs. actual and effective question size, restricting to questions with a same-attribute relationship. Figure 7 shows the same plots for questions without a same-attribute relationship. For both groups of questions we see that accuracy decreases as effective question size increases.
lowing sequence of pruned programs. First we prune the inner filter_shape function:</p>
<p>$$
\begin{gathered}
\text { query_color(unique(filter_shape(cube, } \
\text { relate(behind, unique(scene()))))) }
\end{gathered}
$$</p>
<p>The relate function now returns the set of objects which are behind some object, so it returns the large cube and the cylinder (since it is behind the small cube). The filter_shape function removes the cylinder, and the query_color returns a singleton set containing brown.</p>
<p>Next we prune the inner unique function:</p>
<p>$$
\begin{gathered}
\text { query_color(unique(filter_shape(cube, } \
\text { relate(behind, scene()))))) }
\end{gathered}
$$</p>
<p>Since unique computes the identity for pruned questions, execution is the same as above.</p>
<p>Next we prune the relate function:</p>
<p>$$
\text { query_color(unique(filter_shape(cube, scene()))) }
$$</p>
<p>Now the filter_shape returns the set of both cubes, but since both are brown the query_color still returns a singleton set containing brown.</p>
<p>Next we prune the filter_shape function:</p>
<p>$$
\text { query_color(unique(scene()) ) }
$$</p>
<p>Now the query_color receives the set of all three input objects, so it returns a set containing brown and gray, which is different from the original question.</p>
<p>The effective question is therefore:</p>
<p>$$
\text { query_color(unique(filter_shape(cube, scene()))) }
$$</p>
<h2>B.1. Accuracy vs Question Size</h2>
<p>Figure 7 of the main paper shows model accuracy on query-attribute questions as a function of actual and effective question size, excluding questions with same-attribute relationships. Questions with same-attribute relationships have a maximum question size of 10 but questions without same-attribute relationships have a maximum size of 20 ; combining these questions thus leads to unwanted correla-</p>
<p>tions between question size and difficulty.
In Figure 11 we show model accuracy vs. actual and effective question size for questions with same-attribute relationships. Similar to Figure 7, we see that model accuracy either remains constant or increases as actual question size increases, but all models show a clear decrease in accuracy as effective question size increases.</p>
<h1>C. Dynamic Module Networks</h1>
<p>Module networks [2, 3] are a novel approach to visual question answering where a set of differentiable modules are used to assemble a custom network architecture to answer each question. Each module is responsible for performing a specific function such as finding a particular type of object, describing the current object of attention, or performing a logical and operation to merge attention masks. This approach seems like a natural fit for the rich, compositional questions in CLEVR; unfortunately we found that parsing heuristics tuned for the VQA dataset did not generalize to the longer, more complex questions in CLEVR.</p>
<p>Dynamic module networks [2] generate network architectures by performing a dependency parse of the question, using a set of heuristics to compute a set of layout fragments, combining these fragments to create candidate layouts, and ranking the candidate layouts using an MLP.</p>
<p>For some questions, the heuristics are unable to produce any layout fragments; in this case, the system uses a simple default network architecture as a fallback for answering that question. On a random sample of 10,000 questions from the VQA dataset [4], we found that dynamic module networks resorted to default architecture for $7.8 \%$ of questions; on a random sample of 10,000 questions from CLEVR, the default network architecture was used for $28.9 \%$ of questions. This suggests that the same parsing heuristics used for VQA do not apply to the questions in CLEVR; therefore the method of [2] did not work out-of-the box on CLEVR.</p>
<h2>D. Example images and questions</h2>
<p>The remaining pages show randomly selected images and questions from CLEVR. Each question is annotated with its answer, question type, and size. Recall from Section 3 that a question's type is the outermost function in the question's functional program, and a question's size is the number of functions in its program.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Q: There is a rubber cube in front of the big cylinder in front of the big brown matte thing; what is its size?
A: small
Q-type: query_size
Size: 14</p>
<p>Q: There is another cube that is made of the same material as the small brown block; what is its size?
A: large
Q-type: query_size
Size: 9</p>
<p>Q: What color is the object that is on the left side of the small rubber thing?
A: gray
Q-type: query_color
Size: 7</p>
<p>Q: What number of other matte objects are the same shape as the small rubber object?
A: 1
Q-type: count
Size: 7</p>
<p>Q: Are there fewer metallic objects that are on the left side of the large cube than cylinders to the left of the cyan shiny block? A: yes
Q-type: less_than
Size: 16</p>
<p>Q: What is the size of the matte thing that is on the left side of the large cyan object and in front of the small blue thing?
A: large
Q-type: query_size
Size: 14</p>
<p>Q: The green matte thing that is behind the large green thing that is to the right of the big cyan metal object is what shape?
A: cylinder
Q-type: query_shape
Size: 14</p>
<p>Q: Is there a tiny thing that has the same material as the brown cylinder?
A: yes
Q-type: exist
Size: 7</p>
<p>Q: What is the material of the tiny object to the right of the brown shiny ball behind the tiny shiny cylinder?
A: metal
Q-type:
query_material
Size: 14
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Q: Are there fewer metalic objects that are on the left side of the large cube than cylinders to the left of the cyan shiny block? A: yes
Q-type: less_than Size: 16</p>
<p>Q: What is the type of rubber thing that is left of the rubber thing that is right of the rubber cylinder behind the gray shiny block; what is its size?
A: large
Q-type: query_size
Size: 17</p>
<p>Q: What is the size of the matte thing that is behind the large green thing that is to the right of the big cyan metal object is what shape?
A: cylinder
Q-type: query_shape
Size: 14
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Q: Is the number of the tiny metal objects left of the purple metallic cylinder the same as the number of small metal objects to the sphere? A: no
Q-type: equal_shape
Size: 13</p>
<p>Q: What is the material of the tiny object to the right of the brown shiny ball behind the tiny shiny cylinder?
A: yes
Q-type: count
Size: 9</p>
<p>Q: Is there anything that is the same color as the tiny shiny ball; what is it made of?
A: rubber
Q-type:
query_material
Size: 9
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Q: There is a large $\mathbf{Q}$ : There is a big brown block in front of the tiny rubber cylinder that is behind the cyan block; are the cyan block; the type: color as the tiny
A: no
Q-type: equal_shape
Size: 14</p>
<p>Q: There is a object $\mathbf{Q}$ : There is a big $\mathbf{Q}$ : The cyan block that
that is both on the purple shiny object; left side of the brown $\mathbf{A}$ : sphere
metal block and in $\mathbf{A}$ : sphere
front of the large $\mathbf{Q}$-type: query_shape purple shiny ball; how
big is it?
A: small
Q-type: query_size
Size: 16
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Q: Is the number of $\mathbf{Q}$ : Do the large shiny object and the thing to the left of the big gray object have the same shape?
A: no
Q-type: equal_shape
Size: 13</p>
<p>Q: Is there anything that is the same color as the tiny shiny yellow matte thing? A: no
Q-type: exist
Size: 7</p>
<p>Q: Is there anything that is behind the tiny
A: no
Q-type: equal_shape
Size: 14</p>
<p>Q: Is there anything that is the same material as the tiny object that is behind the matte thing?
A: no
Q-type: equal_shape
Size: 14</p>
<p>Q: There is a large $\mathbf{Q}$ : There is a big brown block in front of the tiny rubber cylinder that is behind the cyan block; are the cyan block; the type: color as the tiny
A: sphere
Q-type: query_shape
Size: 14</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Q: How big is the gray rubber object that is behind the big shiny thing behind the big metallic thing that is on the left side of the purple ball?
A: small
Q-type: query_size
Size: 17</p>
<p>Q: Is there another green rubber cube that has the same size as the green matte cube? A: no
Q-type: exist
Size: 10</p>
<p>Q: There is a purple ball that is the same size as the red cylin-
der; what material is it?
A: metal
Q-type:
query_material
Size: 9</p>
<p>Q: Is the large matte thing the same shape as the big red object? A: yes
Q-type: equal_shape
Size: 11</p>
<p>Q: There is a tiny rubber thing that is the same color as the metal cylinder; what shape is it?
A: cylinder
Q-type: query_shape
Size: 9</p>
<p>Q: What is the shape of the tiny green thing that is made of the same material as the large cylinder?
A: cylinder
Q-type: query_shape
Size: 9</p>
<p>Q: The big matte thing is what color?
A: purple
Q-type: query_color
Size: 5</p>
<p>Q: The light blue
metallic object and the
green metal thing have
the same shape?
A: no
Q-type: equal_shape
Size: 11</p>
<p>Q: The light blue
metal as the large
purple ball?
A: yes
Q-type:
equal_material
Size: 12</p>
<p>Q: The blue blue
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Q: How many small
spheres are the same color as the big rubber cube?
A: 0
Q-type: count
Size: 9</p>
<p>Q: Is the tiny ball made of the same material as the large purple ball?
A: yes
Q-type:
equal_material
Size: 12</p>
<p>Q: Do the matte cylinder and the purple shiny object have the same size?
A: no
Q-type: equal_size
A: 1
Q-type: count
Size: 18</p>
<p>Q: Is the tiny ball made of the same material as the large
purple ball?
A: yes
Q-type: equal_size
Size: 12</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Q: How many small
green things are
behind the green
rubber sphere in front
of the blue thing that
is in front of the large
purple metal cylinder?
A: 1
Q-type: count
Size: 18</p>
<p>Q: The cylinder that is the same size as the blue metallic sphere is what color?
A: purple
Q-type: query_color
Size: 9</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Q: There is a small Q: Is the size of the ball that is made of red rubber sphere the the same material as the large block; what metal thing?
A: yes
Q-type: equal_size
Size: 9</p>
<p><img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Q: What is the color of the matte thing that is left of the thing behind the tiny green thing behind the tiny shiny sphere?
A: green
Q-type: query_color
Size: 15</p>
<p>Q: There is a blue ball that is the same size as the brown thing; what material is it?
A: metal
Q-type:
query_material
Size: 8</p>
<p><img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Q: What is the color of the matte thing that is left of the thing behind the tiny green thing behind the tiny shiny sphere?
A: green
Q-type: query_color
Size: 15</p>
<p><img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>Q: The green thing behind the small green thing on the right side of the brown matte object is what shape?
A: cube
Q-type: query_shape
Size: 12</p>
<p><img alt="img-23.jpeg" src="img-23.jpeg" /></p>
<p>Q: There is a small Q: Is the size of the red rubber sphere the same as the purple metal thing?
A: yes
Q-type: equal_size
Size: 12</p>
<p><img alt="img-24.jpeg" src="img-24.jpeg" /></p>
<p>Q: There is a tiny Q: Is the size of the red rubber sphere the same material as the purple metal thing?
A: yes
Q-type: equal_size
Size: 12</p>
<p><img alt="img-25.jpeg" src="img-25.jpeg" /></p>
<p>Q: The green thing is the same size as the shiny cylinder; what size is it?
A: large
Q-type: query_size
Size: 9</p>
<p><img alt="img-26.jpeg" src="img-26.jpeg" /></p>
<p>Q: The green thing is the same size as the shiny</p>
<p><img alt="img-27.jpeg" src="img-27.jpeg" /></p>
<p>Q: Are there more
brown shiny objects behind the large rubber cylinder than gray blocks?
A: yes
Q-type: greater,than Size: 14</p>
<p>Q: What color is the matte object to the right of the large rubber cylinder?
A: blue
Q-type: query_color
Size: 8</p>
<p><img alt="img-28.jpeg" src="img-28.jpeg" /></p>
<p>Q: Are there any other things that are the same shape as the large green thing?
A: no
Q-type: exist
Size: 6</p>
<p><img alt="img-29.jpeg" src="img-29.jpeg" /></p>
<p>Q: What number of cubes are the same color as the rubber ball?
A: 1
Q-type: count
Size: 7</p>
<p><img alt="img-30.jpeg" src="img-30.jpeg" /></p>
<p>Q: What number of cubes are the same color as the rubber ball?
A: 1
Q-type: count
Size: 7</p>
<p>Q: Is the big cyan metal thing the same shape as the brown thing?
A: no
Q-type: equal.shape
Size: 11</p>
<p><img alt="img-31.jpeg" src="img-31.jpeg" /></p>
<p>Q: There is a tiny brown rubber thing; is its shape the same as the thing that is in front of the small matte cylinder?
A: no
Q-type: equal.shape
Size: 15</p>
<p>Q: Are there fewer small rubber cylinders in front of the green ball than purple cylinders?
A: no
Q-type: less,than Size: 14</p>
<p><img alt="img-32.jpeg" src="img-32.jpeg" /></p>
<p>Q: What number of yellow rubber things have the same size as the yellow metallic cylinder?
A: no
Q-type: count
Size: 7</p>
<p><img alt="img-33.jpeg" src="img-33.jpeg" /></p>
<p>Q: Is the size of the cyan cube the same behind the large block as the metal cylinder that is behind the cyan cylinder?
A: 0
Q-type: count
Size: 6</p>
<p>Q: What shape is the tiny metal thing that is behind the large block in front of the tiny brown block?
A: cube
Q-type: query_shape
Size: 14</p>
<p><img alt="img-34.jpeg" src="img-34.jpeg" /></p>
<p>Q: What is the shape of the shiny thing that is behind the small blue rubber object and to the right of the tiny brown thing?
A: cube
Q-type: less,than Size: 15</p>
<p><img alt="img-35.jpeg" src="img-35.jpeg" /></p>
<p>Q: What is the shape of the shiny thing that is behind the small blue rubber object in front of the brown object to the right of the big metallic block?
A: cylinder
Q-type: query_shape
Size: 13</p>
<p><img alt="img-36.jpeg" src="img-36.jpeg" /></p>
<p>Q: What is the shape of the blue rubber object in front of the brown object to the right of the big metallic block?
A: cylinder
Q-type: query_shape
Size: 9</p>
<p><img alt="img-37.jpeg" src="img-37.jpeg" /></p>
<p>Q: There is a red shiny thing right of the purple metal sphere; what is its shape?
A: cylinder
Q-type: query_shape Size: 10</p>
<p>Q: What number of purple shiny things are there?
A: 1
Q-type: count
Size: 4</p>
<p>Q: Does the small ball have the same color as the small cylinder in front of the big sphere? sphere?
A: no
Q-type: equal_color Size: 6
Size: 15</p>
<p>Q: How many blocks are yellow rubber things or large green shiny things?
A: 0
Q-type: count
Size: 10</p>
<p>Q: Are there more metallic objects that are right of the large red shiny cylinder than gray matte objects?
A: no
Q-type: greater_than Size: 14
<img alt="img-38.jpeg" src="img-38.jpeg" /></p>
<p>Q: How big is the
yellow thing behind the brown shiny thing? A: small
Q-type: query_size Size: 8</p>
<p>Q: Is the size of the brown metal ball the same as the yellow thing that is behind the yellow rubber sphere? A: no
Q-type: equal_size Size: 16
<img alt="img-39.jpeg" src="img-39.jpeg" /></p>
<p>Q: Does the small ball have the same color as the green in front of the big sphere? sphere?
A: 0
Q-type: count
Size: 6</p>
<p>Q: How many blocks are yellow rubber things or large green shiny things?
A: 0
Q-type: count
Size: 10</p>
<p>Q: How many blocks are yellow rubber things or large green shiny things?
A: 0
Q-type: count
Size: 10</p>
<p>Q: How many blocks are yellow rubber things or large green shiny things?
A: 0
Q-type: equal_shape Size: 13
<img alt="img-40.jpeg" src="img-40.jpeg" /></p>
<p>Q: How many things
are either green things
or matte cubes behind the green ball?
A: 2
Q-type: count
Size: 11</p>
<p>Q: There is a cyan object that is to the rubber sphere; is its size the same as the gray rubber cylinder? A: no
Q-type: equal_size Size: 16
<img alt="img-41.jpeg" src="img-41.jpeg" /></p>
<p>Q: There is a object that is behind the big green metal cylinder; what is its material? A: rubber
Q-type: query_material Size: 9</p>
<p>A: large
Q-type: query_size Size: 4</p>
<p>Q: Do the green object behind the tiny rubber blocks are green matte cylinder there? A: 1
object have the same material? A: yes
Q-type: equal_shape Size: 13
<img alt="img-42.jpeg" src="img-42.jpeg" /></p>
<p>Q: What color is the rubber ball in front cyan shiny thing that of the metal cube to the same size as the cube left of the blue metallic sphere? A: gray
Q-type: query_color Size: 18
<img alt="img-43.jpeg" src="img-43.jpeg" /></p>
<p>Q: What color is the rubber ball in front cyan shiny thing that of the metal cube to the same size as the red matte object? A: cube
Q-type: query_shape
Size: 9</p>
<p>Q: What is the size of the green block behind the big gray matte sphere? A: cube
Q-type: query_size Size: 11
A: no
Q-type: equal_integer Size: 17
<img alt="img-44.jpeg" src="img-44.jpeg" /></p>
<p>A: large
Q-type: query_size Size: 4</p>
<p>Q: Do the green object behind the tiny rubber blocks are green matte cylinder there? A: 1
object have the same material? A: yes
Q-type: equal_shape Size: 13
<img alt="img-45.jpeg" src="img-45.jpeg" /></p>
<p>Q: What color is the rubber ball in front cyan shiny thing that of the metal cube to the same size as the cube left of the blue metallic sphere? A: gray
Q-type: query_color Size: 18
<img alt="img-46.jpeg" src="img-46.jpeg" /></p>
<p>Acknowledgments We thank Deepak Pathak, Piotr Dollár, Ranjay Krishna, Animesh Garg, and Danfei Xu for helpful comments and discussion.</p>
<h2>References</h2>
<p>[1] A. Agrawal, D. Batra, and D. Parikh. Analyzing the behavior of visual question answering models. In EMNLP, 2016. 1
[2] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Learning to compose neural networks for question answering. In NAACL, 2016. 1, 2, 4, 5, 8, 11
[3] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural module networks. In CVPR, 2016. 1, 2, 4, 8, 11
[4] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Zitnick, and D. Parikh. VQA: Visual question answering. In ICCV, 2015. 1, 2, 4, 5, 11
[5] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. TPAMI, 35(8):1798-1828, 2014. 8
[6] Blender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation, Blender Institute, Amsterdam, 2016. 3
[7] J. Chen, P. Kuznetsova, D. Warren, and Y. Choi. Deja imagecaptions: A corpus of expressive image descriptions in repetition. In NAACL, 2015. 2
[8] A. Farhadi, M. Hejrati, A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Generating sentences for images. In ECCV, 2010. 2
[9] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach. Multimodal compact bilinear pooling for visual question answering and visual grounding. In arXiv:1606.01847, 2016. 1, 4, 5
[10] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu. Are you talking to a machine? Dataset and methods for multilingual image question answering. In NIPS, 2015. 1, 2, 4
[11] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact bilinear pooling. In CVPR, 2016. 5
[12] D. Geman, S. Geman, N. Hallonquist, and L. Younes. Visual Turing test for computer vision systems. Proceedings of the National Academy of Sciences, 112(12):3618-3623, 2015. 2
[13] A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwinska, S. Colmenarejo, E. Grefenstette, T. Ramalho, J. Agapiou, A. Badia, K. Hermann, Y. Zwols, G. Ostrovski, A. Cain, H. King, C. Summerfield, P. Blunsom, K. . Kavukcuoglu, and D. Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 2016. 8
[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 5
[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997. 2, 5
[16] A. Jabri, A. Joulin, and L. van der Maaten. Revisiting visual question answering baselines. In ECCV, 2016. 1, 4
[17] J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei. Image retrieval using scene graphs. In CVPR, 2015. 3
[18] A. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In NIPS, 2015. 8
[19] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. 2
[20] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 5
[21] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L. Jia-Li, D. Shamma, M. Bernstein, and L. Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2016. 1, 2, 3
[22] H. J. Levesque, E. Davis, and L. Morgenstern. The Winograd schema challenge. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, page 47, 2011. 2
[23] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 2
[24] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical question-image co-attention for visual question answering. In NIPS, 2016. 1, 2, 4
[25] L. Ma, Z. Lu, and H. Li. Learning to answer questions from image using convolutional neural network. In AAAI, 2016. 4
[26] M. Malinowski and M. Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input. In NIPS, 2014. 1, 2
[27] M. Malinowski and M. Fritz. Towards a visual Turing challenge. In NIPS 2014 Workshop on Learning Semantics, 2014. 2
[28] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neurons: A neural-based approach to answering questions about images. In ICCV, 2015. 2, 4
[29] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. In arXiv 1301.3781, 2013. 5
[30] O. Pfungst. Clever Hans (The horse of Mr. von Osten): A contribution to experimental animal and human psychology. Henry Holt, New York, 1911. 1
[31] A. Ray, G. Christie, M. Bansal, D. Batra, and D. Parikh. Question relevance in vqa: Identifying non-visual and falsepremise questions. In EMNLP, 2016. 1
[32] M. Ren, R. Kiros, and R. Zemel. Exploring models and data for image question answering. In NIPS, 2015. 1, 2, 4
[33] K. Shih, S. Singh, and D. Hoiem. Where to look: Focus regions for visual question answering. In CVPR, 2016. 4
[34] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 15(1):1929-1958, 2014. 5
[35] B. Sturm. A simple method to determine if a music information retrieval system is a horse. IEEE Transactions on Multimedia, 16(6):1636-1644, 2014. 1
[36] B. Sturm. Horse taxonomy and taxidermy. HORSE2016, 2016. 1
[37] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urtasun, and S. Fidler. Movieqa: Understanding stories in movies through question-answering. In CVPR, 2016. 2</p>
<p>[38] J. Weston, A. Bordes, S. Chopra, A. Rush, B. van Merriënboer, A. Joulin, and T. Mikolov. Towards aicomplete question answering: A set of prerequisite toy tasks. In $I C L R, 2016.2$
[39] J. Weston, S. Chopra, and A. Bordes. Memory networks. In $I C L R, 2015.8$
[40] T. Winograd. Understanding Natural Language. Academic Press, 1972. 2
[41] Q. Wu, C. Shen, A. van den Hengel, P. Wang, and A. Dick. Image captioning and visual question answering based on attributes and their related external knowledge. In arXiv 1603.02814, 2016. 4
[42] C. Xiong, S. Merity, and R. Socher. Dynamic memory networks for visual and textual question answering. ICML, 2016. 4, 8
[43] H. Xu and K. Saenko. Ask, attend, and answer: Exploring question-guided spatial attention for visual question answering. In ECCV, 2016. 4
[44] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked attention networks for image question answering. In CVPR, 2016. 1, 2, 4, 5, 8
[45] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. In TACL, pages 67-78, 2014. 2
[46] L. Yu, E. Park, A. Berg, and T. Berg. Visual madlibs: Fill in the blank image generation and question answering. In ICCV, 2015. 1, 2
[47] P. Zhang, Y. Goyal, D. Summers-Stay, D. Batra, and D. Parikh. Yin and yang: Balancing and answering binary visual questions. In CVPR, 2016. 1, 2
[48] B. Zhou, Y. Tian, S. Sukhbataar, A. Szlam, and R. Fergus. Simple baseline for visual question answering. In arXiv:1512.02167, 2015. 1, 4, 5
[49] Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Visual7w: Grounded question answering in images. In CVPR, 2016. 1, 2,4
[50] C. Zitnick and D. Parikh. Bringing semantics into focus using visual abstraction. In CVPR, 2013. 2</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ We exclude questions with same-attribute relations since their max size is 10 , introducing unwanted correlations between size and difficulty. Excluded questions show the same trends (see supplementary material).
${ }^{3}$ Pruned questions may be ill-posed (Section 3) so they are executed with modified semantics; see supplementary material for details.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ We restrict to chain-structured questions to avoid unwanted correlations between question topology and number of relationships.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
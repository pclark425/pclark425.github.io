<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3816 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3816</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3816</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-261214653</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/29872/31521" target="_blank">SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</a></p>
                <p><strong>Paper Abstract:</strong> Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research. Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research. However, current benchmarks are mostly based on pre-collected objective questions. This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability. In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues. Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability. In particular, we design a"dynamic"subset based on scientific principles to prevent evaluation from potential data leakage. Both objective and subjective questions are included in SciEval. These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions. The codes and data are publicly available on https://github.com/OpenDFM/SciEval.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3816.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3816.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-disciplinary English benchmark introduced in this paper to evaluate LLM scientific research ability across four dimensions (Basic Knowledge, Knowledge Application, Scientific Calculation, Research Ability) using ~18k questions spanning biology, chemistry, and physics, and composed of Static, Dynamic, and Experimental subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Ability across four knowledge dimensions mapped to Bloom's taxonomy: Basic Knowledge (recall/understanding), Knowledge Application (comprehension/application/analysis), Scientific Calculation (numerical reasoning and computation), Research Ability (problem formulation, experimental design, data analysis, summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automated scoring against ground-truth for objective questions; dynamic data generation from scientific principles to avoid data leakage; chain-of-thought and answer-only prompting; few-shot (3-shot) evaluation; manual assessment for open-ended experimental questions; use of GPT-4 to convert and curate community Q&A into multiple-choice and generate distractors with human manual checking.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>SciEval benchmark (introduced here) composed of three subsets: Static Data (pre-collected objective Qs from community Q&A and public datasets), Dynamic Data (regularly regenerated questions based on scientific principles for chemistry and physics to mitigate data leakage), and Experimental Data (subjective/open-ended questions from 12 basic lab experiments to evaluate research ability).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Accuracy for objective multiple-choice questions (Static Data and physics Dynamic subset); BLEU (and extract match) for string answers in chemistry Dynamic subset; Mean Square Error (MSE) for numeric answers in chemistry Dynamic subset; manual/qualitative scoring for Experimental Data; comparisons of model performance under Answer-Only, Chain-of-Thought (CoT), and 3-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High — humans manually check GPT-4-generated question/answer/distractor conversions and classify domains; Experimental Data responses are evaluated manually; exemplar selection for few-shot uses dev data; models with RLHF are included but RLHF is not an evaluation method per se.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Risk of data leakage from pretraining corpora (addressed by dynamic data); objective-only benchmarks insufficient to evaluate research skills; subjective Experimental Data requires manual scoring (time-consuming, subjective); many models struggle with calculation-heavy items (especially physics), producing near-random results in some dynamic physics tests; CoT capability varies across models; token-length limits constrain evaluation of long/exhaustive experimental prompts for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>No explicit LLM-generated 'scientific theory' is reported; closest examples are model-generated experimental designs, principles, and analysis in Experimental Data (open-ended answers produced by models such as GPT-4 and Claude, evaluated manually).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across SciEval, GPT-4 attains the strongest performance (state-of-the-art among evaluated LLMs); only GPT-4, GPT-3.5-turbo, and Claude-v1.3 exceed ~60% average accuracy on Static Data. On Dynamic Data GPT-4 leads on average accuracy and BLEU for chemistry but performs poorly on some physics Dynamic items; Galactica models perform relatively better on computational problems. Chain-of-Thought improves GPT-series performance; 3-shot improves many models but physics Dynamic subset frequently remains near-random for most models. Experimental Data: top-tier models handle experimental principles and design reasonably well but generally fail at analyzing experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3816.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3816.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciEval Static Data subset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fixed collection of objective questions (~15.9k referenced in paper) drawn from community Q&A (Socratic), MedQA, PubMedQA, Reagent Selection and processed into multiple-choice, judgment, or fill-in-the-blank items for fast, reproducible automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness of factual knowledge and application as per Basic Knowledge, Knowledge Application, and Scientific Calculation dimensions (mapped to Bloom's taxonomy levels).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automated matching of model outputs to ground-truth labels for multiple-choice and judgment questions; split into dev/valid/test; 3-shot dev exemplars for few-shot evaluations; evaluated under Answer-Only and Chain-of-Thought prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Static Data — fixed objective subset of SciEval compiled from community Q&A and public datasets, used for reliable cross-model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Accuracy (%) per discipline (biology/chemistry/physics) and averaged across domains; tables report model-wise accuracies and averages under AO/CoT/3-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human involvement primarily in data curation: manual checking of GPT-4-generated distractors and domain classification; otherwise evaluation automated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Static data is susceptible to data leakage if models were trained on the sources (mitigated by inclusion of Dynamic Data); objective format insufficient to evaluate higher-level research skills; reliance on GPT-4 for question reformulation introduces potential bias and requires manual verification.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Not applicable — Static Data is objective Q/A rather than generative theory outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Results show GPT-4 highest accuracy; only GPT-4, GPT-3.5-turbo, and Claude-v1.3 exceed ~60% average accuracy on Static Data; many open-weight or smaller LLMs perform substantially worse, indicating substantial room for improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3816.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3816.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciEval Dynamic Data subset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A regularly regenerated subset designed to avoid data leakage by programmatically generating questions from scientific principles: chemistry items use molecular properties and computations; physics items are generated using physics formulas via scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Knowledge Application and Scientific Calculation abilities measured by computed/numerical answers, generation of structured string outputs (e.g., SMILES), and multiple-choice physical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automatic generation of question instances from scientific rules/formulae; evaluate models under AO/CoT/3-shot; for chemistry use numeric/string scoring; for physics use multiple-choice accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Dynamic Data — chemistry (~2000 items) and physics (~890 items) programmatically generated; stable snapshot maintained for fair comparisons plus growing/rotating regeneration to prevent leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Chemistry dynamic: Mean Square Error (MSE) for numeric outputs, BLEU and extract-match for string outputs; Physics dynamic: accuracy for 4-choice questions; average accuracy and BLEU scores reported per model; default extremely large MSE assigned when no numeric predicted (1e10).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Minimal in generation (scripted), but human maintainers regenerate and publish stable snapshots; some manual oversight to ensure correctness of generation scripts and regenerated data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Despite generation, many LLMs show near-random performance on physics dynamic subset (accuracy ~25% baseline); chemistry numeric/string evaluation needs mixed metrics (MSE, BLEU) complicating aggregate scoring; large default MSE penalizes nonnumeric responses; models often lack molecule-specific knowledge causing poor chemistry performance.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Not a theory; dynamic items test model calculation and representation generation (e.g., 'molecular weight of A?' or 'SMILES expression of B?').</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-4 obtains best average accuracy and BLEU on Dynamic Data, but performance on counting/calculation items varies—Galactica-30B performs strongly on certain computation tasks; many models have low chemistry performance and near-random physics accuracy. CoT and 3-shot improve chemistry subset substantially for many models; physics subset improvements are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3816.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3816.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experimental Data</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciEval Experimental Data subset (12 basic experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A subjective/open-ended subset containing questions derived from 12 basic laboratory experiments designed to probe Research Ability: experimental principle, process/design, and analysis/summarization of results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Research ability assessed via correctness and quality of experimental principle explanation, experimental design appropriateness, and analytic interpretation of experimental results (higher-order cognitive tasks: Analyze, Evaluate, Create).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Open-ended prompts answered by models and evaluated manually by humans; longer context inputs may be used (but some models limited by context length).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Experimental Data — set of open-ended questions per experiment used to evaluate higher-level research skills, requiring manual scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Qualitative manual scores per experiment and per model (detailed scores in appendices); aggregated statements about strengths (principle and design) versus weaknesses (result analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High — responses are assessed manually; some models were only evaluated via web interfaces due to access restrictions and manual scoring was required to judge open-ended answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Manual scoring is subjective and resource-intensive; long responses exceed context limits of many models, limiting evaluation coverage; models generally perform poorly on analysis of experimental results even when they can describe principles and designs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Models produced experimental principle descriptions and proposed experimental designs (these are closest to generated scientific hypotheses/theories in the paper), but no fully-formed novel scientific theory is reported or validated.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>GPT-series and Claude-series models perform relatively well on experimental principle and design questions, but almost all models struggle to analyze experimental results; other models evaluated via web (ERNIE Bot, SparkDesk) performed less well overall.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3816.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3816.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Settings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation prompting and shot settings (Answer-Only, Chain-of-Thought, 3-Shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three protocol variants used to probe model reasoning and robustness: Answer-Only (AO) forces a single final answer, Chain-of-Thought (CoT) elicits intermediate reasoning steps, and 3-Shot provides three in-context exemplars from dev set for few-shot learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Improvement or degradation in final task metrics (accuracy, BLEU, MSE) under different prompting regimes; CoT aims to reveal and improve internal reasoning leading to higher-quality outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compare model outputs and metrics across AO, CoT, and 3-shot settings; report per-model delta indicators (↑/↓/∼) for Dynamic Data and full metric tables for Static Data.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied across SciEval subsets (Static, Dynamic, Experimental where applicable); exemplars for 3-shot selected from dev set.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Change in accuracy, BLEU, and other metrics between settings (e.g., CoT increases accuracy for GPT-series on Static Data; 3-shot yields highest Dynamic physics accuracy for GPT-4 in one result).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human curation of 3-shot exemplars from dev set; manual inspection of CoT outputs when needed; but metric computation remains automated for objective items.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not all models reliably support CoT; CoT benefits concentrated in GPT-series; some models have limited or low-quality CoT generation; CoT sometimes leads to incorrect application of formulas (GPT-4 used wrong formulas on some physics problems under CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>No explicit generated scientific theories, but CoT outputs reveal models' stepwise reasoning when they attempt experimental analysis or calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>CoT improved performance mainly for GPT-series models on Static Data; 3-shot often improved roughly half of the evaluated LLMs versus Answer-Only; on Dynamic chemistry CoT and 3-shot substantially improved many LLMs, while physics Dynamic subset usually remained difficult except isolated gains (e.g., GPT-3.5-turbo under CoT achieved 47.19% on physics dynamic in one reported condition).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3816.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3816.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metrics & Scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated and manual metrics used in SciEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of quantitative and qualitative metrics used to evaluate model outputs across SciEval: accuracy for multiple-choice/judgement, BLEU and extract-match for string outputs, MSE for numeric outputs, and manual scoring for experimental open-ended responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness (accuracy), closeness to reference strings (BLEU, extract match), numeric error magnitude (MSE), and human-judged quality for research-style open answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automated computation of accuracy, BLEU, and MSE for objective/dynamic items; assign extremely large MSE (1e10) when numeric prediction absent; manual scoring protocols for Experimental Data (not fully formalized in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Metrics applied to Static Data, Dynamic Data (chemistry/physics), and Experimental Data (manual).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Per-model accuracies per domain and averaged; BLEU and MSE reported for chemistry Dynamic; extract match scores computed; manual evaluation scores summarized per experiment in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Manual adjudication of ambiguous cases and all Experimental Data scoring; humans also validated GPT-4 generated distractors and simplified answers during dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>MSE handling of missing numeric outputs (penalty large) may bias comparisons; BLEU imperfect for scientific string matching (e.g., SMILES) though extract-match used to complement; manual scoring not standardized across raters in paper text (no inter-rater reliability reported).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>No generative-theory-specific metric (e.g., novelty, falsifiability, or explanatory power) is defined—evaluation focuses on factual correctness, computation, and human-judged experimental reasoning quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported metrics show wide variance across models and domains: accuracy shows GPT-4 top on Static Data; BLEU and MSE show GPT-4 best on average chemistry dynamic but other models (Galactica) excel on some computational tasks; manual scores indicate models are better at principles/design than at experimental result analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring Massive Multitask Language Understanding <em>(Rating: 2)</em></li>
                <li>Holistic Evaluation of Language Models (HELM) <em>(Rating: 2)</em></li>
                <li>AGIEval: A human-centric benchmark for evaluating foundation models <em>(Rating: 2)</em></li>
                <li>C-EVAL: A multilevel multi-discipline Chinese evaluation suite for foundation models <em>(Rating: 2)</em></li>
                <li>MultiMedQA <em>(Rating: 2)</em></li>
                <li>Chem-LLMBench: A comprehensive benchmark on chemistry tasks <em>(Rating: 2)</em></li>
                <li>MATH: Measuring mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
                <li>SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models <em>(Rating: 2)</em></li>
                <li>ScienceQA <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3816",
    "paper_id": "paper-261214653",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "SciEval",
            "name_full": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
            "brief_description": "A multi-disciplinary English benchmark introduced in this paper to evaluate LLM scientific research ability across four dimensions (Basic Knowledge, Knowledge Application, Scientific Calculation, Research Ability) using ~18k questions spanning biology, chemistry, and physics, and composed of Static, Dynamic, and Experimental subsets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Ability across four knowledge dimensions mapped to Bloom's taxonomy: Basic Knowledge (recall/understanding), Knowledge Application (comprehension/application/analysis), Scientific Calculation (numerical reasoning and computation), Research Ability (problem formulation, experimental design, data analysis, summarization).",
            "evaluation_methods": "Automated scoring against ground-truth for objective questions; dynamic data generation from scientific principles to avoid data leakage; chain-of-thought and answer-only prompting; few-shot (3-shot) evaluation; manual assessment for open-ended experimental questions; use of GPT-4 to convert and curate community Q&A into multiple-choice and generate distractors with human manual checking.",
            "benchmark_or_dataset": "SciEval benchmark (introduced here) composed of three subsets: Static Data (pre-collected objective Qs from community Q&A and public datasets), Dynamic Data (regularly regenerated questions based on scientific principles for chemistry and physics to mitigate data leakage), and Experimental Data (subjective/open-ended questions from 12 basic lab experiments to evaluate research ability).",
            "metrics_reported": "Accuracy for objective multiple-choice questions (Static Data and physics Dynamic subset); BLEU (and extract match) for string answers in chemistry Dynamic subset; Mean Square Error (MSE) for numeric answers in chemistry Dynamic subset; manual/qualitative scoring for Experimental Data; comparisons of model performance under Answer-Only, Chain-of-Thought (CoT), and 3-shot settings.",
            "human_involvement": "High — humans manually check GPT-4-generated question/answer/distractor conversions and classify domains; Experimental Data responses are evaluated manually; exemplar selection for few-shot uses dev data; models with RLHF are included but RLHF is not an evaluation method per se.",
            "limitations_or_challenges": "Risk of data leakage from pretraining corpora (addressed by dynamic data); objective-only benchmarks insufficient to evaluate research skills; subjective Experimental Data requires manual scoring (time-consuming, subjective); many models struggle with calculation-heavy items (especially physics), producing near-random results in some dynamic physics tests; CoT capability varies across models; token-length limits constrain evaluation of long/exhaustive experimental prompts for some models.",
            "llm_theory_example": "No explicit LLM-generated 'scientific theory' is reported; closest examples are model-generated experimental designs, principles, and analysis in Experimental Data (open-ended answers produced by models such as GPT-4 and Claude, evaluated manually).",
            "evaluation_results": "Across SciEval, GPT-4 attains the strongest performance (state-of-the-art among evaluated LLMs); only GPT-4, GPT-3.5-turbo, and Claude-v1.3 exceed ~60% average accuracy on Static Data. On Dynamic Data GPT-4 leads on average accuracy and BLEU for chemistry but performs poorly on some physics Dynamic items; Galactica models perform relatively better on computational problems. Chain-of-Thought improves GPT-series performance; 3-shot improves many models but physics Dynamic subset frequently remains near-random for most models. Experimental Data: top-tier models handle experimental principles and design reasonably well but generally fail at analyzing experimental results.",
            "uuid": "e3816.0",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Static Data",
            "name_full": "SciEval Static Data subset",
            "brief_description": "A fixed collection of objective questions (~15.9k referenced in paper) drawn from community Q&A (Socratic), MedQA, PubMedQA, Reagent Selection and processed into multiple-choice, judgment, or fill-in-the-blank items for fast, reproducible automated evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Correctness of factual knowledge and application as per Basic Knowledge, Knowledge Application, and Scientific Calculation dimensions (mapped to Bloom's taxonomy levels).",
            "evaluation_methods": "Automated matching of model outputs to ground-truth labels for multiple-choice and judgment questions; split into dev/valid/test; 3-shot dev exemplars for few-shot evaluations; evaluated under Answer-Only and Chain-of-Thought prompts.",
            "benchmark_or_dataset": "Static Data — fixed objective subset of SciEval compiled from community Q&A and public datasets, used for reliable cross-model comparisons.",
            "metrics_reported": "Accuracy (%) per discipline (biology/chemistry/physics) and averaged across domains; tables report model-wise accuracies and averages under AO/CoT/3-shot settings.",
            "human_involvement": "Human involvement primarily in data curation: manual checking of GPT-4-generated distractors and domain classification; otherwise evaluation automated.",
            "limitations_or_challenges": "Static data is susceptible to data leakage if models were trained on the sources (mitigated by inclusion of Dynamic Data); objective format insufficient to evaluate higher-level research skills; reliance on GPT-4 for question reformulation introduces potential bias and requires manual verification.",
            "llm_theory_example": "Not applicable — Static Data is objective Q/A rather than generative theory outputs.",
            "evaluation_results": "Results show GPT-4 highest accuracy; only GPT-4, GPT-3.5-turbo, and Claude-v1.3 exceed ~60% average accuracy on Static Data; many open-weight or smaller LLMs perform substantially worse, indicating substantial room for improvement.",
            "uuid": "e3816.1",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Dynamic Data",
            "name_full": "SciEval Dynamic Data subset",
            "brief_description": "A regularly regenerated subset designed to avoid data leakage by programmatically generating questions from scientific principles: chemistry items use molecular properties and computations; physics items are generated using physics formulas via scripts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Knowledge Application and Scientific Calculation abilities measured by computed/numerical answers, generation of structured string outputs (e.g., SMILES), and multiple-choice physical reasoning.",
            "evaluation_methods": "Automatic generation of question instances from scientific rules/formulae; evaluate models under AO/CoT/3-shot; for chemistry use numeric/string scoring; for physics use multiple-choice accuracy.",
            "benchmark_or_dataset": "Dynamic Data — chemistry (~2000 items) and physics (~890 items) programmatically generated; stable snapshot maintained for fair comparisons plus growing/rotating regeneration to prevent leakage.",
            "metrics_reported": "Chemistry dynamic: Mean Square Error (MSE) for numeric outputs, BLEU and extract-match for string outputs; Physics dynamic: accuracy for 4-choice questions; average accuracy and BLEU scores reported per model; default extremely large MSE assigned when no numeric predicted (1e10).",
            "human_involvement": "Minimal in generation (scripted), but human maintainers regenerate and publish stable snapshots; some manual oversight to ensure correctness of generation scripts and regenerated data.",
            "limitations_or_challenges": "Despite generation, many LLMs show near-random performance on physics dynamic subset (accuracy ~25% baseline); chemistry numeric/string evaluation needs mixed metrics (MSE, BLEU) complicating aggregate scoring; large default MSE penalizes nonnumeric responses; models often lack molecule-specific knowledge causing poor chemistry performance.",
            "llm_theory_example": "Not a theory; dynamic items test model calculation and representation generation (e.g., 'molecular weight of A?' or 'SMILES expression of B?').",
            "evaluation_results": "GPT-4 obtains best average accuracy and BLEU on Dynamic Data, but performance on counting/calculation items varies—Galactica-30B performs strongly on certain computation tasks; many models have low chemistry performance and near-random physics accuracy. CoT and 3-shot improve chemistry subset substantially for many models; physics subset improvements are limited.",
            "uuid": "e3816.2",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Experimental Data",
            "name_full": "SciEval Experimental Data subset (12 basic experiments)",
            "brief_description": "A subjective/open-ended subset containing questions derived from 12 basic laboratory experiments designed to probe Research Ability: experimental principle, process/design, and analysis/summarization of results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Research ability assessed via correctness and quality of experimental principle explanation, experimental design appropriateness, and analytic interpretation of experimental results (higher-order cognitive tasks: Analyze, Evaluate, Create).",
            "evaluation_methods": "Open-ended prompts answered by models and evaluated manually by humans; longer context inputs may be used (but some models limited by context length).",
            "benchmark_or_dataset": "Experimental Data — set of open-ended questions per experiment used to evaluate higher-level research skills, requiring manual scoring.",
            "metrics_reported": "Qualitative manual scores per experiment and per model (detailed scores in appendices); aggregated statements about strengths (principle and design) versus weaknesses (result analysis).",
            "human_involvement": "High — responses are assessed manually; some models were only evaluated via web interfaces due to access restrictions and manual scoring was required to judge open-ended answers.",
            "limitations_or_challenges": "Manual scoring is subjective and resource-intensive; long responses exceed context limits of many models, limiting evaluation coverage; models generally perform poorly on analysis of experimental results even when they can describe principles and designs.",
            "llm_theory_example": "Models produced experimental principle descriptions and proposed experimental designs (these are closest to generated scientific hypotheses/theories in the paper), but no fully-formed novel scientific theory is reported or validated.",
            "evaluation_results": "GPT-series and Claude-series models perform relatively well on experimental principle and design questions, but almost all models struggle to analyze experimental results; other models evaluated via web (ERNIE Bot, SparkDesk) performed less well overall.",
            "uuid": "e3816.3",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Evaluation Settings",
            "name_full": "Evaluation prompting and shot settings (Answer-Only, Chain-of-Thought, 3-Shot)",
            "brief_description": "Three protocol variants used to probe model reasoning and robustness: Answer-Only (AO) forces a single final answer, Chain-of-Thought (CoT) elicits intermediate reasoning steps, and 3-Shot provides three in-context exemplars from dev set for few-shot learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Improvement or degradation in final task metrics (accuracy, BLEU, MSE) under different prompting regimes; CoT aims to reveal and improve internal reasoning leading to higher-quality outputs.",
            "evaluation_methods": "Compare model outputs and metrics across AO, CoT, and 3-shot settings; report per-model delta indicators (↑/↓/∼) for Dynamic Data and full metric tables for Static Data.",
            "benchmark_or_dataset": "Applied across SciEval subsets (Static, Dynamic, Experimental where applicable); exemplars for 3-shot selected from dev set.",
            "metrics_reported": "Change in accuracy, BLEU, and other metrics between settings (e.g., CoT increases accuracy for GPT-series on Static Data; 3-shot yields highest Dynamic physics accuracy for GPT-4 in one result).",
            "human_involvement": "Human curation of 3-shot exemplars from dev set; manual inspection of CoT outputs when needed; but metric computation remains automated for objective items.",
            "limitations_or_challenges": "Not all models reliably support CoT; CoT benefits concentrated in GPT-series; some models have limited or low-quality CoT generation; CoT sometimes leads to incorrect application of formulas (GPT-4 used wrong formulas on some physics problems under CoT).",
            "llm_theory_example": "No explicit generated scientific theories, but CoT outputs reveal models' stepwise reasoning when they attempt experimental analysis or calculations.",
            "evaluation_results": "CoT improved performance mainly for GPT-series models on Static Data; 3-shot often improved roughly half of the evaluated LLMs versus Answer-Only; on Dynamic chemistry CoT and 3-shot substantially improved many LLMs, while physics Dynamic subset usually remained difficult except isolated gains (e.g., GPT-3.5-turbo under CoT achieved 47.19% on physics dynamic in one reported condition).",
            "uuid": "e3816.4",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Metrics & Scoring",
            "name_full": "Automated and manual metrics used in SciEval",
            "brief_description": "Set of quantitative and qualitative metrics used to evaluate model outputs across SciEval: accuracy for multiple-choice/judgement, BLEU and extract-match for string outputs, MSE for numeric outputs, and manual scoring for experimental open-ended responses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Correctness (accuracy), closeness to reference strings (BLEU, extract match), numeric error magnitude (MSE), and human-judged quality for research-style open answers.",
            "evaluation_methods": "Automated computation of accuracy, BLEU, and MSE for objective/dynamic items; assign extremely large MSE (1e10) when numeric prediction absent; manual scoring protocols for Experimental Data (not fully formalized in paper).",
            "benchmark_or_dataset": "Metrics applied to Static Data, Dynamic Data (chemistry/physics), and Experimental Data (manual).",
            "metrics_reported": "Per-model accuracies per domain and averaged; BLEU and MSE reported for chemistry Dynamic; extract match scores computed; manual evaluation scores summarized per experiment in appendices.",
            "human_involvement": "Manual adjudication of ambiguous cases and all Experimental Data scoring; humans also validated GPT-4 generated distractors and simplified answers during dataset construction.",
            "limitations_or_challenges": "MSE handling of missing numeric outputs (penalty large) may bias comparisons; BLEU imperfect for scientific string matching (e.g., SMILES) though extract-match used to complement; manual scoring not standardized across raters in paper text (no inter-rater reliability reported).",
            "llm_theory_example": "No generative-theory-specific metric (e.g., novelty, falsifiability, or explanatory power) is defined—evaluation focuses on factual correctness, computation, and human-judged experimental reasoning quality.",
            "evaluation_results": "Reported metrics show wide variance across models and domains: accuracy shows GPT-4 top on Static Data; BLEU and MSE show GPT-4 best on average chemistry dynamic but other models (Galactica) excel on some computational tasks; manual scores indicate models are better at principles/design than at experimental result analysis.",
            "uuid": "e3816.5",
            "source_info": {
                "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring Massive Multitask Language Understanding",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Holistic Evaluation of Language Models (HELM)",
            "rating": 2,
            "sanitized_title": "holistic_evaluation_of_language_models_helm"
        },
        {
            "paper_title": "AGIEval: A human-centric benchmark for evaluating foundation models",
            "rating": 2,
            "sanitized_title": "agieval_a_humancentric_benchmark_for_evaluating_foundation_models"
        },
        {
            "paper_title": "C-EVAL: A multilevel multi-discipline Chinese evaluation suite for foundation models",
            "rating": 2,
            "sanitized_title": "ceval_a_multilevel_multidiscipline_chinese_evaluation_suite_for_foundation_models"
        },
        {
            "paper_title": "MultiMedQA",
            "rating": 2,
            "sanitized_title": "multimedqa"
        },
        {
            "paper_title": "Chem-LLMBench: A comprehensive benchmark on chemistry tasks",
            "rating": 2,
            "sanitized_title": "chemllmbench_a_comprehensive_benchmark_on_chemistry_tasks"
        },
        {
            "paper_title": "MATH: Measuring mathematical problem solving with the math dataset",
            "rating": 2,
            "sanitized_title": "math_measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
            "rating": 2,
            "sanitized_title": "scibench_evaluating_collegelevel_scientific_problemsolving_abilities_of_large_language_models"
        },
        {
            "paper_title": "ScienceQA",
            "rating": 1
        }
    ],
    "cost": 0.01152475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</p>
<p>Liangtai Sun 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Yang Han csyanghan@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zihan Zhao 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Da Ma 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Zhennan Shen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Baocai Chen 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Lu Chen chenlusz@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Kai Yu kai.yu@sjtu.edu.cn 
Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>Department of Computer Science and Engineering MoE Key Lab of Artificial Intelligence
LANCE Lab
AI Institute
Shanghai Jiao Tong University Shanghai Jiao Tong University
ShanghaiChina</p>
<p>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research
8DE5F8747D39882235665D42A59CA88C
Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research.Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research.However, current benchmarks are mostly based on pre-collected objective questions.This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability.In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues.Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability.In particular, we design a "dynamic" subset based on scientific principles to prevent evaluation from potential data leakage.Both objective and subjective questions are included in SciEval.These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs.Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions.The codes and data are publicly available on https://github.com/OpenDFM/SciEval.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), such as ChatGPT (Schulman et al. 2022), have attracted widespread attention in general scenarios, including information search, code generation, and more.In the field of science, LLMs have also shown preliminary potential in improving scientific research efficiency and transforming scientific research paradigms (Blanco-Gonzalez et al. 2023;WANG and MIAO 2023).In the meanwhile, several scientific LLMs have been proposed by researchers (Taylor et al. 2022;Luo et al. 2022;Frey et al. 2022).In the general field, there are already numerous evaluation benchmarks to evaluate the language understanding, language generation and reasoning capabilities of LLMs, such as MMLU (Hendrycks et al. 2020), AGIEval (Zhong et al. 2023), and C-EVAL (Huang et al. 2023), shown in Table 1.Although these benchmarks cover data of science domain, the data sources are usually confined to educational materials, which can not adequately as-sess the research ability of LLMs and not align with real-life scientific research scenarios.In addition, some benchmarks have been proposed to evaluate the scientific capability of LLMs, such as MultiMedQA (Singhal et al. 2023), Chem-LLMBench (Guo et al. 2023), and MATH (Hendrycks et al. 2021), while these benchmarks are restricted to a specific scientific discipline, leaving a lack of a more general scientific evaluation benchmark. 1In addition, these benchmarks (1) lack evaluation systems for scientific capabilities, (2) are all based on objective questions, which are insufficient to assess scientific abilities, and (3) face the risk of data leakage.</p>
<p>In response to this gap, we present SciEval, an English benchmark designed to evaluate advanced abilities of LLMs in the scientific domain.SciEval consists of a total of about 18000 challenging scientific questions, spanning three important basic science fields: chemistry, physics and biology, each of which is further divided into multiple sub-topics.SciEval mainly has the following three characteristics:</p>
<p>• Multi-level and comprehensive evaluation of the ability of LLMs in the scientific field.Scientific ability of LLMs needs to be evaluated from multiple aspects.Leveraging cognitive domains of Bloom's taxonomy (Krathwohl 2002;Forehand 2010), which covers six levels, SciEval evaluates the scientific capabilities of large language models across four dimensions: basic knowledge, knowledge application, scientific calculation, and research ability, where each capability aligns with one or more cognitive levels.• Combination of objective and subjective questions.</p>
<p>SciEval is mainly based on objective questions, which allow for quick and standard model evaluations, involving multiple-choice, fill-in-the-blank, and judgment questions.These questions can help us understand whether the model can correctly understand and memorize scientific knowledge.However, objective questions are insufficient to assess scientific capability holistically.To better assess scientific reasoning and application ability, SciEval introduces a small number of subjective questions, involving a total of twelve basic science experiments, which is named Experimental Data.We conduct experiments to evaluate LLMs on SciEval in answer-only, chain-of-thought and few-shot settings.Results indicate that GPT-4 is the strongest model, with only GPT-4, GPT-3.5-turbo and Claude-v1.3surpassing 60% average accuracy on Static Data, signifying considerable opportunities for improvement.With the results of Dynamic Data, we find that these LLMs have little knowledge about molecules, and most models could only retain near-random accuracy in the physics subset.As for Experimental Data, some top-tier models could perform satisfactorily in experimental principle and design, while almost all models struggle to analyze the experimental results.With the analysis of experiment results, we claim that training on large-scale scientific corpus is helpful for the scientific ability of LLMs, and most LLMs perform bad on calculation problems, especially in physics domain.We hope SciEval can provide an excellent benchmark for the assessment of scientific capability of LLMs, and promote wide application in science.</p>
<p>Related Work</p>
<p>General Benchmarks for LLMs</p>
<p>To evaluate the performance of LLMs across different tasks, several benchmarks have been proposed.MMLU (Hendrycks et al. 2020) aims to develop a comprehensive test for evaluating text models in multi-task contexts.HELM (Liang et al. 2022) offers a comprehensive assessment, evaluating LLMs across various aspects, such as language understanding and common-sense reasoning.Big-Bench (Srivastava et al. 2022) introduces 204 challenging tasks covering various domains, aiming to evaluate tasks beyond the capabilities of existing language models.AGIEval (Zhong et al. 2023) serves as an evaluation framework for assessing the performance of foundation models in human-centric standardized exams.C-Eval (Huang et al. 2023) assesses the advanced knowledge and reasoning capabilities of foundation models in Chinese.</p>
<p>Specific Benchmarks for LLMs</p>
<p>Apart from general tasks, specific benchmarks are designed for certain downstream tasks.MultiMedQA (Singhal et al. 2023) focuses on medical question-answering, evaluating LLMs in terms of clinical knowledge and QA abilities.MATH (Hendrycks et al. 2021) assesses reasoning and problem-solving proficiencies of LLMs in mathematics.Sci-enceQA (Lu et al. 2022) proposes a multi-modal benchmark with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations, collected from elementary and high school science curricula.SCIBENCH (Wang et al. 2023) examines the reasoning capabilities required for complex scientific problem-solving and proposes two datasets of college-level scientific problems.Compared to these benchmarks, SciEval (1) evaluates scientific capabilities from multiple aspects, having a broader coverage, (2) uses data of community Q&amp;A, which is more flexible and diverse, (3) designs a subset of dynamic data, making an effort to mitigate data leakage.</p>
<p>3 The SciEval Dataset cognitive domain is frequently used to structure curriculum learning objectives, assessments and activities, and is broken into six levels: Remember, Understand, Apply, Analyze, Evaluate and Create, as is shown in Figure 1, which are suitable for the evaluation of scientific capability.</p>
<p>Based on the cognitive domain of Bloom's taxonomy, the evaluation system of SciEval consists of four knowledge dimensions: Basic Knowledge (BK), Knowledge Application (KA), Scientific Calculation (SC), and Research Ability (RA).As is shown in Figure 1, BK primarily assesses the fundamental scientific knowledge of LLMs.KA focuses on how to apply basic knowledge to solve scientific problems, requiring models to have comprehension, application, and analysis abilities.SC is a specialized application of knowledge that further examines complex reasoning capabilities of LLMs based on their general knowledge application abilities.RA assesses evaluation capabilities at a higher cognitive level, requiring models to participate in various aspects of scientific research, including problem formulation, experimental design, data analysis, and summarization.</p>
<p>Based on the evaluation system, we design three different types of data: Static Data, Dynamic Data, and Experimental Data.The Static Data covers all these four knowledge dimensions and will remain constant throughout, while the Dynamic Data examines from the aspects of Knowledge Application and Scientific Calculation and will be regularly updated to prevent any data leakage.The Experimental Data comprises a set of questions for twelve scientific experiments and can be used to evaluate the Research Ability.</p>
<p>Data Collection</p>
<p>Static Data The collection steps of Static Data are shown in Figure 2. The primary source of Static Data is Socratic Q&amp;A 2 , a community-driven website that covers a wide range of subjects such as science and literature.Specifically, we collect data from the fields of biology, chemistry, and physics.To ensure quality, we employ rule-based methods 2 https://socratic.org to preprocess the crawled data.While gathering the questions, we found that not all of them are suitable as titles.To address this, we utilize GPT-4 with the "Task 1" prompt, as depicted in Figure 2, to process these questions.Since most of the collected questions are open-ended and challenging to evaluate, we employ GPT-4 to simplify ground-truth answers and generate three wrong answers to formulate them as multiple-choice questions.Additionally, we classify the questions into their respective knowledge domains.And during this process, we manually check the generated content of GPT-4 to ensure data quality.</p>
<p>To make the dataset more diverse and comprehensive, we further integrate data from some publicly available datasets:</p>
<p>• MedQA (Jin et al. 2021) is a free-form multiple-choice OpenQA dataset for solving medical problems, collected from professional medical board exams.We use the test set of USMLE, which is the English subset of MedQA.</p>
<p>• PubMedQA (Jin et al. 2019) is a biomedical questionanswering dataset collected from PubMed abstracts.The task of PubMedQA is to answer research questions with yes/no/maybe using the corresponding abstracts, which is fit for evaluating the literature comprehension ability.We incorporate 1000 expert-annotated data from it and frame them as judgment questions.</p>
<p>• Reagent Selection (Guo et al. 2023) involves the identification and proposal of the most fitting reagents for a specific chemical reaction or process, which is a subset of ChemLLMBench.We randomly select 40% data and formulate them as multiple-choice questions.</p>
<p>Dynamic Data</p>
<p>The current training of LLMs often uses a large amount of data, resulting in a risk of data leakage for evaluation.In order to solve this problem, we design a "dynamic" subset, which can generate data dynamically according to scientific principles.The dynamic subset covers two disciplines, chemistry and physics.For chemistry data, we use the basic information and properties of molecules</p>
<p>The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)</p>
<p>Socratic Q&amp;A</p>
<p>Crawl &amp; Filter</p>
<p>Raw Data</p>
<p>GPT-4 Filtered Data</p>
<p>Instruction: Given a question and its ground-truth answer, judge whether it is suitable to be used as the title of a multiple-choice question.Your answer should be "YES" or "NO".And please directly give the results without any explanation.</p>
<p>Task 1</p>
<p>Instruction: Given a question and a ground-truth answer, please simplify the answer as concise as possible.And I want to generate a 4-choice question using it, please generate 3 fake answers for me.Note that the length of the simplified answer and these 3 fake answers should be about the same and these 3 fake answers should be as confusing as possible.Furthermore, please help me to classify the domain of the question.There are three domains in total: Base Knowledge, Scientific Calculation, Knowledge Application.For physics data, we manually write some Python scripts according to the physics formulas.When obtaining the evaluation dataset, we will provide a regenerated version to users and we will update it regularly, while at the same time, we will maintain a stable version of the dynamic data to make a fair comparison.</p>
<p>Experimental Data To better evaluate the scientific thoughts and abilities of LLMs, SciEval introduces a subset of experimental data, involving 12 different basic scientific experiments.These experiments are collected from basic science experiment courses at university, and each experiment conducts a comprehensive investigation of the ability of LLMs in scientific research and experimentation from the perspectives of experimental principle, process, and analysis and summarization of experimental results.</p>
<p>Data Statistics</p>
<p>Summarized statistics are shown in For Static Data, we further split the data into dev, valid, and test set.For each data source, each knowledge domain, and each discipline, we randomly select 5 data to form the 3 https://pubchem.ncbi.nlm.nih.gov/dev set, which can be used for few-shot learning, and we split the remaining data with a ratio of 1:9 to construct the valid set and test set respectively.</p>
<p>Experiment</p>
<p>Given a question and four options, please select the right answer.Your answer should be "A", "B", "C" or "D".Please directly give the answer without any explanation.The red text is the response from the model, while the black text is the inputted prompt.</p>
<p>Given a question and four options, please select the right answer.Your answer should be "A", "B", "C" or "D".How many atoms are in 3.5 moles of arsenic atoms?</p>
<p>Experiment Setup</p>
<p>Prompts We evaluate LLMs in both Answer-Only (AO) and Chain-Of-Thought (CoT) (Kojima et al. 2022) settings.The prompts we used are shown in Figures 3 and 4 respectively.Furthermore, we also evaluate using 3-shot setting, where the three exemplars are selected from the dev set.Models In order to comprehensively assess the scientific capabilities of Large Language Models (LLMs), we evaluate 15 high-performing LLMs that are widely accessible.These models are selected to represent a diverse range of organizations and vary in size.The details of these models are summarized in Table 3.</p>
<p>Model</p>
<p>• GPT-3.5-turbo and GPT-4 (Schulman et al. 2022;Ope-nAI 2023) are the strongest GPT model variants from OpenAI that have undergone pretraining, instruction tuning, and reinforcement learning from human feedback (RLHF, (Ouyang et al. 2022)).• Claude 4 , developed by Anthropic, is often considered 4 https://www.anthropic.com/index/introducing-claude.comparable to GPT-3.5-turbo.We evaluate both the Claude-v1.We evaluate GPT-3.5-turbo,GPT4 and Claude on all three subsets, including Static Data, Dynamic Data, and Experimental Data.Since we can only assess ERNIE Bot and SparkDesk through web interface, we evaluate these two models on the Experimental Data.And for the rest LLMs with billions or tens of billions of parameters, since the length of the Experimental Data exceeds the length limit of these models7 , we evaluate them on Static Data and Dynamic Data, as is shown in Table 3.</p>
<p>Evaluation Metrics In the case of Static Data, all questions are objective, making accuracy the appropriate evaluation metric.For Dynamic Data, the physics questions are presented as multiple-choice questions, which can also be evaluated using accuracy.Conversely, the chemistry questions involve complex components, such as "What is the</p>
<p>Experiment Results</p>
<p>Answer-Only Setting Answer-only results of all the models on the test set are shown in Table 4 and detailed results of Static Data across different knowledge domains are provided in Appendix B. Analyzing the results of Static Data, GPT-4 demonstrates significantly superior performance compared to other models.And only GPT-4, GPT-3.5-turbo, and Claude-v1.3achieve an average accuracy exceeding 60%, which highlights the challenge posed by SciEval.</p>
<p>For the results of Dynamic Data, GPT-4 performs the best in terms of average accuracy and BLEU score.However, for counting and calculation questions, Galactica-30B yields the best results, indicating its strong aptitude in the field of science.Conversely, models with billions or tens of billions of parameters perform poorly on the chemistry subset, suggesting their limited knowledge about molecules.Regarding the performance of models on the physics subset, since all questions are 4-choices questions, the accuracy should be at least 25%.However, none of these models achieve satisfactory results in this subset.</p>
<p>As for Experimental Data, GPT-series models and Claude-series models achieve good results, while the other two models are not.The detailed scores models reached in each experiment are shown in Appendix C.However, although some models could get a great performance, during experiments, we find that these models are good at experimental principles and designing, while when it comes to analyzing the experiment results, the performances are not satisfying.</p>
<p>CoT Setting and 3-Shot setting Comparison of experiment results among Answer-Only, Chain-of-Thought and 3-Shot settings are shown in Figure 5 and Table 5. 9 And we refer detailed results to Appendix A and B.</p>
<p>The experimental results from Static Data reveal that solely the GPT-series LLMs get performance enhancement within the CoT setting due to the limited CoT capabilities of other LLMs.As for the 3-Shot setting, roughly half of the LLMs analyzed demonstrate superior performances relative to the Answer-Only setting.The performances of the remaining LLMs are closely similar to those observed within the Answer-Only setting.</p>
<p>From the experimental results of Dynamic Data, it is observed that both CoT and 3-Shot significantly enhance the performance of most Language Learning Models (LLMs) in the chemistry subset.However, the performances achieved are still not up to the mark.In the physics subset, the impact of CoT and 3-Shot on most LLMs is less pronounced, resulting in nearly random performances.Under the CoT setting, GPT-3.5-turboachieves an accuracy of 47.19, suggesting a robust understanding of physical principles.Conversely, the performance of GPT-4 is markedly poor, from which we find that despite its extensive knowledge of physical principles, it frequently employs incorrect formulas to solve problems.Nevertheless, GPT-4 attains an accuracy of 51.01 under 3-Shot setting, the highest among all models, demonstrating its ability to learn from a mere three examples.</p>
<p>Discussion</p>
<p>Training on large-scale scientific corpus is helpful.Based on experimental results (Table 4), Galactica (Taylor et al. 2022), which has been trained on an extensive scientific corpus, significantly outperforms other LLMs with a comparable number of parameters, although Galactica is trained with a much smaller amount of data.Remarkably, when tested on Dynamic Data, Galactica surpasses the GPTseries and Claude-series LLMs in computational problems.</p>
<p>Most LLMs perform bad on calculation problems, especially in physics domain.Detailed results across various knowledge domains on Static Data (refer to Appendix B) reveal that most LLMs underperform in the Scientific Calculation domain, while demonstrate relatively superior performance in other domains, which is particularly acute in the field of physics.Similar issues are also observed in Dynamic Data and Experimental Data.In the context of Dynamic Data, the mean square error, employed to evaluate calculation abilities within the chemistry subset, is exceedingly high for most LLMs, and almost all LLMs can only achieve nearly random performance within the physics subset.Regarding Experimental Data, our findings indicate that these LLMs struggle with the analysis of experimental results.</p>
<p>Conclusion</p>
<p>In this paper, we introduce SciEval, a benchmark designed to evaluate scientific capabilities of LLMs.SciEval comprises about 18,000 challenging scientific questions, covering three fundamental fields of science.SciEval assesses the scientific ability of LLMs across four dimensions.It incorporates both objective and subjective questions, and employs dynamic data generation to mitigate potential data leakage.We conduct comprehensive experiments on various advanced LLMs using SciEval and perform thorough analyses.Our experimental results reveal that most LLMs do not perform well on our benchmark, with the exception of the GPT-series and Claude-series LLMs.We hope that SciEval can serve as a robust benchmark for assessing scientific capabilities of LLMs.</p>
<p>Figure 1 :
1
Figure 1: The illustration of the evaluation system.SciEval covers three disciplines with amounts of sub-topics, and investigates four abilities, corresponding to six cognitive levels.</p>
<p>Figure 2 :
2
Figure 2: Data Collection steps of Static Data</p>
<p>Figure 3 :
3
Figure 3: An example of the prompt we used for AO setting.The red text is the response from the model, while the black text is the inputted prompt.</p>
<p>AFigure 4 :
4
Figure 4: An example of the prompt we used for CoT setting.The red text is the response from the model, while the blue text and black text are the inputted prompt.</p>
<p>Figure 5: Accuracy on Answer Only, Chain-of-Thought and 3-Shot settings of each LLMs for Static Data.</p>
<p>Table 1 :
1
Dataset comparison of SciEval and some other datasets covering science domain."BK"stands for Basic Knowledge, "KA" stands for Knowledge Application, "SC" stands for Scientific Calculation, and "RA" stands for Research Ability.
NameCategoryAbilitySourceData Type Dynamic #DataMMLUhumanities, social science, STEM, otherBK, KA, SCexam, book, course objective14079AGIEvalsocial science, STEM BK, KA, SCexamobjective8062C-EVALhumanities, social science, STEM, otherBK, KA, SCexamobjective12342MultiMedQAmedicalBK, KA, RAexam, researchobjective13115ChemLLMBench chemistryBK,KAknowledge baseobjective800MATHmathematicsSCexamobjective5000SciEvalscienceBK, KA,SC, RAcommunity QA, knowledge baseobjective + subjective15901model perfor-mance. And the objective questions other than DynamicData are referred to as Static Data.
• Dynamic data generation based on basic scientific principles.The huge amount of training data used for pre-training LLMs may cause the risk of data leakage for evaluation.In order to solve this problem, one of the main features of SciEval is the use of Dynamic Data, which can prevent potential data leakage and ensure the fairness and credibility of the evaluation results.The Dynamic Data will be updated regularly, and we will maintain a stable version to make a fair comparison of</p>
<p>Table 2
2, where we onlycount Static Data. For Dynamic Data, the chemistry part ex-amines the KA ability and contains 2000 data, while thephysics part evaluates the SC ability and involves 890 data.All these questions are in English and we show some dataexamples in Appendix D.AbilityBioChem PhyBasic Knowledge2147 2914456Knowledge Application 1379 372036Scientific Calculation3013401 1165Research Ability100000Total4830 10035 1657</p>
<p>Table 2 :
2
Statistics of Static Data</p>
<p>Table 3 :
3
Models evaluated in this paper.The "access" columns show whether we have full access to the model weights or we can only access through API or web.SD stands for Static Data, DD stands for Dynamic Data, and ED stands for Experimental Data.Marking " " means we evaluate the corresponding model on this subset.
Creator#Parameters Access SD DD EDGPT-4OpenAIundisclosedAPIGPT-3.5-turboOpenAIundisclosedAPIClaude-v1.3AnthropicundisclosedAPIClaude-instant-v1.1 AnthropicundisclosedAPIERNIE BotBaiduundisclosedWebSparkDeskiFLYTEKundisclosedWebVicunaLMSYS13BWeightsGalacticaMeta30B, 6.7BWeightsChatGLM2Tsinghua6BWeightsChatGLMTsinghua6BWeightsAlpacaStanford7BWeightsMOSSFudan16BWeightsLLaMaMeta7B, 13BWeightsModelStatic Data Biology Chemistry Physics Avg.Chemistry(DD) Acc. BLEU MSEPhysics(DD) Exp Acc. ScoreGPT-484.4969.3865.2273.93 11.05 23.78891.0925.8493.31GPT-3.5-turbo76.4264.3052.3066.97 7.6518.862008.7221.8088.27Claude-v1.372.5859.7254.9463.45 5.7521.981489.8726.1485.73Claude-instant-v1.170.4353.3652.3058.92 0.4516.078258.4621.4687.50Galactica-30B66.4850.1644.6554.960.94.14485.9922.47-Vicuna-13B58.3953.0645.1353.93 0.956.50766.6421.24-Galactica-6.7B57.8450.7730.9950.87 1.556.475519.8220.79-ChatGLM2-6B58.6244.0040.2648.440.21.863449.4424.83-ChatGLM-6B52.5445.3640.8047.23 0.752.4410303.9021.01-Alpaca-7B56.6642.4337.0146.540.22.92428419.2726.74-MOSS-16B47.7133.8731.7338.230.17.3730505.1724.27-LLaMa-13B48.5933.5619.4836.960.35.213707.017.08-LLaMa-7B36.2426.3815.0228.370.51.2611305.6514.38-ERNIE Bot--------61.12SparkDesk--------33.69</p>
<p>Table 4 :
4
Model performances of Answer-Only setting.The leaderboard is sorted by the average accuracy of Static Data.</p>
<p>Table 5 :
5
Results on Answer-Only, Chain-of-Thought and 3-Shot settings of each LLM for Dynamic Data.↑ means the performance is slightly better than that under Answer-Only setting, ↓ means worse, and ∼ means the performance is nearly the same.
ModelAOChemistry CoT3-ShotAOPhysics CoT3-ShotGPT-411.05 11.65 ↑ 12.42↑ 25.84 17.98 ↓ 51.01 ↑GPT-3.5-turbo7.65 10.20 ↑ 8.85 ↑ 21.80 47.19 ↑ 25.39 ∼Galactica-6.7B1.551.75 ↑3.05 ↑ 20.79 23.37 ∼ 21.12 ∼Vicuna-13B0.951.95 ↑1.80 ↑ 21.24 18.65 ∼ 23.37∼Galactica-30B0.902.60 ↑3.30 ↑ 22.47 14.72 ↓ 22.58 ∼ChatGLM-6B0.750.80 ↑1.15 ↑ 21.01 25.39 ∼ 23.37 ∼LLaMa-7B0.500.10 ↓1.55 ↑ 18.659.66 ↓27.53 ↑LLaMa-13B0.300.25 ∼ 2.11 ↑7.085.84 ∼22.70 ↑ChatGLM2-6B 0.202.65 ↑1.60 ↑ 24.83 25.39 ∼ 26.74 ∼Alpaca-7B0.200.65 ↑2.10 ↑ 26.71 28.43 ∼ 25.62 ∼MOSS-16B0.100.85 ↑0.65 ↑ 24.27 25.06 ∼ 26.40 ∼
Due to the page limitation, we only compare some widely used benchmarks. For more information, we refer to(Chang et al.<br />
).The Thirty-Eighth AAAI Conference on Artificial Intelligence 
Scientific research requires different dimensions of knowledge, such as understanding and calculation, thence evaluation of scientific ability should be conducted at multiple levels. Bloom's taxonomy is a set of three hierarchical methods used for classification of educational learning objectives covering cognitive, affective and psychomotor domains. TheThe Thirty-Eighth AAAI Conference on Artificial Intelligence 
The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
https://yiyan.baidu.com/
https://xinghuo.xfyun.cn/ The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
The maximum context length of ChatGLM2 is extended to 32k, while it has limited ability to understand long texts. molecular weight of A?" and "What is the SMILES expression of B?". Hence, for questions with numerical answers, we employ
MSE 8 as the evaluation metric, while for questions with string answers, we utilize the BELU score(Papineni et al. 2002). Additionally, we also calculate the extract match scores. As for Experimental Data, each experiment consists of multiple open-ended questions. As a result, we assess the model-generated responses manually.
If the predictions do not contain any number, we will regard the MSE as 1 × 10 10The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
When evaluating on CoT and 3-Shot settings, Claude-Instant and Claude are not available for us, due to the limitation of API.
AcknowledgementsThis work is funded by the China NSFC Projects (92370206, U23B2057, 62106142 and 62120106006) and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).
The role of ai in drug discovery: challenges, opportunities, and strategies. A Blanco-Gonzalez, A Cabezon, A Seco-Gonzalez, D Conde-Torres, P Antelo-Riveiro, A Pineiro, R Garcia-Fandino, Pharmaceuticals. 1668912023</p>
<p>Y Chang, X Wang, J Wang, Y Wu, K Zhu, H Chen, L Yang, X Yi, C Wang, Y Wang, arXiv:2307.03109A survey on evaluation of large language models. 2023arXiv preprint</p>
<p>GLM: General Language Model Pretraining with Autoregressive Blank Infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>M Forehand, Blooms taxonomy. Emerging perspectives on learning, teaching, and technology. 201041</p>
<p>N Frey, R Soklaski, S Axelrod, S Samsi, R Gomez-Bombarelli, C Coley, V Gadepally, Neural scaling of deep chemical models. 2022</p>
<p>What indeed can GPT models do in chemistry?. T Guo, K Guo, Z Liang, Z Guo, N V Chawla, O Wiest, X Zhang, arXiv:2305.18365A comprehensive benchmark on eight tasks. 2023arXiv preprint</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>C-eval: A multilevel multi-discipline chinese evaluation suite for foundation models. Y Huang, Y Bai, Z Zhu, J Zhang, J Zhang, T Su, J Liu, C Lv, Y Zhang, J Lei, arXiv:2305.083222023arXiv preprint</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, E Pan, N Oufattole, W.-H Weng, H Fang, P Szolovits, Applied Sciences. 111464212021</p>
<p>Q Jin, B Dhingra, Z Liu, W W Cohen, X Lu, arXiv:1909.06146Pubmedqa: A dataset for biomedical research question answering. 2019arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>A revision of Bloom's taxonomy: An overview. Theory into practice. D R Krathwohl, 200241</p>
<p>P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, arXiv:2211.09110Holistic evaluation of language models. 2022arXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>BioGPT: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Briefings in Bioinformatics. 2364092022</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, arXiv:2303.08774Advances in Neural Information Processing Systems. 202235Technical Report</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>ChatGPT: Optimizing language models for dialogue. J Schulman, B Zoph, C Kim, J Hilton, J Menick, J Weng, J F C Uribe, L Fedus, L Metz, M Pokorny, Nature. 2022. 2023Large language models encode clinical knowledge</p>
<p>A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>MOSS: Training Conversational Language Models from Synthetic Data. T Sun, X Zhang, Z He, P Li, Q Cheng, H Yan, X Liu, Y Shao, Q Tang, X Zhao, K Chen, Y Zheng, Z Zhou, R Li, J Zhan, Y Zhou, L Li, X Yang, L Wu, Z Yin, X Huang, X Qiu, R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, N Hambro, E Azhar, F , arXiv:2302.13971Novel Paradigm for AIdriven Scientific Research: From AI4S to Intelligent Science. G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, Goyal, 2023. 2023. 2022. 2023. 202338arXiv preprintLlama: Open and efficient foundation language models</p>
<p>X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, arXiv:2307.10635SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. 2023arXiv preprint</p>
<p>L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, arXiv:2306.05685Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. 2023arXiv preprint</p>
<p>Agieval: A humancentric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, A Saied, W Chen, N Duan, arXiv:2304.063642023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
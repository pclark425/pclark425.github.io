<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8504 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8504</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8504</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-bf790cb8a95da05dcafd6bf1a4d02efce784720b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bf790cb8a95da05dcafd6bf1a4d02efce784720b" target="_blank">ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Zoom Eye is proposed, a training-free, model-agnostic tree search algorithm tailored for vision-level reasoning that enables MLLMs to simulate human-like zooming behavior by navigating from root to leaf nodes in search of task-relevant visual evidence.</p>
                <p><strong>Paper Abstract:</strong> Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in vision-language understanding. Recently, with the integration of test-time scaling techniques, these models have also shown strong potential in visual reasoning. However, most existing reasoning approaches remain text-level in nature: MLLMs are prompted to explore various combinations of textual tokens via their underlying language model, while the visual input remains fixed throughout the reasoning process. This paradigm limits the model's ability to fully exploit rich visual information, particularly when dealing with images containing numerous fine-grained elements. In such cases, vision-level reasoning becomes crucial - where models dynamically zoom into specific regions of the image to gather detailed visual cues necessary for accurate decision-making. In this paper, we propose Zoom Eye, a training-free, model-agnostic tree search algorithm tailored for vision-level reasoning. Zoom Eye treats an image as a hierarchical tree structure, where each child node represents a zoomed-in sub-region of its parent, and the root corresponds to the full image. The algorithm enables MLLMs to simulate human-like zooming behavior by navigating from root to leaf nodes in search of task-relevant visual evidence. We experiment on a series of high-resolution benchmarks and the results demonstrate that Zoom Eye consistently improves the performance of multiple MLLMs by a large margin (e.g., InternVL2.5-8B increases by 15.71% and 17.69% on HR-Bench) and also enables small 3-8B MLLMs to outperform strong large models such as GPT-4o. Code: https://github.com/om-ai-lab/ZoomEye</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8504.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8504.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zoom Eye</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ZoomEye: Tree-based Vision-Level Reasoning Algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-free, model-agnostic tree search algorithm that treats an image as a hierarchical tree of zoomed subregions and uses an MLLM's token probabilities (prompted 'Yes'/'No' confidences) to rank nodes and decide stopping, enabling multi-step vision-level reasoning (zoom-in, backtrack, union of found patches) to answer visual questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Zoom Eye (applied to MLLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a single LLM but a search/meta-algorithm applied at test time on top of off-the-shelf multimodal LLMs (MLLMs). It uses the MLLM as a scorer/generator by prompting for three confidence queries and final answer generation; requires no additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Tree-based search (image-as-tree)', 'Prompted binary-confidence scoring (existing, latent, answering)', 'Guided zooming (local / global+local inputs)', 'Vision-level test-time scaling (increasing search depth/steps)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Zoom Eye represents the image as a quadtree of patches (nodes). For each node it queries the MLLM with three prompted binary questions to obtain (a) existing confidence c_e ('Is there o in this patch?'), (b) latent confidence c_l ('Could o be found by further zooming?'), and (c) answering confidence c_a ('Can you answer q now?'). A weighted priority W(depth) combines c_e and c_l to rank nodes; a stopping threshold τ on c_a ends search. Two search modes: type-1 (find single instance, stop when c_a ≥ τ) and type-2 (find all instances, collect nodes with c_e ≥ τ2). Final answer is generated by feeding the union patch(s) back to the MLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablations varied number of search steps (by lowering τ), varied number of sub-regions (4/9/16), and compared successful vs failed zoom cases. The study observes performance as a function of search depth/steps (vision-level test-time scaling) but does not compare different internal reasoning paradigms (e.g., CoT vs analogical).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>High-resolution visual understanding benchmarks: V* Bench (avg 2246x1582; attribute & spatial tasks), HR-Bench (8K average, sub-tasks FSP/FCP; plus 4K crops), and MME-RealWorld (real-world tasks across Monitoring, AD, OCR, Remote Sensing).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Applied to multiple MLLMs, Zoom Eye consistently improved accuracy: example deltas reported include LLaVA-v1.5-7B: V* Bench spatial 43.47 -> 83.45 (+40.48 pp), overall 56.57 -> 82.89 (+26.32 pp); Qwen2.5VL-3B: V* Bench 80.87 -> 88.70 (+7.83 pp); LLaVA-ov-7B: V* Bench +14.19 pp, HR-4K +6.63 pp, HR-8K +10.00 pp; InternVL2.5-8B: V* Bench 67.83 -> 86.09 (+18.26 pp), HR-4K +15.71 pp, HR-8K +16.25 pp. On MME-RealWorld many sub-tasks improved strongly (e.g., Person_color +20.22 pp; Motion_vehicle +29.11 pp) while some tasks decreased (e.g., RS/Position −12.95 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Zoom Eye enables MLLMs to locate fine-grained cues via multi-step zooms and backtracking, increasing robustness on dense high-resolution scenes. The authors observe a vision-level test-time scaling effect: performance improves as the number of search steps increases and then stabilizes. Failure modes include inability to relate positional relationships between global and cropped views (leading to declines on some position/orientation subtasks) and sensitivity to semantic fragmentation when fixed-size patching splits relevant cues.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Vision-level reasoning (Zoom Eye) supplements text-level reasoning by allowing active visual exploration; it is training-free, model-agnostic, and yields large consistent gains across high-resolution benchmarks. Exploring more zoom steps yields a test-time scaling benefit analogous to exploring more chain-of-thought tokens in text-only reasoning. The method is robust to patch granularity but limited by heuristic ranking/stopping and fixed patch partitioning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8504.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8504.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-v1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-v1.5 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal LLM that combines a vision encoder with an autoregressive LLM base (7B) using naive image preprocessing; used as a backbone MLLM in Zoom Eye experiments (Local input mode).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-v1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal large language model (7B parameters) that takes visual encoder outputs and text prompts to produce autoregressive text; in experiments it used naive image preprocessing (single-image local input mode).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Generative token-level reasoning (standard MLLM autoregressive generation)', 'Zoom Eye tree-based vision-level search (when adapted)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Out-of-the-box it uses the MLLM's generative capacity to answer from a fixed visual input. With Zoom Eye it is repeatedly prompted on cropped patches to produce 'Yes/No' confidences (c_e, c_l, c_a) and final answers — effectively coupling the LLM's textual reasoning with vision-level tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Applied Zoom Eye (Local input, τ=0.8, weighted depth function W) and measured performance across V* Bench and HR-Bench; ablations include number of sub-regions, search depth, and success vs failure of zooming.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>V* Bench (high-res attribute & spatial), HR-Bench 4K/8K</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Baseline: V* Bench spatial 43.47, overall 56.57; With Zoom Eye: V* Bench spatial 83.45 (+40.48 pp), overall 82.89 (+26.32 pp); HR-Bench 4K 48.68 -> 83.25 (+34.57 pp); HR-Bench 8K overall 32.13 -> 50.75 (+18.62 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Large gains when Zoom Eye successfully locates relevant patches; demonstrates Zoom Eye can enable a smaller MLLM to perceive fine-grained details it missed in the global view. Errors persist in some global-local relational tasks when unioning distant patches is lossy.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>When paired with Zoom Eye, LLaVA-v1.5-7B's ability to answer high-resolution, fine-grained questions increases substantially, showing that vision-level search complements the model's intrinsic textual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8504.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8504.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5VL-3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-VL (3B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3B-parameter multimodal variant of the Qwen family used with AnyRes preprocessing and Global+Local input in Zoom Eye experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5VL-3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal LLM (3B) from the Qwen family supporting high-resolution AnyRes processing; used as an MLLM backbone with Zoom Eye in Global+Local mode.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Autoregressive generative reasoning', 'Zoom Eye tree-based vision-level search (global+local prompts and confidence queries)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model is prompted with the global image plus zoomed local patches; Zoom Eye queries it for c_e, c_l, and c_a on nodes and uses those to drive the prioritized tree traversal and final answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Applied Zoom Eye with τ=0.6 and AnyRes settings; compared baseline vs Zoom Eye across V* Bench and HR-Bench.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>V* Bench, HR-Bench 8K/4K</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Baseline Qwen2.5VL-3B: V* Bench 80.87, overall 71.05; With Zoom Eye: V* Bench 88.70 (+7.83 pp), overall 89.47 (+18.42 pp); HR-Bench 8K overall 62.88 -> 68.38 (+5.50 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Zoom Eye yields consistent improvements even for smaller MLLMs; benefits from Global+Local input because AnyRes supplies local high-resolution encoding alongside the global context.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Zoom Eye raises performance even for smaller-capacity MLLMs and leverages AnyRes/global+local input to better resolve fine-grained cues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8504.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8504.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-ov-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-OneVision (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B multimodal LLM variant using AnyRes; evaluated with Zoom Eye (Global+Local) and shown to gain large improvements on high-resolution benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-ov-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal LLM (7B) that supports AnyRes processing (global+local) and was evaluated both as baseline and with Zoom Eye applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Standard MLLM generative reasoning', 'Zoom Eye tree-based vision-level exploration']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The MLLM is used to (1) generate visual cues (in-context examples), (2) answer existing/latent/answering prompts on nodes, producing token logits used as confidences for ranking and stopping, and (3) synthesize final answers from unioned patches.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablations included varying the number of sub-regions (4/9/16), lowering τ to allow more search steps (vision-level test-time scaling), and measuring performance when zoom succeeded vs failed (>=50% coverage).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>V* Bench, HR-Bench 4K/8K, MME-RealWorld subtasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported deltas: V* Bench +14.19 pp, HR-Bench 4K +6.63 pp, HR-Bench 8K +10.00 pp. Example raw: baseline V* Bench overall 75.39 -> 90.58 with Zoom Eye in some tables. On MME-RealWorld sub-tasks: significant gains in Intention (+11.23 pp), Color (+12.90 pp), Motion (+12.10 pp) but declines on Orientation and RS/Position.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Zoom Eye substantially improves many sub-tasks, but also exposes dataset/model weaknesses (e.g., orientation inference, linking global/sub-image positions). Performance increases with more search steps until stabilizing.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Applying Zoom Eye to LLaVA-ov-7B yields large improvements on fine-grained high-resolution tasks; the method reveals both strengths and specific weaknesses (e.g., positional linking) in the underlying MLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8504.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8504.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVL2.5-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL2.5 (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 8B multimodal LLM evaluated with AnyRes processing and Zoom Eye, showing large absolute and relative gains on high-resolution benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternVL2.5-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An 8B-parameter open-source MLLM used as a backbone with AnyRes global+local inputs in Zoom Eye experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Autoregressive multimodal generation', "Zoom Eye's vision-level tree search and prompted confidence scoring"]</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Used identically to other MLLMs under Zoom Eye: In-context cue generation, per-node c_e/c_l/c_a via prompted Yes/No logits, W(depth)-weighted priority ranking, and τ-based stopping to collect patches which are unioned for final answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Evaluated baseline vs Zoom Eye on V* Bench and HR-Bench; ablations on thresholds, search depth, and sub-region split counts were run (reported in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>V* Bench, HR-Bench 4K/8K</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Baseline V* Bench 67.83 -> With Zoom Eye 86.09 (+18.26 pp); HR-Bench 4K FSP +15.71 pp; HR-Bench overall +16.25 pp (e.g., overall HR-8K 57.38 -> 73.63 in a table).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Large improvements indicate Zoom Eye's compatibility with larger open-source MLLMs; demonstrates that vision-level search recovers fine-grained details these models miss when viewing static, resized images.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Zoom Eye enables InternVL2.5-8B to substantially improve fine-grained perception on high-resolution data, supporting the claim that active vision-level exploration is complementary to static image perception.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Alphazero-like tree-search can guide large language model decoding and training <em>(Rating: 2)</em></li>
                <li>V?: Guided visual search as a core mechanism in multimodal llms <em>(Rating: 2)</em></li>
                <li>Divide, Conquer and Combine: A training-free framework for high-resolution image perception in multimodal large language models <em>(Rating: 2)</em></li>
                <li>Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8504",
    "paper_id": "paper-bf790cb8a95da05dcafd6bf1a4d02efce784720b",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Zoom Eye",
            "name_full": "ZoomEye: Tree-based Vision-Level Reasoning Algorithm",
            "brief_description": "A training-free, model-agnostic tree search algorithm that treats an image as a hierarchical tree of zoomed subregions and uses an MLLM's token probabilities (prompted 'Yes'/'No' confidences) to rank nodes and decide stopping, enabling multi-step vision-level reasoning (zoom-in, backtrack, union of found patches) to answer visual questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Zoom Eye (applied to MLLMs)",
            "model_description": "Not a single LLM but a search/meta-algorithm applied at test time on top of off-the-shelf multimodal LLMs (MLLMs). It uses the MLLM as a scorer/generator by prompting for three confidence queries and final answer generation; requires no additional training.",
            "reasoning_methods": [
                "Tree-based search (image-as-tree)",
                "Prompted binary-confidence scoring (existing, latent, answering)",
                "Guided zooming (local / global+local inputs)",
                "Vision-level test-time scaling (increasing search depth/steps)"
            ],
            "reasoning_methods_description": "Zoom Eye represents the image as a quadtree of patches (nodes). For each node it queries the MLLM with three prompted binary questions to obtain (a) existing confidence c_e ('Is there o in this patch?'), (b) latent confidence c_l ('Could o be found by further zooming?'), and (c) answering confidence c_a ('Can you answer q now?'). A weighted priority W(depth) combines c_e and c_l to rank nodes; a stopping threshold τ on c_a ends search. Two search modes: type-1 (find single instance, stop when c_a ≥ τ) and type-2 (find all instances, collect nodes with c_e ≥ τ2). Final answer is generated by feeding the union patch(s) back to the MLLM.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Ablations varied number of search steps (by lowering τ), varied number of sub-regions (4/9/16), and compared successful vs failed zoom cases. The study observes performance as a function of search depth/steps (vision-level test-time scaling) but does not compare different internal reasoning paradigms (e.g., CoT vs analogical).",
            "task_or_benchmark": "High-resolution visual understanding benchmarks: V* Bench (avg 2246x1582; attribute & spatial tasks), HR-Bench (8K average, sub-tasks FSP/FCP; plus 4K crops), and MME-RealWorld (real-world tasks across Monitoring, AD, OCR, Remote Sensing).",
            "performance_results": "Applied to multiple MLLMs, Zoom Eye consistently improved accuracy: example deltas reported include LLaVA-v1.5-7B: V* Bench spatial 43.47 -&gt; 83.45 (+40.48 pp), overall 56.57 -&gt; 82.89 (+26.32 pp); Qwen2.5VL-3B: V* Bench 80.87 -&gt; 88.70 (+7.83 pp); LLaVA-ov-7B: V* Bench +14.19 pp, HR-4K +6.63 pp, HR-8K +10.00 pp; InternVL2.5-8B: V* Bench 67.83 -&gt; 86.09 (+18.26 pp), HR-4K +15.71 pp, HR-8K +16.25 pp. On MME-RealWorld many sub-tasks improved strongly (e.g., Person_color +20.22 pp; Motion_vehicle +29.11 pp) while some tasks decreased (e.g., RS/Position −12.95 pp).",
            "qualitative_findings": "Zoom Eye enables MLLMs to locate fine-grained cues via multi-step zooms and backtracking, increasing robustness on dense high-resolution scenes. The authors observe a vision-level test-time scaling effect: performance improves as the number of search steps increases and then stabilizes. Failure modes include inability to relate positional relationships between global and cropped views (leading to declines on some position/orientation subtasks) and sensitivity to semantic fragmentation when fixed-size patching splits relevant cues.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Vision-level reasoning (Zoom Eye) supplements text-level reasoning by allowing active visual exploration; it is training-free, model-agnostic, and yields large consistent gains across high-resolution benchmarks. Exploring more zoom steps yields a test-time scaling benefit analogous to exploring more chain-of-thought tokens in text-only reasoning. The method is robust to patch granularity but limited by heuristic ranking/stopping and fixed patch partitioning.",
            "uuid": "e8504.0",
            "source_info": {
                "paper_title": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LLaVA-v1.5-7B",
            "name_full": "LLaVA-v1.5 (7B)",
            "brief_description": "An open-source multimodal LLM that combines a vision encoder with an autoregressive LLM base (7B) using naive image preprocessing; used as a backbone MLLM in Zoom Eye experiments (Local input mode).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-v1.5-7B",
            "model_description": "A multimodal large language model (7B parameters) that takes visual encoder outputs and text prompts to produce autoregressive text; in experiments it used naive image preprocessing (single-image local input mode).",
            "reasoning_methods": [
                "Generative token-level reasoning (standard MLLM autoregressive generation)",
                "Zoom Eye tree-based vision-level search (when adapted)"
            ],
            "reasoning_methods_description": "Out-of-the-box it uses the MLLM's generative capacity to answer from a fixed visual input. With Zoom Eye it is repeatedly prompted on cropped patches to produce 'Yes/No' confidences (c_e, c_l, c_a) and final answers — effectively coupling the LLM's textual reasoning with vision-level tree search.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Applied Zoom Eye (Local input, τ=0.8, weighted depth function W) and measured performance across V* Bench and HR-Bench; ablations include number of sub-regions, search depth, and success vs failure of zooming.",
            "task_or_benchmark": "V* Bench (high-res attribute & spatial), HR-Bench 4K/8K",
            "performance_results": "Baseline: V* Bench spatial 43.47, overall 56.57; With Zoom Eye: V* Bench spatial 83.45 (+40.48 pp), overall 82.89 (+26.32 pp); HR-Bench 4K 48.68 -&gt; 83.25 (+34.57 pp); HR-Bench 8K overall 32.13 -&gt; 50.75 (+18.62 pp).",
            "qualitative_findings": "Large gains when Zoom Eye successfully locates relevant patches; demonstrates Zoom Eye can enable a smaller MLLM to perceive fine-grained details it missed in the global view. Errors persist in some global-local relational tasks when unioning distant patches is lossy.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "When paired with Zoom Eye, LLaVA-v1.5-7B's ability to answer high-resolution, fine-grained questions increases substantially, showing that vision-level search complements the model's intrinsic textual reasoning.",
            "uuid": "e8504.1",
            "source_info": {
                "paper_title": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Qwen2.5VL-3B",
            "name_full": "Qwen2.5-VL (3B)",
            "brief_description": "A 3B-parameter multimodal variant of the Qwen family used with AnyRes preprocessing and Global+Local input in Zoom Eye experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5VL-3B",
            "model_description": "A multimodal LLM (3B) from the Qwen family supporting high-resolution AnyRes processing; used as an MLLM backbone with Zoom Eye in Global+Local mode.",
            "reasoning_methods": [
                "Autoregressive generative reasoning",
                "Zoom Eye tree-based vision-level search (global+local prompts and confidence queries)"
            ],
            "reasoning_methods_description": "The model is prompted with the global image plus zoomed local patches; Zoom Eye queries it for c_e, c_l, and c_a on nodes and uses those to drive the prioritized tree traversal and final answer generation.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Applied Zoom Eye with τ=0.6 and AnyRes settings; compared baseline vs Zoom Eye across V* Bench and HR-Bench.",
            "task_or_benchmark": "V* Bench, HR-Bench 8K/4K",
            "performance_results": "Baseline Qwen2.5VL-3B: V* Bench 80.87, overall 71.05; With Zoom Eye: V* Bench 88.70 (+7.83 pp), overall 89.47 (+18.42 pp); HR-Bench 8K overall 62.88 -&gt; 68.38 (+5.50 pp).",
            "qualitative_findings": "Zoom Eye yields consistent improvements even for smaller MLLMs; benefits from Global+Local input because AnyRes supplies local high-resolution encoding alongside the global context.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Zoom Eye raises performance even for smaller-capacity MLLMs and leverages AnyRes/global+local input to better resolve fine-grained cues.",
            "uuid": "e8504.2",
            "source_info": {
                "paper_title": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LLaVA-ov-7B",
            "name_full": "LLaVA-OneVision (7B)",
            "brief_description": "A 7B multimodal LLM variant using AnyRes; evaluated with Zoom Eye (Global+Local) and shown to gain large improvements on high-resolution benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-ov-7B",
            "model_description": "A multimodal LLM (7B) that supports AnyRes processing (global+local) and was evaluated both as baseline and with Zoom Eye applied.",
            "reasoning_methods": [
                "Standard MLLM generative reasoning",
                "Zoom Eye tree-based vision-level exploration"
            ],
            "reasoning_methods_description": "The MLLM is used to (1) generate visual cues (in-context examples), (2) answer existing/latent/answering prompts on nodes, producing token logits used as confidences for ranking and stopping, and (3) synthesize final answers from unioned patches.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Ablations included varying the number of sub-regions (4/9/16), lowering τ to allow more search steps (vision-level test-time scaling), and measuring performance when zoom succeeded vs failed (&gt;=50% coverage).",
            "task_or_benchmark": "V* Bench, HR-Bench 4K/8K, MME-RealWorld subtasks",
            "performance_results": "Reported deltas: V* Bench +14.19 pp, HR-Bench 4K +6.63 pp, HR-Bench 8K +10.00 pp. Example raw: baseline V* Bench overall 75.39 -&gt; 90.58 with Zoom Eye in some tables. On MME-RealWorld sub-tasks: significant gains in Intention (+11.23 pp), Color (+12.90 pp), Motion (+12.10 pp) but declines on Orientation and RS/Position.",
            "qualitative_findings": "Zoom Eye substantially improves many sub-tasks, but also exposes dataset/model weaknesses (e.g., orientation inference, linking global/sub-image positions). Performance increases with more search steps until stabilizing.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Applying Zoom Eye to LLaVA-ov-7B yields large improvements on fine-grained high-resolution tasks; the method reveals both strengths and specific weaknesses (e.g., positional linking) in the underlying MLLM.",
            "uuid": "e8504.3",
            "source_info": {
                "paper_title": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "InternVL2.5-8B",
            "name_full": "InternVL2.5 (8B)",
            "brief_description": "An open-source 8B multimodal LLM evaluated with AnyRes processing and Zoom Eye, showing large absolute and relative gains on high-resolution benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InternVL2.5-8B",
            "model_description": "An 8B-parameter open-source MLLM used as a backbone with AnyRes global+local inputs in Zoom Eye experiments.",
            "reasoning_methods": [
                "Autoregressive multimodal generation",
                "Zoom Eye's vision-level tree search and prompted confidence scoring"
            ],
            "reasoning_methods_description": "Used identically to other MLLMs under Zoom Eye: In-context cue generation, per-node c_e/c_l/c_a via prompted Yes/No logits, W(depth)-weighted priority ranking, and τ-based stopping to collect patches which are unioned for final answer generation.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Evaluated baseline vs Zoom Eye on V* Bench and HR-Bench; ablations on thresholds, search depth, and sub-region split counts were run (reported in tables).",
            "task_or_benchmark": "V* Bench, HR-Bench 4K/8K",
            "performance_results": "Baseline V* Bench 67.83 -&gt; With Zoom Eye 86.09 (+18.26 pp); HR-Bench 4K FSP +15.71 pp; HR-Bench overall +16.25 pp (e.g., overall HR-8K 57.38 -&gt; 73.63 in a table).",
            "qualitative_findings": "Large improvements indicate Zoom Eye's compatibility with larger open-source MLLMs; demonstrates that vision-level search recovers fine-grained details these models miss when viewing static, resized images.",
            "explicit_comparison": false,
            "key_claims_or_conclusions": "Zoom Eye enables InternVL2.5-8B to substantially improve fine-grained perception on high-resolution data, supporting the claim that active vision-level exploration is complementary to static image perception.",
            "uuid": "e8504.4",
            "source_info": {
                "paper_title": "ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Alphazero-like tree-search can guide large language model decoding and training",
            "rating": 2
        },
        {
            "paper_title": "V?: Guided visual search as a core mechanism in multimodal llms",
            "rating": 2
        },
        {
            "paper_title": "Divide, Conquer and Combine: A training-free framework for high-resolution image perception in multimodal large language models",
            "rating": 2
        },
        {
            "paper_title": "Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search",
            "rating": 1
        }
    ],
    "cost": 0.01742775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration</h1>
<p>Haozhan Shen ${ }^{1}$ Kangjia Zhao ${ }^{1}$ Tiancheng Zhao ${ }^{2,3 \boxtimes}$ Ruochen Xu ${ }^{2}$<br>Zilun Zhang ${ }^{1}$ Mingwei Zhu ${ }^{1}$ Jianwei Yin ${ }^{1}$<br>${ }^{1}$ Zhejiang University ${ }^{2}$ Om AI Research ${ }^{3}$ Binjiang Institute of Zhejiang University<br>$\boxtimes$ Correspondence: tianchez@zju-bj.com</p>
<h4>Abstract</h4>
<p>Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in vision-language understanding. Recently, with the integration of test-time scaling techniques, these models have also shown strong potential in visual reasoning. However, most existing reasoning approaches remain text-level in nature: MLLMs are prompted to explore various combinations of textual tokens via their underlying language model, while the visual input remains fixed throughout the reasoning process. This paradigm limits the model's ability to fully exploit rich visual information, particularly when dealing with images containing numerous fine-grained elements. In such cases, vision-level reasoning becomes crucial-where models dynamically zoom into specific regions of the image to gather detailed visual cues necessary for accurate decision-making. In this paper, we propose Zoom Eye, a training-free, model-agnostic tree search algorithm tailored for vision-level reasoning. Zoom Eye treats an image as a hierarchical tree structure, where each child node represents a zoomed-in subregion of its parent, and the root corresponds to the full image. The algorithm enables MLLMs to simulate human-like zooming behavior by navigating from root to leaf nodes in search of task-relevant visual evidence. We experiment on a series of elaborate high-resolution benchmarks and the results demonstrate that Zoom Eye not only consistently improves the performance of a series of MLLMs with large margin (e.g., InternVL2.5-8B increases by $15.71 \%$ and $17.69 \%$ on HR-Bench) but also enables small 3-8B MLLMs to outperform strong large models such as GPT-4o. Our code is available at https://github.com/om-ai-lab/ZoomEye.</p>
<h2>1 Introduction</h2>
<p>By integrating powerful language models (Touvron et al., 2023; Yang et al., 2024) with visual encoders (Radford et al., 2021; Sun et al., 2023;
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Top: When dealing with a high-resolution image, MLLMs effectively perceive the dominant objects but often fail to recognize finer details, highlighting the need for vision-level reasoning. Bottom: Applied with Zoom Eye, MLLMs could perform vision-level reasoning, allowed to explore the image details until they can answer the question.</p>
<p>Zhai et al., 2023), Multimodal large language models (MLLMs) are able to jointly process textual and visual inputs, achieving impressive performance in vision-language understanding (Zhao et al., 2024a; Bai et al., 2023; Chen et al., 2024b; Li et al., 2024). Recently, drawing on test-time scaling techniques that enhance reasoning abilities in LLMs, such as OpenAI-o1 (Jaech et al., 2024) and DeepSeekR1 (Guo et al., 2025), a series of literature tries to investigate these reasoning techniques in MLLMs</p>
<p>to further improve the visual reasoning capabilities <em>Xu et al. (2024); Dong et al. (2024); Yao et al. (2024a); Shen et al. (2025); Meng et al. (2025)</em></p>
<p>However, these methods predominantly operate at the textual level, leveraging the generative capacity of the underlying language model without modifying the perception of the image itself. That is, the visual input remains static throughout the reasoning process, restricting the model's ability to process fine-grained visual content, especially on an elements-rich high-resolution image. As illustrated in the top of Figure 1, for the same image, the MLLM accurately recognizes the dominant object whereas it struggles to perceive the detailed one. This gap highlights the need for vision-level reasoning, where the model actively interacts with the image by zooming in and out to selectively attend to informative regions, as demonstrated in the bottom of Figure 1, much like how humans visually process complex scenes. A similar vision-level zooming mechanism has been adopted in the closed-source OpenAI-o3 <em>OpenAI (2025)</em>. In contrast, our goal is to develop an open-source vision-level reasoning method, making this capability accessible to the broader research community.</p>
<p>When viewing a high-resolution image, humans typically start with a global scan, then gradually zoom into areas of interest for closer inspection (Figure 2(b)). If the desired information is not found, they zoom out and explore alternative regions (as shown in Figure 2 (c)). Inspired by this, structuring an image as a tree is highly logical for simulating similar actions in an MLLM: the root denotes the full image, each child node corresponds to a zoomed-in sub-region of its parent, and deeper nodes indicate higher zoom levels. This hierarchical representation, combined with a search algorithm, allows models to (1) explore fine-grained regions (<em>node lookahead</em>) and (2) return to the previous view to inspect other regions (<em>node backtracking</em>). Similar tree-based search strategies have shown strong performance in text-based LLM reasoning<em>Yao et al. (2024b); Hao et al. (2023); Feng et al. (2023); Zhu et al. (2023)</em>.</p>
<p>In this paper, we propose Zoom Eye, a tree search algorithm for vision-level reasoning, which navigates MLLMs in the dense image context by the hierarchical and visual nature of images (contribution #1). This method simulates the actions of zooming in and out to inspect image details and seek out crucial information. Given a question, the adopted MLLM first identifies the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Zoom Eye enables MLLMs to (a) answer the question directly when the visual information is adequate, (b) zoom in gradually for a closer examination, and (c) zoom out to the previous view and explore other regions if the desired information is not initially found.</p>
<p>pertinent objects. We then introduce two types of <em>confidence values</em> by prompting the MLLM to recognize the presence of these relevant objects. These <em>confidence values</em> are used to prioritize each candidate node during the tree search, determining the sequence of node selection. The search concludes based on a stopping criterion when the MLLM can confidently answer the question. This process is illustrated in the bottom part of Figure 1. Finally, the MLLM formulates a final response based on the visual information gathered during the search.</p>
<p>We adapt Zoom Eye to a series of mainstream MLLMs, including Qwen2.5VL <em>Bai et al. (2025)</em>, LLaVA-v1.5 <em>Liu et al. (2024a)</em>, LLaVA-OneVision <em>Li et al. (2024)</em>, InternVL2.5 <em>Chen et al. (2024a)</em>, and evaluate them on a suite of elaborate high-resolution visual understanding benchmarks. Equipped with Zoom Eye, all evaluated models achieve substantial performance improvements compared to the baseline (contribution #2).</p>
<p>Additionally, our analysis also reveals certain deficiencies in visual understanding exhibited by these models, which we detail in §4.3 (contribution #3). Addressing these limitations is part of our future work. More importantly, as discussed in §4.4.1, we observe a vision-level test-time scaling phenomenon analogous to what has been observed in text-based LLMs: performance consistently improves with an increasing number of search steps. This finding suggests that vision-level reasoning benefits from deeper exploratory search and opens new avenues for scaling MLLM inference beyond static image perception (contribution #4).</p>
<h2>2 Preliminary</h2>
<p>In this section, we describe briefly the prevalently adopted image preprocessing methods and imagetext input ways of MLLMs.
Image preprocessing. For a given image I, a naive processing style is to simply resize it to a preset fixed resolution and then feed it into a vision encoder to generate visual representations. These representations can be treated as visual tokens and subsequently passed to an LLM, enabling the model to perceive the visual content of $\mathbf{I}$. Formally, this process can be expressed as: $\mathbf{v}=\mathcal{F}(R(\mathbf{I}))=\left(v_{1}, v_{2}, \ldots, v_{L_{v}}\right)$, where $\mathcal{F}$ is the vision encoder, $R$ is the resize operation, and $L_{v}$ is the number of visual representations, which also corresponds to the number of visual tokens accepted by the LLM. Due to the constraints of the naive version's fixed and limited resolution, another method, known as AnyRes, was introduced. It divides the original image into several equal-area blocks and imposes a maximum limit, $M$, on the number of divided blocks. The vision encoder then independently encodes each block and the overall image. Finally, all the encoded visual representations are integrated together. This allows flexible processing of various resolutions. Denoting $\mathbf{I}^{(0)}$ as the whole image and $\left{\mathbf{I}^{(1)}, \ldots, \mathbf{I}^{(a)}\right}(a \leq M)$ as the blocks, the AnyRes could be formulated as: $\mathbf{v}=\mathcal{F}(A(\mathbf{I}))=\left[\mathbf{v}<em 1="1">{0}, \mathbf{v}</em>}, \ldots, \mathbf{v<em i="i">{a}\right]$, where $A$ denotes the AnyRes operation and $\mathbf{v}</em>\right), i=0,1, \ldots, a$. It is noteworthy that the naive method can be considered a special case of AnyRes when $a=0$.
Imga-Text joint input for MLLM. Common MLLMs link a vision encoder to the pre-trained LLM via projection or alignment modules, allowing language generation through the autoregressive capabilities of their LLM base. Specifically, given an image $\mathbf{I}$ and an input prompt $\mathbf{x}, \mathbf{I}$ is first encoded into a set of visual representations as described in the previous sub-section. Subsequently, these visual representations, along with the text input, are fed into the LLM base of the MLLM. Assuming the length of the output sequence and text input are $L_{y}$ and $L_{x}$ respectively, the probability for a MLLM $\Phi_{\theta}$ to generate an output $\mathbf{y}=\left(y_{1}, y_{2}, \ldots, y_{L_{y}}\right)$ conditioned on the visual input $\mathcal{F}(\cdot(\mathbf{I}))=\left(v_{(0,1)}, \ldots, v_{\left(a, L_{v}\right)}\right)$ and the text input $\mathbf{x}=\left(x_{1}, x_{2}, \ldots, x_{L_{x}}\right)$ is: $\Phi_{\theta}(\mathbf{y} \mid \mathcal{F}(\cdot(\mathbf{I})), \mathbf{x})=$ $\prod_{i=1}^{L_{y}} \Phi_{\theta}\left(y_{i} \mid v_{(0,1) \cdot\left(a, L_{v}\right)}, x_{1: L_{x}}, y_{1: i-1}\right), \quad$ where $\mathcal{F}(\cdot)$ could represent $\mathcal{F}(R)$ as naive resize or
$\mathcal{F}(A)$ as AnyRes.}=\mathcal{F}\left(R\left(\mathbf{I}^{(i)}\right)\right)=$ $\left(v_{(i, 1)}, v_{(i, 2)}, \ldots, v_{\left(i, L_{v}\right)</p>
<h2>3 Methodology</h2>
<p>In this section, we introduce the Zoom Eye algorithm. Firstly, we brief the general tree search algorithm. Subsequently, we elaborate on our implementation by initializing the components of the tree search algorithms in detail.</p>
<h3>3.1 Abstraction of Tree Search</h3>
<p>Tree node. Typically, a node in the tree structure comprises the following attributes:(1) id: The unique identifier of the node. (2) depth: Represents the level of the node within the tree. (3) value: Used to store numeric or textual data in the node. (4) children: A list of references to the node's children nodes, which facilitates traversal of the tree structure. (5) Other custom attributes
Tree search. The abstraction of the tree search algorithm could be modeled as a tuple $(T, Q, \mathcal{R}, \mathcal{S})$, where $T$ is the tree structure consisting of a set of nodes, $Q$ is a container that holds all the nodes that might be accessed in the next search step, $\mathcal{R}$ is a ranking function used to select the highest priority node based on the used search algorithm, and $\mathcal{S}$ represents the stopping criterion. The abstract search process is shown in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Alg</span><span class="p">.</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Abstraction</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">Tree</span><span class="w"> </span><span class="k">Search</span><span class="w"> </span><span class="n">Algorithm</span>
<span class="nl">Require</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="w"> </span><span class="n">Q</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">S</span><span class="err">}\</span><span class="p">)</span>
<span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="k">Initialize</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="n">queue</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\{\}\</span><span class="p">)</span>
<span class="mi">2</span><span class="err">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">append</span><span class="err">}</span><span class="p">(</span><span class="n">T</span><span class="err">\</span><span class="p">).</span><span class="w"> </span><span class="n">root</span><span class="p">)</span>
<span class="mi">3</span><span class="err">:</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="n">do</span>
<span class="mi">4</span><span class="err">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="n">n_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">Q</span><span class="err">\</span><span class="p">).</span><span class="n">pop</span><span class="p">()</span>
<span class="mi">5</span><span class="err">:</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">S</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">n_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="o">==</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">True</span><span class="w"> </span><span class="k">then</span>
<span class="mi">6</span><span class="err">:</span><span class="w"> </span><span class="k">break</span>
<span class="mi">7</span><span class="err">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">s</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">n_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="p">).</span><span class="n">children</span><span class="p">.</span><span class="k">size</span>
<span class="mi">8</span><span class="err">:</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="mi">9</span><span class="err">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="n">Q</span><span class="err">\</span><span class="p">).</span><span class="n">append</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">n_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">children</span><span class="err">}</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="mi">10</span><span class="err">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">Q</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">sort</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>Consider the example of a DFS search for a node with a value of 5 in the tree, in this case, $\mathcal{R}$ is a function that sorts the nodes in $Q$ in descending order of depth, and in ascending order of id when depths are equal. Meanwhile, $\mathcal{S}$ is a function checking if a node's value equals 5.</p>
<p>A specific implementation of Zoom Eye search involves three key questions: 1 . How to formulate the image as a tree $T$ (§3.2). 2. How to set the ranking function $\mathcal{R}$ (§3.3). 3. How to determine the stopping criterion $\mathcal{S}$ (§3.4). Finally, we provide a description of the overall algorithm in $\S 3.5$.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Two image input methods for MLLMs with distinct image processing.</p>
<h3>3.2 Tree Representation for Image</h3>
<p>We model the overall image as a tree <em>T</em>. A specific <em>node</em>, denoted as <em>n<sub>t</sub></em>, represents an image patch view {I, b<sub>t</sub>}, where I is the image and b<sub>t</sub> = (x<sub>1,t</sub>, y<sub>1,t</sub>, x<sub>2,t</sub>, y<sub>2,t</sub>) is the normalized bounding box coordinates. If the size of <em>n<sub>t</sub></em>'s image patch exceeds the predefined resolution by the image encoder, it can be further divided into four equal-sized sub-patches, serving as its children with size 4. Nodes are recursively divided until they meet the resolution limit. At the start of the search, the root node <em>T</em>.root = {I, (0, 0, 1, 1)} representing the overall image is visited.</p>
<p>However, due to the detailed nature of high-resolution images and information loss from down-sampling to the vision encoder's fixed resolution, MLLMs frequently struggle to accurately capture key parts of an image initially. Consequently, MLLMs should be allowed to continuously scan and zoom into the current view (i.e., explore deeper nodes) for more focused information. In our implementation, we consider two image input methods to enable MLLMs to perceive the local patch represented by <em>n<sub>t</sub></em>: (1) Local Input: only the local patch is provided, suitable for earlier single-image input MLLMs with naive image preprocessing method (Li et al., 2023; Liu et al., 2024c,a). (2) Global+Local Input: both the global image and local patch are input, ideal for advanced MLLMs using AnyRes preprocessing method (Liu et al., 2024b; Li et al., 2024; Chen et al., 2024b). In this case, we use the visual prompt with a red rectangle to emphasize the local focus, applying naive processing to the global image and AnyRes to the local patch, as shown in Figure 3. Denoting</p>
<p><strong>V</strong>(n<sub>t</sub>) as the final image input, we have:</p>
<p>$$
\mathcal{V}(n_t) = \begin{cases}
\mathcal{F}(R(\mathbf{I}\cdot \text{crop}(\mathbf{b}_t))) &amp; \text{Local} \
\mathcal{F}(R(\mathbf{I})), \mathcal{F}(A(\mathbf{I}\cdot \text{crop}(\mathbf{b}_t))) &amp; \text{Global+Local}
\end{cases}
\tag{1}
$$</p>
<p><strong>Alg. 2 Ranking Function &amp; Stopping Criterion</strong></p>
<p><strong>Require:</strong> Φ<sub>θ</sub>, W, {p<sub>a</sub>, p<sub>l</sub>, p<sub>a</sub>}, τ, o, q<sub>s</sub></p>
<p>1: <strong>function</strong> R(n<sub>1</sub>, n<sub>2</sub>) ▷ Ranking Function
2: <strong>return</strong> GET PRIORITY(n<sub>1</sub>) &gt; GET PRIORITY(n<sub>2</sub>)
3:
4: <strong>function</strong> S(n<sub>t</sub>) ▷ Stopping Criterion
5: c<sub>a</sub> ← LOGITS RATIO(n<sub>t</sub>, p<sub>a</sub>(q<sub>s</sub>))
6: <strong>return</strong> c<sub>a</sub> ≥ τ
7:
8: <strong>function</strong> GET PRIORITY(n<sub>t</sub>)
9: <strong>if</strong> n<sub>t</sub>.priority is None <strong>then</strong>
10: c<sub>e</sub> ← LOGITS RATIO(n<sub>t</sub>, p<sub>e</sub>(o))
11: c<sub>l</sub> ← LOGITS RATIO(n<sub>t</sub>, p<sub>l</sub>(o))
12: α ← W(n<sub>t</sub>.depth) ▷ weighted factor
13: n<sub>t</sub>.priority ← α · c<sub>l</sub> + (1 − α) · c<sub>e</sub>
14: <strong>return</strong> n<sub>t</sub>.priority
15:
16: <strong>function</strong> LOGITS RATIO(n<sub>t</sub>, x)
17: z<sub>1</sub> ← Φ<sub>θ</sub>(y = Yes | V(n<sub>t</sub>), x)
18: z<sub>2</sub> ← Φ<sub>θ</sub>(y = No | V(n<sub>t</sub>), x)
19: z ← (softmax(z<sub>1</sub>, z<sub>2</sub>)[0] − 0.5) × 2)
20: <strong>return</strong> z ▷ z ∈ (−1, 1)</p>
<h3>3.3 Ranking Function</h3>
<p>As shown in Algorithm 1, R is used to rank the nodes with the priority value to determine which one to visit in the next step. A well-defined R strategically steers the search process. In Zoom Eye, we adopt the MLLM to calculate the priority value and use R to sort nodes by the value. Specifically, let <em>o</em> denote the visual cue that is crucial for answering the question, a MLLM should have the following capabilities: (1) It could perceive whether <em>o</em> exists within the visible view; (2) If <em>o</em> occupies a small area and is not clearly visible, it can leverage the common sense knowledge to infer whether <em>o</em> might be discerned through further zooming. Thus, we query the MLLM with two prompts p<sub>e</sub>(<em>o</em>) and p<sub>l</sub>(<em>o</em>) (e.g., "Is there a <em>o</em> in the sub-patch?", "Is it possible to find a <em>o</em> by further zooming the sub-patch?") to trigger these two capabilities, and use the ratio of the next-word probability of the token "Yes" and "No" as priority values. We refer to these two values as <em>existing confidence</em> and <em>latent confidence</em>, denoted as c<sub>e</sub> and c<sub>l</sub>.</p>
<p>The overall priority value for a node is the weighted sum of c<sub>e</sub> and c<sub>l</sub>. We introduce a weight function W(d) that is related to a node's depth. When the depth is shallow, indicating minimal zoom and the MLLM might not clearly perceive</p>
<p>the cue, assign more weight $c_{l}$. As depth increases, shift more weight to $c_{e}$. Finally, ranking function $\mathcal{R}$ is introduced to rank nodes by the overall priority value, as shown in Algorithm 2.</p>
<h3>3.4 Stopping Criterion</h3>
<p>Zoom Eye exits the search process when the MLLM provides feedback that the current view is sufficient to answer the provided question, denoted as $q_{s}$. Specifically, we query the MLLM with a prompt $\mathrm{p}<em s="s">{a}\left(q</em>$ is shown in Algorithm 2.}\right)$ (e.g., "Could you answer $q_{s}$ now?") and use the same method as described in $\S 3.3$ to quantify the positive feedback. We refer to it as answering confidence, denoted as $c_{a}$. When $c_{a}$ exceeds a predefined threshold $\tau$, the search terminates. The implementation of $\mathcal{S</p>
<h3>3.5 Overall Search Algorithm</h3>
<p>With the above notations in place, we now describe how Zoom Eye works for a given image-question pair (I, $q$ ). The complete algorithm workflow is shown in Appendix D.4.
Generating visual cues to guide the search. Before search, the MLLM has to predefine the visual cues essential for addressing $q$, enabling a targeted and guided search based on these cues. We utilize the in-context capability from the LLM base of the MLLM, using a sequence of contextual examples as prefixes to generate visual cues. Ultimately, the MLLM produces $k$ visual cues $\left{o_{1}, \ldots, o_{k}\right}$ pertinent to $q$. Each $o_{i}(i \in{1, \ldots, k})$ can be categorized into two types: (type 1) those requiring a search for a single instance, and (type 2) those requiring identification of all instances in the image.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Visual cues</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">What is the color of the dog?</td>
<td style="text-align: center;">dog</td>
<td style="text-align: center;">type 1</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">What is the relative position <br> of the dog to the cat?</td>
<td style="text-align: center;">dog, <br> cat</td>
<td style="text-align: center;">type 1, <br> type 1</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">How many dogs in the image?</td>
<td style="text-align: center;">all dogs</td>
<td style="text-align: center;">type 2</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of visual cues and their types.
Searching for cues. For each cue $o_{i}(i \in$ ${1, \ldots, k})$, Zoom Eye explores the image tree to capture pertinent visual information. When searching for type 1 cues, the search is guided with $\mathcal{R}$ and concludes as soon as it meets $\mathcal{S}$, then the current node is recorded in a list $L$. For a single type 1 clue, as shown in line 1 of Table 1, the applied $q_{s}$ for $\mathcal{S}$ is the input question $q$. If multiple type 1 clues are generated as in line 2 of Table 1, we introduce a de-
composed question template $\mathrm{p}<em i="i">{d q}\left(o</em>}\right)$ such as "what is the location of the $\left{o_{i}\right}$ ?" specific to each cue. In this case, the applied $q_{s}$ of $o_{i}$ is $\mathrm{p<em i="i">{d q}\left(o</em>$ is not applied, and we search the whole tree to add all nodes with sufficient existing confidence to $L$.
Answering the question using the searched cues. Given the searched nodes $L=\left{n_{1}^{}\right)$. If a type 2 cue is generated, as shown in line 3 of Table $1, \mathcal{S<em>}, \ldots, n_{K}^{</em>}\right}$ , the MLLM formulates a response to the input question $q$ by synthesizing information of these nodes. Denoting $\mathbf{b}<em 1_="1," i="i">{i}^{<em>}=\left(x_{1, i}^{</em>}, y</em>^{<em>}, x_{2, i}^{</em>}, y_{2, i}^{<em>}\right)$ as the bounding-box of $n_{i}^{</em>}(i \in{1, \ldots, K})$, we union the bounding-box coordinates of all nodes in $L$ to create a union bounding-box $\mathbf{b}^{<em>}=$ $\left(\min <em 1_="1," i="i">{i} x</em>^{</em>}, \min <em 1_="1," i="i">{i} y</em>^{<em>}, \max <em 2_="2," i="i">{i} x</em>^{</em>}, \max <em 2_="2," i="i">{i} y</em>^{<em>}\right)$. For the two distinct image input methods, we apply Eq. 1 to feed the focused region $\mathbf{b}^{</em>}$ along with $q$ into models and derive the final response.</p>
<h2>4 Experiments</h2>
<h3>4.1 Implementation Details</h3>
<p>Local input. We select LLaVA-v1.5-7B (Liu et al., 2024a) as the base MLLM, with the naive image processing. We set $\tau$ at 0.8 and define $\mathcal{W}$ as $\frac{1-b}{D^{2}} \times$ $d^{2}+b$, where $D$ denotes the depth of the image tree, $d$ is the depth of the visited node during the search, and $b$ is a bias value, set here at 0.2 .
Global + Local input. We select Qwen2.5VL3B (Bai et al., 2025), LLaVA-ov(oneVision)-7B (Li et al., 2024), and InternVL2.5-8B (Chen et al., 2024a) as our MLLMs, with the AnyRes image processing. For LLaVA-ov and InternVL, we define the maximum AnyRes block as 12, and for QwenVL, we set the max pixels as $12,845,056$. We set $\tau$ at 0.6 and define $\mathcal{W}$ similarly to the above, except with $b$ of 0.6 .
For both input implementation, we set the maximum search depth at 2 when searching for type 2 cues to save costs. Additionally, the decomposed question template $\mathrm{p}<em i="i">{d q}\left(o</em>\right}$ ?". More details are described in Appendix D.}\right)$ is assigned as "What is the appearance of the $\left{o_{i</p>
<h3>4.2 Results on High-Resolution Benchmark</h3>
<p>Evaluated benchmark. We evaluate Zoom Eye on two meticulously curated high-resolution benchmarks. The first, $\mathbf{V}^{*}$ Bench (Wu and Xie, 2024), with an average resolution of 2246x1582, features sub-tasks in attribute recognition and spatial reasoning. The second, HR-Bench 8K (Wang et al., 2024)</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>$V^{*}$ Bench Spatial</th>
<th>Overall</th>
<th>HR-Bench 4K FSP</th>
<th>HR-Bench 8K FEP</th>
<th>HR-Bench 8K</th>
</tr>
</thead>
<tbody>
<tr>
<td>Open-source MLLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>minigpro2-7B <em>Chen et al. (2023a)</em></td>
<td>-</td>
<td>-</td>
<td>25.75</td>
<td>25.25</td>
<td>25.50</td>
</tr>
<tr>
<td>LLaVA-v1.6-7B <em>Liu et al. (2024b)</em></td>
<td>60.87</td>
<td>63.16</td>
<td>61.78</td>
<td>49.0</td>
<td>46.75</td>
</tr>
<tr>
<td>LLaVA-v1.6-13B <em>Liu et al. (2024b)</em></td>
<td>60.0</td>
<td>64.47</td>
<td>61.78</td>
<td>49.75</td>
<td>41.25</td>
</tr>
<tr>
<td>Yi-VL-34B <em>Al et al. (2024)</em></td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>40.0</td>
<td>42.75</td>
</tr>
<tr>
<td>LLaVA-HR-X-7B <em>Luo et al. (2024)</em></td>
<td>51.30</td>
<td>64.47</td>
<td>56.54</td>
<td>57.75</td>
<td>46.25</td>
</tr>
<tr>
<td>Closed-source MLLMs</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>QWen-VL-max <em>Bai et al. (2023)</em></td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>65.0</td>
<td>52.0</td>
</tr>
<tr>
<td>GPT4e <em>Achiam et al. (2023)</em></td>
<td>-</td>
<td>-</td>
<td>66.0</td>
<td>70.0</td>
<td>48.0</td>
</tr>
<tr>
<td>Baseline and Local Input Zoom Eye</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaVA-v1.5-7B <em>Liu et al. (2024a)</em></td>
<td>43.47</td>
<td>56.57</td>
<td>48.68</td>
<td>38.5</td>
<td>33.75</td>
</tr>
<tr>
<td>LLaVA-v1.5-7B w/ Zoom Eye</td>
<td>83.45</td>
<td>82.89</td>
<td>83.25</td>
<td>67.75</td>
<td>38.75</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>$+40.48$</td>
<td>$+26.32$</td>
<td>$+34.57$</td>
<td>$+29.25$</td>
<td>$+5.0$</td>
</tr>
<tr>
<td>Baseline and Global+Local Input Zoom Eye</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Qwen2.5VL-3B <em>Bai et al. (2025)</em></td>
<td>80.87</td>
<td>71.05</td>
<td>76.96</td>
<td>82.75</td>
<td>49.0</td>
</tr>
<tr>
<td>Qwen2.5VL-3B w/ Zoom Eye</td>
<td>88.70</td>
<td>89.47</td>
<td>89.01</td>
<td>86.75</td>
<td>53.50</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>$+7.83$</td>
<td>$+18.42$</td>
<td>$+12.05$</td>
<td>$+4.0$</td>
<td>$+4.50$</td>
</tr>
<tr>
<td>LLaVA-ov-7B <em>Li et al. (2024)</em></td>
<td>75.65</td>
<td>75.0</td>
<td>75.39</td>
<td>72.0</td>
<td>54.0</td>
</tr>
<tr>
<td>LLaVA-ov-7B w/ Zoom Eye</td>
<td>93.91</td>
<td>85.53</td>
<td>90.58</td>
<td>84.25</td>
<td>55.0</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>$+18.26$</td>
<td>$+10.53$</td>
<td>$+14.19$</td>
<td>$+12.25$</td>
<td>$+1.0$</td>
</tr>
<tr>
<td>InternVL2.5-8B <em>Chen et al. (2024a)</em></td>
<td>67.83</td>
<td>71.05</td>
<td>69.11</td>
<td>75.75</td>
<td>56.25</td>
</tr>
<tr>
<td>InternVL2.5-8B w/ Zoom Eye</td>
<td>86.09</td>
<td>82.89</td>
<td>84.82</td>
<td>88.75</td>
<td>61.50</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>$+18.26$</td>
<td>$+11.84$</td>
<td>$+15.71$</td>
<td>$+13.0$</td>
<td>$+5.25$</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of different models on high-resolution benchmarks. FSP: Fine-grained Single-instance Perception; FCP: Finegrained Cross-instance Perception. More results are displayed in Table 8.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Calculate</th>
<th>Intention</th>
<th>MO</th>
<th></th>
<th></th>
<th></th>
<th>AD</th>
<th></th>
<th></th>
<th></th>
<th>RS</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>Property</td>
<td>Orientation</td>
<td>Color ${ }^{\dagger}$</td>
<td></td>
<td>Intention ${ }^{\dagger}$</td>
<td>Attention</td>
<td>Motion ${ }^{\dagger}$</td>
<td></td>
<td>Count</td>
</tr>
<tr>
<td>LLaVA-ov-7B</td>
<td>36.33</td>
<td>27.55</td>
<td>55.0</td>
<td>14.94</td>
<td>34.19</td>
<td></td>
<td>37.32</td>
<td>71.89</td>
<td>30.61</td>
<td></td>
<td>32.95</td>
</tr>
<tr>
<td>w/ Zoom Eye</td>
<td>38.67</td>
<td>38.78</td>
<td>60.0</td>
<td>14.62</td>
<td>47.09</td>
<td></td>
<td>38.56</td>
<td>68.66</td>
<td>42.71</td>
<td></td>
<td>35.56</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>$+2.34$</td>
<td>$+11.23$</td>
<td>$+5.0$</td>
<td>$-0.32$</td>
<td>$+12.90$</td>
<td></td>
<td>$+1.24$</td>
<td>$-3.23$</td>
<td>$+12.10$</td>
<td></td>
<td>$+2.61$</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance comparison on MME-RealWorld benchmark. This benchmark comprises numerous sub-tasks, and we only list those that exhibit obvious performance changes of Zoom Eye against the baseline. MO (Monitoring), AD (Autonomous Driving), and RS (Remote Sensing) are data categories within this benchmark. ${ }^{\dagger}$ This result is an average derived from multiple similar sub-tasks (e.g., Color is the average of Vehicle Color and Person Color).
boasts average resolution of 7680 , which consists of two sub-tasks: Fine-grained Single-instance Perception (FSP) and Fine-grained Cross-instance Perception (FCP). The 8 K images are cropped around the objects in question to produce HR-Bench 4K. Both benchmarks are comprised of rich visual elements and required detailed perception to accurately respond. More results are displayed in Table 8.</p>
<p>Main results. As shown in Table 2, all evaluated models exhibit significant performance gains after incorporating Zoom Eye, highlighting its modelagnostic applicability. For instance, LLaVA-ov-7B achieves performance improvements of $14.19 \%$, $6.63 \%$, and $10.00 \%$ on $V^{*}$ Bench, HR-Bench 4 K , and HR-Bench 8 K , respectively. In conjunction with the case studies presented in Figure 5, these results demonstrate that vision-level reasoning enables MLLMs to more effectively capture finegrained and task-relevant visual information in complex scenes, thereby enhancing their overall visual understanding capabilities.</p>
<h3>4.3 Results on Real-World Benchmark</h3>
<p>Evaluated benchmark. We further evaluate Zoom Eye on MME-RealWorld <em>Zhang et al. (2024)</em>, a manually annotated benchmark tailored for realworld applications, featuring an average resolution of $2000 \times 1500$. It includes 5 data categories and 43 sub-class tasks. Due to the page limit, we report on only 13 sub-tasks that show significant performance changes with Zoom Eye. These sub-tasks span 3 data categories, with similar types merged (e.g., Vehicle Color and Person Color into Color) to present average scores. Detailed results are provided in Appendix C.
Results. As shown in Table 3, Zoom Eye improves the performance of LLaVA-ov-7B on most sub-tasks, especially on MO/Intention ( $+11.23 \%$ ), MO/Color ( $+12.9 \%$ ), and AD/Motion ( $+12.1 \%$ ). However, we also notice that the model's performance with Zoom Eye decline on some sub-tasks. We selecte one error example each from MO/Orientation and RS/Position and display them in Figure 5. For MO/Orientation, the low direct response</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The relationship between the number of search steps and the performance of the MLLM. The experimental statistics are derived from LLaVA-ov-7B's results on <em>V</em>* Bench.</p>
<p>scores for LLaVA-ov, as seen in the Table 3, along with error example in the figure, suggest a probable deficiency of orientation data during training, negatively impacting model performance in this aspect. For RS/Position, despite Zoom Eye locates the target, the final response was incorrect, suggesting the model struggles to link positional relationships between the full image and sub-images, resulting in a marked decline in performance on this sub-task. These error examples reveal the model's deficiencies, by which we will guide the direction of improvements in the model's capabilities in our future work.</p>
<h3>4.4 Ablation Studies</h3>
<h3>4.4.1 Vision-level test-time scaling</h3>
<p>We progressively reduce the answering confidence threshold τ and analyze the relationship between the number of search steps and the performance of the MLLM, as illustrated in Figure 4.</p>
<p>From the figure, it can be seen that as the number of search steps increases, the model performance improves and eventually stabilizes. This behavior is analogous to the test-time scaling in text-level reasoning, where the accuracy of the final answer improves with more CoT tokens being explored. This finding could be viewed as a form of <strong>vision-level test-time scaling</strong>, where exploring more detailed zoomed information instead of the static image could enhance the ability of MLLM to generate more accurate responses.</p>
<p>When deploying Zoom Eye in real-world scenarios, we can adjust the confidence threshold or the maximum number of search steps based on specific needs to achieve the best trade-off between performance and efficiency.</p>
<table>
<thead>
<tr>
<th>Used MLLM</th>
<th>Zoom Successfully</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaVA-ov-7B</td>
<td>✓</td>
<td>93.45</td>
</tr>
<tr>
<td>LLaVA-ov-7B</td>
<td>✗</td>
<td>54.55</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of MLLM performance conditioned on whether zoom is successful. A zoom is considered successful when the searched box covers at least 50% of the target object. The experimental statistics are derived from <em>V</em>* Bench.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Sub-region</th>
<th><em>V</em>*</th>
<th>HR-4K</th>
<th>HR-8K</th>
<th>Avg. Search</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaVA-ov-7B</td>
<td>-</td>
<td>75.39</td>
<td>63.00</td>
<td>59.75</td>
<td>-</td>
</tr>
<tr>
<td>w/ Zoom Eye</td>
<td>4</td>
<td>90.58</td>
<td>69.63</td>
<td>69.25</td>
<td>8.20</td>
</tr>
<tr>
<td>w/ Zoom Eye</td>
<td>9</td>
<td>93.19</td>
<td>69.75</td>
<td>67.63</td>
<td>5.71</td>
</tr>
<tr>
<td>w/ Zoom Eye</td>
<td>16</td>
<td>92.15</td>
<td>70.38</td>
<td>69.75</td>
<td>5.02</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of MLLM performance conditioned on various number of the split sub-regions. Avg. Search means the number of average search steps in this setting.</p>
<h3>4.4.2 Does the Zoom operation contribute to the improvement of the MLLM?</h3>
<p>By comparing the answer accuracy of MLLM when Zoom is successful versus when it fails, we investigate the contribution of the Zoom operation to the model. As shown in Table 4, the accuracy sees a remarkable improvement (from 54.55% to 93.45%) when Zoom is successfully applied. This substantial gain highlights the critical role of the Zoom operation. By effectively refining the model's focus on relevant visual details, it contributes to more accurate and reliable responses, reinforcing its importance as a key mechanism for optimizing visual understanding.</p>
<h3>4.4.3 Impact of the various number of the split sub-regions</h3>
<p>In this part, we conduct an ablation study to examine how the number of sub-regions split in the image tree affects the performance of Zoom Eye. The results are summarized in Table 5. We observe that, as the number of sub-regions increases, the performance of Zoom Eye improves slightly, while the number of search steps decreases. Overall, the results remain stable across different sub-region settings, suggesting that Zoom Eye is robust to variations in zooming granularity. These findings highlight the role of zooming granularity in the Zoom Eye algorithm.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Examples of Zoom Eye. The resolution of the image is displayed. Red rectangles are patches searched by Zoom Eye.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Method</th>
<th>Training-free</th>
<th>V<sup>+</sup> Bench</th>
<th>HR-Bench 4K</th>
<th>HR-Bench 8K</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaVA-v1.5</td>
<td>7B</td>
<td>DC<sup>2</sup></td>
<td>✓</td>
<td>57.60</td>
<td>-</td>
<td>39.50</td>
</tr>
<tr>
<td></td>
<td>7B</td>
<td>VisCrop</td>
<td>✓</td>
<td>62.30</td>
<td>46.25</td>
<td>35.75</td>
</tr>
<tr>
<td></td>
<td>7B</td>
<td>Zoom Eye (Ours)</td>
<td>✓</td>
<td>83.25</td>
<td>53.25</td>
<td>50.75</td>
</tr>
<tr>
<td>Qwen2.5-VL</td>
<td>7B</td>
<td>Pixel Reasoner</td>
<td>✗</td>
<td>84.82</td>
<td>-</td>
<td>66.00</td>
</tr>
<tr>
<td></td>
<td>3B</td>
<td>Zoom Eye (Ours)</td>
<td>✓</td>
<td>89.01</td>
<td>-</td>
<td>68.38</td>
</tr>
</tbody>
</table>
<p>Table 6: Performance comparison between Zoom Eye and DC<sup>2</sup> <em>Wang et al. (2024)</em>, VisCrop <em>Zhang et al. (2025)</em>, and Pixel Reasoner <em>Su et al. (2025)</em>.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Input Res.</th>
<th>Search Res.</th>
<th>Zero shot</th>
<th>Indep. search</th>
<th>V<sup>+</sup> Bench</th>
<th>HR Bench</th>
</tr>
</thead>
<tbody>
<tr>
<td>V<sup>+</sup> search</td>
<td>224</td>
<td>768</td>
<td>✗</td>
<td>✗</td>
<td>75.39</td>
<td>37.81</td>
</tr>
<tr>
<td>Zoom Eye</td>
<td>224</td>
<td>224</td>
<td>✓</td>
<td>✓</td>
<td>81.58</td>
<td>47.63</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance comparison between Zoom Eye and V<sup>+</sup> Search <em>Wu and Xie (2024)</em>. Input Res.: The input resolution of the model generating the final response; Search Res.: The resolution required during the search process; Zero shot: Whether the method could be adapted for models without specialized additional training; Indep. search: Whether the method could be applied to an MLLM independently instead of requiring an additional search model.</p>
<h3>4.5 Compared with Other HR Processing Methods</h3>
<h4>4.5.1 Zoom Eye vs. V<sup>+</sup></h4>
<p>V<sup>+</sup> <em>Wu and Xie (2024)</em> is a LLM-guided search pipeline for MLLMs. To match the input resolution of the V<sup>+</sup> model, we specifically trained a 224px version of the LLaVA-v1.5 model for a fair comparison. Apart from using CLIP-224 <em>Radford et al. (2021)</em> as the vision encoder, all other settings were identical to those of LLaVA-v1.5.</p>
<p>From Table 7, it is evident that compared to V<sup>+</sup>, our method offers several advantages: (1) The V<sup>+</sup> pipeline requires specifically targeted training data, making zero-shot searches impossible, whereas our method utilizes the native capabilities of MLLMs, allowing adaptation to any MLLM without additional training; (2) V<sup>+</sup>'s search process necessitates the integration of another specially trained MLLM to guide the search, along with an extra high-resolution image encoder <em>Minderer et al. (2022)</em> (768px), while our approach operates at the native resolution of MLLMs and conducts searches independently; (3) Our method demonstrates superior performance.</p>
<h4>4.5.2 Zoom Eye vs. Others</h4>
<p>We also provide a comparison between Zoom Eye and DC<sup>2</sup> <em>Wang et al. (2024)</em>, VisCrop <em>Zhang et al. (2025)</em>, and Pixel Reasoner <em>Su et al. (2025)</em>. The results in Table 6 consistently demonstrate the superior performance of Zoom Eye. We provide a further discussion regarding the comparison between Zoom Eye and these methods in Appendix B.</p>
<h3>4.6 Case Study</h3>
<p>We visualize some cases in Figure 5, along with error examples mentioned in §4.3. We present cases for single type 1 cue, multiple type 1 cues, and</p>
<p>type 2 cue, which is corresponding to the examples in Table 1. From the figure, it can be observed that Zoom Eye accurately seeks out cues, enabling the MLLM to focus on the crucial visual information and respond to queries precisely.</p>
<h2>5 Related Work</h2>
<p>Multimodal LLMs. Since the advent of large language models (LLMs), they have achieved success across various linguistic applications, such as in-context learning (Dong et al., 2022; Zhang et al., 2022; Li et al., 2025b,a) and retrieval augmented generation (Liu et al., 2024d; Zhao et al., 2024b,c), which facilitated the emergence of Multimodal LLMs, with pioneering works including (Alayrac et al., 2022; Li et al., 2023; Koh et al., 2023). Following these, LLaVA (Liu et al., 2024c) employed GPT-4 (Achiam et al., 2023) to develop training data, inspiring a series of works focused on visual instruction data (Liu et al., 2024a; Dai et al., 2023; Chen et al., 2023b). Since these models utilize pretrained vision encoders (Radford et al., 2021; Zhai et al., 2023) to process image, the resolution that MLLMs can handle is limited by the input resolution of these encoders. To address it, AnyRes was developed to flexibly manage varying resolutions (Liu et al., 2024b; Chen et al., 2024b). Additionally, there are efforts focused on utilizing high-resolution encoders (Lu et al., 2024; Wei et al., 2025) or investigating the selected layer of the encoders (Chen et al., 2025). However, despite these efforts, the perception of the image by the MLLM remains as the original image itself. We hope to enable MLLMs to explore the varying hierarchical features of images to capture key information.
Tree-based search. Tree-based search algorithms have been applied in text-only LLM reasoning and have demonstrated superior performance. Early works such as (Wei et al., 2022; Wang et al., 2022) relied on chain reasoning, a method susceptible to errors in one step propagating through subsequent steps. Consequently, ToT (Yao et al., 2024b) proposed a tree-based reasoning method that leverages the expansiveness of tree structures to widen the reasoning space. Simultaneously, several similar studies were also introduced, which define a decomposed question step as a node and utilize beam search (Xie et al., 2023) and Monte-Carlo Tree Search (Hao et al., 2023) to uncover optimal solutions. Subsequently, TS-LLM (Feng et al., 2023) utilized reinforcement learning to increase search depth, further enhancing reasoning performance. In our work, we conceptualize an image as a tree to search for crucial visual information using a specific algorithm. A close-related work is $V^{*}$ (Wu and Xie, 2024), and we describe the detailed comparison with it in $\S 4.5 .1$.</p>
<h2>6 Limitations</h2>
<p>Although Zoom Eye offers several advantages, such as strong interpretability, model-agnostic, and training-free, it also comes with certain limitations. First, the current search procedure relies on heuristic strategies, including manually defined ranking functions and stopping criteria. While these designs are effective in many settings, they may not generalize optimally across all image types or task conditions. Second, the image is partitioned into fixed-size patches to construct the hierarchical tree structure, which may not align well with the semantic regions of the image. As a result, some visual cues may be fragmented or overlooked during traversal. Lastly, Zoom Eye is primarily tailored for natural images with spatially distributed visual elements. It is less applicable to document understanding tasks, where layout, reading order, and structured information (e.g., tables, forms) are central. Addressing these challenges-such as by integrating learnable search strategies or adaptive patch partitioning-will be an important direction for future work.</p>
<h2>7 Conclusion</h2>
<p>To address the limitations of text-level visual reasoning, we propose Zoom Eye, a type of visionlevel reasoning method, a tree search algorithm designed to navigate the hierarchical and visual nature of images to capture detailed crucial information. Through prompts guiding MLLMs, we develop a ranking function and stopping criterion for Zoom Eye, which steers models to efficiently search along the image tree, seek out pertinent information, and accurately respond to related queries. Experiments show the broad-applicability and effectiveness of Zoom Eye, which substantially improves MLLMs' performance. Notably, Zoom Eye exhibits a testtime scaling phenomenon analogous to that observed in text-level reasoning. Meanwhile, through the analysis of failure cases, we identify several inherent limitations in current MLLMs' visual reasoning capabilities, which we aim to address in future work.</p>
<h2>8 Acknowledgements</h2>
<p>This research is supported by National Key R\&amp;D Program of China under grant (2022YFF0902600) and "Pioneer" and "Leading Goose" R\&amp;D Program of Zhejiang (2023C01045).</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.
01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, and 13 others. 2024. Yi: Open foundation models by 01.ai. Preprint, arXiv:2403.04652.</p>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, and 1 others. 2022. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736.</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2):3.</p>
<p>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923.</p>
<p>Haoran Chen, Junyan Lin, Xinhao Chen, Yue Fan, Xin Jin, Hui Su, Jianfeng Dong, Jinlan Fu, and Xiaoyu Shen. 2025. Rethinking visual layer selection in multimodal llms. Preprint, arXiv:2504.21447.</p>
<p>Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023a. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478.</p>
<p>Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023b. Sharegpt4v: Improving large multimodal models with better captions. arXiv preprint arXiv:2311.12793.</p>
<p>Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, and 1 others. 2024a.</p>
<p>Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271.</p>
<p>Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, and 1 others. 2024b. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821.</p>
<p>Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Preprint, arXiv:2305.06500.</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, and 1 others. 2022. A survey on incontext learning. arXiv preprint arXiv:2301.00234.</p>
<p>Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. 2024. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432.</p>
<p>Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2023. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179.</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948.</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8154-8173.</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai ol system card. arXiv preprint arXiv:2412.16720.</p>
<p>Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. 2023. Grounding language models to images for multimodal inputs and outputs. In International Conference on Machine Learning, pages 17283-17300. PMLR.</p>
<p>Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326.</p>
<p>Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 19730-19742. PMLR.</p>
<p>Yanshu Li, Hongyang He, Yi Cao, Qisen Cheng, Xiang Fu, and Ruixiang Tang. 2025a. M2iv: Towards efficient and fine-grained multimodal in-context learning in large vision-language models. arXiv preprint arXiv:2504.04633.</p>
<p>Yanshu Li, Tian Yun, Jianjiang Yang, Pinyuan Feng, Jinfa Huang, and Ruixiang Tang. 2025b. Taco: Enhancing multimodal in-context learning via task mapping-guided sequence configuration. arXiv preprint arXiv:2505.17098.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024a. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296-26306.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024b. Llavanext: Improved reasoning, ocr, and world knowledge.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024c. Visual instruction tuning. Advances in neural information processing systems, 36.</p>
<p>Jingyu Liu, Jiaen Lin, and Yong Liu. 2024d. How much can rag help the reasoning of llm? arXiv preprint arXiv:2410.02338.</p>
<p>Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, and 1 others. 2024. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525.</p>
<p>Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. 2024. Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003.</p>
<p>Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, and 1 others. 2025. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365.</p>
<p>M Minderer, A Gritsenko, A Stone, M Neumann, D Weissenborn, A Dosovitskiy, A Mahendran, A Arnab, M Dehghani, Z Shen, and 1 others. 2022. Simple open-vocabulary object detection with vision transformers. arxiv 2022. arXiv preprint arXiv:2205.06230, 2.</p>
<p>OpenAI. 2025. o3/o4 mini system card. https:// openai.com/index/o3-o4-mini-system-card/.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR.</p>
<p>Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. 2025. Vlm-r1: A stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615.</p>
<p>Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. 2025. Pixel reasoner: Incentivizing pixel-space reasoning with curiositydriven reinforcement learning. arXiv preprint arXiv:2505.15966.</p>
<p>Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. 2023. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and 1 others. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, and Dacheng Tao. 2024. Divide, conquer and combine: A training-free framework for high-resolution image perception in multimodal large language models. arXiv preprint arXiv:2408.15556.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. 2025. Vary: Scaling up the vision vocabulary for large vision-language model. In European Conference on Computer Vision, pages 408-424. Springer.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837.</p>
<p>Penghao Wu and Saining Xie. 2024. V?: Guided visual search as a core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094.</p>
<p>Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, MinYen Kan, Junxian He, and Qizhe Xie. 2023. Decomposition enhances reasoning via self-evaluation guided decoding. Preprint, arXiv:2305.00633.</p>
<p>Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. 2024. Llava-cot: Let vision language models reason step-by-step. Preprint, arXiv:2411.10440.</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, and 1 others. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671.</p>
<p>Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, and 1 others. 2024a. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024b. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.</p>
<p>Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. Preprint, arXiv:2303.15343.</p>
<p>Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. 2025. MLLMs know where to look: Training-free perception of small visual details with multimodal LLMs. In The Thirteenth International Conference on Learning Representations.</p>
<p>Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, and 1 others. 2024. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<p>Tiancheng Zhao, Qianqian Zhang, Kyusong Lee, Peng Liu, Lu Zhang, Chunxin Fang, Jiajia Liao, Kelei Jiang, Yibo Ma, and Ruochen Xu. 2024a. Omchat: A recipe to train multimodal language models with strong long context and video understanding. arXiv preprint arXiv:2407.04923.</p>
<p>Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, and Min Zhang. 2024b. Seer: Self-aligned evidence extraction for retrieval-augmented generation. arXiv preprint arXiv:2410.11315.</p>
<p>Xinping Zhao, Yan Zhong, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Dongfang Li, Baotian Hu, and Min Zhang. 2024c. Funnelrag: A coarse-to-fine progressive retrieval paradigm for rag. arXiv preprint arXiv:2410.10293.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Jiaxing Zhang, Yujiu Yang, and 1 others. 2023. Solving math word problems via cooperative reasoning induced language models. In The 61st Annual Meeting Of The Association For Computational Linguistics.</p>
<h2>A Results of More MLLMs on High-Resolution Benchmark</h2>
<p>We present the results of additional MLLMs on high-resolution benchmarks in Table 8, including models of smaller or larger scale. Consistent with the findings in the main paper, all evaluated models exhibit improved performance after being adapted to Zoom Eye, further demonstrating the effectiveness of vision-level reasoning in handling complex visual scenarios.</p>
<h2>B Compared with Other HR Processing Methods</h2>
<h3>B. 1 Zoom Eye vs. DC ${ }^{2}$</h3>
<p>$\mathrm{DC}^{2}$ (Wang et al., 2024) (Divide, Conquer, and Combine) is a framework that supplements visual information using text for high-resolution images understanding. Like our approach, it builds an image as a tree. The MLLM then generates textual descriptions for each leaf patch. These descriptions are then relayed to the parent nodes, which create combined descriptions by synthesizing the contents from their child nodes with their own. This process continues up to the root node.</p>
<p>Our approach differs from $\mathrm{DC}^{2}$ in two key ways: (1) $\mathrm{DC}^{2}$ uses textual modalities to supplement the missing visual information at high resolutions, whereas Zoom Eye employs simulated zooming operations, allowing the MLLM to actively discover missing visual details; (2) $\mathrm{DC}^{2}$ is question-agnostic, generating descriptions consistently across different questions, which may lead to unfocused textual content. In contrast, Zoom Eye is question-driven in its visual cues searching, yielding more precise visual information that is instrumental in answering the input question. Table 6 shows the better performance of Zoom Eye.</p>
<h3>B. 2 Zoom Eye vs. Pixel Reasoner</h3>
<p>Pixel Reasoner (Su et al., 2025) is a multimodal model that combines curated reasoning trajectories with curiosity-driven reinforcement learning to enable effective zooming operations and significantly improve fine-grained visual reasoning.</p>
<p>The results on Table 6 demonstrate that: (1) Zoom Eye outperforms Pixel Reasoner on both benchmarks, even with a smaller backbone (Qwen2.5VL-3B vs. Qwen2.5VL-7B), demonstrating its superior capability in enhancing vision-level visual reasoning within MLLMs; (2) More importantly, Zoom Eye is entirely training-free, relying solely on prompting. In contrast, Pixel Reasoner requires constructing a supervised fine-tuning dataset pipeline and involves resource-intensive reinforcement learning.</p>
<p>This comparison underscores Zoom Eye's core strength: achieving competitive or superior performance without any fine-tuning or task-specific training, making it a more adaptable solution in vision-level visual reasoning.</p>
<h3>B. 3 Zoom Eye vs. VisCrop</h3>
<p>VisCrop crops and re-feeds the region focused by the attention map into the model - essentially enabling the MLLM to "look again" at a single focal point.</p>
<p>In contrast, Zoom Eye models the image as a tree, and guides the MLLM through a confidencedriven zoom-in process until a high-confidence answer node is found. This enables the MLLM to "look multiple times" in a more structured and semantically informed way.</p>
<p>From Table 6, we note that as resolution increases (from HR-4K to HR-8K), MKWTL's performance degrades significantly, likely because a single "look again" fails to capture fine-grained cues in these complex scenarios. In contrast, Zoom Eye maintains stable performance, showcasing the advantage of "look multiple times, until desirable cues are found to answer the question".</p>
<p>This comparison illustrates that MKWTL enables "a second glance", while Zoom Eye further enables "multi-step visual reasoning", being increasingly beneficial as visual complexity grows.</p>
<h2>C Complete Results on MME-RealWorld Benchmark</h2>
<p>We provide the complete results of Zoom Eye on MME-RealWorld Benchmark (Zhang et al., 2024), as show in Table 9. This benchmark includes 5 data categories: Monitoring (MO), Autonomous Driving (AD), OCR, emote Sensing (RS), and Diagram and Table (TD). Since Zoom Eye is not applicable to the TD task, we do not conduct tests on it. It could be observed that Zoom Eye improves the performance of LLaVA-ov-7B across most subtasks, with particularly significant improvements in certain tasks. For instance, it achieves a 20.22\% improvement in the Person ${ }<em _text="\text" _vehicle="{vehicle">{\text {color }}$ task, a $29.11 \%$ improvement in the Motion ${ }</em>$ task, demonstrating the effectiveness of Zoom Eye. However,}}$ task, and a $12.93 \%$ improvement in the Visual ${ }_{\text {trafficsignal }</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>$V^{*}$ Bench</th>
<th></th>
<th></th>
<th>HR-Bench 4K</th>
<th></th>
<th></th>
<th>HR-Bench 8K</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Attr</td>
<td>Spatial</td>
<td>Overall</td>
<td>FSP</td>
<td>FCP</td>
<td>Overall</td>
<td>FSP</td>
<td>FCP</td>
<td>Overall</td>
</tr>
<tr>
<td>Baseline and Local Input Zoom Eye</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaVA-v1.5-13B <em>Liu et al. (2024a)</em></td>
<td>41.74</td>
<td>55.26</td>
<td>47.12</td>
<td>45.25</td>
<td>41.25</td>
<td>43.25</td>
<td>37.50</td>
<td>38.0</td>
<td>37.75</td>
</tr>
<tr>
<td>LLaVA-v1.5-13B w/ Zoom Eye</td>
<td>87.83</td>
<td>81.58</td>
<td>85.34</td>
<td>73.0</td>
<td>43.25</td>
<td>58.13</td>
<td>67.25</td>
<td>45.50</td>
<td>56.38</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>$+46.09$</td>
<td>$+26.32$</td>
<td>$+38.22$</td>
<td>$+27.75$</td>
<td>$+2.00$</td>
<td>$+14.88$</td>
<td>$+29.75$</td>
<td>$+7.50$</td>
<td>$+18.63$</td>
</tr>
<tr>
<td>Baseline and Global+Local Input Zoom Eye</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaVA-sv-0.5B <em>Li et al. (2024)</em></td>
<td>63.48</td>
<td>64.47</td>
<td>63.87</td>
<td>63.50</td>
<td>39.50</td>
<td>51.50</td>
<td>47.25</td>
<td>38.25</td>
<td>42.75</td>
</tr>
<tr>
<td>LLaVA-sv-0.5B w/ Zoom Eye</td>
<td>85.22</td>
<td>73.68</td>
<td>80.62</td>
<td>75.50</td>
<td>39.75</td>
<td>57.63</td>
<td>68.50</td>
<td>38.25</td>
<td>53.38</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>$+21.74$</td>
<td>$+9.21$</td>
<td>$+16.75$</td>
<td>$+12.00$</td>
<td>$+0.25$</td>
<td>$+6.13$</td>
<td>$+21.25$</td>
<td>$+0.00$</td>
<td>$+10.63$</td>
</tr>
<tr>
<td>InternVL2.5-4B <em>Chen et al. (2024a)</em></td>
<td>69.57</td>
<td>71.05</td>
<td>70.16</td>
<td>77.50</td>
<td>53.75</td>
<td>65.63</td>
<td>63.00</td>
<td>49.25</td>
<td>56.13</td>
</tr>
<tr>
<td>InternVL2.5-4B w/ Zoom Eye</td>
<td>85.22</td>
<td>77.63</td>
<td>82.20</td>
<td>81.25</td>
<td>56.75</td>
<td>69.00</td>
<td>80.00</td>
<td>52.25</td>
<td>66.13</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>$+15.65$</td>
<td>$+6.58$</td>
<td>$+12.04$</td>
<td>$+3.75$</td>
<td>$+3.00$</td>
<td>$+3.27$</td>
<td>$+17.00$</td>
<td>$+3.00$</td>
<td>$+10.00$</td>
</tr>
<tr>
<td>InternVL2.5-26B <em>Chen et al. (2024a)</em></td>
<td>73.91</td>
<td>72.37</td>
<td>73.30</td>
<td>82.00</td>
<td>66.25</td>
<td>74.13</td>
<td>73.00</td>
<td>61.75</td>
<td>67.38</td>
</tr>
<tr>
<td>InternVL2.5-26B w/ Zoom Eye</td>
<td>91.30</td>
<td>86.84</td>
<td>89.53</td>
<td>89.75</td>
<td>68.25</td>
<td>79.00</td>
<td>89.25</td>
<td>63.00</td>
<td>76.13</td>
</tr>
<tr>
<td>$\Delta$</td>
<td>$+17.39$</td>
<td>$+14.47$</td>
<td>$+16.23$</td>
<td>$+7.75$</td>
<td>$+2.00$</td>
<td>$+4.87$</td>
<td>$+16.25$</td>
<td>$+1.25$</td>
<td>$+8.75$</td>
</tr>
</tbody>
</table>
<p>Table 8: Results of more models on high-resolution benchmarks. performance declines were observed in some subtasks when using Zoom Eye. We have analyzed these cases in the main paper, revealing certain limitations of the employed MLLM. Addressing these issues will be a focus of our future work.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th></th>
<th>LLaVA${ }_{s v}$-7B</th>
<th>+ZoomEye</th>
<th>$\Delta \uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td>MO</td>
<td>Calculate</td>
<td>36.33</td>
<td>38.67</td>
<td>+2.34</td>
</tr>
<tr>
<td></td>
<td>Intention</td>
<td>27.55</td>
<td>38.78</td>
<td>+11.23</td>
</tr>
<tr>
<td></td>
<td>Property</td>
<td>55.0</td>
<td>60.0</td>
<td>+5.0</td>
</tr>
<tr>
<td></td>
<td>Vehicle ${ }_{\text {counting }}$</td>
<td>59.89</td>
<td>61.14</td>
<td>+1.25</td>
</tr>
<tr>
<td></td>
<td>Person ${ }_{\text {counting }}$</td>
<td>61.35</td>
<td>61.87</td>
<td>+0.52</td>
</tr>
<tr>
<td></td>
<td>Vehicle ${ }_{\text {location }}$</td>
<td>33.82</td>
<td>33.82</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Vehicle ${ }_{\text {orientation }}$</td>
<td>19.35</td>
<td>18.71</td>
<td>-0.64</td>
</tr>
<tr>
<td></td>
<td>Vehicle ${ }_{\text {color }}$</td>
<td>43.65</td>
<td>49.24</td>
<td>+5.59</td>
</tr>
<tr>
<td></td>
<td>Person ${ }_{\text {color }}$</td>
<td>24.72</td>
<td>44.94</td>
<td>+20.22</td>
</tr>
<tr>
<td></td>
<td>Person ${ }_{\text {orientation }}$</td>
<td>10.53</td>
<td>10.53</td>
<td>-</td>
</tr>
<tr>
<td>AD</td>
<td>Intention ${ }_{\text {age }}$</td>
<td>28.62</td>
<td>28.95</td>
<td>+0.33</td>
</tr>
<tr>
<td></td>
<td>Intention ${ }_{\text {pedestrian }}$</td>
<td>52.43</td>
<td>53.40</td>
<td>+0.97</td>
</tr>
<tr>
<td></td>
<td>Intention ${ }_{\text {vehicle }}$</td>
<td>30.92</td>
<td>33.33</td>
<td>+2.41</td>
</tr>
<tr>
<td></td>
<td>Interaction ${ }_{\text {other/other }}$</td>
<td>12.94</td>
<td>13.43</td>
<td>+0.49</td>
</tr>
<tr>
<td></td>
<td>Attention ${ }_{\text {trafficequal }}$</td>
<td>71.89</td>
<td>68.66</td>
<td>-3.23</td>
</tr>
<tr>
<td></td>
<td>Interaction ${ }_{\text {age/pedestrian }}$</td>
<td>27.36</td>
<td>28.30</td>
<td>+0.94</td>
</tr>
<tr>
<td></td>
<td>Interaction ${ }_{\text {age/trafficequal }}$</td>
<td>22.86</td>
<td>25.71</td>
<td>+2.85</td>
</tr>
<tr>
<td></td>
<td>Interaction ${ }_{\text {age/vehicle }}$</td>
<td>20.79</td>
<td>19.80</td>
<td>-0.99</td>
</tr>
<tr>
<td></td>
<td>Objects ${ }_{\text {identity }}$</td>
<td>64.40</td>
<td>64.85</td>
<td>+0.45</td>
</tr>
<tr>
<td></td>
<td>Motion ${ }_{\text {vehicle }}$</td>
<td>23.42</td>
<td>52.53</td>
<td>+29.11</td>
</tr>
<tr>
<td></td>
<td>Motion ${ }_{\text {authoriteles }}$</td>
<td>34.26</td>
<td>34.75</td>
<td>+0.49</td>
</tr>
<tr>
<td></td>
<td>Visual ${ }_{\text {trafficequal }}$</td>
<td>60.20</td>
<td>73.13</td>
<td>+12.93</td>
</tr>
<tr>
<td></td>
<td>Motion ${ }_{\text {pedestrain }}$</td>
<td>34.15</td>
<td>40.85</td>
<td>+6.70</td>
</tr>
<tr>
<td></td>
<td>Object ${ }_{\text {count }}$</td>
<td>37.92</td>
<td>39.86</td>
<td>+1.94</td>
</tr>
<tr>
<td></td>
<td>Motion ${ }_{\text {managementian }}$</td>
<td>31.24</td>
<td>31.64</td>
<td>+0.40</td>
</tr>
<tr>
<td>OCR</td>
<td>Scene understanding</td>
<td>64.80</td>
<td>64.80</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Character identification</td>
<td>57.60</td>
<td>56.40</td>
<td>-1.20</td>
</tr>
<tr>
<td></td>
<td>Adver \&amp; product</td>
<td>76.64</td>
<td>78.37</td>
<td>+1.73</td>
</tr>
<tr>
<td></td>
<td>Book map poster</td>
<td>77.17</td>
<td>75.24</td>
<td>-1.93</td>
</tr>
<tr>
<td></td>
<td>License</td>
<td>80.16</td>
<td>82.39</td>
<td>+2.23</td>
</tr>
<tr>
<td></td>
<td>Phone \&amp; address</td>
<td>77.82</td>
<td>81.28</td>
<td>+3.46</td>
</tr>
<tr>
<td></td>
<td>Text recog</td>
<td>74.87</td>
<td>77.13</td>
<td>+2.26</td>
</tr>
<tr>
<td>RS</td>
<td>Color</td>
<td>59.60</td>
<td>60.56</td>
<td>+0.96</td>
</tr>
<tr>
<td></td>
<td>Count</td>
<td>32.95</td>
<td>35.56</td>
<td>+2.61</td>
</tr>
<tr>
<td></td>
<td>Position</td>
<td>61.40</td>
<td>48.45</td>
<td>-12.95</td>
</tr>
</tbody>
</table>
<p>Table 9: Performance comparison between Zoom Eye and the baseline model on MME-RealWorld benchmark. MO (Monitoring), AD (Autonomous Driving), OCR and RS (Remote Sensing) are data categories within this benchmark.</p>
<h2>D Implementation Details</h2>
<p>Due to the page limit of the main paper, we provide more implementation details here. In §D. 1 and §D.2, we detail the implementation of Local Input and Global+Local Input, respectively. §D. 3 describes the implementations common to both. Finally, based on the introductions in the first three subsections, we present the complete algorithm workflow of Zoom Eye in §D. 4 .</p>
<h3>D.1 Local Input</h3>
<p>We select LLaVA-v1.5-7B <em>Liu et al. (2024a)</em> and 13B as our MLLMs, with the vision encoder’s input resolution as 336px and naive processing. We set the threshold of the stopping criterion at $\tau=0.8$ and define the weighted function as $\mathcal{W}=\frac{1-b}{D^{2}} \times d^{2}+b$, where $D$ denotes the depth of the image tree, $d$ is the depth of the visited node during the search, and $b$ is a bias value, set here at 0.2 . The prompt templates for calculating existing confidence, latent confidence, and answering confidence (please refer to $\S 3.3$ and $\S 3.4$ for the discussion on these three confidence values) are set as:</p>
<h1>Prompt Templates of Local Input</h1>
<ul>
<li>$p_{e}(o)$ : <local patch> Is there a ${o}$ in the image? Answer Yes or No.</li>
<li>$p_{l}(o)$ : <local patch> According to your common sense knowledge and the content of the image, is it possible to find a ${o}$ in the image? Answer Yes or No and tell the reason.</li>
<li>$p_{a}(q)$ : <local patch> Question: ${q}$ \nCould you answer the question based on the the available visual information? Answer Yes or No.
where the $o$ and $q$ are the input visual cue and question, which could be referred to $\S 3.5$.</li>
</ul>
<p>As mentioned in $\S 3.5$, the final visual input uses the union of all searched patches. However, when multiple distant patches are combined, they may form a large image. For MLLMs using naive resize processing, information can still be lost during downsampling. Therefore, for the Local Input Zoom Eye with naive resize processing, when the area of $b^{*}$ is relatively large (with the longer side exceeding 1000px), we skip the Union operation. Instead, we paste the searched patches onto a blank image according to their relative positions in the original image, and then feed it to the MLLMs. An example is shown in Figure 6.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: If the area of the union bounding box is too large, we paste the searched patches onto a blank image according to their relative positions in the original image, and then feed it to the MLLMs.It is notable that this operation is only applied to Local Input, while for Local+Global, we consistently provide the MLLMs with the full union patch as input.</p>
<h2>D. 2 Global + Local Input</h2>
<p>We select LLaVA-ov(oneVision)-0.5B (Li et al., 2024) and 7B as our MLLMs, with the vision encoder's input resolution as 384px and AnyRes processing. We define the maximum AnyRes block as 12 , set $\tau$ at 0.6 and define $\mathcal{W}$ as $\frac{1-b}{D^{2}} \times d^{2}+b$, where $D$ denotes the depth of the image tree, $d$ is the depth of the visited node during the search, and $b$ is a bias value, set here at 0.6 . The prompt templates for calculating existing confidence, latent confidence, and answering confidence are set as:</p>
<h2>Prompt Templates of Global + Local Input</h2>
<ul>
<li>$p_{e}(o)$ : <global image><local patch> Is there a ${o}$ in the zoomed-in view? Answer Yes or No.</li>
<li>$p_{l}(o)$ : <global image><local patch> According to your common sense knowledge and the content of the zoomed-in view, along with its location in the image, is it possible to find a ${o}$ by further zooming in the current view? Answer Yes or No and tell the reason.</li>
<li>$p_{a}(q)$ : <global image><local patch> Question: ${q} \quad \backslash n$ Could you answer the question based on the the available visual information? Answer Yes or No.</li>
</ul>
<h2>D. 3 Additional Settings</h2>
<p>For both input implementation, we set the maximum search depth at 2 when searching for type 2 cues to save costs. In $\S 3.5$, we state that we search the whole tree to add all nodes with sufficient existing confidence to $L$ if type 2 cue is generated. Thus, we introduce an additional threshold $\tau_{2}$ for this condition, which is set at 0.8 for both implementation. The decomposed question template $\mathrm{p}<em i="i">{d q}\left(o</em>$ values of all nodes in the tree, would compel MLLMs to search the entire tree unnecessarily, thus wasting time. Therefore, we adopt a strategy where $\tau$ is progressively reduced as the number of search steps increases. Specifically, if the number of search steps exceeds}\right)$ is assigned as "What is the appearance of the $\left{o_{i}\right}$ ?". For type 1 search, a key aspect is determining the value of $\tau$. If it is set too low, an incorrect patch, which probably lead to erroneous guidance for MLLMs, may be selected. Conversely, setting $\tau$ too high, surpassing the $c_{a</p>
<p>the step threshold $C$, we reduce the value of $\tau$ by 0.1 . This reduction occurs every $\delta$ steps, until the $c_{a}$ value of a node having been visited surpasses $\tau$ or $\tau$ falls below a predefined minimum limit $\tau_{\min }$. For both implementation, we set $\delta$ at $2, \tau_{\min }$ at 0 , and $C$ as $D \times 3$. Finally, the in-context examples we utilized to generate visual cues are denote as $\left(q^{(1)}, \mathbf{o}^{(1)}, \ldots, q^{(m)}, \mathbf{o}^{(m)}\right)$ and are presented at the end of this document.</p>
<h3>13.4 Complete Algorithm Workflow</h3>
<p>With the aforementioned notation and description in place, we provide the complete algorithm workflow in Algorithm 3, where the Zoom Eye search method is shown in Algorithm 4.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 3 Complete Algorithm Workflow of
Zoom Eye
Require: Multimodal LLM \(\Phi_{\theta}\), input question-image pair (I,
    \(q\), decomposed question template \(\mathrm{p}_{\mathrm{dq}},\) in-context exam-
    ples \(\left(q^{(1)}, \mathbf{o}^{(1)}, \ldots, q^{(m)}, \mathbf{o}^{(m)}, q\right)\)
\(1:\left\{o_{1}, \ldots, o_{k}\right\}\)
    \(\Phi_{\theta}\).generate \(\left(q^{(1)}, \mathbf{o}^{(1)}, \ldots, q^{(m)}, \mathbf{o}^{(m)}, q\right)\)
2: Initialize \(L\) as the empty list
3: Build I as a tree \(T\)
4: for \(i=1, \ldots, k\) do
5: if \(k==1\) then
6: \(\quad q_{s} \leftarrow q\)
7: else
8: \(\quad q_{s} \leftarrow \mathrm{p}_{\mathrm{dq}}\left(o_{i}\right)\)
9: \(\quad \text { L.extend }\left(\operatorname{ZOOM} \operatorname{EYe}\left(T, o_{i}, q_{s}\right)\right)\)
10: \(\mathbf{b}^{*} \leftarrow\) Union bounding-boxes of all nodes in \(L\)
11: \(n^{*} \leftarrow\{\mathbf{I}, \mathbf{b}^{*}\}\)
12: Final response \(\leftarrow \Phi_{\theta}\).generate \(\left(V\left(n^{*}\right), q\right)\)
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span> <span class="mi">4</span> <span class="n">Zoom</span> <span class="n">Eye</span> <span class="n">Search</span>
<span class="n">Require</span><span class="p">:</span> <span class="n">Threshold</span> <span class="n">of</span> <span class="nb">type</span> <span class="mi">1</span> <span class="n">cue</span> <span class="ow">and</span> <span class="nb">type</span> <span class="mi">2</span> <span class="n">cue</span> \<span class="p">(</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">tau</span><span class="p">,</span> \<span class="n">tau_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">),</span>
    <span class="n">minimum</span> <span class="n">limit</span> \<span class="p">(</span>\<span class="n">tau_</span><span class="p">{</span>\<span class="nb">min</span> <span class="p">}</span>\<span class="p">),</span> <span class="n">interval</span> \<span class="p">(</span>\<span class="n">delta</span>\<span class="p">)</span>
    <span class="n">function</span> <span class="n">ZOOM</span> <span class="n">EYE</span><span class="p">(</span> \<span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">o_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span> <span class="n">q_</span><span class="p">{</span><span class="n">s</span><span class="p">}</span>\<span class="p">)</span> <span class="p">)</span>
        <span class="n">Initialize</span> \<span class="p">(</span><span class="n">Q</span>\<span class="p">)</span> <span class="k">as</span> <span class="n">the</span> <span class="n">empty</span> <span class="n">queue</span> \<span class="p">(</span>\<span class="p">}</span>\<span class="p">)</span>
        \<span class="p">(</span><span class="n">Q</span> <span class="o">.</span> \<span class="n">operatorname</span><span class="p">{</span><span class="n">append</span><span class="p">}(</span><span class="n">T</span>\<span class="p">)</span><span class="o">.</span> <span class="n">root</span><span class="p">)</span>
        <span class="n">Initialize</span> \<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span> <span class="k">as</span> <span class="n">the</span> <span class="n">empty</span> <span class="nb">list</span>
        <span class="n">search</span> <span class="nb">all</span> \<span class="p">(</span>\<span class="n">leftarrow</span> <span class="n">o_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">search</span> <span class="nb">all</span> <span class="n">then</span>
            <span class="n">ZOOM</span> <span class="n">EYE</span> <span class="n">TYPE</span> <span class="mi">1</span><span class="p">(</span> \<span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span> <span class="n">q_</span><span class="p">{</span><span class="n">s</span><span class="p">},</span> \<span class="n">tau</span>\<span class="p">)</span> <span class="p">)</span>
        <span class="k">else</span>
            <span class="n">ZOOM</span> <span class="n">EYE</span> <span class="n">TYPE</span> <span class="mi">2</span><span class="p">(</span> \<span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span> \<span class="n">tau_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span>\<span class="p">)</span> <span class="p">)</span>
        <span class="k">return</span> \<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span>
    <span class="n">function</span> <span class="n">ZOOM</span> <span class="n">EYE</span> <span class="n">TYPE</span> <span class="mi">1</span><span class="p">(</span> \<span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span> <span class="n">q_</span><span class="p">{</span><span class="n">s</span><span class="p">},</span> \<span class="n">tau</span>\<span class="p">)</span> <span class="p">)</span>
        <span class="kn">import</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">R</span><span class="p">}</span>\<span class="p">)</span> <span class="ow">and</span> \<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">Algorithm</span> <span class="mi">2</span>
        <span class="n">count</span> \<span class="p">(</span>\<span class="n">leftarrow</span> <span class="mi">0</span>\<span class="p">)</span>
        \<span class="p">(</span><span class="n">C</span> \<span class="n">leftarrow</span> <span class="n">T</span> <span class="o">.</span> <span class="n">d</span> <span class="n">e</span> <span class="n">p</span> <span class="n">t</span> <span class="n">h</span> \<span class="n">times</span> <span class="mi">3</span>\<span class="p">)</span>
        <span class="n">Initialize</span> \<span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">m</span><span class="p">}</span>\<span class="p">)</span> <span class="k">as</span> \<span class="p">(</span><span class="n">T</span>\<span class="p">)</span><span class="o">.</span> <span class="n">root</span> <span class="n">to</span> <span class="n">record</span> <span class="n">the</span> <span class="n">node</span> <span class="k">with</span> <span class="n">the</span>
        <span class="n">best</span> \<span class="p">(</span><span class="n">c_</span><span class="p">{</span><span class="n">a</span><span class="p">}</span>\<span class="p">)</span>
        <span class="k">while</span> \<span class="p">(</span><span class="n">Q</span>\<span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">empty</span> <span class="n">do</span>
            \<span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span> \<span class="n">leftarrow</span> <span class="n">Q</span> <span class="o">.</span> \<span class="n">operatorname</span><span class="p">{</span><span class="n">pop</span><span class="p">}()</span>\<span class="p">)</span>
            \<span class="p">(</span><span class="n">N</span>\<span class="p">)</span><span class="o">.</span> <span class="n">append</span> \<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
            <span class="n">count</span> \<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span> <span class="n">count</span> <span class="o">+</span><span class="mi">1</span>
            <span class="k">if</span> <span class="n">count</span> \<span class="p">(</span>\<span class="n">geq</span>\<span class="p">)</span> <span class="n">C</span> <span class="n">then</span>
                \<span class="p">(</span>\<span class="n">tau</span> \<span class="n">leftarrow</span> \<span class="n">tau</span><span class="o">-</span><span class="mf">0.1</span>\<span class="p">)</span>
                \<span class="p">(</span><span class="n">C</span> \<span class="n">leftarrow</span> <span class="n">C</span><span class="o">+</span>\<span class="n">delta</span>\<span class="p">)</span>
                <span class="k">if</span> \<span class="p">(</span>\<span class="n">tau</span><span class="o">&lt;</span>\<span class="n">tau_</span><span class="p">{</span>\<span class="nb">min</span> <span class="p">}</span>\<span class="p">)</span> <span class="n">then</span>
                    <span class="k">break</span>
            <span class="k">if</span> \<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">},</span> <span class="n">q_</span><span class="p">{</span><span class="n">s</span><span class="p">},</span> \<span class="n">tau</span>\<span class="n">right</span><span class="p">)</span><span class="o">==</span>\<span class="p">)</span> <span class="kc">True</span> <span class="n">then</span>
                \<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">append</span> \<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
            <span class="k">break</span>
            <span class="k">else</span> <span class="k">if</span> \<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">m</span><span class="p">},</span> <span class="n">q_</span><span class="p">{</span><span class="n">s</span><span class="p">},</span> \<span class="n">tau</span>\<span class="n">right</span><span class="p">)</span><span class="o">==</span>\<span class="p">)</span> <span class="kc">True</span> <span class="n">then</span>
                \<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">append</span> \<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">m</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
                <span class="k">break</span>
            <span class="k">if</span> \<span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span> <span class="o">.</span> <span class="n">c_</span><span class="p">{</span><span class="n">a</span><span class="p">}</span> \<span class="n">geq</span> <span class="n">n_</span><span class="p">{</span><span class="n">m</span><span class="p">}</span> \<span class="n">cdot</span> <span class="n">c_</span><span class="p">{</span><span class="n">a</span><span class="p">}</span>\<span class="p">)</span> <span class="n">then</span>
                \<span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">m</span><span class="p">}</span> \<span class="n">leftarrow</span> <span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="p">)</span>
            \<span class="p">(</span><span class="n">s</span> \<span class="n">leftarrow</span> <span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">children</span><span class="o">.</span><span class="n">size</span>
            <span class="k">for</span> \<span class="p">(</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> \<span class="n">ldots</span><span class="p">,</span> <span class="n">s</span>\<span class="p">)</span> <span class="n">do</span>
                \<span class="p">(</span><span class="n">Q</span>\<span class="p">)</span><span class="o">.</span><span class="n">append</span> \<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
                \<span class="p">(</span><span class="n">Q</span> <span class="o">.</span> \<span class="n">operatorname</span><span class="p">{</span><span class="n">sort</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">R</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">o_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
    <span class="n">function</span> <span class="n">ZOOM</span> <span class="n">EYE</span> <span class="n">TYPE</span> <span class="mi">2</span><span class="p">(</span> \<span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span> \<span class="n">tau_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span>\<span class="p">)</span> <span class="p">)</span>
        <span class="k">while</span> \<span class="p">(</span><span class="n">Q</span>\<span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">empty</span> <span class="n">do</span>
            <span class="k">if</span> \<span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">depth</span> \<span class="p">(</span>\<span class="n">geq</span> <span class="mi">2</span>\<span class="p">)</span> <span class="n">then</span>
                <span class="k">break</span>
            \<span class="p">(</span><span class="n">c_</span><span class="p">{</span><span class="n">e</span><span class="p">}</span> \<span class="n">leftarrow</span>\<span class="p">)</span> <span class="n">calculate</span> <span class="n">the</span> <span class="n">existing</span> <span class="n">confidence</span> <span class="n">of</span> \<span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="p">)</span>
            <span class="k">if</span> \<span class="p">(</span><span class="n">c_</span><span class="p">{</span><span class="n">e</span><span class="p">}</span> \<span class="n">geq</span> \<span class="n">tau_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span>\<span class="p">)</span> <span class="n">then</span>
                \<span class="p">(</span><span class="n">L_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">append</span> \<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
            \<span class="p">(</span><span class="n">s</span> \<span class="n">leftarrow</span> <span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">children</span><span class="o">.</span><span class="n">size</span>
            <span class="k">for</span> \<span class="p">(</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> \<span class="n">ldots</span><span class="p">,</span> <span class="n">s</span>\<span class="p">)</span> <span class="n">do</span>
                \<span class="p">(</span><span class="n">Q</span>\<span class="p">)</span><span class="o">.</span><span class="n">append</span> \<span class="p">(</span>\<span class="n">left</span><span class="p">(</span><span class="n">n_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span> <span class="o">.</span> \<span class="n">operatorname</span><span class="p">{</span><span class="n">children</span><span class="p">[</span><span class="n">j</span><span class="p">]}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
</code></pre></div>

<h2>In-Context Examples of $V^{\circ}$ Bench</h2>
<p>User: Question: What is the color of the boy's bag? If you want to answer the question, which objects' information do you need?</p>
<p>Assistant: To answer the question, I need know the location of the boy with a bag so that I can determine the color of the bag. So I need the information about the following objects: boy with a bag.</p>
<p>User: Is the yellow car on the left or right side of the white car?</p>
<p>Assistant: To answer the question, I need know the location of the yellow car and the white car so that I can determine the positional relationship between the two of them. So I need the information about the following objects: white car and yellow car.</p>
<p>User: Tell me the number on the black board.
Assistant: To answer the question, I need know the location of the black board so that I can determine the number on it. So I need the information about the following objects: black board</p>
<p>User: Is the girl with pink hair on the left or right side of the man with backpack?</p>
<p>Assistant: To answer the question, I need know the location of the girl with pink hair and the man with backpack so that I can determine the positional relationship between the two of them. So I need the information about the following objects: girl with pink hair and man with backpack.</p>
<p>User: What kind of animal is on the red sign?
Assistant: To answer the question, I need know the location of the red sign so that I can determine the kind of animal on it. So I need the information about the following objects: red sign.</p>
<p>User: From the information on that advertising board, what is the type of this shop?</p>
<p>Assistant: To answer the question, I need know the location of the advertising board so that I can determine the type of the shop. So I need the information about the following objects: advertising board.</p>
<h2>In-Context Examples of HR-Bench</h2>
<p>User: Question: What is the color of the boy's bag? If you want to answer the question, which objects' information do you need?</p>
<p>Assistant: To answer the question, I need know the location of the boy with a bag so that I can determine the color of the bag. So I need the information about the following objects: boy with a bag.</p>
<p>User: Is the yellow car on the left or right side of the white car?</p>
<p>Assistant: To answer the question, I need know the location of the yellow car and the white car so that I can determine the positional relationship between the two of them. So I need the information about the following objects: white car and yellow car.</p>
<p>User: Tell me the number on the black board above the dog.</p>
<p>Assistant: To answer the question, I need know the location of the black board above the dog so that I can determine the number on it. So I need the information about the following objects: black board above the dog.</p>
<p>User: Is the girl with pink hair on the left or right side of the man with backpack?</p>
<p>Assistant: To answer the question, I need know the location of the girl with pink hair and the man with backpack so that I can determine the positional relationship between the two of them. So I need the information about the following objects: girl with pink hair and man with backpack.</p>
<p>User: What kind of animal is on the red sign?
Assistant: To answer the question, I need know the location of the red sign so that I can determine the kind of animal on it. So I need the information about the following objects: red sign.</p>
<p>User: How many cars in the image?
Assistant: To answer the question, I need know the location of all cars so that I can determine the number of cars. So I need the information about the following objects: all cars.</p>            </div>
        </div>

    </div>
</body>
</html>
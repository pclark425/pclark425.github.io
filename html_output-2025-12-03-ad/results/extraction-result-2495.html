<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2495 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2495</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2495</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-252683209</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.01071v3.pdf" target="_blank">New Paradigms for Exploiting Parallel Experiments in Bayesian Optimization</a></p>
                <p><strong>Paper Abstract:</strong> Bayesian optimization (BO) is one of the most effective methods for closed-loop experimental design and black-box optimization. However, a key limitation of BO is that it is an inherently sequential algorithm (one experiment is proposed per round) and thus cannot directly exploit high-throughput (parallel) experiments. Diverse modifications to the BO framework have been proposed in the literature to enable exploitation of parallel experiments but such approaches are limited in the degree of parallelization that they can achieve and can lead to redundant experiments (thus wasting resources and potentially compromising performance). In this work, we present new parallel BO paradigms that exploit the structure of the system to partition the design space. Specifically, we propose an approach that partitions the design space by following the level sets of the performance function and an approach that exploits partially-separable structures of the performance function found. We conduct extensive numerical experiments using a reactor case study to benchmark the effectiveness of these approaches against a variety of state-of-the-art parallel algorithms reported in the literature. Our computational results show that our approaches significantly reduce the required search time and increase the probability of finding a global (rather than local) solution.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2495.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2495.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequential BO using a Gaussian process surrogate and an acquisition function (here µ - κ·σ) to propose one experiment per iteration, balancing predicted performance and uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Standard Bayesian Optimization (S-BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Build a GP surrogate f(x) from collected experiments and at each iteration optimize an acquisition function AF(x; κ)=µ_f(x) - κ·σ_f(x) to select a single next experiment; retrain the GP after each real evaluation and repeat for L iterations. Key components: Gaussian process surrogate, acquisition function balancing mean and variance with hyperparameter κ, iterative closed-loop sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General closed-loop experimental design / black-box optimization (demonstrated on chemical reactor operating temperature optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequential single-point selection by minimizing AF(x; κ). Resources are allocated one experiment per BO iteration; the exploration/exploitation trade-off is controlled by κ.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time (includes experiment time, AF optimization, and GP training); also number of experiments/iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uncertainty quantified by GP predictive variance σ_f(x) used in AF; no explicit mutual-information objective reported (AF uses variance term as proxy).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit via AF: AF(x;κ)=µ - κ·σ; larger κ favors exploration (high σ) while smaller κ favors exploitation (low µ).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>None explicit beyond AF-induced exploration; sequential nature inherently limits batch diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of iterations/experiments (L) and experiment time (wall-clock).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Stop after L iterations or when satisfactory performance reached; no explicit budget-aware allocation beyond iteration limit.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improved objective (lower cost) relative to global minimum; identification of global vs local minima (objective value in USD/yr).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Best-found objective value (USD/yr) and wall-clock total time; example: average best performance = -394,500 USD/yr across runs; total wall-clock time ≈ 12% higher than experiment time (includes computational overhead).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as baseline for parallel BO variants (HS-BO, HP-BO, MC-BO, q-BO, LS-BO, VP-BO).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>S-BO is slower and finds worse average solutions compared to parallel variants; e.g., typically converges in >500 s and often to local minima versus parallel methods which reached better values ~200 s.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Baseline - served as reference; parallel methods achieved up to >2× faster wall-clock convergence relative to S-BO in case study.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes S-BO can get stuck in local minima and lacks natural stopping criterion; trade-offs are handled only via κ; no explicit cost-information tradeoff beyond AF.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Sequential one-at-a-time allocation is sample-efficient but limited in exploiting high-throughput parallel experiments; partitioning and batch strategies can yield faster convergence and higher chance of global optimum.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2495.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2495.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ref-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference-based Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bayesian optimization that incorporates a fixed low-fidelity or physics-based reference model g(x) and models the residual ε(x)=f(x)-g(x) with a GP to accelerate search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reference-Based Bayesian Optimization (Ref-BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Decompose performance f(x)=g(x)+ε(x) where g is a deterministic reference (physics/low-fidelity model) and ε is modeled with a GP. AF is constructed using predictive mean g(x)+µ_ε(x) and σ_ε(x): AF_ε(x;κ)=(g(x)+µ_ε(x)) - κ·σ_ε(x). The algorithm samples the true system at suggested points, updates the residual GP, and repeats.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Closed-loop experimental design for systems where a cheap/approximate model exists (demonstrated for reactor control / optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Prioritizes regions suggested promising by reference g; sampling driven by acquisition function on residual GP, effectively allocating experiments to reduce residual uncertainty and exploit promising g predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time (includes reference model evaluation, residual GP training, AF optimization, and experiment time).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP predictive variance of the residual σ_ε(x) used in AF as proxy for information gain (uncertainty reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>AF uses g(x)+µ_ε(x) - κ·σ_ε(x) so exploration is encouraged via σ_ε(x) term; g(x) biases exploitation toward regions the reference deems promising.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Indirectly via the reference model guiding partitioning or focus; no separate explicit diversity penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed iterations L, batch size K when combined with parallel variants; implicit experiment-time budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Reference biases search to promising regions to reduce number of costly true-system experiments; otherwise same loop as BO.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Objective improvement (e.g., approach to global minimum) and faster convergence to high-value solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported faster convergence and better average outcomes when a good reference is used; used in experiments to guide LS-BO/VP-BO partitions. No universal numeric for general case; reactor case shows improved probability of finding global solution and reduced search time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared implicitly to S-BO (no reference) and to MFBO literature; used within parallel algorithms (LS-BO, VP-BO) to guide partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reference improves convergence in many cases (paper notes faster, more targeted sampling), though VP-BO sometimes performed faster without using reference in this case study (reference encouraged exploration for VP-BO).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Case-dependent; in reactor case Ref-BO/using reference in partitions produced reductions in search time and higher probability of global optimum vs unreferenced BO variants.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses trade-off: a good reference reduces experiments but a poor reference or over-reliance could mislead; Ref-BO keeps sampling true system (unlike some MFBO) to avoid locking into reference errors.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Using a fixed reference to shape search (via residual modeling) can reduce sample complexity and focus expensive experiments in promising regions, while retaining robustness by always sampling the real system.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2495.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2495.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HP-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyperparameter-Sampling Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parallel BO that generates a batch by optimizing the acquisition function with multiple exploration-exploitation hyperparameter values κ_k (one AF minimization per κ_k), producing diverse points across exploration settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hyperparameter Sampling BO (HP-BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Sample K values of the AF exploration hyperparameter κ_k (e.g., from an exponential distribution) and independently optimize AF(x;κ_k) for each κ_k to produce a batch of K experiments to run in parallel. Each AF-optimization is over the full domain X and then points are evaluated in parallel and the GP updated with all results.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Parallel closed-loop experimental design / batch BO for HTE platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experiments by solving K AF optimization problems with different κ_k values to span exploration-exploitation regimes; aims to create batch diversity by parameter variability rather than explicit correlation modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time dominated by solving K separate AF optimizations and GP training; reported wall-clock overhead ≈ 44% higher than experiment time in case study.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>AF uses σ_f(x) (GP variance) term; information gain implicitly promoted by choosing larger κ samples.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Batch diversity arises from sampling κ from distribution (e.g., exponential) so some AFs are exploratory (high κ) and others exploitative (low κ).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via heterogeneous κ values; no explicit covariance-based de-correlation between batch points, which can allow clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size K and iteration count L; computational budget for solving K AF problems.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Parallelizes by solving K AF optimizations concurrently but increases computational cost linearly with K; no explicit budget-aware scheduling beyond choosing K and κ distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best found objective in batch; probability to find global solution across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Found better solutions faster than S-BO in case study but sometimes produced redundant / clustered experiments; wall-clock overhead ≈ 44% above experiment time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against S-BO, HS-BO, MC-BO, q-BO, LS-BO, VP-BO in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Outperformed S-BO in time to good solution but underperformed LS-BO/VP-BO and q-BO in robustness to find global optimum due to potential redundant sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Parallelization reduces wall-clock search time vs S-BO, but computational overhead may be significant due to multiple AF optimizations.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Trade-off between parallelizability (easy to scale K) and redundancy/clustering risk; sampling κ alleviates need to tune a single κ but does not prevent batch point clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>HP-BO is simple and parallelizable but needs mechanisms to prevent redundancy (e.g., spacing constraints or covariance-aware AF) for better exploration of batch space.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2495.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2495.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HS-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HyperSpace Partitioning Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parallel BO that partitions the design space into K equal blocks (with optional overlap φ), runs an independent GP and BO instance per partition, and proposes one experiment per partition in parallel.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hyperspace: Distributed Bayesian hyperparameter optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HyperSpace Partitioning BO (HS-BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Partition X into K (e.g., 2^d) subdomains X_k (possibly overlapping by fraction φ). Build an independent GP f_k and AF per partition using only data from that partition; concurrently minimize AF_f,k over X_k to propose experiments x_k for each partition and evaluate in parallel. Overlap φ allows some cross-partition communication.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>High-dimensional parallel BO and distributed hyperparameter optimization; demonstrated on reactor optimization (low-dimensional example used K=4).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experiments by enforcing spatial partitioning so each parallel worker explores a different subdomain; prevents redundancy by forcing sampling in distinct regions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time; HS-BO reported total wall-clock time ≈ 14% higher than experiment time (low computational overhead because AF optimization per partition is low-dimensional / bounded).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Local GP variances σ_f,k(x) used in AF in each partition as proxy for information; information gain is local to partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Each partition's AF balances µ and σ locally via κ; global exploration achieved by partition diversity and overlap φ.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit: non-overlapping partitions enforce spatial diversity; overlap φ trades information sharing and parallelism.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of parallel experiments K and iterations L; trade-off with partition count K and overlap φ.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Parallelizes by assigning each partition one experiment per cycle; tuning K and φ trades off speed (more partitions) vs convergence (need communication/overlap).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Ability to find global minimum (objective value) and robustness across initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>HS-BO reached global minima reliably but required more experiments than LS-BO/VP-BO; wall-clock overhead ≈ 14% above experiment time in study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against S-BO, HP-BO, MC-BO, q-BO, LS-BO, VP-BO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Performed better than S-BO in time and solution quality; similar performance to q-BO in finding global minima but needed more experiments than LS-BO/VP-BO.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reduced redundant sampling and improved global-search probability vs S-BO; computationally cheap per partition.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights the K-φ trade-off: smaller φ and larger K increase parallelism but can reduce convergence/communication; partitions of equal box shape may not capture complex function geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Informed/adaptive partitioning could improve performance; HS-BO effective when partitions align with function heterogeneity but requires tuning of K and φ.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2495.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2495.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>N×MCMC / Fantasy-Sampling Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Parallel BO that uses 'fantasy' sampling: draw GP samples for previously-chosen batch points, retrain fantasy GPs to compute average acquisition functions and sequentially select batch points to account for predicted observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>N×MCMC Bayesian Optimization (MC-BO / fantasy sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Select first point by standard AF. To propose subsequent batch points, sample S fantasy evaluations from the GP at chosen points, append each fantasy to the data, retrain fantasy GPs and compute corresponding AFs ÂF_f,s; average these ÂF's to form mean AF and minimize to get next point; repeat until K batch points are obtained. Key components: GP sampling, multiple GP retrainings per fantasy, sequential batch construction.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch BO for HTE where dependencies between batch points should be considered.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Generates batch points sequentially using fantasy observations to avoid naive redundancy; allocates experiments by approximating posterior updates had those evaluations been observed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time dominated by repeated GP retraining and AF computation for S fantasies; in case study, total wall-clock time ≈ 384% higher than experiment time (most computationally intensive).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Fantasy-derived mean AF uses mean of fantasy AFs which implicitly incorporates posterior uncertainty reduction; does not directly compute mutual information but accounts for expected effect of batch observations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>AF includes uncertainty term (κ·σ) and fantasy sampling approximates the effect of observations on future uncertainty; exploration induced via variance and fantasy variability.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Indirect: fantasy sampling tends to reduce redundancy by considering posterior changes from prior points, but can still produce clustered points near convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size K and iterations L; heavy computational budget for AF construction (S fantasies).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>The method attempts to internally account for information value of batch points, but at high computational cost; trade-off between S (fantasy samples) and compute budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Objective improvement and ability to avoid local traps through fantasy-aware batch choices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Effective in many settings but in this case study had high computational overhead; produced decent batch choices but sometimes clustered as convergence approached.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared versus S-BO, HP-BO, HS-BO, q-BO, LS-BO, VP-BO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Comparable to some parallel approaches in solution quality but far more computationally expensive (≈4× compute overhead relative to experiment time).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Improved batch coordination but with steep computational cost, making it less practical when AF retraining cost dominates.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper explicitly states MC-BO's major trade-off: better batch accounting for posterior updates vs very high computational burden due to repeated GP retraining; can still cluster near convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>MC-BO is effective at accounting for mutual effects of batch experiments but is often impractical at scale due to compute; lighter-weight or partition-based methods may be preferable when experiment time and compute trade-offs favor cheaper AF computations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2495.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2495.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>q-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Batch / q-LCB Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Batch Bayesian optimization selecting a set of q points by optimizing a multipoint acquisition function (q-LCB) that accounts for covariance between batch points, implemented via Monte Carlo approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>q-Bayesian Optimization (q-BO; q-LCB)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Construct a multipoint acquisition AF_q(x_K;κ)= (1/S)∑_s max(µ_q(x_K) - κ·|A_q(x_K) z_s|) where µ_q and A_q are GP batch mean and Cholesky of covariance Σ(x_K); optimize AF_q jointly over q points to produce a batch that accounts for correlation, using Monte Carlo sampling z_s. Ensures batch diversity by modeling covariance (Cholesky) and places minimum-distance safeguards to avoid singular covariance.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch BO for multi-point experiments / HTE where correlated evaluations matter.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experiments by joint optimization of AF over q points to explicitly account for covariance, avoiding redundant (correlated) evaluations within batch.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time including joint AF optimization and Monte Carlo sampling; in case study wall-clock overhead ≈ 64% above experiment time.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP mean and covariance in batch (via A_q) used to capture joint uncertainty and inform selection; Monte Carlo sampling approximates expectation over possible outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>AF_q jointly balances mean and uncertainty across q points using κ and Monte Carlo sampling of latent variables.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit: covariance factor A_q prevents redundant selections; optimization includes minimum-distance constraint among batch points to prevent singular covariance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size q and iterations L; computational budget scales with q and S (MC samples).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Joint optimization over q points under distance tolerances; computationally heavier as q increases, so practical q may be limited by compute budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Best objective value found by batches, probability to reach global minimum faster.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>q-BO reached global minima reliably, but required more experiments and had higher computational cost than LS-BO/VP-BO; wall-clock overhead ≈ 64% in study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against S-BO, HS-BO, HP-BO, MC-BO, LS-BO, VP-BO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Better than HP-BO/MC-BO in preventing redundancy; similar to HS-BO in solution quality but more computationally expensive; LS-BO/VP-BO were faster overall in study.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Joint batch reasoning reduces redundant sampling vs naive batching but at nontrivial computational expense.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights trade-off: covariance-aware batch selection reduces redundancy but requires Monte Carlo and larger optimization problems, increasing computational cost as q grows.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>q-BO is effective when batch correlation matters and compute budget allows; for large q or when AF optimization becomes too costly, partitioning or variable-reduction approaches may be preferable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2495.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2495.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LS-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Level-Set Partitioning Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Newly proposed parallel BO that partitions the design space along level sets of a reference function g (approximated by a surrogate ĝ) and runs a single global GP/AF constrained to each level-set partition to propose batch experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Level-Set Partitioning BO (LS-BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Construct a surrogate ĝ of a reference model g(x) and discretize ĝ's range into K level-set thresholds α_k to form non-overlapping subdomains X_k = {x | α_k ≤ ĝ(x) ≤ α_{k+1}}. Use a single global GP surrogate of f(x) and an AF defined over the entire X, but optimize AF subject to the constraint x ∈ X_k for each partition to propose K parallel experiments (one per partition). Retrain the global GP after evaluating all suggested experiments. This leverages domain knowledge in g to prioritize promising regions and reduce redundant sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>High-throughput experimental optimization where a (possibly low-fidelity) reference model is available; demonstrated on chemical reactor temperature optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experiments by enforcing level-set partition constraints so each parallel worker explores a distinct g-informed subdomain; prioritizes partitions corresponding to promising reference-level values to focus expensive experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time including reference surrogate evaluation, constrained AF optimization per partition, GP training, and experiment time; in study LS-BO total wall-clock time was ≈ 46% higher than experiment time (due to constrained AF optimization), but overall search time reduced vs S-BO (converged ~200s vs >500s for S-BO).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses GP predictive variance σ_f(x) in AF as proxy for information gain; partitions aim to concentrate information collection where reference indicates promise.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>AF balances µ and σ via κ within each partition; partitioning trades exploration across level-set bands and exploitation within partitions guided by reference model.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit: partition-by-level-set prevents redundant sampling across partitions and forces spatial diversity aligned to reference-level structure.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed batch size equal to number of partitions K and iteration count L; computational budget also considered since constrained AF optimization is more expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Uses K partitions to produce K parallel experiments per cycle; reference-guided partitions concentrate resources in promising regions, reducing required experiments to find global optimum; constrained AF optimization increases compute but reduces total wall-clock search time.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Objective performance (USD/yr) and probability of finding the global minimum; emphasis on reaching global (breakthrough) solution rather than local minima.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In the reactor case: LS-BO consistently reached global minimum, converged significantly faster than S-BO (~200 s vs >500 s), and required fewer experiments; total wall-clock overhead ≈ 46% above experiment time. LS-BO robust across 25 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared experimentally to S-BO, Ref-BO variants, HP-BO, HS-BO, MC-BO, q-BO, VP-BO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LS-BO outperformed S-BO, HP-BO, MC-BO, and q-BO in convergence speed and solution quality; LS-BO and VP-BO were fastest and most robust in case study. LS-BO reached global minima more reliably and faster than HS-BO and q-BO.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Significant reduction in search time (≈2–3× faster wall-clock to high-quality solutions compared to S-BO) and increased probability of finding global optimum; fewer wasted redundant experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses increased AF optimization complexity (constrained optimization using GP surrogate of g) raising computational cost per AF solve (hence higher overhead), versus improved sample efficiency and faster overall convergence; also notes manual input required to craft refined level-set partitions and desires for automated/adaptive partitioning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Guiding parallel partitions with an informative reference model yields better allocation of expensive experiments — concentrating trials in promising level-set bands increases chance of breakthrough discovery and reduces wall-clock time despite increased AF computation per partition.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2495.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2495.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VP-BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variable Partitioning Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Newly proposed BO that partitions the decision variables into (non-overlapping) blocks following partially separable structure (e.g., subsystem variables) and runs BO per variable-block (Gauss-Seidel style) while sharing coupling variables, reducing AF dimensionality and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Variable Partitioning BO (VP-BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Decompose x into K disjoint variable subsets x_k corresponding to subsystems and model each subsystem's contribution f_k(x_k; x_-k) with a GP per partition. At each iteration, perform Gauss-Seidel-like updates: for each k optimize AF_{f_k}(x_k; x_-k,κ) holding x_-k fixed at current values to propose an experiment for that partition; evaluate full system (all f_k), update dataset and retrain per-partition GPs. Partitioning may be informed by a reference model via feature importance (ARD, SPCA, MCR). Each partition's AF optimization has reduced dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Modular physical systems with partially separable structure (chemical process units, modular experimental systems); demonstrated on serial reactor pair.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experiments across variable partitions so each parallel module searches its low-dimensional space; this concentrates experiments on subsystem variables and reduces AF optimization cost, increasing throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time including per-partition AF optimization, multiple per-iteration subsystem experiments, GP training for each partition; reported total wall-clock overhead ≈ 7% above experiment time (lowest computational overhead among methods in study).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Per-partition GP predictive variance σ_{f_k}(x_k) used in AF; information gain is localized to subsystem variables and learned contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Per-partition AF balances mean and variance via κ; Gauss-Seidel updates exploit local improvements while shared coupling variables allow propagation of information across partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit: different partitions optimize different variable subsets, promoting diversity across variable-space axes; partitions themselves reduce redundant sampling by limiting variable overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of partitions (parallel experiments per cycle) K and iterations L; computational budget for per-partition AFs is small due to reduced dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Reduced AF dimensionality speeds AF optimization and reduces computational overhead, enabling more experiments for a fixed wall-clock budget; partitions can be run distributedly.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Objective improvement and convergence to global minimum; VP-BO showed highest exploitative sampling and fastest convergence in study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In reactor case: VP-BO was among the fastest to converge and had the lowest computational overhead (total wall-clock ≈ 7% above experiment time); converged reliably to global minimum on average and exhibited very clustered sampling near optimum (high exploitative efficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to S-BO, HS-BO, HP-BO, MC-BO, q-BO, LS-BO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>VP-BO outperformed most methods in wall-clock efficiency and sample clustering near optimum; slightly better or comparable solution quality to LS-BO but more exploitative (more likely to settle into first solution found without reference).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Lowest computational overhead among studied methods (≈7% above experiment time) and very fast convergence to high-quality solutions; sample-efficient in locating optima within fewer AF iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>VP-BO trades increased exploitative speed and low compute cost for risk of premature convergence to local optima if partitions or coupling values are not handled carefully; using reference can mitigate or encourage exploration depending on setup.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Exploiting partial separability (variable partitioning) and reducing AF dimensionality is an effective resource-allocation principle: run many cheap AF solves in low-dim spaces rather than expensive AF solves in full-dim, yielding faster wall-clock convergence and lower computational burden.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2495.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2495.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MFBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Fidelity Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BO that leverages cheaper, lower-fidelity models (simulators) to guide sampling and progressively increases fidelity to reduce the number of expensive real-system evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-Fidelity Bayesian Optimization (MFBO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Use a hierarchy of fidelity models to propose candidates: low-fidelity models identify promising regions cheaply, then higher-fidelity (or true) evaluations are used selectively to refine and confirm candidates. Can be implemented via co-kriging or hierarchical GP models and strategies to allocate budget across fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Settings where simulators or approximations are available (engineering design, materials, controllers); referenced as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate cheaper evaluations to broad search and reserve expensive, high-fidelity experiments for promising regions; selection policies decide how many and when to query each fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Counts of low- vs high-fidelity evaluations and wall-clock time; aim is to minimize expensive-evaluation count.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Often uses expected improvement or fidelity-aware acquisition functions (not detailed in this paper's experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Fidelity trade-off: exploration with cheap models, exploitation with expensive fidelity; selection rules balance uncertainties across fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Depends on implementation; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Monetary or experiment-cost budget (minimize number of high-fidelity experiments), wall-clock.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Prioritize low-cost fidelity evaluations and restrict high-fidelity experiments to promising subregions; referenced as alternative to Ref-BO.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Same objective improvement metrics (e.g., final objective value) while using fewer high-fidelity experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mentioned in related literature; not directly benchmarked in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned relative to Ref-BO and other BO variants in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantitatively reported in this paper; referenced as existing approach to reduce real-system experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Methodologies in literature show reductions in expensive experiment count; specifics depend on fidelity models and allocation policy.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper contrasts MFBO with Ref-BO: MFBO restricts sampling to lower-fidelity identified regions while Ref-BO always samples real system and leaves reference fixed, affecting robustness and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use of fidelity hierarchies can reduce real-system evaluations but requires careful fidelity modeling and may need additional assumptions and hyperparameters.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hyperspace: Distributed Bayesian hyperparameter optimization <em>(Rating: 2)</em></li>
                <li>Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization <em>(Rating: 2)</em></li>
                <li>Multi-fidelity Bayesian optimisation with continuous approximations <em>(Rating: 2)</em></li>
                <li>Differentiating the multipoint expected improvement for optimal batch design <em>(Rating: 2)</em></li>
                <li>Practical Bayesian optimization of machine learning algorithms <em>(Rating: 2)</em></li>
                <li>Taking the human out of the loop: A review of Bayesian optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2495",
    "paper_id": "paper-252683209",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "S-BO",
            "name_full": "Standard Bayesian Optimization",
            "brief_description": "Sequential BO using a Gaussian process surrogate and an acquisition function (here µ - κ·σ) to propose one experiment per iteration, balancing predicted performance and uncertainty.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Standard Bayesian Optimization (S-BO)",
            "system_description": "Build a GP surrogate f(x) from collected experiments and at each iteration optimize an acquisition function AF(x; κ)=µ_f(x) - κ·σ_f(x) to select a single next experiment; retrain the GP after each real evaluation and repeat for L iterations. Key components: Gaussian process surrogate, acquisition function balancing mean and variance with hyperparameter κ, iterative closed-loop sampling.",
            "application_domain": "General closed-loop experimental design / black-box optimization (demonstrated on chemical reactor operating temperature optimization).",
            "resource_allocation_strategy": "Sequential single-point selection by minimizing AF(x; κ). Resources are allocated one experiment per BO iteration; the exploration/exploitation trade-off is controlled by κ.",
            "computational_cost_metric": "Wall-clock time (includes experiment time, AF optimization, and GP training); also number of experiments/iterations.",
            "information_gain_metric": "Uncertainty quantified by GP predictive variance σ_f(x) used in AF; no explicit mutual-information objective reported (AF uses variance term as proxy).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit via AF: AF(x;κ)=µ - κ·σ; larger κ favors exploration (high σ) while smaller κ favors exploitation (low µ).",
            "diversity_mechanism": "None explicit beyond AF-induced exploration; sequential nature inherently limits batch diversity.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of iterations/experiments (L) and experiment time (wall-clock).",
            "budget_constraint_handling": "Stop after L iterations or when satisfactory performance reached; no explicit budget-aware allocation beyond iteration limit.",
            "breakthrough_discovery_metric": "Improved objective (lower cost) relative to global minimum; identification of global vs local minima (objective value in USD/yr).",
            "performance_metrics": "Best-found objective value (USD/yr) and wall-clock total time; example: average best performance = -394,500 USD/yr across runs; total wall-clock time ≈ 12% higher than experiment time (includes computational overhead).",
            "comparison_baseline": "Used as baseline for parallel BO variants (HS-BO, HP-BO, MC-BO, q-BO, LS-BO, VP-BO).",
            "performance_vs_baseline": "S-BO is slower and finds worse average solutions compared to parallel variants; e.g., typically converges in &gt;500 s and often to local minima versus parallel methods which reached better values ~200 s.",
            "efficiency_gain": "Baseline - served as reference; parallel methods achieved up to &gt;2× faster wall-clock convergence relative to S-BO in case study.",
            "tradeoff_analysis": "Paper notes S-BO can get stuck in local minima and lacks natural stopping criterion; trade-offs are handled only via κ; no explicit cost-information tradeoff beyond AF.",
            "optimal_allocation_findings": "Sequential one-at-a-time allocation is sample-efficient but limited in exploiting high-throughput parallel experiments; partitioning and batch strategies can yield faster convergence and higher chance of global optimum.",
            "uuid": "e2495.0"
        },
        {
            "name_short": "Ref-BO",
            "name_full": "Reference-based Bayesian Optimization",
            "brief_description": "Bayesian optimization that incorporates a fixed low-fidelity or physics-based reference model g(x) and models the residual ε(x)=f(x)-g(x) with a GP to accelerate search.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Reference-Based Bayesian Optimization (Ref-BO)",
            "system_description": "Decompose performance f(x)=g(x)+ε(x) where g is a deterministic reference (physics/low-fidelity model) and ε is modeled with a GP. AF is constructed using predictive mean g(x)+µ_ε(x) and σ_ε(x): AF_ε(x;κ)=(g(x)+µ_ε(x)) - κ·σ_ε(x). The algorithm samples the true system at suggested points, updates the residual GP, and repeats.",
            "application_domain": "Closed-loop experimental design for systems where a cheap/approximate model exists (demonstrated for reactor control / optimization).",
            "resource_allocation_strategy": "Prioritizes regions suggested promising by reference g; sampling driven by acquisition function on residual GP, effectively allocating experiments to reduce residual uncertainty and exploit promising g predictions.",
            "computational_cost_metric": "Wall-clock time (includes reference model evaluation, residual GP training, AF optimization, and experiment time).",
            "information_gain_metric": "GP predictive variance of the residual σ_ε(x) used in AF as proxy for information gain (uncertainty reduction).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "AF uses g(x)+µ_ε(x) - κ·σ_ε(x) so exploration is encouraged via σ_ε(x) term; g(x) biases exploitation toward regions the reference deems promising.",
            "diversity_mechanism": "Indirectly via the reference model guiding partitioning or focus; no separate explicit diversity penalty.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed iterations L, batch size K when combined with parallel variants; implicit experiment-time budget.",
            "budget_constraint_handling": "Reference biases search to promising regions to reduce number of costly true-system experiments; otherwise same loop as BO.",
            "breakthrough_discovery_metric": "Objective improvement (e.g., approach to global minimum) and faster convergence to high-value solutions.",
            "performance_metrics": "Reported faster convergence and better average outcomes when a good reference is used; used in experiments to guide LS-BO/VP-BO partitions. No universal numeric for general case; reactor case shows improved probability of finding global solution and reduced search time.",
            "comparison_baseline": "Compared implicitly to S-BO (no reference) and to MFBO literature; used within parallel algorithms (LS-BO, VP-BO) to guide partitions.",
            "performance_vs_baseline": "Reference improves convergence in many cases (paper notes faster, more targeted sampling), though VP-BO sometimes performed faster without using reference in this case study (reference encouraged exploration for VP-BO).",
            "efficiency_gain": "Case-dependent; in reactor case Ref-BO/using reference in partitions produced reductions in search time and higher probability of global optimum vs unreferenced BO variants.",
            "tradeoff_analysis": "Paper discusses trade-off: a good reference reduces experiments but a poor reference or over-reliance could mislead; Ref-BO keeps sampling true system (unlike some MFBO) to avoid locking into reference errors.",
            "optimal_allocation_findings": "Using a fixed reference to shape search (via residual modeling) can reduce sample complexity and focus expensive experiments in promising regions, while retaining robustness by always sampling the real system.",
            "uuid": "e2495.1"
        },
        {
            "name_short": "HP-BO",
            "name_full": "Hyperparameter-Sampling Bayesian Optimization",
            "brief_description": "Parallel BO that generates a batch by optimizing the acquisition function with multiple exploration-exploitation hyperparameter values κ_k (one AF minimization per κ_k), producing diverse points across exploration settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Hyperparameter Sampling BO (HP-BO)",
            "system_description": "Sample K values of the AF exploration hyperparameter κ_k (e.g., from an exponential distribution) and independently optimize AF(x;κ_k) for each κ_k to produce a batch of K experiments to run in parallel. Each AF-optimization is over the full domain X and then points are evaluated in parallel and the GP updated with all results.",
            "application_domain": "Parallel closed-loop experimental design / batch BO for HTE platforms.",
            "resource_allocation_strategy": "Allocates experiments by solving K AF optimization problems with different κ_k values to span exploration-exploitation regimes; aims to create batch diversity by parameter variability rather than explicit correlation modeling.",
            "computational_cost_metric": "Wall-clock time dominated by solving K separate AF optimizations and GP training; reported wall-clock overhead ≈ 44% higher than experiment time in case study.",
            "information_gain_metric": "AF uses σ_f(x) (GP variance) term; information gain implicitly promoted by choosing larger κ samples.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Batch diversity arises from sampling κ from distribution (e.g., exponential) so some AFs are exploratory (high κ) and others exploitative (low κ).",
            "diversity_mechanism": "Implicit via heterogeneous κ values; no explicit covariance-based de-correlation between batch points, which can allow clustering.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch size K and iteration count L; computational budget for solving K AF problems.",
            "budget_constraint_handling": "Parallelizes by solving K AF optimizations concurrently but increases computational cost linearly with K; no explicit budget-aware scheduling beyond choosing K and κ distribution.",
            "breakthrough_discovery_metric": "Best found objective in batch; probability to find global solution across runs.",
            "performance_metrics": "Found better solutions faster than S-BO in case study but sometimes produced redundant / clustered experiments; wall-clock overhead ≈ 44% above experiment time.",
            "comparison_baseline": "Compared against S-BO, HS-BO, MC-BO, q-BO, LS-BO, VP-BO in experiments.",
            "performance_vs_baseline": "Outperformed S-BO in time to good solution but underperformed LS-BO/VP-BO and q-BO in robustness to find global optimum due to potential redundant sampling.",
            "efficiency_gain": "Parallelization reduces wall-clock search time vs S-BO, but computational overhead may be significant due to multiple AF optimizations.",
            "tradeoff_analysis": "Trade-off between parallelizability (easy to scale K) and redundancy/clustering risk; sampling κ alleviates need to tune a single κ but does not prevent batch point clustering.",
            "optimal_allocation_findings": "HP-BO is simple and parallelizable but needs mechanisms to prevent redundancy (e.g., spacing constraints or covariance-aware AF) for better exploration of batch space.",
            "uuid": "e2495.2"
        },
        {
            "name_short": "HS-BO",
            "name_full": "HyperSpace Partitioning Bayesian Optimization",
            "brief_description": "Parallel BO that partitions the design space into K equal blocks (with optional overlap φ), runs an independent GP and BO instance per partition, and proposes one experiment per partition in parallel.",
            "citation_title": "Hyperspace: Distributed Bayesian hyperparameter optimization",
            "mention_or_use": "use",
            "system_name": "HyperSpace Partitioning BO (HS-BO)",
            "system_description": "Partition X into K (e.g., 2^d) subdomains X_k (possibly overlapping by fraction φ). Build an independent GP f_k and AF per partition using only data from that partition; concurrently minimize AF_f,k over X_k to propose experiments x_k for each partition and evaluate in parallel. Overlap φ allows some cross-partition communication.",
            "application_domain": "High-dimensional parallel BO and distributed hyperparameter optimization; demonstrated on reactor optimization (low-dimensional example used K=4).",
            "resource_allocation_strategy": "Allocates experiments by enforcing spatial partitioning so each parallel worker explores a different subdomain; prevents redundancy by forcing sampling in distinct regions.",
            "computational_cost_metric": "Wall-clock time; HS-BO reported total wall-clock time ≈ 14% higher than experiment time (low computational overhead because AF optimization per partition is low-dimensional / bounded).",
            "information_gain_metric": "Local GP variances σ_f,k(x) used in AF in each partition as proxy for information; information gain is local to partitions.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Each partition's AF balances µ and σ locally via κ; global exploration achieved by partition diversity and overlap φ.",
            "diversity_mechanism": "Explicit: non-overlapping partitions enforce spatial diversity; overlap φ trades information sharing and parallelism.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of parallel experiments K and iterations L; trade-off with partition count K and overlap φ.",
            "budget_constraint_handling": "Parallelizes by assigning each partition one experiment per cycle; tuning K and φ trades off speed (more partitions) vs convergence (need communication/overlap).",
            "breakthrough_discovery_metric": "Ability to find global minimum (objective value) and robustness across initializations.",
            "performance_metrics": "HS-BO reached global minima reliably but required more experiments than LS-BO/VP-BO; wall-clock overhead ≈ 14% above experiment time in study.",
            "comparison_baseline": "Compared against S-BO, HP-BO, MC-BO, q-BO, LS-BO, VP-BO.",
            "performance_vs_baseline": "Performed better than S-BO in time and solution quality; similar performance to q-BO in finding global minima but needed more experiments than LS-BO/VP-BO.",
            "efficiency_gain": "Reduced redundant sampling and improved global-search probability vs S-BO; computationally cheap per partition.",
            "tradeoff_analysis": "Paper highlights the K-φ trade-off: smaller φ and larger K increase parallelism but can reduce convergence/communication; partitions of equal box shape may not capture complex function geometry.",
            "optimal_allocation_findings": "Informed/adaptive partitioning could improve performance; HS-BO effective when partitions align with function heterogeneity but requires tuning of K and φ.",
            "uuid": "e2495.3"
        },
        {
            "name_short": "MC-BO",
            "name_full": "N×MCMC / Fantasy-Sampling Bayesian Optimization",
            "brief_description": "Parallel BO that uses 'fantasy' sampling: draw GP samples for previously-chosen batch points, retrain fantasy GPs to compute average acquisition functions and sequentially select batch points to account for predicted observations.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "N×MCMC Bayesian Optimization (MC-BO / fantasy sampling)",
            "system_description": "Select first point by standard AF. To propose subsequent batch points, sample S fantasy evaluations from the GP at chosen points, append each fantasy to the data, retrain fantasy GPs and compute corresponding AFs ÂF_f,s; average these ÂF's to form mean AF and minimize to get next point; repeat until K batch points are obtained. Key components: GP sampling, multiple GP retrainings per fantasy, sequential batch construction.",
            "application_domain": "Batch BO for HTE where dependencies between batch points should be considered.",
            "resource_allocation_strategy": "Generates batch points sequentially using fantasy observations to avoid naive redundancy; allocates experiments by approximating posterior updates had those evaluations been observed.",
            "computational_cost_metric": "Wall-clock time dominated by repeated GP retraining and AF computation for S fantasies; in case study, total wall-clock time ≈ 384% higher than experiment time (most computationally intensive).",
            "information_gain_metric": "Fantasy-derived mean AF uses mean of fantasy AFs which implicitly incorporates posterior uncertainty reduction; does not directly compute mutual information but accounts for expected effect of batch observations.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "AF includes uncertainty term (κ·σ) and fantasy sampling approximates the effect of observations on future uncertainty; exploration induced via variance and fantasy variability.",
            "diversity_mechanism": "Indirect: fantasy sampling tends to reduce redundancy by considering posterior changes from prior points, but can still produce clustered points near convergence.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch size K and iterations L; heavy computational budget for AF construction (S fantasies).",
            "budget_constraint_handling": "The method attempts to internally account for information value of batch points, but at high computational cost; trade-off between S (fantasy samples) and compute budget.",
            "breakthrough_discovery_metric": "Objective improvement and ability to avoid local traps through fantasy-aware batch choices.",
            "performance_metrics": "Effective in many settings but in this case study had high computational overhead; produced decent batch choices but sometimes clustered as convergence approached.",
            "comparison_baseline": "Compared versus S-BO, HP-BO, HS-BO, q-BO, LS-BO, VP-BO.",
            "performance_vs_baseline": "Comparable to some parallel approaches in solution quality but far more computationally expensive (≈4× compute overhead relative to experiment time).",
            "efficiency_gain": "Improved batch coordination but with steep computational cost, making it less practical when AF retraining cost dominates.",
            "tradeoff_analysis": "Paper explicitly states MC-BO's major trade-off: better batch accounting for posterior updates vs very high computational burden due to repeated GP retraining; can still cluster near convergence.",
            "optimal_allocation_findings": "MC-BO is effective at accounting for mutual effects of batch experiments but is often impractical at scale due to compute; lighter-weight or partition-based methods may be preferable when experiment time and compute trade-offs favor cheaper AF computations.",
            "uuid": "e2495.4"
        },
        {
            "name_short": "q-BO",
            "name_full": "Batch / q-LCB Bayesian Optimization",
            "brief_description": "Batch Bayesian optimization selecting a set of q points by optimizing a multipoint acquisition function (q-LCB) that accounts for covariance between batch points, implemented via Monte Carlo approximations.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "q-Bayesian Optimization (q-BO; q-LCB)",
            "system_description": "Construct a multipoint acquisition AF_q(x_K;κ)= (1/S)∑_s max(µ_q(x_K) - κ·|A_q(x_K) z_s|) where µ_q and A_q are GP batch mean and Cholesky of covariance Σ(x_K); optimize AF_q jointly over q points to produce a batch that accounts for correlation, using Monte Carlo sampling z_s. Ensures batch diversity by modeling covariance (Cholesky) and places minimum-distance safeguards to avoid singular covariance.",
            "application_domain": "Batch BO for multi-point experiments / HTE where correlated evaluations matter.",
            "resource_allocation_strategy": "Allocates experiments by joint optimization of AF over q points to explicitly account for covariance, avoiding redundant (correlated) evaluations within batch.",
            "computational_cost_metric": "Wall-clock time including joint AF optimization and Monte Carlo sampling; in case study wall-clock overhead ≈ 64% above experiment time.",
            "information_gain_metric": "GP mean and covariance in batch (via A_q) used to capture joint uncertainty and inform selection; Monte Carlo sampling approximates expectation over possible outcomes.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "AF_q jointly balances mean and uncertainty across q points using κ and Monte Carlo sampling of latent variables.",
            "diversity_mechanism": "Explicit: covariance factor A_q prevents redundant selections; optimization includes minimum-distance constraint among batch points to prevent singular covariance.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch size q and iterations L; computational budget scales with q and S (MC samples).",
            "budget_constraint_handling": "Joint optimization over q points under distance tolerances; computationally heavier as q increases, so practical q may be limited by compute budget.",
            "breakthrough_discovery_metric": "Best objective value found by batches, probability to reach global minimum faster.",
            "performance_metrics": "q-BO reached global minima reliably, but required more experiments and had higher computational cost than LS-BO/VP-BO; wall-clock overhead ≈ 64% in study.",
            "comparison_baseline": "Compared against S-BO, HS-BO, HP-BO, MC-BO, LS-BO, VP-BO.",
            "performance_vs_baseline": "Better than HP-BO/MC-BO in preventing redundancy; similar to HS-BO in solution quality but more computationally expensive; LS-BO/VP-BO were faster overall in study.",
            "efficiency_gain": "Joint batch reasoning reduces redundant sampling vs naive batching but at nontrivial computational expense.",
            "tradeoff_analysis": "Paper highlights trade-off: covariance-aware batch selection reduces redundancy but requires Monte Carlo and larger optimization problems, increasing computational cost as q grows.",
            "optimal_allocation_findings": "q-BO is effective when batch correlation matters and compute budget allows; for large q or when AF optimization becomes too costly, partitioning or variable-reduction approaches may be preferable.",
            "uuid": "e2495.5"
        },
        {
            "name_short": "LS-BO",
            "name_full": "Level-Set Partitioning Bayesian Optimization",
            "brief_description": "Newly proposed parallel BO that partitions the design space along level sets of a reference function g (approximated by a surrogate ĝ) and runs a single global GP/AF constrained to each level-set partition to propose batch experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Level-Set Partitioning BO (LS-BO)",
            "system_description": "Construct a surrogate ĝ of a reference model g(x) and discretize ĝ's range into K level-set thresholds α_k to form non-overlapping subdomains X_k = {x | α_k ≤ ĝ(x) ≤ α_{k+1}}. Use a single global GP surrogate of f(x) and an AF defined over the entire X, but optimize AF subject to the constraint x ∈ X_k for each partition to propose K parallel experiments (one per partition). Retrain the global GP after evaluating all suggested experiments. This leverages domain knowledge in g to prioritize promising regions and reduce redundant sampling.",
            "application_domain": "High-throughput experimental optimization where a (possibly low-fidelity) reference model is available; demonstrated on chemical reactor temperature optimization.",
            "resource_allocation_strategy": "Allocates experiments by enforcing level-set partition constraints so each parallel worker explores a distinct g-informed subdomain; prioritizes partitions corresponding to promising reference-level values to focus expensive experiments.",
            "computational_cost_metric": "Wall-clock time including reference surrogate evaluation, constrained AF optimization per partition, GP training, and experiment time; in study LS-BO total wall-clock time was ≈ 46% higher than experiment time (due to constrained AF optimization), but overall search time reduced vs S-BO (converged ~200s vs &gt;500s for S-BO).",
            "information_gain_metric": "Uses GP predictive variance σ_f(x) in AF as proxy for information gain; partitions aim to concentrate information collection where reference indicates promise.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "AF balances µ and σ via κ within each partition; partitioning trades exploration across level-set bands and exploitation within partitions guided by reference model.",
            "diversity_mechanism": "Explicit: partition-by-level-set prevents redundant sampling across partitions and forces spatial diversity aligned to reference-level structure.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed batch size equal to number of partitions K and iteration count L; computational budget also considered since constrained AF optimization is more expensive.",
            "budget_constraint_handling": "Uses K partitions to produce K parallel experiments per cycle; reference-guided partitions concentrate resources in promising regions, reducing required experiments to find global optimum; constrained AF optimization increases compute but reduces total wall-clock search time.",
            "breakthrough_discovery_metric": "Objective performance (USD/yr) and probability of finding the global minimum; emphasis on reaching global (breakthrough) solution rather than local minima.",
            "performance_metrics": "In the reactor case: LS-BO consistently reached global minimum, converged significantly faster than S-BO (~200 s vs &gt;500 s), and required fewer experiments; total wall-clock overhead ≈ 46% above experiment time. LS-BO robust across 25 runs.",
            "comparison_baseline": "Compared experimentally to S-BO, Ref-BO variants, HP-BO, HS-BO, MC-BO, q-BO, VP-BO.",
            "performance_vs_baseline": "LS-BO outperformed S-BO, HP-BO, MC-BO, and q-BO in convergence speed and solution quality; LS-BO and VP-BO were fastest and most robust in case study. LS-BO reached global minima more reliably and faster than HS-BO and q-BO.",
            "efficiency_gain": "Significant reduction in search time (≈2–3× faster wall-clock to high-quality solutions compared to S-BO) and increased probability of finding global optimum; fewer wasted redundant experiments.",
            "tradeoff_analysis": "Paper discusses increased AF optimization complexity (constrained optimization using GP surrogate of g) raising computational cost per AF solve (hence higher overhead), versus improved sample efficiency and faster overall convergence; also notes manual input required to craft refined level-set partitions and desires for automated/adaptive partitioning.",
            "optimal_allocation_findings": "Guiding parallel partitions with an informative reference model yields better allocation of expensive experiments — concentrating trials in promising level-set bands increases chance of breakthrough discovery and reduces wall-clock time despite increased AF computation per partition.",
            "uuid": "e2495.6"
        },
        {
            "name_short": "VP-BO",
            "name_full": "Variable Partitioning Bayesian Optimization",
            "brief_description": "Newly proposed BO that partitions the decision variables into (non-overlapping) blocks following partially separable structure (e.g., subsystem variables) and runs BO per variable-block (Gauss-Seidel style) while sharing coupling variables, reducing AF dimensionality and computational cost.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Variable Partitioning BO (VP-BO)",
            "system_description": "Decompose x into K disjoint variable subsets x_k corresponding to subsystems and model each subsystem's contribution f_k(x_k; x_-k) with a GP per partition. At each iteration, perform Gauss-Seidel-like updates: for each k optimize AF_{f_k}(x_k; x_-k,κ) holding x_-k fixed at current values to propose an experiment for that partition; evaluate full system (all f_k), update dataset and retrain per-partition GPs. Partitioning may be informed by a reference model via feature importance (ARD, SPCA, MCR). Each partition's AF optimization has reduced dimensionality.",
            "application_domain": "Modular physical systems with partially separable structure (chemical process units, modular experimental systems); demonstrated on serial reactor pair.",
            "resource_allocation_strategy": "Allocates experiments across variable partitions so each parallel module searches its low-dimensional space; this concentrates experiments on subsystem variables and reduces AF optimization cost, increasing throughput.",
            "computational_cost_metric": "Wall-clock time including per-partition AF optimization, multiple per-iteration subsystem experiments, GP training for each partition; reported total wall-clock overhead ≈ 7% above experiment time (lowest computational overhead among methods in study).",
            "information_gain_metric": "Per-partition GP predictive variance σ_{f_k}(x_k) used in AF; information gain is localized to subsystem variables and learned contributions.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Per-partition AF balances mean and variance via κ; Gauss-Seidel updates exploit local improvements while shared coupling variables allow propagation of information across partitions.",
            "diversity_mechanism": "Implicit: different partitions optimize different variable subsets, promoting diversity across variable-space axes; partitions themselves reduce redundant sampling by limiting variable overlap.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of partitions (parallel experiments per cycle) K and iterations L; computational budget for per-partition AFs is small due to reduced dimension.",
            "budget_constraint_handling": "Reduced AF dimensionality speeds AF optimization and reduces computational overhead, enabling more experiments for a fixed wall-clock budget; partitions can be run distributedly.",
            "breakthrough_discovery_metric": "Objective improvement and convergence to global minimum; VP-BO showed highest exploitative sampling and fastest convergence in study.",
            "performance_metrics": "In reactor case: VP-BO was among the fastest to converge and had the lowest computational overhead (total wall-clock ≈ 7% above experiment time); converged reliably to global minimum on average and exhibited very clustered sampling near optimum (high exploitative efficiency).",
            "comparison_baseline": "Compared to S-BO, HS-BO, HP-BO, MC-BO, q-BO, LS-BO.",
            "performance_vs_baseline": "VP-BO outperformed most methods in wall-clock efficiency and sample clustering near optimum; slightly better or comparable solution quality to LS-BO but more exploitative (more likely to settle into first solution found without reference).",
            "efficiency_gain": "Lowest computational overhead among studied methods (≈7% above experiment time) and very fast convergence to high-quality solutions; sample-efficient in locating optima within fewer AF iterations.",
            "tradeoff_analysis": "VP-BO trades increased exploitative speed and low compute cost for risk of premature convergence to local optima if partitions or coupling values are not handled carefully; using reference can mitigate or encourage exploration depending on setup.",
            "optimal_allocation_findings": "Exploiting partial separability (variable partitioning) and reducing AF dimensionality is an effective resource-allocation principle: run many cheap AF solves in low-dim spaces rather than expensive AF solves in full-dim, yielding faster wall-clock convergence and lower computational burden.",
            "uuid": "e2495.7"
        },
        {
            "name_short": "MFBO",
            "name_full": "Multi-Fidelity Bayesian Optimization",
            "brief_description": "BO that leverages cheaper, lower-fidelity models (simulators) to guide sampling and progressively increases fidelity to reduce the number of expensive real-system evaluations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Multi-Fidelity Bayesian Optimization (MFBO)",
            "system_description": "Use a hierarchy of fidelity models to propose candidates: low-fidelity models identify promising regions cheaply, then higher-fidelity (or true) evaluations are used selectively to refine and confirm candidates. Can be implemented via co-kriging or hierarchical GP models and strategies to allocate budget across fidelities.",
            "application_domain": "Settings where simulators or approximations are available (engineering design, materials, controllers); referenced as related work.",
            "resource_allocation_strategy": "Allocate cheaper evaluations to broad search and reserve expensive, high-fidelity experiments for promising regions; selection policies decide how many and when to query each fidelity.",
            "computational_cost_metric": "Counts of low- vs high-fidelity evaluations and wall-clock time; aim is to minimize expensive-evaluation count.",
            "information_gain_metric": "Often uses expected improvement or fidelity-aware acquisition functions (not detailed in this paper's experiments).",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Fidelity trade-off: exploration with cheap models, exploitation with expensive fidelity; selection rules balance uncertainties across fidelities.",
            "diversity_mechanism": "Depends on implementation; not detailed in this paper.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Monetary or experiment-cost budget (minimize number of high-fidelity experiments), wall-clock.",
            "budget_constraint_handling": "Prioritize low-cost fidelity evaluations and restrict high-fidelity experiments to promising subregions; referenced as alternative to Ref-BO.",
            "breakthrough_discovery_metric": "Same objective improvement metrics (e.g., final objective value) while using fewer high-fidelity experiments.",
            "performance_metrics": "Mentioned in related literature; not directly benchmarked in this paper's experiments.",
            "comparison_baseline": "Mentioned relative to Ref-BO and other BO variants in literature.",
            "performance_vs_baseline": "Not quantitatively reported in this paper; referenced as existing approach to reduce real-system experiments.",
            "efficiency_gain": "Methodologies in literature show reductions in expensive experiment count; specifics depend on fidelity models and allocation policy.",
            "tradeoff_analysis": "Paper contrasts MFBO with Ref-BO: MFBO restricts sampling to lower-fidelity identified regions while Ref-BO always samples real system and leaves reference fixed, affecting robustness and complexity.",
            "optimal_allocation_findings": "Use of fidelity hierarchies can reduce real-system evaluations but requires careful fidelity modeling and may need additional assumptions and hyperparameters.",
            "uuid": "e2495.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hyperspace: Distributed Bayesian hyperparameter optimization",
            "rating": 2,
            "sanitized_title": "hyperspace_distributed_bayesian_hyperparameter_optimization"
        },
        {
            "paper_title": "Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization",
            "rating": 2,
            "sanitized_title": "parallelizing_explorationexploitation_tradeoffs_in_gaussian_process_bandit_optimization"
        },
        {
            "paper_title": "Multi-fidelity Bayesian optimisation with continuous approximations",
            "rating": 2,
            "sanitized_title": "multifidelity_bayesian_optimisation_with_continuous_approximations"
        },
        {
            "paper_title": "Differentiating the multipoint expected improvement for optimal batch design",
            "rating": 2,
            "sanitized_title": "differentiating_the_multipoint_expected_improvement_for_optimal_batch_design"
        },
        {
            "paper_title": "Practical Bayesian optimization of machine learning algorithms",
            "rating": 2,
            "sanitized_title": "practical_bayesian_optimization_of_machine_learning_algorithms"
        },
        {
            "paper_title": "Taking the human out of the loop: A review of Bayesian optimization",
            "rating": 1,
            "sanitized_title": "taking_the_human_out_of_the_loop_a_review_of_bayesian_optimization"
        }
    ],
    "cost": 0.02477325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>New Paradigms for Exploiting Parallel Experiments in Bayesian Optimization</p>
<p>Leonardo D González 
Department of Chemical and Biological Engineering
University of Wisconsin-Madison
1415 Engineering Dr53706MadisonWIUSA</p>
<p>Victor M Zavala 
Department of Chemical and Biological Engineering
University of Wisconsin-Madison
1415 Engineering Dr53706MadisonWIUSA</p>
<p>New Paradigms for Exploiting Parallel Experiments in Bayesian Optimization
Bayesian optimizationhigh-throughput experimentsparallelization
Bayesian optimization (BO) is one of the most effective methods for closed-loop experimental design and black-box optimization. However, a key limitation of BO is that it is an inherently sequential algorithm (one experiment is proposed per round) and thus cannot directly exploit high-throughput (parallel) experiments. Diverse modifications to the BO framework have been proposed in the literature to enable exploitation of parallel experiments but such approaches are limited in the degree of parallelization that they can achieve and can lead to redundant experiments (thus wasting resources and potentially compromising performance). In this work, we present new parallel BO paradigms that exploit the structure of the system to partition the design space. Specifically, we propose an approach that partitions the design space by following the level sets of the performance function and an approach that exploits partially separable structures of the performance function found. We conduct extensive numerical experiments using a reactor case study to benchmark the effectiveness of these approaches against a variety of state-of-the-art parallel algorithms reported in the literature. Our computational results show that our approaches significantly reduce the required search time and increase the probability of finding a global (rather than local) solution.result, HTE platforms rely on the use of design of experiments (DoE) algorithms, which aim to systematically explore the design space.Screening is a simple DoE approach in which experiments are performed at points on a discretized grid of the design space [23]; this approach is intuitive but does not scale well with the number of design variables and can ultimately lead to significant waste of resources (conduct experiments that do not provide significant information). The central aim of advanced DoE approaches is to maximize the value provided by each experiment and ultimately reduce the number of experiments and resources used (e.g., experiment time). The value of an experiment is usually measured either by information content (e.g., reduces model uncertainty) or if it results in a desirable outcome (e.g., improves an economic objective)[2]. A widely used DoE approach that aims to tackle this problem is response surface methodology or RSM [3]; this approach is generally sample-efficient (requires few experiments) but uses second-degree polynomial surrogate models that can fail to accurately capture system trends. In addition, parameters used in the RSM surrogate model are subject to uncertainty and this uncertainty is not resolved via further experiments [12] (i.e., RSM is an open-loop DoE technique).Another powerful approach to DoE that aims to maximize value of experiments is Bayesian experimental design[5]. Recently, the machine learning (ML) community has been using variants of this paradigm to conduct closed-loop experimental design[7]. One of the most effective variations of this paradigm is the Bayesian optimization (BO) algorithm [1]; BO has been shown to be sample-efficient and scalable (requires minimal experiments and can explore large design spaces)[28]. BO is widely used in applications such as experimental design, hyper-parameter tuning, and reinforcement learning. Of particular interest is the flexibility of the BO paradigm as it is capable of accommodating both continuous and discrete (e.g., categorical) design variables as well as constraints (which help encode domain knowledge and restrict the design space)[4]. Additionally, BO uses probabilistic surrogate models (e.g. Gaussian process models) which greatly facilitates the quantification of uncertainty and information in different regions of the design space [9]; this feature is particularly useful in guiding experiments where information gain can be as important as performance. BO can also be tuned to emphasize exploration (by sampling regions with high uncertainty) over exploitation (by sampling from regions with high economic performance)[17]; this trade-off is achieved by tuning the so-called acquisition function (AF), which is a composite function that captures uncertainty and performance.A fundamental caveat of BO is that it is inherently a sequential algorithm (samples a single point in the design space at each iteration), limiting its ability to exploit HTE platforms. Modifications to the BO algorithm have been proposed in the literature to overcome these limitations[10,6,15]. Relevant variants include Hyperspace partitioning[33], batch Bayesian optimization[31], NxMCMC [26], and AF optimization over a set of exploratory designs[11]. These parallel BO approaches have been shown to perform better than sequential BO in terms of search time</p>
<p>Introduction</p>
<p>The use of high-throughput experimental (HTE) platforms is accelerating scientific discovery in diverse fields such as catalysis [18], pharmaceuticals [16], synthetic biology [25], and chemical engineering [21]. Such platforms permit large numbers of experiments to be executed in parallel, sometimes automatically; this enables the exploration of wider design spaces, reduces time to discovery, and can potentially decrease the use of resources. However, due to the large number of design variables involved, determining optimal conditions manually is often infeasible. As a [34]; however, these approaches are limited in the degree of parallelization that can be achieved and can lead to redundant experiments, thus wasting resources and potentially getting trapped in local solutions.</p>
<p>In this work, we propose a set of new parallel BO paradigms that exploit the structure of the system in order to guide partitioning of the design space (see Figure 1). Our first approach, which we call level-set partitioning, decomposes the design space by following the level sets of the performance function. Because the performance function cannot be evaluated (it is unknown), a key feature of this approach is that it leverages a reference function (which can be a low-fidelity model or a physics model) to approximate the level sets and guide the partitioning. Our second approach, called variable partitioning, partitions the design space by exploiting partially separable structures that typically result when a system is composed of multiple subsystems (e.g., a chemical process is composed of multiple units). We benchmark the performance of our approaches over sequential BO and state-of-the-art parallel BO variants from the literature using a reactor system. Our results show that the proposed approaches can achieve significant reductions in search time; in addition, we observe improvements in performance values found and in search robustness (sensitivity to initial guess). </p>
<p>Sequential Bayesian Optimization</p>
<p>Standard BO (S-BO)</p>
<p>The aim of closed-loop DoE is to identify experimental inputs that achieve optimal performance; we cast this as the optimization problem:
min x f (x) (1a) s.t. x ∈ X (1b)
where f : X → R is a scalar performance function, x ∈ X ⊆ R d is a given experiment (trial) point; and X is the experimental design space (of dimension d). Generally, an explicit relationship between the design variables x and performance function f is not known a priori and motivates the need to evaluate the performance at a selected set of trial points to build a surrogate model of the performance function that can be used to optimize the system.</p>
<p>The open-loop DoE approach most commonly used in HTE platforms is screening; this is a grid-search method that discretizes the design space X into a set of experiments x k ∈ X, k ∈ K := {1, ..., K} (we denote this set compactly as x K ); the performance of the system is then evaluated (potentially in parallel) at these points to obtain f K and the experiments that achieve the best performance are selected. This screening approach provides good exploratory capabilities, but it is not scalable in the sense that the number of trial experiments needed to cover the design space grows exponentially with the number of design variables d and with the width of the space X. Moreover, this approach cannot be guaranteed to find an optimal solution.</p>
<p>To derive our closed-loop DoE approach based on S-BO, we assume that we have a set of experimental data D = {x K , f K } at the initial iteration = 1. We use this data to develop a probabilistic surrogate modelf : X → R (a Gaussian process -GP) that approximates the performance function f over the design space X. The GP generates this approximation by first constructing a prior over f (x 1:n ) of the form f (x 1:n ) ∼ N (m(x), K(x, x )). Here m(x) ∈ R d is the prior mean function and is commonly set equal to 0. The prior covariance matrix, K(x, x ) ∈ R d×d , is constructed using a kernel function, h(x, x ), such that K i,j = h(x i , x j ). There exists a large selection of kernel functions (e.g, rotational quadratic, squared exponential, Mátern), and determining which to use is largely dependent on identifying a choice for h(x, x ) that will fit the generated data well. The GP then uses these elements to construct a predictive posterior distribution at a new experimental point x that generates an estimatef (x) of the value of the performance function at x. This estimate can be shown to be Gaussian random variablef (x) ∼ N (µ f (x), σ f (x)) with mean and variance:
µ f (x) := K(x, x K )K(x K , x K ) −1 f K , x ∈ X (2a) σ f (x) := K(x, x) − K(x, x K ) T K(x K , x K ) −1 K(x K , x), x ∈ X.(2b)
The mean and variance of the prediction capture the expected performance and its uncertainty/information; these measures are used to construct an acquisition function (AF), which is used to guide the selection of the next experiment. Here, we use an AF of the form:
AF f (x; κ) = µ f (x) − κ · σ f (x), x ∈ X(3)
where κ ∈ R + is a hyperparameter that prioritizes exploration (information) over exploitation (performance). The next experiment, x +1 ∈ X, is obtained by solving the AF optimization problem:</p>
<p>x +1 ← argmin
x AF f (x; κ) (4a) s.t. x ∈ X (4b)
Once the experiment x +1 has been computed, the system performance is evaluated at this point (by running the experiment) to obtain the new data point {x +1 , f (x +1 )}; this point is added to the data collection D +1 ← D ∪ {x +1 , f (x +1 )}. The GP model is re-trained using this new data collection to obtainf +1 (x) ∼ N (µ +1 f (x), σ +1 f (x)) and the acquisition function is updated to AF +1 f and minimized to obtain a new design x +2 . This process is repeated over multiple cycles/iterations = 1, 2, ..., L until a satisfactory performance is obtained (or no additional improvement in performance is observed). We highlight that the standard BO algorithm (which we refer to as S-BO) does not have a natural stopping criterion and can get stuck in a local solution (as opposed to finding a global solution). The pseudocode for S-BO can be found in Algorithm 1.</p>
<p>Algorithm 1: Standard Bayesian Optimization (S-BO)</p>
<p>Given κ, L, and D ; Train GPf using initial dataset D and obtain AF f ; for = 1, 2, ..., L do</p>
<p>Compute experiment x +1 ← argmin x AF f (x; κ) s.t. x ∈ X; Evaluate performance at x +1 to obtain f +1 ; Update dataset D +1 ← D ∪ x +1 , f +1 ; Train GP using D +1 to obtainf +1 and AF +1 f ; end</p>
<p>Reference-Based BO (Ref-BO)</p>
<p>Recent work has shown that allowing the BO algorithm to exploit available preexisting information can improve its performance. One approach found in [19], for example, exploits the fact that the performance function is usually a known composite function that can be expressed as f (y(x)) where a set of intermediate variables, y(x), are the unknowns. As a result, the performance function can be optimized using derivative-based methods, allowing one to set targets for the various </p>
<p>Model Training</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M c 2 / G s / 5 Q t o F P 6 W Z c C G s / 8 Y + G d M = " &gt; A A A B / n i c b V B N S w M x E M 3 6 W e v X q n j y E i y C p 7 L b g 3 q s C O L N C v Y D 2 l K y a b Y N T b J L M i v U p e B f 8 e J B E a / + D m / + G 9 P t H r T 1 w c D j v R l m 5 g W x 4 A Y 8 7 9 t Z W l 5 Z X V s v b B Q 3 t 7 Z 3 d t 2 9 / Y a J E k 1 Z n U Y i 0 q 2 A G C a 4 Y n X g I F g r 1 o z I Q L B m M L q a + s 0 H p g 2 P 1 D 2 M Y 9 a V Z K B 4 y C k B K / X c w 7 Q T h P j y G t / G w C V / z O R J z y 1 5 Z S 8 D X i R + T k o o R 6 3 n f n X 6 E U 0 k U 0 A F M a b t e z F 0 U 6 K B U 8 E m x U 5 i W E z o i A x Y 2 1 J F J D P d N D t / g k + s 0 s d h p G 0 p w J n 6 e y I l 0 p i x D G y n J D A 0 8 9 5 U / M 9 r J x B e d F O u 4 g S Y o r N F Y S I w R H i a B e 5 z z S i I s S W E a m 5 v x X R I N K F g E y v a E P z 5 l x d J o 1 L 2 z 8 r + X a V U r e R x F N A R O k a n y E f n q I p u U A 3 V E U U p e k a v 6 M 1 5 c l 6 c d + d j 1 r r k 5 D M H 6 A + c z x / R n p V b &lt; / l a t e x i t &gt;</p>
<p>AF Optimization</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 R P B 9 4 s 
x d A V f k N P f Y X O V V w W E O e g = " &gt; A A A C A H i c b V B N S 8 N A E N 3 UI i Z I Y Q q b m 7 F d E w U o W A y q 5 g Q 3 O W X V 0 m 7 X n M v a + 5 9 v d q o F 3 G U 0 S k 6 Q x f I R V e
o g e 5 Q E 7 U Q R R l 6 R q / o z X q y X q x 3 6 2 P R W r K K m W P 0 B 9 b n D 8 y C l n 4 = &lt; / l a t e x i t &gt; System Evaluation &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A P 9 4 B 6 / N Q P r n 4 N s 3
H E r O X H N c Z + A = " &gt; A A A B + n i c b V D L S g N B E O y N r x h f G z 1 6 G Q y C I I T d I O o x 4 M V j B P O A J I b Z S W 8 y Z P b B z K w a 1 n y K F w + K e P V L v P k 3 T p I 9 a G J B Q 1 H V T X e X F w u u t O N 8 W 7 m V 1 b X 1 j f x m Y W t 7 Z 3 f P L u 4 3 V J R I h n U W i U i 2 P K p Q 8 B D r m m u B r V g i D T y B T W 9 0 N f W b 9 y g V j 8 J b P Y 6 x G 9 B B y H 3 O q D Z S z y 6 S O R 7 v 0 g 4 K c e p O e n b J K T s z k G X i Z q Q E G W o 9 + 6 v T j 1 g S Y K i Z o E q 1 X S f W 3 Z R K z Z n A S a G T K I w p G 9 E B t g 0 N a Y C q m 8 5 O n 5 B j o / S J H 0 l T o S Y z 9 f d E S g O l x o F n O g O q h 2 r R m 4 r / e e 1 E + 5 f d l I d x o j F k 8 0 V + I o i O y D Q H 0 u c S m R Z j Q y i T 3 N x K 2 J B K y r R J q 2 B C c B d f X i a N S t k 9 L 7 s 3 Z 6 V q J Y s j D 4 d w B C f g w g V U 4 R p q U A c G D / A M r / B m P V k v 1 r v 1 M W / N W d n M A f y B 9
f k D G c e R 7 g = = &lt; / l a t e x i t &gt;</p>
<p>x`+ 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c u P u R / / y G W K q L g r 5 l X t a O M S 4 y R 4 = "   x +1 is obtained from the optimization of the acquisition function. The system performance is evaluated at the experiment and the collected data is added to the database. The database is used to re-train the probabilistic GP model (which captures performance and uncertainty). The GP model is used to construct the acquisition function, which balances performance and uncertainty.</p>
<blockquote>
<p>A A A C M 3 i c b V D L S g M x F M 3 4 r P V V d e k m W I Q W p c w U U Z c F X Y i r C v Y B n Vk I H p A c t T X 3 i g W z H o 8 w J P t R K F 7 u B 0 M d X e K R O b 8 T E k 3 L o O X o y z S F n v V T 8 z 2 t F y j 1 v x 8 w P I w U + H T / k R h y r A K c F 4 i 4 T Q B U f a k K o Y P q v m P a J I F T p m r O 6 B G s 2 8 j y p l 0 v W a c m 6 O c l X y p M 6 M m g f H a A C s t A Z q q A r V E U 1 R N E j e k H v 6 M N 4 M t 6 M T + N r P L p g T H b 2 0 B 8 Y 3 z 9 O p q s w &lt; / l a t e x i t &gt; D<code>+ 1 D</code>[ (x<code>+ 1 , f(x</code>+ 1 )) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F Y Q P M L N o h K g G G E 7 x h Q 6 v w 2 V l R J I = " &gt; A A A C N 3 i c b V D L a h R B F K 2 O r z i + R l 2 6 K R y E i D B 0 h 6 A B N x F B X E k E J x m Y H p v b N b c n x d S L q t t x h q b z V
y(x) which are modeled by the BO algorithm to determine the appropriate input x values. Another approach uses a low fidelity estimate of f (x) to identify promising regions in X and then increases the fidelity of the estimate while sampling from this reduced space; this process is then repeated iteratively. This method, known as multi-fidelity BO (MFBO), gradually zeros in on an optimal region, reducing the number of experiments that have to be performed with the real system [29,13,32]. The approach we have opted to use, which we refer to as reference-based BO or Ref-BO, is based on the framework presented in [14], and is similar to MFBO. Specifically, Ref-BO initializes the BO algorithm with an approximation of f (x) in order to highlight promising regions in the design space where the solution might be located; BO can then focus sampling in such regions from the start and avoid unnecessary experiments. We refer to this initial approximation as the reference model, and it can be obtained through various means (e.g., physics models, empirical correlations, or low-fidelity simulators). Unlike MFBO, Ref-BO always samples from the real system and does not use the low-fidelity model to restrict sampling to any one region of the design space; additionally, the reference model is not modified after it has been loaded into the algorithm. As a result, the Ref-BO algorithm is simple to implement. Once the reference model has been obtained, only a few minor modifications to S-BO are required without the need to make any additional assumptions or set additional hyperparameters beyond those associated with S-BO.</p>
</blockquote>
<p>To incorporate a reference model in the BO search, we reformulate the optimization problem in (1) as:
min x g(x) + ε(x) (5a) s.t. x ∈ X (5b)
where g : X → R is the reference model and ε : X → R is the residual or error model satisfying
ε(x) = f (x) − g(x), x ∈ X.
The reference model g is assumed to be a deterministic function that captures features of the performance function f . We also assume that, during the BO search, the reference model is fixed and unaffected by new data collected. The coarse features of f are often captured by g and thus the residual ε is typically easier to learn using a GP model.</p>
<p>The form of the residual model ε is not known (because f is not known) and thus a surrogate needs to be built from experimental data. Given a set of data D ε :
= {x K , ε K }, with ε K := {f (x k ) − g(x k )} k∈K ,
we construct a GP model for the residual; the prediction of the residual at a new point
x is the Gaussianε (x) ∼ N (µ ε (x), σ ε (x)) with: µ ε (x) := K(x, x K )K(x K , x K ) −1 ε K , x ∈ X (6a) σ ε (x) := K(x, x) − K(x, x K ) T K(x K , x K ) −1 K(x K , x), x ∈ X. (6b)
We build the surrogatef = g +ε of the performance function based on the surrogate of the residual; because g is assumed to be deterministic, we have that:
f (x) ∼ N (g(x) + µ ε (x), σ ε (x) 2 )(7)
This indicates that we need to modify the AF used in S-BO as:
AF ε (x; κ) = (g(x) + µ ε (x)) − κ · σ ε (x), x ∈ X(8)
This AF is minimized to obtain the next experiment and residual evaluation
{x +1 , ε(x +1 )}, with ε(x +1 ) = g(x +1 ) − f (x +1 )
, and we use this data point to update the data set D +1 ε . We can thus see that BO with a reference model is analogous to standard BO. The general Ref-BO framework is presented in Algorithm 2.</p>
<p>Algorithm 2: Reference-Based Bayesian Optimization (Ref-BO)</p>
<p>Given reference model g, κ, L, and D ε ; Train GPε using D ε and obtain AF ε ;
for = 1, 2, ..., L do Compute experiment x +1 ← argmin x AF ε (x; κ) s.t. x ∈ X; Evaluate performance at x +1 to obtain f +1 and residual ε +1 ; Update dataset D +1 ε ← D ε ∪ x +1 , ε +1 ;
Train GP using D +1 ε to obtainε +1 ;</p>
<p>end 3 Parallel Bayesian Optimization</p>
<p>The S-BO and Ref-BO frameworks discussed are sample efficient (typically require few experiments to identify optimal performance) but they are inherently sequential. Several approaches for proposing multiple experiments per cycle have been developed, each with varying degrees of complexity and sample efficiency. These parallel BO variants can grouped into four main parallelization paradigms: AF optimization over a set of hyperparameters, design space partitioning, fantasy sampling, and AF optimization over a batch of points. The most used approach is the NxMCMC method, which falls under the fantasy sampling paradigm, and is used in popular BO packages such as Spearmint [27]. We now proceed to discuss the specifics of different existing algorithms that are based on these parallelization paradigms.</p>
<p>Hyperparameter Sampling Algorithm (HP-BO)</p>
<p>The hyperparameter sampling algorithm (which we refer to as HP-BO) identifies a new batch of experiments x +1 K by optimizing the acquisition function AF (x; κ k ) using K different values of the exploratory hyperparameter κ k , k ∈ K. In other words, we obtain the new experiments by solving:
x +1 k ← argmin x AF f (x; κ k ) (9a) s.t. x ∈ X (9b)
for k ∈ K. The hyperparameter values κ k , k ∈ K, can be selected manually or sampled from a distribution. In the approach that we consider here, we generate the values by sampling from an exponential distribution κ ∼ E(λ) with rate parameter λ = 1 as shown in [11]. Once the batch of experiments has been determined, we can evaluate their performance (in parallel) to obtain f +1
k = f (x +1 k ), k ∈ K and update the data D +1 ← D ∪ x +1 K , f +1 K .
The updated data is then used to train a new GPf +1 , which is used to form a new acquisition AF +1 f , and to compute a next batch of experiments x +2 K . The process is repeated over multiple cycles. The pseudocode for implementing HP-BO is shown in Algorithm 3.</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d q w 4 X L k 6 L k A P 3 Z / / L z S K G 3 m n a t I = "</p>
<blockquote>
<p>A A A B 9 X i c b V A 9 S w N B E J 2 L X z F + R S 1 t F h P B 6 r h N o T Z C w M Y y g v m A 5 A x 7 m 7 1 k y d 7 e s b u n h C P / w 8 Z C E V v / i 5 3 / x k 1 y h S Y + G H i 8 N 8 P M v C A R X B v P + 3 Y K a + s b m 1 v F 7 d L O 7 t 7 + Q f n w q K X j V F H W p L G I V S c g m g k u W d N w I 1 g n U Y x E g W D t Y H w z 8 9 u P T G k e y 3 s z S Z g f k a H k I a f E W O m h 2 h u T J C F 9 f O 2 5 u N o v V z z X m w O t E p y T C u R o 9 M t f v U F M 0 4 h J Q w X R u o u 9 x P g Z U Y Z T w a a l X q p Z Q u i Y D F n X U k k i p v 1 s f v U U n V l l g M J Y 2 Z I G z d X f E x m J t J 5 E g e 2 M i B n p Z W 8 m / u d 1 U x N e + R m X S W q Y p I t F Y S q Q i d E s A j T g i l E j J p Y Q q r i 9 F d E R U Y Q a G 1 T J h o C X X 1 4 l r Z q L L 1 x 8 V 6 v U a 3 k c R T i B U z g H D J d Q h 1 t o Q B M o K H i G V 3 h z n p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B T x u R B w = = &lt; / l a t e x i t &gt;  1 = 0.1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 6 H I t g j 0 a 2 O L u L i B W J K E B V 2 U 9 5 0 = " &gt; A A A B 9 X i c b V A 9 S w N B E J 2 L X z F + R S 1 t F h P B 6 r i 7 Q m 2 E g I 1 l B P M B y R n 2 N p t k y d 7 e s r u n h C P / w 8 Z C E V v / i 5 3 / x k 1 y h S Y + G H i 8 N 8 P M v E h y p o 3 n f T u F t f W N z a 3 i d m l n d 2 / / o H x 4 1 N R J q g h t k I Q n q h 1 h T T k T t G G Y 4 b Q t F c V x x G k r G t / M / N Y j V Z o l 4 t 5 M J A 1 j P B R s w A g 2 V n q o d s d Y S t w L r n 3 X q / b K F c / 1 5 k C r x M 9 J B X L U e + W v b j 8 h a U y F I R x r 3 f E 9 a c I M K 8 M I p 9 N S N 9 V U Y j L G Q 9 q x V O C Y 6 j C b X z 1 F Z 1 b p o 0 G i b A m D 5 u r v i Q z H W k / i y H b G 2 I z 0 s j c T / / M 6 q R l c h R k T M j V U k M W i Q c q R S d A s A t R n i h L D J 5 Z g o p i 9 F Z E R V p g Y G 1 T J h u A v v 7 x K m o H r X 7 j + X V C p B X k c R T i B U z g H H y 6 h B r d Q h w Y Q U P A M r / D m P D k v z r v z s W g t O P n M M f y B 8 / k D U K a R C A = = &lt; / l a t e x i t &gt;  2 = 1.0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m C o d g M b + V h J U F h D D Y e F S z 9 l u R 9 A = " &gt; A A A B 9 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t D h P B K t x F U B s h Y G M Z w c R A c o S 5 z S Z Z s r e 3 7 u 4 F w p H f Y W O h i K 0 / x s 5 / 4 y a 5 Q h M f D D z e m 2 F m X i g 5 0 8 b z v p 3 c 2 v r G 5 l Z + u 7 C z u 7 d / U D w 8 a u o 4 U Y Q 2 S M x j 1 Q p R U 8 4 E b R h m O G 1 J R T E K O X 0 M R 7 c z / 3 F M l W a x e D A T S Y M I B 4 L 1 G U F j p a D c G a G U 2 L 2 4 8 b 1 y t 1 j y K t 4 c 7 i r x M 1 K C D P V u 8 a v T i 0 k S U W E I R 6 3 b v i d N k K I y j H A 6 L X Q S T S W S E Q 5 o 2 1 K B E d V B O j 9 6 6 p 5 Z p e f 2 Y 2 V L G H e u / p 5 I M d J 6 E o W 2 M 0 I z 1 M v e T P z P a y e m f x 2 k T M j E U E E W i / o J d 0 3 s z h J w e 0 x R Y v j E E i S K 2 V t d M k S F x N i c C j Y E f / n l V d K s V v z L i n 9 f L d W q W R x 5 O I F T O A c f r q A G d 1 C H B h B 4 g m d 4 h T d n 7 L w 4 7 8 7 H o j X n Z D P H 8 A f O 5 w / j g 5 D R &lt; / l a t e x i t &gt;  3 = 10 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h E t c S 6 F Z a t x c V B S Z K 2 F F x U L C K m 8 = " &gt; A A A B 9 X i c b V B N S w M x E M 3 W r 1 q / q h 6 9 B F t B E M q m B / V Y 8 O K x g v 2 A d l u y 6 W w b m s 0 u S V Y t S / + H F w + K e P W / e P P f m L Z 7 0 N Y H A 4 / 3 Z p i Z 5 8 e C a + O 6 3 0 5 u b X 1 j c y u / X d j Z 3 d s / K B 4 e N X W U K A Y N F o l I t X 2 q Q X A J D c O N g H a s g I a + g J Y / v p n 5 r Q d Q m k f y 3 k x i 8 E I 6 l D z g j B o r 9 c p P f d J L u y D E B Z m W + 8 W S W 3 H n w K u E Z K S E M t T 7 x a / u I G J J C N I w Q b X u E D c 2 X k q V 4 U z A t N B N N M S U j e k Q O p Z K G o L 2 0 v n V U 3 x m l Q E O I m V L G j x X f 0 + k N N R 6 E v q 2 M 6 R m p J e 9 m f i f 1 0 l M c O 2 l X M a J A c k W i 4 J E Y B P h W Q R 4 w B U w I y a W U K a 4 v R W z E V W U G R t U w Y Z A l l 9 e J c 1 q h V x W y F 2 1 V K t m c e T R C T p F 5 4 i g K 1 R D t 6 i O G o g h h Z 7 R K 3 p z H p 0 X 5 9 3 5 W L T m n G z m G P 2 B 8 / k D J y u R l Q = = &lt; / l a t e x i t &gt; x<code>+ 1 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 Q F G c p F / J M R E N M w 9 S L 0 X R G L 2 i f Y = " &gt; A A A B 9 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 B F t B E E q S g 3 o s e P F Y w X 5 A m 5 b N d t o u 3 W z C 7 k Y t o f / D i w d F v P p f v P l v 3 L Y 5 a O u D g c d 7 M 8 z M C 2 L O l H a c b y u 3 t r 6 x u Z X f L u z s 7 u 0 f F A + P G i p K J M U 6 j X g k W w F R y J n A u m a a Y y u W S M K A Y z M Y 3 8 z 8 5 g N K x S J x r y c x + i E Z C j Z g l G g j d c t P P a + b d p D z C 3 d a 7 h V L T s W Z w 1 4 l b k Z K k K H W K 3 5 1 + h F N Q h S a c q J U 2 3 V i 7 a d E a k Y 5 T g u d R G F M 6 J g M s W 2 o I C E q P 5 1 f P b X P j N K 3 B 5 E 0 J b Q 9 V 3 9 P p C R U a h I G p j M k e q S W v Z n 4 n 9 d O 9 O D a T 5 m I E 4 2 C L h Y N E m 7 r y J 5 F Y P e Z R K r 5 x B B C J T O 3 2 n R E J K H a B F U w I b j L L 6 + S h l d x L y v u n V e q e l k c e T i B U z g H F 6 6 g C r d Q g z p Q k P A M r / B m P V o v 1 r v 1 s W j N W d n M M f y B 9 f k D K L m R l g = = &lt; / l a t e x i t &gt; x</code>+ 1 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W b p B J J f a g a Y K S U s X Z E c N O D 5 2 k K k = " &gt; A A A B 9 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X Y C s I Q k k q q M e C F 4 8 V 7 A e 0 a d l s J + 3 S z S b s b t Q S + j + 8 e F D E q / / F m / / G b Z u D t j 4 Y e L w 3 w 8 w 8 P + Z M a c f 5 t l Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w s F h Q 0 W J p F i n E Y 9 k y y c K O R N Y 1 0 x z b M U S S e h z b P q j m 6 n f f E C p W C T u 9 T h G L y Q D w Q J G i T Z S t / T U u + i m H e T 8 3 J 2 U e o W i U 3 Z m s J e J m 5 E i Z K j 1 C l + d f k S T E I W m n C j V d p 1 Y e y m R m l G O k 3 w n U R g T O i I D b B s q S I j K S 2 d X T + x T o / T t I J K m h L Z n 6 u + J l I R K j U P f d I Z E D 9 W i N x X / 8 9 q J D q 6 9 l I k 4 0 S j o f F G Q c F t H 9 j Q C u 8 8 k U s 3 H h h A q m b n V p k M i C d U m q L w J w V 1 8 e Z k 0 K m X 3 s u z e V Y r V S h Z H D o 7 h B M 7 A h S u o w i 3 U o A 4 U J D z D K 7
x Z j 9 a L 9 W 5 9 z F t X r G z m C P 7 A + v w B K k e R l w = = &lt; / l a t e x i t &gt;</p>
</blockquote>
<p>x`+ 1 3 Figure 4: HP-BO optimizes the AF for a set of hyperparameters κ k , k ∈ K to obtain experiments x k , k ∈ K that can be evaluated in parallel. Here, we show an example with K = 3.</p>
<p>The main advantages of HP-BO are that it is easy to implement, that it is highly parallelizable, and that it allows for the selection of experiments under various exploration and exploitation settings (eliminating the need for tuning κ). The effect of the hyperparameter κ on the AF is highlighted in Figure 4. However, in this approach it is not possible to prevent the proposal of redundant experiments and, as the algorithm converges, the suggested experiments can begin to cluster in a region of low uncertainty (this can cause the algorithm to get trapped at local solutions). The HP-BO algorithm can be easily be extended to incorporate a reference function g; in this case, the GP learns the residual instead of the performance function.</p>
<p>Algorithm 3: Hyperparameter Sampling BO (HP-BO)</p>
<p>Given κ k , k ∈ K, L, and D ; Train GPf using D and obtain AF f (x; κ k ), k ∈ K;
for = 1, 2, ..., L do for k ∈ K do Compute experiment x +1 k ← argmin x AF f (x; κ k ) s.t. x ∈ X; Evaluate performance at x +1 k to obtain f +1 k ; end Update data D +1 ← D ∪ x +1 K , f +1 K ;
Train GP using D +1 to obtainf +1 ; end</p>
<p>HyperSpace Partitioning Algorithm (HS-BO)</p>
<p>The HyperSpace partitioning algorithm (which we refer to as HS-BO) was presented in [33]; this parallelizes BO by partitioning the design space X into K equally-sized blocks X k ⊆ X, k ∈ K. Importantly, this approach does not use a surrogate GP model over the entire design space; instead, a separate GP model is constructed at each partition X k and is updated using only information collected within this partition. Specifically, each partition k ∈ K builds a GPf k ∼ N (µ f,k (x), σ f,k (x)) that is used to construct an acquisition function AF f,k (x; κ). With this, we can obtain a set of new experiments by solving the following subproblems:
x +1 k ← argmin x AF f,k (x; κ) (10a) s.t. x ∈ X k (10b)
for k ∈ K. The domain partitions can also be constructed to have a certain degree of overlap; specifically, an overlap hyperparameter φ ∈ [0, 1] is introduced to allow the partitions to share a fraction of the design space. A value of φ = 0 indicates that the partitions X k are completely separate, while a value of φ = 1 indicates that X k = X for k ∈ K (the partitions are copies of the full design space); this is shown in Figure 5. The overlap hyperparameter provides a communication window, allowing the GP model of a given partition to observe system behavior beyond its prescribed partition (share information with other partitions). This, however, introduces a fundamental trade-off; from a parallelization perspective it is desirable that φ is small, but from convergence perspective (e.g., reducing number of iterations) it might be desirable that φ is large. A similar trade-off is observed as one decreases or increases the number of partitions K; as such, there is a complex interplay between the hyperparameters K, φ, and these need to be tuned. In the implementation reported in [33], the number of partitions is set to K = 2 d . Similar types of tradeoffs have been observed in the context of overlapping decomposition approaches for optimization problems defined over graph domains [24].</p>
<p>The HS-BO approach, summarized in Algorithm 4, is easy to implement, is scalable to highdimensional spaces, and enables the development of GP models for systems that may exhibit different behaviors at various regions of the design space (compared to using a single GP model that captures the entire design space). HS-BO also eliminates redundant sampling by forcing the algorithm to sample from distinct regions of the design space. This also results in a more thorough search, which improves the probability that the global solution will be located; domain partitioning is, in fact, a paradigm widely used in global optimization. A limitation of HS-BO is that the partitions are boxes of equal size (this can limit capturing complex shapes of the performance function); moreover, one needs to tune K and φ. In principle, it might be possible to extend this approach to account for automatic tuning and adaptive partitions, but this would require much more difficult implementations that carefully trade-off parallelization and convergence (this is left as a topic of future work). The HS-BO approach can also be easily executed using a reference model by learning the residual instead of the performance function. Given κ, K, L, φ, and D ; Partition X into X k ⊆ X, k ∈ K with overlap φ; Split initial data into domains D k , k ∈ K; for k ∈ K do Train GPf k in X k using D k ; end for = 1, 2, ..., L do
for k ∈ K do Compute experiment x +1 k ← argmin x AF f,k (x; κ) s.t. x ∈ X k ; Evaluate performance at x +1 k to obtain f +1 k ; Update data D +1 k ← D k ∪ x +1 k , f +1 k ;
Train GP using D +1 k to obtainf +1 k ; end end</p>
<p>N×MCMC Algorithm (MC-BO)</p>
<p>The N×MCMC (N times Markov Chain Monte Carlo) algorithm is a popular approach used for proposing multiple experiments [22]. We refer to this approach simply as MC-BO. Assume that we currently have a set of experimental data D ; we use this to generate the GPf , acquisition function AF f , and to compute the next experiment x +1 k for k = 1. Our goal is now to obtain the remaining set of experiments x +1 k , k = 2, ..., K that we can use to evaluate performance. To do so, we consider a set of fantasy predictions obtained by generating S samples from the GP f s (x +1 k ), s ∈ S. The term fantasy alludes to the fact that the evaluation of performance is based on the GP (and not on the actual system). The fantasy data has the goal of creating an approximate AF; specifically, for each sample s, we generate a data pointD s := {x +1 k ,f s (x +1 k )} and this is appended to the existing dataset D ∪D s ; this data is then used to obtain a GPf s and associated acquisition functionÂF f,s . The AFs for all samples s ∈ S are collected and used to compute the mean AF:
AF f (x; κ) := 1 S s∈SÂ F f,s (x; κ), x ∈ X.(11)
The new experiment is then obtained by solving:
x +1 k+1 ← argmin x AF f (x; κ) (12a) s.t. x ∈ X (12b)
To generate another experiment, we repeat the sampling process (using a new set of S samples) and create a different mean acquisition function AF f , and we minimize this to obtain x +1 k+2 . The sampling process is repeated until we have the full batch of new experiments x +1</p>
<p>K . Once we have these, we evaluate the performance function at these points (in parallel) to obtain the dataset
{x +1 K , f +1 K }, which we append to the data collection D +1 ← D ∪ {x +1 K , f +1 K }.
We use this data collection to re-train the GP of the performance function and repeat the process. The framework for the MC-BO algorithm is presented in Algorithm 5.</p>
<p>The MC-BO algorithm has proven to be an effective parallel extension of the BO algorithm; however, computing the mean AF requires significant computational time (as the GP model needs to be retrained continuously). This algorithm also has the tendency to propose experiments that are close in the design space, especially when it begins to converge. This does not necessarily pose an issue if the algorithm is converging to global solution; however, if the solution approached is local, this behavior can limit the ability of the algorithm to escape this region. The MC-BO approach can be executed using a reference model by simply learning the residual instead of the performance function.</p>
<p>Algorithm 5: N×MCMC BO Algorithm (MC-BO)</p>
<p>Given κ, S, L, and D ; Train GPf using initial dataset D ;
for = 1, 2, ..., L do Compute x +1 k ← argmin x AF f (x; κ) s.t.
x ∈ X for k = 1;
for k = 1, ..., K − 1 do for s ∈ S do
Generate fantasy data pointD s = x +1 k ,f s (x +1 k ) ; Use dataset D ∪D s to train GPf s and obtainÂF f,s (x; κ);
end Set AF f (x; κ) ← 1 S s∈SÂ F f,s (x; κ); Compute experiment x +1 k+1 ← argmin x AF f (x; κ) s.t. x ∈ X; end for k ∈ K do Evaluate performance at x +1 k to obtain f +1 k ; end Update data D +1 ← D ∪ x +1 K , f +1 K ;
Train GP using D +1 to obtainf +1 ; end</p>
<p>Batch Bayesian Optimization Algorithm (q-BO)</p>
<p>The q-BO or batch Bayesian optimization algorithm uses a multipoint acquisition function, AF q (x K ; κ), like the q-LCB presented in [31], to select a batch of q experiments that can be run in parallel. Unlike most adaptations of BO where the AF is optimized over a single point, the q-LCB is optimized over a set of q points. By selecting the experiments in a batch rather than independently, as in HP-BO, or sequentially, as in MC-BO, q-LCB is able to measure the correlation between the suggested sample locations, allowing it to more easily avoid the issue of redundant sampling. Given a desired batch size, the value of a particular set of experiments x K is measured according to:
AF q (x K ; κ) = 1 S S s=1 max µ q (x K ) − κ · |A q (x K ) z s |(13)
where µ q (x K ) ∈ R q and A q (x K ) ∈ R q×q are the GP mean and Cholesky factor of the GP covariance (AA T = Σ) at a batch of points x K respectively, z s ∈ R q is a random variable with z s ∼ N (0, I), and | · | is the absolute value (element-wise) operator. The new batch is then selected by solving:
x +1 K ← argmin x K AF q (x K ; κ) (14a) s.t. x K ∈ X (14b)
where, again, the optimization is done over the entire batch of q points in x K . The experiments are then run (in parallel) and the collected performance measurements are used to update the dataset D +1 ← D ∪ {x +1 K , f +1 K }. This data is used to retrain the model which enables the selection of the next batch of experiments. The pseudocode for q-BO is summarized in Algorithm 6.</p>
<p>The q-BO algorithm has proven to be especially popular in the multi-objective optimization setting and is, in principle, not difficult to implement. However, unlike single-point AFs, multipoint AFs do not have a closed-form representation and, as a result, constructing and optimizing AF q (x K ; κ) requires the use of numerical methods like Monte Carlo, as seen in (13), making this an intensive process, especially as q increases. Additionally, while the use of the Cholesky factor ensures that the algorithm cannot select redundant experiments, safeguards must be placed when constructing the AF optimization problem to ensure that the optimizer cannot select identical points as this will result in the covariance matrix Σ (x K ) being singular and cause the optimizer to fail. In this work, that safeguard was implemented as a tolerance value that set the minimum allowable distance between any two points within x K . We should note that while using this strategy we observed that q-BO can and does occasionally select points that are within of each other. This can be practically as undesirable as redundant sampling, depending on the value of . A reference model can also be easily incorporated into the q-BO approach by having the algorithm learn the residual instead of the performance function.</p>
<p>Algorithm 6: Batch Bayesian Optimization Algorithm (q-BO)</p>
<p>Given κ, S, L, and D ; Train GPf using initial dataset D ; for = 1, 2, ..., L do
Compute x +1 K ← argmin x K AF q (x K ; κ) s.t. x K ∈ X; for k ∈ K do Evaluate performance at x +1 k to obtain f +1 k ; end Update data D +1 ← D ∪ x +1 K , f +1 K ;
Train GP using D +1 to obtainf +1 ;</p>
<p>end 4 Parallel Bayesian Optimization using Informed Partitioning</p>
<p>We propose new paradigms for parallel BO that conduct informed partitioning of the design space. Specifically, we propose a domain partitioning approach (analogous to HS-BO) that conducts partitioning by following the level sets of the performance function. Because the performance function cannot be easily evaluated, we use a reference model to guide the partitioning; this approach allows us to leverage expert or physical knowledge, which might highlight certain regions of the design space that are promising or non-promising (and with this prioritize). We also propose a variable partitioning approach that aims to exploit partially separable structures that are commonly found in complex systems; specifically, in these systems, the performance function is composed of a collection of functions for different subsystems (but the functions are coupled together via common variables). The key idea is then to search the design space by following this separable structure, while sharing information between the coupling variables. We refer to these paradigms as level-set partitioning (LS-BO) and variable partitioning (VP-BO).</p>
<p>Level-Set Partitioning Algorithm (LS-BO)</p>
<p>LS-BO uses domain partitions of the design space X that follow the levels sets of the reference function g. We recall that the α-level set (sublevel) of this scalar function is:
X(α) := {x ∈ X | g(x) ≤ α} ⊆ X(15)
for any α ∈ R. We now note that solving the AF optimization problem:
min x AF (x) (16a) s.t. x ∈X(α)(16b)
would force the BO algorithm to restrict the search over a restricted subdomainX(α). However, solving this optimization problem can be difficult if g does not have an explicit algebraic form (e.g., low-fidelity simulator) or has a complex form (e.g., physics model). To overcome this limitation, we construct a GP modelĝ of g to define the approximate level set:
X(α) := {x ∈ X|ĝ(x) ≤ α}.(17)
We use the previous basic observations to derive our domain partitioning approach; we construct a set of domain partitionsX k ⊆ X, k ∈ K by following different level-sets of the function. Specifically, we construct the subdomains:
X k := {x ∈ X|α k ≤ĝ(x) ≤ α k+1 }, k ∈ K,(18)
We note that the subdomains are upper and lower bounded in order to obtain non-overlapping partitions. The level set thresholds α k are set by discretizing the range ofĝ(x). The simplest method for generating the subdomains is to uniformly discretize the interval between the extreme lower and upper values of the reference model as follows:
α k = α 1 + (k − 1)∆, k = 2, ..., K(19)
where ∆ = α K+1 −α 1 K , α 1 = min x∈Xĝ (x), and α K+1 = max x∈Xĝ (x). In cases where additional specificity is desired, the partitions can be further adapted by setting the intervals according to various factors such as a focus on a particular region of the design space, the desired level of exploration vs exploitation, the level of confidence in the quality of the reference model, the geometry ofĝ(x), and so on. The partitioning approach is illustrated in Figure 6. Figure 6: Level set partitioning (LS-BO) uses the α-level sets of the reference g to split X into subdomainsX k , k ∈ K. Depending on the complexity of g, enforcing level set constraints in the AF optimization problem can be difficult; therefore, the level sets are approximated using the surrogate modelĝ.</p>
<p>As in S-BO, we begin with dataset D = {x K , f K } and build a GPf and the acquisition function AF . We then obtain a new set x +1 K by solving the following collection of optimization problems:
x +1 k ← argmin x AF (x) (20a) s.t. x ∈X k (20b)
for k ∈ K. Using the new experiments x +1 K we evaluate system performance f +1 K (in parallel) and we append the collected data to the dataset D +1 ← D ∪ {x +1 K , f +1 K }. The new dataset is used to update the GPf +1 and the acquisition function AF +1 . A summary of this procedure is presented in Algorithm 7.</p>
<p>Algorithm 7: Level-Set Partitioning BO (LS-BO)</p>
<p>Given κ, g, L, K; Build surrogateĝ of g; Construct partitionsX k ⊆ X, k ∈ K using level sets ofĝ; Train GPf with initial dataset D ;
for = 1, 2, ..., L do for k ∈ K do Compute experiment x +1 k ← argmin x AF f (x; κ) s.t. x ∈X k ; Evaluate performance at x +1 k to obtain f +1 k ; end Update D +1 ← D ∪ x +1 K , f +1 K ;
Retrain GP using D +1 to obtainf +1 ;</p>
<p>end</p>
<p>It is important to highlight that the LS-BO approach that we propose uses a GP model of the performance function and an AF that are defined over the entire design space X; this approach thus differs from HS-BO (which uses a different GP and AF in each partition X k ). Moreover, we note that the partitioning of the space follows the level sets of the reference function, and this allows us to concentrate experiments over regions that are most promising. The proposed LS-BO approach can also be implemented in such a way that the reference function is exploited to learn the residual (as opposed to learning the performance function). As such, we can leverage the reference function for constructing the domain partitions and for guiding the search.</p>
<p>Variable Partitioning Algorithm (VP-BO)</p>
<p>Many physical systems are typically composed of individual components that are partially interconnected (e.g., they are modular). For instance, a chemical process includes units (e.g., reactors and separations) that are interconnected, and the performance of each unit contributes to the total system performance. Moreover, the performance of each unit is typically strongly affected by the unit variables and less affected by variables of other units. This partially separable structure can be captured as the following optimization problem:
min x k∈K f k (x k ; x −k ) (21a) s.t. x ∈ X (21b)
where f k : X → R is the performance contribution of component k, the entire set of decision variables is split into K subsets as x := {x 1 , x 2 , ...., x K }, and we define x −k := x \ {x k } (entire set of variables that does not include x k ). We should note that the variable partitions should be nonoverlapping subsets (i.e., x i ∩ x j =i = {0}, i, j ∈ K). Additionally, we assume that that performance function can be decomposed as f = k∈K f k .</p>
<p>VP-BO follows a Gauss-Seidel paradigm; assume we have an initial set of data D = {x K , f K }; Here, we assume that we measure f 1 , ..., f K in each experimental module so that f K ∈ R ×K rather than R as in the previous algorithms; note that this means that f k corresponds to the k th column of f K . We optimize the individual performance of subcomponent k using the variables x k , while keeping the rest of the variables x −k constant (to the values of the previous iteration ):
min x k f k (x k ; x −k ) (22a) s.t. (x k ; x −k ) ∈ X (22b)
for k ∈ K. Accordingly, we decompose the AF optimization problem into the subproblems:
x +1 k ← argmin x k AF f k (x k ; x −k , κ) (23a) s.t. (x k ; x −k ) ∈ X. (23b) for k ∈ K.
Here, AF f k is the acquisition function of component k which is built using the GPf k of the performance f k . Moreover, x −k is the value of the variables not in partition k at the current iteration (which are held fixed when optimizing the AF f k ).</p>
<p>We partition the variables by leveraging the reference model g; specifically, we use this the reference to identify which variables have the most impact on individual components of the system; this can be done in various ways. The most straightforward method would be via inspection using a combination of information provided by the reference model and any available expert knowledge over the importance of the various inputs on the subsystems. If such information is not available, g(x) can instead be analyzed with an appropriate feature importance technique such as sparse principal components analysis (SPCA) [35], automatic relevance determination (ARD) [30], model class reliance (MCR) [8], etc., to determine the appropriate variable-subsystem pairings. Because the partitions must not overlap, the results of this analysis should be checked for instances where an input is paired with multiple subsystems. If this occurs, we recommend that the input in question be paired with the subsystem where it has the highest relative importance. The pseudocode for implementing VP-BO is shown in Algorithm 8.</p>
<p>One of the advantages of the VP-BO approach is that the AF optimization over each partition only uses a subset of variables; this can significantly reduce the computational time. Moreover, this approach is amenable for implementation in a distributed manner (e.g., each unit of a process has its own separate BO algorithm). The VP-BO approach (and the LS-BO approach) also takes system-specific behavior into account when developing partitions (informed by the reference model). As we will show in the next section, the use of prior knowledge can lead to significant reductions in computational time and in the number of experiments performed. Moreover, we will see that such knowledge can help identify solutions and surrogate models of higher quality. VP-BO can also be implemented in such a way that reference model is also used to guide the construction of the performance function (by learning the residual instead of the performance function). We also highlight that the VP-BO approach proposed is implemented in a way that each partition has its own GP model and AF; however, it is also possible to implement this approach by building a central GP and AF that are optimized in each partition using a different set of variables.</p>
<p>Algorithm 8: Variable Partitioning BO (VP-BO)</p>
<p>Given κ, K, L, and D ;
Decompose f (x) into f k (x k , x −k ) for k ∈ K; Use D to train GPsf k , k ∈ K; for = 1, 2, ..., L do for k ∈ K do Compute experiment x +1 k ← argmin x k AF f k x k ; x −k , κ s.t. x k , x −k ∈ X; Evaluate f 1 , ..., f K at x +1 k to obtain f +1 K [k, :]; end for k ∈ K do x +1 −k ← argmin x −k f +1 K [:, k]; end Update D +1 ← D ∪ x +1 K , f +1 K ;
Retrain GPsf k , k ∈ K using D +1 end</p>
<p>Numerical Case Studies</p>
<p>We now present numerical results using the different BO strategies discussed; our goal is to demonstrate that the parallel BO approaches proposed provide significant advantages over S-BO and over other state-of-the-art parallel approaches. Our study simulates the performance of a pair of reactors connected in series; the operating cost of this system is a complex function of the operating temperatures. The detailed physical model used to simulate the performance of the system is discussed in the Appendix. To guide our partitioning approaches, we develop a reference model that approximates the physical model. All data and code needed to reproduce the results can be found at https://github.com/zavalab/bayesianopt.</p>
<p>The optimization problem that we aim to solve with BO can be written as:  Note that the reference model captures the overall (coarse) structure of the performance function but misses some finer details.
min T 1 ,T 2 f (T 1 , T 2 ) = f 1 (T 1 , T 2 ) + f 2 (T 1 , T 2 ) (24a) s.t. (T 1 , T 2 ) ∈ T (24b)
The reference model g is derived from a simplified physical model (see the Appendix); however, this model would be difficult to incorporate directly in the AF formulation for the LS-BO and VP-BO approaches (because the model involves a complex set of algebraic equations). As such, we approximate this model using a GP,ĝ, and use this as the reference. Figure 8 illustrates that the GPĝ is virtually indistinguishable from the simplified physical model g; as such, we can safely use this to guide our search and to guide our partitioning approaches.</p>
<p>The HS-BO algorithm is restricted to 2 2 = 4 partitions when dealing with a 2D design space; as such, and in order to achieve fair comparisons, we limit the number of parallel experiments collected with MC-BO, HP-BO, q-BO, and LS-BO to 4. The VP-BO algorithm was run using 2 partitions (one for each reactor). All algorithms were implemented in Python 3.7 and the GP modeling was done using the gaussian process package in Scikitlearn. Specifically, we used the built-in Matern method as the kernel function. This selection was motivated by the ability of the Mátern kernel to control the smoothness of the resultant function making it highly flexible and capable of accurately modeling systems that exhibit significant nonlinearity and non-smoothness; we set the smoothness parameter ν = 2.5, which tends to be the standard choice. At every iteration, the optimal value of the kernel's hyperparameters, the characteristic length scales l, was updated using the package's built-in optimizer that sets l by solving a log-marginal-likelihood (LML) problem. A more detailed description of the gaussian process package can be found in [20]. The optimization of the AF was done in Scipy using an unconstrained minimization solver (based on L-BFGS-B) for every BO algorithm except LS-BO. The introduction of the reference GP model in the constraints of the AF minimization problem required the selection of a method capable of constrained optimization; for this, we selected SLSQP. Except for HS-BO, the exploratory parameter of the acquisition function was set to the same fixed value. All algorithms were initialized using the same starting point and we conducted 25 different runs with different starting points in order to evaluate robustness. We also ran instances of LS-BO and VP-BO with and without using a reference in the AF (for learning the residual or the performance function); this allowed us to isolate the impacts of the use of the reference model and ensure that observed performance improvements can be attributed to parallel capabilities. For both LS-BO and VP-BO, the reference function was always used to guide the selection of the partitions. Figure 9 highlights the level sets that we used to partition the design space for the LS-BO approach. These partitions were generated by first locating the minima (local and global) ofĝ(x). After determining that there were two, we discretized the range ofĝ(x) by building a search interval around each of the minima where the lower bound of the interval was the value of the corresponding minimum. The value ofĝ was then evaluated at various points on a line connecting the two minima to determine the spacing of the level sets. This information was used to select the upper bound of these search intervals. We were also able to use this analysis to gauge the size of both of the partitions and observed that the search region around the global minima appeared to cover a significant portion of the design space. As a result, this partition was split along the level set value that resulted in two roughly equal-sized partitions. The fourth and final partition was constructed to search the remaining space outside of the three existing partitions. Figure 9 also provides an illustrative summary of this workflow. Note that one of the regions is near the region of the global minimum of f . Figure 9: Domain partitions for reactor system obtained using reference modelĝ (left); the line connecting the two minima of the reference model is shown in blue. Values ofĝ along this line (right) indicate that the level setĝ = −383 (black line) provides an acceptable split between the two partitions surrounding the minima, while the level set (red line)ĝ = −461 allows for the partition surrounding the global minimum (denoted as (T * 1 , T * 2 )) to be split into two roughly equal-sized partitions. Note that domain X III is in the region of the global minimum of f .</p>
<p>Given that the reactors are arranged in series, it is clear that the performance of the first reactor is independent of T 2 , while the performance of the second reactor will likely have some dependence on T 1 . Figure 10 demonstrates this partially-separable structure; note how the first function g 1 is not affected by T 2 (vertical lines), while g 2 does depend on T 1 . Using ARD, we confirmed that T 1 , which had a characteristic length scale of l = 0.145, was a more important input to g 1 than T 2 (l = 1000), while for g 2 , T 2 (l = 0.399) was determined to be more important than T 1 (l = 0.498).</p>
<p>We thus implemented the VP-BO approach according to the following variable partitions: x 1 = T 1 , x −1 = T 2 and x 2 = T 2 , x −2 = T 1 . Figure 11 summarizes the average performance (over the 25 runs) of LS-BO and VP-BO (using reference models) along with the remaining algorithms; here, we visualize the total experiment time (wall-clock time needed to evaluate performance function) against the best found perfor- Figure 10: Reference model for the first reactor g 1 (left) and for the second reactor g 2 (right). We can see that g 1 is not affected by T 2 ; the combination of these functions give rise to the reference function g = g 1 + g 2 . mance up to the corresponding time. Overall, we observe that all parallel BO variants performed better than standard BO both in terms of speed and best performance found. The performances at the local minima for (T 1 , T 2 ) = (423, 340) and (T 1 , T 2 ) = (423, 423) were approximately -395,000 USD/yr and -387,000 USD/yr respectively, while the performance of the global minimum at (T 1 , T 2 ) = (333, 322) was -410,000 USD/yr. On average, the best performance obtained using BO was -394,500 USD/yr, indicating that this approach converges to a local minimum most of the time. By comparison, all the parallel BO variants found a solution that, on average, was below -400,000 USD/yr. We should also note that this improvement in performance value also comes with a significant reduction in the required wall-clock time: BO takes over 500 seconds to converge to its final solution whereas all of the parallel BO variants are able to locate a better solution in approximately 200 seconds.</p>
<p>The magnified profiles of Figure 11 provide a better comparison between the parallel BO variants. It is clear that LS-BO and VP-BO are significantly faster than all other variants. We also observed that LS-BO, VP-BO, HS-BO and q-BO consistently reached the global minimum. This illustrates how the redundant sampling seen in MC-BO and HP-BO can degrade performance. Additionally, while the performances of HS-BO and q-BO were similar to LS-BO and VP-BO, they required significantly more experiments to reach this performance level. From these observations we can draw a couple of conclusions: the use of a reference model for both generating systemspecific partitions and simplifying the learning task delivers significant benefit; and allowing the algorithm to pool the data into a single dataset that is used to build a global surrogate model increases the predictive value of this model, resulting in faster identification of optimal regions.    Figure 11 but we run LS-BO and VP-BO without a reference model. By comparing with the results in Figure 11 , we observe that using the reference model can help with convergence but not always. LS-BO is 24% slower in the average convergence time, though it maintains its ability to consistently converge to the global minima. VP-BO, meanwhile, converges on average 40% faster compared to when the reference model is used; the solution it returns is also unchanged. These results indicate that g has a similar effect on LS-BO as with traditional BO, as outlined in [14]. Namely, that it makes the search more targeted, resulting in more efficient sampling and faster conversion. Meanwhile, with VP-BO, the reference model appears to encourage more exploration of the domain, which can prevent the algorithm from converging prematurely and potentially returning a suboptimal solution. We base this claim on the fact that, when testing VP-BO without a reference, we observed that, while it is not especially sensitive to the initial values of the design variables in a given partition, it is quite sensitive to variable values of other partitions. Overall, however, we observe that both LS-BO and VP-BO still outperform the remaining parallel algorithms (without or without a reference). These results highlight that using the proposed partitioning approaches has a larger effect on overall convergence. This allows us to confirm that the improvements we observe when using LS-BO and VP-BO can be attributed to the parallelization schemes.</p>
<p>The results presented in Figure 11 indicate that LS-BO and VP-BO are consistently more robust and sample efficient than the other approaches. The average values seen in Figures  we can see that, regardless of where LS-BO and VP-BO were initialized, they are always able to converge to the same region (unlike S-BO). We also see that convergence of the algorithms is in general fast but, as expected, it is sensitive to the starting point. The sensitivity to the starting point is further indication of why it is important to have expert knowledge (e.g., via use of a reference model) when initializing the search.</p>
<p>Because evaluating the performance function tends to be expensive, reducing the number of experiments (samples) is also essential. Figure 14 illustrates how S-BO, LS-BO, and VP-BO compare when it comes to sample efficiency. Standard BO samples in a significantly distributed manner with a considerable number of samples drawn from the boundaries of the domain. For LS-BO, we see that in regions where a solution exists (e.g., regions II and III in our case study), sampling is heavily concentrated at or near the solution. In regions where there is not a solution, the sampling is more distributed, however, the majority of samples tend to cluster around partition boundaries that are located near a solution. Samples drawn from partition X I appear to be the most widely distributed, however, this is not surprising as this partition contains a mostly flat region. Another noticeable difference when compared to traditional BO is that there is significantly less sampling at the boundaries of the domain where f has unfavorable (high) values; only 8 out of 2500 samples were taken at the left and bottom bounds. VP-BO exhibits the most clustered sampling; in fact, the vast majority of the samples are drawn from or near the optimal region.  Note that, while the majority of samples for X I (Partition 1) occur at the top domain boundary, this partition corresponds to reactor 1 which only depends on T 1 as seen in Figure 10. Aside from these samples, there is a clear lack of sampling happening at the domain boundaries compared to LS-BO and traditional BO. This result, coupled with exhibiting the lowest convergence time out of all of the tested algorithms, confirm our belief that the VP-BO algorithm tends to be more exploitative. This is likely due to fact that the partitions for this algorithm are optimized over a lower dimensional space and, for a fixed x −k , VP-BO can find the optimal local variables x k much faster than the remaining algorithms can find an optimal global variables x. As a result, without the reference model to indicate the potential existence of a solution elsewhere, VP-BO seems more susceptible to settle into the first solution that it finds than algorithms like LS-BO and HS-BO whose partitions force the algorithm to search more widely.</p>
<p>To estimate the computational cost associated with the different algorithms, we measured the total wall-clock time (average across the 25 runs). The total wall-clock time includes time for performance evaluation (experiment time) and all time required to conduct other computations (e.g., AF optimization, GP training, and reference model evaluation). The results are shown in Figure 15. The closer this time is to the experimental time, the less computationally expensive the algorithm is. For instance, the total wall-clock time of S-BO was 12% higher than the experiment time. We observe that HS-BO and VP-BO are the least computationally intensive methods, with the total wall-clock time being only 14% and 7% higher than the experimental time, respectively. We attribute this to the fact that HS-BO runs separate instances of BO across multiple reduced domains and, because the boundaries are rectangular, ensuring that the AF optimizer stays inside of the partition only involves bounding the upper and lower limits of x it is allowed to search over, and those domains tend to be small. VP-BO, on the other hand, only optimizes over a subset of variables and this greatly reduces the time required for AF optimization. The wall-clock time of HP-BO was 44% higher than the experiment time, this is because it requires solving multiple AF optimization problems across the entire design space. LS-BO had a total wall-clock time that was 46% higher than the experiment time; this is attributed to the more difficult AF optimization problem that it has to solve (which has constraints defined by a GP model). The total wall-clock time for q-BO was 64% higher than the experiment time, which we attributed to the fact that AF optimization is done over a set of points, increasing the size of the problem that is solved. Additionally, the calculation involves more complex matrix operations and requires repetitive sampling. MC-BO was the most computationally intensive algorithm, with a total wall-clock time that was 384% higher than the experiment time. We attribute this to the repetitive computations in this algorithm, which require sequential sampling and GP training.</p>
<p>Conclusions and Future Work</p>
<p>We have proposed new decomposition paradigms for BO that enable the exploitation of parallel experiments. These approaches decompose the design space by following the level sets of the per- Figure 15: Profiles of wall-clock time against performance. Note that this time is comparable to the total experiment time for all algorithms; the only exception is MC-BO, indicating that the AF optimization step (and not the function evaluation) is the bottleneck for this approach.</p>
<p>formance function and by exploiting the partially separable structure of the performance function.</p>
<p>A key innovation of these approaches is the use of a reference function to guide the partitions. Using a case study for a reactor system, we have found that the proposed approaches outperform existing parallel approaches in terms of time and quality of solution found. When using LS-BO, we observed that building partitions that are specialized beyond those that would be generated by the uniform discretization of the range ofĝ(x), like those we used in our case study, can require significant user input. Moving forward we would like to explore methods for developing more efficient and automated protocols for generating the partitions. Additionally, we are also interested in incorporating an element of adaptivity to LS-BO and VP-BO via live modification/tuning of the partitions as samples from the system are collected. The proposed parallel paradigms can also open the door to a number of applications and potentially other decomposition paradigms that we will aim to explore in the future. Specifically, we are interested in exploring more complex systems that involve higher-dimensional design spaces and large numbers of parallel experiments. This will allow us to investigate the asymptotic properties of the proposed approaches. Moreover, we are interested in designing alternative paradigms that selectively exchange information between partitions to accelerate the search and that use different types of reference models to guide the search. We are also interested in exploring the application of these approaches to the tuning of complex controllers. ars program.</p>
<p>A Reactor System Model</p>
<p>A.1 Exact Model</p>
<p>The reactor system consists of a pair of CSTRs operating at steady-state and connected in series. In the first reactor, reactant A is converted into a desired product P , which can react further to form an undesired product U . An additional reactant D reacts with U to form A, and is fed to the first reactor to reduce the amount of U formed. In order to further reduce the amount of U present and increase the value of the product stream, the outlet of the first reactor is then fed to a second reactor along with an additional reactant B, which can react with U to form a secondary product E. The reaction mechanism is complex and given by:
2A ←→ P (25a) P ←→ 2U (25b) U + B ←→ E (25c) U + D → 2A (25d)
The rates of each reaction are assumed to be elementary and thus:
r 1 = k 1 C 2 A − k 1r C P (26a) r 2 = k 2 C P − k 2r C 2 U (26b) r 3 = k 3 C U C B − k 3r C E (26c) r 4 = k 4 C U C D (26d)
where C i is the concentration of species i and k j and k jr are the forward and reverse rate constants of the j th reaction. In our analysis we assumed that k jr = 0.01k j , indicating that the forward reaction is favored. The material balances are:
0 = F in C A in − F out C A − 2(r 1 − r 4 )V (27a) 0 = F in C P in − F out C P + (r 1 − r 2 )V (27b) 0 = F in C U in − F out C U + (2r 2 − r 3 − r 4 )V (27c) 0 = F in C B in − F out C B − r 3 V (27d) 0 = F in C E in − F out C E + r 3 V (27e) 0 = F in C D in − F out C D − r 4 V (27f)
where F in and F out are the volumetric flowrates of the reactor feed and outlet respectively, C i in is the concentration of species i in the feed stream, and V is the volume of the CSTR. The reactions in (25) are assumed to be exothermic and a cooling jacket is used to remove excess heat and control the temperature inside of the reactors. The jacket uses a fluid entering at a temperature T ic and flowing at a mass flowrate ofṁ c as the coolant. The coolant flowrate required to maintain the desired temperature can be determined from the reactor energy balance:
H in = ρC p F T in (28a) H out = ρC p F T (28b) Q = −r 1 V ∆H 1 − r 2 V ∆H 2 − r 3 V ∆H 3 − r 4 V ∆H 4 (28c) m c = H in − H out +Q C pc (T oc − T ic ) = ρC p F (T in − T ) +Q C pc (T oc − T ic ) (28d)
where T in , is the temperature of the inlet stream, ∆H j is the heat of reaction for the j th reaction, and C pc is the specific heat capacity of the coolant. Additionally, we assume that reactions do not change the heat capacity C pin or density ρ in of the reactor inlet. This allows us to set C p = C pin = C pout , ρ = ρ in = ρ out , and F = F in = F out . The relation between the rate constants and temperature is described by the Arrhenius equation:
k = k 0 exp −E A RT(29)
where k 0 is the pre-exponential factor, E A is the activation energy of the reaction, and R is the universal gas constant.</p>
<p>The outlet of the second reactor is fed to series of flash separation units to recover the products from the effluent stream. Product E is recovered in the vapor fraction of the first vessel as streamv 1 , and product P is recovered in the liquid fraction of the second vessel as streaml 2 . The relative volatility of the chemicals is set with respect to the vapor-liquid equilibrium ratio K P ; compositions and flows for the exiting streams can be determined from the following vapor-liquid equilibrium calculation:
x i = z i f (K P α i − 1) + 1 (30a) y i = K p α i x i (30b)
where z i , x i , and y i are the molar fractions of species i in the feed, liquid, and vapor streams respectively. The relative volatility of each chemical is denoted by α i and f is the fraction (on a molar basis) of the feed that exits the vessel in the vapor stream. We set f according to the molar fraction of the recovered product in the feed, f = z E for the first vessel, and f = 1 − z P for the second vessel. The energy required to vaporize the desired fraction of the flash's feed was supplied by a heater that uses steam as the heating agent. The required flowrate of steam,ṁ stm , was determined from the flash vessel energy balancė Q vap = i∈{A,P,U,B,E,D} L i y iv (31a)
m stm =Q vap L H 2 O(31b)
where L i and L H 2 O are the latent molar heat of species i and water, respectively.</p>
<p>The performance of the system is expressed as a cost function (negative profit) that measures the quality of the product streams along with the corresponding utility requirements at various temperatures and is formulated as: f 1 (T 1 , T 2 ) = i∈{A,P,U,B,E,D} w i y i1v1 + i∈{A,P,U,B,E,D}
w i x i2l2 + i∈{A,B,D} w i F i C i0
(32a) f 2 (T 1 , T 2 ) = w c ṁ c +ṁ c + w stm ṁ stm +ṁ stm (32b)
f (T 1 , T 2 ) = f 1 (T 1 , T 2 ) + f 2 (T 1 , T 2 )(32c)
where T 1 and T 2 are the operating temperatures of the first and second reactor. The molar fraction of species i in first product stream,v 1 , is denoted by y i1 , and x i2 is the molar fraction of species i in the second product streaml 2 . The price of species i is represented by w i , and w c and w stm are the costs of the cooling and heating utilities respectively. The cost of the reagents supplied to the network is captured by the final term in (32a) where F i and C i0 are the volumetric flow rate and inlet concentration respectively of species i into the process. Figure 16: Schematic diagram of the serial CSTR reactor system and product recovery system</p>
<p>A.2 Reference Model</p>
<p>By substituting the Arrhenius expression (29) into the rate expressions (26), we can determine that reaction rates are functions of temperature and concentration; this is a major source of nonlinearity in the system. We draw inspiration from the use of inferential sensors that are used in industry to correlate the rates directly to temperature (bypassing concentrations) in order to develop a reference model. Specifically, in our reference model we develop a polynomial function that approximates the dependence of the rate on the temperature. We develop our polynomial model based on the following transformation of the rate expression:
log r = −E A R 1 T + n log C(33)
where we ignore the reverse reaction due to the comparatively small k r values used. From the mass balances, we can also determine that the concentration is an implicit function of the temperature. We choose to capture this relation using higher-order polynomials. During our analysis, we determined that a third-order polynomial provided satisfactory performance, resulting in the following approximation for the rate expression:
log r = θ 1 1 T + θ 2 1 T 2 + θ 3 1 T 3 + θ 0(34)
where θ 0 , θ 1 , θ 2 , and θ 3 are the model coefficients. Using (34), we can rewrite the material balances purely as functions of temperature and obtain the following expressions for the various species concentrations:
C A = C A in − 2(r 1 (T ) − r 4 (T )) V F (35a) C P = C P in + (r 1 (T ) − r 2 (T )) V F (35b)
C U = C U in + (2r 2 (T ) − r 3 (T ) − r 4 (T )) V F (35c)
C B = C B in − r 3 (T ) V F (35d) C E = C E in + r 3 (T ) V F (35e) C D = C D in − r 4 (T ) V F (35f)
we then substitute these values into a performance function similar to (32) to obtain the reference model for the system g. This reference model can be seen as an approximate physical model of the real system (captured by the exact model) and is much easier to evaluate. However, because this model is comprised of a complex set of algebraic equations, we further approximate the dependence of the performance function on the temperatures using a GP model.</p>
<p>Figure 1 :
1Schematic of proposed BO parallelization paradigms using level set partitioning (left) and variable partitioning (right).</p>
<p>Figure 2 :
2Workflow of a standard Bayesian optimization (S-BO) framework. A new experiment</p>
<p>Figure 3 :
3Reference-based BO decomposes the performance function f into a reference model g and a residual model ε.</p>
<p>Figure 5 :
5HS-BO partitions the domain X into K = 2 d subdomains and runs a separate instance of S-BO within each partition. A hyperparameter φ is introduced to define the degree of overlap in the partitions (the overlapping region aims to share information across subdomains). When φ = 0 there is no overlap between the partitions and when φ = 1 we have that all partitions are the entire domain X.Algorithm 4: HyperSpace Partitioning (HS-BO)</p>
<p>Figure 7
7shows the performance function f (T 1 , T 2 ) over the box domain T = [303, 423] 2 . The performance function is nonconvex and contains three minima, with local solutions at (T 1 , T 2 ) = (423, 340) and (T 1 , T 2 ) = (423, 423) and a global solution at (T 1 , T 2 ) = (333, 322).</p>
<p>Figure 7 :
7Performance function f of the reactor system (left) and reference model (right).</p>
<p>Figure 8 :
8Reference model g (left) and GP approximationĝ (right); note that the GP provides an accurate representation and can thus be used to guide partitioning approaches.</p>
<p>Figure 11 :
11Total experiment time against value of best solution for tested algorithms. LS-BO and VP-BO were run using the reference model to partition the domain and guide the search.</p>
<p>Figure 12 :
12Total experiment time against value of best solution for the tested algorithms. LS-BO and VP-BO were run using the reference model to partition the domain but not to guide the search.</p>
<p>Figure 12
12presents results similar to</p>
<p>11 and 12 provide a measure of robustness: deviations between the final reported average value and one of the three minima are due to the algorithm converging to different solutions during the various runs. For example, the final average reported value for S-BO of $394,500 USD/yr is the result of this algorithm converging to the minima at (T 1 , T 2 ) = (423, 340) 13 out of the 25 runs, (T 1 , T 2 ) = (423, 423) for 8 runs, and to (T 1 , T 2 ) = (333, 322) the remaining 4 runs. As a result, the fact that the final reported average values for LS-BO and VP-BO are near to the global minimum indicate that these algorithms converge to or near the global solution for most if not all runs (they are robust). The convergence data collected across all runs and shown in Figure 13 confirms this;</p>
<p>Figure 13 :
13Distribution of the performance profiles across the 25 runs for BO (top), LS-BO (middle), and VP-BO (bottom) with the average algorithm performance is shown in color.</p>
<p>Figure 14 :
14Experiment locations across the 25 runs for BO (top), LS-BO (middle), and VP-BO (bottom).
AcknowledgmentsWe acknowledge financial support via the NSF-EFRI award 2132036 and the Advanced Opportunity Fellowship from the University of Wisconsin-Madison Graduate Engineering Research Schol-
Multi-objective Bayesian optimization of ferroelectric materials with interfacial control for memory and energy storage applications. A Biswas, A N Morozovska, M Ziatdinov, E A Eliseev, S V Kalinin, Journal of Applied Physics. 13020A. Biswas, A. N. Morozovska, M. Ziatdinov, E. A. Eliseev, and S. V. Kalinin. Multi-objective Bayesian optimization of ferroelectric materials with interfacial control for memory and en- ergy storage applications. Journal of Applied Physics, 130(20):204102-1-204102-1, 2021.</p>
<p>Statistics for Experimenters: Design, Innovation, and Discovery. G E P Box, J S Hunter, W G Hunter, WileySecond editionG. E. P. Box, J. S. Hunter, and W. G. Hunter. Statistics for Experimenters: Design, Innovation, and Discovery. Wiley, Second edition, 2005.</p>
<p>On the experimental attainment of optimum conditions. G E P Box, K B Wilson, Journal of the Royal Statistical Society. 131G. E. P. Box and K. B. Wilson. On the experimental attainment of optimum conditions. Journal of the Royal Statistical Society, 13(1):1-38, 1951.</p>
<p>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. E Brochu, V M Cora, N. De Freitas, arXiv:1012.2599arXiv preprintE. Brochu, V. M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expen- sive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010.</p>
<p>Bayesian experimental design: A review. K Chaloner, I Verdinelli, Statistical Science. K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science, pages 273-304, 1995.</p>
<p>Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization. T Desautels, A Krause, J W Burdick, Journal of Machine Learning Research. 15119T. Desautels, A. Krause, and J. W. Burdick. Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization. Journal of Machine Learning Research, 15(119):4053- 4103, 2014.</p>
<p>Data-driven design and autonomous experimentation in soft and biological materials engineering. A L Ferguson, K A Brown, Annual Review of Chemical and Biomolecular Engineering. 132022A. L. Ferguson and K. A. Brown. Data-driven design and autonomous experimentation in soft and biological materials engineering. Annual Review of Chemical and Biomolecular Engi- neering, 13, 2022.</p>
<p>All models are wrong, but many are useful: Learning a variable's importance by studying an entire class of prediction models simultaneously. A Fisher, C Rudin, F Dominici, Journal of Machine Learning Research. 20177A. Fisher, C. Rudin, and F. Dominici. All models are wrong, but many are useful: Learn- ing a variable's importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177):1-81, 2019.</p>
<p>Bayesian Optimization. R Garnett, Cambridge University PressR. Garnett. Bayesian Optimization. Cambridge University Press, 2021.</p>
<p>Kriging is well-suited to parallelize optimization. D Ginsbourger, R Le Riche, L Carraro, Computation Intelligence in Expensive Optimization Problems. SpringerD. Ginsbourger, R. Le Riche, and L. Carraro. Kriging is well-suited to parallelize optimiza- tion. In Computation Intelligence in Expensive Optimization Problems, pages 131-162. Springer, 2010.</p>
<p>F Hutter, H H Hoos, K Leyton-Brown, International Conference on Learning and Intelligent Optimization. SpringerParallel algorithm configurationF. Hutter, H. H. Hoos, and K. Leyton-Brown. Parallel algorithm configuration. In International Conference on Learning and Intelligent Optimization, pages 55-70. Springer, 2012.</p>
<p>A taxonomy of global optimization methods based on response surfaces. D R Jones, Journal of Global Optimization. 214D. R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global Optimization, 21(4):345-383, 2001.</p>
<p>Multi-fidelity Bayesian optimisation with continuous approximations. K Kandasamy, G Dasarathy, J Schnieder, B Pózcos, Uncertainty in Artifical Intelligence. PMLRK. Kandasamy, G. Dasarathy, J. Schnieder, and B. Pózcos. Multi-fidelity Bayesian optimisa- tion with continuous approximations. In Uncertainty in Artifical Intelligence, pages 1799-1808. PMLR, 2017.</p>
<p>Bayesian optimization with reference models: A case study in MPC for HVAC central plants. Q Lu, L D González, R Kumar, V M Zavala, Computers &amp; Chemical Engineering. 154107491Q. Lu, L. D. González, R. Kumar, and V. M. Zavala. Bayesian optimization with reference models: A case study in MPC for HVAC central plants. Computers &amp; Chemical Engineering, 154:107491, 2021.</p>
<p>Differentiating the multipoint expected improvement for optimal batch design. S Marmin, C Chevalier, D Ginsbourger, Machine Learning, Optimization, and Big Data. S. Marmin, C. Chevalier, and D. Ginsbourger. Differentiating the multipoint expected im- provement for optimal batch design. In Machine Learning, Optimization, and Big Data. 2015.</p>
<p>The evolution of highthroughput experimentation in pharmaceutical development and perspectives on the future. S M Mennen, C Alhambra, C L Allen, M Barberis, S Berritt, T A Brandt, A D Campbell, J Castañón, A H Cherney, M Christensen, D B Damon, J Eugenio De Diego, S García-Cerrada, P García-Losada, R Haro, J Janey, D C Leitch, L Li, F Liu, P C Lobben, D W C Macmillan, J Magano, E Mcinturff, S Monfette, R J Post, D Schultz, B J Sitter, J M Stevens, I I Strambeanu, J Twilton, K Wang, M A Zajac, Organic Process Research &amp; Development. 236S. M. Mennen, C. Alhambra, C. L. Allen, M. Barberis, S. Berritt, T. A. Brandt, A. D. Campbell, J. Castañón, A. H. Cherney, M. Christensen, D. B. Damon, J. Eugenio de Diego, S. García- Cerrada, P. García-Losada, R. Haro, J. Janey, D. C. Leitch, L. Li, F. Liu, P. C. Lobben, D. W. C. MacMillan, J. Magano, E. McInturff, S. Monfette, R. J. Post, D. Schultz, B. J. Sitter, J. M. Stevens, I. I. Strambeanu, J. Twilton, K. Wang, and M. A. Zajac. The evolution of high- throughput experimentation in pharmaceutical development and perspectives on the future. Organic Process Research &amp; Development, 23(6):1213-1242, 2019.</p>
<p>Bayesian approach to global optimization: theory and applications. J Mockus, Springer Science &amp; Business Media37J. Mockus. Bayesian approach to global optimization: theory and applications, volume 37. Springer Science &amp; Business Media, 2012.</p>
<p>High-throughput experimentation and catalyst informatics for oxidative coupling of methane. T N Nguyen, T T P Nhat, K Takimoto, A Thakur, S Nishimura, J Ohyama, I Miyazato, L Takahashi, J Fujima, K Takahashi, T Taniike, ACS Catalysis. 102T. N. Nguyen, T. T. P. Nhat, K. Takimoto, A. Thakur, S. Nishimura, J. Ohyama, I. Miyazato, L. Takahashi, J. Fujima, K. Takahashi, and T. Taniike. High-throughput experimentation and catalyst informatics for oxidative coupling of methane. ACS Catalysis, 10(2):921-932, 2012.</p>
<p>COBALT: COnstrained Bayesian optimizAtion of computaionaLly expensive grey-box models exploiting derivaTive information. J A Paulson, C Lu, Computers &amp; Chemical Engineering. 160107700J. A. Paulson and C. Lu. COBALT: COnstrained Bayesian optimizAtion of computaionaLly expensive grey-box models exploiting derivaTive information. Computers &amp; Chemical Engi- neering, 160:107700, 2021.</p>
<p>Scikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 12F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pret- tenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Per- rot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.</p>
<p>Highthroughput automation in chemical process development. J A Selekman, J Qiu, K Tran, J Stevens, V Rosso, E Simmons, Y Xiao, J Janey, Annual Review of Chemical and Biomolecular Engineering. 8J. A. Selekman, J. Qiu, K. Tran, J. Stevens, V. Rosso, E. Simmons, Y. Xiao, and J. Janey. High- throughput automation in chemical process development. Annual Review of Chemical and Biomolecular Engineering, 8:525-547, 2017.</p>
<p>Taking the human out of the loop: A review of Bayesian optimization. B Shahriari, K Swersky, Z Wang, R P Adams, N De Freitas, Proceedings of the IEEE. 1041B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1):148-175, 2016.</p>
<p>Practical high-throughput experimentation for chemists. M Shevlin, ACS Medicinal Chemistry Letters. 86M. Shevlin. Practical high-throughput experimentation for chemists. ACS Medicinal Chemistry Letters, 8(6):601-607, 2017.</p>
<p>Decentralized schemes with overlap for solving graph-structured optimization problems. S Shin, V M Zavala, M Anitescu, IEEE Transactions on Control of Network Systems. 73S. Shin, V. M. Zavala, and M. Anitescu. Decentralized schemes with overlap for solving graph-structured optimization problems. IEEE Transactions on Control of Network Systems, 7(3):1225-1236, 2020.</p>
<p>Synthetic biology to access and expand nature's chemical diversity. M J Smanski, H Zhou, B S Claesen, M A Fischbach, C A Voigt, Nature Reviews Microbiology. 14M. J. Smanski, H. Zhou, B. S. Claesen, M. A. Fischbach, and C. A. Voigt. Synthetic biology to access and expand nature's chemical diversity. Nature Reviews Microbiology, 14:135-149, 2016.</p>
<p>Practical Bayesian optimization of machine learning algorithms. J Snoek, H Larochelle, R P Adams, Advances in Neural Information Processing Systems. Curran Associates, Inc25J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems, volume 25, pages 2951-2959. Curran Associates, Inc., 2012.</p>
<p>. J Snoek, H Larochelle, R P Adams, J. Snoek, H. Larochelle, and R. P. Adams. Spearmint. https://github.com/HIPS/ Spearmint, 2012.</p>
<p>Scalable Bayesian optimization using deep neural networks. J Snoek, O Rippel, K Swersky, R Kiros, N Satish, N Sundaram, M Patwary, M Prabhat, R Adams, International Conference on Machine Learning. PMLRJ. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, M. Prabhat, and R. Adams. Scalable Bayesian optimization using deep neural networks. In International Conference on Machine Learning, pages 2171-2180. PMLR, 2015.</p>
<p>Computationally efficient integrated design and predictive control of flexible energy systems using multi-fidelity simulation-based Bayesian optimization. F Sorourifar, N Choksi, J A Paulson, Optimal Control Applications and Methods. F. Sorourifar, N. Choksi, and J. A. Paulson. Computationally efficient integrated design and predictive control of flexible energy systems using multi-fidelity simulation-based Bayesian optimization. Optimal Control Applications and Methods, pages 1-28, 2021.</p>
<p>Gaussian processes for regression. C K I Williams, C E Rasmussen, Advances in Neural Information Processing Systems. MIT Press8C. K. I. Williams and C. E. Rasmussen. Gaussian processes for regression. In Advances in Neural Information Processing Systems, volume 8, pages 514-520. MIT Press, 1996.</p>
<p>J T Wilson, R Moriconi, F Hutter, M P Deisenroth, arXiv:1712.00424The reparameterization trick for acquisition functions. arXiv preprintJ. T. Wilson, R. Moriconi, F. Hutter, and M. P. Deisenroth. The reparameterization trick for acquisition functions. arXiv preprint arXiv:1712.00424, 2017.</p>
<p>Practical multi-fidelity Bayesian optimization for hyperaparameter tuning. J Wu, S Toscano-Palmerin, P I Frazier, A G Wilson, Uncertainty in Artifical Intelligence. PMLRJ. Wu, S. Toscano-Palmerin, P. I. Frazier, and A. G. Wilson. Practical multi-fidelity Bayesian optimization for hyperaparameter tuning. In Uncertainty in Artifical Intelligence, pages 788- 798. PMLR, 2020.</p>
<p>Hyperspace: Distributed Bayesian hyperparameter optimization. M T Young, J Hinkle, A Ramanathan, R Kannan, 30th International Symposium on Computer Architecture and High Performance Computing. IEEEM. T. Young, J. Hinkle, A. Ramanathan, and R. Kannan. Hyperspace: Distributed Bayesian hyperparameter optimization. In 2018 30th International Symposium on Computer Architecture and High Performance Computing, pages 339-347. IEEE, 2018.</p>
<p>Distributed Bayesian optimization of reinforcement learning algorithms. M T Young, J D Hinkle, R Kannan, A Ramanathan, Journal of the Parallel and Distributed Computing. 1391M. T. Young, J. D. Hinkle, R. Kannan, and A. Ramanathan. Distributed Bayesian optimiza- tion of reinforcement learning algorithms. Journal of the Parallel and Distributed Computing, 139(1):43-52, 2020.</p>
<p>Sparse principal component analysis. H Zou, T Hastie, R Tibshirani, Journal of Computational and Graphical Statistics. 152H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of Compu- tational and Graphical Statistics, 15(2):265-286, 2006.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1931 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1931</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1931</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-278394531</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.04769v1.pdf" target="_blank">Vision-Language-Action Models: Concepts, Progress, Applications and Challenges</a></p>
                <p><strong>Paper Abstract:</strong> Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence.>Vision-language-action, Agentic AI, AI Agents, Vision-language Models</p>
                <p><strong>Cost:</strong> 0.031</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1931.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1931.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-conditioned visuomotor policy that uses pretrained CLIP image-text embeddings to semantically condition pixel-level manipulation policies, bypassing explicit language parsing for direct instruction-conditioned control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cliport: What and where pathways for robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses CLIP vision-language embeddings (frozen) as the semantic input, coupled with a convolutional/pixel-decoder transport-style policy to predict spatiality-aware manipulation actions (pixel affordance maps → pick/place). Processes RGB (image) and natural language instruction; action outputs are spatial pick/place coordinates (discrete pixel grid) suitable for tabletop manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language on image-text pairs (CLIP-style contrastive pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on large image-caption corpora (CLIP family): contains object descriptions, visual attributes and common relations; semantic priors about objects and attributes but no robot trajectories or action primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>pixel-level robotic manipulation / pick-and-place</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tabletop manipulation tasks (object pick and place, object rearrangement) using pixel affordance maps as action space (discrete pixel coordinates converted to robot end-effector poses); evaluated in real or simulated tabletop settings with object clutter.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper highlights strong semantic alignment between CLIP embeddings and language task descriptors (color, object type, spatial phrases), enabling direct conditioning of policies; overlap is high for common objects/attributes but limited for fine-grained physical affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Qualitative/benchmark claims: enables language-conditioned pixel-level manipulation and improved generalization to instructive commands; no single numeric success rate for CLIPort in this review beyond description.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not quantified for CLIPort here; presented as enabling bypass of explicit parsing and reducing need for hand-labeled scene descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported for CLIPort specifically in this review; described as using CLIP embeddings as semantic grounding rather than attention visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Authors note semantic grounding via CLIP embedding alignment but no detailed clustering/PCA analyses provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Evidence is functional: CLIP embeddings used to condition visuomotor decoder yields successful manipulation conditioned on language, cited as semantic grounding mechanism (no mechanistic probe data included).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed in depth for CLIPort in this review; CLIP embeddings provide higher-level semantics while the decoder handles spatial details.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best when instruction vocabulary and object categories overlap with CLIP pretraining; limited for fine-grained affordances not present in image-text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Review states improved generalization to unseen objects via semantic priors but provides no numeric split for CLIPort.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>CLIP-conditioned policies enable better zero/few-shot instruction generalization qualitatively; no numeric zero-shot success rates provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-level ablations for CLIPort reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer reported for CLIPort in this review; grounding limits for physical affordances are noted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>CLIP-based vision-language conditioning is presented as superior to pure vision-only pixel policies for instruction following; explicit quantitative comparisons not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed for CLIPort here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1931.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1931.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based VLM policy that jointly processes object-centric visual tokens and instruction tokens to enable few-shot generalization across spatial reasoning manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VIMA: General robot manipulation with multimodal prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision encoder (ViT + Mask R-CNN for object-centric tokens) + T5 language encoder fused in a transformer policy / autoregressive decoder to output action tokens (discrete motor primitives). Uses multimodal prompting to condition on instructions and object references.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining for visual/language encoders (standard VLM pretraining) followed by policy fine-tuning on robot demonstration data (task-specific imitation)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Vision-language datasets for semantic priors (image-text pairs) + VIMA-Data (self-collected multimodal datasets of manipulation tasks) containing object labels, spatial relations and action labels enabling mapping from language to object-centric tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>spatial reasoning and compositional manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-object, spatially complex tabletop manipulation tasks (e.g., arrange/stack/relocate) with discrete action primitives; demonstrated few-shot generalization across spatial compositions in simulated and real setups.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Explicit: object-centric tokens + language tokens fused—paper emphasizes preserved CLIP-like semantic alignment enabling resolution of spatial references (e.g., 'left of', 'on top').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported as enabling few-shot generalization across spatial reasoning tasks; review reports qualitative success and few-shot capability but no single numeric aggregate in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>VIMA described as achieving few-shot generalization; exact sample efficiency numbers not given in review.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Transforms attend across object tokens and instruction tokens; review emphasizes cross-attention as the fusion mechanism but provides no visualization metrics for VIMA specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Object-centric tokenization argued to produce disentangled object representations aiding compositional generalization; no numeric embedding analysis provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Functional grounding: mapping language tokens to object-centric visual tokens to produce correct action primitives; evidence is behavioral (few-shot generalization) rather than mechanistic probing.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Architecture uses object-level (mid/high-level) tokens fused with language; review suggests high-level semantics benefit most from language pretraining while low-level control is learned during imitation fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performs well when object-centric visual encodings and instruction vocabulary overlap with fine-tuning data; struggles under heavy occlusion unless augmented.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Reported few-shot capability to novel spatial compositions; no quantified novel vs familiar object success rates reported in review.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Explicit few-shot generalization reported (qualitative/benchmark-level) but no exact sample counts provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer ablation details reported in the review for VIMA.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>VIMA's multimodal conditioning described as superior to vision-only policies for instruction following; no explicit numeric comparison in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Temporal aspects mentioned (handling sequences) but no training phase dynamics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1931.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1931.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Robotics Transformer 1 — an early large-scale supervised visuomotor model trained on a large corpus of demonstrations (~130k) to map images and language instructions to robot actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RT-1: Robotics transformer for real-world control at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language-conditioned transformer policy that maps RGB observations and textual instructions directly to discretized action outputs, trained primarily via behavior cloning on large-scale human-collected demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>primarily supervised on robot demonstration data; vision and language encoders may leverage pretrained components but core mapping learned from demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained on ~130,000 demonstrations capturing image-action pairs with natural language annotations; demonstration data includes object manipulation trajectories and instruction labels (affordance and action descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>real-world kitchen and household manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task real-world manipulation (kitchen objects): discrete or discretized continuous action outputs tailored to a real robot arm; tasks drawn from everyday manipulations under realistic sensor noise.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High alignment because dataset pairs language instructions directly with visual frames and actions; provides strong grounding of words to motor outcomes for in-domain objects and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>RT-1 shown to enable robust real-world control across many tasks due to large demonstration corpus; exact success rates not restated in review besides being foundational.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not quantified here, but RT-1's large demonstration corpus is presented as critical for performance; no explicit comparison numbers to non-language pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No detailed attention visualization presented in the review for RT-1.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not analyzed in detail in review; behaviorally indicates mappings from language to action are learned.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Dataset pairing of language+image+trajectory provides direct evidence of grounding (supervised mapping), enabling generalization to in-domain variations.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not explicitly analyzed; policy is end-to-end transformer mapping perception+language to actions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Large in-domain demonstration diversity supports transfer within household/kitchen domain; cross-domain transfer limited unless co-fine-tuned with web-scale VLM data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>RT-1 demonstrates generalization within nearby object families in its dataset; no explicit novel vs familiar numbers in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not emphasized as zero-shot; relies on large supervised demos for breadth.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer probing reported in review.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared in review; RT-1 leverages language annotations making it stronger for instruction following than vision-only imitation baselines generally.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Trained on trajectory sequences; temporal autoregressive decoding used but no dynamics-of-training analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1931.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1931.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (Robotic Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA model that treats action generation as text generation by tokenizing actions; co-trains on web-scale vision-language data and thousands of robot demonstrations to enable zero-shot generalization to unseen instructions and objects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RT-2: Vision-language-action models transfer web knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified transformer taking vision and language tokens as prefix and autoregressively generating action tokens (action discretized via DCT + BPE). Pretrained/fine-tuned on both web-scale VLM corpora and robot demonstration trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multistage: vision-language pretraining on internet-scale image-text corpora + supervised fine-tuning on robot demonstration datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Web-scale multimodal corpora (image-text pairs) impart semantic priors; thousands of robot demonstrations with trajectory/action labels provide grounded action mappings. Action tokenization includes DCT compression and BPE-style tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>zero-shot/transfer robotic manipulation (novel object instruction following)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Manipulation tasks where the model must interpret novel instructions and produce motor command sequences; action space is discretized token stream representing motor trajectories (continuous actions compressed into tokens). Evaluated on novel object generalization tests.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Explicitly measured: co-fine-tuning aligns web semantic priors with robotic action spaces, improving semantic grounding for unseen object references.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported: '63% improvement in performance on novel objects' (metric: improvement in performance relative to baseline when using web-scale multimodal pretraining + robot demos).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not numeric in review for RT-2; improved zero-shot/generalization attributed to web-scale semantic pretraining reducing need for per-task demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Architecture uses fused visual-language tokens with cross-attention; review mentions chain-of-thought-style visual reasoning in RT-2 but no explicit attention maps provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>RT-2 described as unifying vision-language and action tokens into shared embedding, enabling transfer; no explicit clustering analyses given.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Functional evidence: co-training and discrete action tokenization produced large gains on novel-object tasks (63% improvement), indicating semantic-language priors grounded into motor outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed layer-wise here; RT-2 treats entire pipeline as unified autoregressive model combining semantic and motor tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer benefits when semantic concepts present in web pretraining map to object affordances in robot demos; success diminishes when affordances not represented in pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Explicit metric: 63% improvement on novel objects with language/web pretraining vs baseline (baseline unspecified here).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>RT-2 supports zero-shot generalization to unseen instructions/objects; quantitative zero-shot success rates not further detailed beyond the 63% improvement statement.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer ablation/probing reported in review for RT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>None reported in review; overall improvement emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>RT-2's co-fine-tuning on web vision-language corpora is highlighted as providing substantial gains over models trained only on robot demos; precise numeric comparator not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Action tokenization via DCT compresses trajectories temporally; review does not quantify training-phase dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1931.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1931.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B-parameter vision-language-action model co-fine-tuned on nearly one million real robot demonstrations that achieves strong real-world generalization with parameter-efficient methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenVLA: An open-source vision-language-action model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter VLA combining dual vision encoders (DINOv2, SigLIP) with a Llama-2 language backbone and an autoregressive action decoder; trained via co-fine-tuning on large vision-language corpora and ~970k real-world robot demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>co-fine-tuning: vision-language pretraining (LAION-scale data) + large-scale robot demonstration fine-tuning (behavioral cloning)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>LAION-5B style web image-text corpora yield semantic priors; RT-X/robot trajectory datasets (~970k demonstrations) provide action grounding including object manipulations and spatial relations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>generalist robotic manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-task real-world robotic manipulation across many tasks collected in demonstration dataset; action outputs are autoregressive tokens mapping to motor commands; evaluated for success rate on held-out tasks and novel objects.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Explicit: co-fine-tuning aligns web-scale semantic priors with robot trajectories; review states this alignment is core to OpenVLA's generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported: 'OpenVLA (7B) achieves a 16.5% higher success rate than a 55B-parameter RT-2 variant' on reported benchmarks (metric: success rate).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>OpenVLA uses LoRA adapters and parameter-efficient methods to adapt with much less compute; explicit demonstration-sample comparisons not provided, but LoRA reduced GPU hours by ~70% in general examples.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Review notes cross-attention fusion and preservation of CLIP-like alignment but does not present attention visualizations for OpenVLA specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Co-fine-tuning reported to bring semantic and motor spaces into alignment; explicit embedding-space probes not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Empirical evidence: higher success rates on held-out tasks, indicating successful grounding of language semantics into action policies via joint training on web VLM + robot demos.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not explicitly layer-probed; architecture benefits from frozen pretrained vision encoders and a trainable policy head.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performs well with large, diverse demonstration corpora and preserved VLM encoders; out-of-distribution objects still challenging but improved over alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Quantified indirectly via higher success on benchmarks vs larger RT-2-X model, suggesting better transfer; explicit per-category novel vs familiar numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Demonstrates improved zero/few-shot generalization behaviorally; exact counts not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>LoRA-based adaptation used (adapter injection) but no detailed frozen-vs-finetuned ablation numbers beyond computational savings.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported; OpenVLA generally presented as improving generalization while being parameter-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Co-fine-tuning with language data emphasized as key advantage vs purely robot-demo-trained models; explicit vision-only comparison numbers not shown here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not directly analyzed in review for OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1931.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1931.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Octo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Octo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source generalist robot policy (93M parameters) using diffusion decoders trained on a very large OpenX-Embodiment demonstration dataset to scale embodied policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Octo: An opensource generalist robot policy.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Octo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small-scale generalist VLA (93M) with diffusion-based action decoders and CNN vision encoder, trained on the Open X-Embodiment dataset (800k demonstrations) to produce stochastic, diverse trajectories for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>pretrained vision encoder (standard) + large-scale supervised imitation from multi-robot demonstration dataset; diffusion policy pretraining on action sequences</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Open X-Embodiment: 800,000 robot demonstrations from multi-robot sources covering diverse morphologies and manipulation tasks; contains action trajectories, object interactions, and instruction annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>generalist manipulation across many robot types</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Many manipulation tasks across different embodiments; action decoder generates trajectories via diffusion (continuous action space represented stochastically); tested for scalability and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Trained on robot trajectories with instruction annotations to ground language to actions; review highlights diffusion decoding for multimodal action distributions rather than explicit large-scale VLM alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Octo presented as broadly generalizable and dataset-scaled; review gives architecture/training scale but not specific numeric success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not explicitly quantified; diffusion policies are noted to require more compute but modeled as more expressive (no sample numbers given).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported for Octo specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral: trained on diverse demonstration set yielding robust long-horizon decision-making; diffusion decoder captures multiple viable action modes.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Diffusion decoder provides expressive low-level action modes; high-level semantics depend on instruction annotations in dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Scale of OpenX-Embodiment data and diffusion sampling enables transfer across robot types; success depends on demonstration diversity and similarity to target embodiment.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Review emphasizes generalist capabilities but quantification not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative transfer reported; diffusion policies noted to be computationally heavy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Emphasis is on large-scale robot trajectory training rather than vision-only pretraining comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Diffusion policies model temporal multimodality; no training dynamics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1931.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1931.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion Policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion Policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of policies using diffusion models to produce diverse, smooth multimodal action distributions for visuomotor control, improving representation of multiple valid strategies at the cost of computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diffusion policy: Visuomotor policy learning via action diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diffusion Policy</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses diffusion-based generative modeling for action sequence generation conditioned on visual (and sometimes language) context; models continuous trajectories as samples from learned diffusion process.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>trained on robot trajectory datasets (imitation) with diffusion modeling of actions; vision encoders often pretrained separately (vision-language or vision-only)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large collections of demonstration trajectories with visual context and instruction labels; contains multi-modal possible actions for similar inputs (supports multimodal action distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>dexterous and multimodal manipulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Contact-rich manipulation and tasks with multiple valid action strategies; actions are continuous trajectories sampled from diffusion model, typically in simulation and transferred to real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Enables richer action representations which can be conditioned on language; review states diffusion policies better capture multimodal action possibilities and thereby improve alignment between language-specified goals and motor outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Diffusion policies reported to improve diversity and robustness of generated actions; review notes improved success in complex tasks but also higher compute (rough relative statements, not absolute metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not numerically given here; diffusion policies are computationally heavier (~3×) but produce better multimodal coverage which can reduce failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not discussed specifically for diffusion policies in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Argued qualitatively: diffusion captures multiple plausible motor patterns corresponding to the same high-level instruction, supporting better grounding of verbs to affordance-contingent motor outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Diffusion used as low-level controller within hierarchical VLA systems; review suggests low-level action diversity benefits from diffusion while high-level planning uses LLM-style modules.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Compute and latency constraints limit real-time use; transfer success depends on ability to compress/accelerate sampling for deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported numerically here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not stated.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>High compute of diffusion methods can limit real-time applicability (practical negative effect).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Diffusion concerns action generation rather than vision pretraining comparison; stronger multimodal action modeling than simple autoregressive decoders is emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Diffusion models represent temporal multimodality natively; no training phase time-course statistics given.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1931.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1931.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pi-0 (Pi-Zero)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pi-0</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA model using diffusion-based policies (Pi-0) with a 'fast' variant that compresses action tokens for high-frequency control, trading some trajectory granularity for speed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pi-0: A vision-language-action flow model for general robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pi-0 / Pi-0 Fast</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diffusion-based low-level action policy conditioned on fused vision-language-state tokens; Pi-0-Fast uses compressed action tokenization (frequency-domain FAST tokens) to enable high control rates (e.g., 200 Hz) with minimal accuracy loss.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>imitation learning from robot demonstrations with diffusion action modeling; vision-language encoders may be pretrained separately.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Robot trajectory datasets with language annotations; includes temporally dense action sequences suitable for DCT/frequency compression approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>high-frequency control for dexterous manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Contact-rich, high-bandwidth manipulation tasks requiring sub-second action updates; continuous action trajectories compressed into tokens for fast decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Tokenization preserves semantic mapping from instruction to action while enabling compressed decoding; review emphasizes preservation of semantic fidelity with Pi-0 Fast.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Pi-0 Fast variant described as delivering continuous 200 Hz control with negligible accuracy loss using small adapter parameterizations; no absolute success rate numbers given here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not quantified numerically in review; Pi-0 variants presented as computational vs expressivity trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided for Pi-0 specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Functional: diffusion policies model multimodal action distributions enabling varied grasps/trajectories that better match language-specified goals; no mechanistic grounding probes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Pi-0 used as low-level controller within hierarchical VLA stacks; high-level planners remain separate.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>High-frequency decoding requires compressed tokenization and hardware optimizations to transfer to embedded robots.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Higher computational cost is a practical limitation for real-time use.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable; comparisons are between diffusion vs autoregressive action decoders rather than vision-only pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Pi-0-Fast leverages frequency compression (DCT) to represent temporal windows as fewer tokens enabling high update rates.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1931.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1931.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive vision-language-action dataset and model work for autonomous driving that pairs hours of driving video, LiDAR/odometry, language annotations and trajectories to train VLA models for driving tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Covla: Comprehensive vision-language-action dataset for autonomous driving.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset and modeling framework that uses CLIP for visual grounding and LLaMA-2 for language embedding, with trajectory decoders for driving action prediction; integrates multi-sensor streams (camera, LiDAR, odometry).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>dataset-driven supervised training using paired driving videos, sensor streams and natural language annotations; vision encoders often pretrained on image-text corpora</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Over 80 hours of driving videos with synchronized sensor streams and rich natural language annotations about traffic situations, objects, and navigation directives — contains dynamic behaviors, affordance-relevant events and action labels.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>autonomous driving: instruction-conditioned driving and decision making</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Urban driving tasks: trajectory prediction, instruction-following ('yield to ambulance'), and scene understanding; action space continuous (steering, throttle, brake) with world-relative and ego-centric representations in real-world driving data.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>High: language annotations directly describe driving goals and situations; CLIP+LLaMA encoders provide semantic grounding for driving cues and instruction resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Dataset enables training models that align perception and language for driving; review does not state single numeric success rate but highlights interpretability and improved planning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not provided numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here for CoVLA models specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Explicit dataset pairing of language descriptions to driving trajectories provides direct evidence for language-to-action grounding in driving domain.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>OpenDriveVLA and ORION build on CoVLA-like data to provide hierarchical 2D/3D alignment and long-horizon planning; CoVLA provides the grounded dataset backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer hinges on sensor modality match (LiDAR/cameras) and similarity of driving scenarios; domain shift (different cities/traffic) will affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not quantified in review.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not explicitly reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>In- review argues multimodal (vision+language) training yields richer, more interpretable driving policies than purely perception-to-control pipelines, though no numeric comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Driving datasets provide long-horizon temporal context used by QT-Former style history encoders in downstream models (ORION); detailed training dynamics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1931.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1931.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PointVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PointVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that injects 3D point cloud features into frozen pretrained VLA backbones via modular skip-blocks to enable 3D spatial reasoning and few-shot, long-horizon control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pointvla: Injecting the 3d world into vision-language-action models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PointVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Augments frozen 2D VLA backbones with 3D point-cloud features via modular skip connections, allowing models to preserve learned 2D semantic knowledge while integrating 3D spatial geometry for manipulation and navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>foundation VLA pretraining on 2D vision-language corpora and robot demos, with additional 3D point-cloud modules trained/fine-tuned on 3D datasets or inserted modularly without retraining the full backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>2D image-text/web corpora provide semantics; point-cloud datasets yield geometric/affordance information (3D positions, shapes, spatial relations); training aims to preserve 2D semantics while adding 3D spatial priors.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>3D-aware robotic manipulation and spatial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tasks requiring explicit 3D reasoning (depth, occlusion resolution, grasp points) across few-shot and long-horizon scenarios; action space continuous/parametric for manipulation in 3D workspaces.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed to align 2D learned semantics with 3D geometry; review claims strong 3D spatial reasoning and few-shot capability while preserving 2D semantic knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported as enabling strong few-shot and dynamic task performance with no retraining of the 2D backbone; review gives qualitative claims of strong performance but no numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Modular insertion allows few-shot adaptation to 3D tasks; no quantitative sample counts provided in review.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided specifically; modular skip-blocks imply cross-modal attention between 2D/3D features but no attention visualizations presented.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Claimed to preserve 2D attractor semantics while adding geometric features, but no explicit embedding analyses presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral evidence: improved spatial reasoning and manipulation in 3D tasks when 3D features integrated; direct mechanistic grounding probes not supplied.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Integration shows high-level semantics (2D) preserved while low/mid-level geometric features added for better physical interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works best when 3D sensors are available and target embodiment requires precise spatial reasoning; modular design helps cross-embodiment transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot capability emphasized qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Modular insertion avoids full backbone retraining; layer-level importance not numerically analyzed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>PointVLA improves over pure 2D VLA on 3D tasks by injecting geometric features; review reports qualitative superiority without numerical comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1931.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1931.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DexVLA / DexGraspVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DexVLA / DexGraspVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>VLA variants focused on general dexterous grasping and dexterous manipulation using hierarchical planners combined with diffusion-based low-level controllers to achieve robust grasping across diverse objects and conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dexgraspvla: A vision-language-action framework towards general dexterous grasping.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DexVLA / DexGraspVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hierarchical architecture: pretrained vision-language planner produces goals and affordance points; diffusion-based low-level controller generates smooth, compliant joint trajectories for dexterous hands. Trained on diverse demonstration sets and uses iterative domain-invariant representations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining for semantic priors + imitation learning and diffusion policy training on grasping datasets; iterative domain-invariant pretraining to support zero-shot transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale grasp/dexterity datasets across lighting/background variations, synthetic augmentation and real-world demos; includes object affordance labels and action trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>dexterous grasping and manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Fine-grained grasping with multi-fingered hands across varied object geometries and visual conditions; continuous high-precision joint trajectories executed in real-world settings.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Pretraining on vision-language and affordance labels aligns nouns and adjectives to grasp affordances; iterative domain-invariant methods aim to preserve this mapping across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Review states '90%+ success on unseen scenarios' for DexGraspVLA (claim in review) indicating strong zero-shot generalization for grasping across varied conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Iterative domain-invariant representation and pretraining reduce reliance on task-specific demos; no exact sample counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided specifically; architecture uses affordance prediction modules which likely focus on graspable regions but no attention maps provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Iterative domain-invariant representations claimed to support cross-domain robustness; no explicit embedding metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Reported behavioral evidence: >90% success on unseen scenarios indicates effective grounding of action semantics (grasp) to visual affordances and motor patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>High-level planner (VLM) benefits from language pretraining for goal/affordance selection; low-level diffusion controller supplies trajectory precision.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performs well under domain shifts when domain-invariant pretraining and synthetic augmentation used; sensitive to sensor modality mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Reported high success on unseen scenarios (90%+), indicating small gap for novel objects in tests described by review.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Explicit strong zero-shot generalization claimed for dexterous grasping benchmarks (90%+ success on unseen scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Language/affordance pretraining combined with diffusion controllers is claimed superior to vision-only approaches for zero-shot grasp generalization; exact numeric baseline comparisons not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Low-level diffusion used for smooth temporal trajectory sampling; no training-phase curves reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics transformer for real-world control at scale. <em>(Rating: 2)</em></li>
                <li>Cliport: What and where pathways for robotic manipulation. <em>(Rating: 2)</em></li>
                <li>VIMA: General robot manipulation with multimodal prompts. <em>(Rating: 2)</em></li>
                <li>Diffusion policy: Visuomotor policy learning via action diffusion. <em>(Rating: 2)</em></li>
                <li>Octo: An opensource generalist robot policy. <em>(Rating: 2)</em></li>
                <li>OpenVLA: An open-source vision-language-action model. <em>(Rating: 2)</em></li>
                <li>Pointvla: Injecting the 3d world into vision-language-action models. <em>(Rating: 2)</em></li>
                <li>Dexgraspvla: A vision-language-action framework towards general dexterous grasping. <em>(Rating: 2)</em></li>
                <li>Covla: Comprehensive vision-language-action dataset for autonomous driving. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1931",
    "paper_id": "paper-278394531",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "CLIPort",
            "name_full": "CLIPort",
            "brief_description": "A vision-language-conditioned visuomotor policy that uses pretrained CLIP image-text embeddings to semantically condition pixel-level manipulation policies, bypassing explicit language parsing for direct instruction-conditioned control.",
            "citation_title": "Cliport: What and where pathways for robotic manipulation.",
            "mention_or_use": "mention",
            "model_name": "CLIPort",
            "model_description": "Uses CLIP vision-language embeddings (frozen) as the semantic input, coupled with a convolutional/pixel-decoder transport-style policy to predict spatiality-aware manipulation actions (pixel affordance maps → pick/place). Processes RGB (image) and natural language instruction; action outputs are spatial pick/place coordinates (discrete pixel grid) suitable for tabletop manipulation.",
            "pretraining_type": "vision-language on image-text pairs (CLIP-style contrastive pretraining)",
            "pretraining_data_description": "Pretrained on large image-caption corpora (CLIP family): contains object descriptions, visual attributes and common relations; semantic priors about objects and attributes but no robot trajectories or action primitives.",
            "target_task_name": "pixel-level robotic manipulation / pick-and-place",
            "target_task_description": "Tabletop manipulation tasks (object pick and place, object rearrangement) using pixel affordance maps as action space (discrete pixel coordinates converted to robot end-effector poses); evaluated in real or simulated tabletop settings with object clutter.",
            "semantic_alignment": "Paper highlights strong semantic alignment between CLIP embeddings and language task descriptors (color, object type, spatial phrases), enabling direct conditioning of policies; overlap is high for common objects/attributes but limited for fine-grained physical affordances.",
            "performance_with_language_pretraining": "Qualitative/benchmark claims: enables language-conditioned pixel-level manipulation and improved generalization to instructive commands; no single numeric success rate for CLIPort in this review beyond description.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Not quantified for CLIPort here; presented as enabling bypass of explicit parsing and reducing need for hand-labeled scene descriptors.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported for CLIPort specifically in this review; described as using CLIP embeddings as semantic grounding rather than attention visualizations.",
            "embedding_space_analysis": "Authors note semantic grounding via CLIP embedding alignment but no detailed clustering/PCA analyses provided here.",
            "action_grounding_evidence": "Evidence is functional: CLIP embeddings used to condition visuomotor decoder yields successful manipulation conditioned on language, cited as semantic grounding mechanism (no mechanistic probe data included).",
            "hierarchical_features_evidence": "Not analyzed in depth for CLIPort in this review; CLIP embeddings provide higher-level semantics while the decoder handles spatial details.",
            "transfer_conditions": "Works best when instruction vocabulary and object categories overlap with CLIP pretraining; limited for fine-grained affordances not present in image-text corpora.",
            "novel_vs_familiar_objects": "Review states improved generalization to unseen objects via semantic priors but provides no numeric split for CLIPort.",
            "zero_shot_or_few_shot": "CLIP-conditioned policies enable better zero/few-shot instruction generalization qualitatively; no numeric zero-shot success rates provided here.",
            "layer_analysis": "No layer-level ablations for CLIPort reported in this review.",
            "negative_transfer_evidence": "No explicit negative transfer reported for CLIPort in this review; grounding limits for physical affordances are noted.",
            "comparison_to_vision_only": "CLIP-based vision-language conditioning is presented as superior to pure vision-only pixel policies for instruction following; explicit quantitative comparisons not provided in this review.",
            "temporal_dynamics": "Not discussed for CLIPort here.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1931.0"
        },
        {
            "name_short": "VIMA",
            "name_full": "VIMA",
            "brief_description": "A transformer-based VLM policy that jointly processes object-centric visual tokens and instruction tokens to enable few-shot generalization across spatial reasoning manipulation tasks.",
            "citation_title": "VIMA: General robot manipulation with multimodal prompts.",
            "mention_or_use": "mention",
            "model_name": "VIMA",
            "model_description": "Vision encoder (ViT + Mask R-CNN for object-centric tokens) + T5 language encoder fused in a transformer policy / autoregressive decoder to output action tokens (discrete motor primitives). Uses multimodal prompting to condition on instructions and object references.",
            "pretraining_type": "vision-language pretraining for visual/language encoders (standard VLM pretraining) followed by policy fine-tuning on robot demonstration data (task-specific imitation)",
            "pretraining_data_description": "Vision-language datasets for semantic priors (image-text pairs) + VIMA-Data (self-collected multimodal datasets of manipulation tasks) containing object labels, spatial relations and action labels enabling mapping from language to object-centric tokens.",
            "target_task_name": "spatial reasoning and compositional manipulation",
            "target_task_description": "Multi-object, spatially complex tabletop manipulation tasks (e.g., arrange/stack/relocate) with discrete action primitives; demonstrated few-shot generalization across spatial compositions in simulated and real setups.",
            "semantic_alignment": "Explicit: object-centric tokens + language tokens fused—paper emphasizes preserved CLIP-like semantic alignment enabling resolution of spatial references (e.g., 'left of', 'on top').",
            "performance_with_language_pretraining": "Reported as enabling few-shot generalization across spatial reasoning tasks; review reports qualitative success and few-shot capability but no single numeric aggregate in this review.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "VIMA described as achieving few-shot generalization; exact sample efficiency numbers not given in review.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Transforms attend across object tokens and instruction tokens; review emphasizes cross-attention as the fusion mechanism but provides no visualization metrics for VIMA specifically.",
            "embedding_space_analysis": "Object-centric tokenization argued to produce disentangled object representations aiding compositional generalization; no numeric embedding analysis provided here.",
            "action_grounding_evidence": "Functional grounding: mapping language tokens to object-centric visual tokens to produce correct action primitives; evidence is behavioral (few-shot generalization) rather than mechanistic probing.",
            "hierarchical_features_evidence": "Architecture uses object-level (mid/high-level) tokens fused with language; review suggests high-level semantics benefit most from language pretraining while low-level control is learned during imitation fine-tuning.",
            "transfer_conditions": "Performs well when object-centric visual encodings and instruction vocabulary overlap with fine-tuning data; struggles under heavy occlusion unless augmented.",
            "novel_vs_familiar_objects": "Reported few-shot capability to novel spatial compositions; no quantified novel vs familiar object success rates reported in review.",
            "zero_shot_or_few_shot": "Explicit few-shot generalization reported (qualitative/benchmark-level) but no exact sample counts provided here.",
            "layer_analysis": "No layer ablation details reported in the review for VIMA.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "VIMA's multimodal conditioning described as superior to vision-only policies for instruction following; no explicit numeric comparison in the review.",
            "temporal_dynamics": "Temporal aspects mentioned (handling sequences) but no training phase dynamics reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1931.1"
        },
        {
            "name_short": "RT-1",
            "name_full": "RT-1",
            "brief_description": "Robotics Transformer 1 — an early large-scale supervised visuomotor model trained on a large corpus of demonstrations (~130k) to map images and language instructions to robot actions.",
            "citation_title": "RT-1: Robotics transformer for real-world control at scale.",
            "mention_or_use": "mention",
            "model_name": "RT-1",
            "model_description": "Vision-language-conditioned transformer policy that maps RGB observations and textual instructions directly to discretized action outputs, trained primarily via behavior cloning on large-scale human-collected demonstrations.",
            "pretraining_type": "primarily supervised on robot demonstration data; vision and language encoders may leverage pretrained components but core mapping learned from demonstrations",
            "pretraining_data_description": "Trained on ~130,000 demonstrations capturing image-action pairs with natural language annotations; demonstration data includes object manipulation trajectories and instruction labels (affordance and action descriptions).",
            "target_task_name": "real-world kitchen and household manipulation",
            "target_task_description": "Multi-task real-world manipulation (kitchen objects): discrete or discretized continuous action outputs tailored to a real robot arm; tasks drawn from everyday manipulations under realistic sensor noise.",
            "semantic_alignment": "High alignment because dataset pairs language instructions directly with visual frames and actions; provides strong grounding of words to motor outcomes for in-domain objects and tasks.",
            "performance_with_language_pretraining": "RT-1 shown to enable robust real-world control across many tasks due to large demonstration corpus; exact success rates not restated in review besides being foundational.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Not quantified here, but RT-1's large demonstration corpus is presented as critical for performance; no explicit comparison numbers to non-language pretraining.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No detailed attention visualization presented in the review for RT-1.",
            "embedding_space_analysis": "Not analyzed in detail in review; behaviorally indicates mappings from language to action are learned.",
            "action_grounding_evidence": "Dataset pairing of language+image+trajectory provides direct evidence of grounding (supervised mapping), enabling generalization to in-domain variations.",
            "hierarchical_features_evidence": "Not explicitly analyzed; policy is end-to-end transformer mapping perception+language to actions.",
            "transfer_conditions": "Large in-domain demonstration diversity supports transfer within household/kitchen domain; cross-domain transfer limited unless co-fine-tuned with web-scale VLM data.",
            "novel_vs_familiar_objects": "RT-1 demonstrates generalization within nearby object families in its dataset; no explicit novel vs familiar numbers in this review.",
            "zero_shot_or_few_shot": "Not emphasized as zero-shot; relies on large supervised demos for breadth.",
            "layer_analysis": "No layer probing reported in review.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not directly compared in review; RT-1 leverages language annotations making it stronger for instruction following than vision-only imitation baselines generally.",
            "temporal_dynamics": "Trained on trajectory sequences; temporal autoregressive decoding used but no dynamics-of-training analysis provided.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1931.2"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (Robotic Transformer 2)",
            "brief_description": "A VLA model that treats action generation as text generation by tokenizing actions; co-trains on web-scale vision-language data and thousands of robot demonstrations to enable zero-shot generalization to unseen instructions and objects.",
            "citation_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control.",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "Unified transformer taking vision and language tokens as prefix and autoregressively generating action tokens (action discretized via DCT + BPE). Pretrained/fine-tuned on both web-scale VLM corpora and robot demonstration trajectories.",
            "pretraining_type": "multistage: vision-language pretraining on internet-scale image-text corpora + supervised fine-tuning on robot demonstration datasets",
            "pretraining_data_description": "Web-scale multimodal corpora (image-text pairs) impart semantic priors; thousands of robot demonstrations with trajectory/action labels provide grounded action mappings. Action tokenization includes DCT compression and BPE-style tokenization.",
            "target_task_name": "zero-shot/transfer robotic manipulation (novel object instruction following)",
            "target_task_description": "Manipulation tasks where the model must interpret novel instructions and produce motor command sequences; action space is discretized token stream representing motor trajectories (continuous actions compressed into tokens). Evaluated on novel object generalization tests.",
            "semantic_alignment": "Explicitly measured: co-fine-tuning aligns web semantic priors with robotic action spaces, improving semantic grounding for unseen object references.",
            "performance_with_language_pretraining": "Reported: '63% improvement in performance on novel objects' (metric: improvement in performance relative to baseline when using web-scale multimodal pretraining + robot demos).",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Not numeric in review for RT-2; improved zero-shot/generalization attributed to web-scale semantic pretraining reducing need for per-task demonstrations.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Architecture uses fused visual-language tokens with cross-attention; review mentions chain-of-thought-style visual reasoning in RT-2 but no explicit attention maps provided.",
            "embedding_space_analysis": "RT-2 described as unifying vision-language and action tokens into shared embedding, enabling transfer; no explicit clustering analyses given.",
            "action_grounding_evidence": "Functional evidence: co-training and discrete action tokenization produced large gains on novel-object tasks (63% improvement), indicating semantic-language priors grounded into motor outputs.",
            "hierarchical_features_evidence": "Not analyzed layer-wise here; RT-2 treats entire pipeline as unified autoregressive model combining semantic and motor tokens.",
            "transfer_conditions": "Transfer benefits when semantic concepts present in web pretraining map to object affordances in robot demos; success diminishes when affordances not represented in pretraining data.",
            "novel_vs_familiar_objects": "Explicit metric: 63% improvement on novel objects with language/web pretraining vs baseline (baseline unspecified here).",
            "zero_shot_or_few_shot": "RT-2 supports zero-shot generalization to unseen instructions/objects; quantitative zero-shot success rates not further detailed beyond the 63% improvement statement.",
            "layer_analysis": "No layer ablation/probing reported in review for RT-2.",
            "negative_transfer_evidence": "None reported in review; overall improvement emphasized.",
            "comparison_to_vision_only": "RT-2's co-fine-tuning on web vision-language corpora is highlighted as providing substantial gains over models trained only on robot demos; precise numeric comparator not provided here.",
            "temporal_dynamics": "Action tokenization via DCT compresses trajectories temporally; review does not quantify training-phase dynamics.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1931.3"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA",
            "brief_description": "An open-source 7B-parameter vision-language-action model co-fine-tuned on nearly one million real robot demonstrations that achieves strong real-world generalization with parameter-efficient methods.",
            "citation_title": "OpenVLA: An open-source vision-language-action model.",
            "mention_or_use": "mention",
            "model_name": "OpenVLA (7B)",
            "model_description": "7B-parameter VLA combining dual vision encoders (DINOv2, SigLIP) with a Llama-2 language backbone and an autoregressive action decoder; trained via co-fine-tuning on large vision-language corpora and ~970k real-world robot demonstrations.",
            "pretraining_type": "co-fine-tuning: vision-language pretraining (LAION-scale data) + large-scale robot demonstration fine-tuning (behavioral cloning)",
            "pretraining_data_description": "LAION-5B style web image-text corpora yield semantic priors; RT-X/robot trajectory datasets (~970k demonstrations) provide action grounding including object manipulations and spatial relations.",
            "target_task_name": "generalist robotic manipulation / instruction following",
            "target_task_description": "Multi-task real-world robotic manipulation across many tasks collected in demonstration dataset; action outputs are autoregressive tokens mapping to motor commands; evaluated for success rate on held-out tasks and novel objects.",
            "semantic_alignment": "Explicit: co-fine-tuning aligns web-scale semantic priors with robot trajectories; review states this alignment is core to OpenVLA's generalization.",
            "performance_with_language_pretraining": "Reported: 'OpenVLA (7B) achieves a 16.5% higher success rate than a 55B-parameter RT-2 variant' on reported benchmarks (metric: success rate).",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "OpenVLA uses LoRA adapters and parameter-efficient methods to adapt with much less compute; explicit demonstration-sample comparisons not provided, but LoRA reduced GPU hours by ~70% in general examples.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Review notes cross-attention fusion and preservation of CLIP-like alignment but does not present attention visualizations for OpenVLA specifically.",
            "embedding_space_analysis": "Co-fine-tuning reported to bring semantic and motor spaces into alignment; explicit embedding-space probes not provided here.",
            "action_grounding_evidence": "Empirical evidence: higher success rates on held-out tasks, indicating successful grounding of language semantics into action policies via joint training on web VLM + robot demos.",
            "hierarchical_features_evidence": "Not explicitly layer-probed; architecture benefits from frozen pretrained vision encoders and a trainable policy head.",
            "transfer_conditions": "Performs well with large, diverse demonstration corpora and preserved VLM encoders; out-of-distribution objects still challenging but improved over alternatives.",
            "novel_vs_familiar_objects": "Quantified indirectly via higher success on benchmarks vs larger RT-2-X model, suggesting better transfer; explicit per-category novel vs familiar numbers not provided.",
            "zero_shot_or_few_shot": "Demonstrates improved zero/few-shot generalization behaviorally; exact counts not provided here.",
            "layer_analysis": "LoRA-based adaptation used (adapter injection) but no detailed frozen-vs-finetuned ablation numbers beyond computational savings.",
            "negative_transfer_evidence": "Not reported; OpenVLA generally presented as improving generalization while being parameter-efficient.",
            "comparison_to_vision_only": "Co-fine-tuning with language data emphasized as key advantage vs purely robot-demo-trained models; explicit vision-only comparison numbers not shown here.",
            "temporal_dynamics": "Not directly analyzed in review for OpenVLA.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1931.4"
        },
        {
            "name_short": "Octo",
            "name_full": "Octo",
            "brief_description": "An open-source generalist robot policy (93M parameters) using diffusion decoders trained on a very large OpenX-Embodiment demonstration dataset to scale embodied policies.",
            "citation_title": "Octo: An opensource generalist robot policy.",
            "mention_or_use": "mention",
            "model_name": "Octo",
            "model_description": "Small-scale generalist VLA (93M) with diffusion-based action decoders and CNN vision encoder, trained on the Open X-Embodiment dataset (800k demonstrations) to produce stochastic, diverse trajectories for manipulation.",
            "pretraining_type": "pretrained vision encoder (standard) + large-scale supervised imitation from multi-robot demonstration dataset; diffusion policy pretraining on action sequences",
            "pretraining_data_description": "Open X-Embodiment: 800,000 robot demonstrations from multi-robot sources covering diverse morphologies and manipulation tasks; contains action trajectories, object interactions, and instruction annotations.",
            "target_task_name": "generalist manipulation across many robot types",
            "target_task_description": "Many manipulation tasks across different embodiments; action decoder generates trajectories via diffusion (continuous action space represented stochastically); tested for scalability and generalization.",
            "semantic_alignment": "Trained on robot trajectories with instruction annotations to ground language to actions; review highlights diffusion decoding for multimodal action distributions rather than explicit large-scale VLM alignment.",
            "performance_with_language_pretraining": "Octo presented as broadly generalizable and dataset-scaled; review gives architecture/training scale but not specific numeric success rates.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Not explicitly quantified; diffusion policies are noted to require more compute but modeled as more expressive (no sample numbers given).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported for Octo specifically.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Behavioral: trained on diverse demonstration set yielding robust long-horizon decision-making; diffusion decoder captures multiple viable action modes.",
            "hierarchical_features_evidence": "Diffusion decoder provides expressive low-level action modes; high-level semantics depend on instruction annotations in dataset.",
            "transfer_conditions": "Scale of OpenX-Embodiment data and diffusion sampling enables transfer across robot types; success depends on demonstration diversity and similarity to target embodiment.",
            "novel_vs_familiar_objects": "Not reported numerically.",
            "zero_shot_or_few_shot": "Review emphasizes generalist capabilities but quantification not provided.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "No explicit negative transfer reported; diffusion policies noted to be computationally heavy.",
            "comparison_to_vision_only": "Emphasis is on large-scale robot trajectory training rather than vision-only pretraining comparisons.",
            "temporal_dynamics": "Diffusion policies model temporal multimodality; no training dynamics reported.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1931.5"
        },
        {
            "name_short": "Diffusion Policy",
            "name_full": "Diffusion Policy",
            "brief_description": "A class of policies using diffusion models to produce diverse, smooth multimodal action distributions for visuomotor control, improving representation of multiple valid strategies at the cost of computation.",
            "citation_title": "Diffusion policy: Visuomotor policy learning via action diffusion.",
            "mention_or_use": "mention",
            "model_name": "Diffusion Policy",
            "model_description": "Uses diffusion-based generative modeling for action sequence generation conditioned on visual (and sometimes language) context; models continuous trajectories as samples from learned diffusion process.",
            "pretraining_type": "trained on robot trajectory datasets (imitation) with diffusion modeling of actions; vision encoders often pretrained separately (vision-language or vision-only)",
            "pretraining_data_description": "Large collections of demonstration trajectories with visual context and instruction labels; contains multi-modal possible actions for similar inputs (supports multimodal action distributions).",
            "target_task_name": "dexterous and multimodal manipulation tasks",
            "target_task_description": "Contact-rich manipulation and tasks with multiple valid action strategies; actions are continuous trajectories sampled from diffusion model, typically in simulation and transferred to real robots.",
            "semantic_alignment": "Enables richer action representations which can be conditioned on language; review states diffusion policies better capture multimodal action possibilities and thereby improve alignment between language-specified goals and motor outcomes.",
            "performance_with_language_pretraining": "Diffusion policies reported to improve diversity and robustness of generated actions; review notes improved success in complex tasks but also higher compute (rough relative statements, not absolute metrics).",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Not numerically given here; diffusion policies are computationally heavier (~3×) but produce better multimodal coverage which can reduce failure modes.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not discussed specifically for diffusion policies in this review.",
            "embedding_space_analysis": "Not provided.",
            "action_grounding_evidence": "Argued qualitatively: diffusion captures multiple plausible motor patterns corresponding to the same high-level instruction, supporting better grounding of verbs to affordance-contingent motor outcomes.",
            "hierarchical_features_evidence": "Diffusion used as low-level controller within hierarchical VLA systems; review suggests low-level action diversity benefits from diffusion while high-level planning uses LLM-style modules.",
            "transfer_conditions": "Compute and latency constraints limit real-time use; transfer success depends on ability to compress/accelerate sampling for deployment.",
            "novel_vs_familiar_objects": "Not reported numerically here.",
            "zero_shot_or_few_shot": "Not stated.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "High compute of diffusion methods can limit real-time applicability (practical negative effect).",
            "comparison_to_vision_only": "Diffusion concerns action generation rather than vision pretraining comparison; stronger multimodal action modeling than simple autoregressive decoders is emphasized.",
            "temporal_dynamics": "Diffusion models represent temporal multimodality natively; no training phase time-course statistics given.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1931.6"
        },
        {
            "name_short": "Pi-0 (Pi-Zero)",
            "name_full": "Pi-0",
            "brief_description": "A VLA model using diffusion-based policies (Pi-0) with a 'fast' variant that compresses action tokens for high-frequency control, trading some trajectory granularity for speed.",
            "citation_title": "Pi-0: A vision-language-action flow model for general robot control.",
            "mention_or_use": "mention",
            "model_name": "Pi-0 / Pi-0 Fast",
            "model_description": "Diffusion-based low-level action policy conditioned on fused vision-language-state tokens; Pi-0-Fast uses compressed action tokenization (frequency-domain FAST tokens) to enable high control rates (e.g., 200 Hz) with minimal accuracy loss.",
            "pretraining_type": "imitation learning from robot demonstrations with diffusion action modeling; vision-language encoders may be pretrained separately.",
            "pretraining_data_description": "Robot trajectory datasets with language annotations; includes temporally dense action sequences suitable for DCT/frequency compression approaches.",
            "target_task_name": "high-frequency control for dexterous manipulation",
            "target_task_description": "Contact-rich, high-bandwidth manipulation tasks requiring sub-second action updates; continuous action trajectories compressed into tokens for fast decoding.",
            "semantic_alignment": "Tokenization preserves semantic mapping from instruction to action while enabling compressed decoding; review emphasizes preservation of semantic fidelity with Pi-0 Fast.",
            "performance_with_language_pretraining": "Pi-0 Fast variant described as delivering continuous 200 Hz control with negligible accuracy loss using small adapter parameterizations; no absolute success rate numbers given here.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Not quantified numerically in review; Pi-0 variants presented as computational vs expressivity trade-offs.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided for Pi-0 specifically.",
            "embedding_space_analysis": "Not provided.",
            "action_grounding_evidence": "Functional: diffusion policies model multimodal action distributions enabling varied grasps/trajectories that better match language-specified goals; no mechanistic grounding probes reported.",
            "hierarchical_features_evidence": "Pi-0 used as low-level controller within hierarchical VLA stacks; high-level planners remain separate.",
            "transfer_conditions": "High-frequency decoding requires compressed tokenization and hardware optimizations to transfer to embedded robots.",
            "novel_vs_familiar_objects": "Not reported numerically.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Higher computational cost is a practical limitation for real-time use.",
            "comparison_to_vision_only": "Not applicable; comparisons are between diffusion vs autoregressive action decoders rather than vision-only pretraining.",
            "temporal_dynamics": "Pi-0-Fast leverages frequency compression (DCT) to represent temporal windows as fewer tokens enabling high update rates.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1931.7"
        },
        {
            "name_short": "CoVLA",
            "name_full": "CoVLA",
            "brief_description": "A comprehensive vision-language-action dataset and model work for autonomous driving that pairs hours of driving video, LiDAR/odometry, language annotations and trajectories to train VLA models for driving tasks.",
            "citation_title": "Covla: Comprehensive vision-language-action dataset for autonomous driving.",
            "mention_or_use": "mention",
            "model_name": "CoVLA",
            "model_description": "Dataset and modeling framework that uses CLIP for visual grounding and LLaMA-2 for language embedding, with trajectory decoders for driving action prediction; integrates multi-sensor streams (camera, LiDAR, odometry).",
            "pretraining_type": "dataset-driven supervised training using paired driving videos, sensor streams and natural language annotations; vision encoders often pretrained on image-text corpora",
            "pretraining_data_description": "Over 80 hours of driving videos with synchronized sensor streams and rich natural language annotations about traffic situations, objects, and navigation directives — contains dynamic behaviors, affordance-relevant events and action labels.",
            "target_task_name": "autonomous driving: instruction-conditioned driving and decision making",
            "target_task_description": "Urban driving tasks: trajectory prediction, instruction-following ('yield to ambulance'), and scene understanding; action space continuous (steering, throttle, brake) with world-relative and ego-centric representations in real-world driving data.",
            "semantic_alignment": "High: language annotations directly describe driving goals and situations; CLIP+LLaMA encoders provide semantic grounding for driving cues and instruction resolution.",
            "performance_with_language_pretraining": "Dataset enables training models that align perception and language for driving; review does not state single numeric success rate but highlights interpretability and improved planning performance.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Not provided numerically.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here for CoVLA models specifically.",
            "embedding_space_analysis": "Not provided.",
            "action_grounding_evidence": "Explicit dataset pairing of language descriptions to driving trajectories provides direct evidence for language-to-action grounding in driving domain.",
            "hierarchical_features_evidence": "OpenDriveVLA and ORION build on CoVLA-like data to provide hierarchical 2D/3D alignment and long-horizon planning; CoVLA provides the grounded dataset backbone.",
            "transfer_conditions": "Transfer hinges on sensor modality match (LiDAR/cameras) and similarity of driving scenarios; domain shift (different cities/traffic) will affect performance.",
            "novel_vs_familiar_objects": "Not quantified in review.",
            "zero_shot_or_few_shot": "Not explicitly reported here.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "In- review argues multimodal (vision+language) training yields richer, more interpretable driving policies than purely perception-to-control pipelines, though no numeric comparisons provided.",
            "temporal_dynamics": "Driving datasets provide long-horizon temporal context used by QT-Former style history encoders in downstream models (ORION); detailed training dynamics not provided.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1931.8"
        },
        {
            "name_short": "PointVLA",
            "name_full": "PointVLA",
            "brief_description": "A model that injects 3D point cloud features into frozen pretrained VLA backbones via modular skip-blocks to enable 3D spatial reasoning and few-shot, long-horizon control.",
            "citation_title": "Pointvla: Injecting the 3d world into vision-language-action models.",
            "mention_or_use": "mention",
            "model_name": "PointVLA",
            "model_description": "Augments frozen 2D VLA backbones with 3D point-cloud features via modular skip connections, allowing models to preserve learned 2D semantic knowledge while integrating 3D spatial geometry for manipulation and navigation.",
            "pretraining_type": "foundation VLA pretraining on 2D vision-language corpora and robot demos, with additional 3D point-cloud modules trained/fine-tuned on 3D datasets or inserted modularly without retraining the full backbone.",
            "pretraining_data_description": "2D image-text/web corpora provide semantics; point-cloud datasets yield geometric/affordance information (3D positions, shapes, spatial relations); training aims to preserve 2D semantics while adding 3D spatial priors.",
            "target_task_name": "3D-aware robotic manipulation and spatial reasoning",
            "target_task_description": "Tasks requiring explicit 3D reasoning (depth, occlusion resolution, grasp points) across few-shot and long-horizon scenarios; action space continuous/parametric for manipulation in 3D workspaces.",
            "semantic_alignment": "Designed to align 2D learned semantics with 3D geometry; review claims strong 3D spatial reasoning and few-shot capability while preserving 2D semantic knowledge.",
            "performance_with_language_pretraining": "Reported as enabling strong few-shot and dynamic task performance with no retraining of the 2D backbone; review gives qualitative claims of strong performance but no numeric metrics.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Modular insertion allows few-shot adaptation to 3D tasks; no quantitative sample counts provided in review.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided specifically; modular skip-blocks imply cross-modal attention between 2D/3D features but no attention visualizations presented.",
            "embedding_space_analysis": "Claimed to preserve 2D attractor semantics while adding geometric features, but no explicit embedding analyses presented here.",
            "action_grounding_evidence": "Behavioral evidence: improved spatial reasoning and manipulation in 3D tasks when 3D features integrated; direct mechanistic grounding probes not supplied.",
            "hierarchical_features_evidence": "Integration shows high-level semantics (2D) preserved while low/mid-level geometric features added for better physical interaction.",
            "transfer_conditions": "Works best when 3D sensors are available and target embodiment requires precise spatial reasoning; modular design helps cross-embodiment transfer.",
            "novel_vs_familiar_objects": "Not explicitly quantified here.",
            "zero_shot_or_few_shot": "Few-shot capability emphasized qualitatively.",
            "layer_analysis": "Modular insertion avoids full backbone retraining; layer-level importance not numerically analyzed in review.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "PointVLA improves over pure 2D VLA on 3D tasks by injecting geometric features; review reports qualitative superiority without numerical comparison.",
            "temporal_dynamics": "Not detailed.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1931.9"
        },
        {
            "name_short": "DexVLA / DexGraspVLA",
            "name_full": "DexVLA / DexGraspVLA",
            "brief_description": "VLA variants focused on general dexterous grasping and dexterous manipulation using hierarchical planners combined with diffusion-based low-level controllers to achieve robust grasping across diverse objects and conditions.",
            "citation_title": "Dexgraspvla: A vision-language-action framework towards general dexterous grasping.",
            "mention_or_use": "mention",
            "model_name": "DexVLA / DexGraspVLA",
            "model_description": "Hierarchical architecture: pretrained vision-language planner produces goals and affordance points; diffusion-based low-level controller generates smooth, compliant joint trajectories for dexterous hands. Trained on diverse demonstration sets and uses iterative domain-invariant representations.",
            "pretraining_type": "vision-language pretraining for semantic priors + imitation learning and diffusion policy training on grasping datasets; iterative domain-invariant pretraining to support zero-shot transfer.",
            "pretraining_data_description": "Large-scale grasp/dexterity datasets across lighting/background variations, synthetic augmentation and real-world demos; includes object affordance labels and action trajectories.",
            "target_task_name": "dexterous grasping and manipulation",
            "target_task_description": "Fine-grained grasping with multi-fingered hands across varied object geometries and visual conditions; continuous high-precision joint trajectories executed in real-world settings.",
            "semantic_alignment": "Pretraining on vision-language and affordance labels aligns nouns and adjectives to grasp affordances; iterative domain-invariant methods aim to preserve this mapping across domains.",
            "performance_with_language_pretraining": "Review states '90%+ success on unseen scenarios' for DexGraspVLA (claim in review) indicating strong zero-shot generalization for grasping across varied conditions.",
            "performance_without_language_pretraining": "",
            "sample_efficiency_comparison": "Iterative domain-invariant representation and pretraining reduce reliance on task-specific demos; no exact sample counts provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided specifically; architecture uses affordance prediction modules which likely focus on graspable regions but no attention maps provided.",
            "embedding_space_analysis": "Iterative domain-invariant representations claimed to support cross-domain robustness; no explicit embedding metrics provided.",
            "action_grounding_evidence": "Reported behavioral evidence: &gt;90% success on unseen scenarios indicates effective grounding of action semantics (grasp) to visual affordances and motor patterns.",
            "hierarchical_features_evidence": "High-level planner (VLM) benefits from language pretraining for goal/affordance selection; low-level diffusion controller supplies trajectory precision.",
            "transfer_conditions": "Performs well under domain shifts when domain-invariant pretraining and synthetic augmentation used; sensitive to sensor modality mismatches.",
            "novel_vs_familiar_objects": "Reported high success on unseen scenarios (90%+), indicating small gap for novel objects in tests described by review.",
            "zero_shot_or_few_shot": "Explicit strong zero-shot generalization claimed for dexterous grasping benchmarks (90%+ success on unseen scenarios).",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Language/affordance pretraining combined with diffusion controllers is claimed superior to vision-only approaches for zero-shot grasp generalization; exact numeric baseline comparisons not provided here.",
            "temporal_dynamics": "Low-level diffusion used for smooth temporal trajectory sampling; no training-phase curves reported.",
            "dimensionality_analysis": "Not provided.",
            "uuid": "e1931.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control.",
            "rating": 2
        },
        {
            "paper_title": "RT-1: Robotics transformer for real-world control at scale.",
            "rating": 2
        },
        {
            "paper_title": "Cliport: What and where pathways for robotic manipulation.",
            "rating": 2
        },
        {
            "paper_title": "VIMA: General robot manipulation with multimodal prompts.",
            "rating": 2
        },
        {
            "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion.",
            "rating": 2
        },
        {
            "paper_title": "Octo: An opensource generalist robot policy.",
            "rating": 2
        },
        {
            "paper_title": "OpenVLA: An open-source vision-language-action model.",
            "rating": 2
        },
        {
            "paper_title": "Pointvla: Injecting the 3d world into vision-language-action models.",
            "rating": 2
        },
        {
            "paper_title": "Dexgraspvla: A vision-language-action framework towards general dexterous grasping.",
            "rating": 2
        },
        {
            "paper_title": "Covla: Comprehensive vision-language-action dataset for autonomous driving.",
            "rating": 1
        }
    ],
    "cost": 0.030862,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges
7 May 2025</p>
<p>Ranjan Sapkota 
Cornell University, Biological &amp; Environmental Engineering
IthacaNew YorkUSA</p>
<p>Yang Cao 
Department of Computer Science and Engineering
Department of Informatics and Telecommunications
The Hong Kong University of Science and Technology
Hong Kong c University of the Peloponnese
Greece</p>
<p>Konstantinos I Roumeliotis 
Manoj Karkee 
Cornell University, Biological &amp; Environmental Engineering
IthacaNew YorkUSA</p>
<p>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges
7 May 202568F7A2F0A1D6055674A12EB96B06E5FEarXiv:2505.04769v1[cs.CV]Preprint submitted to Proceedings of the IEEE May 9, 2025Vision-Language-ActionVLAArtificial IntelligenceRoboticsVision-Language ModelsAI AgentsAgentic AI Contents
Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework.This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field.We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers.Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years.Key progress areas include architectural innovations, parameterefficient training strategies, and real-time inference accelerations.We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation.The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks.Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning.In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents.This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence.</p>
<p>Control Behavior</p>
<p>Integrate vision, language understanding, and motor actions, enabling robots to perceive, reason, and act coherently Vision-Language-Action Models</p>
<p>Understand Text
Understand Image
Figure 1: Evolution from Isolated Modalities to Unified Vision-Language-Action Models.This figure illustrates the transition from separate vision, language, and action systems-each limited to its own domain-to integrated VLA models.VLA models enable robots to jointly perceive, understand language, and act, overcoming the fragmentation of earlier approaches and marking a major step toward adaptive, generalizable, and intelligent embodied agents.</p>
<p>Introduction</p>
<p>Before Vision-Language-Action (VLA) models were developed, progress in robotics and artificial intelligence happened mostly in separate areas: vision systems that could see and recognize images [44,69], language systems that could understand and generate text [164,137], and action systems that could control movement [49].These systems worked well on their own, but they struggled to work together or handle new and unpredictable situations [46,21].As a result, 'erstand complex environments or respond flexibly to real-world challenges.</p>
<p>As illustrated in Figure 1, traditional computer vision models primarily based on convolutional neural networks (CNNs) , were tailored for narrowly specified tasks such as object detection or classification, requiring extensive labeled datasets and cumbersome retraining for even slight shifts in environment or objectives [156,62].These vision models could "see" (e.g., identifying apples in an orchard, as shown in Figure 1) but lacked any understanding of language or the ability to convert visual insights into purposeful actions.Language models, particularly large language models (LLMs), revolutionized text-based understanding and generation [23]; however, they remained restricted to processing language without the capability to perceive or reason about the physical world [76] ("Ripe apples in orchard" in Figure 1 exemplifies this limitation).Meanwhile, action-based systems in robotics, relying heavily on hand-crafted policies or reinforcement learning [122], enabled specific behaviors like object manipulation but demanded painstaking engineering and failed to generalize beyond narrowly scripted scenarios [119].</p>
<p>Despite progress with VLMs, which achieved impressive multimodal understanding by combining vision and language [149,25,148], there remained a conspicuous integration gap: the inability to generate or execute coherent actions based on multimodal input [121,107].As further visualized in Figure 1 , most AI systems specialized at most in two modalities-visionlanguage, vision-action, or language-action-but struggled to fully integrate all three into a unified, end-to-end framework.Consequently, robots could recognize objects visually ("apple"), understand a corresponding textual instruction ("pick the apple"), or perform a predefined motor action (grasping), yet orchestrating these abilities into fluid, adaptable behavior was beyond reach.The result was a fragmented pipeline architecture that could not flexibly adapt to new tasks or environments, leading to brittle generalization and labor-intensive engineering efforts.This highlighted a critical bottleneck in embodied AI: without systems that could jointly perceive, understand, and act, intelligent autonomous behavior remained an elusive goal.</p>
<p>The pressing need to bridge these gaps catalyzed the emergence of VLA models.VLA models, conceptualized around 2021-2022, and pioneered by efforts such as Google Deep-Mind's Robotic Transformer 2 (RT-2) [224], introduced a transformative architecture that unified perception, reasoning, and control within a single framework.As a solution to the limitations outlined in Figure 1, VLAs integrate vision inputs, language comprehension, and motor control capabilities, enabling embodied agents to perceive their surroundings, under-stand complex instructions, and execute appropriate actions dynamically.Early VLA approaches achieved this integration by extending vision-language models to include action tokens-numerical or symbolic representations of robot motor commands, thereby allowing the model to learn from paired vision, language, and trajectory data [121].This methodological innovation dramatically improved robots' ability to generalize to unseen objects, interpret novel language commands, and perform multi-step reasoning in unstructured environments [83].</p>
<p>VLA models represent a transformative step in the pursuit of unified multimodal intelligence, overcoming the long-standing limitations of treating vision, language, and action as separate domains [121].By leveraging internet-scale datasets that integrate visual, linguistic, and behavioral information, VLAs empower robots to not only recognize and describe their environments but also to reason contextually and execute appropriate actions in complex, dynamic settings [196].The progression illustrated in Figure 1 from isolated vision, language, and action systems to an integrated VLA paradigm-captures a fundamental shift toward the development of truly adaptive and generalizable embodied agents.Given the profound implications of this innovation, it is crucial to undertake a thorough and systematic review that draws from a comprehensive body of literature and critical analysis.First, such a review is necessary to clarify the foundational concepts and architectural principles that distinguish VLAs from their predecessors.Second, it provides a structured account of the rapid progress and key milestones in the field, enabling researchers and practitioners to appreciate the trajectory of technological advancements.Third, an in-depth review is essential for mapping the diverse range of real-world applications-from household robotics to industrial automation and assistive technologies-where VLAs are already demonstrating transformative potential.Furthermore, by critically examining the current challenges, such as data efficiency, safety, generalization, and ethical considerations, the review identifies barriers that must be addressed for widespread deployment.And Fifth, synthesizing these insights helps to inform the broader AI and robotics communities about emerging research directions and practical considerations, fostering collaboration and innovation.</p>
<p>In this review, we systematically analyze the foundational principles, developmental progress, and technical challenges associated with VLA models.Our objective is to consolidate the current understanding of VLAs while identifying limitations and proposing future directions for their evolution.The review begins with a detailed examination of key conceptual foundations (Figure 2), including what constitutes a VLA model, its historical evolution, multimodal integration mechanisms, and language-based tokenization and encoding strategies.These conceptual components set the stage for understanding how VLAs are structured and function across modalities.</p>
<p>Building upon this, we present a unified view of recent progress and training efficiency strategies (Figure 3).This includes architectural innovations that have enabled more capable and generalizable VLA models, as well as data-efficient learning frameworks, parameter-efficient modeling techniques, and  model acceleration strategies designed to reduce computational overhead without compromising performance.These advancements are critical for scaling VLA systems to real-world applications.</p>
<p>Following this, we delve into a comprehensive discussion of the current limitations faced by VLA systems (Figure 4).These include inference bottlenecks, safety concerns, high computational demands, limited generalization, and ethical implications.We not only highlight these pressing challenges but also provide an analytical discussion of potential solutions to address them.</p>
<p>Together, these three figures offer a visual framework that supports the textual analysis of this review.By outlining the conceptual landscape, recent innovations, and open challenges, this work aims to guide future research and encourage the development of more robust, efficient, and ethically grounded Figure 4: Mindmap -VLA Challenges.This diagram highlights key barriers to robust VLA deployment, including inference limitations, bias, system complexity, generalization gaps, and ethical concerns.It also motivates the need for innovative solutions and future research directions to overcome these challenges.models typically employ multimodal fusion techniques-such as cross-attention, concatenated embeddings, or token unification-to align sensory observations with textual instructions.</p>
<p>Unlike traditional visuomotor pipelines, VLAs support semantic grounding, enabling context-aware reasoning, affordance detection, and temporal planning.A typical VLA model observes the environment through camera or sensor data, interprets goals expressed in language (e.g., "pick up the red apple") (Figure 5), and outputs low-level or high-level action sequences.Recent advancements integrate imitation learning, reinforcement learning, or retrieval-augmented modules to improve sample efficiency and generalization.This review examines how VLA models have evolved from foundational fusion architectures to general-purpose agents capable of real-world deployment across robotics, navigation, and human-AI collaboration.</p>
<p>VLA models are multimodal artificial intelligence systems that unify visual perception, language comprehension, and physical action generation into a single framework.These models enable robots or AI agents to interpret sensory inputs (e.g., images, text), understand contextual meaning, and autonomously execute tasks in real-world environments-all through end-to-end learning rather than isolated subsystems.As shown conceptually in Figure 5, VLA models bridge the historical disconnect between visual recognition, language comprehension, and motor execution that limited the capabilities of earlier robotic and AI systems.</p>
<p>Evolution and Timeline</p>
<p>The rapid development of VLA models from 2022-2025 demonstrates three distinct evolutionary phases:</p>
<ol>
<li>Foundational Integration (2022-2023).Early VLAs established basic visuomotor coordination through multimodal fusion architectures.[157] first combined CLIP embeddings with motion primitives, while [141] demonstrated generalist capabilities across 604 tasks.[18] achieved 97% success rates in manipulation through scaled imitation learning, and [86] introduced temporal reasoning via transformer-based planners.By 2023, [224] enabled visual chain-of-thought reasoning, and [34] advanced stochastic action prediction through diffusion processes.These foundations addressed low-level control but lacked compositional reasoning [216], prompting innovations in affordance grounding [78].</li>
</ol>
<p>Specialization and Embodied Reasoning (2024).</p>
<p>Second-generation VLAs incorporated domain-specific inductive biases.[202] enhanced few-shot adaptation through retrieval-augmented training, while [210] optimized navigation via 3D scene-graph integration.[39] introduced reversible architectures for memory efficiency, and [183] addressed partial observability with physics-informed attention.Simultaneously, [5] improved compositional understanding through object-centric disentanglement, and [220] extended applications to autonomous driving via multi-modal sensor fusion.These advances required new benchmarking methodologies [196].</p>
<p>Generalization and Safety-Critical Deployment (2025).</p>
<p>Current systems prioritize robustness and human alignment.[205] integrated formal verification for risk-aware decisions, while [42] demonstrated whole-body control through hierarchical VLAs.[19] optimized compute efficiency for embedded deployment, and [102] combined neural-symbolic reasoning for causal inference.Emerging paradigms like [100]'s affordance chaining and [13]'s simto-real transfer learning address cross-embodiment challenges, while [108] bridges VLAs with human-in-the-loop interfaces through natural language grounding.</p>
<p>Figure 6 presents a comprehensive timeline highlighting the evolution of 47 VLA models developed between 2022 and 2025.The earliest VLA systems, including CLIPort [157], Gato [141], RT-1 [18], and VIMA [86], laid the foundation by combining pretrained vision-language representations with task-conditioned policies for manipulation and control.These were followed by ACT [216], RT-2 [224], and Vox-Poser [78], which integrated visual chain-of-thought reasoning and affordance grounding.Models like Diffusion Policy [34] and Octo [167] introduced stochastic modeling and scalable data pipelines.In 2024, systems such as Deer-VLA [202], ReVLA [39], and Uni-NaVid [210] added domain specialization and memory-efficient designs, while Occllama [183] and ShowUI [108] tackled partial observability and user interaction.The trajectory continued with robotics-focused VLAs like Quar-VLA [43] and RoboMamba [111].Recent innovations emphasize generalization and deployment: SafeVLA [205], Humanoid-VLA [42], and MoManipVLA [190] incorporate verification, full-body control, and memory systems.Models such as Gr00t N1 [13] and SpatialVLA [136] further bridge sim-to-real transfer and spatial grounding.This timeline illustrates how VLAs have advanced from modular learning to general-purpose, safe, and embodied intelligence.</p>
<p>Multimodal Integration: From Isolated Pipelines to Unified Agents</p>
<p>A central advancement in the emergence of VLA models lies in their ability to perform multimodal integration, the joint processing of vision, language, and action within a unified architecture.Traditional robotic systems treated perception, natural language understanding, and control as discrete modules, often linked through manually defined interfaces or data transformations [109,20,168].For instance, classic pipelinebased frameworks required a perception model to output symbolic labels, which were then mapped by a planner to specific actions-frequently with domain-specific hand engineering [138,90].These approaches lacked adaptability, failed in ambiguous or novel environments, and could not generalize instructions beyond pre-encoded templates.</p>
<p>In contrast, modern VLAs fuse modalities end-to-end using large-scale pretrained encoders and transformer-based architectures [188].This shift enables the model to interpret visual observations and linguistic instructions within the same computational space, allowing flexible, context-aware reasoning [99].For example, in the task "Pick up the red ripe apple," (Figure 5) the vision encoder-typically a Vision Transformer (ViT) or ConvNeXt-segments and classifies objects in the scene (e.g., apples, leaves, background), identifying color and ripeness attributes [187].Meanwhile, the language model, often a variant of T5, GPT, or BERT, encodes the instruction into a highdimensional embedding.These representations are then fused via cross-attention or joint tokenization schemes, producing a unified latent space that informs the action policy [68].</p>
<p>This multimodal synergy was first effectively demonstrated in CLIPort [157], which used CLIP embeddings for semantic grounding and a convolutional decoder for pixel-level manipulation.CLIPort bypassed the need for explicit language parsing and directly conditioned visuomotor policies on natural language.Similarly, VIMA [86] advanced this approach by employing a transformer encoder to jointly process object-centric visual tokens and instruction tokens, enabling few-shot generalization across spatial reasoning tasks.</p>
<p>Recent developments push this fusion further by incorporating temporal and spatial grounding.VoxPoser [78] employs voxel-level reasoning to resolve ambiguities in 3D object selection, while RT-2 [224] fuses visual-language tokens into a unified transformer that supports zero-shot generalization to unseen instructions.Another noteworthy contribution is Octo [167], which introduces a memory-augmented transformer that enables long-horizon decision-making across diverse scenes, demonstrating the scalability of joint perceptionlanguage-action learning.</p>
<p>Crucially, VLAs offer robust solutions to challenges in realworld grounding.For example, Occllama [183] handles occluded object references through attention-based mechanisms, while ShowUI [108] demonstrates natural language interfaces that allow non-expert users to command agents through voice or typed input.These capabilities are only possible because the integration is not limited to surface-level fusion; rather, it captures semantic, spatial, and temporal alignment across modalities.</p>
<p>Tokenization and Representation: How VLAs Encode the World</p>
<p>A core innovation that sets VLA models apart from conventional vision-language architectures lies in their token-based representation framework, which enables holistic reasoning over perceptual [125,215], linguistic, and physical action spaces [106].Inspired by autoregressive generative models like transformers, modern VLAs encode the world using discrete tokens that unify all modalities-vision, language, state, and action into a shared embedding space [110].This allows the model to not only understand "what needs to be done" (semantic reasoning), but also "how to do it" (control policy execution) in a fully learnable and compositional way [192,117,170].</p>
<p>• Prefix Tokens: Encoding Context and Instruction:Prefix tokens serve as the contextual backbone of VLA models [195,83].These tokens encode the environmental scene (via images or video) and the accompanying natural language instruction into compact embeddings that prime the model's internal representations [16].</p>
<p>For instance, as depicted in Figure 7in a task such as "stack the green blocks on the red tray," the image of a cluttered tabletop is processed through a vision encoder like ViT or ConvNeXt, while the instruction is embedded by a large language model (e.g., T5 or LLaMA).These are then 2022 2023 CLIPort [157] Gato [141] RT-1 [18] VIMA [86] ACT [216] RT-2 [224] VoxPoser [78] Diffusion Policy [34] Octo [167] OpenVLA [94] 2024 2024</p>
<p>Deer-VLA [202] Uni-NaVid [210] ReVLA [39] Occllama [183] Pi-0 [14] RDT-1B [112] CogAct [102] EdgeVLA [19] ShowUI [108] NaviLa [32] Quar-VLA [43] Bi-VLA [59] RoboMamba [111] Otter [75] PointVLA [96] CombatVLA [29] HybridVLA [110] 2025 2025</p>
<p>CoVLA [5] OpenDriveVLA [220] ORION [56] ObjectVLA [223] ConRFT [31] Hi Robot [155] TLA [70] RaceVLA [153] DexVLA [185] Humanoid-VLA [42] SafeVLA [205] MoManipVLA [190] VLA-Cache [195] TinyVLA [186] Gr00t N1 [13] NORA [79] SpatialVLA [136] MoLe-VLA [213] Figure 6: Comprehensive timeline of Vision-Language-Action models (2022-2025), showing evolution from foundation to 45 specialized VLA systems.Organized chronologically with thematic grouping.</p>
<p>transformed into a sequence of prefix tokens that establish the model's initial understanding of the goal and environmental layout.This shared representation enables crossmodal grounding, allowing the system to resolve spatial references (e.g., "on the left," "next to the blue cup") and object semantics ("green blocks") across both modalities.</p>
<p>• State Tokens: Embedding the Robot's Configuration:</p>
<p>In addition to perceiving external stimuli, VLAs must be aware of their internal physical state [186,111].This is achieved through the use of state tokens, which encode real-time information about the agent's configuration-joint positions, force-torque readings, gripper status, end-effector pose, and even the locations of nearby objects [97].These tokens are crucial for ensuring situational awareness and safety, especially during manipulation or locomotion [163,81].</p>
<p>Figure 8 illustrates how VLA models utilize state tokens to enable dynamic, context-aware decision-making in both manipulation and navigation settings.In Figure 8a, a robot arm is shown partially extended near a fragile object.In such scenarios, state tokens play a critical role by encoding real-time proprioceptive information, such as joint angles, gripper pose, and end-effector proximity.These tokens are continuously fused with visual and languagebased prefix tokens, allowing the transformer to reason about physical constraints.The model can thus infer that a collision is imminent and adjust the motor commands accordingly-e.g., rerouting the arm trajectory or modulating force output.In mobile robotic platforms, as depicted in Figure 8b, state tokens encapsulate spatial features such as odometry, LiDAR scans, and inertial sen-sor data.These are essential for terrain-aware locomotion and obstacle avoidance.The transformer model integrates this state representation with environmental and instructional context to generate navigation actions that dynamically adapt to changing surroundings.Whether grasping objects in cluttered environments or autonomously navigating uneven terrain, state tokens provide a structured mechanism for situational awareness, enabling the autoregressive decoder to produce precise, context-informed action sequences that reflect both internal robot configuration and external sensory data.</p>
<p>• Action Tokens: Autoregressive Control Generation:</p>
<p>The final layer of the VLA token pipeline involves action tokens [93,94], which are autoregressively generated by the model to represent the next step in motor control [186].Each token corresponds to a low-level control signal, such as joint angle updates, torque values, wheel velocities, or high-level movement primitives [64].During inference, the model decodes these tokens one step at a time, conditioned on prefix and state tokens, effectively turning VLA models into language-driven policy generators [54,161].This formulation allows seamless integration with real-world actuation systems, supports variablelength action sequences [10,77], and enables model finetuning via reinforcement or imitation learning frameworks [214].Notably, models like RT-2 [224] and PaLM-E [47] exemplify this design, where perception, instruction, and embodiment are merged into a unified token stream.</p>
<p>For instance, in the apple-picking task as depicted in Figure 9, the model may receive prefix tokens that include the image of the orchard and the text instruction.The state tokens describe the robot's current arm posture and whether the gripper is open or closed.Action tokens are then predicted step by step to guide the robotic arm toward the apple, adjust the gripper orientation, and execute a grasp with appropriate force.The beauty of this approach is that it allows transformers, which are traditionally used for text generation, to now generate sequences of physical actions in a manner similar to generating a sentence-only here, the sentence is the motion.</p>
<p>To operationalize the VLA paradigm in robotics, we present in Figure 9 a structured pipeline that demonstrates how multimodal information-specifically vision, language, and proprioceptive state-is encoded, fused, and converted into executable action sequences.This end-to-end loop allows a robot to interpret complex tasks like "pick the ripe apple near the green leaf" and execute precise, context-sensitive manipulations.The system begins with multimodal input acquisition, where three distinct data streams are collected: visual observations (e.g., RGB-D frames), natural language commands, and real-time robot state information (e.g., joint angles or velocity).These are independently tokenized into discrete embeddings using pretrained modules [41,212].As depicted in the diagram, the image is processed through a Vision Transformer (ViT) backbone to generate vision tokens, the instruction is parsed by a Figure 9: Illustrating the process of how VLAs Encode the World.VLAs encode the world by converting vision, language, and sensor inputs into tokens, fusing them through cross-attention, predicting action sequences via transformers, and executing tasks with real-time feedback-enabling robots to interpret scenes, follow instructions, and adapt actions dynamically.language model such as BERT or T5 to produce language tokens, and state inputs are transformed via a lightweight MLP encoder into compact state tokens.</p>
<p>These tokens are then fused using a cross-modal attention mechanism, where the model jointly reasons over object semantics, spatial layout, and physical constraints [61].This fused representation forms the contextual basis for decision-making [74,116].In Figure 9, this is denoted as the multimodal fusion step.The fused embedding is passed into an autoregressive decoder-typically a transformer-that generates a series of action tokens.These tokens may correspond to joint displacements, gripper force modulation, or high-level motor primitives (e.g., "move to grasp pose", "rotate wrist").The action tokens are subsequently translated into control commands and passed to the execution loop, which closes the perception-action cycle by feeding back the robot's updated state, thus informing the next inference step.This closed-loop mechanism enables the model to dynamically adapt to perturbations, object shifts, or occlusions in real time [206,120,194].</p>
<p>To offer concrete implementation details, Algorithm 1 formalizes the VLA tokenization process.Given an RGB-D frame I, natural language instruction T , and joint angle vector θ, the algorithm produces a set of action tokens that can be executed in sequence.The image I is processed via a ViT to produce V, a set of 400 visual tokens.In parallel, the instruction T is encoded by a BERT model to yield L, a sequence of 12 semantic language tokens.Simultaneously, robot state θ is passed through a multilayer perceptron to generate a 64-dimensional state embedding S .These tokens are then fused via a cross-attention module to produce a shared 512-dimensional representation F, capturing the semantics, intent, and situational awareness needed for grounded action.Finally, a policy decoder such as FAST [133] maps the fused features into 50 discrete action tokens, which can then be decoded into motor commands τ 1:N .</p>
<p>The decoding process is implemented using a transformerbased architecture, as shown in the code snippet titled Action Prediction Code.A 'Transformer' object is initialized with 12 layers, a model dimension of 512, and 8 attention heads.The fused tokens are passed to the decoder, which autoregressively predicts the next most likely action token conditioned on previous tokens and context.The final motor command sequence is obtained by detokenizing the output.This implementation mirrors how text generation works in large language models, but here the "sentence" is a motion trajectory-a novel repurposing of natural language generation techniques for physical action synthesis.</p>
<p>Together, Figure 9, Algorithm 1, and the pseudocode illustrate how VLAs unify perception, instruction, and embodiment within a coherent and interpretable token space.This modularity allows the framework to generalize across tasks and robot morphologies, facilitating rapid deployment in real-world applications like apple picking, household tasks, and mobile navigation.Importantly, the clarity and separability of the tokenization steps make the architecture extensible, enabling further research on token learning, hierarchical planning, or symbolic grounding in VLA systems.gies Training VLA models requires a hybrid learning paradigm that integrates both semantic knowledge from the web and taskgrounded information from robotics datasets [30].As shown in prior sections, the multimodal architecture of VLAs must be exposed to diverse forms of data that support language understanding, visual recognition, and motor control.This is typically achieved through two primary data sources.</p>
<p>First, as depicted in figure 10, large-scale internet-derived corpora form the backbone of the model's semantic prior.These datasets include image-caption pairs (e.g., COCO, LAION-400M), instruction-following datasets (e.g., HowTo100M, We-bVid), and visual question-answering corpora (e.g., VQA, GQA).Such datasets enable pretraining of the visual and language encoders, helping the model acquire general representations of objects, actions, and concepts [2].This phase often uses contrastive or masked modeling objectives, such as CLIP-style contrastive learning or language modeling losses, to align vision and language modalities within a shared embedding space [146,199].Importantly, this stage gives VLAs a foundational "understanding of the world" that facilitates compositional generalization, object grounding, and zero-shot transfer [28,15].However, semantic understanding alone is insufficient for physical task execution [36,178,107].Thus, the second phase focuses on grounding the model in embodied experience [178].Robot trajectory datasets-collected either from realworld robots or high-fidelity simulators-are used to teach the model how language and perception translate into action [54].These include datasets like RoboNet [37], BridgeData [50], and RT-X [175], which provide video-action pairs, joint trajectories, and environment interactions under natural language instructions [123].Demonstration data may come from kinesthetic teaching, teleoperation, or scripted policies [89,12].This phase typically employs supervised learning (e.g., behavior cloning) [55], reinforcement learning (RL), or imitation learning to train the autoregressive policy decoder to predict action tokens based on fused visual-language-state embeddings [65].</p>
<p>Recent works increasingly adopt multistage or multitask training strategies.For example, models are often pretrained on vision-language datasets using masked language modeling, then fine-tuned on robot demonstration data using token-level autoregressive loss [94,221,195].Others use curriculum learning, where simpler tasks (e.g., object pushing) precede more complex ones (e.g., multistep manipulation) [217].Some approaches further leverage domain adaptation such as in Open-VLA [94] or sim-to-real transfer to bridge the gap between synthetic and real-world distributions [96].By unifying semantic priors with task execution data, these learning paradigms allow VLA models to generalize across tasks, domains, and embodiments-forming the backbone of scalable, instructionfollowing agents capable of robust real-world operation.</p>
<p>Through co-fine-tuning, these datasets are brought into alignment [179,52].The model learns to map from visual and linguistic inputs to appropriate action sequences [136].This training paradigm not only helps the model understand object affordances (e.g., apples can be grasped) and action outcomes (e.g., lifting requires force and trajectory), but also promotes generalization to novel scenarios [100].A model trained on kitchen manipulation tasks may be able to infer how to pick an apple in an outdoor orchard if it has learned general principles of object localization, grasping, and following language directives.</p>
<p>Recent architectures, such as Google DeepMind's RT-2 (Robotic Transformer 2) [224], have demonstrated this principle in action.RT-2 treats action generation as a form of text generation, where each action token corresponds to a discrete command in a robot's control space.Because the model is trained on both web-scale multimodal data and thousands of robot demonstrations, it can flexibly interpret novel instructions and perform zero-shot generalization to new objects and tasks-something that was largely impossible with traditional control systems or even early multimodal models.</p>
<p>Adaptive Control and Real-Time Execution</p>
<p>Another strength of VLAs lies in their ability to perform adaptive control, using real-time feedback from sensors to adjust behavior on the fly [153].This is particularly important in dynamic, unstructured environments like orchards, homes, or hospitals, where unexpected changes (e.g., wind moving an apple, lighting changes, human presence) can alter the task parameters.During execution, state tokens are updated in real time, reflecting sensor inputs and joint feedback [195].The model can then revise its planned actions accordingly.For instance, in the apple-picking scenario, if the target apple shifts slightly or another apple enters the field of view, the model dynamically reinterprets the scene and adjusts the grasp trajectory.This capability mimics human-like adaptability and is a core advantage of VLA systems over pipeline-based robotics.</p>
<p>Progress in Vision-Language-Action Models</p>
<p>The inception of VLA models was catalyzed by the remarkable success of transformer-based LLMs, notably ChatGPT, released in November 2022, which demonstrated unprecedented semantic reasoning capabilities (ChatGPT) [139].This breakthrough inspired researchers to extend language models to multimodal domains, integrating perception and action for robotics.By 2023, GPT-4 introduced multimodal capabilities, processing both text and images, which spurred efforts to incorporate physical actions (GPT-4) [1].Concurrently, VLMs like CLIP (2022) [157] and Flamingo (2022) [3] had established robust visualtext alignment through contrastive learning, enabling zero-shot object recognition and laying the groundwork for VLA models (CLIP).These models leveraged large-scale web datasets to align images with textual descriptions, a critical precursor to integrating actions.</p>
<p>A pivotal development was the creation of large-scale robotic datasets, such as RT-1's 130,000 demonstrations, which provided action-grounding data essential for co-training vision, language, and action components [18].These datasets captured diverse tasks and environments, enabling models to learn generalizable behaviors.Architectural breakthroughs followed with Google's RT-2 in 2023 [17], a landmark VLA model that unified vision, language, and action tokens, treating robotic control as an autoregressive sequence prediction task (RT-2 Blog).RT-2 discretized actions using Discrete Cosine Transform (DCT) compression and Byte-Pair Encoding (BPE), achieving a 63% improvement in performance on novel objects.Multimodal fusion techniques, such as cross-attention transformers, integrated Vision Transformer (ViT)-processed images (e.g., 400 patch tokens) with language embeddings, enabling robots to execute complex commands like "Pick the red cup left of the bowl."Additionally, UC Berkeley's Octo model (2023) introduced an open-source approach with 93M parameters and diffusion decoders, trained on 800,000 robot demonstrations from the OpenX-Embodiment Dataset, further broadening the research landscape [167].</p>
<p>Architectural Innovations in VLA Models</p>
<p>From 2023 to 2024, VLA models underwent significant architectural advancements and refined training methodologies.Dual-system architectures emerged as a key innovation, exemplified by NVIDIA's Groot N1 (2025) [13], which combined System 1 (fast diffusion policies with 10ms latency for lowlevel control) and System 2 (LLM-based planners for highlevel task decomposition).This separation enabled efficient coordination between strategic planning and real-time execution, enhancing adaptability in dynamic environments.Other models, like Stanford's OpenVLA (2024) [94], introduced a 7B-parameter open-source VLA trained on 970k real-world robot demonstrations, using dual vision encoders (DINOv2 [128] and SigLIP [204]) and a Llama 2 language model [172], outperforming larger models like RT-2-X (55B) [94].Training paradigms evolved to leverage co-fine-tuning on web-scale vision-language data (e.g., LAION-5B) [152] and robotic trajectory data (e.g., RT-X) [175], aligning semantic knowledge with physical constraints [152].Synthetic data generation tools like UniSim addressed data scarcity by creating photorealistic scenarios, such as occluded objects, crucial for robust training (UniSim [200]).Parameter efficiency was enhanced through Low-Rank Adaptation (LoRA) adapters [72], which allowed domain adaptation without full retraining, reducing GPU hours by 70%.The introduction of diffusion-based policies, as seen in Physical Intelligence's pi 0 model (2024) [14], offered improved action diversity but required significant computational resources.These advancements democratized VLA technology, fostering collaboration and accelerating innovation.</p>
<p>Recent VLA models have converged toward three major architectural paradigms that balance efficiency, modularity, and robustness: early fusion models, dual-system architectures, and self-correcting frameworks.Each of these innovations addresses specific challenges in grounding, generalization, and action reliability in real-world robotic systems.</p>
<ol>
<li>
<p>Early Fusion Models: One class of approaches focuses on fusing vision and language representations at the input stage before passing them to the policy module.Huang et al.'s EF-VLA model [74], presented at ICLR 2025, exemplifies this trend by retaining the representational alignment established by CLIP [157].EF-VLA accepts image-text pairs, encodes them with CLIP's frozen encoders, and fuses the resulting embeddings early in the transformer backbone-prior to action prediction.This design ensures that the semantic consistency learned during CLIP pretraining is preserved, reducing overfitting and enhancing generalization.Notably, EF-VLA demonstrated a 20% performance improvement on compositional manipulation tasks and reached 85% success on previously unseen goal descriptions.By avoiding fine-tuning of the vision-language modules, this approach also preserves computational efficiency and prevents catastrophic forgetting during domain-specific training.</p>
</li>
<li>
<p>Dual-System Architectures: Inspired by dual-process theories of human cognition, models like NVIDIA's Groot N1 (2025) [13] implement two complementary subsystems: a fastreactive module (System 1) and a slow-reasoning planner (System 2).System 1 comprises a diffusion-based control policy that operates at 10 ms latency, ideal for fine-grained, low-level control such as end-effector stabilization or adaptive grasping.In contrast, System 2 uses a LLM for task planning, skill composition, and high-level sequencing.The planner parses longhorizon goals (e.g., "clean the table") into atomic subtasks, while the low-level controller ensures real-time execution.This decomposition enables multi-timescale reasoning and improved safety, especially in environments where rapid reaction and deliberation must co-exist.In benchmark tests on multi-stage household manipulation, Groot N1 outperformed monolithic models by 17% in success rate and reduced collision failures by 28 3. Self-Correcting Frameworks: A third architectural evolution is the development of self-correcting VLA models, designed to detect and recover from failure conditions without external supervision.SC-VLA (2024) introduces a hybrid execution loop featuring a fast inference path and a slow correction path.In this framework, the default behavior is to predict poses or actions directly from the fused embedding using a lightweight transformer.When failures are detected-e.g., unsuccessful grasps or obstacle collisions-the model invokes a secondary process that performs chain-of-thought reasoning [211,203].This path queries an internal LLM (or external expert system) to diagnose failure modes and generate correction strategies [48].For example, if the robot repeatedly misidentifies an occluded object, the LLM may suggest an active viewpoint change or gripper reorientation.In closed-loop experiments, SC-VLA reduced task failure rates by 35% and significantly improved recoverability in cluttered and adversarial environments.</p>
</li>
</ol>
<p>VLA models exhibit a rich diversity of architectural designs and functional emphases, which can be systematically organized along the dimensions of end-to-end versus modular pipelines, hierarchical versus flat policy structures, and the balance between low-level control and high-level planning (Table 1).End-to-end VLAs, such as CLIPort [157], RT-1 [18], and OpenVLA [94], process raw sensory inputs directly into motor commands via a single unified network.By contrast, component-focused models like VLATest [182] and Chain-of-Affordance [100] decouple perception, language grounding, and action modules, enabling targeted improvements in individual submodules.</p>
<p>Hierarchical architectures have emerged to tackle complex, long-horizon tasks by separating strategic decision making from reactive control.For instance, CogACT [102] and NaV-ILA [32] employ a two-tier hierarchy where an LLM-based planner issues subgoals to a low-level controller, thereby combining the strengths of System 2 reasoning and System 1 execution.Similarly, ORION [56] integrates a QT-Former for longterm context aggregation with a generative trajectory planner in a cohesive framework.</p>
<p>Low-level policy emphasis is typified by diffusion-based controllers (e.g.Pi-0 [14], DexGraspVLA [219]), which excel at producing smooth, diverse motion distributions but often incur higher computational cost.In contrast, high-level planners (e.g.FAST Pi-0 Fast [133], CoVLA [5]) focus on rapid subgoal generation or coarse trajectory prediction, delegating fine-grained control to specialized modules or traditional motion planners.End-to-end dual-system models like HybridVLA [110] and Helix [166] blur these distinctions by jointly training both components while preserving modular interpretability.</p>
<p>Table 1 further highlights how recent VLAs balance these trade-offs.Models such as OpenDriveVLA [220] and Combat-VLA [29] prioritize hierarchical planning in dynamic, safetycritical domains, whereas lightweight, edge-targeted systems like Edge VLA [19] and TinyVLA [186] emphasize real-time low-level policies at the expense of high-level reasoning.This classification framework not only clarifies the design space of VLAs but also guides future development by pinpointing underexplored combinations-such as fully end-to-end, hierarchical models optimized for embedded deployment-that promise to advance both the capabilities and the applicability of VLA systems across robotics, autonomous driving, and beyond.</p>
<p>The classification in Table 1 is significant because it provides a clear framework for comparing diverse VLA architectures, highlighting how design choices-such as end-to-end integration versus hierarchical decomposition-impact task performance, scalability, and adaptability.By categorizing models along dimensions like low-level policy execution and highlevel planning, researchers can pinpoint strengths and limitations of existing approaches and identify opportunities for innovation.This taxonomy aids in selecting appropriate architectures for specific applications (e.g., real-time control vs. strategic reasoning) and guides future development toward hybrid systems that balance responsiveness with cognitive planning, ultimately accelerating progress in embodied AI.Additionally, to synthesize recent advancements in VLA models, Table 2 presents a comparative summary of notable systems developed from 2022 through 2025.Building upon architectural innovations such as early fusion, dual-system processing, and selfcorrecting feedback loops, these models incorporate diverse design philosophies and training strategies.Each entry highlights the model's key components-vision and language encoders, action decoders-and the datasets used to ground their capabilities.Models like CLIPort [157] and RT-2 [224] laid early foundations by aligning semantic embeddings with action policies, while more recent frameworks like Pi-Zero, CogACT [102], and Groot N1 [13] introduce scalable architectures with diffusion-based or high-frequency controllers.Several models leverage multimodal pretraining with internet-scale visionlanguage corpora and robot trajectory datasets, enhancing generalization and zero-shot capabilities [223,219,218,198].This tabulated comparison serves as a reference point for researchers seeking to understand the functional diversity, domain applicability, and emerging trends in VLA design across real and simulated environments.</p>
<p>Table 1: Taxonomy of VLA models showing structured classification based on architectural paradigms and scientific priorities.We differentiate models by their support for end-to-end execution, hierarchical planning-control decomposition, or component-focused modularity, and further by their emphasis on low-level motor policies versus high-level task planners.</p>
<p>Model Name</p>
<p>Year Endto-End Gato [141] • Vision Encoder: ViT
Hie rarc hi- cal Comp onent Fo- cused Low- Level Pol- icy High- Level Plan- ner CLIPort [157] ✓ ✗ ✗ ✓ ✗ RT-1 [18] ✓ ✗ ✗ ✓ ✗ Gato [141] ✓ ✗ ✗ ✓ ✗ VIMA [86] ✓ ✗ ✗ ✓ ✗ Diffusion Policy [34] ✓ ✗ ✗ ✓ ✗ ACT [216] ✓ ✗ ✗ ✓ ✗ VoxPoser [78] ✓ ✗ ✗ ✓ ✗ Seer [63] ✓ ✗ ✗ ✓ ✗ Octo [167] ✓ ✗ ✗ ✓ ✗ OpenVLA [94] ✓ ✗ ✗ ✓ ✗ CogACT [102] ✗ ✓ ✗ ✓ ✓ VLATest [182] ✗ ✗ ✓ ✗ ✗ NaVILA [32] ✗ ✓ ✗ ✓ ✓ RoboNurse- VLA [103] ✓ ✗ ✗ ✓ ✗ Mobility VLA [35] ✗ ✓ ✗ ✓ ✓ RevLA [39] ✗ ✗ ✓ ✗ ✗ Uni-NaVid [210] ✗ ✓ ✗ ✓ ✓ RDT-1B [112] ✓ ✗ ✗ ✓ ✗ RoboMamba [111] ✓ ✗ ✗ ✓ ✗ Chain-of- Affordance [100] ✗ ✗ ✓ ✗ ✗ Edge VLA [19] ✗ ✗ ✓ ✗ ✗ ShowUI-2B [108] ✓ ✗ ✗ ✓ ✗ Pi-0 [14] ✓ ✗ ✗ ✓ ✗ FAST (Pi-0 Fast) [133] ✗ ✗ ✓ ✓ ✗ OpenVLA-OFT [93] ✓ ✗ ✗ ✓ ✗ CoVLA [5] ✗ ✓ ✗ ✓ ✓ OpenDriveVLA [220] ✗ ✓ ✗ ✓ ✓ ORION [56] ✗ ✓ ✗ ✓ ✓ UAV-VLA [150] ✗ ✓ ✗ ✓ ✓ CombatVLA [29] ✓ ✗ ✗ ✓ ✗ HybridVLA [110] ✗ ✓ ✗ ✓ ✓ NORA [79] ✓ ✗ ✗ ✓ ✗ SpatialVLA [136] ✗ ✗ ✓ ✓ ✗ MoLe-VLA [213] ✗ ✗ ✓ ✓ ✗ JARVIS-VLA [101] ✓ ✗ ✗ ✓ ✗ UP-VLA [209] ✓ ✗ ✗ ✓ ✗ Shake-VLA [92] ✗ ✗ ✓ ✓ ✗ DexGraspVLA [219] ✗ ✓ ✗ ✓ ✓ DexVLA [185] ✗ ✓ ✗ ✓ ✓ Humanoid-VLA [42] ✓ ✗ ✗ ✓ ✗ ObjectVLA [223] ✓ ✗ ✗ ✓ ✗
• Language Encoder: Sen-tencePiece
• Action Decoder: Trans- former Self- collected [SC]
Generalist agent handling Atari, captioning, and robotics through unified tokenization.</p>
<p>VIMA [86] • Vision Encoder: ViT + Mask R-CNN</p>
<p>• Language Encoder: T5</p>
<p>• Action Decoder: Transformer</p>
<p>VIMA-Data [SC]</p>
<p>Multi-modal prompt handling with 6 types of vision-language grounding tasks.</p>
<p>ACT [216] • Vision Encoder: ResNet-18</p>
<p>• Language Encoder: -
• Action Decoder: CVAE- Transformer ALOHA [SC]
Temporal ensembling for smooth bimanual manipulation with 0.1mm precision.</p>
<p>Octo [167] • Vision Encoder: CNN</p>
<p>• Language Encoder: T5base</p>
<p>• Action Decoder: Diffusion Transformer Open X-Embodiment</p>
<p>First policy trained on 4M+ robot trajectories from 22 robot types.</p>
<p>VoxPoser [78] • Vision Encoder: ViLD + MDETR</p>
<p>• Language Encoder: GPT-4</p>
<p>• Action Decoder: MPC Zero-shot LLM+VLM composition for constraintaware motion planning without training.</p>
<p>Diffusion</p>
<p>Policy [34] • Vision Encoder: ResNet-18 • Data-Efficient Learning.</p>
<p>-Co-fine-tuning on massive vision-language corpora (e.g.LAION-5B) and robotic trajectory collections (e.g.Open X-Embodiment) aligns semantic understanding with motor skills.OpenVLA (7 B params) achieves a 16.5 % higher success rate than a 55 Bparameter RT-2 variant, demonstrating that co-finetuning yields strong generalization with fewer parameters [152,175,94].</p>
<p>-Synthetic Data Generation via UniSim produces photorealistic scenes-including occlusions and dynamic lighting-to augment rare edge-case scenarios, improving model robustness in cluttered environments by over 20 % [200,167].</p>
<p>-Self-Supervised Pretraining adopts contrastive objectives (à la CLIP) to learn joint visual-text embeddings before action fine-tuning, reducing reliance on task-specific labels.Qwen2-VL leverages self-supervised alignment to accelerate downstream grasp-and-place convergence by 12 % [137,76].</p>
<p>• Parameter-Efficient Adaptation.Low-Rank Adaptation (LoRA) inserts lightweight adapter matrices into frozen transformer layers, cutting trainable weights by up to 70 % while retaining performance [72].The Pi-0 Fast variant uses merely 10 M adapter parameters atop a static backbone to deliver continuous 200 Hz control with negligible accuracy loss [133].</p>
<p>• Inference Acceleration.</p>
<p>-Compressed Action Tokens (FAST) and Parallel Decoding in dual-system frameworks (e.g.Groot N1) yield 2.5× faster policy steps, achieving sub-5 ms latencies at a modest cost to trajectory smoothness [13,161].</p>
<p>-Hardware-Aware Optimizations-including tensorcore quantization and pipelined attention kernels-shrink runtime memory footprints below 8 GB and enable real-time inference on embedded GPUs [93].</p>
<p>Together, these methods have transformed VLAs into practical agents capable of handling language-conditioned, visionguided tasks in dynamic, real-world settings.</p>
<p>Parameter-Efficient Methods and Acceleration Techniques</p>
<p>in VLA Models Building on advances in data-efficient training, recent work has focused on reducing the parameter footprint and improving inference speed of VLA models-critical for deployment on resource-constrained robotic platforms.The iRe-VLA framework alternates between reinforcement learning (RL) in simulation and supervised finetuning on human demonstrations to stabilize policy updates.By leveraging Direct Preference Optimization (DPO) to shape reward models and Conservative Q-Learning to avoid extrapolation error, iRe-VLA reduces sample complexity by 60 % versus pure RL, while maintaining the semantic fidelity imparted by languageconditioned priors [123,65].This hybrid approach yields robust policies for tasks with sparse feedback, such as dynamic obstacle avoidance.7. Hardware-Aware Optimizations.Compiler-level graph rewrites and kernel fusion (e.g. via NVIDIA TensorRT-LLM) exploit target hardware features-tensor cores, fused attention, and pipelined memory transfers-to accelerate both transformer inference and diffusion sampling.</p>
<p>In OpenVLA-OFT, such optimizations reduced inference latency by 30 % on RTX A2000 GPUs and lowered energy per inference by 25 % compared to standard PyTorch execution [93].This makes real-time VLAs feasible on mobile robots and drones with strict power budgets.</p>
<p>Discussion.Parameter-efficient adaptation and inference acceleration techniques collectively democratize VLA deployment:</p>
<p>• LoRA and quantization empower smaller labs to fine-tune and operate billion-parameter VLAs on consumer-grade hardware, unlocking cutting-edge semantic understanding for robots [72,94].</p>
<p>• Pruning and FAST tokenization compress model and action representations, enabling sub-4 GB, sub-5 ms control loops without sacrificing precision in dexterous tasks [112,133].</p>
<p>• Parallel decoding and action chunking overcome sequential bottlenecks of autoregressive policies, supporting 100-200 Hz decision rates needed for agile manipulation and legged locomotion [13,161].</p>
<p>• Hybrid RL-SL training stabilizes exploration in complex environments, while hardware-aware compilation ensures real-time performance on edge accelerators [123,93].</p>
<p>Together, these advances make it practical to embed VLA models across industrial manipulators, assistive drones, and consumer robots, bridging the gap from research prototypes to real-world autonomy.</p>
<p>Applications of Vision-Language-Action Models</p>
<p>VLA models are rapidly emerging as foundational building blocks for embodied intelligence, integrating perception, natural language understanding, and motor control within a unified architecture.By encoding visual and linguistic modalities into shared semantic spaces and generating contextually grounded actions, VLA models enable seamless interaction between agents and their environments [102,220].This multimodal capacity has positioned VLAs as transformative agents across a wide spectrum of real-world applications.In humanoid robotics, systems like Helix and RoboNurse-VLA combine vision, language, and dexterous manipulation to assist with domestic tasks and surgical operations, demonstrating real-time reasoning and safety-aware control [103,186].In autonomous vehicles, models such as OpenDriveVLA and ORION process dynamic visual streams and natural language instructions to make transparent, adaptive driving decisions in complex urban environments [56,220].Industrial deployments leverage VLA architectures for high-precision assembly, inspection, and collaborative manufacturing [102].In agriculture, VLA-powered robotic systems enable vision-guided fruit harvesting, plant monitoring, and anomaly detection, reducing labor dependency and increasing sustainability.Furthermore, recent advances in interactive augmented reality systems utilize VLA models for real-time, language-conditioned spatial navigation, guiding users in indoor and outdoor settings based on voice or visual cues [150,59].Across these domains, VLAs offer a unified framework for robust, adaptable, and semantically aligned task execution, marking a pivotal shift toward embodied generalist agents.</p>
<p>Table 3 in the appendix shows the recent VLA models by summarizing their methodologies, application domains, and key innovations.</p>
<p>The following subsections chronologically explore the application areas in depth as shown in Figure 11.</p>
<p>Applications of VLA</p>
<p>Humanoid Robotics</p>
<p>Autonomous Vehicle Systems</p>
<p>Industrial Robotics</p>
<p>Healthcare &amp; Medical Robotics</p>
<p>Precision &amp; Automated Agriculture</p>
<p>Interactive AR Navigation</p>
<p>Figure 11: Mind-map of application domains for Vision-Language-Action models.</p>
<p>Humanoid Robotics</p>
<p>Humanoid robots, designed to mimic the form and functionality of the human body, represent one of the most demanding yet impactful domains for the deployment of VLA models.These platforms must seamlessly perceive complex environments, understand spoken or written natural language, and perform intricate physical tasks with human-level dexterity [144,22].The core strength of VLA models lies in their ability to unify perception, cognition, and control into a single, endto-end trainable framework-allowing humanoid robots to interpret visual inputs (e.g., RGB-D imagery of cluttered scenes), comprehend linguistic instructions (e.g., "place the spoon in the drawer"), and generate precise motor trajectories [118,222].</p>
<p>Recent advances have significantly accelerated the deployment of VLAs in humanoid robotics.For example, Helix 2 , a humanoid robot developed by Figure AI, leverages a fully integrated VLA model to perform full-body manipulation at high frequency, controlling arms, hands, torso, and even fine-grained finger motion in real time.The architecture follows a dualsystem design: a multimodal transformer processes inputs such as language commands and vision streams, while a real-time motor policy outputs dense action vectors at 200 Hz.This allows Helix to generalize across previously unseen objects and tasks, adapting fluidly to changing environments without the need for task-specific retraining.The key advantage of VLAs in humanoid systems is their ability to scale across diverse tasks using shared representations [8].Unlike traditional robotic systems that rely on taskspecific programming or modular pipelines, VLA-powered humanoids operate under a unified token-based framework.Vision inputs are encoded via pretrained vision-language models like DINOv2 or SigLIP, while instructions are processed using large language models such as Llama-2 or GPT-style encoders.These representations are fused into prefix tokens that capture the full context of the scene and task.Action tokens are then generated autoregressively, similar to language decoding, but represent motor commands for the robot's joints and effectors.</p>
<p>This capability enables humanoid robots to operate effectively in human-centric spaces, such as households, hospitals, and retail environments.In domestic settings, VLA-powered robots can clean surfaces, prepare simple meals, or organize objects simply by interpreting voice commands [118,222].In healthcare, systems like RoboNurse-VLA [103] have demonstrated the ability to perform precise instrument handovers to surgeons using real-time voice and visual cues.In retail, humanoid platforms equipped with VLAs can assist with customer queries, restock shelves, and navigate store layouts without explicit pre-programming [8].</p>
<p>What distinguishes modern humanoid VLAs is their ability to run on embedded, low-power hardware, making real-world deployment viable.For instance, systems such as TinyVLA [186] and MoManipVLA [190] demonstrate efficient inference pipelines that run on Jetson-class GPUs, enabling mobile deployment without compromising performance.These models exploit techniques like diffusion-based policies, LoRA-based fine-tuning, and dynamic token caching to minimize compute cost while retaining high precision and generalization.</p>
<p>In logistics and manufacturing, humanoid VLAs are already making a commercial impact.Robots like Figure 01 are deployed in warehouses to perform repetitive, physically intensive tasks-such as picking, sorting, and shelving-alongside human workers.Their ability to handle novel object categories and dynamically changing scenes is powered by continual learning and robust multimodal grounding [195,102].</p>
<p>As VLA models continue to advance in their capacity for diverse action generation, spatial reasoning, and real-time adaptation, humanoid robots are emerging as highly capable assistants across homes, industrial settings, and public spaces.Their strength lies in their ability to unify perception, language comprehension, and motor control through a shared token-based architecture-enabling seamless, context-aware behavior in unstructured human environments.</p>
<p>For example, as depicted in the figure 12, consider 'Helix', a state-of-the-art humanoid robot equipped with a nextgeneration VLA model.When instructed verbally, "Please take the water bottle from the fridge," Helix activates its integrated perception system, where a foundation vision-language model (e.g., SigLIP or DINOv2) segments the visual scene to identify the refrigerator, its handle, and the bottle.The language input is processed by a large language model such as LLaMA-2, which tokenizes the instruction and fuses it with the visual context.This fused representation is passed to a hierarchical controller: the high-level policy plans the task sequence (locate handle, pull door, identify bottle, grasp), while a mid-level planner defines motor primitives, such as grasp type and joint trajectories.The low-level VLA controller-often based on diffusion policy networks-executes these actions with sub-second latency.Upon encountering variations (e.g., a tilted bottle or slippery grip), Helix's agentic AI module performs micro-policy refinement in real time, adjusting its grip based on feedback.This example illustrates the transformative potential of humanoid VLAs.From kitchens to clinics, these systems not only interpret complex instructions and execute physical tasks with dexterity but also adapt to environmental unpredictability.By embedding agentic reasoning and safety alignment mechanisms, modern humanoid robots powered by VLAs are transitioning from narrow-task performers to generalist, trustworthy collaborators.As energy-efficient models like TinyVLA and MoMa-nipVLA mature, deployment on mobile, low-power platforms becomes increasingly practical-ushering in a new era of embodied, socially aligned AI.</p>
<p>Autonomous Vehicle Systems</p>
<p>Autonomous vehicles (AVs), including self-driving cars, trucks, and aerial drones, represent a frontier application domain for VLA models, where safety-critical decision-making demands tightly coupled perception, semantic understanding, and real-time action generation.Unlike traditional modular AV pipelines that decouple perception, planning, and control, VLA frameworks offer an integrated architecture that processes multimodal inputs-including visual streams, natural language instructions, and internal state information-within a unified autoregressive model capable of outputting precise control signals.</p>
<p>VLA models empower AVs to comprehend complex environments beyond pixel-level object recognition.For instance, a self-driving car navigating an urban setting must detect traffic signs, understand pedestrian behavior, and interpret navigation commands such as "take the second right after the gas station."These tasks involve fusing visual and linguistic signals to understand spatial relationships, predict intent, and generate context-aware driving actions.VLAs encode this information through token-based representations, where visual encoders (e.g., ViT, CLIP), language models (e.g., LLaMA-2), and trajectory decoders operate in a coherent semantic space, enabling the vehicle to reason about high-level goals and translate them into low-level motion.</p>
<p>A notable contribution in this direction is CoVLA [5], which provides a comprehensive dataset pairing over 80 hours of realworld driving videos with synchronized sensor streams (e.g., LiDAR, odometry), detailed natural language annotations, and high-resolution driving trajectories.This dataset enables training VLA models to align perceptual and linguistic features with physical actions.CoVLA employs CLIP for visual grounding, LLaMA-2 for instruction embedding, and trajectory decoders for motion prediction.This configuration allows AVs to interpret verbal cues (e.g., "yield to ambulance") and environmental conditions (e.g., merging traffic) to make transparent and safe driving decisions.</p>
<p>OpenDriveVLA [220] advances the state of VLA modeling by integrating hierarchical alignment of 2D/3D multi-view vision tokens with natural language inputs.Its architecture leverages both egocentric spatial perception and external scene understanding to construct a dynamic agent-environment-ego interaction model.Through autoregressive decoding, Open-DriveVLA generates both action plans (e.g., steering angle, acceleration) and trajectory visualizations interpretable to humans.Its end-to-end framework achieves state-of-the-art performance on planning benchmarks and question-answering tasks related to driving scenarios, demonstrating its robustness in urban navigation and behavioral prediction.</p>
<p>Another seminal model, ORION [56], pushes the boundaries of closed-loop autonomous driving by incorporating a QT-Former to retain long-horizon visual context, a large language model for reasoning over traffic narratives, and a generative trajectory planner.ORION excels at aligning the discrete reasoning space of vision-language models with the continuous control space of AV motion.This unified optimization results in accurate visual question answering (VQA) and trajectory planning, crucial for scenarios involving ambiguous human instructions or occluded obstacles (e.g., "take the exit after the red truck").</p>
<p>For example, as depicted in Figure 13 consider an autonomous delivery vehicle, "AutoNav," operating in a dense urban environment using a next-generation VLA architecture.As AutoNav receives a cloud-based instruction-"Drop off the package near the red awning beside the bakery, then return to base avoiding construction zones"-its onboard VLM (e.g., CLIP or SigLIP) parses the visual stream from multiple cameras, identifying dynamic landmarks such as bakery signs, red awnings, and traffic cones.Simultaneously, the LLM module grounded in LLaMA-2 decodes the instruction and fuses it with real-time sensory context including LiDAR, GPS, and inertial odometry.A hierarchical control stack processes these multimodal signals via an autoregressive VLA decoder that integrates egocentric views and world-centric maps to plan adaptive paths.As the vehicle approaches the delivery location, unexpected pedestrian activity prompts an agentic submodule to trigger trajectory re-planning using a reinforcement learning-inspired policy refinement routine.At the same time, AutoNav audibly warns pedestrians and recalibrates its speed to maintain safety margins.This interplay of semantic understanding, perceptual grounding, and adaptive control exemplifies the power of VLA-based systems in achieving interpretable, human-aligned behavior in safety-critical scenarios.It also demonstrates how such integration can surpass traditional perception-planning-control pipelines in autonomy, transparency, and decision-making agility.</p>
<p>In aerial robotics, VLAs enhance the capabilities of delivery drones and UAVs.Models such as UAV-VLA [150] combine satellite imagery, natural language mission descriptions, and onboard sensing to execute high-level commands (e.g., "deliver to the rooftop pad with the blue tarp").These systems use modular VLA architectures, where a vision-language planner parses global context and a flight controller executes precise waypoints, supporting applications in logistics, disaster response, and military reconnaissance.</p>
<p>As autonomous systems increasingly operate in unstructured environments, VLAs provide a scalable, interpretable, and dataefficient alternative to traditional pipelines.By learning from large-scale multimodal datasets and modeling decision-making as token prediction, VLAs align human-level semantics with robotic motion, paving the way for safer, smarter autonomous</p>
<p>VLA Driving</p>
<p>Agentic AI VLA decoder Figure 13: This illustration depicts an autonomous delivery vehicle powered by a VLA system, integrating VLMs for visual grounding, LLMs for instruction parsing, and a VLA decoder for path planning.Agentic AI enables adaptive trajectory refinement in dynamic environments, exemplifying how multimodal integration drives safe, interpretable, and autonomous decision-making in realworld navigation tasks.</p>
<p>driving and navigation technologies.</p>
<p>Industrial Robotics</p>
<p>Industrial robotics is undergoing a paradigm shift with the integration of VLA models, enabling a new generation of intelligent robots capable of high-level reasoning, flexible task execution, and natural communication with human operators [27,7].Traditional industrial robots typically operate in highly structured environments using rigid programming, often requiring extensive reconfiguration and manual intervention when adapting to new assembly lines or product variants [6,142].Such systems lack the semantic grounding and adaptability required for modern dynamic manufacturing settings.</p>
<p>VLA models, by contrast, offer a more human-interpretable and generalizable framework.Through the joint embedding of visual inputs (e.g., component layout or conveyor belt state), natural language instructions (e.g., "tighten the screw on the red module"), and robot state, VLAs can infer context and execute appropriate control commands in real-time [105,58,121].Vision transformers (e.g., ViT, DINOv2), large language models (e.g., LLaMA-2), and autoregressive or diffusion-based action decoders form the backbone of these systems, allowing the robot to parse multimodal instructions and perform actions grounded in its environment.</p>
<p>One of the most significant contributions in this domain is CogACT [102], a componentized VLA framework explicitly designed for industrial robotic manipulation.Unlike early VLAs that relied on frozen language-vision embeddings followed by direct action quantization, CogACT introduces a diffusion-based action transformer that models action sequences more robustly and adaptively.The system uses a visual-language encoder (e.g., Prismatic-7B) to extract highlevel scene and instruction embeddings, which are then passed to a diffusion transformer (DiT-Base) to generate fine-grained motor actions.This modular separation enables superior generalization to unseen tools, parts, and layouts while preserving interpretability and robustness under real-world constraints.</p>
<p>Furthermore, CogACT demonstrates rapid adaptation across different robot embodiments-such as 6-DoF arms or bimanual systems-through efficient fine-tuning, making it suitable for deployment across heterogeneous factory environments [102].Empirical evaluations show that CogACT outperforms prior models like OpenVLA by over 59% in real-world task success rates, especially in complex, high-precision tasks such as multistep assembly, screw fastening, and part sorting.</p>
<p>As manufacturing shifts toward Industry 4.0 paradigms, VLAs promise to reduce programming overhead, support voice-commanded robot programming, and facilitate real-time human-robot collaboration on mixed-initiative tasks.While execution precision, safety guarantees, and latency optimizations remain areas of active research, the use of VLA models in industrial robotics marks a substantial step toward autonomous, intelligent, and adaptable robotic factories.</p>
<p>Healthcare and Medical Robotics</p>
<p>Healthcare and medical robotics represent a high-stakes domain where precision, safety, and adaptability are paramount-qualities that VLA models are increasingly well-suited to provid [103,151].Traditional medical robotic systems rely heavily on teleoperation or pre-programmed behaviors [130,158], limiting their autonomy and responsiveness in dynamic surgical or care environments.In contrast, VLA models offer a flexible framework that integrates real-time visual perception, language comprehension, and fine-grained motor control, enabling medical robots to understand high-level instructions and autonomously perform intricate procedures or assistance tasks [102,43,174].</p>
<p>In surgical robotics, VLAs can dramatically enhance capabilities in minimally invasive operations [40,177].These systems can fuse laparoscopic video feeds [98], anatomical maps [114,40], and voice commands into a unified tokenized representation using vision encoders (e.g., ViT, SAM-2) and language models (e.g., LLaMA, T5) [181].For instance, as depicted in Figure 14a, in a task like "apply a suture to the left coronary artery," the vision module identifies the anatomical target, while the language module contextualizes the instruction.The action decoder then translates the fused semantic embedding into stepwise motion commands with sub-millimeter precision.This enables the robot to adaptively reposition tools, apply dynamic force feedback, and avoid critical structures, reducing the need for surgeon micromanagement and minimizing risk of human error.</p>
<p>Beyond the operating room, VLA models are powering a new generation of patient-assistive robots in eldercare, rehabilitation, and hospital logistics.These systems can autonomously perceive patient behavior, understand spoken or gestural input, and execute responsive tasks such as retrieving medication, guiding mobility aids, or notifying caregivers during emergencies.For example, as depicted in Figure 14b, a VLA-enabled robot can visually detect a patient attempting to rise from bed,</p>
<p>(a) (b)</p>
<p>Figure 14: a) This figure illustrates a VLA surgical system executing the task "apply a suture to the left coronary artery."The vision module identifies anatomical targets, the language model interprets the instruction, and the action decoder generates precise motor commands, enabling adaptive tool control, real-time feedback, and safe autonomous operation; b) A VLA-powered assistive robot perceives patient behavior, processes verbal requests (e.g., "bring my walker"), and autonomously executes context-aware motion plans, enabling real-time assistance in eldercare, rehabilitation, and hospital logistics without relying on predefined scripts or manual oversight.interpret a verbal request such as "bring my walker," and generate a context-appropriate motion plan to assist-without predefined scripts or constant supervision.</p>
<p>Recent VLA frameworks such as RoboNurse-VLA [103] highlight the real-world feasibility of this approach.RoboNurse employs SAM-2 for semantic scene segmentation and LLaMA-2 for command comprehension, integrated into a real-time voice-to-action pipeline that enables robots to assist with surgical instrument handovers in operating rooms [103].The system demonstrates robustness to diverse tools, varied lighting conditions, and noisy environments-common challenges in clinical settings.</p>
<p>Additionally, VLA architectures offer advantages in explainability and auditability, both critical in regulated medical domains [173,113].Scene grounding and trajectory prediction can be visualized and reviewed post-hoc [208], which could facilitate clinical trust and enabling FDA-style validation pipelines.LoRA-based fine-tuning allows adaptation to specific hospital environments or procedural workflows with minimal data and compute [9,176,114].</p>
<p>Importantly, the multimodal foundation of VLA models enables cross-domain transferability: the same model trained on surgical tool manipulation can be adapted to patient mobility tasks with modest retraining [45].This modularity significantly reduces development time and cost compared to task-specific automation systems [73].As medical robotics transitions from teleoperated assistance to semi-autonomous and collaborative systems, VLA models stand at the core of this transformation.</p>
<p>By combining high-level semantic understanding with lowlevel control, VLAs provide a unified solution for scalable, human-aligned, and adaptive robotic healthcare [193,221,209].As healthcare systems face increasing demand and workforce shortages, VLA-driven robotics will play a crucial role in enhancing medical precision, operational efficiency, and patient-centered care.</p>
<p>Precision and Automated Agriculture</p>
<p>As illustrated in Figure 15, VLA models are emerging as transformative tools in precision and automated agriculture, offering intelligent, adaptive solutions for labor-intensive tasks across diverse farming landscapes [57,150].Unlike traditional agricultural automation systems that depend on rigid, sensordriven pipelines and require manual reprogramming for each task or environmental variation [169,84], VLAs integrate multimodal perception, natural language understanding, and realtime action generation within a unified framework [131,66].This enables autonomous ground robots and drones to interpret complex field scenes, follow spoken or text-based farming instructions, and generate context-aware actions such as selective fruit picking or adaptive irrigation.The ability of VLAs to dynamically adjust to occlusions, terrain irregularities, or varying crop types-combined with training on synthetic, photorealistic datasets-allows them to generalize across geographies and seasons.By leveraging action tokenization [189], transformer-based policy generation [11,67], and techniques like LoRA fine-tuning [72], these systems are redefining the scalability and intelligence of agricultural robotics for sustainable and precision-driven farming.</p>
<p>In modern orchards and crop fields, VLAs can process visual inputs from RGB-D cameras, multispectral sensors, or drones to monitor plant growth, detect diseases, and identify nutrient deficiencies.Vision transformers (e.g., ConvNeXt, DINOv2) encode spatial and semantic information from visual scenes, while large language models (e.g., T5, LLaMA) parse natural language commands-such as "inspect the east plot for powdery mildew" or "harvest ripe apples near the irrigation trench."Through token fusion, these modalities are aligned in a shared representation space, allowing robots to execute fine-grained, context-aware actions with precision.</p>
<p>For instance, in fruit-picking tasks, as illustrated in Figure 15, a VLA-equipped ground robot can identify ripe produce using image-based ripeness cues, interpret user-specified criteria such as "pick only Grade A fruits," and execute motion sequences via action tokens that control its end-effector.This approach ensures minimal crop damage, optimizes pick rates, and allows real-time adaptation to unexpected variables like occlusions or terrain shifts.In irrigation management, drones guided by VLA models can interpret field maps and verbal instructions to selectively water stressed zones, reducing water usage by up to 30%.</p>
<p>Moreover, VLA models support dynamic reconfiguration and lifelong learning.With access to synthetic training datasets generated from photorealistic simulations of crop environments (e.g., 3D orchard renderings), models can be trained to recognize pests, weeds, and crop maturity stages without extensive manual annotation.Techniques like LoRA adapters and diffusion-based policy tuning further enhance generalization to novel crops, seasons, and geographical regions.</p>
<p>The integration of VLAs into agricultural workflows offers significant benefits: reduced dependence on skilled labor, higher yield through targeted intervention, and enhanced environmental sustainability through optimized input usage.As global food systems grapple with climate variability and resource constraints, VLA-enabled agriculture will play a pivotal role in advancing scalable, intelligent, and sustainable farming practices tailored to real-world complexity.</p>
<p>Interactive AR Navigation with Vision-Language-Action</p>
<p>Models Interactive Augmented Reality (AR) navigation represents a frontier where the VLA models can significantly enhance human-environment interaction by providing intelligent, context-aware guidance in real-time [26,80,197].In this paradigm, VLAs process continuous streams of visual data from AR-enabled devices-such as smart glasses or smartphones-alongside natural language queries to generate dynamic navigational cues overlaid directly onto the user's view of the physical world.Unlike traditional GPS-based systems that rely on rigid maps and limited user input [24,159], VLAbased AR agents interpret complex visual scenes (e.g., intersections, indoor hallways, signage) and respond to free-form instructions such as "take me to the nearest pharmacy with a wheelchair ramp" or "show the quietest route to the conference room."</p>
<p>Technically, these models integrate a vision encoder (e.g., ViT, DINOv2) that extracts scene representations from firstperson RGB frames, a language encoder (e.g., T5 or LLaMA) that processes user prompts or voice commands, and an action decoder that predicts tokenized navigation cues such as directional overlays, waypoints, or voice instructions.A transformer-based architecture fuses these modalities to reason about both the spatial layout and semantic intent, allowing the AR agent to adaptively highlight paths, landmarks, and hazards directly within the user's field of view [163,129].For example, as shown in Figure 16, in a crowded airport, the VLA agent could visually identify escalators, gates, or baggage claims while understanding a query like "how do I reach Gate 22 without stairs?",adjusting the route in response to real-time occupancy and obstacles.</p>
<p>VLAs also support interaction loops that enable users to refine instructions (e.g., "avoid busy areas" or "take the scenic route") and receive context-aware feedback, improving accessibility for the visually impaired or cognitively challenged.In logistics and indoor navigation, these systems can be integrated with IoT sensors and digital twins to guide warehouse workers, maintenance teams, or delivery robots through complex environments.Furthermore, personalized navigation can be achieved through continual fine-tuning, where VLA models learn user preferences and local spatial layouts over time.</p>
<p>As AR hardware becomes more affordable and integrated into daily life, VLA-powered navigation systems will enable seamless spatial understanding, multimodal interaction, and autonomous guidance in public, industrial, and assistive contexts-redefining how humans perceive, explore, and interact with physical spaces.</p>
<p>Challenges and Limitations of Vision-Language-Action Models</p>
<p>VLA models face a spectrum of interrelated challenges that impede their translation from research prototypes to robust, real-world systems.First, achieving real-time, resource-aware inference remains difficult: models like DeeR-VLA leverage dynamic early-exit architectures to cut computation 5-6× on manipulation benchmarks while preserving accuracy, yet their gains diminish in complex scenarios [202].Similarly, Uni-NaVid compresses egocentric video tokens for 5 Hz navigation but still struggles under highly ambiguous instructions and In dynamic environments such as airports, VLAs interpret user queries like "avoid stairs to Gate 22," analyze visual scenes (e.g., detecting escalators), and adjust navigational paths accordingly, supporting personalized, accessible, and context-aware mobility guidance.</p>
<p>longer horizons [210].Coupled with limited object generalization, even advanced hybrid vision-language grounding schemes (e.g., ObjectVLA) generalize to only 64 % of novel objects, un-derscoring persistent gaps in open-world robustness [223].</p>
<p>Second, adapting VLA models with minimal supervision and ensuring stable policy updates under scarce, noisy data is nontrivial.ConRFT combines behavior cloning and Qlearning with human-in-the-loop fine-tuning to rapidly converge to 96.3% success over eight contact-rich tasks, yet it relies heavily on expert interventions and reward shaping [31].Hierarchical frameworks such as Hi Robot decouple high-level reasoning from low-level execution to improve instruction fidelity, but coordinating these modules and grounding ambiguous feedback remains challenging [155].Likewise, TLA's fusion of tactile streams with language commands achieves over 85 % success on unseen peg-in-hole tasks, but dataset breadth and real-time multi-step decoding still limit broader generalization [70].</p>
<p>Furthermore, ensuring safety, generalization, and end-to-end reliability in dynamic environments demands new modeling and evaluation standards.Occupancy-Language-Action models like OccLLaMA unify 3D scene understanding with action planning, yet they must scale to richer scene dynamics and semantic consistency across modalities [183].RaceVLA pushes high-speed drone navigation via quantized, iterative control loops, but its visual-physical generalization trails larger VLAs and dedicated reasoners [153].Model-merging strategies in ReVLA recover lost out-of-domain visual robustness-improving OOD grasp success by up to 77 %-but introduce extra computation and complexity [39].Finally, SafeVLA formulates constraints via constrained Markov decision processes to cut unsafe behavior by over 80 %, yet defining comprehensive, non-restrictive safety rules for diverse real-world tasks remains an open problem [205].Addressing these intersecting limitations is critical for VLA models to achieve reliable, autonomous operation against the full complexity of realworld robotics.</p>
<p>Building upon the critical limitations outlined above, it is imperative to map each challenge to targeted mitigation strategies and forecast their system-level impact.Table 4 distills this mapping into three columns-identifying core limitations, proposing concrete technical remedies drawn from recent advances, and articulating the anticipated benefits for real-world VLA deployment.For instance, tackling real-time inference constraints leverages parallel decoding and quantized transformer pipelines with hardware acceleration (e.g., TensorRT) to sustain control loop rates in drones and manipulators [100,94,60,110].Addressing multimodal action representation via hybrid diffusion-autoregressive policies enriches a model's capacity to produce varied, context-sensitive motor commands for complex tasks [133,121].To guarantee safety in open worlds, dynamic risk assessment modules and adaptive planning layers can be integrated, ensuring robust emergency stop behaviors in unpredictable settings [143,180,87].Similarly, dataset bias and grounding are countered through curated debiased corpora and advanced contrastive fine-tuning, bolstering fairness and semantic fidelity when generalizing to novel objects and scenes [145,16,136].Together, these solution pathways-and others spanning simulation-to-real transfer, tactile integration, and energy-efficient architectures-frame a com-prehensive roadmap for transitioning VLA research into reliable, scalable autonomy.</p>
<p>The remainder of this section is organized into five focused subsections, each examining a distinct cluster of VLA challenges identified in the literature.First, we analyze real-time inference constraints and the emerging methods that address them.Next, we delve into multimodal action representation alongside safety assurance in open-world settings.We then discuss dataset bias, grounding strategies, and generalization to unseen tasks, followed by an exploration of system integration complexity and computational demands.Finally, we consider robustness and the ethical implications of deploying VLAs in real-world applications.</p>
<p>Real-Time Inference Constraints</p>
<p>Real-time inference remains a significant limitation in deploying VLA models, particularly in latency-critical applications like robotic manipulation, autonomous driving, and drone control.VLAs typically depend on autoregressive decoding strategies, which sequentially generate action tokens based on previous predictions.While effective for many tasks, this method severely restricts inference speed, typically achieving only 3-5 Hz.This rate falls dramatically short of the 100 Hz or greater frequency required by robotic systems for precise and fluid real-time control.For instance, when a robotic arm manipulates delicate objects, frequent positional updates are essential to maintain accuracy and prevent damage.Models such as OpenVLA [94] and Pi-0 [14] face inherent challenges with this sequential token generation approach, thereby limiting their effectiveness in dynamic environments.</p>
<p>Emerging solutions such as parallel decoding, exemplified by NVIDIA's Groot N1 model [13], aim to accelerate inference by predicting multiple tokens simultaneously.Groot N1 achieves approximately a 2.52× speedup over traditional decoding methods; however, this parallelism often introduces tradeoffs in trajectory smoothness, resulting in jerky or suboptimal robot movements.Such movements are undesirable in sensitive applications like surgical robotics, where precision and fluidity are paramount.Thus, achieving rapid inference without compromising output quality remains an open challenge.</p>
<p>Additionally, hardware limitations exacerbate real-time inference constraints.For example, processing high-dimensional visual embeddings, typically involving over 400 vision tokens at 512 dimensions each, requires approximately 1.2 GB/s memory bandwidth.This demand significantly exceeds the capacity of current embedded systems or edge-AI hardware such as NVIDIA Jetson platforms, thereby restricting practical deployment.Even with efficient quantization techniques, which reduce the precision of floating-point operations to alleviate memory constraints, models frequently experience accuracy degradation, especially in tasks demanding sub-millimeter precision, such as bimanual robotic manipulation or medical robotics.</p>
<p>Multimodal Action Representation and Safety Assurance</p>
<p>Multimodal Action Representation: One significant limitation of current VLA models is accurately representing mul-timodal actions, particularly in scenarios requiring continuous and nuanced control [51,38].Traditional discrete tokenization methods, such as those dividing actions into 256 distinct bins, inherently lack precision, creating substantial errors in fine-grained tasks like delicate robotic grasping or intricate surgical procedures [133].For instance, during precise robotic manipulation in assembly tasks, discrete representations can result in misaligned or imprecise actions, undermining performance and reliability.On the other hand, continuous multilayer perceptron (MLP) based approaches face the risk of mode collapse [126,179], where models converge prematurely to single action trajectories, despite multiple viable paths available.This diminishes the flexibility necessary for adaptive decisionmaking in highly dynamic environments.Emerging diffusionbased policies, exemplified by models like Pi-Zero and RDT-1B [112], offer richer multimodal action representation capable of capturing diverse action possibilities.However, their substantial computational overhead-approximately three times that of conventional transformer-based decoders-renders them impractical for real-time deployment.Consequently, VLA models currently struggle with complex dynamic tasks, such as robotic navigation in densely crowded spaces or sophisticated bimanual manipulations [59,191], where multiple strategic actions may be equally valid and contextually dependent.</p>
<p>Safety Assurance in Open Worlds: Another critical challenge facing VLAs is ensuring robust safety in dynamic, unpredictable environments characteristic of real-world scenarios [33,205].Many current implementations depend heavily on predefined hardcoded force and torque thresholds, significantly constraining their adaptability in encountering unforeseen or novel conditions, such as unexpected obstacles or sudden environmental changes [121].Models used for collision prediction typically attain only about 82% accuracy in cluttered and dynamic spaces, posing serious risks in applications such as warehouse logistics or domestic robotics, where safety margins are minimal [217,94].Moreover, the essential safety mechanisms like emergency stops incorporate substantial latency-often between 200 and 500 milliseconds-due to comprehensive safety verifications [132,94].This delay, although seemingly minor, can prove hazardous in high-speed operations or critical interventions, such as automated driving or emergency robotic responses.</p>
<p>Dataset Bias, Grounding, and Generalization to Unseen</p>
<p>Tasks A significant obstacle limiting the effectiveness of VLA models is the pervasive presence of dataset bias and grounding deficiencies.Current training datasets, predominantly sourced from web-crawled repositories, frequently exhibit inherent biases [165,91].Studies indicate that approximately 17% of associations within standard datasets are skewed toward stereotypical interpretations, such as disproportionately associating terms like "doctor" with male figures [171,95].These biases propagate through training, resulting in VLAs that produce semantically misaligned or contextually inappropriate responses when deployed in diverse environments.For instance, models such as OpenVLA have been documented to overlook ap-proximately 23% of object references in novel settings, significantly impairing their practical utility in real-world applications where accurate interpretation of instructions is critical [94].This grounding issue also extends to challenges in compositional generalization, where VLAs often falter when encountering rare or unconventional combinations, such as interpreting a phrase like "yellow horse" because of underrepresentation in training corpora.These shortcomings highlight an urgent need for carefully curated, balanced, and comprehensive datasets, coupled with advanced grounding algorithms designed to mitigate biases and enhance semantic alignment across varied contexts.</p>
<p>Complementing the challenges posed by dataset bias is the broader issue of generalization to unseen tasks, a critical barrier for the practical deployment of VLAs.While existing models demonstrate proficiency in familiar environments or tasks similar to their training scenarios, their performance significantly degrades-often by as much as 40%-when encountering entirely novel tasks or unfamiliar variations.For example, a VLA trained specifically on domestic tasks may struggle or outright fail when introduced into industrial or agricultural settings, largely due to discrepancies in object types, environmental dynamics, and operational constraints.This limitation arises primarily from overfitting to narrowly scoped training distributions and insufficient exposure to diverse task representations.Consequently, current VLAs exhibit limited proficiency in zero-shot or few-shot learning scenarios, impeding their adaptability and scalability.</p>
<p>System Integration Complexity and Computational Demands</p>
<p>Integrating VLA models within dual-system architectures, which combine high-level cognitive planning (System 2) and real-time physical control (System 1), presents significant complexity in robotic applications.A primary challenge arises from temporal mismatches between these two systems.Typically, System 2 leverages large language models (LLMs) such as GPT or Llama-2 for complex task decomposition and strategic planning.These models, due to their substantial computational requirements, often exhibit processing times of approximately 800 ms or more per inference cycle.Conversely, System 1 components, tasked with executing rapid, low-level motor actions through control loops, operate on millisecond timescales-often around 10 ms intervals.This stark discrepancy in operational cadence leads to synchronization difficulties, causing delays and potentially suboptimal execution trajectories.For example, NVIDIA's Groot N1 model demonstrates an effective integration of these two systems but still suffers from occasional jerkiness in motion due to asynchronous interaction, highlighting this intrinsic challenge.</p>
<p>Furthermore, the feature space misalignment between highdimensional vision encoders, such as Vision Transformers (ViT), and lower-dimensional action decoders exacerbates integration complexity.When attempting to reconcile these disparate embeddings, the coherence between perceptual understanding and actionable commands can deteriorate significantly.OpenVLA [94] and RoboMamba [111], which utilize transformer-based visual processing and subsequent action decoding, illustrate these integration challenges-resulting in diminished performance when ported from simulation environments to physical hardware deployments.Such discrepancies manifest as high as a 32% reduction in performance, primarily due to mismatches between simulated dynamics and real-world sensor noise or calibration issues.</p>
<p>Energy and compute demands constitute another significant barrier for VLA deployment, particularly in edge computing contexts typical of autonomous drones, mobile robots, and wearable robotic systems.The substantial parameter counts typical of advanced VLAs-for instance, models possessing upwards of 7 billion parameters-necessitate computational resources often exceeding 28 GB of VRAM in their native form.These requirements vastly outpace the capabilities of most current edge-oriented processors and GPUs, restricting the practical applicability of sophisticated VLAs outside specialized, high-resource environments.</p>
<p>Robustness and Ethical Challenges in VLA Deployment</p>
<p>The practical deployment of VLA models faces substantial challenges regarding robustness to environmental variability and ethical considerations.Environmental robustness refers to the VLA's capacity to maintain stable and accurate performance across dynamically changing conditions.Real-world environments frequently introduce unpredictable variations such as fluctuating lighting, weather conditions, or partial occlusions.For instance, vision modules within VLAs, such as those employed by OpenDriveVLA [220], exhibit accuracy reductions of approximately 20-30% under low-contrast or shadow-heavy scenarios due to inadequate processing capabilities of current visual encoders.Similarly, linguistic comprehension in VLAs like CoVLA [5] is adversely affected in acoustically noisy or ambiguous contexts, where instructions can become difficult to interpret accurately, leading to task execution errors.Additionally, robotic manipulation tasks using VLA-equipped systems such as RoboMamba [111] frequently struggle with cluttered environments, misjudging positions or orientations of partially occluded objects, thereby compromising task success.</p>
<p>Discussion</p>
<p>As illustrated in Figure 17, VLA models face a multifaceted set of challenges that span algorithmic, computational, and ethical dimensions.First, achieving real-time inference on resource-constrained hardware remains difficult due to the sequential nature of autoregressive decoders and the high dimensionality of multimodal inputs.Second, fusing vision, language, and action into coherent policies introduces safety vulnerabilities when encountering unanticipated environmental changes.Third, dataset bias and grounding errors compromise generalization, often causing models to fail on out-of-distribution tasks.Fourth, integrating diverse components-perception, reasoning, control-yields complex architectures that are hard to optimize and maintain.Fifth, the energy and compute demands of large VLA systems hinder de-ployment on embedded or mobile platforms.Finally, robustness to environmental variability and ethical considerations, such as privacy and bias mitigation, add layers of societal and regulatory concern.Collectively, these limitations constrain the practical adoption of VLA models in real-world robotics, autonomous systems, and interactive applications.The potential solutions to these challenges are discussed in the below points.</p>
<p>Potential Solutions</p>
<p>• Real-Time Inference Constraints.Future research must develop VLA architectures that harmonize latency, throughput, and task-specific accuracy.One promising direction is the integration of specialized hardware accelerators-such as FPGA-based vision processors and tensor cores optimized for sparse matrix operations-to execute convolutional and transformer layers at sub-millisecond scales [94,100].Model compression techniques like Low-Rank Adaptation (LoRA) [72] and knowledge distillation can shrink parameter counts by up to 90%, reducing both memory footprint and inference time while retaining over 95% of original performance on benchmark tasks.Progressive quantization strategies that combine mixed-precision arithmetic (e.g., FP16/INT8) with blockwise calibration can further cut computation by 2-4× with minimal accuracy loss [93].Adaptive inference architectures that dynamically adjust network depth or width based on input complexity-akin to early-exit branches in DeeR-VLA [202]-can reduce average compute by selectively bypassing transformer layers when visual scenes or linguistic commands are simple.Finally, efficient tokenization schemes leveraging subword patch embeddings and dynamic vocabulary allocation can compress visual and linguistic input into compact representations, minimizing token counts without sacrificing semantic richness [133].Together, these innovations can enable sub-50 ms end-to-end inference on commodity edge GPUs, paving the way for latency-sensitive applications in autonomous drone flight, real-time teleoperation, and collaborative manufacturing.</p>
<p>• Multimodal Action Representation and Safety Assurance.Addressing multimodal action representation and robust safety requires end-to-end frameworks that unify perception, reasoning, and control under stringent safety constraints.Hybrid policy architectures combining diffusion-based sampling for low-level motion primitives [34] with autoregressive high-level planners [186] enable compact stochastic representations of diverse action trajectories, improving adaptability in dynamic environments.Safety can be enforced via real-time risk assessment modules that ingest multi-sensor fusion streams-visual, depth, and proprioceptive data-to predict collision probability and joint stress thresholds, triggering emergency stop circuits when predefined safety envelopes are breached [143,180].Reinforcement learning algorithms augmented with constrained optimization (e.g., Lagrangian methods in SafeVLA [205]) can learn  policies that maximize task success while strictly respecting safety constraints.Online model adaptation techniques-such as rule-based RL (GRPO) and Direct Preference Optimization (DPO)-further refine action selection under new environmental conditions, ensuring consistent safety performance across scenarios [87].Crucially, embedding formal verification layers that symbolically analyze planner outputs before execution can guarantee compliance with safety invariants, even for neuralnetwork-based controllers.Integrating these methodologies will produce VLA systems that not only execute complex, multimodal actions but do so with provable safety in unstructured, real-world settings.</p>
<p>Challenges Solutions</p>
<p>• Dataset Bias, Grounding, and Generalization to Unseen Tasks.</p>
<p>Robust generalization demands both broadened data diversity and advanced learning paradigms.Curating large-scale, debiased multimodal datasets-combining web-scale image-text corpora like LAION-5B [152] with robot-centric trajectory archives such as Open X-Embodiment [175]-lays the groundwork for equitable semantic grounding.Hard-negative sampling and contrastive fine-tuning of vision-language backbones (e.g., CLIP variants) can mitigate spurious correlations and enhance semantic fidelity [16,212].Meta-learning frameworks enable rapid adaptation to novel tasks by learning shared priors across task families, as demonstrated in vision-language robotic navigation models [136].Continual learning algorithms-with replay buffers and regularization strategies-preserve old knowledge while integrating new concepts, addressing catastrophic forgetting in VLA models [39].Transfer learning from 3D perception domains (e.g., point cloud reasoning in 3D-VLA [217]) can imbue models with spatial inductive biases, improving out-of-distribution robustness.Finally, simulation-to-real (sim2real) fine-tuning with domain randomization and real-world calibration-such as dynamic lighting, texture, and physics variations-ensures that policies learned in synthetic environments transfer effectively to physical robots [4,53].These combined strategies will empower VLAs to generalize confidently to unseen objects, scenes, and tasks in real-world deployments.</p>
<p>• System Integration Complexity and Computational Demands.To manage the intricate orchestration of multimodal pipelines under tight compute budgets, researchers must embrace model modularization and hardware-software co-design.Low-Rank Adaptation (LoRA) adapters can be injected into pre-trained transformer layers, enabling task-specific fine-tuning without modifying core weights [72].Knowledge distillation from large "teacher" VLAs into lightweight "student" networks-using student-teacher mutual information objectives-yields compact models with 5-10× fewer parameters while retaining 90-95% task performance [93].Mixed-precision quantization augmented by quantizationaware training can compress weights to 4-8 bits, cutting memory bandwidth and energy consumption by over 60% [94].Hardware accelerators tailored for VLA workloads-supporting sparse tensor operations, dynamic token routing, and fused vision-language kernels-can deliver sustained 100+ TOPS throughput within a 20-30 W power envelope, meeting the demands of embedded robotic platforms [133,186].Toolchains like TensorRT-LLM [100] and TVM can optimize end-to-end VLA graphs for specific edge devices, fusing layers and precomputing static subgraphs.Emerging architectures such as TinyVLA demonstrate that sub-1 B parameter VLAs can achieve near-state-of-the-art performance on manipulation benchmarks with real-time inference, charting a path for widespread deployment in resource-constrained settings.</p>
<p>• Robustness and Ethical Challenges in VLA Deployment.Ensuring VLA robustness and ethical integrity requires both technical and governance measures.Domain randomization and synthetic augmentation pipelines-like UniSim's closed-loop sensor simulator-generate photorealistic variations in lighting, occlusion, and sensor noise, enhancing model resilience to environmental shifts [200].Adaptive recalibration modules, which adjust perception thresholds and control gains based on real-time feedback, further mitigate drift and sensor degradation over prolonged operation.On the ethical front, bias auditing tools must scan training datasets for skewed demographic or semantic distributions, followed by corrective fine-tuning using adversarial debiasing and counterfactual augmentation [145,212].Privacy-preserving inferencing-via on-device processing, homomorphic encryption for sensitive data streams, and differential privacy during training-safeguards user data in applications like healthcare and smart homes [124,140].Socioeconomic impacts can be managed through transparent impact assessments and stakeholder engagement, ensuring that VLA adoption complements human labor through upskilling programs rather than displacing workers en masse.Finally, establishing regulatory frameworks and industry standards for VLA safety and accountability will underpin responsible innovation, balancing technical capabilities with societal values.</p>
<p>Future Roadmap</p>
<p>The future of VLA models lies at the intersection of increasingly powerful multimodal foundations, agentic reasoning, and embodied continual learning.Over the next decade, we anticipate several converging trends that will propel VLAs from capable but narrow task specialists toward the core of truly generalist robotic intelligence.</p>
<ol>
<li>Multimodal Foundation Models as the "Cortex."Today's VLAs typically couple a vision-language backbone with task-specific policy heads.Tomorrow, we expect a single, massive multimodal foundation model-trained on web-scale image, video, text, and affordance data-to serve as a shared perceptual and conceptual "cortex."This foundation will encode not only static scenes but also dynamics, physics, and common-sense world knowledge, enabling downstream action learners to tap into a unified representational substrate rather than reinventing basic perceptual skills for every robot or domain.2. Agentic, Self-Supervised Lifelong Learning.Rather than static pretraining, future VLAs will engage in continual, self-supervised interaction with their environments.Agentic frameworks-where the model generates its own exploration objectives, hypothesizes outcomes, and selfcorrects via simulated or real rollouts-will drive rapid skill acquisition.By formulating internal sub-goals ("learn to open drawers," "map furniture affordances") and integrating reinforcement-style feedback, a VLA-driven humanoid could autonomously expand its capabilities over years of deployment, much like a human apprentice.3. Hierarchical, Neuro-Symbolic Planning.To scale from low-level motor primitives to high-level reasoning, VLAs will adopt hierarchical control architectures.A top-level language-grounded planner (perhaps an LLM variant finetuned for affordance reasoning) will decompose complex instructions ("prepare a cup of tea") into sequences of sub-tasks ("fetch kettle," "fill water," "heat water," "steep tea bag").Mid-level modules will translate these into parameterized motion plans, and low-level diffusion or transformer-based controllers will generate smooth, compliant trajectories in real time.This neuro-symbolic blend ensures both the interpretability of structured plans and the flexibility of learned policies.4. Real-Time Adaptation via World Models.Robustness in unstructured settings demands that VLAs maintain an internal, predictive world model-an up-to-date simulation of objects, contacts, and agent dynamics.As the robot acts, it will continuously reconcile its predictions with sensor feedback, using model-based corrective actions when discrepancies arise (e.g., slipping grasp).Advances in differentiable physics and video-to-state encoders will make these world models both accurate and efficient enough for on-board, real-time use.Cross-Embodiment and Transfer Learning: The era of training separate VLAs for each robot morphology will give way to embodiment-agnostic policies.By encoding actions in an abstract, kinematicagnostic space (e.g., "apply grasp force at these affordance points"), future VLAs will transfer skills seamlessly between wheeled platforms, quadrupeds, and humanoids.Combined with meta-learning, a new robot can bootstrap prior skills with only a few minutes of calibration data.Safety, Ethics, and Human-Centered Alignment As VLAs gain autonomy, built-in safety and value alignment become non-negotiable.Future systems will integrate real-time risk estimators-assessing potential harm to humans or property before executing high-risk maneuvers-and seek natural language consent for ambiguous situations.Regulatory constraints and socially aware policies will be baked into the VLA stack, ensuring that robots defer to human preferences and legal norms.As illustrated in Figure 18, the future of VLA-based robotics lies in the integration of three foundational components: Vision-Language Models (VLMs), VLA architectures, and agentic AI systems.Consider "Eva," a generalist humanoid assistant operating in a household.At the perception layer, Eva's foundation VLM interprets multimodal inputs by segmenting visual scenes into discrete object-level representations, predicting affordances (e.g., graspable, fragile), and simulating dynamic behaviors through an internal world model.This VLM layer enables high-level visual understanding grounded in language semantics and physical properties.Upon receiving a user command such as "Eva, clean the coffee spill and water the plants," the VLA module activates.This core architecture combines tokenized language inputs and sensory feedback to perform hierarchical task planning.A high-level planner decomposes the instruction into actionable subtasks (e.g., locate cloth, wipe spill, retrieve watering can), which are then converted into motion trajectories via a mid-level policy module.These plans are passed to a low-level diffusion-policy controller, responsible for generating smooth, physics-aware joint movements tailored to the robot's embodiment.Complementing these is Eva's agentic AI module, which supports continual learning and adaptation.When confronted with unexpected challenges-like a sticky stain-Eva invokes an internal selfimprovement loop, running real-time simulated variations to refine its wiping strategy without human supervision.Safety and alignment are ensured through human-aware policies: proxim-ity sensors, real-time monitoring, and verbal confirmations before high-risk actions.Overnight, Eva performs autonomous review of performance logs, refining sub-policies via simulated rollouts.Together, this VLM-VLA-agentic triad marks a significant leap toward embodied AGI.It enables robots like Eva to perceive, plan, act, adapt, and safely coexist with humans, ultimately transforming how intelligent systems interact with real-world environments in robust, interpretable, and humanaligned ways.</li>
</ol>
<p>Conclusion</p>
<p>In this comprehensive review, we systematically evaluated the recent developments, methodologies, and applications of VLA models published over the last three years.Our analysis began with the foundational concepts of VLAs, defining their role as multimodal systems that unify visual perception, natural language understanding, and action generation in physical or simulated environments.We traced their evolution and timeline, detailing key milestones that marked the transition from isolated perception-action modules to fully unified, instructionfollowing robotic agents.We highlighted how multimodal integration has matured-from loosely coupled pipelines to transformer-based architectures that enable seamless coordination between modalities.</p>
<p>Next, we examined tokenization and representation techniques, focusing on how VLAs encode visual and linguistic information, including action primitives and spatial semantics.We explored learning paradigms, detailing the datasets and training strategies-from supervised learning and imitation learning to reinforcement learning and multimodal pretraining-that have shaped VLA performance.In our section on adaptive control and real-time execution, we addressed how modern VLAs are optimized for dynamic environments, discussing policies that support latency-sensitive tasks.We then categorized major architectural innovations, surveying over 50 recent VLA models.This included advancements in model design, memory systems, and interaction fidelity.We further studied training and efficiency strategies, including parameterefficient methods like LoRA, quantization, and model pruning, alongside acceleration techniques such as parallel decoding and hardware-aware inference.Our analysis continued with realworld applications of VLA models, showcasing their deployment across six domains: humanoid robotics, autonomous vehicles, industrial automation, healthcare, agriculture, and augmented reality (AR) navigation.Each application was reviewed with examples of model performance, domain-specific challenges, and generalizability.</p>
<p>In addressing challenges and limitations, we focused on five core areas: real-time inference, multimodal action representation and safety, bias and generalization, system integration and compute constraints, and ethical deployment.We proposed potential solutions drawn from current literature, including model compression, cross-modal grounding, domain adaptation, and agentic learning frameworks.Finally, our discussion and future roadmap articulated how the convergence of VLMs, VLA architectures, and agentic AI systems is steering robotics toward artificial general intelligence (AGI).This review provides a unified understanding of VLA advancements, identifies unresolved challenges, and outlines a structured path forward for developing intelligent, embodied, and human-aligned agents.Componentized VLA with specialized action module using diffusion transformers Industrial robotics, languageguided manipulation Robust action modeling, rapid adaptation, strong generalization, much higher task success rates VLATest [182] &amp; 2024 Automated framework for large-scale VLA model testing in manipulation Robotic manipulation: benchmarking VLA robustness and reliability Diverse scene generation, multi-model/task evaluation, reveals robustness gaps, guides VLA improvement NaVILA [32] &amp; 2024 Two-level VLA: high-level vision-language generates mid-level nav commands, RL locomotion executes Legged robot navigation via natural language in cluttered, realworld scenes Modular mid/low-level split, strong generalization, 88% real-world success, robust to diverse terrains RoboNurse-VLA [103] &amp; 2024 SAM 2 vision, Llama 2 language, real-time voiceto-action pipeline Surgical assistance: precise instrument grasp and handover in OR Accurate, real-time handover, robust to unseen tools, excels at complex, dynamic surgical tasks</p>
<p>Real-Time Inference Constraints</p>
<p>Adopt parallel decoding, quantized transformers, and hardware acceleration <a href="e.g., Ten-sorRT">100,94</a>; minimize autoregressive overhead [60,110] Supports real-time robotic control and broader deployment in time-sensitive domains [194,150] (e.g., drones, manipulators) Multimodal Action Representation Hybrid tokenization using diffusion and autoregressive policies [133]; train on diverse demonstrations and multi-modal outputs [121] Improves handling of complex, dynamic manipulation tasks with multiple viable solutions [59] Safety Assurance in Open Worlds Integrate dynamic risk assessment modules [143,180]; low-latency emergency stop circuits and adaptive planning layers [87] Ensures reliability and safety in unpredictable environments (homes, factories, healthcare settings) Dataset Bias and Grounding Curate diverse, debiased datasets [145]; apply improved grounding techniques such as CLIP finetuning with hard negatives [212,16] Enhances model fairness, semantic fidelity [85], and generalizability to novel realworld inputs [175,217,136] Limited 3D Perception and Reasoning Integrate 3D sensors (e.g., depth, LiDAR), develop 3D-aware architectures, and leverage point cloud fusion with vision-language inputs Enables more accurate spatial reasoning, manipulation, and navigation in complex real-world environments [100] Cross-Embodiment Generalization Train with diverse robot types and morphologies, use embodiment-agnostic representations, and apply cross-domain adaptation techniques [201] Facilitates transfer of policies and knowledge across different robot platforms and configurations [209,94] Annotation Complexity and Cost Employ weak supervision, active learning, and synthetic data generation to reduce reliance on extensive manual annotation [115] Lowers development costs and accelerates scaling to new tasks and domains [180,215] Sim-to-Real Transfer Gap Use domain adaptation, sim-to-real fine-tuning, and real-world calibration strategies [162,104] Improves reliability and consistency of VLA models when deployed outside simulation environments [4,53] Integration of Physical Knowledge Incorporate physics-based priors, simulation environments, and dynamics modeling into training pipelines [42] Enhances the model's ability to predict and plan actions that respect real-world physical constraints [82] Multi-Modal Integration (e.g., tactile, audio) Fuse additional sensory modalities (tactile, audio) with vision and language [88]; develop multimodal transformers Expands task repertoire and robustness to ambiguous or visually occluded scenarios [61,106,71] Handling Long-Horizon, Multi-Stage Tasks Design hierarchical policies, memory-augmented networks, and trajectory planning modules [106] Improves performance on complex, sequential tasks requiring planning and memory [102,215,175,136] System Integration Complexity Develop unified Transformer backbones [218]; incorporate temporal alignment layers and sim-toreal transfer learning strategies [127,207] Enables seamless planning-control coordination and robust transfer to physical robots [147,160] Energy and Compute Demands Apply model pruning, LoRA adapters, quantization-aware training, and deployment on low-power accelerators Facilitates scalable, efficient deployment of VLAs in embedded and mobile platforms [195,213,96,190] Generalization to Unseen Tasks Use compositional generalization, few-shot metalearning, and task-agnostic pretraining pipelines [135,113] Reduces task-specific overfitting, enabling robust zero-and few-shot adaptation [75,186,215] Robustness to Environmental Variability Use domain randomization, sensor fusion, and real-time recalibration of perception-action pipelines [121] Enhances performance in changing or cluttered environments with minimal degradation [214,186] Ethical and Societal Implications Enforce privacy via on-device processing and anonymization [116,154,195,29]; audit model fairness; build regulatory frameworks for trust Promotes equitable and trustworthy VLA adoption across social, medical, and labor domains [124,140,166,134]</p>
<p>1 Introduction 2 2 10 3 21 4
121021
Concepts of Vision-Language-Action Models 3 2.1 Evolution and Timeline . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Multimodal Integration: From Isolated Pipelines to Unified Agents . . . . 4 2.3 Tokenization and Representation: How VLAs Encode the World . . . . . 5 2.4 Learning Paradigms: Data Sources and Training Strategies . . . . . . . .8 2.5 Adaptive Control and Real-Time Execution . . . . . . . . . . . . . . . .Progress in Vision-Language-Action Models 10 3.1 Architectural Innovations in VLA Models . . . . . . . . . . . . . . . . . 10 3.2 Training and Efficiency Advancements in Vision-Language-Action Models 15 3.3 Parameter-Efficient Methods and Acceleration Techniques in VLA Models 15 3.4 Applications of Vision-Language-Action Models . . . . . . . . . . . . .16 3.4.1 Humanoid Robotics . . . . . . . . . . . . . . . . . . . . . . . .17 3.4.2Autonomous Vehicle Systems . . . . . . . . . . . . . . . . . . .18 3.4.3Industrial Robotics . . . . . . . . . . . . . . . . . . . . . . . . .19 3.4.4Healthcare and Medical Robotics . . . . . . . . . . . . . . . . .20 3.4.5Precision and Automated Agriculture . . . . . . . . . . . . . . .21 3.4.6Interactive AR Navigation with Vision-Language-Action Models Challenges and Limitations of Vision-Language-Action Models 22 4.1 Real-Time Inference Constraints . . . . . . . . . . . . . . . . . . . . . .23 4.2 Multimodal Action Representation and Safety Assurance . . . . . . . . .23 4.3 Dataset Bias, Grounding, and Generalization to Unseen Tasks . . . . . .24 4.4 System Integration Complexity and Computational Demands . . . . . . .24 4.5 Robustness and Ethical Challenges in VLA Deployment . . . . . . . . . 25 5 Discussion 25 5.1 Potential Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.2 Future Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27 6 Conclusion 28 * Ranjan Sapkota Email address: rs2672@cornell.edu(Manoj Karkee)</p>
<p>Figure 2 :
2
Figure 2: Mindmap for VLA Concepts.This diagram outlines the foundational components of Vision-Language-Action models, including their definitions, historical development, integration of multimodal signals, tokenization techniques, and adaptive execution.It sets the conceptual stage for understanding the structure and purpose of VLAs.</p>
<p>Figure 5 :
5
Figure 5: Foundational Concept of VLA Models (in an Apple-Picking Scenario) This illustration depicts a robotic arm autonomously picking a ripe apple in an orchard, guided by a VLA model.On the right, a flowchart outlines the four key stages of VLA models: Multimodal Integration, Tokenization and Representation, Learning Paradigms, and Adaptive Control and Real-Time Execution.</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: The diagram illustrates the end-to-end tokenization and representation process in VLA models.Visual input (e.g., cluttered tabletop) is encoded by a vision encoder (e.g., ViT), while natural language instructions (e.g., "stack the green blocks") are processed by a language encoder (e.g., T5).The system fuses prefix, state, and action tokens through a transformer and autoregressively predicts motor actions.</p>
<p>Figure 10 :
10
Figure 10: Learning Paradigms: Data Sources and Training Strategies for VLAs.</p>
<p>2</p>
<p>https://www.figure.ai/news/helix</p>
<p>Figure 12 :
12
Figure12: This figure illustrates "Helix," a next-generation humanoid robot executing a household task using a VLA framework.Upon receiving a verbal command, Helix integrates a vision-language model (e.g., SigLIP) and a language model (e.g., LLaMA-2) to jointly perceive and interpret the environment.A hierarchical VLA controller plans and executes sub-tasks-opening the fridge, grasping a bottle-while an agentic AI module adapts actions in real time.This demonstrates VLA-based generalist robotics with dynamic task adaptation and safe, semantically grounded manipulation.</p>
<p>Figure 15 :
15
Figure15: This diagram illustrates the application of VLA models in precision and automated agriculture.A ground robot uses vision encoders to detect ripe fruits and interprets instructions such as "pick only Grade A fruits" through language encoders.Action tokens then guide robotic manipulators for efficient, damage-free picking.Drones leverage VLA models to analyze aerial imagery and verbal commands for targeted irrigation.Synthetic training environments and LoRA-based adaptation enable models to generalize across crop types, environmental conditions, and geographies.This VLA-driven pipeline promotes sustainable agriculture by improving productivity, reducing manual labor, and enhancing decision-making through multimodal perception and control.</p>
<p>Figure 16 :
16
Figure 16: Showing how VLA models enable interactive AR navigation by fusing real-time visual perception, language understanding, and action planning.In dynamic environments such as airports, VLAs interpret user queries like "avoid stairs to Gate 22," analyze visual scenes (e.g., detecting escalators), and adjust navigational paths accordingly, supporting personalized, accessible, and context-aware mobility guidance.</p>
<p>Figure 17 :
17
Figure 17: Figure maps six core VLA challenges-real-time inference, multimodal fusion safety, dataset bias, integration complexity, compute demands, and robustness/ethics-against six targeted solutions: adaptive pruning, hybrid policy architectures, meta/transfer learning, LoRA/quantization, domain randomization, and ethical oversight.This systematic alignment clarifies pathways to robust, efficient, and safe VLA deployment across broader real-world robotic domains.</p>
<p>Figure 18 :
18
Figure18: This conceptual illustration presents "Eva," a future humanoid assistant powered by Vision-Language Models (VLMs), VLA frameworks, and agentic AI systems.VLMs enable semantic scene understanding and object affordance prediction, while VLAs translate language-grounded instructions into hierarchical motor plans.Agentic AI modules ensure adaptive learning, selfrefinement, and interactive decision-making in open-ended environments.Together, these components represent a foundational blueprint for Artificial General Intelligence (AGI) in robotics, where perception, language understanding, planning, and safe autonomous behavior converge in real-world, socially aware tasks.</p>
<p>long-context VLM for multimodal goal localization, topological graph for lowlevel navigationMultimodal instruction navigation with demonstration tours (MINT) in real-world environments High success on complex language+image tasks, robust to novel queries, leverages demonstration videos, scalable to large spaces CoVLA[5] &amp; 2025 CLIP for vision, Llama-2 for language, trajectory prediction for action Autonomous driving, dataset for VLA model training Large-scale, richly annotated dataset; enables interpretable scene understanding and robust path planning OpenDriveVLA[220] &amp; 2025 Hierarchical alignment of 2D/3D visual tokens and language embeddings; autoregressive agent-envego modeling End-to-end autonomous driving Unified semantic space, dynamic interaction modeling, state-of-the-art planning and QA performance ORION[56] &amp; 2025QT-Former for history context, LLM for reasoning, generative planner for trajectory predictionHolistic E2E autonomous drivingAligns reasoning and action spaces, unified optimization for VQA and planning, superior closed-loop results QUAR-VLA[43] &amp; 2025 QUART model fuses vision and language for action generation Quadruped robots: navigation, manipulation, whole-body tasks Tight vision-language-action integration, fine-grained instruction alignment, strong sim-to-real generalization TinyVLA[186] &amp; 2025Compact VLA with fast multimodal backbone, diffusion policy decoder Robotic manipulation: fast, data-efficient visuomotor control No pre-training needed, faster inference, strong generalization, outperforms OpenVLA on efficiency and accuracy UAV-VLA [150] &amp; 2025 Modular VLA: GPT for goal extraction, VLM for object search, GPT for action generation Large-scale UAV mission planning from natural language and satellite imagery Efficient flight path/action plan generation, no prior training, intuitive human-UAV interaction, benchmarked performance Bi-VLA [59] &amp; 2025 Multimodal transformer links vision, language, and bimanual action modules Bimanual dexterous manipulation for household tasks Accurate code/action generation, high adaptability, strong vision-language-action integration, robust realworld performance ChatVLA [221] &amp; 2025 Phased Alignment Training, Mixture-of-Experts for vision-language-action integration Unified multimodal understanding and robot control Minimizes forgetting/interference, excels at VQA and manipulation, efficient, outperforms SOTA VLA models RoboMamba [111] &amp; 2025 Mamba-based VLA: vision encoder co-trained with SSM for reasoning and SE(3) action Robotic reasoning and manipulation, efficient pose prediction Linear-complexity inference, minimal fine-tuning, fast and accurate reasoning and manipulation, SOTA efficiency OTTER [75] &amp; 2025 Text-aware visual feature extraction with frozen pre-trained VLMs Robotic manipulation: zero-shot generalization to novel tasks Preserves semantic alignment, no VLM fine-tuning, task-relevant feature selection, strong zero-shot performance PointVLA [96] &amp; 2025 Injects 3D point cloud features into frozen pretrained VLA via modular skip-blocks Robotic manipulation: spatial reasoning, few-shot, longhorizon tasks No retraining, preserves 2D knowledge, strong 3D spatial reasoning, excels at few-shot and dynamic tasks VLA-Cache [195] &amp; 2025 Adaptive token caching with selective reuse of static visual tokens Robotic manipulation: efficient, real-time VLA inference 40-50% faster, minimal accuracy loss, dynamic layerwise token reuse, practical for real-world robots CombatVLA [29] &amp; 2025 Trains on video-action AoT sequences, integrates truncated AoT for fast inference 3D ARPGs: real-time combat understanding and tactical action 50x faster inference, outperforms all baselines, surpasses human success rate, strong tactical reasoning HybridVLA [110] &amp; 2025 Unified LLM with collaborative diffusion and autoregressive action policies Robotic manipulation: singlearm, dual-arm, diverse real/sim tasks Adaptive action ensemble, robust control, strong generalization, outperforms SOTA on complex manipulations NORA [79] &amp; 2025 3B-parameter VLA using Qwen-2.5-VL-3Bbackbone and FAST+ tokenizer Generalist embodied robotics: efficient real-world and simulated task execution Low computational overhead, strong visual reasoning, fast action decoding, outperforms larger VLA models SpatialVLA [136] &amp; 2025 Ego3D Position Encoding and Adaptive Action Grids for spatially-aware VLA Generalist robot manipulation: cross-robot, multi-task, zeroshot control 3D spatial integration, adaptive action discretization, strong generalization and transfer, open-sourced code MoLe-VLA [213] &amp; 2025 Mixture-of-Layers with dynamic layer-skipping via STAR router and CogKD Efficient robot manipulation: RLBench and real-world tasks Selective layer activation, 5.6x faster, preserves cognition, +8% mean success, brain-inspired efficiency JARVIS-VLA [101] &amp; 2025 Post-trains large VLMs with visual-language guidance and action head for keyboard/mouse control Visual games (Minecraft): 1k+ tasks, open-world, instruction following Self-supervised post-training, 40%+ over baselines, strong world knowledge, state-of-the-art generalization, open-sourced UP-VLA [209] &amp; 2025 Unified VLA with joint multi-modal understanding and future prediction objectives Embodied agents: manipulation tasks, precise spatial reasoning Captures both high-level semantics and low-level spatial dynamics, 33% better on Calvin ABC-D, excels at realworld tasks needing fine spatial control Shake-VLA [92] &amp; 2025 Modular VLA system with vision, speech-to-text, RAG, anomaly detection, and bimanual arms Bimanual robotic cocktail mixing: ingredient detection, recipe adaptation, liquid measurement 100% task success, robust in noisy/cluttered settings, accurate ingredient handling, flexible recipe adaptation, real-world deployment MoRE [214] &amp; 2025 Sparse Mixture-of-Experts (MoE) with LoRA modules, RL-based Q-function training Quadruped robots: multi-task navigation, locomotion, and manipulation Scalable RL fine-tuning on mixed-quality data, strong multi-task and OOD generalization, outperforms baselines in sim and real-world DexGraspVLA [219] &amp; 2025 Hierarchical VLA: pre-trained vision-language planner + diffusion-based low-level controller General dexterous grasping: robust across diverse objects, lighting, and backgrounds Iterative domain-invariant representation, strong zeroshot generalization, 90%+ success on unseen scenarios, consistent performance across variations DexVLA [185] &amp; 2025 Plug-in billion-param diffusion action expert, embodiment curriculum learning General robot control: singlearm, bimanual, dexterous hand, long-horizon tasks Cross-embodiment action modeling, efficient curriculum training, rapid adaptation, SOTA on complex tasks without task-specific tuning</p>
<p>Table 2 :
2
Summary of VLA models, detailing each model's name, architecture features, training dataset, and highlighting their key strengths or unique capabilities in robotics and AI tasks.
ModelArchitecture ComponentsTrainingKey Strength /(Refer-DatasetUniquenessence)CLIPort [157]• Vision Encoder: CLIP-Self-Combines semanticResNet50 + Transporter-collectedCLIP features withResNet[SC]spatial Transporternetwork for precise• Language Encoder: CLIP-SE(2) manipulation.GPT• Action Decoder: LingUNetRT-1 [18]• Vision Encoder: Efficient-RT-1-Pioneering Trans-NetKitchenformer architecture[SC]with discretized ac-• Language Encoder: Uni-tions for multi-taskversal Sentence Encoderkitchen manipula-tion.• Action Decoder: Trans-formerRT-• Vision Encoder: ViT-VQA +First large VLA co-2 [224]22B/ViT-4BRT-1-finetuned on internetKitchenVQA data and robot• Language Encoder: PaLI-data for emergentX/PaLM-Ecapabilities.• Action Decoder: Symbol-tuning</p>
<p>[133]102]ning.Structured pruning removes entire attention heads or feed-forward sublayers identified as redundant.While less explored in VLA than in pure vision or language models, early studies on Diffusion Policy demonstrate that pruning up to 20 % of ConvNet-based vision encoders yields negligible performance degradation in grasp stability[34].Similar schemes applied to transformer-based VLAs (e.g.RDT-1B) can reduce memory footprint by 25 % with under 2 % drop in task success, paving the way for sub-4 GB deployments[112,102]. 4. Compressed Action Tokenization (FAST).FAST reformulates continuous action outputs as frequency-domain tokens, compressing long control sequences into concise descriptors.The Pi-0 Fast variant achieved 15× faster inference with a 300 M-parameter diffusion head by tokenizing 1000 ms action windows into 16 discrete tokens, enabling 200 Hz policy rates on desktop GPUs[133].This approach trades minimal trajectory granularity for large speedups, suited for high-frequency control in dynamic tasks like bimanual assembly.5. Parallel Decoding and Action Chunking.Autoregressive VLAs traditionally decode actions token by token, incurring sequential latency.Parallel decoding architectures
1. Low-Rank Adaptation (LoRA). LoRA injects smalltrainable rank-decomposition matrices into frozen trans-former layers, enabling fine-tuning of billion-parameterVLAs with only a few million additional weights. InOpenVLA, LoRA adapters (20 M parameters) tuned a 7B-parameter backbone on commodity GPUs in under 24h, cutting GPU compute by 70 % compared to full back-propagation [72, 94]. Crucially, LoRA-adapted models re-tain their high-level language grounding and visual rea-soning capabilities while adapting to new robotic manipu-lation tasks (e.g. novel object shapes), making large VLAsaccessible to labs without supercomputing resources.2. Quantization. Reducing weight precision to 8-bit integers(INT8) shrinks model size by half and doubles on-chipthroughput. OpenVLA experiments show that INT8 quan-tization on Jetson Orin maintains 97 % of full-precisiontask success across pick-and-place benchmarks, with onlya 5 % drop in fine-grained dexterity tasks [152, 94]. Com-
[86]161]ry methods such as post-training quantization with per-channel calibration further minimize accuracy loss in high-dynamic-range sensor inputs[128].These optimizations allow continuous control loops at 30 Hz on 50 W edge modules.3.(e.g. in Groot N1) decode groups of spatial-temporal tokens concurrently, achieving a 2.5× reduction in end-toend latency on 7-DoF arms at 100 Hz, with less than 3 mm positional error increase[13,161].Action chunking further abstracts multi-step routines into single tokens (e.g."pick-and-place-cup"), cutting inference steps by up to 40 % in long-horizon tasks like kitchen workflows[86].6. Reinforcement Learning-Supervised Hybrid Training.</p>
<p>Table 3 :
3
Comparison of VLA methodologies, application areas, and innovations.This comprehensive table compares cutting-edge VLA models by summarizing their methodologies, application domains, and key innovations.
Reference&amp;VLA MethodologyVLA Application AreaStrength and Key InnovationYearCogACT [102] &amp;2024</p>
<p>Table 4 :
4
Challenges, Potential Solutions, and Expected Impact of VLA Models
Challenge / Limita-Potential SolutionExpected Impacttion
AcknowledgementThis work was supported by the National Science Foundation and the United States Department of Agriculture, National Institute of Food and Agriculture through the "Artificial Intelligence (AI) Institute for Agriculture" Program under Award AWD003473, and AWD004595, Accession Number 1029004, "Robotic Blossom Thinning with Soft Manipulators".DeclarationsThe authors declare no conflicts of interest.Statement on AI Writing AssistanceChatGPT and Perplexity were utilized to enhance grammatical accuracy and refine sentence structure; all AI-generated revisions were thoroughly reviewed and edited for relevance.Additionally, ChatGPT-4o was employed to generate realistic visualizations.Appendix TableThe following appendix tables present a comprehensive overview of recent developments and challenges in VLA models.Table3systematically compares state-of-the-art VLA methodologies, their application domains, and key innovations across robotics, autonomous systems, and embodied AI platforms.This comparative summary highlights core architectural advances, deployment contexts, and technical contributions-providing valuable insight into the evolving landscape of generalist and task-specific VLA models.Meanwhile, Table4presents a structured synthesis of the major technical and practical challenges facing VLA model deployment, alongside potential solutions and their expected impact.This includes limitations such as real-time inference constraints, multimodal integration issues, and ethical concerns, with proposed resolutions ranging from architectural innovations to scalable training techniques and cross-modal alignment strategies.Together, these tables serve as a detailed reference for researchers, developers, and practitioners aiming to understand both the current capabilities and outstanding barriers in VLA-based intelligent systems.
J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>From methods to datasets: A survey on image-caption generators. L Agarwal, B Verma, Multimedia Tools and Applications. 832024</p>
<p>Flamingo: a visual language model for few-shot learning. J B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in neural information processing systems. 202235</p>
<p>Sim-to-real transfer for vision-and-language navigation. P Anderson, A Shrivastava, J Truong, A Majumdar, D Parikh, D Batra, S Lee, Conference on Robot Learning, PMLR. 2021</p>
<p>Covla: Comprehensive vision-language-action dataset for autonomous driving. H Arai, K Miwa, K Sasaki, K Watanabe, Y Yamaguchi, S Aoki, I Yamamoto, 2025 IEEE/CVF Winter Conference on Applications of Computer Vision. IEEE2025</p>
<p>Rapid and automated configuration of robot manufacturing cells. S Asif, M Bueno, P Ferreira, P Anandan, Z Zhang, Y Yao, G Ragunathan, L Tinkler, M Sotoodeh-Bahraini, N Lohse, Robotics and Computer-Integrated Manufacturing. 922025. 102862</p>
<p>State-of-the-art and challenges of engineering ml-enabled software systems in the deep learning era. G Assres, G Bhandari, A Shalaginov, T M Gronli, G Ghinea, 2025ACM Computing Surveys</p>
<p>Human-robot interaction through joint robot planning with large language models. K Asuzu, H Singh, M Idrissi, Intelligent Service Robotics. 2025</p>
<p>Medvlm: Medical vision-language model for consumer devices. M Ayaz, M Khan, M Saqib, A Khelifi, M Sajjad, A Elsaddik, 2024IEEE Consumer Electronics Magazine</p>
<p>Vavim and vavam: Autonomous driving through video generative modeling. F Bartoccioni, E Ramzi, V Besnier, S Venkataramanan, T H Vu, Y Xu, L Chambon, S Gidaris, S Odabas, D Hurych, arXiv:2502.156722025arXiv preprint</p>
<p>Policy learning-based image captioning with vision transformer. N V Bathula, I Paleti, S Pagidi, S S Akkumahanthi, N T Guduru, 2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS). IEEE2024</p>
<p>S Belkhale, T Ding, T Xiao, P Sermanet, Q Vuong, J Tompson, Y Chebotar, D Dwibedi, D Sadigh, arXiv:2403.01823Rt-h: Action hierarchies using language. 2024arXiv preprint</p>
<p>J Bjorck, F Castañeda, N Cherniadev, X Da, R Ding, L Fan, Y Fang, D Fox, F Hu, S Huang, arXiv:2503.14734Gr00t n1: An open foundation model for generalist humanoid robots. 2025arXiv preprint</p>
<p>K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, arXiv:2410.24164Pi-0: A visionlanguage-action flow model for general robot control. 2024arXiv preprint</p>
<p>D Bolya, P Y Huang, P Sun, J H Cho, A Madotto, C Wei, T Ma, J Zhi, J Rajasegaran, H Rasheed, arXiv:2504.13181Perception encoder: The best visual embeddings are not at the output of the network. 2025arXiv preprint</p>
<p>An introduction to vision-language modeling. F Bordes, R Y Pang, A Ajay, A C Li, A Bardes, S Petryk, O Mañas, Z Lin, A Mahmoud, B Jayaraman, arXiv:2405.172472024arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Edgevla: Efficient vision-language-action models. environments. P Budzianowski, W Maa, M Freed, J Mo, A Xie, V Tipnis, B Bolte, 2024203</p>
<p>Integration of action and language knowledge: A roadmap for developmental robotics. A Cangelosi, G Metta, G Sagerer, S Nolfi, C Nehaniv, K Fischer, J Tani, T Belpaeme, G Sandini, F Nori, IEEE Transactions on Autonomous Mental Development. 22010</p>
<p>Behind the scene: Revealing the secrets of pre-trained vision-and-language models. J Cao, Z Gan, Y Cheng, L Yu, Y C Chen, J Liu, Computer Vision-ECCV 2020: 16th European Conference. UKSpringer2020. August 23-28, 2020Proceedings, Part VI 16</p>
<p>L Cao, arXiv:2405.15775Ai robots and humanoid ai: Review, perspectives and directions. 2024arXiv preprint</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM transactions on intelligent systems and technology. 152024</p>
<p>Mobile augmented reality survey: From where we are to where we go. D Chatzopoulos, C Bermejo, Z Huang, P Hui, Ieee Access. 52017</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. B Chen, Z Xu, S Kirmani, B Ichter, D Sadigh, L Guibas, F Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024a</p>
<p>Augmented reality, deep learning and vision-language query system for construction worker safety. H Chen, L Hou, S Wu, G Zhang, Y Zou, S Moon, M Bhuiyan, 2024b. 105158157</p>
<p>Human-in-the-loop robot learning for smart manufacturing: A human-centric perspective. H Chen, S Li, J Fan, A Duan, C Yang, D Navarro-Alarcon, P Zheng, IEEE Transactions on Automation Science and Engineering. 2025a</p>
<p>H Chen, B Liu, S Wang, X Wang, W Han, Y Zhu, X Wang, Y Bi, arXiv:2501.13628Language modulates vision: Evidence from neural networks and human brain-lesion models. 2025barXiv preprint</p>
<p>Combatvla: An efficient visionlanguage-action model for combat tasks in 3d action role-playing games. P Chen, P Bu, Y Wang, X Wang, Z Wang, J Guo, Y Zhao, Q Zhu, J Song, S Yang, arXiv:2503.095272025carXiv preprint</p>
<p>Vision-semantics-label: A new two-step paradigm for action recognition with large language model. X Chen, W Xu, S Kan, L Zhang, Y Jin, Y Cen, Y Li, 2025d</p>
<p>Y Chen, S Tian, S Liu, Y Zhou, H Li, D Zhao, arXiv:2502.05450Conrft: A reinforced fine-tuning method for vla models via consistency policy. 2025earXiv preprint</p>
<p>A C Cheng, Y Ji, Z Yang, Z Gongye, X Zou, J Kautz, E Bıyık, H Yin, S Liu, X Wang, arXiv:2412.04453Navila: Legged robot visionlanguage-action model for navigation. 2024aarXiv preprint</p>
<p>Manipulation facing threats: Evaluating physical vulnerabilities in end-to-end vision language action models. H Cheng, E Xiao, C Yu, Z Yao, J Cao, Q Zhang, J Wang, M Sun, K Xu, J Gu, arXiv:2409.131742024barXiv preprint</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, Z Xu, S Feng, E Cousineau, Y Du, B Burchfiel, R Tedrake, S Song, The International Journal of Robotics Research. 027836492412736682023</p>
<p>Mobility vla: Multimodal instruction navigation with long-context vlms and topological graphs. H T L Chiang, Z Xu, Z Fu, M G Jacob, T Zhang, T W E Lee, W Yu, C Schenck, D Rendleman, D Shah, arXiv:2407.077752024arXiv preprint</p>
<p>Ecbench: Can multi-modal foundation models understand the egocentric world? a holistic embodied cognition benchmark. R Dang, Y Yuan, W Zhang, Y Xin, B Zhang, L Li, L Wang, Q Zeng, X Li, L Bing, arXiv:2501.050312025arXiv preprint</p>
<p>S Dasari, F Ebert, S Tian, S Nair, B Bucher, K Schmeckpeper, S Singh, S Levine, C Finn, arXiv:1910.11215Robonet: Large-scale multi-robot learning. 2019arXiv preprint</p>
<p>Graspvla: a grasping foundation model pre-trained on billion-scale synthetic action data. S Deng, M Yan, S Wei, H Ma, Y Yang, J Chen, Z Zhang, T Yang, X Zhang, H Cui, Z Zhang, H Wang, arXiv:2505.032332025</p>
<p>Revla: Reverting visual domain limitation of robotic foundation models. S Dey, J N Zaech, N Nikolov, L Van Gool, D P Paudel, arXiv:2409.152502024arXiv preprint</p>
<p>Visual question answering in robotic surgery: A comprehensive review. D Ding, T Yao, R Luo, X Sun, 2025aIEEE Access</p>
<p>Understanding world or predicting future? a comprehensive survey of world models. J Ding, Y Zhang, Y Shang, Y Zhang, Z Zong, J Feng, Y Yuan, H Su, N Li, N Sukiennik, arXiv:2411.144992024aarXiv preprint</p>
<p>P Ding, J Ma, X Tong, B Zou, X Luo, Y Fan, T Wang, H Lu, P Mo, J Liu, arXiv:2502.14795Humanoid-vla: Towards universal humanoid control with visual integration. 2025barXiv preprint</p>
<p>Quar-vla: Vision-language-action model for quadruped robots. P Ding, H Zhao, W Zhang, W Song, M Zhang, S Huang, N Yang, D Wang, European Conference on Computer Vision. Springer2024b</p>
<p>Long-term recurrent convolutional networks for visual recognition and description. J Donahue, Anne Hendricks, L Guadarrama, S Rohrbach, M Venugopalan, S Saenko, K Darrell, T , Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Advances in multimodal adaptation and generalization: From traditional approaches to foundation models. H Dong, M Liu, K Zhou, E Chatzi, J Kannala, C Stachniss, O Fink, arXiv:2501.185922025arXiv preprint</p>
<p>Teaching structured vision &amp; language concepts to vision &amp; language models. S Doveh, A Arbelle, S Harary, E Schwartz, R Herzig, R Giryes, R Feris, R Panda, S Ullman, L Karlinsky, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, A Wahid, J Tompson, Q Vuong, T Yu, W Huang, 2023Openreview</p>
<p>Aha: A visionlanguage-model for detecting and reasoning over failures in robotic manipulation. J Duan, W Pumacay, N Kumar, Y R Wang, S Tian, W Yuan, R Krishna, D Fox, A Mandlekar, Y Guo, arXiv:2410.003712024arXiv preprint</p>
<p>Action anticipation: Reading the intentions of humans and robots. N F Duarte, M Raković, J Tasevski, M I Coco, A Billard, J Santos-Victor, IEEE Robotics and Automation Letters. 32018</p>
<p>Bridge data: Boosting generalization of robotic skills with cross-domain datasets. F Ebert, Y Yang, K Schmeckpeper, B Bucher, G Georgakis, K Daniilidis, C Finn, S Levine, arXiv:2109.133962021arXiv preprint</p>
<p>Interleave-vla: Enhancing robot manipulation with interleaved image-text instructions. C Fan, X Jia, Y Sun, Y Wang, J Wei, Z Gong, X Zhao, M Tomizuka, X Yang, J Yan, M Ding, arXiv:2505.021522025</p>
<p>Language reasoning in vision-language-action model for robotic grasping. L Fan, K Chen, Z Xu, M Yuan, P Huang, W Huang, 20242024 China Automation Congress (CAC), IEEE</p>
<p>Rebot: Scaling robot learning with real-to-sim-to-real robotic video synthesis. Y Fang, Y Yang, X Zhu, K Zheng, G Bertasius, D Szafir, M Ding, arXiv:2503.145262025arXiv preprint</p>
<p>Foundation models in robotics: Applications, challenges, and the future. R Firoozi, J Tucker, S Tian, A Majumdar, J Sun, W Liu, Y Zhu, S Song, A Kapoor, K Hausman, The International Journal of Robotics Research. 2023. 02783649241281508</p>
<p>Is behavior cloning all you need? understanding horizon in imitation learning. D J Foster, A Block, D Misra, arXiv:2407.150072024arXiv preprint</p>
<p>Orion: A holistic end-to-end autonomous driving framework by vision-language instructed action generation. H Fu, D Zhang, Z Zhao, J Cui, D Liang, C Zhang, D Zhang, H Xie, B Wang, X Bai, arXiv:2503.197552025arXiv preprint</p>
<p>A vision-language model for predicting potential distribution land of soybean double cropping. B Gao, Y Liu, Y Li, H Li, M Li, W He, Frontiers in Environmental Science. 1215157522025a</p>
<p>J Gao, S Belkhale, S Dasari, A Balakrishna, D Shah, D Sadigh, arXiv:2503.01238A taxonomy for evaluating generalist robot policies. 2025barXiv preprint</p>
<p>Bi-vla: Vision-language-action model-based system for bimanual robotic dexterous manipulations. K F Gbagbe, SMCM A Cabrera, SMCA Alabbas, SMCO Alyunes, SMCA Lykov, SMCD Tsetserukou, SMC2024 IEEE International Conference on Systems, Man, and Cybernetics. IEEE2024</p>
<p>Bringing generative ai to edge devices through interoperable compute cores. R Geens, Flanders AI Research Day2024GhentLocation</p>
<p>Exploring the frontier of vision-language models: A survey of current methodologies and future directions. A Ghosh, A Acharya, S Saha, V Jain, A Chadha, arXiv:2404.072142024arXiv preprint</p>
<p>Recent advances in convolutional neural networks. J Gu, Z Wang, J Kuen, L Ma, A Shahroudy, B Shuai, T Liu, X Wang, G Wang, J Cai, Pattern recognition. 772018</p>
<p>Seer: Language instructed video prediction with latent diffusion models. X Gu, C Wen, W Ye, J Song, Y Gao, arXiv:2303.148972023arXiv preprint</p>
<p>Humanoid locomotion and manipulation: Current progress and challenges in control, planning, and learning. Z Gu, J Li, W Shen, W Yu, Z Xie, S Mccrory, X Cheng, A Shamsah, R Griffin, C K Liu, arXiv:2501.021162025arXiv preprint</p>
<p>Improving vision-language-action model with online reinforcement learning. Y Guo, J Zhang, X Chen, X Ji, Y J Wang, Y Hu, J Chen, arXiv:2501.166642025arXiv preprint</p>
<p>Benchmarking vision, language, &amp; action models on robotic learning tasks. P Guruprasad, H Sikka, J Song, Y Wang, P P Liang, arXiv:2411.058212024arXiv preprint</p>
<p>Baku: An efficient transformer for multi-task policy learning. S Haldar, Z Peng, L Pinto, arXiv:2406.075392024arXiv preprint</p>
<p>A review of large language models: Fundamental architectures, key technological evolutions, interdisciplinary technologies integration, optimization and compression techniques, applications, and challenges. S Han, M Wang, J Zhang, D Li, J Duan, 2024135040</p>
<p>The visions image-understanding system. A Hanson, E Riseman, Advances in Computer Vision. Psychology Press2014</p>
<p>P Hao, C Zhang, D Li, X Cao, X Hao, S Cui, S Wang, arXiv:2503.08548Tla: Tactile-language-action model for contact-rich manipulation. 2025arXiv preprint</p>
<p>Building 3D Foundation Models for the Embodied Minds. Y Hong, 2025Los AngelesUniversity of CaliforniaPh.D. thesis</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, 202213</p>
<p>Vision-based multimodal interfaces: A survey and taxonomy for enhanced context-aware system design. Y Hu, J Tang, X Gong, Z Zhou, S Zhang, D S Elvitigala, F Mueller, W Hu, A J Quigley, arXiv:2501.134432025arXiv preprint</p>
<p>Early fusion helps vision language action models generalize better. H Huang, F Liu, L Fu, T Wu, M Mukadam, J Malik, K Goldberg, P Abbeel, 20241st Workshop on X-Embodiment Robot Learning</p>
<p>Otter: A vision-language-action model with textaware visual feature extraction. H Huang, F Liu, L Fu, T Wu, M Mukadam, J Malik, K Goldberg, P Abbeel, arXiv:2503.037342025aarXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. S Huang, L Dong, W Wang, Y Hao, S Singhal, S Ma, T Lv, L Cui, O K Mohammed, B Patra, Advances in Neural Information Processing Systems. 362023a</p>
<p>Decision spikeformer: Spike-driven transformer for decision making. W Huang, Q Gu, N Ye, arXiv:2504.038002025barXiv preprint</p>
<p>W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023barXiv preprint</p>
<p>Nora: A small open-sourced generalist vision language action model for embodied tasks. C Y Hung, Q Sun, P Hong, A Zadeh, C Li, U Tan, N Majumder, S Poria, arXiv:2504.198542025arXiv preprint</p>
<p>Marcer: Multimodal augmented reality for composing and executing robot tasks. B Ikeda, M Gramopadhye, L Nekervis, D Szafir, 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE2025. 2025</p>
<p>Foundation models in robotics. A Imran, K Gopalakrishnan, AI for Robotics: Toward Embodied and General Intelligence in the Physical World. Springer2025</p>
<p>5: a vision-language-action model with open-world generalization. P Intelligence, K Black, N Brown, J Darpinian, K Dhabalia, D Driess, A Esmail, M Equi, C Finn, N Fusai, arXiv:2504.1605420250arXiv preprint</p>
<p>A survey of robot intelligence with large language models. H Jeong, H Lee, C Kim, S Shin, Applied Sciences. 1488682024</p>
<p>A comprehensive review on automation in agriculture using artificial intelligence. K Jha, A Doshi, P Patel, M Shah, Artificial Intelligence in Agriculture. 22019</p>
<p>Solami: Social vision-language-action modeling for immersive interaction with 3d autonomous characters. J Jiang, W Xiao, Z Lin, H Zhang, T Ren, Y Gao, Z Lin, Z Cai, L Yang, Z Liu, arXiv:2412.001742024arXiv preprint</p>
<p>Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, arXiv:2210.030942Vima: General robot manipulation with multimodal prompts. 20226arXiv preprint</p>
<p>Y Jiang, R Zhang, J Wong, C Wang, Y Ze, H Yin, C Gokmen, S Song, J Wu, L Fei-Fei, arXiv:2503.05652Behavior robot suite: Streamlining real-world whole-body manipulation for everyday household activities. 2025arXiv preprint</p>
<p>Beyond sight: Finetuning generalist robot policies with heterogeneous sensors via language grounding. J Jones, O Mees, C Sferrazza, K Stachowicz, P Abbeel, S Levine, arXiv:2501.046932025arXiv preprint</p>
<p>Learning visually guided latent actions for assistive teleoperation. S Karamcheti, A J Zhai, D P Losey, D Sadigh, 2021</p>
<p>A Model-Driven Framework for Domain-Specific Adaptation of Time Series Forecasting Pipeline. N Katiyar, 2023McGill University (Canada)</p>
<p>Visiongpt: Vision-language understanding agent using generalized multimodal framework. C Kelly, L Hu, B Yang, Y Tian, D Yang, C Yang, Z Huang, Z Li, J Hu, Y Zou, arXiv:2403.090272024arXiv preprint</p>
<p>M H Khan, S Asfaw, D Iarchuk, M A Cabrera, L Moreno, I Tokmurziyev, D Tsetserukou, arXiv:2501.06919Shake-vla: Vision-language-action model-based system for bimanual robotic manipulations and liquid mixing. 2025arXiv preprint</p>
<p>Fine-tuning vision-languageaction models: Optimizing speed and success. M J Kim, C Finn, P Liang, arXiv:2502.196452025arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>N Lee, Y Bang, H Lovenia, S Cahyawijaya, W Dai, P Fung, arXiv:2309.14381Survey of social bias in vision-language models. 2023arXiv preprint</p>
<p>C Li, J Wen, Y Peng, Y Peng, F Feng, Y Zhu, arXiv:2503.07511Pointvla: Injecting the 3d world into vision-language-action models. 2025aarXiv preprint</p>
<p>What foundation models can bring for robot learning in manipulation: A survey. D Li, Y Jin, Y Sun, H Yu, J Shi, X Hao, P Hao, H Liu, F Sun, J Zhang, arXiv:2404.182012024aarXiv preprint</p>
<p>J Li, G Skinner, G Yang, B R Quaranto, S D Schwaitzberg, P C Kim, J Xiong, arXiv:2408.07981Llava-surg: towards multimodal surgical assistant via structured surgical video learning. 2024barXiv preprint</p>
<p>Intentqa: Context-aware video intent reasoning. J Li, P Wei, W Han, L Fan, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Improving vision-language-action models via chain-of-affordance. J Li, Y Zhu, Z Tang, J Wen, M Zhu, X Liu, C Li, R Cheng, Y Peng, F Feng, arXiv:2412.204512024carXiv preprint</p>
<p>M Li, Z Wang, K He, X Ma, Y Liang, arXiv:2503.16365Jarvis-vla: Posttraining large-scale vision language models to play visual games with keyboards and mouse. 2025barXiv preprint</p>
<p>Cogact: A foundational visionlanguage-action model for synergizing cognition and action in robotic manipulation. Q Li, Y Liang, Z Wang, L Luo, X Chen, M Liao, F Wei, Y Deng, S Xu, Y Zhang, arXiv:2411.196502024darXiv preprint</p>
<p>S Li, J Wang, R Dai, W Ma, W Y Ng, Y Hu, Z Li, arXiv:2409.19590Robonurse-vla: Robotic scrub nurse system based on vision-languageaction model. 2024earXiv preprint</p>
<p>Hamster: Hierarchical action models for open-world robot manipulation. Y Li, Y Deng, J Zhang, J Jang, M Memmel, R Yu, C R Garrett, F Ramos, D Fox, A Li, arXiv:2502.054852025carXiv preprint</p>
<p>Y Li, Z Gong, H Li, X Huang, H Kang, G Bai, X Ma, arXiv:2505.00693Robotic visual instruction. 2025darXiv preprint</p>
<p>Y Li, Z Lai, W Bao, Z Tan, A Dao, K Sui, J Shen, D Liu, H Liu, Y Kong, arXiv:2501.02765Visual large language models for generalized and specialized applications. 2025earXiv preprint</p>
<p>Benchmark evaluations, applications, and challenges of large vision language models: A survey. Z Li, X Wu, H Du, H Nghiem, G Shi, arXiv:2501.0218912025farXiv preprint</p>
<p>Showui: One vision-language-action model for gui visual agent. K Q Lin, L Li, D Gao, Z Yang, S Wu, Z Bai, W Lei, L Wang, M Z Shou, arXiv:2411.174652024arXiv preprint</p>
<p>Automatic sorting system for industrial robot with 3d visual perception and natural language interaction. Y Lin, H Zhou, M Chen, H Min, Measurement and Control. 522019</p>
<p>J Liu, H Chen, P An, Z Liu, R Zhang, C Gu, X Li, Z Guo, S Chen, M Liu, arXiv:2503.10631Hybridvla: Collaborative diffusion and autoregression in a unified vision-language-action model. 2025aarXiv preprint</p>
<p>Robomamba: Efficient vision-languageaction model for robotic reasoning and manipulation. J Liu, M Liu, Z Wang, P An, X Li, K Zhou, S Yang, R Zhang, Y Guo, S Zhang, Advances in Neural Information Processing Systems. 2024a37</p>
<p>S Liu, L Wu, B Li, H Tan, H Chen, Z Wang, K Xu, H Su, J Zhu, arXiv:2410.07864Rdt-1b: a diffusion foundation model for bimanual manipulation. 2024barXiv preprint</p>
<p>From screens to scenes: A survey of embodied ai in healthcare. Y Liu, X Cao, T Chen, Y Jiang, J You, M Wu, X Wang, M Feng, Y Jin, J Chen, arXiv:2501.074682025barXiv preprint</p>
<p>Y Liu, X Cao, T Chen, Y Jiang, J You, M Wu, X Wang, M Feng, Y Jin, J Chen, arXiv:2501.07468A survey of embodied ai in healthcare: Techniques, applications, and opportunities. 2025carXiv preprint</p>
<p>Synthvlm: High-efficiency and high-quality synthetic data for vision language models. Z Liu, H Liang, X Huang, W Xiong, Q Yu, L Sun, C Chen, C He, B Cui, W Zhang, arXiv:2407.207562024carXiv preprint</p>
<p>Probing a vision-language-action model for symbolic states and integration into a cognitive architecture. H Lu, H Li, P S Shahani, S Herbers, M Scheutz, arXiv:2502.045582025arXiv preprint</p>
<p>Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. J Lu, C Clark, S Lee, Z Zhang, S Khosla, R Marten, D Hoiem, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Towards happy housework: Scenario-based experience design for a household cleaning robotic system. Y Lu, Z Liao, EAI Endorsed Transactions on Scalable Information Systems. 102023</p>
<p>Precise and dexterous robotic manipulation via human-in-the-loop reinforcement learning. J Luo, C Xu, J Wu, S Levine, arXiv:2410.218452024arXiv preprint</p>
<p>J Lyu, Z Li, X Shi, C Xu, Y Wang, H Wang, arXiv:2503.16806Dywa: Dynamics-adaptive world action model for generalizable non-prehensile manipulation. 2025arXiv preprint</p>
<p>A survey on vision-language-action models for embodied ai. Y Ma, Z Song, Y Zhuang, J Hao, I King, arXiv:2405.140932024arXiv preprint</p>
<p>Review of deep reinforcement learning-based object grasping: Techniques, open challenges, and recommendations. M Q Mohammed, K L Chung, C S Chyi, Ieee Access. 82020</p>
<p>Integrating reinforcement learning with foundation models for autonomous robotics: Methods and perspectives. A Moroncelli, V Soni, A A Shahid, M Maccarini, M Forgione, D Piga, B Spahiu, L Roveda, arXiv:2410.164112024arXiv preprint</p>
<p>Large language models for artificial general intelligence (agi): A survey of foundational principles and approaches. A Mumuni, F Mumuni, arXiv:2501.031512025arXiv preprint</p>
<p>Peria: Perceive, reason, imagine, act via holistic language and vision planning for manipulation. F Ni, J Hao, S Wu, L Kou, Y Yuan, Z Dong, J Liu, M Li, Y Zhuang, Y Zheng, Advances in Neural Information Processing Systems. 372024</p>
<p>Y Nie, L Li, Z Gan, S Wang, C Zhu, M Zeng, Z Liu, M Bansal, L Wang, arXiv:2112.04453Mlp architectures for vision-and-language modeling: An empirical study. 2021arXiv preprint</p>
<p>From abstraction to reality: Darpa's vision for robust sim-to-real autonomy. E Noorani, Z Serlin, B Price, A Velasquez, arXiv:2503.110072025arXiv preprint</p>
<p>M Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Fernandez, D Haziza, F Massa, A El-Nouby, arXiv:2304.07193Dinov2: Learning robust visual features without supervision. 2023arXiv preprint</p>
<p>Towards cognition-augmented human-centric assembly: A visual computation perspective. J Pang, P Zheng, J Fan, T Liu, Robotics and Computer-Integrated Manufacturing. 911028522025</p>
<p>Robotassisted surgery in space: pros and cons. a review from the surgeon's point of view. D Pantalone, G S Faini, F Cialdai, E Sereni, S Bacci, D Bani, M Bernini, C Pratesi, P Stefàno, L Orzalesi, 202156npj Microgravity 7</p>
<p>Visual language integration: A survey and open challenges. S M Park, Y G Kim, Computer Science Review. 482023. 100548</p>
<p>Pretrained language models as visual planners for human assistance. D Patel, H Eghbalzadeh, N Kamra, M L Iuzzolino, U Jain, R Desai, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Fast: Efficient action tokenization for vision-language-action models. K Pertsch, K Stachowicz, B Ichter, D Driess, S Nair, Q Vuong, O Mees, C Finn, S Levine, arXiv:2501.097472025arXiv preprint</p>
<p>A Plaat, M Van Duijn, N Van Stein, M Preuss, P Van Der Putten, K J Batenburg, arXiv:2503.23037Agentic large language models, a survey. 2025arXiv preprint</p>
<p>A Polubarov, N Lyubaykin, A Derevyagin, I Zisman, D Tarasov, A Nikulin, V Kurenkov, arXiv:2501.19400Vintix: Action model via in-context reinforcement learning. 2025arXiv preprint</p>
<p>D Qu, H Song, Q Chen, Y Yao, X Ye, Y Ding, Z Wang, J Gu, B Zhao, D Wang, arXiv:2501.15830Spatialvla: Exploring spatial representations for visual-language-action model. 2025arXiv preprint</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>An Intelligent Versatile Pipeline for 6D Localization of Industrial Components in a Production Environment. P K Rawal, 2025Fraunhofer VerlagPh.D. thesis</p>
<p>Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. P P Ray, ternet of Things and Cyber-Physical Systems. 32023</p>
<p>Who is responsible? the data, models, users or regulations? responsible generative ai for a sustainable future. S Raza, R Qureshi, A Zahid, J Fioresi, F Sadak, M Saeed, R Sapkota, A Jain, A Zafar, M U Hassan, arXiv:2502.086502025arXiv preprint</p>
<p>S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.06175A generalist agent. 2022arXiv preprint</p>
<p>Human-robot interaction review: Challenges and solutions for modern industrial environments. D Rodriguez-Guerra, G Sorrosal, I Cabanes, C Calleja, Ieee Access. 92021</p>
<p>Integrating advanced vision-language models for context recognition in risks assessment. J Rodriguez-Juan, D Ortiz-Perez, J Garcia-Rodriguez, D Tomás, G J Nalepa, Neurocomputing. 6181291312025</p>
<p>Perception for humanoid robots. A Roychoudhury, S Khorshidi, S Agrawal, M Bennewitz, Current Robotics Reports. 42023</p>
<p>Scaling for fairness? analyzing model size, data composition, and multilinguality in vision-language bias. Z A Sahili, I Patras, M Purver, arXiv:2501.132232025arXiv preprint</p>
<p>Building vision-language models on solid foundations with masked distillation. S Sameni, K Kafle, H Tan, S Jenni, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Scalable, trainingfree visual language robotics: a modular multi-model framework for consumer-grade gpus. M Samson, B Muraccioli, F Kanehiro, 2025 IEEE/SICE International Symposium on System Integration (SII). IEEE2025</p>
<p>Object detection with multimodal large vision-language models: An in-depth review. R Sapkota, M Karkee, SSRN 52339532025</p>
<p>A review of 3d object detection with vision-language models. R Sapkota, K I Roumeliotis, R H Cheppally, M F Calero, M Karkee, arXiv:2504.187382025arXiv preprint</p>
<p>Uav-vla: Vision-language-action system for large scale aerial mission generation. O Sautenkov, Y Yaqoot, A Lykov, M A Mustafa, G Tadevosyan, A Akhmetkazy, M A Cabrera, M Martynov, S Karaf, D Tsetserukou, arXiv:2501.050142025arXiv preprint</p>
<p>S Schmidgall, J Cho, C Zakka, W Hiesinger, arXiv:2407.19305Gp-vls: A general-purpose vision language model for surgery. 2024arXiv preprint</p>
<p>Laion-5b: An open large-scale dataset for training next generation image-text models. C Schuhmann, R Beaumont, R Vencu, C Gordon, R Wightman, M Cherti, T Coombes, A Katta, C Mullis, M Wortsman, Advances in neural information processing systems. 352022</p>
<p>V Serpiva, A Lykov, A Myshlyaev, M H Khan, A A Abdulkarim, O Sautenkov, D Tsetserukou, arXiv:2503.02572Racevla: Vla-based racing drone navigation with human-like behaviour. 2025arXiv preprint</p>
<p>A Sharshar, L U Khan, W Ullah, M Guizani, arXiv:2502.07855Vision-language models for edge networks: A comprehensive survey. 2025arXiv preprint</p>
<p>Hi robot: Open-ended instruction following with hierarchical vision-language-action models. L X Shi, B Ichter, M Equi, L Ke, K Pertsch, Q Vuong, J Tanner, A Walling, H Wang, N Fusai, arXiv:2502.194172025arXiv preprint</p>
<p>Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning. H C Shin, H R Roth, M Gao, L Lu, Z Xu, I Nogues, J Yao, D Mollura, R M Summers, IEEE transactions on medical imaging. 352016</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on robot learning, PMLR. 2022</p>
<p>A review on manipulation skill acquisition through teleoperation-based learning from demonstration. W Si, N Wang, C Yang, Cognitive Computation and Systems. 32021</p>
<p>Augmented reality and gps-based resource efficient navigation system for outdoor environments: Integrating device camera, sensors, and storage. S Singh, J Singh, B Shah, S S Sehra, F Ali, 2022. 12720Sustainability 14</p>
<p>A survey on diffusion policy for robotic manipulation: Taxonomy, analysis, and future directions. M Song, X Deng, Z Zhou, J Wei, W Guan, L Nie, 2025aAuthorea Preprints</p>
<p>Accelerating vision-language-action model integrated with action chunking via parallel decoding. W Song, J Chen, P Ding, H Zhao, W Zhao, Z Zhong, Z Ge, J Ma, H Li, arXiv:2503.023102025barXiv preprint</p>
<p>Prism: Projection-based reward integration for scene-aware real-to-sim-to-real transfer with few demonstrations. H Sun, H Wang, C Ma, S Zhang, J Ye, X Chen, X Lan, arXiv:2504.205202025aarXiv preprint</p>
<p>A review of embodied grasping. J Sun, P Mao, L Kong, J Wang, Sensors. 8522025b</p>
<p>Generating text with recurrent neural networks. I Sutskever, J Martens, G E Hinton, Proceedings of the 28th international conference on machine learning (ICML-11). the 28th international conference on machine learning (ICML-11)2011</p>
<p>Grounding multimodal large language models in actions. A Szot, B Mazoure, H Agrawal, R D Hjelm, Z Kira, A Toshev, Advances in Neural Information Processing Systems. 202437</p>
<p>G R Team, S Abeyruwan, J Ainslie, J B Alayrac, M G Arenas, T Armstrong, A Balakrishna, R Baruch, M Bauza, M Blokzijl, arXiv:2503.20020Gemini robotics: Bringing ai into the physical world. 2025arXiv preprint</p>
<p>O M Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, T Kreiman, C Xu, arXiv:2405.12213Octo: An opensource generalist robot policy. 2024arXiv preprint</p>
<p>Robots that use language. S Tellex, N Gopalan, H Kress-Gazit, C Matuszek, Annual Review of Control, Robotics, and Autonomous Systems. 32020</p>
<p>Computer vision technology in agricultural automation-a review. H Tian, T Wang, Y Liu, X Qiao, Y Li, 20207Information processing in agriculture</p>
<p>Visual autoregressive modeling: Scalable image generation via next-scale prediction. K Tian, Y Jiang, Z Yuan, B Peng, L Wang, Advances in neural information processing systems. 372024</p>
<p>A comprehensive analysis of gender, racial, and prompt-induced biases in large language models. N Torres, C Ulloa, I Araya, M Ayala, S Jara, International Journal of Data Science and Analytics. 2024</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Explainable ai for industry 5.0: vision, architecture, and potential directions. C Trivedi, P Bhattacharya, V K Prasad, V Patel, A Singh, S Tanwar, R Sharma, S Aluvala, G Pau, G Sharma, IEEE Open Journal of Industry Applications. 2024</p>
<p>Perception and control with large language models in robotic manipulation. L Verbaan, 2024TU Delft Library</p>
<p>Open xembodiment: Robotic learning datasets and rt-x models. Q Vuong, S Levine, H R Walke, K Pertsch, A Singh, R Doshi, C Xu, J Luo, L Tan, D Shah, Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@ CoRL2023. 2023</p>
<p>Surgical-lvlm: Learning to adapt large vision-language model for grounded visual question answering in robotic surgery. G Wang, L Bai, W J Nah, J Wang, Z Zhang, Z Chen, J Wu, M Islam, H Liu, H Ren, arXiv:2405.109482024aarXiv preprint</p>
<p>Non-invasive to invasive: Enhancing ffa synthesis from cfp with a benchmark dataset and a novel network. H Wang, Z Xing, W Wu, Y Yang, Q Tang, M Zhang, Y Xu, L Zhu, Proceedings of the 1st International Workshop on Multimedia Computing for Health and Medicine. the 1st International Workshop on Multimedia Computing for Health and Medicine2024b</p>
<p>Where to learn: Embodied perception learning planned by vision-language models. J Wang, D Guo, H Liu, IEEE Transactions on Cognitive and Developmental Systems. 2025a</p>
<p>Roboflamingo-plus: Fusion of depth and rgb perception with vision-language models for enhanced robotic manipulation. S Wang, arXiv:2503.195102025arXiv preprint</p>
<p>Exploring the adversarial vulnerabilities of vision-language-action models in robotics. T Wang, C Han, J C Liang, W Yang, D Liu, L X Zhang, Q Wang, J Luo, R Tang, arXiv:2411.135872024carXiv preprint</p>
<p>Multimodal chain-of-thought reasoning: A comprehensive survey. Y Wang, S Wu, Y Zhang, S Yan, Z Liu, J Luo, H Fei, arXiv:2503.126052025barXiv preprint</p>
<p>Towards testing and evaluating vision-language-action models for robotic manipulation: An empirical study. Z Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, arXiv:2409.128942024darXiv preprint</p>
<p>Occllama: An occupancy-language-action generative world model for autonomous driving. J Wei, S Yuan, P Li, Q Hu, Z Gan, W Ding, arXiv:2409.032722024arXiv preprint</p>
<p>Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. J Wen, M Zhu, Y Zhu, Z Tang, J Li, Z Zhou, C Li, X Liu, Y Peng, C Shen, arXiv:2412.032932024arXiv preprint</p>
<p>Dexvla: Vision-language model with plug-in diffusion expert for general robot control. J Wen, Y Zhu, J Li, Z Tang, C Shen, F Feng, arXiv:2502.058552025aarXiv preprint</p>
<p>Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. J Wen, Y Zhu, J Li, M Zhu, Z Tang, K Wu, Z Xu, N Liu, R Cheng, C Shen, IEEE Robotics and Automation Letters. 2025b</p>
<p>Convnext v2: Co-designing and scaling convnets with masked autoencoders. S Woo, S Debnath, R Hu, X Chen, Z Liu, I S Kweon, S Xie, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2023</p>
<p>Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. J Wu, M Zhong, S Xing, Z Lai, Z Liu, Z Chen, W Wang, X Zhu, L Lu, T Lu, Advances in Neural Information Processing Systems. 372024a</p>
<p>Smart: scalable multiagent real-time motion generation via next-token prediction. W Wu, X Feng, Z Gao, Y Kan, Advances in Neural Information Processing Systems. 372024b</p>
<p>Momanipvla: Transferring vision-language-action models for general mobile manipulation. Z Wu, Y Zhou, X Xu, Z Wang, H Yan, arXiv:2503.134462025arXiv preprint</p>
<p>T Y Xiang, A Q Jin, X H Zhou, M J Gui, X L Xie, S Q Liu, S Y Wang, S B Duang, S C Wang, Z Lei, arXiv:2503.04163Vla modelexpert collaboration for bi-directional manipulation learning. 2025arXiv preprint</p>
<p>J Xiong, G Liu, L Huang, C Wu, T Wu, Y Mu, Y Yao, H Shen, Z Wan, J Huang, arXiv:2411.05902Autoregressive models in vision: A survey. 2024arXiv preprint</p>
<p>Mlevlm: Improve multi-level progressive capabilities based on multimodal large language model for medical visual question answering. D Xu, Y Chen, J Wang, Y Huang, H Wang, Z Jin, H Wang, W Yue, J He, H Li, Findings of the Association for Computational Linguistics ACL 2024. 2024a</p>
<p>When embodied ai meets industry 5.0: human-centered smart manufacturing. J Xu, Q Sun, Q L Han, Y Tang, IEEE/CAA Journal of Automatica Sinica. 122025a</p>
<p>Vlacache: Towards efficient vision-language-action model via adaptive token caching in robotic manipulation. S Xu, Y Wang, C Xia, D Zhu, T Huang, C Xu, arXiv:2502.021752025barXiv preprint</p>
<p>Z Xu, K Wu, J Wen, J Li, N Liu, Z Che, J Tang, arXiv:2402.02385A survey on robotics with foundation models: toward embodied ai. 2024barXiv preprint</p>
<p>H Xue, J Ren, W Chen, G Zhang, Y Fang, G Gu, H Xu, C Lu, arXiv:2503.02881Reactive diffusion policy: Slow-fast visual-tactile policy learning for contact-rich manipulation. 2025arXiv preprint</p>
<p>R Yang, G Chen, C Wen, Y Gao, arXiv:2503.08950Fp3: A 3d foundation policy for robotic manipulation. 2025arXiv preprint</p>
<p>Attentive mask clip. Y Yang, W Huang, Y Wei, H Peng, X Jiang, H Jiang, F Wei, Y Wang, H Hu, L Qiu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023a</p>
<p>Unisim: A neural closed-loop sensor simulator. Z Yang, Y Chen, J Wang, S Manivasagam, W C Ma, A J Yang, R Urtasun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023b</p>
<p>S Ye, J Jang, B Jeon, S Joo, J Yang, B Peng, A Mandlekar, R Tan, Y W Chao, B Y Lin, arXiv:2410.11758Latent action pretraining from videos. 2024arXiv preprint</p>
<p>Deer-vla: Dynamic inference of multimodal large language models for efficient robot execution. Y Yue, Y Wang, B Kang, Y Han, S Wang, S Song, J Feng, G Huang, Advances in Neural Information Processing Systems. 202437</p>
<p>Robotic control via embodied chain-of-thought reasoning. M Zawalski, W Chen, K Pertsch, O Mees, C Finn, S Levine, arXiv:2407.086932024arXiv preprint</p>
<p>Sigmoid loss for language image pre-training. X Zhai, B Mustafa, A Kolesnikov, L Beyer, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>B Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, arXiv:2503.03480Safevla: Towards safety alignment of vision-language-action model via safe reinforcement learning. 2025aarXiv preprint</p>
<p>Gevrm: Goalexpressive video generation model for robust visual manipulation. H Zhang, P Ding, S Lyu, Y Peng, D Wang, arXiv:2502.092682025barXiv preprint</p>
<p>Slim: Sim-to-real legged instructive manipulation via long-horizon visuomotor learning. H Zhang, H Yu, L Zhao, A Choi, Q Bai, B Yang, W Xu, arXiv:2501.099052025carXiv preprint</p>
<p>H Zhang, N Zantout, P Kachana, Z Wu, J Zhang, W Wang, arXiv:2411.03540Vla-3d: A dataset for 3d semantic scene understanding and navigation. 2024aarXiv preprint</p>
<p>Upvla: A unified understanding and prediction model for embodied agent. J Zhang, Y Guo, Y Hu, X Chen, X Zhu, J Chen, arXiv:2501.188672025darXiv preprint</p>
<p>Uni-navid: A video-based vision-languageaction model for unifying embodied navigation tasks. J Zhang, K Wang, S Wang, M Li, H Liu, S Wei, Z Wang, Z Zhang, H Wang, arXiv:2412.062242024barXiv preprint</p>
<p>Learning manipulation skills through robot chain-of-thought with sparse failure guidance. K Zhang, Z H Yin, W Ye, Y Gao, arXiv:2405.135732024carXiv preprint</p>
<p>K Zhang, P Yun, J Cen, J Cai, D Zhu, H Yuan, C Zhao, T Feng, M Y Wang, Q Chen, arXiv:2503.03464Generative artificial intelligence in robotic manipulation: A survey. 2025earXiv preprint</p>
<p>Mole-vla: Dynamic layer-skipping vision language action model via mixture-of-layers for efficient robot manipulation. R Zhang, M Dong, Y Zhang, L Heng, X Chi, G Dai, L Du, D Wang, Y Du, S Zhang, arXiv:2503.203842025farXiv preprint</p>
<p>More: Unlocking scalability in reinforcement learning for quadruped vision-language-action models. H Zhao, W Song, D Wang, X Tong, P Ding, X Cheng, Z Ge, arXiv:2503.080072025aarXiv preprint</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Q Zhao, Y Lu, M J Kim, Z Fu, Z Zhang, Y Wu, Z Li, Q Ma, S Han, C Finn, arXiv:2503.220202025barXiv preprint</p>
<p>Learning finegrained bimanual manipulation with low-cost hardware. T Z Zhao, V Kumar, S Levine, C Finn, arXiv:2304.137052023arXiv preprint</p>
<p>H Zhen, X Qiu, P Chen, J Yang, X Yan, Y Du, Y Hong, C Gan, arXiv:2403.096313d-vla: A 3d vision-language-action generative world model. 2024arXiv preprint</p>
<p>Universal actions for enhanced embodied foundation models. J Zheng, J Li, D Liu, Y Zheng, Z Wang, Z Ou, Y Liu, J Liu, Y Q Zhang, X Zhan, arXiv:2501.101052025arXiv preprint</p>
<p>Dexgraspvla: A vision-language-action framework towards general dexterous grasping. Y Zhong, X Huang, R Li, C Zhang, Y Liang, Y Yang, Y Chen, arXiv:2502.209002025arXiv preprint</p>
<p>X Zhou, X Han, F Yang, Y Ma, A C Knoll, arXiv:2503.23463Opendrivevla: Towards end-to-end autonomous driving with large vision language action model. 2025aarXiv preprint</p>
<p>Z Zhou, Y Zhu, M Zhu, J Wen, N Liu, Z Xu, W Meng, R Cheng, Y Peng, C Shen, arXiv:2502.14420Chatvla: Unified multimodal understanding and robot control with vision-language-action model. 2025barXiv preprint</p>
<p>Robot with humanoid hands cooks food better? effect of robotic chef anthropomorphism on food quality prediction. D H Zhu, Y P Chang, International Journal of Contemporary Hospitality Management. 322020</p>
<p>M Zhu, Y Zhu, J Li, Z Zhou, J Wen, X Liu, C Shen, Y Peng, F Feng, arXiv:2502.19250Objectvla: End-to-end open-world object manipulation without demonstration. 2025arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. B Zitkovich, T Yu, S Xu, P Xu, T Xiao, F Xia, J Wu, P Wohlhart, S Welker, A Wahid, Conference on Robot Learning, PMLR. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
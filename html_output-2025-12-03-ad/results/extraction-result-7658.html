<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7658 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7658</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7658</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‚Äëtuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-269293007</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.14294v3.pdf" target="_blank">A Survey on Efficient Inference for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7658",
    "paper_id": "paper-269293007",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.012923,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Efficient Inference for Large Language Models
19 Jul 2024</p>
<p>Zixuan Zhou 
Equal contribution</p>
<p>Xuefei Ning 
Equal contribution</p>
<p>Ke Hong 
Equal contribution</p>
<p>Tianyu Fu 
Jiaming Xu 
Shiyao Li 
Yuming Lou 
Luning Wang 
Zhihang Yuan 
Xiuhong Li 
Shengen Yan 
Guohao Dai daiguohao@sjtu.edu.cn 
Fellow, IEEE, HuazhongXiao-Ping Zhang xpzhang@ieee.org 
IEEEYang Fellow 
Yuhan Dong dongyuhan@sz.tsinghua.edu.cn 
Fellow, IEEE ‚ú¶Yu Wang yu-wang@tsinghua.edu.cn 
‚Ä¢ Z Zhou 
H Yang </p>
<p>Department of Electronic Engineering
Tsinghua University
China</p>
<p>Department of Electronic Engineering
Tsinghua University
China</p>
<p>Department of Electronic Engineering
Shanghai Jiaotong University
China</p>
<p>Tsinghua Shenzhen International Graduate School</p>
<p>Peking University</p>
<p>A Survey on Efficient Inference for Large Language Models
19 Jul 202406519982B6E24D98D949F37762ECC7D3arXiv:2404.14294v3[cs.CL]
Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks.However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios.Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference.This paper presents a comprehensive survey of the existing literature on efficient LLM inference.We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadraticcomplexity attention operation, and the auto-regressive decoding approach.Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization.Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights.Last but not least, we provide some knowledge summary and discuss future research directions.</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) have garnered substantial attention from both academia and industry in recent years.The field of LLMs has experienced notable growth and significant achievements.Numerous open-source LLMs have emerged, including the GPT-series (GPT-1 [1], GPT-2 [2], and GPT-3 [3]), OPT [4], LLaMA-series (LLaMA [5], LLaMA 2 [5], Baichuan 2 [6], Vicuna [7], LongChat [8]), BLOOM [9], FALCON [10], GLM [11], and Mistral [12], which are used for both academic research and commercial purposes.The success of LLMs stems from their robust capability in handling diverse tasks such as neural language understanding (NLU), neural language generation (NLG), reasoning [13], [14], and code generation [15], consequently enabling impactful applications like ChatGPT, Copilot, and Bing.There is a growing belief [16] that the rise and achievements of LLMs signify a significant stride towards Artificial General Intelligence (AGI) for humanity.</p>
<p>Higher Computational Cost Higher Memory Access Cost</p>
<p>Higher Memory Cost Higher Latency Lower Throughput However, the deployment of LLMs is not always going smoothly.As shown in Fig. 1, LLMs typically demand higher computational cost, memory access cost and memory usage in their inference process (we will analyse the root causes in the Sec.2.3), which deteriorates the efficiency indicators (e.g., latency, throughput, power consumption and storage) in the resource-constrained scenarios.This poses challenges for the application of LLMs in both edge and cloud scenarios.For example, the immense storage requirements render the deployment of a 70-billion-parameter model impractical on personal laptops for tasks such as development assistance.Additionally, the low throughput would result in significant costs if LLMs are used for every search engine request, leading to a considerable reduction in the profits of the search engine.</p>
<p>Higher Power Consumption
Higher Storage
Fortunately, a substantial array of techniques has been proposed to enable efficient inference for LLMs.To gain a comprehensive understanding of existing studies and inspire further research, this survey employs a hierarchical classification and systematic summarization of the current landscape of efficient LLM inference.Specifically, we categorize relevant studies into three levels: data-level optimization, model-level optimization, and system-level optimization (refer to Sec. 3 for elaboration).Moreover, we conduct experimental analyses on representative methods within critical sub-fields to consolidate knowledge, offer practical recommendations, and provide guidance for future research endeavors.</p>
<p>Survey Optimization Levels</p>
<p>Experimental Analysis Data-level Model-level System-level [17], [18], [19] ‚úì [20] ‚úì ‚úì [21] ‚úì ‚úì [22] ‚úì ‚úì [23], [24] ‚úì ‚úì ‚úì Ours ‚úì ‚úì ‚úì ‚úì</p>
<p>Currently, several surveys [17], [18], [19], [20], [21], [22], [23] have been conducted in the field of efficient LLMs.These surveys primarily focus on different aspects of LLM efficiency but offer opportunities for further improvement.Zhu et al. [17], Park et al. [18], Wang et al. [19] and Tang et al. [20] concentrate on model compression techniques within model-level optimization.Ding et al. [21] center on efficiency research considering both data and model architecture perspectives.Miao et al. [22] approach efficient LLM inference from a machine learning system (MLSys) research perspective.In contrast, our survey provides a more comprehensive research scope, addressing optimization at three levels: data-level, model-level, and system-level, with the inclusion of recent advancements.While Wan et al. [23] and Xu et al. [24] also deliver comprehensive review of efficient LLM research, our work extends by incorporating comparative experiments and offering practical insights and recommendations based on experimental analyses in several critical sub-fields like model quantization and serving systems.A comparison of these surveys is summarized in Table 1.</p>
<p>The remainder of this survey is organized as follows: Sec. 2 introduces the basic concept and knowledge about LLMs and presents a detailed analysis of the efficiency bottlenecks during the inference process of LLMs.Sec. 3 demonstrates our taxonomy.Sec. 4 to Sec. 6 respectively present and discuss studies on efficiency optimization at three distinct levels.Sec.7 offers broader discussions for several key application scenarios.Sec. 8 concludes the key contributions provided by this survey.</p>
<p>PRELIMINARIES</p>
<p>Transformer-Style LLMs</p>
<p>Language modeling, as the fundamental function of language models (LMs), involves modeling the likelihood of the word sequence and predicting the distribution of subsequent words.Over recent years, researchers have discovered that scaling up language models not only enhances their language modeling ability but also engenders emergent capabilities for tackling more intricate tasks beyond conventional NLP tasks [25].These scaled-up language models are referred to as large language models (LLMs).</p>
<p>The mainstream LLMs are designed based on the Transformer architecture [26].Specifically, a typical Transformer architecture is composed of several stacked Transformer blocks.Typically, a Transformer block consists of a Multi-Head Self-Attention (MHSA) block, a Feed Forward Network (FFN), and a LayerNorm (LN) operation.For each block, it receives the output features of the previous one as the input, and passes the features through each submodule to obtain the output.Specially, before the first block, a tokenizer is used to convert the original input sentence into a sequence of tokens, and a following embedding layer serves to convert the tokens into the input features.Then, the additional position embeddings are added into the input features to encode the sequential order of each input token.</p>
<p>The core concept of the Transformer architecture is the self-attention mechanism, which is adopted in the MHSA block.Specifically, denoted the input features as X = [x 1 , x 2 , ..., x n ], the MHSA block applies linear projection to them and obtains a set of queries Q, keys K and values V as Eq.1:
Q i = XW Qi , K i = XW Ki , V i = XW Vi ,(1)
where W Qi , W Ki and W Vi are the projection matrices corresponding to the i-th attention head.Then the selfattention operation is applied to each tuple of (Q i , K i , V i ) and get the feature of the i-th attention head Z i as Eq.2:
Z i = Attention(Q i , K i , V i ) = Softmax( Q i K T i ‚àö d k )V i ,(2)
where d k is the dimension of the queries (keys).Note that the self-attention operation contains the matrix multiplication operation, its computation complexity is quadratic in the input length.Finally, the MHSA block concatenates the features of all the attention heads and applies a linear projection to them to form its output Z as Eq.3:
Z = Concat(Z 1 , Z 2 , ..., Z h )W O ,(3)
where W O is the projection matrix.As can be seen, the self-attention mechanism allows the model to identify the importance of different input parts regardless of the distance, and thus can capture the long-range dependencies and complex relationships in the input sentence.Another important module in the Transformer block is the FFN.Typically, FFN is placed after the MHSA block and consists of two linear transformation layers with a nonlinear activation function.It receives the output features X from the MHSA block and processes them as Eq 4:
FFN(X) = W 2 œÉ(W 1 X),(4)
where W 1 and W 2 denote the weight matrices of the two linear layers, and œÉ(‚Ä¢) denotes the activation function.</p>
<p>Inference Process of LLMs</p>
<p>The most popular LLMs, i.e., decoder-only LLMs, often adopt the auto-regressive method to generate the output sentence.Specifically, the auto-regressive method generates the tokens one by one.In each generation step, the LLM takes as input the whole token sequences, including the input tokens and previously generated tokens, and generates the next token.With the increase in sequence length, the time cost of the generation process grows rapidly.To address this challenge, a crucial technique, namely key-value</p>
<p>Output: ['Processing'] (1*dim)</p>
<p>Input: ['I', 'like', 'natural', 'language']
W Q W V W O Self-Attention W K Add &amp; LayerNorm Add &amp; LayerNorm FC1 FC2 Activation Q K V Softmax ùë∏ùë≤ ùëª ùíÖùíå ùëΩ
where , ,  ‚àà  √ó</p>
<p>Input: ['I', 'like', 'natural', 'language', 'Processing']</p>
<p>Output: ['! '] (1*dim)
W Q W V W O Self-Attention W K K Cache V Cache Add &amp; LayerNorm Add &amp; LayerNorm FC1 FC2 Activation Q K V Softmax ùë∏ùë≤ ùëª ùíÖùíå ùëΩ
where  ‚àà  1√ó ,  ‚àà  √ó (KV) cache, has been introduced to expedite the generation process.The KV cache technique, as its name suggests, involves storing and reusing previous key (K) and value (V) pairs within the Multi-Head Self-Attention (MHSA) block.This technique has been widely adopted in LLM inference engines and systems due to its substantial optimization of generation latency.Based on the above methods and techniques, the inference process of LLMs can be divided into two stages:</p>
<p>‚Ä¢ Prefilling Stage: The LLM calculates and stores the KV cache of the initial input tokens, and generates the first output token, as shown in Fig. 2(a).‚Ä¢ Decoding Stage: The LLM generates the output tokens one by one with the KV cache, and then updates it with the key (K) and value (V) pairs of the newly generated token, as shown in Fig. 2(b).As shown in Fig. 3, we illustrate some critical efficiency indicators.As for the latency, we denote first token latency as the latency to generate the first output token in the prefilling stage, while we denote per-output token latency as the average latency to generate one output token in the decoding stage.Besides, we use generation latency to denote the latency to generate the whole output token sequences.As for the memory, we use model size to denote the memory to store the model weights, and use KV cache size to denote the memory to store the KV cache.Additionally, peak memory denotes the maximum memory usage during the generation process, which is approximately equal to the memory sum of model weights and KV cache.Apart from the latency and memory, throughput is also a widelyused indicator in the LLM serving system.We use token throughput to denote the number of generated tokens per second, and use request throughput to denote the number of completed requests per second.</p>
<p>Efficiency Analysis</p>
<p>Deploying LLMs on resource-constrained scenarios while preserving their powerful capabilities poses a significant challenge for both practitioners and researchers.For instance, let's consider to deploy a LLaMA-2-70B model, which contains 70 billion parameters.Storing its weights in FP16 format necessitates 140 GB of VRAM, requiring at least 6 RTX 3090Ti GPUs (each with 24 GB VRAM) Fig. 3. Illustration of the memory variation through time (latency) during one generation process.Note that we ignore the activation size in this figure for a simplification.</p>
<p>or 2 NVIDIA A100 GPUs (each with 80 GB VRAM) for inference.As for latency, generating one token on 2 NVIDIA A100 GPUs requires approximately 100 milliseconds.Consequently, generating a sequence with hundreds of tokens requires more than 10 seconds.In addition to storage and latency, the efficiency indicators, such as throughput, energy and power consumption, also need to be considered.During the LLM inference process, three important factors would largely affect these indicators, i.e., the computational cost, the memory access cost and the memory usage.Yuan et al. [27] provide a more systematic analysis to demonstrate how these factors affect the inference inefficiency with a roofline model.In the following, we further analyze three root causes of inefficiency in the LLM inference process, focusing on the above three key factors:</p>
<p>‚Ä¢ Model Size: Mainstream LLMs typically incorporate billions or even trillions of parameters.For instance, the LLaMA-70B model comprises 70 billion parameters, while the GPT-3 model scales up to 175 billion parameters.This considerable model size contributes significantly to the elevated computational cost, memory access cost, and memory usage during the LLM inference process.‚Ä¢ Attention Operation: As illustrated in Sec.2.1 and Sec.2.2, in the prefilling stage, the self-attention operation exhibits quadratic computational complexity in the input length.Consequently, as the input length increases, the computational cost, memory access cost, and memory usage of the attention operation escalate rapidly.</p>
<p>‚Ä¢ Decoding Approach: The auto-regressive decoding approach generates the tokens one by one.In each decoding step, all the model weights are loaded from the offchip HBM to the GPU chip, leading to a large memory access cost.In addition, the size of KV cache increases with the growth in the input length, potentially leading to fragmented memory and irregular memory access patterns.</p>
<p>TAXONOMY</p>
<p>In the aforementioned discussion, we identify key factors (i.e., computational cost, memory access cost and memory usage) that significantly impact the efficiency during the LLM inference process, and further analyze three root causes (i.e., model size, attention operation and decoding approach).Many efforts have been made to optimize the inference efficiency from different perspectives.By carefully reviewing and summarizing these studies, we classify them into three levels, i.e., data-level optimization, model-level optimization and system-level optimization (as shown in Fig. 4):</p>
<p>‚Ä¢ Data-level Optimization refers to improving the efficiency via optimizing the input prompts (i.e., input compression) or better organizing the output content (i.e., output organization).</p>
<p>is typically lossless in model performance 1 .In addition, we provide a brief introduction for hardware accelerator design in Sec.6.3.</p>
<p>DATA-LEVEL OPTIMIZATION</p>
<p>In the data level, prior studies can be divided into two categories, i.e., input compression and output organization.Input compression techniques directly shorten the model input to reduce the inference cost.While output organization techniques enable batch (parallel) inference via organizing the structure of output content, which can improve the hardware utilization and reduce the generation latency.</p>
<p>Input Compression</p>
<p>In the practical application of LLMs, prompts are crucial.</p>
<p>Numerous studies suggest new ways to design prompts effectively and show in practice that well-designed prompts can unleash the capabilities of LLMs.For instance, In-Context Learning (ICL) [45] suggests to include multiple relevant examples within the prompt.This approach encourages LLMs to learn through analogy.Chain-of-Thought (CoT) [14] proposes to incorporate a sequence of intermediate reasoning steps within the in-context examples, which help LLMs to conduct complex reasoning.However, these prompting techniques inevitably lead to longer prompts, which poses a challenge because the computational cost and memory usage increase quadratically during the prefilling stage (as illustrated in Sec.2.3).</p>
<p>To address this challenge, input prompt compression [33] has been proposed to shorten prompts without significantly impacting the quality of answers from LLMs.Within this field, relevant studies are categorized into four groups, as depicted in Figure 5: prompt pruning, prompt summary, soft prompt-based compression, and retrievalaugmented generation.</p>
<p>Prompt Pruning</p>
<p>The core idea behind the prompt pruning is to remove unimportant tokens, sentences, or documents online from each input prompt based on predefined or learnable importance indicators.DYNAICL [38] proposes to dynamically decide the optimal number of in-context examples for a given input based on the computational budget via a welltrained LLM-based meta controller.Selective Context [39] proposes to merge tokens into units, and then applies a unit-level prompt pruning based on the self-information indicator (i.e., negative log likelihood).STDC [40] prunes the prompts based on the parse tree, which iteratively removes phrase nodes that cause the smallest performance drop after pruning it.PCRL [41] introduces a token-level pruning scheme based on reinforcement learning.The main idea behind PCRL is to train a policy LLM by combining faithfulness and compression ratio into the reward function.Faithfulness is measured as the output similarity between the compressed prompt and the original prompt.RECOMP [36] implements a sentence-level pruning strategy to compress prompts for Retrieval-Augmented Language Models (RALMs).The approach involves encoding the input question and documents into latent embeddings using a pre-trained encoder.Then, it decides which documents to remove based on the similarity of their embeddings with the question's embedding.LLMLingua [42] introduces a coarse-to-fine pruning scheme for prompt compression.Initially, it performs a demonstration-level pruning followed by token-level pruning based on perplexity.To enhance performance, LLMLingua proposes a budget controller that dynamically allocates the pruning budget across different parts of prompts.Additionally, it utilizes an iterative tokenlevel compression algorithm to address inaccuracies introduced by conditional independence assumptions.Furthermore, LLMLingua incorporates a distribution alignment strategy to align the output distribution of the target LLM with a smaller LLM used for perplexity calculation.LongLLMLingua [43] builds upon LLMLingua with several enhancements: (1) It utilizes perplexity conditioned on the input question as the indicator for prompt pruning.(2) It allocates varying pruning ratios to different demonstrations and reorders the demonstrations within the final prompt based on their indicator values.(3) It restores the original content based on the response.CoT-Influx [44] introduces a coarse-to-grained pruning method for Chain-of-Thought (CoT) prompts using reinforcement learning.Specifically, it prunes unimportant examples, followed by pruning unimportant tokens within the remaining examples.</p>
<p>Prompt Summary</p>
<p>The core idea of prompt summary is to condense the original prompt into a shorter summary while preserving similar semantic information.These techniques also serve as online compression methods for prompts.In contrast to the aforementioned prompt pruning techniques that preserve the unpruned tokens unchanged, this line of methods converts the entire prompt into its summation.RECOMP [36] introduces an Abstractive Compressor that takes an input question and retrieved documents as input, and produces a concise summary.Specifically, it distills a lightweight compressor from the extreme-scale LLMs to perform the summary.SemanticCompression [37] proposes a semantic compression method.It starts by breaking down the text into sentences.Next, it groups sentences together by topic and then summarizes the sentences within each group.</p>
<p>Soft Prompt-based Compression</p>
<p>The core idea of this kind of compression techniques is to design a soft prompt, significantly shorter than the original prompt, for use as input to LLMs.The soft prompt is defined as a sequence of learnable continuous tokens.Some techniques adopt offline compression for the fixed prefix prompt (e.g., system prompt, task-specific prompt).For example, PromptCompression [33] trains a soft prompt to emulate a predetermined system prompt.The approach involves adding several soft tokens before the input tokens and enabling these soft tokens to be adjusted during backpropagation.Following fine-tuning on the prompt dataset, the sequence of soft tokens serves as the soft prompt.Gisting [34] introduces a method to condense task-specific</p>
<p>Input Compression</p>
<p>Retrieval-Augmented Generation RAG [29], FLARE [30], REPLUG [31], Self-RAG [32] Soft Prompt-based Compression PromptCompression [33], Gisting [34], Auto-Compressors [30], ICAE [35] Prompt Summary RECOMP [36], SemanticCompression [37] Prompt Pruning DYNAICL [38], Selective Context [39], STDC [40], PCRL [41], RECOMP [36], LLM-Lingua [42], LongLLMLingua [43], CoT-Influx [44] Fig. 5. Taxonomy of the input compression methods for Large Language Models.</p>
<p>prompts into a concise set of gist tokens using prefixtuning [46].Given that task-specific prompts differ across tasks, prefix-tuning is applied individually for each task.</p>
<p>To enhance efficiency, Gisting further introduces a metalearning approach that predicts gist tokens for new unseen tasks based on the gist tokens of previous tasks.Other techniques adopt online compression for every new input prompts.For instance, AutoCompressors [30] train a pre-trained LM to compress the prompts into summary vectors via unsupervised learning.ICAE [35] trains an autoencoder to compress the original context into short memory slots.Specifically, ICAE employs a LoRA-adapted LLM as the encoder, and uses the target LLM as the decoder.A set of memory tokens is added before the input tokens and encoded into memory slots.</p>
<p>Retrieval-Augmented Generation</p>
<p>Retrieval-Augmented Generation (RAG) [29] aims to improve the quality of LLMs' responses by incorporating external knowledge sources.RAG can be also viewed as a technique to improve the inference efficiency when handling a large amount of data.Instead of merging all information into an excessively long prompt, RAG only adds relevant retrieved information to the original prompt, ensuring that the model receives necessary information while reducing prompt length significantly.FLARE [30] uses predictions of upcoming sentences to proactively decide when and what information to retrieve.REPLUG [31] treats the LLM as a black box and augments it with a tuneable retrieval model.It prepends retrieved documents to the input for the frozen black-box LLM, and further utilizes the LLM to supervise the retrieval model.Self-RAG [32] enhances LLM's quality and factuality through retrieval and self-reflection.It introduces reflection tokens to make the LLM controllable during the inference phase.</p>
<p>Output Organization</p>
<p>The traditional generation process of LLMs is entirely sequential, leading to significant time consumption.Output organization techniques aim to (partially) parallelize generation via organizing the structure of output content.</p>
<p>Skeleton-of-Thought (SoT) [47] is pioneering in this direction.The core idea behind SoT is to leverage the emerging ability of LLMs to plan the output content's structure.Specifically, SoT consists of two main phases.In the first phase (i.e., skeleton phase), SoT instructs the LLM to generate a concise skeleton of the answer using a predefined "skeleton prompt."For instance, given a question like "What are the typical types of Chinese dishes?", the output at this stage would be a list of dishes (e.g., noodles, hot pot, rice) without elaborate descriptions.Then, in the second phase (i.e., point-expanding phase), SoT instructs the LLM to expand each point in the skeleton simultaneously using a "point-expanding prompt," and then concatenates these expansions to form the final answer.When applied to opensource models, point-expanding can be performed through batch inference, which optimizes hardware utilization and reduces overall generation latency using the same computational resources.overhead brought by the extra prompt (i.e., skeleton prompt and point-expanding prompt), SoT discusses the possibility of sharing the KV cache of the common prompt prefix across multiple points in the point expansion phase.Additionally, SoT uses a router model to decide whether applying SoT is appropriate for specific questions, aiming to limit its use to suitable cases.As a result, SoT achieves up to a 2.39√ó speedup on 12 recently released LLMs, and improves the answer quality for many questions by improving the diversity and relevance of their answer.SGD [48] further extends the idea of SoT by organizing sub-problem points into a Directed Acyclic Graph (DAG) and answering the logic-independent sub-problems in parallel in one turn.Similar to SoT, SGD also leverages the emerging ability of LLMs to generate the output structure by providing manually-crafted prompts along with several examples.SGD relaxes the strict independence assumption among different points to enhance the quality of answers, especially for math and coding problems.Compared with SoT, SGD prioritizes answer quality over speed.Additionally, SGD introduces an adaptive model selection approach, assigning an optimal model size to handle each sub-problem based on its estimated complexity, thus further improving efficiency.</p>
<p>APAR [49] adopts a similar idea with SoT, leveraging LLMs to output special control tokens (i.e., [fork]) for automatically and dynamically triggering the parallel decoding.To effectively exploit the inherent parallelizable structure within the output content and accurately generate control tokens, APAR fine-tunes the LLMs on carefully-designed data that formed in specific tree structure.As a result, APAR achieves an average 1.4‚àº2.0√óspeed-up on benchmarks and cases a negligible impact on the answer quality.Furthermore, APAR combines their decoding approach with the speculative decoding technique (i.e., Medusa [50]) and serving system (i.e.vLLM [51]) to further improve the inference latency and system throughput, respectively.</p>
<p>SGLang [52] introduces a domain-specific language (DSL) in Python featuring primitives that flexibly facilitate LLM programming.The core idea behind SGLang is to analyze dependencies among various generation calls automatically, and perform batch inference and KV cache sharing based on this analysis.With this language, users can implement various prompting strategies easily and benefit from the automatic efficiency optimization of SGLang (e.g., SoT [47], ToT [53]).Furthermore, SGLang introduces and combines several system-level compilation techniques, such as code movement and prefetching annotations.</p>
<p>Knowledge, Suggestions and Future Direction</p>
<p>The growing demand for LLMs to handle longer inputs and generate longer outputs highlights the importance of the data-level optimization techniques.Within these techniques, input compression methods primarily target enhancing the prefilling stage by diminishing the computational and memory cost resulting from the attention operation.Additionally, for API-based LLMs, these methods can reduce the API cost associated with input tokens.In contrast, output organization methods concentrate on optimizing the decoding stage by alleviating the substantial memory access cost associated with auto-regressive decoding approach.</p>
<p>As LLMs become more and more capable, there is potential to utilize them to compress the input prompts or structure the output content.Recent advancements in output organization methods [47], [48], [49] demonstrate the effectiveness of leveraging LLMs to organize the output content into independent points or a dependency graph, facilitating batch inference for improving generation latency.These methods capitalize on the inherent parallelizable structure within output content, enabling LLMs to perform parallel decoding to enhance hardware utilization and thereby reduce end-to-end generation latency.</p>
<p>Recently, diverse prompting pipelines (e.g., ToT [53], GoT [54]) and agent frameworks [55], [56], [57] are emerging.While these innovations enhance LLMs' capabilities, they also extend the length of inputs, leading to increased computational cost.To address this challenge, adopting input compression techniques to reduce input length shows promise as a solution.Simultaneously, these pipelines and frameworks naturally introduce more parallelism into output structures, offering increased potential for parallel decoding and key-value (KV) cache sharing across different decoding threads.SGLang [52] supports flexible LLM programming and offers opportunities for front-end and backend co-optimization, laying the groundwork for further extensions and improvements in this area.In summary, data-level optimization, including input compression and output organization techniques, would become increasingly necessary to enhance efficiency in the foreseeable future.In addition to optimizing the efficiency of existing frameworks, certain studies focus on designing more efficient agent frameworks directly.For example, FrugalGPT [58] proposes a model cascade comprising LLMs of varying sizes, with the inference process being halted early if the model reaches a sufficient level of certainty regarding the answer.This approach aims to achieve efficiency by leveraging a tiered model architecture and intelligent inference termination based on model confidence estimation.Compared with model-level dynamic inference techniques (Sec.5.2.5),FrugalGPT performs dynamic inference at the pipeline level.</p>
<p>MODEL-LEVEL OPTIMIZATION</p>
<p>The model-level optimization for LLM efficient inference mainly concentrates on optimizing the model structure or data representation.Model structure optimization involves directly designing efficient model structure, modifying the original model and adjusting the inference-time architecture.In terms of data representation optimization, the model quantization technique is commonly employed.</p>
<p>In this section, we categorize model-level optimization techniques based on the additional training overhead they require.The first category involves designing more efficient model structures (referred to as efficient structure design).Models developed using this approach typically require training from scratch.The second category focuses on compressing pre-trained models (referred to as model compression).Compressed models in this category generally require only minimal fine-tuning to restore their performance.</p>
<p>Efficient Structure Design</p>
<p>Currently, state-of-the-art LLMs commonly employ the Transformer architecture, as discussed in Section 2.1.However, the key components of Transformer-based LLMs, including the Feed Forward Network (FFN) and attention operation, present efficiency challenges during inference.We identify the causes as follows:</p>
<p>‚Ä¢ The FFN contributes a substantial portion of the model parameters in Transformer-based LLMs, resulting in significant memory access cost and memory usage, particularly during the decoding stage.For instance, the FFN module accounts for 63.01% of the parameters in the LLaMA-7B model and 71.69% in the LLaMA-70B model.‚Ä¢ The attention operation demonstrates quadratic complexity in the input length, leading to substantial computational cost and memory usage, especially when dealing with longer input contexts.To tackle these efficiency challenges, several studies have concentrated on developing more efficient model structures.We categorize these studies into three groups (as depicted in Fig. 7): efficient FFN design, efficient attention design, and Transformer alternates.</p>
<p>Efficient Structure Design</p>
<p>Transformer Alternates</p>
<p>Others</p>
<p>SGConv [59], CKConv [60], Hyena [61], RWKV [62], RetNet [63] SSM HiPPO [64], LSSL [65], S4 [66], DSS [67], S4D [68], GSS [69], H3 [70], Liquid S4 [71], S5 [72], BST [73], BiGS [74], Mamba [75], MambaFormer [76] Efficient Attention Design Multi/Group-Query Attention MQA [77], GQA [78] Low-Complexity Attention</p>
<p>Low-Rank Attention</p>
<p>Linformer [79], LRT [80], FLuRKA [81],Luna [82],</p>
<p>Set Transformer [83] Kernel-based Attention</p>
<p>Linear Transformer [84], Performers [85], RFA [86], PolySketchFormer [87] Efficient FFN Design Switch Transformers [88], MoEfication [89], MPOE [90], Sparse Upcycling [91], BASE [92], Expert Choice [93], SE-MoE [94], StableMoE [95], SMoE-Dropout [96], GLaM [97], Mixtral 8x7B [12] Fig. 7. Taxonomy of the efficient structure design for Large Language Models.</p>
<p>Efficient FFN Design</p>
<p>In this field, many studies concentrate on integrating the Mixture-of-Experts (MoE) technique [98] into LLMs to enhance their performance while maintaining the computational cost.The core idea of MoE is to dynamically allocate varying computational budgets to different input tokens.In MoE-based Transformers, multiple parallel Feed Forward Networks (FFNs), namely experts, are utilized alongside a trainable routing module.During inference, the model selectively activates specific experts for each token controlled by the routing module.Some researches concentrate on the construction of FFN expert, which mainly focus on optimizing the process of acquiring expert weights or making these experts more lightweight for efficiency.For instance, MoEfication [89] devises a method to transform a non-MoE LLM into the MoE version using its pre-trained weights.This approach eliminates the need for expensive pre-training of the MoE model.To accomplish this, MoEfication first divides FFN neurons of the pre-trained LLM into multiple groups.Within each group, the neurons are commonly activated simultaneously by the activation function.Then, it restructures each group of neurons as an expert.Sparse Upcycling [91] introduces a method to initialize the weights of MoE-based LLM directly from a dense model's checkpoint.In this approach, the experts within the MoE-based LLM are exact replicas of the FFN from the dense model.By employing this straightforward initialization, Sparse Upcycling can efficiently train the MoE model to achieve high performance.MPOE [90] proposes to reduce the parameters of MoE-based LLMs through Matrix Product Operators (MPO) decomposition.This method involves decomposing each weight matrix of the FFN into a global shared tensor containing common information and a set of local auxiliary tensors that capture specialized features.</p>
<p>Another line of researches focuses on improving the design of the routing module (or strategy) within MoE models.In previous MoE models, the routing module often causes the load imbalance problem, which denotes that some experts are assigned a large number of tokens while the others handle only a few.This imbalance not only wastes the capacities of the under-utilized experts, which degrades model performance, but also degrades the inference efficiency.Current MoE implementations [88], [99], [100] often use batched matrix multiplication to compute all FFN experts simultaneously.This requires that the input matrices of each expert must have the same shape.However, since the load imbalance problem exists, input token sets for these under-utilized experts are needed to be padded to meet the shape constraint, resulting in a waste of computation.Therefore, the major aim of routing module design is achieving better balance in token assignment for MoE experts.Switch Transformers [88] introduces an additional loss, namely the load balancing loss, into the final loss function to penalize imbalanced assignments by the routing module.This loss is formulated as the scaled dot-product between the token assignment fraction vector and a uniform distribution vector.As a result, the loss is minimized only when the token assignment is balanced across all experts.This approach encourages the routing module to distribute tokens evenly among experts, promoting load balance and ultimately improving model performance and efficiency.BASE [92] learns an embedding for each expert in an endto-end manner and then assigns experts to tokens based on the similarity of their embeddings.To ensure load balance, BASE formulates a linear assignment problem and utilizes the auction algorithm [101] to solve this problem efficiently.Expert Choice [93] introduces a simple yet effective strategy to ensure perfect load balance within MoE-based models.Unlike previous methods that assign experts to tokens, Expert Choice allows each expert to independently select the top-k tokens based on their embedding similarities.This approach ensures that each expert handles a fixed number of tokens, even though each token might be assigned to a different number of experts.</p>
<p>In addition to the aforementioned researches focusing on the model architecture itself, there are also studies that concentrate on improving the training methods for MoEbased models.SE-MoE [94] introduces a new auxiliary loss called the router z-loss, which aims to enhance the stability of model training without compromising performance.SE-MoE identifies that the exponential functions introduced by softmax operations in the routing module can exacerbate roundoff errors, leading to training instability.To address this issue, the router z-loss penalizes large logits that are input into exponential functions, thereby minimizing roundoff errors during training.StableMoE [95] points out the routing fluctuation problem existing in the MoE-based LLMs, which denotes the inconsistency of the expert assignment in the training and inference stage.For the same input token, it is assigned to different experts along with training, but only activates one expert at inference time.To address this issue, StableMoE suggests a more consistent training approach.It first learns a routing strategy and then keeps it fixed during both the model backbone training and the inference stage.SMoE-Dropout [96] designs a novel training method for MoE-based LLMs, which proposes to gradually increase the number of activated experts during the training process.This approach enhances the scalability of MoE-based models for inference and downstream fine-tuning.GLaM [97] pre-trains and releases a series of models with various parameter sizes, demonstrating their comparable performance to dense LLMs on few-shot tasks.The largest model in this family has a parameter size of up to 1.2 trillion.Mixtral 8x7B [12] is a remarkable recently released open-source model.During inference, it utilizes only 13 billion active parameters and achieves superior performance compared to the LLaMA-2-70B model across different benchmarks.Mixtral 8x7B consists of 8 Feed-Forward Network (FFN) experts in each layer, with each token assigned to two experts during inference.</p>
<p>Efficient Attention Design</p>
<p>The attention operation is a critical component in the Transformer architecture.However, its quadratic complexity in relation to input length leads to substantial computational cost, memory access cost, and memory usage, especially when dealing with long contexts.To address this issue, researchers are exploring more efficient approaches to approximate the functionality of the original attention operation.These studies can be broadly categorized into two main branches: multi-query attention and low-complexity attention.Multi-Query Attention.Multi-query attention (MQA) [77] optimizes the attention operation by sharing the key (K) and value (V) cache across different attention heads.This strategy effectively reduces both memory access cost and memory usage during inference, contributing to improved efficiency in Transformer models.As introduced in Sec.2.2, the Transformer-style LLMs typically adopts multi-head attention (MHA) operation.This operation requires storing and retrieving K and V pairs for each attention head during the decoding stage, leading to substantial increases in memory access cost and memory usage.MQA tackles this challenge by using the same K and V pairs across different heads while maintaining distinct query (Q) values.Through extensive testing, it has been demonstrated that MQA significantly reduces memory requirements with only a minimal impact on model performance, making it a crucial strategy for enhancing inference efficiency.The concept of MQA is further extended by Grouped-query attention (GQA) [78], which can be seen as a blend of MHA and MQA.Specifically, GQA segments the attention heads into groups, storing a single set of K and V values for each group.This method not only sustains the benefits of MQA in reducing memory overhead but also offers an enhanced balance between inference speed and output quality.Low-Complexity Attention.Low-complexity attention methods aim to design new mechanisms that reduce the computational complexity of each attention head.To simplify the discussion, we assume that the dimensions of the Q (query), K (key), and V (value) matrices are identical, with Q, K, V ‚àà R n√ód .Since the following work does not involve altering the number of attention heads like MQA, our discussions focus on the attention mechanism within each head.As introduced in Section 2.2, the computational complexity of the conventional attention mechanism scales as O(n 2 ), exhibiting quadratic growth with respect to the input length n.To address the inefficiency issue, kernel-based attention and low-rank attention methods are proposed to reduce the complexity to O(n).</p>
<p>‚Ä¢ Kernel-based Attention.Kernel-based attention designs kernel œï to approximate the non-linear softmax operation of Softmax(QK T ) with a linear dot product between kernel-transformed feature maps, i.e., œï(Q)œï(K) T .It avoids the conventional quadratic computation associated with QK T ‚àà R n√ón by prioritizing the computation of œï(K) T V ‚àà R d√ód , followed by its multiplication with œï(Q) ‚àà R n√ód .Specifically, the input Q and K matrices are first mapped into kernel space using a kernel function œï, while maintaining their original dimensions.Leveraging the associative property of matrix multiplication allows for the multiplication of K and V prior to their interaction with Q.The attention mechanism is reformulated as:
Softmax(QK T )V ‚âà œï(Q)(œï(K) T V ),(5)
where œï(Q), œï(K) ‚àà R n√ód .This strategy effectively reduces the computational complexity to O(nd 2 ), rendering it linear with respect to the input length.Linear Transformer [84] is the first work to propose the kernelbased attention.It adopts œï(x) = elu(x) + 1 as the kernel function, where elu(‚Ä¢) denotes the exponential linear unit activation function.Performers [85] and RFA [86] proposes to use random feature projection to better approximate the softmax function.PolySketchFormer [87] employs polynomial functions and sketching techniques to approximate the softmax function.</p>
<p>‚Ä¢ Low-Rank Attention.Low-Rank Attention technique employs compression on the token dimensions (i.e., n) of the K and V matrices to a smaller, fixed length (i.e., k) before performing the attention computation.The approach is based on the insight that the n √ó n attention matrix often exhibits a low-rank property, making it feasible to compress it in the token dimension.The main focus of this line of researches is to design effective methods for the compression, where X can be context matrix or K and V matrices:
X ‚àà R n√ód ‚Üí X ‚Ä≤ ‚àà R k√ód .(6)
One line of work uses linear projection to compress the token dimension.It is done by multiplying K and V matrices with projection matrices P k , P v ‚àà R k√ón .In this way, the computational complexity of the attention operation is reduced to O(nkd), which is linear to the input length.Linformer [79] first observes and analyses the low-rank property of the attention map, and proposes the low-rank attention framework.LRT [80] proposes to simultaneously apply low-rank transformation to both attention block and FFN to further improve the computational efficiency.FLuRKA [81] combines the low-rank transformation and kernalization to the attention matrices to further improve the efficiency.Specifically, it first reduces the token dimension of K and V matrices, and then applies kernel function to the Q and low-rank K matrices.</p>
<p>Aside from linear projection, other token-dimension compression methods are also proposed.Luna [82] and Set Transformer [83] leverage additional attention computations alongside smaller queries to effectively compress the K and V matrices.Luna [82] involves an extra query matrix of fixed length k.The small query performs attention with the original context matrix, termed as pack attention, to compress the context matrix to size R k√ód .Subsequently, the regular attention, termed unpack attention, applies attention to the original Q matrices and the compressed K and V matrices.The extra query matrix can be learnable parameters or acquired from previous layers.Set Transformer [83] designs the similar technique by introducing an inducing points vector with fixed length.Unlike previous works that compress K and V, Funnel-Transformer [102] uses pooling operation to gradually compress the sequence length of the Q matrix.</p>
<p>Transformer Alternates</p>
<p>In addition to applying efficient techniques to the attention operation, recent studies have also innovated to design sequence modeling architectures that are efficient yet effective.Table 2 compares the efficiency of some representative non-Transformer models.These architectures exhibit sub-quadratic computational complexity with respect to sequence length during both training and inference, enabling LLMs to significantly increase their context length.</p>
<p>Within this research field, two prominent lines of study have garnered significant attention.One line of studies concentrates on the State Space Model (SSM), which formulates sequence modeling as a recurrence transformation based on the HiPPO theory [64].Additionally, other studies primarily focus on employing long convolutions or designing attention-like formulations to model sequences.</p>
<p>State Space Model. The State Space Model (SSM) has demonstrated competitive modeling capabilities in certain</p>
<p>Natural Language Processing (NLP) [75] and and Computer Vision (CV) [103] tasks.Compared to attention-based Transformers, SSM exhibits linear computational and memory complexity with respect to the input sequence length, which enhances its efficiency in handling long-context sequences.In this survey, SSM refers to a series of model architectures that satisfy the following two properties: (1) They model sequence based on the following formulation proposed by HiPPO [64] and LSSL [65]:
x k = Ax k‚àí1 + Bu k , y k = Cx k ,(7)
where A, B and C denote the transition matrices, x denotes the intermediate state and u denotes the input sequence.(2) They design the transition matrix A based on the HiPPO theory [64].Specifically, HiPPO proposes to compress the input sequence into a sequence of coefficients (namely state) by projecting it onto a set of polynomial bases.</p>
<p>Building upon the aforementioned framework, several studies concentrate on improving the parameterization or initialization of the transition matrix A. This involves refining how the matrix is formulated or initialized within the SSM to enhance its effectiveness and performance in sequence modeling tasks.LSSL [65] firstly proposes to initialize A with the optimal transition matrix HiPPO-LegS designed by HiPPO.In addition, LSSL also trains the SSM in a convolution manner by unrolling the Eq. 7. Specifically, through a convolution kernel defined as
K L (A, B, C) = (CA i B) i‚àà[L] = (CB, CAB, ..., CA L‚àí1 B
), the Eq.7 can be rewritten as y = K L (A, B, C) * u and also can be computed efficiently via Fast Fourier Transform (FFT).However, computing this convolution kernel is expensive, since it requires multiple times of multiplication by A. To this end, S4 [66], DSS [67] and S4D [68] propose to diagonalize the matrix A, which can accelerate the computing.This can be seen as a parameterization technique to the transition matrix A. Previous SSMs processed each input dimension independently, resulting in a large number of trainable parameters.To enhance efficiency, S5 [72] proposes to simultaneously process all input dimensions using a single set of parameters.Building upon this structure, S5 introduces a parameterization and initialization method for A based on the standard HiPPO matrix.Liquid S4 [71] and Mamba [75] parameterize the transition matrices in a input-dependent manner, which further enhances the modeling capability of SSM.Additionally, both S5 [72] and Mamba [75] adopt a parallel scan technique for efficient model training without the need for convolution operations.This technique offers advantages in implementation and deployment on modern GPU hardware.</p>
<p>Another line of research aim to design better model architecture based on SSMs.GSS [69] and BiGS [74] combines the Gated Attention Unit (GAU) [104] with SSM.Specifically, they replace the attention operation in GAU with SSM operation.BST [73] combines the SSM model with the proposed Block Transformer which introduces a strong local inductive bias.H3 [70] observes that SSM is weak in recalling the earlier tokens and comparing a token across the sequence.To this end, it proposes to add a shift SSM operation before the standard SSM operation, which is used to directly shift the input tokens into the state.MambaFormer [76] combines the standard Transformer and SSM model by substituting the FFN layer in the Transformer with an SSM layer.Jamba [105] introduces another approach to combining the Transformer and SSM models by adding four Transformer layers into an SSM model.Dense-Mamba [106] explores the issue of hidden state degradation Transformer [26] Transformer-like
O(n 2 d) O(n 2 + nd) Transformer-like O(n 2 d) O(nd) S4 [66] Convolution O(nd 2 log n) O(nd) Recurrence O(nd 2 ) O(d 2 ) Mamba [75] Recurrence O(nd 2 log n) O(nd) Recurrence O(nd 2 ) O(d 2 ) Hyena [61] Convolution O(nd log n) O(nd) Convolution O(nd log n) O(nd log n) RetNet [63] Transformer-like O(n 2 d) O(n 2 + nd) Recurrence O(nd 2 ) O(d 2 ) RWKV [62] Recurrence O(nd 2 ) O(nd) Recurrence O(nd 2 ) O(d 2 )
in traditional SSMs and introduces dense connections within the SSM architecture to preserve fine-grained information across deeper layers of the model.BlackMamba [107] and MoE-Mamba [108] propose to enhance SSM models with the Mixture-of-Experts (MoE) technique to optimize the training and inference efficiency while maintaining the model performance.</p>
<p>Other Alternates.In addition to SSMs, several other efficient alternates have also garnered significant attention, including long convolution and attention-like recurrence operation.</p>
<p>Several recent studies have applied long convolution in the context of modeling long sequences [59], [60], [61].These investigations primarily concentrate on refining the parameterization of the convolution kernel.For instance, Hyena [61] employs an data-dependent parameterization method for long convolutions using a shallow feed-forward neural network (FFN).</p>
<p>Other studies [62], [63] aim to design the operation that has a similar form as the attention operation but can be enrolled to the recurrent manner, enabling both efficient training and efficient inference.For instance, RWKV [62] builds upon AFT [109], which proposes to substitute the attention operation in the Transformer model with the following equation:
Y t = œÉ q (Q t ) ‚äô T t ‚Ä≤ =1 exp(K t ‚Ä≤ + w t,t ‚Ä≤ ) ‚äô V t ‚Ä≤ T t ‚Ä≤ =1 exp(K t ‚Ä≤ + w t,t ‚Ä≤ ) ,(8)
where Q, K, and V are the query, key, and value matrices as in Transformer, w ‚àà R T √óT denotes a learnable pairwise position bias and œÉ q (‚Ä¢) denotes a non-linear function.Specifically, it further reparameterizes the position bias as On the other hand, during inference, most studies opt for recurrent architectures to maintain linear computational complexity in the prefilling stage and to remain context length-agnostic in the decoding stage.Furthermore, in the decoding phase, these novel architectures eliminate the need to cache and load features of previous tokens (similar to the key-value cache in Transformer-based language models), resulting in significant memory access cost savings.
w t,t ‚Ä≤ = ‚àí(t ‚àí t ‚Ä≤ )w,</p>
<p>Model Compression</p>
<p>Model compression encompasses a range of techniques designed to enhance the inference efficiency of a pre-trained model by modifying its data representation (e.g., quantization) or altering its architecture (e.g., sparsification, structural optimization, and dynamic inference), as depicted in Fig. 8.</p>
<p>Quantization</p>
<p>Quantization is a widely employed technique that reduces the computational and memory cost of LLMs by converting the models' weights and activations from high bit-width to low bit-width representations.Specifically, many methods involve quantizing FP16 tensors into low-bit integer tensors, which can be represented as follows:
X INT = X FP16 ‚àí Z S ,(9)S = max(X FP16 ) ‚àí min(X FP16 ) 2 N ‚àí1 ‚àí 1 ,(10)
where X FP16 denotes the 16-bit floating-point (FP16) value, X INT denotes the low-precision integer value, N denotes the number of bits, and S and Z denote the scaling factor and zero-point.</p>
<p>In the following, we start with an efficiency analysis to illustrate how quantization techniques reduce the end-toend inference latency of LLMs.Subsequently, we offer a detailed introduction to two distinct quantization workflows: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT), respectively.Efficiency Analysis.As discussed in Section 2.2, the inference process of LLMs involves two stages: the prefilling stage and the decoding stage.During the prefilling stage, LLMs typically handle long token sequences, and the primary operation is general matrix multiplication (GEMM).The latency of the prefilling stage is primarily constrained by the computation performed by high-precision CUDA Cores.To address this challenge, existing methods quantize both weights and activations to accelerate computation using low-precision Tensor Cores.As illustrated in Figure 9 (b), activation quantization is performed online before each GEMM operation, allowing computation with low-precision</p>
<p>Model Compression</p>
<p>Dynamic Inference Token-level CALM [110], SkipDecode [111] Sample-level FastBERT [112], MPEE [113], Global Past-Future Early Exit [114], DeeBERT [115], PABEE [116],HASHEE [117] Knowledge Distillation</p>
<p>Black-box KD Multitask-ICT [118], MCKD [119], Distilling</p>
<p>Step-by-Step [120], SCoTD [121], CoT Prompting [122], MCC-KD [123], Fine-tune-CoT [124], Socratic CoT [125], PaD [126], SCOTT [127], DISCO [128], LaMini-LM [129], Lion [130] White-box KD MiniLLM [131], GKD [132], TED [133], BabyLlama [134], MiniMoE [135], DynaBERT [136], KPTD [137] Structure Optimization</p>
<p>Neural Architecture Search AutoTinyBERT [138], NAS-BERT [139], Structure pruning via NAS [140], LiteTransform-erSearch [141], AutoDistil [142] Structure Factorization LoRD [143], TensorGPT [144], LoSparse [145], LPLR [146], ZeroQuant-V2 [147], DS-Former [148], ASVD [149] Sparsification Sparse Attention Sparse Transformer [150], StreamingLLM [151], Longformer [152], Bigbird [153], Structured Sparse Attention [154], SemSA [155], Spatten [156], SeqBoat [157], Adaptively Sparse Attention [158], Reformer [159], Sparse Flash Attention [160], Routing Transformer [161], Sparse Sinkhorn Attention [162],
H 2 O [163],
Diffuser [164] Weight Pruning SparseGPT [165], Wanda [166], ISC [167], Prune and Tune [168], OWL [169], BESA [170], oBERT [171], FastPruning [172], RIA [173], LLM-Pruner [174], Sheared LLaMA [175], ZipLM [176], LoRAPrune [177], LoRAShear [178], SliceGPT [179], PLATON [180],</p>
<p>CoFi [181], SIMPLE [182], ExpertSparsity [183], SEER-MoE [184], Pruner-Zero [185], DS√òT [186] Quantization Quantizationaware Training LLM-QAT [187], Norm Tweaking [188], QLoRA [189], QA-LoRA [190], LoftQ [191] Post-Training Quantization GPTQ [192], LUT-GEMM [193], AWQ [194], OWQ [195], SpQR [196], SqueezeLLM [197], QuIP [198], FineQuant [199], QuantEase [200], LLM-MQ [201], ZeroQuant [202], Flex-Gen [203], LLM.int8() [204], Smoothquant [205], ZeroQuant-V2 [206], RPTQ [207], OliVe [208], OS+ [169], ZeroQuant-FP [209], Omniquant [210], QLLM [211], ATOM [212], LLM-FP4 [187], BiLLM [213], Li et.al.[214], AffineQuant [215], QuIP [198], QuIP# [216], QuaRot [217], SpinQuant [218] Fig. 8. Taxonomy of model compression methods for Large Language Models.</p>
<p>Tensor Cores (e.g., INT8).This quantization approach is referred to as Weight-Activation Quantization.</p>
<p>In contrast, during the decoding stage, LLMs process only one token at each generation step using general matrixvector multiplication (GEMV) as the core operation.The latency of the decoding stage is mainly influenced by the loading of large weight tensors.To tackle this challenge, existing methods focus on quantizing only the weights to accelerate memory access.This method, known as Weightonly Quantization, involves offline quantization of weights, followed by de-quantization of the low-precision weights into FP16 format for computation, as shown in Figure 9 (a).</p>
<p>Post-Training Quantization.Post-training quantization (PTQ) involves quantizing pre-trained models without the need for retraining, which can be a costly process.While PTQ methods have been well-explored for smaller models, applying existing quantization techniques directly to LLMs presents challenges.This is primarily because the weights and activations of LLMs often exhibit more outliers and have a wider distribution range compared to smaller  [192] ‚úì Uniform Statistic-based ‚úì LUT-GEMM [193] ‚úì Non-uniform Statistic-based AWQ [194] ‚úì Uniform Search-based ‚úì SqueezeLLM [197] ‚úì models, making their quantization more challenging.In summary, the complex nature of LLMs, characterized by their size and complexity, requires specialized approaches to effectively handle the quantization process.The presence of outliers and wider distribution ranges in LLMs necessitates the development of tailored quantization techniques that can account for these unique characteristics without compromising model performance or efficiency.Numerous studies have concentrated on developing effective quantization algorithms to compress LLMs.We provide a synthesis of representative algorithms categorized across four dimensions in Tab. 3. Regarding the types of quantized tensors, certain studies [192], [193], [194], [197] concentrate on weight-only quantization, whereas many others [204], [205], [207] focus on quantizing both weights and activations.Notably, in LLMs, the KV cache represents a distinctive component that impacts memory and memory access.Consequently, some investigations [203], [212], [219] propose KV cache quantization.Regarding data formats, the majority of algorithms adopt a uniform format for straightforward hardware implementation.Concerning the determination of quantized parameters (e.g., scale, zeropoint), most studies rely on statistics derived from weight or activation values.Nevertheless, some research efforts [194], [210] advocate for searching optimal parameters based on reconstruction loss.Furthermore, certain studies [192], [194], [205] suggest updating unquantized weights (referred to as Quantized Value Update) before or during the quantization process to enhance performance.
Non-uniform Statistic-based LLM.int8() [204] ‚úì ‚úì Uniform Statistic-based SmoothQuant [205] ‚úì ‚úì Uniform Statistic-based ‚úì RPTQ [207] ‚úì ‚úì Uniform Statistic-based OmniQuant [210] ‚úì ‚úì Uniform Search-based FlexGen [203] ‚úì ‚úì Uniform Statistic-based Atom [212] ‚úì ‚úì ‚úì Uniform Statistic-based KVQuant [219] ‚úì Non-uniform Statistic-based KIVI [220] ‚úì Uniform Statistic-based FP16 GEMM/GEMV FP16 Accumulator Activation (FP16) (a) Weight-only Quantization Weight (INT8) Activation (FP16)</p>
<p>De-quantize Bias</p>
<p>In weight-only quantization, GPTQ [192] represents an early advancement in LLM quantization, building upon the traditional algorithm OBQ [221].OBQ utilizes an optimal quantization order per row of the weight matrix, guided by the reconstruction error relative to the Hessian matrix of unquantized weights.After each quantization step, OBQ iteratively adjusts the unquantized weights to mitigate reconstruction errors.However, the frequent updating of the Hessian matrix during quantization escalates computational complexity.GPTQ streamlines this process by adopting a uniform left-to-right order for quantizing each row, thus circumventing the need for extensive Hessian matrix updates.This strategy substantially reduces computational demands by computing the Hessian matrix solely during the quantization of one row, then leveraging the computing results for subsequent rows, expediting the overall quantization procedure.LUT-GEMM [193] presents a novel dequantization method utilizing a Look-Up Table (LUT), aiming to accelerate the inference process of quantized LLMs by reducing the dequantization overhead.Additionally, it adopts a non-uniform quantization approach known as Binary-Coding Quantization (BCQ), which incorporates learnable quantization intervals.AWQ [194] observes that weight channels vary in importance for performance, particularly emphasizing those aligned with input channels exhibiting outliers in activations.To enhance the preservation of critical weight channels, AWQ utilizes a reparameterization method.This technique selects reparameterization coefficients via grid search to minimize reconstruction errors effectively.OWQ [195] observes the difficulty of quantizing weights associated with activation outliers.To address this challenge, OWQ employs a mixed-precision quantization strategy.This method identifies weak columns in the weight matrix and allocates higher precision to these specific weights, while quantizing the rest of the weights at a lower precision level.SpQR [196] introduces a methodology where weight outliers are identified and allocated higher precision during quantization, while the rest of the weights are quantized to 3 bits.SqueezeLLM [197] proposes to store the outliers in a full-precision sparse matrix, and apply non-uniform quantization to the remaining weights.The values for non-uniform quantization are determined based on quantization sensitivity, which contributes to improved performance of the quantized model.QuIP [198] introduces LDLQ, an optimal adaptive method for a quadratic proxy objective.The study reveals that ensuring incoherence between weight and Hessian matrices can enhance the effectiveness of LDLQ.QuIP utilizes LDLQ and achieves incoherence by employing random orthogonal matrix multiplication.FineQuant [199] utilizes a heuristic approach to determine the granularity of quantization per column, combining empirical insights gained from experiments to design a quantization scheme.QuantEase [200] builds upon GPTQ.When quantizing each layer, it proposes a method based on Coordinate Descent to compensate for the unquantized weights more precisely.Additionally, QuantEase can leverage quantized weights from GPTQ as an initialization and further refine the compensation process.LLM-MQ [201] protects the weight outliers with FP16 format, and stores them in Compressed Sparse Row (CSR) format for efficient computation.Besides, LLM-MQ models the bitwidth assignment to each layer as an integer programming problem, and employs an efficient solver to solve it within a few seconds.Moveover, LLM-MQ designs a efficient CUDA kernel to integrate dequantization operators, thereby reducing memory access cost during computation.Inspired by the equivalent transformations used in the previous PTQ methods, AffineQuant [215] firstly introduces equivalent affine transformations in quantization, which extends the optimization scope and further reduces the quantization errors.Recently, many studies [198], [216], [217], [218] follows the computational invariance idea, by multiplying rotation matrices to the weight matrices and activation matrices.In this way, they can effectively eliminate the outliers in the weights and activations, thus help to quantize the LLMs.These studies use different rotation matrices.For example, QuaRot [217] applies randomize Hadamard transformations to the weights and activations.SpinQuant [218] finds the optimal rotation matrices by training on a small validation dataset.</p>
<p>For weight-activation quantization, ZeroQuant [202] employs finer-grained quantization for weights and activations, leveraging kernel fusion to minimize the memory access cost during quantization and conducting layer-bylayer knowledge distillation to recover the performance.FlexGen [203] quantizes weights and KV cache directly into INT4 to reduce the memory footprint during inference with large batch sizes.LLM.int8() [204] identifies that outliers in activations are concentrated within a small subset of channels.Leveraging this insight, LLM.int8() splits activations and weights into two distinct parts based on the outlier distribution within input channels to minimize quantization errors in activations.Channels containing outlier data in both activations and weights are stored in FP16 format, while other channels are stored in INT8 format.SmoothQuant [205] employs a reparameterization technique to address the challenges of quantizing activation values.This method introduces a scaling factor that expands the data range of weight channels while shrinking the data range of corresponding activation channels.Zero-Quant [202] introduces a group-wise quantization strategy for weights and a token-wise quantization approach for activations.Building upon this methodology, ZeroQuant-V2 [206] presents the LoRC (Low-Rank Compensation) technique, employing low-rank matrices to mitigate quantization inaccuracies.RPTQ [207] identifies substantial variations in the distribution of different activation channels, which present challenges for quantization.To mitigate this issue, RPTQ reorganizes channels with similar activation distributions into clusters and independently applies quantization within each cluster.OliVe [208] observes that the normal values neighboring to the outliers are less critical.Therefore, it pairs each outlier with a normal value, sacrificing the latter to achieve a broader representation range for outliers.OS+ [169] observes that the distribution of outliers is concentrated and asymmetrical, posing a challenge to LLM quantization.To address this, OS+ introduces a channel-wise shifting and scaling technique aimed at alleviating these challenges.The shifting and scaling parameters are determined through a search process to effectively handle the concentrated and asymmetrical outlier distribution.ZeroQuant-FP [209] investigates the feasibility of quantizing weight and activation values into FP4 and FP8 formats.The study reveals that quantizing activations into floatingpoint types (FP4 and FP8) produces superior results compared to integer types.Omniquant [210] diverges from prior approaches that rely on empirical design of quantization parameters.Instead, it optimizes the boundaries for weight clipping and the scaling factor for equivalent transformation to minimize quantization errors.QLLM [211] addresses the impact of outliers on quantization by implementing channel reassembly.Additionally, it introduces learnable low-rank parameters to minimize quantization errors in the post-quantized model.Atom [212] employs a strategy involving mixed-precision and dynamic quantization for activations.Notably, it extends this approach to quantize the KV cache into INT4 to enhance throughput performance.LLM-FP4 [187] endeavors to quantize the entire model into FP4 format and introduces a pre-shifted exponent bias technique.This approach combines the scaling factor of activation values with weights to address quantization challenges posed by outliers.BiLLM [213] represents one of the lowest-bit PTQ efforts to date.BiLLM identified the bell-shaped distribution of weights and the exceptionally long-tail distribution of weights' Hessian matrix.Based on this, it proposes to categorize weights into salient and nonsalient values structurally based on the Hessian matrix and binarizes them separately.As a result, BiLLM can extensively quantize LLMs to 1.08 bits without significant degradation in perplexity.KVQuant [219] proposes a nonuniform quantization scheme for KV cache quantization, by deriving the optimal datatypes offline on a calibration set.KIVI [220] proposes a tuning-free 2bit KV cache quantization algorithm, which utilizes per-channel quantization for key cache and per-token quantization for value cache in a  [222] and LMDeploy [223] framework, respectively.We test the speed-ups of prefilling/decoding/end-to-end latency on a single NVIDIA A100 GPU.OOM denotes "Out Of Memory".To reduce the data requirements, LLM-QAT [187] introduces a data-free method to generate the training data by using the original FP16 LLMs.Specifically, LLM-QAT uses every token in the tokenization vocabulary as a starting token to generate sentences.Based on the generated training data, LLM-QAT applies a distillation-based workflow to train the quantized LLM to match the output distribution of the original FP16 LLM.Norm Tweaking [188] limits the selection of the starting token to only those language categories listed among the top languages with the highest proportion.This strategy can effectively improve the generalization of the quantized model on different tasks.</p>
<p>To reduce the computation cost, many methods apply parameter-efficient tuning (PEFT) strategies to accelerate QAT.QLoRA [189] quantizes the weights of LLMs into 4-bit and subsequently employs LoRA [224] in BF16 for each 4-bit weight matrix to fine-tune the quantized model.QLoRA allows for the efficient fine-tuning of a 65B parameter LLM on one GPU with only 30GB of memory.QA-LoRA [190] proposes to incorporate group-wise quantization into QLoRA.The authors observe that the number of quantization parameters in QLoRA is significantly smaller than the number of LoRA parameters, leading to an imbalance between quantization and low-rank adaptation.They suggest that group-wise operations can address this issue by increasing the number of parameters dedicated to quantization.In addition, QA-LoRA can merge the LoRA terms into the corresponding quantized weight matrices.LoftQ [191] identifies that initializing LoRA matrices with zeros in QLoRA is inefficient for downstream tasks.As an alternative, LoftQ suggests initializing the LoRA matrices using the Singular Value Decomposition (SVD) of the difference between the original FP16 weights and quantized weights.LoftQ iteratively applies quantization and SVD to achieve a more accurate approximation of the original weights.Norm Tweaking [188] proposes to train the LayerNorm layer after quantization and use knowledge distillation to match the output distribution of the quantized model with that of the FP16 model, achieving effects similar to LLM-QAT while avoiding high training costs.Comparative Experiments and Analysis.In this section, we conduct experiments to evaluate the speed-ups achieved by employing the weight-only quantization technique in various scenarios.Specifically, we focus on two widely-used large language models (LLMs), LLaMA-2-7B and LLaMA-2-13B, and quantize their weights to 4-bit using the AWQ [194] algorithm.Subsequently, we deploy these quantized models on a single NVIDIA A100 GPU using two different inference frameworks: TensorRT-LLM [222] and LMDeploy [223].We then evaluate the speed-ups achieved by these frameworks across different input sequences characterized by varying batch sizes and context lengths.</p>
<p>We present the speed-ups of prefilling latency, decoding latency, and end-to-end latency, as summarized in Tab. 4. From the results, several key observations can be made:</p>
<p>(1) Weight-only quantization can substantially accelerate the decoding stage, leading to improvements in end-to-end latency.This enhancement primarily stems from the capability of loading the quantized model with low-precision weight tensors much more swiftly from the High Bandwidth Memory (HBM), as illustrated in the preceding "Efficient Analysis" part.Consequently, this approach markedly diminishes the memory access overhead.(2) Regarding the prefilling stage, weight-only quantization may actually increase the latency.This is due to the fact that the bottleneck in the prefilling stage is the computational cost rather than the memory access cost.Therefore, quantizing only the weights without the activations has minimal impact on latency.Additionally, as illustrated in Fig. 9, weight-only quantization necessitates the de-quantization of low-precision weights to FP16, leading to additional computational overhead and consequently slowing down the prefilling stage.(3) As the batch size and input length increase, the extent of speed-up achieved by weight-only quantization gradually diminishes.This is primarily because, with larger batch sizes and input lengths, the computational cost constitutes a larger proportion of latency.While weight-only quantization predominantly reduces memory access cost, its impact on latency becomes less significant as the computational demands become more prominent with larger batch sizes and input lengths.(4) Weight-only quantization offers greater benefits for larger models due to the significant memory access overhead associated with larger model sizes.As models grow in complexity and size, the amount of memory required to store and access weights increases proportionally.By quantizing the model weights, weight-only quantization effectively reduces this memory footprint and memory access overhead.</p>
<p>Sparsification</p>
<p>Sparsification is a compression technique that increases the proportion of zero-valued elements in data structures such as model parameters or activations.This method aims to decrease computational complexity and memory usage by efficiently ignoring zero elements during computation.In the context of LLMs, sparsification is commonly applied to weight parameters and attention activations.It leads to the development of weight pruning strategies and sparse attention mechanisms.Weight Pruning.Weight pruning systematically removes less critical weights and structures from models, aiming to reduce computational and memory cost during both prefilling stages and decoding stages without significantly compromising performance.This sparsification approach is categorized into two main types: unstructured pruning and structured pruning.The categorization is based on the granularity of the pruning process, as illustrated in Figure 10.Unstructured pruning prunes individual weight values with fine granularity.Compared with structured pruning, it typically achieves a greater level of sparsity with minimal impact on model prediction.However, the sparse pattern achieved through unstructured pruning lacks high-level regularity, leading to irregular memory access and computation patterns.This irregularity can significantly hinder the potential for hardware acceleration, as modern computing architectures are optimized for dense, regular data patterns.Consequently, despite achieving higher sparsity levels, the practical benefits of unstructured pruning in terms of hardware efficiency and computational speedup may be limited.</p>
<p>The common focus of this line of work is the pruning criterion, including the weight importance and pruning ratio.Considering the huge parameter size of LLMs, improving the pruning efficiency is also crucial.One pruning criterion is to minimize the reconstruction loss of the model.SparseGPT [165] is a representative approach in this field.It follows the idea of OBS [225], which considers the impact of removing each weight on the network's reconstruction loss.OBS iteratively decides a pruning mask to prune the weights and reconstructs the unpruned weights to compensate for the pruning loss.SparseGPT overcomes the efficiency bottleneck of OBS via the Optimal Partial Updates technique, and designs an adaptive mask selection technique based on the OBS reconstruction error.Prune and Tune [168] improves upon SparseGPT by fine-tuning the LLMs with minimal training steps during pruning.ISC [167] designs a novel pruning criterion by combining the saliency criteria in OBS [225] and OBD [226].It further assigns non-uniform pruning ratios to each layer based on Hessian information.oBERT [171] and FastPruning [172] utilizes the secondorder information of the loss function to decide the pruned weights.BESA [170] learns a differentiable binary mask via gradient descent of the reconstruction loss.The pruning ratio for each layer is sequentially decided by minimizing the reconstruction error.The other popular pruning criterion is magnitude-based.Wanda [166] proposes to use the elementwise product between the weight magnitude and the norm of input activation as the pruning criterion.RIA [173] jointly considers the weights and activations by using the metric of Relative Importance and Activations, which evaluates the importance of each weight element based on all its connected weights.In addition, RIA converts the unstructured sparsity pattern to a structured N:M sparsity pattern, which can enjoy the actual speed-up on NVIDIA GPUs.The recent study, Pruner-Zero [185], proposes to automatically identify the optimal pruning metric for LLMs, going beyond the hand-designed matrics.As a result, the optimal metric tailored for LLaMA and LLaMA-2 is W W W ‚äôW W W ‚äô œÉ(G G G), where W W W and G G G represent the weights and gradients, and œÉ(‚Ä¢) scales a tensor to [0,1] using its mininum and maximum value.Additionally, OWL [169] focuses on deciding the pruning ratio of each layer.It assigns the pruning ratios to each layer based on its activation outlier ratios.DS√òT [186] proposes a training-free appoarch to fine-tune the pruned LLMs.It builds upon the "pruning-and-growing" workflow adopted in Dynamic Sparse Training [227], which first prunes the model and then iteratively adjusts the network topology without training or weight update.DS√òT further designs the pruning and growing metrics tailored for LLMs.</p>
<p>Structured pruning prunes larger structural units of the model, such as entire channels or layers, operating at a coarser granularity compared to unstructured pruning.These methods directly facilitate inference speed-up on conventional hardware platforms due to their alignment with the dense, regular data patterns these systems are optimized to process.However, the coarse granularity of structured pruning often results in a more pronounced impact on model performance.The pruning criterion of this line of work additionally enforces the structured pruning pattern.LLM-Pruner [174] proposes a task-agnostic structured pruning algorithm.Specifically, it first identifies the couple structures in the LLM, based on the connection dependencies between neurons.Then, it decides which structure groups to remove based on a well-designed group-wise pruning metric.After pruning, it further proposes to recover the model performance by a parameter-efficient training technique, i.e., LoRA [224].Sheared LLaMA [175] proposes to prune the original LLM to a specific target architecture of existing pre-trained LLMs.In addition, it designs dynamic batch-loading techniques to improve post-training performance.ZipLM [176] iteratively identifies and prunes the structural components with the worst trade-off between loss and runtime.LoRAPrune [177] proposes a structured pruning framework for the pre-trained LLMs with LoRA modules to enable fast inference of LoRA-based models.It designs a LoRA-guided pruning criterion that uses the weights and gradients of LoRA, and an iterative pruning scheme to remove the unimportant weights based on the criterion.LoRAShear [178] also designs a pruning method for LoRA-based LLMs with (1) a graph algorithm to identify the minimal removal structures, (2) a progressive structured pruning algorithm LHSPG, and (3) a dynamic knowledge recovery mechanism to recover the model performance.SliceGPT [179] builds on the idea of computational invariance of RMSNorm operation.It proposes to structurally arrange the sparsity in each weight matrix, and to slice out the entire rows or columns.PLATON [180] proposes to prune the weights by considering both their importance and uncertainty.It uses the exponential moving average (EMA) of the importance scores to estimate the importance, and adopts the upper confidence bound (UCB) for the uncertainty.CoFi [181] and SIMPLE [182] propose to prune the attention head, FFN neurons and hidden dimension via learning the corresponding sparsity masks.After pruning, they further adopt knowledge distillation to fine-tune the pruned models for performance recovery.MoE techniques (Sec.5.1.1)have attracted much attention in the field of efficient LLMs.Recent studies tend to explore the expert pruning methods for MoE-based LLMs.For example, Ex-pertSparsity [183] proposes to prune some less important FFN experts in each model layer.Specifically, it utilizes the Frobenius norm of the difference between the original output and the output of the pruned layer to quantify the loss of pruned experts.In constrast, SEER-MoE [184] uses the total number of times that one expert gets activated on a calibration dataset, to quantify this expert's importance.Sparse Attention.Sparse attention techniques in Multi-Head Self-Attention (MHSA) components of transformer models strategically omit certain attention calculations to enhance computational efficiency of the attention operation mainly in the prefilling stage.These mechanisms diverge into static and dynamic categories based on their reliance on specific input data.Static sparse attention removes activation values independently of specific inputs [150], [152], [153], [154].These methods pre-determine the sparse attention mask and enforce it on the attention matrix during inference.Previous studies combine different sparse patterns to preserve the most essential elements within each attention matrix.As shown in Figure 11(a), the most common sparse attention patterns are the local and global attention patterns.The local attention pattern captures the local context of each token with a fixed-size window attention surrounding each token.The global attention pattern captures the correlation of specific tokens to all other tokens by computing and attending to all tokens across the sequence.Note that leveraging global patterns can eliminate the need to store key-value (KV) pairs for unused tokens, thereby reducing memory access cost and memory usage during the decoding stage.Sparse Transformer [150] combines these patterns to capture the local context with a local pattern, and then aggregates the information with the global pattern for every few words.StreamingLLM [151] applies the local pattern, along with the global pattern only for the first few tokens.It shows that such a global pattern serves as the attention sink to keep the strong attention scores toward initial tokens.It helps the LLMs to generalize to infinite input sequence length.Bigbird [153] also uses the random pattern, where all tokens attend to a set of random tokens.The combination of local, global and random patterns is proven to encapsulate all continuous sequence-to-sequence functions, affirming its Turing completeness.As shown in Figure 11(b), Longformer [152] additionally introduces the dilated sliding window pattern.It is analogous to dilated CNNs and makes the sliding window "dilated" to increase the receptive field.To adapt the model to the sparse setting, Structured Sparse Attention [154] advocates an entropy-aware training method that congregates high-probability attention values into denser regions.Unlike previous studies that manually design sparse patterns, SemSA [155] uses gradient-based profiling to identify important attention patterns and automatically optimizes the attention density distribution to further improve model efficiency.</p>
<p>In contrast, Dynamic sparse attention adaptively eliminates activation values based on varying inputs, employing real-time monitoring of neuronal activation values to bypass computations for neurons with negligible impact, thereby achieving pruning.Most dynamic sparse attention methods employ the dynamic token-pruning methods, as Figure 11(c) shows.Spatten [156], SeqBoat [157] and Adaptively Sparse Attention [158] leverage the inherent redundancy in linguistic constructs to propose dynamic tokenlevel pruning strategies.Spatten [156] assesses the cumulative importance of each word by aggregating the attention matrix columns, subsequently pruning tokens with minimal cumulative significance from the input in subsequent layers.SeqBoat [157] trains a linear State Space Model (SSM) with a sparse sigmoid function to determine which token to prune for each attention head.Both Spatten and SeqBoat prune the uninformative tokens for the whole input.Adaptively Sparse Attention [158] gradually prunes the tokens during the generation process.It drops parts of the context that are no longer required for future generation.</p>
<p>In addition to dynamic token pruning, dynamic attention pruning strategies are also employed [159], [160], [161], [162], [163].As Figure 11(d) shows, instead of pruning all the attention values of certain tokens, these methods dynamically prune the selective part of the attention based on the input.A prominent approach within this domain is dynamically segmenting input tokens into groups, known as buckets, and strategically omitting the attention calculations for tokens that reside in separate buckets.The challenge and focus of these methods lie in the way to cluster related tokens together, thereby facilitating attention computations solely among them to enhance efficiency.Reformer [159] leverages locality-sensitive hashing to cluster keys and queries that share identical hash codes into the same bucket.Following this, Sparse Flash Attention [160] introduces specialized GPU kernels optimized for this hash-based sparse attention mechanism, further improving computational efficiency.Meanwhile, the Routing Transformer [161] employs a spherical k-means clustering algorithm to aggregate tokens into buckets, optimizing the selection process for attention computations.Sparse Sinkhorn Attention [162] adopts a learned sorting network to align keys with their relevant query buckets, ensuring that attention is computed only between the corresponding query-key pairs.Diverging from the bucket-level operation, H 2 O [163] introduces the tokenlevel dynamic attention pruning mechanism.It combines static local attention with dynamic computations between the current query and a set of dynamically identified key tokens, termed heavy-hitters (H 2 ).These heavy-hitters are dynamically adjusted with an eviction policy aimed at removing the least significant keys at each generation step, effectively managing the size and relevance of the heavyhitter set.</p>
<p>Moreover, viewing each token as a graph node and attention between tokens as edges offers an extended perspective on static sparse attention [153], [164].The original, full attention mechanism equates to a complete graph with a uniform shortest path distance of 1. Sparse attention, with its random mask, introduces random edges, effectively reducing the shortest path distance between any two nodes to O(log n), thus maintaining efficient information flow akin to full attention.Diffuser [164] utilizes the perspective of graph theory to expand the receptive field of sparse attention with multi-hop token correlations.It also takes inspiration from the expander graph properties to design better sparse patterns that approximate the information flow of full attention.</p>
<p>Beyond the attention-level and token-level sparsity, the scope of attention pruning extends to various granularities.Spatten [156] also extends pruning beyond token granularity to attention head granularity, eliminating computations for inessential attention heads to further reduce computational and memory demands.</p>
<p>Structure Optimization</p>
<p>The objective of structure optimization is to refine model architecture or structure with the goal of enhancing the balance between model efficiency and performance.Within this field of research, two prominent techniques stand out: Neural Architecture Search (NAS) and Low Rank Factorization (LRF).Neural Architecture Search.Neural Architecture Search (NAS) [228] aims to automatically search the optimal neural architectures that strike an optimized balance between efficiency and performance.AutoTinyBERT [138] utilizes one-shot Neural Architecture Search (NAS) to discover the hyper-parameters of the Transformer architecture.Notably, it introduces a compelling batch-wise training approach to train a Super Pre-trained Language Model (SuperPLM) and subsequently employs an evolutionary algorithm to identify the optimal sub-models.NAS-BERT [139] trains a large super-net on conventional self-supervised pre-training tasks using several innovative techniques, such as blockwise search, search space pruning, and performance approximation.This approach allows NAS-BERT to be applied efficiently across various downstream tasks without requiring extensive re-training.Structure pruning via NAS [140] treats structural pruning as a multi-objective NAS problem, and solves it via the one-shot NAS method.LiteTransform-erSearch [141] proposes to use a training-free indicator, i.e., the number of parameters, as a proxy indicator to guide the search.This method enables efficient exploration and selection of the optimal architectures without the need for actual training during the search phase.AutoDistil [142] presents a fully task-agnostic few-shot NAS algorithm featuring three primary techniques: search space partitioning, task-agnostic SuperLM training, and task-agnostic search.This approach aims to facilitate efficient architecture discovery across various tasks with minimal task-specific adaptations.Typically, NAS algorithms necessitate evaluating the performance of each sampled architecture, which can incur significant training cost.Consequently, these techniques are challenging to apply to LLMs.Low Rank Factorization.Low Rank Factorization (LRF), or Low Rank Decomposition, aims to approximate a matrix A m√ón with two low-rank matrices B m√ór and C r√ón by:
A m√ón ‚âà B m√ór √ó C r√ón , (11)
where r is much smaller than m and n.In this way, LRF can diminish memory usage and enhance computational efficiency.Furthermore, during the decoding stage of LLM inference, memory access cost presents a bottleneck to the decoding speed.Therefore, LRF can reduce the number of parameters that need to be loaded, thereby accelerating the decoding speed.LoRD [143] shows the potential of compressing the LLMs without largely degrading the performance via LRF.Specifically, it adopts Singular Value Decomposition (SVD) to factorize the weight matrices, and successfully compresses a LLM with 16B parameters to 12.3B with minimal performance drop.TensorGPT [144] introduces a method to compress the embedding layer using Tensor-Train Decomposition.Each token embedding is treated as a Matrix Product State (MPS) and efficiently computed in a distributed manner.LoSparse [145] combines the benefits of LRF and weight pruning for LLM compression.By leveraging low-rank approximation, LoSparse mitigates the risk of losing too many expressive neurons that typically occurs with direct model pruning.LPLR [146] and ZeroQuant-V2 [147] both propose to compress the weight matrix by simultaneously applying LRF and quantization to it.DSFormer [148] proposes to factorize the weight matrix into the product of a semi-structured sparse matrix and a small dense matrix.ASVD [149] designs an activationaware SVD method.This approach involves scaling the weight matrix based on activation distribution prior to applying SVD for matrix decomposition.ASVD also involves determining an appropriate truncation rank for each layer through a search process.SVD-LLM [229] analyses the relationship between the singular values of the transformed weight matrices and the compression loss.Then, it designs a truncation-aware data whitening technique to identify the singular value that causes the minimal loss after removing it.Additionally, SVD-LLM develops a layer-wise closedform update strategy to recover the task performance after the factorization.</p>
<p>Knowledge Distillation</p>
<p>Knowledge Distillation (KD) is a well-established technique for model compression, wherein knowledge from large models (referred to as teacher models) is transferred to smaller models (referred to as student models).In the context of LLMs, KD involves using the original LLMs as teacher models to distill smaller LMs.Numerous studies have focused on effectively transferring various abilities of LLMs to smaller models.In this domain, methods can be categorized into two main types: white-box KD and blackbox KD (as illustrated in Fig. 12).White-box KD.White-box KD refers to distillation methods that leverage access to the structure and parameters of the teacher models.This approach enables KD to effectively utilize the intermediate features and output logits of the teacher models for enhanced performance of the student models.MiniLLM [131] proposes to adopt the standard white-box KD approach but replace the forward Kullback-Leibler divergence (KLD) with the reverse KLD.GKD [132] introduces the use of on-policy data, which includes output sequences generated by the student model itself, to further distill the student model.This method focuses on aligning the output logits between the teacher and student models using these on-policy data.TED [133] presents a task-aware layer-wise KD method.This approach involves adding filters after each layer in both the teacher and student models, training these task-specific filters, and subsequently freezing the teacher model's filters while training the student filters to align their output features with the corresponding teacher filters.MiniMoE [135] mitigates the capacity gap by utilizing a Mixture-of-Experts (MoE) model as the student model.DynaBERT [136] proposes to progressively decrease the models' width and depth, and uses knowledge distillation to train the smaller models.For newly emerging entities, pre-trained language models (LLMs) may lack upto-date information.To address this, one solution involves incorporating additional retrieved texts into prompts, albeit at an increased inference cost.Alternatively, KPTD [137] suggests transferring knowledge from entity definitions into LLM parameters via knowledge distillation.This method generates a transfer set based on entity definitions and distills the student model to match output distributions with the teacher model based on these definitions.Black-box KD.Black-box KD refers to the knowledge distillation methods in which the structure and parameters of teacher models are not available.Typically, black-box KD only uses the final results obtained by the teacher models to distill the student models.In the field of LLMs, blackbox KD mainly guides the student models to learn LLMs' generalization ability and emergent ability, including In-Context Learning (ICL) ability [45], Chain-of-Thought (CoT) reasoning ability [14] and Instruction Following (IF) ability [230].</p>
<p>Teacher Model Student Model</p>
<p>Regarding the ICL ability, Multitask-ICT [118] introduces in-context learning distillation to transfer the multitask few-shot ability of Large Language Models (LLMs), leveraging both in-context learning and language modeling proficiency.MCKD [119] observes that student models distilled from incontext learned teacher models often exhibit superior performance on unseen input prompts.Building on this observation, MCKD devises a multi-stage distillation paradigm where the student model from previous stages is employed to generate distillation data for subsequent stages, enhancing the effectiveness of the distillation method.</p>
<p>To distill the Chain-of-Thought (CoT) reasoning ability, several techniques such as Distilling Step-by-Step [120], SCoTD [121], CoT Prompting [122], MCC-KD [123], and Fine-tune-CoT [124] propose distillation methods that incorporate responses and rationales extracted from LLMs to train student models.Socratic CoT [125] also targets reasoning ability transfer to smaller models.Specifically, it fine-tunes a pair of student models, namely a Question Generation (QG) model and a Question Answering (QA) model.The QG model is trained to generate intermediate questions based on input questions, guiding the QA model in producing the final response.PaD [126] observes that faulty reasoning (i.e., correct final answer but incorrect reasoning steps) can be detrimental to student models.To address this, PaD proposes generating synthetic programs for reasoning problems, which can then be automatically checked by an additional interpreter.This approach helps in removing distillation data with faulty reasoning, enhancing the quality of the training data for student models.</p>
<p>For the IF ability, several methods have been proposed to transfer this capability to smaller models.DISCO [128] introduces a technique where phrasal perturbations are generated using a LLM.These perturbations are then filtered by a task-specific teacher model to distill high-quality counterfactual data.LaMini-LM [129] aims to transfer instruction following ability by designing a diverse instruction set for distilling student models.Lion [130] utilizes the teacher model to identify difficult instructions, and generates new and complex instructions to distill the small model.</p>
<p>Dynamic Inference</p>
<p>Dynamic inference involves the adaptive selection of model sub-structures during the inference process, conditioned on input data.This section focuses on early exiting techniques, which enable a LLM to halt its inference at different model layers depending on specific samples or tokens.Notably, while MoE techniques (discussed in Sec.5.1.1)also adjust model structure during inference, they typically involve expensive pre-training cost.In contrast, early exiting techniques only require training a small module to determine when to conclude the inference.Some previous surveys [231], [232] have reviewed dynamic inference techniques for traditional language models (e.g., RNN, LSTM).In this paper, we categorize studies on early exiting techniques for LLMs into two main types: sample-level early exiting and token-level early exiting (illustrated in Fig. 13).Sample-level.Sample-level early exiting techniques focus on determining the optimal size and structure of Language Models (LLMs) for individual input samples.A common approach is to augment LLMs with additional modules after each layer, leveraging these modules to decide whether to</p>
<p>Sample-level Dynamic Inference</p>
<p>Token-level Dynamic Inference Fig. 13.Illustration of Token-level (up) and Sample-level (down) dynamic inference.</p>
<p>terminate inference at a specific layer.FastBERT [112], Dee-BERT [115], MP [233], and MPEE [113] train these modules directly to make decisions (e.g., outputting 0 to continue or 1 to stop) based on features from the current layer.Global Past-Future Early Exit [114] proposes a method that enriches the input to these modules with linguistic information from both preceding and subsequent layers.Given that future layer features are not directly accessible during inference, a simple feed-forward layer is trained to estimate these future features.PABEE [116] trains the modules as output heads for direct prediction, suggesting inference termination when predictions remain consistent.HASHEE [117] employs a non-parametric decision-making approach based on the hypothesis that similar samples should exit inference at the same layer.Token-level.In the decoding stage of LLM inference, where tokens are generated sequentially, token-level early exiting techniques aim to optimize the size and structure of LLMs for each output token.CALM [110] introduces early exit classifiers after each Transformer layer, training them to output confidence scores that determine whether to halt inference at a specific layer.Notably, in the self-attention block, computing the current token's feature at each layer relies on all previous tokens' features (i.e., KV cache) in the same layer.To address the issue of missing KV cache due to early exiting of previous tokens, CALM proposes directly copying the feature from the exiting layer to subsequent layers, with experimental results showing only minor performance degradation.SkipDecode [111] addresses limitations of previous early exiting methods that hinder their applicability to batch inference and KV caching, thereby limiting actual speed-up gains.For batch inference, SkipDecode proposes a unified exit point for all tokens within a batch.Regarding KV caching, SkipDecode ensures a monotonic decrease in exit points to prevent recomputation of KV cache, facilitating efficiency gains during inference.</p>
<p>Knowledge, Suggestions and Future Direction</p>
<p>In the field of efficient structure design, the pursuit of alternative architectures to Transformers is a burgeoning area of research.Examples such as Mamba [75], RWKV [62], and their respective variants [103], [106] have demonstrated competitive performance across various tasks, garnering increasing attention in recent times.Nevertheless, it remains pertinent to investigate whether these non-Transformer models may exhibit certain shortcomings compared to Transformer models.Concurrently, exploring the integration of non-Transformer architectures with the attention operation [76], [105], [234] represents another promising avenue for future research.</p>
<p>In the realm of model compression, quantization stands out as the predominant method employed in Large Language Model (LLM) deployment, primarily due to two key factors.Firstly, quantization presents a convenient means of compressing LLMs.For instance, employing Post-Training Quantization (PTQ) methods can reduce the parameter count of an LLM with seven billion parameters to a compressed form within a matter of minutes.Secondly, quantization holds the potential to achieve substantial reductions in memory consumption and inference speed, while introducing only minor performance trade-offs.This compromise is generally deemed acceptable for numerous realworld applications.However, it's worth noting that quantization may still compromise certain emergent abilities of LLMs, such as self-calibration or multi-step reasoning.Additionally, in specific scenarios like dealing with long contexts, quantization could lead to significant performance degradation [214].Consequently, it is required to carefully select appropriate quantization methods to mitigate the risk of such degradation in these specialized cases.</p>
<p>Extensive literature has devoted into studying sparse attention techniques for efficient long-context processing.For example, a recent representative work, StreamingLLM [151], can process 4 million tokens by only restoring several attention sink tokens.Nonetheless, these approaches often sacrifice critical information, resulting in performance degradation.Therefore, the challenge of preserving essential information while efficiently managing long contexts remains an important area for future exploration.As for the weight pruning techniques, LLM-KICK [235] notes that current state-of-the-art (SOTA) methods experience considerable performance degradation even at relatively low sparsity ratios.Consequently, developing effective weight pruning methods to maintain LLM performance remains an emerging and critical research direction.</p>
<p>The optimization of model structures often involves the use of Neural Architecture Search (NAS), which typically demands extensive computational resources, posing a potential barrier to its practical application in compressing LLMs.Therefore, investigating the feasibility of employing automatic structure optimization for LLM compression warrants further exploration.Additionally, the challenge remains for techniques like low-rank factorization (LRF) to achieve an optimal balance between compression ratio and task performance.For instance, ASVD [149] achieves only a modest 10% to 20% compression ratio without compromising the reasoning capabilities of LLMs.</p>
<p>In addition to employing individual model compression techniques, several studies explore the combination of different methods to compress LLMs, leveraging their respective advantages for improved efficiency.For instance, MPOE [90] applies weight matrix factorization specifically to the expert Feed-Forward Networks (FFNs) in MoE-based LLMs, with the goal of further reducing memory require-ments.LLM-MQ [201] utilizes weight sparsity techniques to protect weight outliers during model quantization, thereby minimizing quantization errors.LPLR [146] focuses on quantizing low-rank factorized weight matrices to further decrease memory footprint and memory access cost during LLM inference.Furthermore, LoSparse [145] combines lowrank factorization with weight pruning, leveraging pruning to enhance the diversity of low-rank approximation while using low-rank factorization to retain important weights and prevent loss of critical information.These approaches highlight the potential of integrating multiple compression techniques to achieve better optimization of LLMs.</p>
<p>SYSTEM-LEVEL OPTIMIZATION</p>
<p>The system-level optimization for LLM inference primarily involves enhancing the model forward pass.Considering the computational graph of an LLM, there exist multiple operators, with attention and linear operators dominating most of the runtime.As mentioned in Sec.2.3, system-level optimization primarily considers the distinctive characteristics of the attention operator and the decoding approach within LLM.In particular, to address the specific issues related to the decoding approach of LLMs, the linear operator requires special tiling designs, and speculative decoding methods are proposed to improve the utilization.The substantial memory demand of LLMs leads to the offloading of parameters or KV cache to the CPU.Furthermore, in the context of online serving, requests come from multiple users.Therefore, beyond the optimizations discussed earlier, online serving faces challenges related to memory, batching, and scheduling arising from asynchronous requests.</p>
<p>Inference Engine</p>
<p>The optimizations for inference engines are dedicated to accelerating the model forward process.The main operators and the computational graph in LLM inference are highly optimized.Besides, the speculative decoding technique is proposed to accelerate the inference speed without performance degradation, and the offloading technique is introduced to mitigate the memory pressure.</p>
<p>Graph and Operator Optimization</p>
<p>Runtime Profiling.Using HuggingFace [260] implementation, we profile the inference runtime with different models and context lengths.The profiling results in Fig. 15 demonstrate that attention operators and linear operators collectively dominate runtime, with their combined duration often exceeding 75% of the inference duration.Consequently, a significant portion of optimization efforts at the operator level is dedicated to enhancing the performance of the two operators.Furthermore, there are multiple operators occupying a small proportion of runtime, which fragments the operator execution timeline and increases the cost of kernel launch on the CPU side.To address this issue, at the computational graph level, current optimized inference engines implement highly fused operators.Attention Operator Optimization.The standard attention computation (e.g., using Pytorch) involves the multiplication of the Query matrix (Q) with the Key matrix (K), Inference Engine</p>
<p>Speculative Decoding</p>
<p>Speculative decoding [236], Speculative sampling [237], DistillSpec [238], Selfspeculative decoding [239], OSD [240], PaSS [241], REST [242], SpecInfer [243], Stage speculative decoding [244], Cascade Speculative Drafting [245], Lookahead decoding [246], Medusa [50], Eagle [247], Spectr [248], Kangaroo [249] Offloading FlexGen [203], llama.cpp[250], powerinfer [251], FastDecode [252] Graph and Operator Optimization</p>
<p>Linear Operator Optimization</p>
<p>TensorRT-LLM [222], FlashDecoding++ [253], MegaBlocks [254], vLLM [51] Graph-Level Optimization FlashAttention [255], [256], ByteTransformer [257], DeepSpeed [258], FlashDecod-ing++ [253] Attention Operator Optimization FlashAttention [255], [256], FlashDecoding [259], FlashDecoding++ [253] Fig.  resulting in quadratic time and space complexity in relation to the input sequence length.As shown in Fig. 15, the time proportion of the attention operator increases as the context length grows.This translates to high demands on memory size and computational capability, especially when dealing with long sequences.To address the computational and memory overhead of standard attention computation on GPUs, customized attention operators are essential.FlashAttention [255], [256] fuses the entire attention operation into a single, memory-efficient operator to alleviate memory access overhead.The input matrices (Q, K, V) and attention matrix are tiled into multiple blocks, which eliminates the need for complete data loading.Built upon Flash Attention, FlashDecoding [259] aims to maximize computational parallelism for decoding.Due to the application of the decoding approach, the Q matrix degrades into a batch of vectors during decoding, which makes it challenging to fill the computational units if the parallelism is limited to the batch size dimension.FlashDecoding addresses this by introducing parallel computation along the sequence dimension.While this introduces some synchronization overhead to softmax computation, it leads to noticeable improvements in parallelism, particularly for small batch sizes and long sequences.The subsequent work, FlashDe-coding++ [253], observes that in previous works [255], [256], [259], the maximum value within the softmax only serves as a scaling factor to prevent data overflow.However, the dynamical maximum value incurs significant synchronization overhead.Moreover, extensive experiments indicate that in typical LLM (e.g., Llama2 [261], ChatGLM [262]), over 99.99% of the softmax inputs fall within a certain range.Thus, FlashDecoding++ proposes to determine the scaling factor based on statistics in advance.This eliminates the synchronization overhead in softmax computation, enabling parallel execution of subsequent operations alongside the softmax computation.
(b) Llama2-7B, 2k context length (c) Baichuan2-13B, 128 context length (d) Baichuan2-13B,2k context length (e) Mixtral-8x7B, 128 context length (f) Mixtral-8x7B, 2k context length</p>
<p>Linear Operator Optimization</p>
<p>The linear operator plays a pivotal role in LLM inference, performing in feature projection and Feedforward Neural Networks (FFNs).In traditional neural networks, linear operators can be abstracted into General Matrix-Matrix Multiplication (GEMM) operations.However, in the case of LLM, the application of the decoding approach results in a notably reduced dimension, diverging from the conventional GEMM workload.The low-level implementation of traditional GEMM has been highly optimized, and mainstream LLM frameworks (e.g., DeepSpeed [258], vLLM [51], OpenPPL [263] and etc.) primarily call the GEMM APIs offered by cuBLAS [264] for linear operators.Without an explicitly tailored implementation for GEMMs with a reduced dimension, the linear operators during decoding suffer inefficiency.A notable trend to address the issue is observed in the latest release of TensorRT-LLM [222].It introduces a dedicated General Matrix-Vector Multiplication (GEMV) implementation, potentially improving efficiency for the decoding step.Recent research FlashDecoding++ [253] makes a further step, addressing the inefficiency of cuBLAS [264] and CUTLASS [265] libraries when dealing with small batch sizes during the decode step.The authors first introduce the concept of the FlatGEMM operation to represent the workload of GEMM with a highly reduced dimension (dimension size &lt; 8 in FlashDecoding++).As FlatGEMM poses new computational characteristics, the tiling strategy for traditional GEMMs necessitates modification to be applied.The authors observe that two challenges exist as the workload varies: low parallelism and memory access bottleneck.</p>
<p>To tackle the challenges, FlashDecoding++ adopts a finegrained tiling strategy to improve parallelism, and leverages the double buffering technique to hide memory access latency.Furthermore, recognizing that the linear operations in typical LLM (e.g., Llama2 [261], ChatGLM [262]) often have fixed shapes, FlashDecoding++ establishes a heuristic selection mechanism.This mechanism dynamically chooses between different linear operators based on the input size.The options include FastGEMV [266], FlatGEMM, and GEMM provided by cuBLAS [264], [265] libraries.This approach ensures the selection of the most efficient operator for the given linear workload, potentially leading to better end-toend performance.</p>
<p>Recently, the application of the MoE FFN to enhance the model capability has become a trend in LLMs [12].This model structure also puts forward new requirements for operator optimization.As shown in Fig. 15, in the Mixtral model with MoE FFN, the linear operator dominates the runtime due to the non-optimized FFN computation in the HuggingFace implementation.Besides, Mixtral's adoption of the GQA attention structure decreases attention operator's runtime proportion, which further points out the urgent need to optimize the FFN layer.MegaBlocks [254] is the first to optimize the computation for MoE FFN layers.The work formulates the MoE FFN computation into block-sparse operations and proposes tailored GPU kernels for acceleration.However, MegaBlocks concentrates on the efficient training of the MoE models and hence ignores the characteristics of inference (e.g., the decoding approach).Existing frameworks are working hard to optimize the computations of the MoE FFN inference stage.The official repository of vLLM [51] integrates the fused kernels for MoE FFN in Triton [267], seamlessly removing the index overhead.Graph-Level Optimization.Kernel fusion stands out as a prevalent graph-level optimization because of its capability to reduce runtime.There are three main advantages of applying kernel fusion: (1) To reduce memory access.The fused kernel inherently removes the memory access of intermediate results, mitigating the memory bottleneck for operators.(2) To mitigate kernel launching overhead.For some lightweight operators (e.g., residual adding), the kernel launching time occupies most of the latency, and kernel fusion reduces individual kernel launchings.(3) To enhance parallelism.For those operators without data dependency, when one-by-one kernel execution fails to fill the hardware capacity, it is beneficial to parallel the kernels via fusion.</p>
<p>The technique of kernel fusion proves effective with LLM inference, with all of the aforementioned benefits.FlashAttention [255] formulates the attention operator into one single kernel, removing the overhead of accessing the attention results.Based on the fact that the attention operator is memory-bounded, the reduction of memory access effectively transfers to runtime speed-up.ByteTransformer [257] and DeepSpeed [258] propose to fuse lightweight operators including residual adding, layernorm, and activation functions, into the former linear operators to reduce the kernel launching overhead.As a result, those lightweight operators disappear in the timeline with nearly no extra latency.Moreover, kernel fusion is also adopted to enhance the utilization of LLM inference.The projections of Query, Key, and Value matrices are originally three individual linear operations, and are fused into one linear operator to deploy on modern GPUs.Currently, the kernel fusion technique has been exploited in LLM inference practice, and highly optimized inference engines employ only a few fused kernels within the runtime.For example, in FlashDecoding++ [253] implementation, a transformer block integrates merely seven fused kernels.Leveraging the aforementioned operators and kernel fusion optimization, FlashDecoding++ achieves up to 4.86√ó speed-up over the HuggingFace implementation.</p>
<p>Speculative Decoding</p>
<p>Speculative decoding [268], [269] is an innovative decoding technique for auto-regressive LLMs designed to enhance decoding efficiency without compromising the fidelity of outputs.The core idea of this approach involves employing a smaller model, termed a draft model, to predict several subsequent tokens efficiently, followed by validation of these predictions using the target LLM in parallel.This methodology aims to enable the LLM to generate multiple tokens within the time frame typically required for a single inference.Fig. 16 demonstrates the comparison of the traditional auto-regressive decoding method and the speculative decoding approach.Formally, speculative decoding approach consists of two steps:</p>
<p>1) Draft Construction: It employs the draft model to generate several subsequent tokens, namely draft tokens, in parallel or in the auto-regressive manner.2) Draft Verification: It employs the target model to compute the conditional probabilities of all the draft tokens in a single LLM inference step, subsequently determining the acceptance of each draft token sequentially.The acceptance rate, representing the average number of accepted draft tokens per inference step, serves as a key metric for evaluating the performance of a speculative decoding algorithm.</p>
<p>LLM Draft Model</p>
<p>Optional Speculative decoding ensures output equivalence with standard auto-regressive decoding methods.Traditional decoding techniques typically employ two primary sampling TABLE 5 Comparison of several open-source implementations of speculative decoding.In this table, we also show the additional overhead of constructing draft models.Note that for SpD [236], [237], LADE [246], Medusa [50] and Eagle [247], we report the training cost from their original papers.And for SSD [239] and REST [29], we run the sub-LLM search and datastore construction with the code they provide, and report the time cost.Besides, for Medusa, we use Medusa-1 [50] which does not fine-tune the original LLM backbone.</p>
<p>LLM</p>
<p>Method Draft Model Draft Construction</p>
<p>Draft Verifier Additional Overhead (GPU hours) Acceptance Rate Speed-up SpD [236], [237] small speculative model one draft sequence speculative sampling 275 1.77‚àº2.02√ó1.05‚àº1.77√óLADE [246] LLM + N grams one draft sequence greedy sampling 0 1.92‚àº2.14√ó1.12‚àº1.30√óSSD [239] sub-LLM one draft sequence speculative sampling 4 1.64‚àº1.74√ó1.01‚àº1.23√óREST [29] datastore token tree speculative sampling 1.5 2.18‚àº2.31√ó1.72‚àº2.27√óMedusa-1 [50] four LLM heads token tree speculative sampling ‚àº24 2.52‚àº2.62√ó2.04‚àº2.86√óEagle [247] one Transformer Layer token tree speculative sampling 96‚àº192 3.47‚àº3.72√ó2.77‚àº3.74√ó</p>
<p>strategies: greedy sampling and nucleus sampling.Greedy sampling involves selecting the token with the highest probability at each decoding step to generate a specific output sequence.The initial attempt at speculative decoding, known as Blockwise Parallel Decoding [270], aims to ensure that the draft tokens precisely match the tokens sampled via greedy sampling, thus preserving output token equivalence.In contrast, nucleus sampling involves sampling tokens from a probability distribution, resulting in diverse token sequences with each run.This diversity makes nucleus sampling popular.To accommodate nucleus sampling within speculative decoding frameworks, speculative sampling techniques [236], [237] have been proposed.Speculative sampling maintains output distribution equivalence, aligning with the probabilistic nature of nucleus sampling to generate varied token sequences.Formally, given a sequence of tokens x 1 , x 2 , ..., x n and a sequence of draft tokens xn+1 , xn+2 , ..., xn+k , the speculative sampling strategy accepts the i-th draft token with the following probabilities:
min 1, p(x i |x 1 , x 2 , ..., x i‚àí1 ) q(x i |x 1 , x 2 , ..., x i‚àí1 ) ,(12)
where p(‚Ä¢|‚Ä¢) and q(‚Ä¢|‚Ä¢) denote the conditional probabilities from the target LLM and the draft model, respectively.If the i-th draft token is accepted, it sets x i ‚Üê ‚àí xi .Otherwise, it quits the verification of the following draft tokens, and resamples x i from the following distribution:
norm(max(0, p(‚Ä¢|x 1 , x 2 , ..., x i‚àí1 ) ‚àí q(‚Ä¢|x 1 , x 2 , ..., x i‚àí1 ))).(13
) Building upon speculative sampling, several variants [243], [248] have emerged, aimed at validating multiple draft token sequences.Notably, the token tree verifier [243] has become a widely adopted verification strategy within this context.This approach utilizes a tree-structured representation of draft token sets and employs a tree attention mechanism to efficiently perform the verification process.</p>
<p>In the speculative decoding approach, the acceptance rate of draft tokens is significantly influenced by the degree to which the output distributions of draft models align with those of original LLMs.As a result, considerable research efforts have been directed towards improving the design of draft models.DistillSpec [238] directly distills a smaller draft model from the target LLM.SSD [239] involves automatically identifying a sub-model (a subset of model layers) from the target LLM to serve as the draft model, eliminating the need for separate training of the draft model.OSD [240] dynamically adjusts the output distribution of the draft model to match the user query distribution in online LLM services.It achieves this by monitoring rejected draft tokens from the LLM and using this data to refine the draft model through distillation.PaSS [241] proposes utilizing the target LLM itself as the draft model, incorporating trainable tokens (look-ahead tokens) into the input sequence to enable simultaneous generation of subsequent tokens.REST [242] introduces a retrieval-based speculative decoding approach, employing a non-parametric retrieval data store as the draft model.SpecInfer [243] introduces a collective boost-tuning technique to align the output distribution of a group of draft models with that of the target LLM.Lookahead decoding [246] involves generating n-grams of the target LLM in parallel to aid in generating draft tokens.Medusa [50] finetunes several heads of the LLM specifically for generating subsequent draft tokens.Eagle [247] adopts a lightweight transformer layer called an auto-regression head to generate draft tokens in an auto-regressive manner, integrating rich contextual features from the target LLM into the draft model's input.Kangaroo [249] uses a fixed shallow subnetwork as the draft model, and trains a lightweight adapter on the top of the sub-network.In this way, it does not need to train a separate draft model.</p>
<p>Another line of studies focuses on designing more effective draft construction strategies.Conventional approaches often yield single draft token sequences, posing challenges for passing verification.In response, Spectr [248] advocates generating multiple draft token sequences and employs a k-sequential draft selection technique to concurrently verify k sequences.This method leverages speculative sampling, ensuring equivalence in output distributions.Similarly, SpecInfer [243] adopts a comparable approach.However, unlike Spectr, SpecInfer merges draft token sequences into a "token tree" and introduces a tree attention mechanism for validation.This strategy is called the "token tree verifier".Due to its efficacy, token tree verifier has been widely embraced in numerous speculative decoding algorithms [50], [242], [244], [247].In addition to these efforts, Stage Speculative Decoding [244] and Cascade Speculative Drafting (CS Drafting) [245] propose accelerating draft construction by integrating speculative decoding directly into the token generation process.Comparative Experiments and Analysis.We conduct an experiment to evaluate the speed-up performance of the speculative decoding methods.Specifically, we thoroughly review the studies of this field, and select six of them that have open-sourced their codes, i.e., Speculative Decoding (SpD) [236], [237], Lookahead Decoding (LADE) [246], REST [242], Self-speculative Decoding (SSD) [239], Medusa [50] and Eagle [247].As for the evaluation dataset, we use Vicuna-80 [7] to evaluate the above methods, which contains 80 questions that classified into 10 categories.We report the average results on these 80 questions.As for target LLMs, we adopt five fashion opensource LLMs, i.e., Vicuna-7B-V1.3[7], Vicuna-13B-V1.3 [7], Vicuna-33B-V1.3 [7], LLaMA-2-7B [5] and LLaMA-2-13B [5].We report the range of evaluation metrics across these 5 LLMs.As for draft models, we adopt two well-trained draft models, i.e., LLaMA-68M and LLaMA-160M [243] for SpD.For other speculative decoding methods, we follow their proposed draft construction approach and use the checkpoints they provide.As for the evaluation metrics, we adopt acceptance rate, which denotes the ratio of the number of accepted tokens to the number of generation steps, and speed-up, which denotes the ratio of the latency of original auto-regressive decoding to the latency of speculative decoding when fixing the total length of output.</p>
<p>Tab. 5 provides a comparison of various speculative decoding methods, highlighting several key observations:</p>
<p>(1) Eagle demonstrates exceptional performance, achieving a notable 3.47‚àº3.72√óend-to-end speed-up across multiple LLMs.To understand its success, a deeper analysis of Eagle reveals two key factors.Firstly, Eagle employs an autoregressive approach for decoding draft tokens, leveraging information from previously generated tokens directly.Secondly, Eagle integrates rich features from previous tokens of both original LLMs and draft models to enhance the accuracy of the next draft token generation.(2) The token tree verifier proves to be an effective technique in enhancing the performance of speculative decoding methods.(3) The endto-end speed-up achieved by these methods is often lower than the acceptance rate.This difference arises due to the practical consideration that the generation cost associated with draft models cannot be overlooked.</p>
<p>Offloading</p>
<p>Current research investigates the potential of offloading to accommodate the substantial memory demand of LLMs (see Sec. 2.3) in resource-constrained environments.The essence of offloading is to offload part of the storage from the GPU to the CPU when it is free of use.Intuitively, the focus of such kind of research lies in hiding the expensive data movement latency between the GPU and the CPU.FlexGen [203] enables the offloading of weights, activations, and the KV cache, and further formulates a graph traversal problem for offloading to maximize the throughput.The data loading of the next batch and the data storing of the previous batch can be overlapped with the computation of the current batch.Another work llama.cpp[250] also assigns computational tasks to the CPU, mitigating the data transfer overhead at the cost of computing with the low-powered CPU.Powerinfer [251] exploits the sparsity in activations using ReLU [271] in LLMs, and divides the activations into subsets of cold and hot neurons representing the frequency of computation.The cold neurons are offloaded to the CPU for both storage and computation in Powerinfer.Leveraging adaptive predictors and sparse operators, Powerinfer significantly improves the computational efficiency with offloading.FastDecode [252] proposes to offload the storage and the computation of the entire attention operator to the CPU.Since the attention operation is computed on the CPU, the data movement of KV cache is reduced to merely some activations.The number of CPUs is selected to match the workload latency on GPUs so that the bubbles in the heterogeneous pipeline are mitigated.</p>
<p>Serving System</p>
<p>The optimizations for serving systemworks are dedicated to improving the efficiency in handling asynchronous requests.The memory management is optimized to hold more requests, and efficient batching and scheduling strategies are integrated to enhance the system throughput.Besides, optimizations specific to distributed systems are proposed to exploit distributed computational resources.</p>
<p>Memory Management</p>
<p>The storage of KV cache dominates the memory usage in LLM serving, especially when the context length is long (see Sec. 2.3).Since the generation length is uncertain, it is challenging to allocate the space for KV cache storage in advance.Earlier implementations [286] usually allocate storage space in advance based on the preset maximum length of each request.However, in instances where request generation is terminated early, this approach incurs significant wastage of storage resources.To address the issue, S 3 [284] proposes to predict an upper bound of the generation length for each request, in order to reduce the waste of the pre-allocated space.However, the static way of KV cache memory allocation still fails when no such large contiguous space exists.To deal with the fragmented storage, vLLM [51] proposes to store the KV cache in a paged manner following the operating system.vLLM first allocates a memory space as large as possible and divides it equally into multiple physical blocks.When a request comes, vLLM dynamically maps the generated KV cache to the pre-allocated physical blocks in a discontinuous fashion.In this way, vLLM significantly reduces storage fragmentation and achieves a higher throughput in LLM serving.On the basis of vLLM, LightLLM [278] uses a more fine-grained KV cache storage to cut down the waste happening with the irregular boundary.Instead of a block, LightLLM treats the KV cache of a token as a unit, so that the generated KV cache always saturates the pre-allocated space.</p>
<p>Current optimized service systems commonly employ this paged approach to manage the KV cache storage, thereby mitigating the waste of redundant KV cache memory.However, the paged storage leads to irregular memory access in the attention operator.For the attention operator using the paged KV cache, this necessitates the consideration of the mapping relationship between the virtual address space of the KV cache and its corresponding physical address space.To enhance the efficiency of the attention operator, the loading pattern of the KV cache must be tailored to facilitate contiguous memory access.For instance, in the case of the PagedAttention by vLLM [51], the storage Serving System Distributed Systems Splitwise [272], TetriInfer [273], Dist-Serve [274], SpotServe [275], Infinite-LLM [276] Scheduling ORCA [277], vLLM [51], LightLLM [278], DeepSpeed-FastGen [279], FastServe [280], VTC [281] Batching ORCA [277], vLLM [51], Sarathi [282], DeepSpeed-FastGen [279], Sarathi-Serve [283], LightLLM [278] Memory Management S 3 [284], vLLM [51], LightLLM [278], FlashInfer [285] Fig. 17.Taxonomy of the optimization for LLM serving system.</p>
<p>of the head size dimension is structured as a 16-byte contiguous vector for K cache, while FlashInfer [285] orchestrates diverse data layouts for the KV cache, accompanied by an appropriately designed memory access scheme.The optimization of the attention operator in conjunction with paged KV cache storage remains a forefront challenge in the advancement of serving systems.</p>
<p>Continuous Batching</p>
<p>The request lengths in a batch can be different, leading to low utilization when shorter requests are finished and longer requests are still running.Due to the asynchronous nature of requests in serving scenarios, there exists an opportunity that such periods of low utilization could be mitigated.The continuous batching technique is proposed to leverage the opportunity by batching new requests once some old requests are finished.ORCA [277] is the first to utilize the continuous batching technique in LLM serving.The computation of each request encompasses multiple iterations, with each iteration representing either a prefilling step or a decoding step.The author suggests that different requests can be batched at the iteration level.The work implements iteration-level batching in linear operators, concatenating different requests together in the sequence dimension.Hence, the spare storage and computational resources corresponding to the completed requests are promptly released.Following ORCA, vLLM [51] extends the technique to the attention computation, enabling requests with different KV cache lengths to be batched together.Sarathi [282], DeepSpeed-FastGen [279] and Sarathi-Serve [283] further introduce a split-and-fuse method to batch together prefilling requests and decoding requests.Specifically, this method first splits the long prefilling request in the sequence dimension, and then batches it together with multiple short decoding requests.The split-andfuse method balances the workloads among different iterations, and significantly reduces the tail latency via removing the stalls from new requests.LightLLM [278] also adopts the split-and-fuse method.</p>
<p>The split-and-fuse technology operates on the premise that requests during the prefilling stage can be partitioned into discrete chunks.Chunked-prefill methodology involves segmenting prefilling requests along the sequence dimension, thereby preventing the potential bottlenecks for other requests.This strategy capitalizes on the auto-regressive characteristics inherent in LLMs, where attention computation only relies on prior tokens.Consequently, the mathematical equivalence of chunked-prefill technology is guaranteed, positioning it as a leading approach for reducing request latency in LLM serving.</p>
<p>Scheduling Strategy</p>
<p>In LLM serving, the job length of each request exhibits variability, and hence the order of executing requests significantly impacts the throughput of the serving system.The head-of-line blocking [280] happens when long requests are accorded priority.Specifically, memory usage grows rapidly in response to long requests, impeding subsequent requests when the system exhausts its memory capacity.The pioneering work ORCA [277] and open-source systems, including vLLM [51] and LightLLM [278], employ the simple first-come-first-serve (FCFS) principle to schedule requests.DeepSpeed-FastGen [279] gives priority to the decoding requests to enhance the performance.FastServe [280] proposes a preemptive scheduling strategy to optimize the head-ofline blocking problem, achieving low job completion time (JCT) in LLM serving.FastServe employs a multi-level feedback queue (MLFQ) to prioritize the requests with the shortest remaining time.Since the auto-regressive decoding approach poses unknown request lengths, FastServe predicts the length first and utilizes a skip-join fashion to find the proper priority for each request.Unlike previous work, VTC [281] discusses the fairness in LLM serving.VTC introduces a cost function based on token numbers to measure fairness among clients, and further proposes a fair scheduler to ensure fairness.</p>
<p>Distributed Systems</p>
<p>In order to achieve high throughput, LLM services are commonly deployed on distributed platforms.Recent works have additionally focused on optimizing the performance of such inference services by exploiting distributed characteristics.Notably, observing that the computations of prefilling and decoding have interference with each other, splitwise [272], TetriInfer [273] and DistServe [274] demonstrate the efficiency of disaggregating the prefilling and the decoding steps of a request.In this way, the two distinct steps are processed independently based on their characteristics.ExeGPT [287] also adopts such disaggregated architecture, and proposes different strategies with controllable variables to maximize system throughput under certain latency constraints.Llumnix [288] reschedules the requests at runtime for different serving objectives including load balancing, de-fragmentation, and prioritization.Spot-Serve [275] is designed to provide LLM service on clouds with preemptible GPU instances.SpotServe efficiently handles challenges including dynamic parallel control and instance migration, and also utilizes the auto-regressive nature of LLMs to achieve token-level state recovery.Moreover, Infinite-LLM [276] parallels different parts of the sequence in the attention operator across the data center, to address the challenges when serving extremely long contexts.</p>
<p>LoongServe [289] proposes the elastic sequence parallelism to manage the elastic resource demand at the iteration level, reducing the data movement of KV cache via elaborately designed scheduling.</p>
<p>Hardware Accelerator Design</p>
<p>Previous research efforts [290], [291], [292] have focused on optimizing Transformer architectures, particularly enhancing the attention operator, often employing sparse methods to facilitate FPGA deployment.The FACT [293] accelerator achieves superior energy efficiency compared to the NVIDIA V100 GPU through mixed-precision quantization for linear operators and algorithm-hardware co-design, yet these approaches are not tailored for generative LLMs.</p>
<p>Recent work like ALLO [294] highlights FPGA advantages in managing the memory-intensive decoding stage and emphasizes the importance of model compression techniques for LLMs' efficient FPGA deployment.Conversely, DFX [295] focuses on decoding stage optimizations but lacks model compression methods, limiting scalability to larger models and longer inputs (up to 1.5B model and 256 tokens).ALLO builds on these insights, further offering a library of High-level Synthesis (HLS) kernels that are composable and reusable.ALLO's implementation demonstrates superior generation speed-up compared to DFX in the prefilling stage, achieving enhanced energy efficiency and speedup over the NVIDIA A100 GPU during decoding.</p>
<p>FlightLLM [296] also leverages these insights, introducing a configurable sparse digital signal processor (DSP) chain for various sparsity patterns with high computational efficiency.It proposes an always-on-chip decode scheme with mixed-precision support to enhance memory bandwidth utilization.FlightLLM achieves 6.0√ó higher energy efficiency and 1.8√ó better cost efficiency than the NVIDIA V100S GPU for Llama2-7B models, with 1.2√ó higher throughput than the NVIDIA A100 GPU during decoding.</p>
<p>Comparison of LLM Frameworks</p>
<p>We compare the performance of multiple LLM frameworks in Table 6.The inference throughput is measured with Llama2-7B (batch size=1, input length=1k, output length=128).The serving performance is the maximum throughput measured on the ShareGPT [297] dataset.Both are derived on a single NVIDIA A100 80GB GPU.Among the mentioned frameworks, DeepSpeed [258], vLLM [51], LightLLM [278] and TensorRT-LLM [222] integrate the serving function to serve asynchronous requests from multiple users.We also list the optimizations for each framework in the table.All the frameworks except HuggingFace implement operator-level or graph-level optimizations to enhance performance, and some of them also support the speculative decoding technique.Note that the speculative decoding technique is off when we measure the inference performance for all frameworks.The results of inference throughput show that FlashDecoding++ and TensorRT-LLM outperform others with optimizations covering predominant operators and the computational graph.From the aspect of serving, all the frameworks use fine-grained and discontiguous storage for KV cache, and apply the continuous batching techniques to improve the system utilization.Unlike vLLM and LightLLM, DeepSpeed prioritizes the decoding requests in scheduling, which means no new request is merged if there are enough existing decoding requests in the batch.</p>
<p>Knowledge, Suggestions and Future Direction</p>
<p>The system-level optimization improves efficiency while bringing no accuracy degradation, hence becoming prevalent in the LLM inference practice.The optimization for inference is also applicable to serving.Recently, the operator optimization has been closely combined with practical serving scenarios, e.g.,, RadixAttention [52] designed specifically for prefix caching, and tree attention [243] to accelerate speculative decoding verification.The iterating of applications and scenarios will continue to put forward new requirements for operator development.</p>
<p>Given the multifaceted objectives inherent in real-world serving systems, such as JCT, system throughput, and fairness, the design of scheduling strategies becomes correspondingly intricate.Within the domain of LLM serving, where the length of requests is indeterminate, extant literature commonly relies on predictive mechanisms to facilitate the design of scheduling strategies.However, the efficacy of current predictors [273] falls short of ideal standards, indicating the potential for refinement and optimization in serving scheduling strategy development.</p>
<p>DISCUSSIONS OF KEY APPLICATION SCENAR-</p>
<p>IOS</p>
<p>Current research endeavors have made significant strides in exploring the boundaries of efficient LLM inference across various optimization levels.However, further studies are warranted to enhance LLM efficiency in practical scenarios.We have provided promising future directions for optimization techniques at the data-level (Sec.4.3), model-level (Sec.5.3), and system-level (Sec.6.5).In this section, we summarize four critical scenarios: agent and multi-model framework, long-context LLMs, edge scenario deployment, and security-efficiency synergy, and provide a broader discussion on them.Agent and Multi-Model Framework.As discussed in Sec.4.3, recent advancements in agent and multi-model frameworks [55], [56], [57] have significantly improved agents' capabilities to handle complex tasks and human requests by harnessing the powerful abilities of LLMs.These frameworks, while increasing the computational demands of LLMs, introduce more parallelism into the structure of LLMs' output content, thereby creating opportunities for data-level and system-level optimizations such as output organization techniques [52].Furthermore, these frameworks naturally introduce a new optimization level, i.e., pipelinelevel, which holds potential for efficiency enhancements at this level [58].</p>
<p>In addition, there is a growing research trend [298] focused on extending AI agents into the multimodal domain, which often utilize Large Multimodal Models (LMMs) as the core of these agent systems.To enhance the efficiency of these emerging LMM-based agents, designing optimization techniques for LMMs is a promising research direction.Long-Context LLMs.Currently, LLMs face the challenge of handling increasingly longer input contexts.However, the self-attention operation, the fundamental component of Transformer-style LLMs, exhibits quadratic complexity in relation to the context length, imposing constraints on maximum context length during both training and inference phases.Various strategies have been explored to address this limitation, including input compression (Sec.4.1), sparse attention (Sec.5.2.2), design of low-complexity structures (Sec.5.1.3),and optimization of attention operators (Sec.6.1.1).Notably, non-Transformer architectures (Sec.5.1.3)with sub-quadratic or linear complexity have recently garnered significant interest from researchers.</p>
<p>Despite their efficiency, the competitiveness of these novel architectures compared to the Transformer architecture across various abilities, such as in-context learning ability and long-range modeling ability, is still under scrutiny [76], [299].Therefore, exploring the capabilities of these new architectures from multiple angles and addressing their limitations remains a valuable pursuit.Moreover, it is crucial to determine the necessary context lengths for various scenarios and tasks, as well as identify the next-generation architecture that will serve as the foundational backbone for LLMs in the future.Edge Scenario Deployment.While considerable efforts have been directed towards enhancing the efficiency of LLM inference, deploying LLMs onto extremely resourceconstrained edge devices like mobile phones presents ongoing challenges.Recently, numerous researchers [300], [301], [302], [303], [304], [305], [306], [307], [308], [309], [310] have shown interest in pre-training smaller language models with 1B to 3B parameters.Models of this scale offer reduced resource costs during inference and hold potential for achieving generalization abilities and competitive performance compared to larger models.However, the methods to develop such efficient and powerful smaller language models remain under-explored.</p>
<p>Several studies have initiated this promising direction.For instance, MiniCPM [309] conducts sandbox experiments to determine optimal pre-training hyper-parameters.PanGu-œÄ-Pro [302] suggests initializing model weights from pre-trained LLMs using metrics and techniques from model pruning.MobileLLM [310] adopts a"deep and thin" architecture for small model design and proposes weight sharing across different layers to increase the number of layers without additional memory costs.Nevertheless, a performance gap still exists between small and large models, necessitating future studies to narrow this gap.In the future, there is a crucial need for research aimed at identifying the model scale limited in the edge scenarios, and exploring the boundaries of various optimization methods on designing smaller models.</p>
<p>Beyond designing smaller models, system-level optimization offers a promising direction in LLM deployment.A notable recent project, MLC-LLM [311], successfully deploys the LLaMA-7B model on mobile phones.MLC-LLM primarily employs compilation techniques like fusion, memory planning, and loop optimization to enhance latency and reduce memory cost during inference.Additionally, adopting the cloud-edge collaboration techniques, or designing more sophisticated hardware accelerators can also help deploy LLMs onto edge devices.Security-Efficiency Synergy.In addition to task performance and efficiency, security is also a crucial factor that must be considered in LLM applications [312], [313].Current research primarily focuses on efficiency optimization without adequately addressing security considerations.Therefore, it is critical to investigate the interplay between efficiency and security and determine whether the current optimization techniques compromise the security of LLMs.If these techniques negatively impacts LLMs' security, a promising direction would involve developing new optimization methods or refining the existing ones to achieve a better trade-off between LLMs' efficiency and security.</p>
<p>CONCLUSION</p>
<p>Efficient LLM inference focuses on reducing the computational, memory access, and memory costs during LLM inference processes, aiming to optimize efficiency metrics such as latency, throughput, storage, power, and energy.This survey offers a comprehensive review of efficient LLM inference research, presenting insights, recommendations, and future directions for key techniques.Initially, we introduce a hierarchical taxonomy encompassing data-, model-, and system-level optimizations.Subsequently, guided by this taxonomy, we meticulously examine and summarize studies at each level and sub-field.For well-established techniques like model quantization and efficient serving systems, we conduct experiments to evaluate and analyze their performance.Based on these analyses, we offer practical suggestions and identify promising research avenues for practitioners and researchers in the field.</p>
<p>Fig. 1 .
1
Fig. 1.The challenges of LLM deployment.</p>
<p>Fig. 2 .
2
Fig. 2. Demonstration of the prefilling stage (a) and decoding stage (b).</p>
<p>3 .Fig. 6 .
36
Fig. 6.Demonstration of the inference process of SoT.</p>
<p>Fig. 9 .
9
Fig. 9. (a) The inference workflow of Weight-only Quantization.(b) The inference workflow of Weight-Activation Quantization.</p>
<p>Fig. 10 .
10
Fig. 10.Illustration of Unstructured Pruning (left) and Structured Pruning (right).</p>
<p>1 Fig. 11 .
111
Fig. 11.Examples of different sparse attention masks.(a) Static mask with local, global, and random attention pattern.(b) Static mask with dilated attention pattern of different dilated rate.(c) Dynamic token pruning.(d) Dynamic attention pruning.</p>
<p>Fig. 12 .
12
Fig. 12. Illustration of White-Box KD (left) and Black-Box KD (right).</p>
<p>Fig. 15 .
15
Fig. 15.Inference runtime breakdown over multiple LLMs.</p>
<p>Fig. 16 .
16
Fig. 16.Comparison of auto-regressive decoding (a) and speculative decoding (b).</p>
<p>TABLE 1
1
Comparison of existing surveys.</p>
<p>To mitigate the additional computation
1. Noodles: Various noodledishes, such as ‚Ä¶What are the typical types of Chinese dishes?‚Ä¶ 1. Noodles 2. Hot pot 3. Rice2. Hot pot: A communal pot of simmering broth at the center of the table ‚Ä¶</p>
<p>TABLE 2
2
Efficiency comparison of some novel non-Transformer models.Note that we denote n as the input length and d as the input dimension.
ModelTraining FormTraining Computational ComplexityTraining Memory ComplexityInference FormInference Computational Complexity Prefilling Decoding (per token)</p>
<p>and thus can rewrite Eq. 8 in a recursive form.In this way, RWKV can combine the effective parallelizable training feature of Transformer and the efficient inference ability of RNN.
Efficiency Analysis. We analyze and compare the com-putational and memory complexity of several innovativeand representative non-transformer architectures in Table 2.In terms of training time, many studies (e.g., S4, Hyena,RetNet) aim to preserve training parallelism by adopting
training forms such as the convolution or attention.Notably, Mamba utilizes parallel scan techniques for processing input sequences, thereby leveraging training parallelism as well.</p>
<p>TABLE 3
3
Summary of the representative studies on Post-Training Quantization.Quantized Tensor Type denotes which parts of tensors are quantized.Data Format denotes whether to adopt uniform or non-uniform quantization.Quantization Parameter Determination Scheme denotes the how to decide the parameters (e.g., scaling factor, zero-point).Quantized Value Update denotes whether to change the model weight (e.g., compensation, re-parameterization) during the quantization process.
ModelQuantized Tensor Type Weight Activation KV CacheData FormatQuantization Parameter Determination SchemeQuantized Value UpdateGPTQ</p>
<p>TABLE 4
4
Comparison of speed-ups in different scenarios (e.g., model size, batch size, input context length, inference framework) with W4A16 quantization based on TensorRT-LLM</p>
<p>.92/1.77 0.91/1.82/1.710.92/1.65/1.570.93/1.45/1.410.94/1.28/1.2616 0.92/1.92/1.770.91/1.82/1.710.92/1.65/1.570.93/1.45/1.410.94/1.28/1.26.42/2.36 0.92/2.37/2.320.94/2.29/2.25 0.94/2.15/2.120.95/1.95/1.93 4 0.93/2.36/2.260.94/2.29/2.210.94/2.18/2.120.95/2.01/1.970.96/1.78/1.75 8 0.92/2.24/2.100.93/1.93/2.020.94/1.81/1.890.94/1.65/1.710.95/1.45/1.4916 0.93/2.02/1.850.94/1.90/1.760.94/1.73/1.630.95/1.50/1.45OOM
TensorRT-LLMB1282565121024204811.06/2.40/2.37 0.90/2.38/2.34 0.92/2.30/2.28 0.88/2.19/2.17 0.91/2.00/1.9820.88/2.10/2.05 0.91/2.07/2.04 0.89/2.01/1.98 0.91/1.92/1.89 0.88/1.78/1.76LLaMA-2-7B40.92/1.72/1.67 0.89/1.67/1.64 0.90/1.61/1.58 0.87/1.53/1.51 0.84/1.42/1.4080.91/1.43/1.36 0.88/1.38/1.33 0.83/1.33/1.28 0.77/1.25/1.21 0.78/1.16/1.1416 0.91/1.43/1.36 0.88/1.38/1.33 0.83/1.33/1.28 0.77/1.25/1.21 0.78/1.16/1.14B1282565121024204811.24/2.51/2.50 0.89/2.45/2.47 0.94/2.34/2.42 0.90/2.18/2.32 0.83/1.94/2.1620.90/2.51/2.50 0.95/2.45/2.47 0.90/2.34/2.42 0.83/2.18/2.32 0.80/1.94/2.16LLaMA-2-13B40.96/1.80/1.76 0.91/1.78/1.74 0.83/1.73/1.69 0.80/1.65/1.62 0.83/1.54/1.5280.91/1.86/1.77 0.83/1.81/1.73 0.80/1.73/1.66 0.82/1.62/1.56 0.75/1.46/1.4116 0.84/1.84/1.69 0.81/1.77/1.63 0.82/1.63/1.53 0.78/1.46/1.39OOMLMDeployB1282565121024204811.30/2.11/2.09 0.94/2.07/2.05 0.90/2.03/2.02 0.88/1.97/1.96 0.94/1.92/1.9121.03/2.24/2.20 0.90/2.19/2.15 0.88/2.11/2.08 0.93/1.97/1.95 0.85/1.78/1.76LLaMA-2-7B40.90/2.18/2.10 0.87/2.12/2.05 0.93/2.01/1.96 0.92/1.86/1.83 0.92/1.64/1.628 0.92/1B 1282565121024204811.32/2.34/2.32 0.94/2.31/2.28 0.92/2.22/2.20 0.94/2.15/2.13 0.94/2.01/1.99LLaMA-2-13B 1.06/2group-wise manner. Li et al. [214] conducted a thorough 2evaluation to assess the impact of quantization on differenttensor types (including KV Cache), various tasks, 11 LLMfamilies, and SOTA quantization methods.Quantization-Aware Training. Quantization-aware training(QAT) incorporates the influence of quantization within themodel training procedure. By integrating layers that repli-cate quantization effects, this approach facilitates weightadaptation to quantization-induced errors, leading to en-hanced task performance. Nevertheless, training LLMs typ-ically demands substantial training data and considerablecomputational resources, posing potential bottlenecks forQAT implementation. Consequently, current research en-deavors focus on strategies to reduce the training data re-quirements or alleviate the computational burden associatedwith QAT implementation.</p>
<ol>
<li>Taxonomy of the optimization for LLM inference engine.
others 23.63%linear 40.55%others 21.68%38.82% linear27.36% others53.37% linear24.64% others50.42% linearattention 1.84%others 6.72%attention 2.33%others 7.95%attention 35.82%attention 39.50%attention 19.27%attention 29.94%linear 91.44%linear 89.72%(a) Llama2-7B,128 context length</li>
</ol>
<p>TABLE 6
6
Comparison of multiple open-source inference engines and serving systems."-" denotes no serving support.Note that the scheduling method of TensorRT-LLM is not open-sourced.
ModelInference Optimization Attention Linear Graph Speculative DecodingInference (token/s)MemoryServing Optimization BatchingSchedulingServing (req/s)HuggingFace [260]‚úì38.963----DeepSpeed [258]‚úì‚úì80.947blockedsplit-and-fusedecode prioritized6.78vLLM [51]‚úì‚úì90.052pagedcontinuous batchingprefill prioritized7.11OpenPPL [263]‚úì‚úì81.169----FlashDecoding++ [253]‚úì‚úì‚úì106.636----LightLLM [278]‚úì73.599token-wisesplit-and-fuseprefill prioritized10.29TensorRT-LLM [222]‚úì‚úì‚úì‚úì92.512pagedcontinuous batching-5.87
. A recent study[28] shows that FlashAttention, a common-used system-level optimization technique, might cause the numeric deviation.
ACKNOWLEDGEMENTSThis work was supported by National Natural Science Foundation of China (No. 62325405, 62104128, U19B2019, U21B2031, 61832007, 62204164), Tsinghua EE Xilinx AI Research Fund, and Beijing National Research Center for Information Science and Technology (BNRist).We thank for all the support from Infinigence-AI.We thank Xiangsheng Shi, Zinan Lin, Xinhao Yang, Hongyi Wang, Linfeng Zhang, Yulin Wang, Xuemin Sun, Saiqian Zhang for their valuable suggestions on the paper.We thank Shengxiang Wang, Qiuli Mao for providing the efficiency profiling data of quantized operators.
Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Opt: Open pre-trained transformer language models. S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.010682022arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozi√®re, N Goyal, E Hambro, F Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>Baichuan 2: Open large-scale language models. A Yang, B Xiao, B Wang, B Zhang, C Bian, C Yin, C Lv, D Pan, D Wang, D Yan, arXiv:2309.103052023arXiv preprint</p>
<p>Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, April 2023142023</p>
<p>How long can context length of opensource llms truly promise?. D Li, R Shao, A Xie, Y Sheng, L Zheng, J Gonzalez, I Stoica, X Ma, H Zhang, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Bloom: A 176b-parameter open-access multilingual language model. B Workshop, T L Scao, A Fan, C Akiki, E Pavlick, S Iliƒá, D Hesslow, R Castagn√©, A S Luccioni, F Yvon, arXiv:2211.051002022arXiv preprint</p>
<p>The falcon series of open language models. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, √â Goffinet, D Hesslow, J Launay, Q Malartic, arXiv:2311.168672023arXiv preprint</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, arXiv:2103.103602021arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, C Bamford, D S Chaplot, D Casas, E B Hanna, F Bressand, arXiv:2401.04088Mixtral of experts. 2024arXiv preprint</p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. J Yang, H Jin, R Tang, X Han, Q Feng, H Jiang, S Zhong, B Yin, X Hu, ACM Transactions on Knowledge Discovery from Data. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>A survey on model compression for large language models. X Zhu, J Li, Y Liu, C Ma, W Wang, arXiv:2308.076332023arXiv preprint</p>
<p>A comprehensive survey of compression algorithms for language models. S Park, J Choi, S Lee, U Kang, arXiv:2401.153472024arXiv preprint</p>
<p>Model compression and efficient inference for large language models: A survey. W Wang, W Chen, Y Luo, Y Long, Z Lin, L Zhang, B Lin, D Cai, X He, arXiv:2402.097482024arXiv preprint</p>
<p>A survey on transformer compression. Y Tang, Y Wang, J Guo, Z Tu, K Han, H Hu, D Tao, arXiv:2402.059642024arXiv preprint</p>
<p>The efficiency spectrum of large language models: An algorithmic survey. T Ding, T Chen, H Zhu, J Jiang, Y Zhong, J Zhou, G Wang, Z Zhu, I Zharkov, L Liang, arXiv:2312.006782023arXiv preprint</p>
<p>Towards efficient generative large language model serving: A survey from algorithms to systems. X Miao, G Oliaro, Z Zhang, X Cheng, H Jin, T Chen, Z Jia, arXiv:2312.152342023arXiv preprint</p>
<p>Efficient large language models: A survey. Z Wan, X Wang, C Liu, S Alam, Y Zheng, Z Qu, S Yan, Y Zhu, Q Zhang, M Chowdhury, arXiv:2312.0386320231arXiv preprint</p>
<p>A survey of resource-efficient llm and multimodal foundation models. M Xu, W Yin, D Cai, R Yi, D Xu, Q Wang, B Wu, Y Zhao, C Yang, S Wang, arXiv:2401.080922024arXiv preprint</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, ≈Å Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Z Yuan, Y Shang, Y Zhou, Z Dong, C Xue, B Wu, Z Li, Q Gu, Y J Lee, Y Yan, arXiv:2402.16363Llm inference unveiled: Survey and roofline model insights. 2024arXiv preprint</p>
<p>Is flash attention stable. A Golden, S Hsia, F Sun, B Acun, B Hosmer, Y Lee, Z Devito, J Johnson, G.-Y Wei, D Brooks, arXiv:2405.028032024arXiv preprint</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H √úttler, M Lewis, W -T. Yih, T Rockt√§schel, Advances in Neural Information Processing Systems. 202033</p>
<p>Adapting language models to compress contexts. A Chevalier, A Wettig, A Ajith, D Chen, arXiv:2305.147882023arXiv preprint</p>
<p>Replug: Retrieval-augmented black-box language models. W Shi, S Min, M Yasunaga, M Seo, R James, M Lewis, L Zettlemoyer, W Yih, 2023</p>
<p>Selfrag: Learning to retrieve, generate, and critique through selfreflection. A Asai, Z Wu, Y Wang, A Sil, H Hajishirzi, 2023</p>
<p>Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. D Wingate, M Shoeybi, T Sorensen, arXiv:2210.031622022arXiv preprint</p>
<p>Learning to compress prompts with gist tokens. J Mu, X L Li, N Goodman, arXiv:2304.084672023arXiv preprint</p>
<p>. T Ge, J Hu, X Wang, S.-Q Chen, F Wei, arXiv:2307.069452023arXiv preprintIn-context autoencoder for context compression in a large language model</p>
<p>Recomp: Improving retrievalaugmented lms with compression and selective augmentation. F Xu, W Shi, E Choi, arXiv:2310.044082023arXiv preprint</p>
<p>Extending context window of large language models via semantic compression. W Fei, X Niu, P Zhou, L Hou, B Bai, L Deng, W Han, arXiv:2312.095712023arXiv preprint</p>
<p>Efficient prompting via dynamic in-context learning. W Zhou, Y E Jiang, R Cotterell, M Sachan, arXiv:2305.111702023arXiv preprint</p>
<p>Compressing context to enhance inference efficiency of large language models. Y Li, B Dong, F Guerin, C Lin, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. F Yin, J Vig, P Laban, S Joty, C Xiong, C.-S J Wu, arXiv:2306.011502023arXiv preprint</p>
<p>Discrete prompt compression with reinforcement learning. H Jung, K.-J Kim, arXiv:2308.087582023arXiv preprint</p>
<p>Llmlingua: Compressing prompts for accelerated inference of large language models. H Jiang, Q Wu, C.-Y Lin, Y Yang, L Qiu, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. H Jiang, Q Wu, X Luo, D Li, C.-Y Lin, Y Yang, L Qiu, arXiv:2310.068392023arXiv preprint</p>
<p>Boosting llm reasoning: Push the limits of few-shot learning with reinforced in-context pruning. X Huang, L L Zhang, K.-T Cheng, M Yang, arXiv:2312.089012023arXiv preprint</p>
<p>A survey for in-context learning. Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, Z Sui, arXiv:2301.002342022arXiv preprint</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Skeleton-ofthought: Large language models can do parallel decoding. X Ning, Z Lin, Z Zhou, H Yang, Y Wang, arXiv:2307.153372023arXiv preprint</p>
<p>Adaptive skeleton graph decoding. S Jin, Y Wu, H Zheng, Q Zhang, M Lentz, Z M Mao, A Prakash, F Qian, D Zhuo, arXiv:2402.122802024arXiv preprint</p>
<p>Apar: Llms can do auto-parallel auto-regressive decoding. M Liu, A Zeng, B Wang, P Zhang, J Tang, Y Dong, arXiv:2401.067612024arXiv preprint</p>
<p>Medusa: Simple llm inference acceleration framework with multiple decoding heads. T Cai, Y Li, Z Geng, H Peng, J D Lee, D Chen, T Dao, 2024</p>
<p>Efficient memory management for large language model serving with pagedattention. W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J Gonzalez, H Zhang, I Stoica, Proceedings of the 29th Symposium on Operating Systems Principles. the 29th Symposium on Operating Systems Principles2023</p>
<p>Efficiently programming large language models using sglang. L Zheng, L Yin, Z Xie, J Huang, C Sun, C H Yu, S Cao, C Kozyrakis, I Stoica, J E Gonzalez, arXiv:2312.071042023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, M Podstawski, L Gianinazzi, J Gajda, T Lehmann, H Niewiadomski, P Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438690</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Corex: Pushing the boundaries of complex reasoning through multimodel collaboration. Q Sun, Z Yin, X Li, Z Wu, X Qiu, L Kong, arXiv:2310.002802023arXiv preprint</p>
<p>Large language model based multiagents: A survey of progress and challenges. T Guo, X Chen, Y Wang, R Chang, S Pei, N V Chawla, O Wiest, X Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Frugalgpt: How to use large language models while reducing cost and improving performance. L Chen, M Zaharia, J Zou, arXiv:2305.051762023arXiv preprint</p>
<p>What makes convolutional models great on long sequence modeling. Y Li, T Cai, Y Zhang, D Chen, D Dey, arXiv:2210.092982022arXiv preprint</p>
<p>Ckconv: Continuous kernel convolution for sequential data. D W Romero, A Kuzina, E J Bekkers, J M Tomczak, M Hoogendoorn, arXiv:2102.026112021arXiv preprint</p>
<p>Hyena hierarchy: Towards larger convolutional language models. M Poli, S Massaroli, E Nguyen, D Y Fu, T Dao, S Baccus, Y Bengio, S Ermon, C R√©, International Conference on Machine Learning. PMLR20232878</p>
<p>Rwkv: Reinventing rnns for the transformer era. B Peng, E Alcaide, Q Anthony, A Albalak, S Arcadinho, H Cao, X Cheng, M Chung, M Grella, K K Gv, arXiv:2305.130482023arXiv preprint</p>
<p>Retentive network: A successor to transformer for large language models. Y Sun, L Dong, S Huang, S Ma, Y Xia, J Xue, J Wang, F Wei, arXiv:2307.086212023arXiv preprint</p>
<p>Hippo: Recurrent memory with optimal polynomial projections. A Gu, T Dao, S Ermon, A Rudra, C R√©, Advances in neural information processing systems. 202033</p>
<p>Combining recurrent, convolutional, and continuoustime models with linear state space layers. A Gu, I Johnson, K Goel, K Saab, T Dao, A Rudra, C R√©, Advances in neural information processing systems. 202134</p>
<p>Efficiently modeling long sequences with structured state spaces. A Gu, K Goel, C R√©, arXiv:2111.003962021arXiv preprint</p>
<p>Diagonal state spaces are as effective as structured state spaces. A Gupta, A Gu, J Berant, Advances in Neural Information Processing Systems. 202235</p>
<p>On the parameterization and initialization of diagonal state space models. A Gu, K Goel, A Gupta, C R√©, Advances in Neural Information Processing Systems. 202235</p>
<p>Long range language modeling via gated state spaces. H Mehta, A Gupta, A Cutkosky, B Neyshabur, International Conference on Learning Representations. 2023</p>
<p>Hungry hungry hippos: Towards language modeling with state space models. D Y Fu, T Dao, K K Saab, A W Thomas, A Rudra, C R√©, arXiv:2212.140522022arXiv preprint</p>
<p>Liquid structural state-space models. R Hasani, M Lechner, T.-H Wang, M Chahine, A Amini, D Rus, arXiv:2209.129512022arXiv preprint</p>
<p>Simplified state space layers for sequence modeling. J T Smith, A Warrington, S W Linderman, arXiv:2208.049332022arXiv preprint</p>
<p>Block-state transformers. J Pilault, M Fathi, O Firat, C Pal, P.-L Bacon, R Goroshin, Advances in Neural Information Processing Systems. 202436</p>
<p>Pretraining without attention. J Wang, J N Yan, A Gu, A M Rush, arXiv:2212.105442022arXiv preprint</p>
<p>Mamba: Linear-time sequence modeling with selective state spaces. A Gu, T Dao, arXiv:2312.007522023arXiv preprint</p>
<p>Can mamba learn how to learn? a comparative study on in-context learning tasks. J Park, J Park, Z Xiong, N Lee, J Cho, S Oymak, K Lee, D Papailiopoulos, arXiv:2402.042482024arXiv preprint</p>
<p>Fast transformer decoding: One write-head is all you need. N Shazeer, arXiv:1911.021502019arXiv preprint</p>
<p>Gqa: Training generalized multi-query transformer models from multi-head checkpoints. J Ainslie, J Lee-Thorp, M Jong, Y Zemlyanskiy, F Lebr √ìn, S Sanghai, arXiv:2305.132452023arXiv preprint</p>
<p>Linformer: Self-attention with linear complexity. S Wang, B Z Li, M Khabsa, H Fang, H Ma, arXiv:2006.047682020arXiv preprint</p>
<p>Lightweight and efficient end-to-end speech recognition using low-rank transformer. G I Winata, S Cahyawijaya, Z Lin, Z Liu, P Fung, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. ICASSP</p>
<p>Flurka: Fast fused low-rank &amp; kernel attention. A Gupta, Y Yuan, Y Zhou, C Mendis, arXiv:2306.157992023arXiv preprint</p>
<p>Luna: Linear unified nested attention. X Ma, X Kong, S Wang, C Zhou, J May, H Ma, L Zettlemoyer, Advances in Neural Information Processing Systems. 202134</p>
<p>Set transformer: A framework for attention-based permutationinvariant neural networks. J Lee, Y Lee, J Kim, A Kosiorek, S Choi, Y W Teh, International conference on machine learning. PMLR2019</p>
<p>Transformers are rnns: Fast autoregressive transformers with linear attention. A Katharopoulos, A Vyas, N Pappas, F Fleuret, International conference on machine learning. PMLR2020</p>
<p>Rethinking attention with performers. K M Choromanski, V Likhosherstov, D Dohan, X Song, A Gane, T Sarlos, P Hawkins, J Q Davis, A Mohiuddin, L Kaiser, 2020in International Conference on Learning Representations</p>
<p>Random feature attention. H Peng, N Pappas, D Yogatama, R Schwartz, N Smith, L Kong, International Conference on Learning Representations. 2022</p>
<p>Polysketchformer: Fast transformers via sketches for polynomial kernels. P Kacham, V Mirrokni, P Zhong, arXiv:2310.016552023arXiv preprint</p>
<p>Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. W Fedus, B Zoph, N Shazeer, The Journal of Machine Learning Research. 2312022</p>
<p>Moefication: Transformer feed-forward layers are mixtures of experts. Z Zhang, Y Lin, Z Liu, P Li, M Sun, J Zhou, Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>Parameterefficient mixture-of-experts architecture for pre-trained language models. Z.-F Gao, P Liu, W X Zhao, Z.-Y Lu, J.-R Wen, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022</p>
<p>A Komatsuzaki, J Puigcerver, J Lee-Thorp, C R Ruiz, B Mustafa, J Ainslie, Y Tay, M Dehghani, N Houlsby, arXiv:2212.05055Sparse upcycling: Training mixture-of-experts from dense checkpoints. 2022arXiv preprint</p>
<p>Base layers: Simplifying training of large, sparse models. M Lewis, S Bhosale, T Dettmers, N Goyal, L Zettlemoyer, International Conference on Machine Learning. PMLR2021</p>
<p>Mixture-of-experts with expert choice routing. Y Zhou, T Lei, H Liu, N Du, Y Huang, V Zhao, A M Dai, Q V Le, J Laudon, Advances in Neural Information Processing Systems. 202235</p>
<p>St-moe: Designing stable and transferable sparse expert models. B Zoph, I Bello, S Kumar, N Du, Y Huang, J Dean, N Shazeer, W Fedus, arXiv:2202.089062022arXiv preprint</p>
<p>Stablemoe: Stable routing strategy for mixture of experts. D Dai, L Dong, S Ma, B Zheng, Z Sui, B Chang, F Wei, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Sparse moe as the new dropout: Scaling dense and self-slimmable transformers. T Chen, Z Zhang, A K Jaiswal, S Liu, Z Wang, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Glam: Efficient scaling of language models with mixture-of-experts. N Du, Y Huang, A M Dai, S Tong, D Lepikhin, Y Xu, M Krikun, Y Zhou, A W Yu, O Firat, International Conference on Machine Learning. PMLR2022</p>
<p>Outrageously large neural networks: The sparselygated mixture-of-experts layer. N Shazeer, A Mirhoseini, K Maziarz, A Davis, Q Le, G Hinton, J Dean, International Conference on Learning Representations. 2016</p>
<p>Gshard: Scaling giant models with conditional computation and automatic sharding. D Lepikhin, H Lee, Y Xu, D Chen, O Firat, Y Huang, M Krikun, N Shazeer, Z Chen, arXiv:2006.166682020arXiv preprint</p>
<p>Tutel: Adaptive mixture-of-experts at scale. C Hwang, W Cui, Y Xiong, Z Yang, Z Liu, H Hu, Z Wang, R Salas, J Jose, P Ram, Proceedings of Machine Learning and Systems. Machine Learning and Systems20235</p>
<p>Auction algorithms for network flow problems: A tutorial introduction. D P Bertsekas, Computational optimization and applications. 19921</p>
<p>Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Z Dai, G Lai, Y Yang, Q Le, Advances in neural information processing systems. 202033</p>
<p>Vision mamba: Efficient visual representation learning with bidirectional state space model. L Zhu, B Liao, Q Zhang, X Wang, W Liu, X Wang, arXiv:2401.094172024arXiv preprint</p>
<p>Transformer quality in linear time. W Hua, Z Dai, H Liu, Q Le, International Conference on Machine Learning. PMLR2022</p>
<p>Jamba: Ai21's groundbreaking ssm-transformer model. AI21. March 2024</p>
<p>Densemamba: State space models with dense hidden connection for efficient large language models. W He, K Han, Y Tang, C Wang, Y Yang, T Guo, Y Wang, arXiv:2403.008182024arXiv preprint</p>
<p>Blackmamba: Mixture of experts for state-space models. Q Anthony, Y Tokpanov, P Glorioso, B Millidge, arXiv:2402.017712024arXiv preprint</p>
<p>Moe-mamba: Efficient selective state space models with mixture of experts. M Pi √ìro, K Ciebiera, K Kr √ìl, J Ludziejewski, S Jaszczur, arXiv:2401.040812024arXiv preprint</p>
<p>An attention free transformer. S Zhai, W Talbott, N Srivastava, C Huang, H Goh, R Zhang, J Susskind, arXiv:2105.141032021arXiv preprint</p>
<p>Confident adaptive language modeling. T Schuster, A Fisch, J Gupta, M Dehghani, D Bahri, V Tran, Y Tay, D Metzler, Advances in Neural Information Processing Systems. 202235472</p>
<p>Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference. L Del Corro, A Del Giorno, S Agarwal, B Yu, A Awadallah, S Mukherjee, arXiv:2307.026282023arXiv preprint</p>
<p>Fastbert: a self-distilling bert with adaptive inference time. W Liu, P Zhou, Z Wang, Z Zhao, H Deng, Q Ju, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Accelerating inference for pretrained language models by unified multi-perspective early exiting. J Kong, J Wang, L.-C Yu, X Zhang, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022</p>
<p>A global past-future early exit method for accelerating inference of pretrained language models. K Liao, Y Zhang, X Ren, Q Su, X Sun, B He, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Deebert: Dynamic early exiting for accelerating bert inference. J Xin, R Tang, J Lee, Y Yu, J Lin, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Bert loses patience: Fast and robust inference with early exit. W Zhou, C Xu, T Ge, J Mcauley, K Xu, F Wei, Advances in Neural Information Processing Systems. 202033341</p>
<p>A simple hash-based early exiting approach for language understanding and generation. T Sun, X Liu, W Zhu, Z Geng, L Wu, Y He, Y Ni, G Xie, X.-J Huang, X Qiu, Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>In-context learning distillation: Transferring few-shot learning ability of pre-trained language models. Y Huang, Y Chen, Z Yu, K Mckeown, arXiv:2212.106702022arXiv preprint</p>
<p>Multistage collaborative knowledge distillation from large language models. J Zhao, W Zhao, A Drozdov, B Rozonoyer, M A Sultan, J.-Y Lee, M Iyyer, A Mccallum, arXiv:2311.086402023arXiv preprint</p>
<p>Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. C.-Y Hsieh, C.-L Li, C.-K Yeh, H Nakhost, Y Fujii, A Ratner, R Krishna, C.-Y Lee, T Pfister, arXiv:2305.023012023arXiv preprint</p>
<p>Symbolic chain-of-thought distillation: Small models can also" think" step-by-step. L H Li, J Hessel, Y Yu, X Ren, K.-W Chang, Y Choi, arXiv:2306.140502023arXiv preprint</p>
<p>Teaching small language models to reason. L C Magister, J Mallinson, J Adamek, E Malmi, A Severyn, arXiv:2212.084102022arXiv preprint</p>
<p>Mcckd: Multi-cot consistent knowledge distillation. H Chen, S Wu, X Quan, R Wang, M Yan, J Zhang, arXiv:2310.147472023arXiv preprint</p>
<p>Large language models are reasoning teachers. N Ho, L Schmid, S.-Y Yun, arXiv:2212.100712022arXiv preprint</p>
<p>Distilling reasoning capabilities into smaller language models. K Shridhar, A Stolfo, M Sachan, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Pad: Programaided distillation specializes large models in reasoning. X Zhu, B Qi, K Zhang, X Long, B Zhou, arXiv:2305.138882023arXiv preprint</p>
<p>Scott: Self-consistent chain-of-thought distillation. P Wang, Z Wang, Z Li, Y Gao, B Yin, X Ren, arXiv:2305.018792023arXiv preprint</p>
<p>Disco: distilling counterfactuals with large language models. Z Chen, Q Gao, A Bosselut, A Sabharwal, K Richardson, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Lamini-lm: A diverse herd of distilled models from large-scale instructions. M Wu, A Waheed, C Zhang, M Abdul-Mageed, A F Aji, arXiv:2304.144022023arXiv preprint</p>
<p>Lion: Adversarial distillation of proprietary large language models. Y Jiang, C Chan, M Chen, W Wang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Knowledge distillation of large language models. Y Gu, L Dong, F Wei, M Huang, arXiv:2306.085432023arXiv preprint</p>
<p>Gkd: Generalized knowledge distillation for autoregressive sequence models. R Agarwal, N Vieillard, P Stanczyk, S Ramos, M Geist, O Bachem, arXiv:2306.136492023arXiv preprint</p>
<p>Less is more: Task-aware layer-wise distillation for language model compression. C Liang, S Zuo, Q Zhang, P He, W Chen, T Zhao, International Conference on Machine Learning. PMLR202320867</p>
<p>Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. I Timiryasov, J.-L Tastet, arXiv:2308.020192023arXiv preprint</p>
<p>Lifting the curse of capacity gap in distilling language models. C Zhang, Y Yang, J Liu, J Wang, Y Xian, B Wang, D Song, arXiv:2305.121292023arXiv preprint</p>
<p>Dynabert: Dynamic bert with adaptive width and depth. L Hou, Z Huang, L Shang, X Jiang, X Chen, Q Liu, Advances in Neural Information Processing Systems. 202033</p>
<p>Propagating knowledge updates to lms through distillation. S Padmanabhan, Y Onoe, M J Zhang, G Durrett, E Choi, arXiv:2306.093062023arXiv preprint</p>
<p>Autotinybert: Automatic hyper-parameter optimization for efficient pre-trained language models. Y Yin, C Chen, L Shang, X Jiang, X Chen, Q Liu, arXiv:2107.136862021arXiv preprint</p>
<p>Nasbert: task-agnostic and adaptive-size bert compression with neural architecture search. J Xu, X Tan, R Luo, K Song, J Li, T Qin, T.-Y Liu, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining2021</p>
<p>Structural pruning of large language models via neural architecture search. A Klein, J Golebiowski, X Ma, V Perrone, C Archambeau, 2023</p>
<p>Litetransformersearch: Training-free neural architecture search for efficient language models. M Javaheripi, G De Rosa, S Mukherjee, S Shah, T Religa, C C Mendes, S Bubeck, F Koushanfar, D Dey, Advances in Neural Information Processing Systems. 202235</p>
<p>Few-shot task-agnostic neural architecture search for distilling large language models. D D Xu, S Mukherjee, X Liu, D Dey, W Wang, X Zhang, A Awadallah, J Gao, Advances in Neural Information Processing Systems. 202235</p>
<p>Lord: Low rank decomposition of monolingual code llms for one-shot compression. A Kaushal, T Vaidhya, I Rish, arXiv:2309.140212023arXiv preprint</p>
<p>Tensorgpt: Efficient compression of the embedding layer in llms based on the tensor-train decomposition. M Xu, Y L Xu, D P Mandic, arXiv:2307.005262023arXiv preprint</p>
<p>Losparse: Structured compression of large language models based on low-rank and sparse approximation. Y Li, Y Yu, Q Zhang, C Liang, P He, W Chen, T Zhao, arXiv:2306.112222023arXiv preprint</p>
<p>Matrix compression via randomized low rank and low precision factorization. R Saha, V Srivastava, M Pilanci, arXiv:2310.110282023arXiv preprint</p>
<p>Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation. Z Yao, X Wu, C Li, S Youn, Y He, arXiv:2303.083022023arXiv preprint</p>
<p>Dsformer: Effective compression of text-transformers by dense-sparse weight factorization. R Chand, Y Prabhu, P Kumar, arXiv:2312.132112023arXiv preprint</p>
<p>Asvd: Activation-aware singular value decomposition for compressing large language models. Z Yuan, Y Shang, Y Song, Q Wu, Y Yan, G Sun, arXiv:2312.058212023arXiv preprint</p>
<p>Generating long sequences with sparse transformers. R Child, S Gray, A Radford, I Sutskever, arXiv:1904.105092019arXiv preprint</p>
<p>Efficient streaming language models with attention sinks. G Xiao, Y Tian, B Chen, S Han, M Lewis, arXiv:2309.174532023arXiv preprint</p>
<p>Longformer: The longdocument transformer. I Beltagy, M E Peters, A Cohan, arXiv:2004.051502020arXiv preprint</p>
<p>Big bird: Transformers for longer sequences. M Zaheer, G Guruganesh, K A Dubey, J Ainslie, C Alberti, S Ontanon, P Pham, A Ravula, Q Wang, L Yang, Advances in neural information processing systems. 202033</p>
<p>Efficient transformer inference with statically structured sparse attention. S Dai, H Genc, R Venkatesan, B Khailany, 2023 60th ACM/IEEE Design Automation Conference (DAC). IEEE2023</p>
<p>SemSA: Semantic sparse attention is hidden in large language models. Anonymous, 2023</p>
<p>Spatten: Efficient sparse attention architecture with cascade token and head pruning. H Wang, Z Zhang, S Han, 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA. IEEE</p>
<p>Sparse modular activation for efficient sequence modeling. L Ren, Y Liu, S Wang, Y Xu, C Zhu, C Zhai, arXiv:2306.111972023arXiv preprint</p>
<p>Dynamic context pruning for efficient and interpretable autoregressive transformers. S Anagnostidis, D Pavllo, L Biggio, L Noci, A Lucchi, T Hoffmann, arXiv:2305.158052023arXiv preprint</p>
<p>Reformer: The efficient transformer. N Kitaev, ≈Å Kaiser, A Levskaya, arXiv:2001.044512020arXiv preprint</p>
<p>Faster causal attention over large sequences through sparse flash attention. M Pagliardini, D Paliotta, M Jaggi, F Fleuret, arXiv:2306.011602023arXiv preprint</p>
<p>Efficient contentbased sparse attention with routing transformers. A Roy, M Saffar, A Vaswani, D Grangier, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Sparse sinkhorn attention. Y Tay, D Bahri, L Yang, D Metzler, D.-C Juan, International Conference on Machine Learning. PMLR2020</p>
<p>H2o: Heavy-hitter oracle for efficient generative inference of large language models. Z Zhang, Y Sheng, T Zhou, T Chen, L Zheng, R Cai, Z Song, Y Tian, C R√©, C Barrett, Advances in Neural Information Processing Systems. 202436</p>
<p>Diffuser: efficient transformers with multi-hop attention diffusion for long sequences. A Feng, I Li, Y Jiang, R Ying, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337780</p>
<p>Sparsegpt: Massive language models can be accurately pruned in one-shot. E Frantar, D Alistarh, 2023</p>
<p>A simple and effective pruning approach for large language models. M Sun, Z Liu, A Bair, J Z Kolter, arXiv:2306.116952023arXiv preprint</p>
<p>One-shot sensitivity-aware mixed sparsity pruning for large language models. H Shao, B Liu, Y Qian, arXiv:2310.094992023arXiv preprint</p>
<p>Prune and tune: Improving efficient pruning techniques for massive language models. A Syed, P H Guo, V Sundarapandiyan, 2023</p>
<p>Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. X Wei, Y Zhang, Y Li, X Zhang, R Gong, J Guo, X Liu, arXiv:2304.091452023arXiv preprint</p>
<p>Besa: Pruning large language models with blockwise parameter-efficient sparsity allocation. P Xu, W Shao, M Chen, S Tang, K Zhang, P Gao, F An, Y Qiao, P Luo, The Twelfth International Conference on Learning Representations. 2023</p>
<p>The optimal bert surgeon: Scalable and accurate second-order pruning for large language models. E Kurtic, D Campos, T Nguyen, E Frantar, M Kurtz, B Fineran, M Goin, D Alistarh, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>A fast post-training pruning framework for transformers. W Kwon, S Kim, M W Mahoney, J Hassoun, K Keutzer, A Gholami, Advances in Neural Information Processing Systems. 202235</p>
<p>An efficient plug-and-play post-training pruning strategy in large language models. Y Zhang, H Bai, H Lin, J Zhao, L Hou, C V Cannistraci, 2023</p>
<p>Llm-pruner: On the structural pruning of large language models. X Ma, G Fang, X Wang, Advances in neural information processing systems. 202436</p>
<p>Sheared llama: Accelerating language model pre-training via structured pruning. M Xia, T Gao, Z Zeng, D Chen, arXiv:2310.066942023arXiv preprint</p>
<p>Ziplm: Inference-aware structured pruning of language models. E Kurtiƒá, E Frantar, D Alistarh, Advances in Neural Information Processing Systems. 202436</p>
<p>Loraprune: Pruning meets low-rank parameterefficient fine-tuning. M Zhang, H Chen, C Shen, Z Yang, L Ou, X Yu, B Zhuang, 2023</p>
<p>Lorashear: Efficient large language model structured pruning and knowledge recovery. T Chen, T Ding, B Yadav, I Zharkov, L Liang, arXiv:2310.183562023arXiv preprint</p>
<p>S Ashkboos, M L Croci, M G D Nascimento, T Hoefler, J Hensman, arXiv:2401.15024Slicegpt: Compress large language models by deleting rows and columns. 2024arXiv preprint</p>
<p>Platon: Pruning large transformer models with upper confidence bound of weight importance. Q Zhang, S Zuo, C Liang, A Bukharin, P He, W Chen, T Zhao, International Conference on Machine Learning. PMLR2022823</p>
<p>Structured pruning learns compact and accurate models. M Xia, Z Zhong, D Chen, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Structured pruning for efficient generative pre-trained language models. C Tao, L Hou, H Bai, J Wei, X Jiang, Q Liu, P Luo, N Wong, Findings of the Association for Computational Linguistics: ACL 2023. 202310895</p>
<p>Not all experts are equal: Efficient expert pruning and skipping for mixture-of-experts large language models. X Lu, Q Liu, Y Xu, A Zhou, S Huang, B Zhang, J Yan, H Li, arXiv:2402.148002024arXiv preprint</p>
<p>Seer-moe: Sparse expert efficiency through regularization for mixture-of-experts. A Muzio, A Sun, C He, arXiv:2404.050892024arXiv preprint</p>
<p>Pruner-zero: Evolving symbolic pruning metric from scratch for large language models. P Dong, L Li, Z Tang, X Liu, X Pan, Q Wang, X Chu, International Conference on Machine Learning (ICML). 2024</p>
<p>Dynamic sparse no training: Training-free finetuning for sparse llms. Y Zhang, L Zhao, M Lin, S Yunyun, Y Yao, X Han, J Tanner, S Liu, R Ji, International Conference on Learning Representations (ICLR). 2024</p>
<p>Llm-fp4: 4-bit floating-point quantized transformers. S.-Y Liu, Z Liu, X Huang, P Dong, K.-T Cheng, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Norm tweaking: Highperformance low-bit quantization of large language models. L Li, Q Li, B Zhang, X Chu, arXiv:2309.027842023arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, Advances in Neural Information Processing Systems. 202436</p>
<p>Qa-lora: Quantization-aware lowrank adaptation of large language models. Y Xu, L Xie, X Gu, X Chen, H Chang, H Zhang, Z Chen, X Zhang, Q Tian, arXiv:2309.147172023arXiv preprint</p>
<p>Loftq: Lora-fine-tuning-aware quantization for large language models. Y Li, Y Yu, C Liang, P He, N Karampatziakis, W Chen, T Zhao, arXiv:2310.086592023arXiv preprint</p>
<p>Gptq: Accurate post-training quantization for generative pre-trained transformers. E Frantar, S Ashkboos, T Hoefler, D Alistarh, arXiv:2210.173232022arXiv preprint</p>
<p>Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models. G Park, M Kim, S Lee, J Kim, B Kwon, S J Kwon, B Kim, Y Lee, D Lee, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Awq: Activation-aware weight quantization for llm compression and acceleration. J Lin, J Tang, H Tang, S Yang, X Dang, S Han, arXiv:2306.009782023arXiv preprint</p>
<p>Owq: Lessons learned from activation outliers for weight quantization in large language models. C Lee, J Jin, T Kim, H Kim, E Park, arXiv:2306.022722023arXiv preprint</p>
<p>Spqr: A sparse-quantized representation for near-lossless llm weight compression. T Dettmers, R Svirschevski, V Egiazarian, D Kuznedelev, E Frantar, S Ashkboos, A Borzunov, T Hoefler, D Alistarh, arXiv:2306.030782023arXiv preprint</p>
<p>Squeezellm: Dense-and-sparse quantization. S Kim, C Hooper, A Gholami, Z Dong, X Li, S Shen, M W Mahoney, K Keutzer, arXiv:2306.076292023arXiv preprint</p>
<p>Quip: 2-bit quantization of large language models with guarantees. J Chee, Y Cai, V Kuleshov, C De Sa, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Finequant: Unlocking efficiency with fine-grained weight-only quantization for llms. Y J Kim, R Henry, R Fahim, H H Awadalla, arXiv:2308.097232023arXiv preprint</p>
<p>Quantease: Optimization-based quantization for language models-an efficient and intuitive algorithm. K Behdin, A Acharya, A Gupta, S Keerthi, R Mazumder, arXiv:2309.018852023arXiv preprint</p>
<p>Llm-mq: Mixed-precision quantization for efficient llm deployment. S Li, X Ning, K Hong, T Liu, L Wang, X Li, K Zhong, G Dai, H Yang, Y Wang, 2023</p>
<p>Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Z Yao, R Y Aminabadi, M Zhang, X Wu, C Li, Y He, Advances in Neural Information Processing Systems. 2022</p>
<p>Flexgen: High-throughput generative inference of large language models with a single gpu. Y Sheng, L Zheng, B Yuan, Z Li, M Ryabinin, B Chen, P Liang, C Re, I Stoica, C Zhang, 2023</p>
<p>-bit matrix multiplication for transformers at scale. T Dettmers, M Lewis, Y Belkada, L Zettlemoyer, arXiv:2208.07339Llm. 82022arXiv preprintint8 (</p>
<p>Smoothquant: Accurate and efficient post-training quantization for large language models. G Xiao, J Lin, M Seznec, H Wu, J Demouth, S Han, International Conference on Machine Learning. PMLR20233899</p>
<p>Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation. Z Yao, X Wu, C Li, S Youn, Y He, arXiv:2303.083022023arXiv preprint</p>
<p>Rptq: Reorder-based post-training quantization for large language models. Z Yuan, L Niu, J Liu, W Liu, X Wang, Y Shang, G Sun, Q Wu, J Wu, B Wu, arXiv:2304.010892023arXiv preprint</p>
<p>Olive: Accelerating large language models via hardware-friendly outlier-victim pair quantization. C Guo, J Tang, W Hu, J Leng, C Zhang, F Yang, Y Liu, M Guo, Y Zhu, Proceedings of the 50th Annual International Symposium on Computer Architecture. the 50th Annual International Symposium on Computer Architecture2023</p>
<p>Zeroquant-fp: A leap forward in llms post-training w4a8 quantization using floating-point formats. X Wu, Z Yao, Y He, arXiv:2307.097822023arXiv preprint</p>
<p>Omniquant: Omnidirectionally calibrated quantization for large language models. W Shao, M Chen, Z Zhang, P Xu, L Zhao, Z Li, K Zhang, P Gao, Y Qiao, P Luo, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Qllm: Accurate and efficient low-bitwidth quantization for large language models. J Liu, R Gong, X Wei, Z Dong, J Cai, B Zhuang, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Atom: Low-bit quantization for efficient and accurate llm serving. Y Zhao, C.-Y Lin, K Zhu, Z Ye, L Chen, S Zheng, L Ceze, A Krishnamurthy, T Chen, B Kasikci, arXiv:2310.191022023arXiv preprint</p>
<p>Billm: Pushing the limit of post-training quantization for llms. W Huang, Y Liu, H Qin, Y Li, S Zhang, X Liu, M Magno, X Qi, 2024</p>
<p>Evaluating quantized large language models. S Li, X Ning, L Wang, T Liu, X Shi, S Yan, G Dai, H Yang, Y Wang, arXiv:2402.181582024arXiv preprint</p>
<p>Affinequant: Affine transformation quantization for large language models. Y Ma, H Li, X Zheng, F Ling, X Xiao, R Wang, S Wen, F Chao, R Ji, International Conference on Learning Representations (ICLR). 2024</p>
<p>A Tseng, J Chee, Q Sun, V Kuleshov, C De Sa, arXiv:2402.04396Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. 2024arXiv preprint</p>
<p>Quarot: Outlier-free 4-bit inference in rotated llms. S Ashkboos, A Mohtashami, M L Croci, B Li, M Jaggi, D Alistarh, T Hoefler, J Hensman, arXiv:2404.004562024arXiv preprint</p>
<p>Spinquant-llm quantization with learned rotations. Z Liu, C Zhao, I Fedorov, B Soran, D Choudhary, R Krishnamoorthi, V Chandra, Y Tian, T Blankevoort, arXiv:2405.164062024arXiv preprint</p>
<p>Kvquant: Towards 10 million context length llm inference with kv cache quantization. C Hooper, S Kim, H Mohammadzadeh, M W Mahoney, Y S Shao, K Keutzer, A Gholami, arXiv:2401.180792024arXiv preprint</p>
<p>Kivi: A tuning-free asymmetric 2bit quantization for kv cache. Z Liu, J Yuan, H Jin, S Zhong, Z Xu, V Braverman, B Chen, X Hu, arXiv:2402.027502024arXiv preprint</p>
<p>Optimal brain compression: A framework for accurate post-training quantization and pruning. E Frantar, D Alistarh, Advances in Neural Information Processing Systems. 2022</p>
<p>Optimizing inference on large language models with nvidia tensorrt-llm, now publicly available. N Vaidya, F Oh, N Comly, 2023</p>
<p>Lmdeploy. Internlm, 2024</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.096852021arXiv preprint</p>
<p>Optimal brain surgeon and general network pruning. B Hassibi, D G Stork, G J Wolff, 1993IEEE</p>
<p>Optimal brain damage. Y Lecun, J Denker, S Solla, Advances in neural information processing systems. 19892</p>
<p>Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. D C Mocanu, E Mocanu, P Stone, P H Nguyen, M Gibescu, A Liotta, Nature communications. 9123832018</p>
<p>Neural architecture search with reinforcement learning. B Zoph, Q Le, International Conference on Learning Representations. 2016</p>
<p>Svd-llm: Truncationaware singular value decomposition for large language model compression. X Wang, Y Zheng, Z Wan, M Zhang, arXiv:2403.073782024arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, 2022. 202213</p>
<p>Dynamic neural networks: A survey. Y Han, G Huang, S Song, L Yang, H Wang, Y Wang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 44112021</p>
<p>A survey on dynamic neural networks for natural language processing. C Xu, J Mcauley, Findings of the Association for Computational Linguistics: EACL 2023. 2023</p>
<p>Magic pyramid: Accelerating inference with early exiting and token pruning. X He, I Keivanloo, Y Xu, X He, B Zeng, S Rajagopalan, T Chilimbi, Image. 2023</p>
<p>Paving the way to efficient architectures: Stripedhyena-7b, open source models offering a glimpse into a world beyond transformers. Togetherai, December 2023</p>
<p>Compressing llms: The truth is rarely pure and never simple. A Jaiswal, Z Gan, X Du, B Zhang, Z Wang, Y Yang, arXiv:2310.013822023arXiv preprint</p>
<p>Fast inference from transformers via speculative decoding. Y Leviathan, M Kalman, Y Matias, International Conference on Machine Learning. PMLR202319286</p>
<p>Accelerating large language model decoding with speculative sampling. C Chen, S Borgeaud, G Irving, J.-B Lespiau, L Sifre, J Jumper, arXiv:2302.013182023arXiv preprint</p>
<p>Distillspec: Improving speculative decoding via knowledge distillation. Y Zhou, K Lyu, A S Rawat, A K Menon, A Rostamizadeh, S Kumar, J.-F Kagy, R Agarwal, arXiv:2310.084612023arXiv preprint</p>
<p>Draft &amp; verify: Lossless large language model acceleration via self-speculative decoding. J Zhang, J Wang, H Li, L Shou, K Chen, G Chen, S Mehrotra, arXiv:2309.081682023arXiv preprint</p>
<p>Online speculative decoding. X Liu, L Hu, P Bailis, I Stoica, Z Deng, A Cheung, H Zhang, arXiv:2310.071772023arXiv preprint</p>
<p>Pass: Parallel speculative sampling. G Monea, A Joulin, E Grave, arXiv:2311.135812023arXiv preprint</p>
<p>Rest: Retrievalbased speculative decoding. Z He, Z Zhong, T Cai, J D Lee, D He, arXiv:2311.082522023arXiv preprint</p>
<p>Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. X Miao, G Oliaro, Z Zhang, X Cheng, Z Wang, R Y Y Wong, Z Chen, D Arfeen, R Abhyankar, Z Jia, arXiv:2305.097812023arXiv preprint</p>
<p>Accelerating llm inference with staged speculative decoding. B Spector, C Re, arXiv:2308.046232023arXiv preprint</p>
<p>Cascade speculative drafting for even faster llm inference. Z Chen, X Yang, J Lin, C Sun, J Huang, K C , -C Chang, arXiv:2312.114622023arXiv preprint</p>
<p>Breaking the sequential dependency of llm inference using lookahead decoding. Y Fu, P Bailis, I Stoica, H Zhang, November 2023</p>
<p>Eagle: Lossless acceleration of llm decoding by feature extrapolation. Y Li, C Zhang, H Zhang, December 2023</p>
<p>Z Sun, A T Suresh, J H Ro, A Beirami, H Jain, F Yu, arXiv:2310.15141Spectr: Fast speculative decoding via optimal transport. 2023arXiv preprint</p>
<p>Kangaroo: Lossless self-speculative decoding via double early exiting. F Liu, Y Tang, Z Liu, Y Ni, K Han, Y Wang, arXiv:2404.189112024arXiv preprint</p>
<p>Inference of meta's llama model (and others) in pure c/c++. 2024</p>
<p>Powerinfer: Fast large language model serving with a consumer-grade gpu. Y Song, Z Mi, H Xie, H Chen, arXiv:2312.124562023arXiv preprint</p>
<p>Fastdecode: High-throughput gpu-efficient llm serving using heterogeneous pipelines. J He, J Zhai, arXiv:2403.114212024arXiv preprint</p>
<p>K Hong, G Dai, J Xu, Q Mao, X Li, J Liu, K Chen, Y Dong, Y Wang, Flashdecoding++: Faster large language model inference on gpus. 2024</p>
<p>Megablocks: Efficient sparse training with mixture-of-experts. T Gale, D Narayanan, C Young, M Zaharia, Proceedings of Machine Learning and Systems (MLSys). Machine Learning and Systems (MLSys)2023</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. T Dao, D Fu, S Ermon, A Rudra, C R√©, Advances in Neural Information Processing Systems. 202235</p>
<p>Flashattention-2: Faster attention with better parallelism and work partitioning. T Dao, arXiv:2307.086912023arXiv preprint</p>
<p>Bytetransformer: A high-performance transformer boosted for variable-length inputs. Y Zhai, C Jiang, L Wang, X Jia, S Zhang, Z Chen, X Liu, Y Zhu, 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS). </p>
<p>Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. R Y Aminabadi, S Rajbhandari, A A Awan, C Li, D Li, E Zheng, O Ruwase, S Smith, M Zhang, J Rasley, SC22: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2022</p>
<p>Flash-decoding for long-context inference. T Dao, D Haziza, F Massa, G Sizov, 2023</p>
<p>Transformers: State-of-the-art machine learning for pytorch, tensorflow, and jax. Huggingface, 2024</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozi√®re, N Goyal, E Hambro, F Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Openppl: A high-performance deep learning inference platform. Sensetime, 2023</p>
<p>cublas: Basic linear algebra on nvidia gpus. NVIDIA. 2017</p>
<p>Cutlass: Cuda templates for linear algebra subroutines. 2017</p>
<p>Fastgemv: High-speed gemv kernels. S Wang, 2023</p>
<p>Triton: an intermediate language and compiler for tiled neural network computations. P Tillet, H T Kung, D Cox, Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages. the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages2019</p>
<p>Beyond the speculative game: A survey of speculative execution in large language models. C Zhang, arXiv:2404.148972024arXiv preprint</p>
<p>Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. H Xia, arXiv:2401.078512024arXiv preprint</p>
<p>Blockwise parallel decoding for deep autoregressive models. M Stern, N Shazeer, J Uszkoreit, Advances in Neural Information Processing Systems. 201831</p>
<p>Rectified linear units improve restricted boltzmann machines. V Nair, G E Hinton, Proceedings of the 27th international conference on machine learning (ICML-10). the 27th international conference on machine learning (ICML-10)2010</p>
<p>Splitwise: Efficient generative llm inference using phase splitting. P Patel, E Choukse, C Zhang, A √ëigo Goiri, S Shah, R Maleki, Bianchini, arXiv:2311.186772023arXiv preprint</p>
<p>Inference without interference: Disaggregate llm inference for mixed downstream workloads. C Hu, H Huang, L Xu, X Chen, J Xu, S Chen, H Feng, C Wang, S Wang, Y Bao, N Sun, Y Shan, arXiv:2401.111812024arXiv preprint</p>
<p>Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving. Y Zhong, S Liu, J Chen, J Hu, Y Zhu, X Liu, X Jin, H Zhang, arXiv:2401.096702024arXiv preprint</p>
<p>X Miao, C Shi, J Duan, X Xi, D Lin, B Cui, Z Jia, arXiv:2311.15566Spotserve: Serving generative large language models on preemptible instances. 2023arXiv preprint</p>
<p>Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache. B Lin, T Peng, C Zhang, M Sun, L Li, H Zhao, W Xiao, Q Xu, X Qiu, S Li, Z Ji, Y Li, W Lin, arXiv:2401.026692024arXiv preprint</p>
<p>Orca: A distributed serving system for transformer-based generative models. G.-I Yu, J S Jeong, G.-W Kim, S Kim, B.-G Chun, Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation. the 16th USENIX Symposium on Operating Systems Design and Implementation2022</p>
<p>Lightllm. Modeltc, February 2024</p>
<p>Deepspeed-fastgen: High-throughput text generation for llms via mii and deepspeed-inference. C Holmes, M Tanaka, M Wyatt, A A Awan, J Rasley, S Rajbhandari, R Y Aminabadi, H Qin, A Bakhtiari, L Kurilenko, Y He, arXiv:2401.086712024arXiv preprint</p>
<p>Fast distributed inference serving for large language models. B Wu, Y Zhong, Z Zhang, G Huang, X Liu, X Jin, arXiv:2305.059202023arXiv preprint</p>
<p>Y Sheng, S Cao, D Li, B Zhu, Z Li, D Zhuo, arXiv:2401.00588Fairness in serving large language models. 2024arXiv preprint</p>
<p>Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills. A Agrawal, A Panwar, J Mohan, N Kwatra, B Gulavani, R Ramjee, arXiv:2308.163692023arXiv preprint</p>
<p>Taming throughputlatency tradeoff in llm inference with sarathi-serve. A Agrawal, N Kedia, A Panwar, J Mohan, N Kwatra, B S Gulavani, A Tumanov, R Ramjee, arXiv:2403.023102024arXiv preprint</p>
<p>S 3 : Increasing gpu utilization during generative inference for higher throughput. Y Jin, C.-F Wu, D Brooks, G.-Y Wei, arXiv:2306.060002023arXiv preprint</p>
<p>. Z Ye, Flashinfer, March 2024</p>
<p>Fastertransformer: About transformer related optimization, including bert, gpt. NVIDIA. 2017</p>
<p>Exegpt: Constraint-aware resource scheduling for llm inference. H Oh, K Kim, J Kim, S Kim, J Lee, D.-S Chang, J Seo, Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems20242</p>
<p>Llumnix: Dynamic scheduling for large language model serving. B Sun, Z Huang, H Zhao, W Xiao, X Zhang, Y Li, W Lin, arXiv:2406.032432024arXiv preprint</p>
<p>Loongserve: Efficiently serving long-context large language models with elastic sequence parallelism. B Wu, S Liu, Y Zhong, P Sun, X Liu, X Jin, arXiv:2404.095262024arXiv preprint</p>
<p>B Li, S Pandey, H Fang, Y Lyv, J Li, J Chen, M Xie, L Wan, H Liu, C Ding, arXiv:2007.08563Ftrans: Energy-efficient acceleration of transformers using fpga. 2020arXiv preprint</p>
<p>Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks. T J Ham, Y Lee, S H Seo, S Kim, H Choi, S J Jun, J W Lee, ACM/IEEE 48th Annual International Symposium on Computer Architecture. 2021</p>
<p>Adaptable butterfly accelerator for attention-based nns via hardware and algorithm co-design. H Fan, T Chau, S I Venieris, R Lee, A Kouris, W Luk, N D Lane, M S Abdelfattah, IEEE/ACM International Symposium on Microarchitecture. 2022</p>
<p>Fact: Ffn-attention co-optimized transformer architecture with eager correlation prediction. Y Qin, Y Wang, D Deng, Z Zhao, X Yang, L Liu, S Wei, Y Hu, S Yin, Proceedings of the 50th Annual International Symposium on Computer Architecture. the 50th Annual International Symposium on Computer Architecture2023</p>
<p>Understanding the potential of fpga-based spatial acceleration for large language model inference. H Chen, J Zhang, Y Du, S Xiang, Z Yue, N Zhang, Y Cai, Z Zhang, arXiv:2312.151592023arXiv preprint</p>
<p>Dfx: A low-latency multi-fpga appliance for accelerating transformer-based text generation. S Hong, S Moon, J Kim, S Lee, M Kim, D Lee, J.-Y Kim, IEEE Hot Chips 34 Symposium. 2022</p>
<p>S Zeng, J Liu, G Dai, X Yang, T Fu, H Wang, W Ma, H Sun, S Li, Z Huang, arXiv:2401.03868Flightllm: Efficient large language model inference with a complete mapping flow on fpga. 2024arXiv preprint</p>
<p>Sharegpt. S Teams, 2023</p>
<p>Large multimodal agents: A survey. J Xie, Z Chen, R Zhang, X Wan, G Li, arXiv:2402.151162024arXiv preprint</p>
<p>Exploring the relationship between model architecture and in-context learning ability. I Lee, N Jiang, T Berg-Kirkpatrick, arXiv:2310.080492023arXiv preprint</p>
<p>Pythia: A suite for analyzing large language models across training and scaling. S Biderman, H Schoelkopf, Q G Anthony, H Bradley, K O'brien, E Hallahan, M A Khan, S Purohit, U S Prashanth, E Raff, International Conference on Machine Learning. PMLR2023</p>
<p>J Bai, S Bai, Y Chu, Z Cui, K Dang, X Deng, Y Fan, W Ge, Y Han, F Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>Rethinking optimization and architecture for tiny language models. Y Tang, F Liu, Y Ni, Y Tian, Z Bai, Y.-Q Hu, S Liu, S Jui, K Han, Y Wang, arXiv:2402.027912024arXiv preprint</p>
<p>Y Li, S Bubeck, R Eldan, A Del Giorno, S Gunasekar, Y T Lee, arXiv:2309.05463Textbooks are all you need ii: phi-1.5 technical report. 2023arXiv preprint</p>
<p>Textbooks are all you need. S Gunasekar, Y Zhang, J Aneja, C C T Mendes, A Del Giorno, S Gopi, M Javaheripi, P Kauffmann, G De Rosa, O Saarikivi, arXiv:2306.116442023arXiv preprint</p>
<p>Tinyllama: An opensource small language model. P Zhang, G Zeng, T Wang, W Lu, arXiv:2401.023852024arXiv preprint</p>
<p>Towards the law of capacity gap in distilling language models. C Zhang, D Song, Z Ye, Y Gao, arXiv:2311.070522023arXiv preprint</p>
<p>Openllama: An open reproduction of llama. X Geng, H Liu, May 2023</p>
<p>M Bellagente, J Tow, D Mahan, D Phung, M Zhuravinskyi, R Adithyan, J Baicoianu, B Brooks, N Cooper, A Datta, arXiv:2402.17834Stable lm 2 1.6 b technical report. 2024arXiv preprint</p>
<p>Minicpm: Unveiling the potential of end-side large language models. 2024</p>
<p>Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. Z Liu, C Zhao, F Iandola, C Lai, Y Tian, I Fedorov, Y Xiong, E Chang, Y Shi, R Krishnamoorthi, arXiv:2402.149052024arXiv preprint</p>
<p>MLC-LLM. M Team, 2023</p>
<p>A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. Y Yao, J Duan, K Xu, Y Cai, Z Sun, Y Zhang, High-Confidence Computing. 2024100211</p>
<p>Y Li, H Wen, W Wang, X Li, Y Yuan, G Liu, J Liu, W Xu, X Wang, Y Sun, arXiv:2401.05459Personal llm agents: Insights and survey about the capability, efficiency and security. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6954 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6954</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6954</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-272146189</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.16326v3.pdf" target="_blank">Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic</a></p>
                <p><strong>Paper Abstract:</strong> Self-critic has become a crucial mechanism for enhancing the reasoning performance of LLMs. However, current approaches mainly involve basic prompts for intuitive instance-level feedback, which resembles System-1 processes and limits the reasoning capabilities. Moreover, there is a lack of in-depth investigations into the relationship between LLM’s ability to criticize and its task-solving performance. To address these issues, we propose Critic-CoT, a novel framework that pushes LLMs toward System-2-like critic capability. Through a step-wise CoT reasoning paradigm and the automatic construction of weak-supervision data without human annotation, Critic-CoT enables LLMs to engage in slow, analytic self-critique and refinement, thereby improving their reasoning abilities. Experiments on GSM8K and MATH and out-of-domain evaluation demonstrate that our enhanced model significantly boosts task-solving performance by filtering out invalid solutions or iterative refinement. Furthermore, we investigate the intrinsic correlation between critique and task-solving abilities within LLMs, discovering that these abilities can mutually reinforce each other rather than conflict. 1</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6954.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6954.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critic-CoT (training effect) - GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critic-CoT: Step-wise Chain-of-Thought Critic training (effect on GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage weakly supervised training pipeline that teaches an LLM to produce step-wise Chain-of-Thought (CoT) critiques and step-level refinements, then fine-tunes the model to be a critic+refiner; evaluated on GSM8K showing improved top-1 reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following variant of LLaMA-3 pre-trained on large token corpora and further fine-tuned; used as base model and fine-tuned into a critic-refine model via Critic-CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Critic-CoT (step-wise Chain-of-Thought critique + refinement training)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train the model to (a) label each reasoning step as correct/incorrect (step-wise labels), (b) produce a natural-language CoT critique that pinpoints the first incorrect step, and (c) generate a refined continuation starting from that step; data constructed via weak supervision using GPT-4-Turbo and self-critic sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>self-critique with trained critic; used with generate-then-reflect at inference</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>16</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems (short multi-step math reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Top-1 Accuracy (Acc); also Critic precision/recall/F1 and Critic Accuracy (process-level correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>89.6% Top-1 accuracy (baseline before Critic-CoT fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>91.7% Top-1 accuracy after Critic-CoT fine-tuning (stage-1+stage-2 training); critic accuracy (process discrimination) reported ≈92.3% (precision/recall/F1 reported in ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Training-data construction depends on verifiable final answers (math); weak-supervision uses GPT-4-Turbo which can bias the data; critic sometimes fails to find an error in wrong attempts (necessitating retries up to K=16); improvement magnitude depends on dataset difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6954.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6954.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critic-CoT (training effect) - MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critic-CoT: Step-wise Chain-of-Thought Critic training (effect on MATH/MATH500)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same Critic-CoT training pipeline applied to more challenging symbolic/math problems (MATH); produces larger absolute improvement but harder refinement dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following LLaMA-3 70B fine-tuned with Critic-CoT data constructed primarily from GPT-4 outputs and self-critic distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Critic-CoT (step-wise Chain-of-Thought critique + refinement training)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two-stage training: (1) collect critic/refine pairs using an external critic (GPT-4-Turbo) and generator samples; (2) let the learned critic criticize/refine its own outputs to generate additional data; fine-tune to produce stepwise CoT critiques and refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>self-critique with trained critic; generate-then-reflect available at inference</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>16</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH / MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Advanced math competition-style problems requiring long multi-step symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Top-1 Accuracy (Acc); Critic precision/recall/F1 and Critic Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>50.4% Top-1 accuracy (baseline before Critic-CoT fine-tuning reported in paper's main table)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>57.6% Top-1 accuracy after Critic-CoT fine-tuning; critic process-accuracy reported ≈72.2% for Critic-CoT on MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smaller iterative-refinement gains on harder math problems; data construction required additional hinting for GPT-4 to produce usable critiques in stage-1; higher noise in process labels due to complex solutions; reliance on gold-answer verifiability limits generalization to tasks without deterministic final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6954.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6954.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative Refinement (inference) - GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Refinement using trained Critic-CoT critic (inference-time generate->critique->refine loops) on GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>At inference, the trained critic examines a generated solution step-wise and, if it finds an error, requests a refinement starting from the first incorrect step; this process repeats until the critic validates the solution or a max depth is reached, improving final answer accuracy on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B-Instruct (Critic-CoT fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned Llama-3 70B that can both generate CoT solutions and produce stepwise critiques and refinements at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative Refinement (Critic-CoT inference)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Single-sample generate-then-reflect loop: generate one solution, critic labels steps, if an error is found refine starting at first incorrect step; repeat until critic accepts or maximum refine depth reached.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (recursive iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>8</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Top-1 Accuracy (Refine-Acc reported; rounds tracked as Round 0..7), average tokens and latency per round</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>91.7% (accuracy at Round 0 / initial single-shot after Critic-CoT fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>93.3% accuracy by the 7th refinement round (reported Round 7 result); overall Iterative Refinement final reported accuracy 93.3% on GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Risk of error accumulation across refinement rounds (True→False edits); diminishing returns after several rounds; increased token usage and latency with more rounds; to avoid de-generation they limit max refine depth to d=8 and restart policy; iterative refinement is less effective than majority-vote filtering in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6954.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6954.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative Refinement (inference) - MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Refinement using trained Critic-CoT critic (inference-time) on MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same iterative refinement pipeline applied to harder MATH problems; refinement yields only marginal gains and tends to converge quickly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B-Instruct (Critic-CoT fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned Llama-3 70B critic+refiner model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative Refinement (Critic-CoT inference)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate a solution, critic examines steps, refine from first detected error, repeat until critic accepts or maximum depth reached (d=8).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (recursive iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>8</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Challenging contest-style mathematics requiring long derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Top-1 Accuracy (Refine-Acc), per-round accuracy and cost metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>57.6% initial accuracy (Round 0 after Critic-CoT training)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>57.8% by Round 7 (marginal gain); final reported Iterative Refinement accuracy 57.8% on MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Very small net improvement (near plateau); critic/refinement pipeline more sensitive to error accumulation on longer, harder traces; refinement often converges without meaningful corrections on very hard problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6954.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6954.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critic-as-Filter + Majority Vote - GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critic as Filter: filter generated candidates by step-wise critic then majority-vote (Critic+Maj)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate many candidate solutions, run the trained Critic-CoT model to label steps and filter out any candidate with a detected error, then take majority vote among the filtered set; yields higher accuracy than naive majority vote.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B-Instruct (Critic-CoT fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned Llama-3 70B used both to generate candidate solutions and to perform stepwise critique/filtering; majority vote taken over filtered candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Critic-as-Filter + Majority Vote</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample m candidate solutions (paper uses N samples e.g., 96 for GSM8K), run critic on each to obtain per-step labels, discard any sample where critic finds an error, then perform majority vote on remaining candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples with critic-based filtering</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Majority Vote Accuracy (Maj1@N), Pass@N and Critic+Maj1@N</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Maj1@96 = 94.8% (vanilla majority vote with 96 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Critic + Maj1@96 = 95.4% (after critic-based filtering then majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Computational cost grows linearly with number of samples; requires large sampling budgets to beat single-sample refinement; relies on critic precision—incorrectly filtering correct answers reduces effective sample diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6954.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6954.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critic-as-Filter + Majority Vote - MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critic as Filter: filter generated candidates by step-wise critic then majority-vote (effect on MATH500)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying critic-based filtering before majority voting yields larger gains on harder math problems (MATH500) compared to iterative refinement alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B-Instruct (Critic-CoT fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned 70B model performing candidate generation and step-wise criticism for filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Critic-as-Filter + Majority Vote</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Filter out candidates flagged as incorrect by trained critic, then perform majority vote among the remaining candidates (paper reports Maj@512 and Critic+Maj@512 values).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples with critic-based filtering</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Advanced math problems requiring extended reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Majority Vote Accuracy (Maj1@512), Critic+Maj1@512</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Maj1@512 = 65.4% (vanilla majority vote with 512 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Critic + Maj1@512 = 68.4% (after critic-based filtering then majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High sampling costs (512 samples used); if critic has imperfect recall/precision it can either pass wrong solutions or filter out correct ones; relies on massive compute to outperform single-sample iterative refinement in some regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6954.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6954.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative Refinement (code) - HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Refinement applied to code generation (HumanEval) using Critic-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Apply the trained critic+refiner to code generation problems by iteratively critiquing and refining generated solutions; majority-vote filtering not applicable for HumanEval but iterative refinement still provides gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B-Instruct (Critic-CoT fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned LLaMA-3 70B model adapted to produce code, critique code-line/step correctness, and emit refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative Refinement (Critic-CoT inference) for code</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate a candidate program, critic inspects stepwise/program behaviors in natural language, request refinement when faults are detected, repeat until critic accepts or max depth reached.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>8</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code-generation benchmark of Python programming problems (functional unit tests determine pass/fail).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pass@1 (single-sample pass rate) / pass-rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Base model pass rate reported as 76.2% (pre-Critic-CoT baseline for the evaluated setup)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>After Critic-CoT training and iterative refinement: 78.1% (model trained on GSM8K) and 84.8% (model trained on MATH) reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Majority-vote filtering not applicable to code evaluation in their reported setup; critic must reason about program correctness which may be more complex than verifiable math answers; iterative refinement can increase latency and still fail when critic or refiner misdiagnoses faults.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Self-RAG: Learning to retrieve, generate, and critique through self-reflection <em>(Rating: 1)</em></li>
                <li>Mirror: Multiple-perspective selfreflection method for knowledge-rich reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6954",
    "paper_id": "paper-272146189",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "Critic-CoT (training effect) - GSM8K",
            "name_full": "Critic-CoT: Step-wise Chain-of-Thought Critic training (effect on GSM8K)",
            "brief_description": "A two-stage weakly supervised training pipeline that teaches an LLM to produce step-wise Chain-of-Thought (CoT) critiques and step-level refinements, then fine-tunes the model to be a critic+refiner; evaluated on GSM8K showing improved top-1 reasoning accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B-Instruct",
            "model_description": "Instruction-following variant of LLaMA-3 pre-trained on large token corpora and further fine-tuned; used as base model and fine-tuned into a critic-refine model via Critic-CoT.",
            "model_size": "70B",
            "reflection_method_name": "Critic-CoT (step-wise Chain-of-Thought critique + refinement training)",
            "reflection_method_description": "Train the model to (a) label each reasoning step as correct/incorrect (step-wise labels), (b) produce a natural-language CoT critique that pinpoints the first incorrect step, and (c) generate a refined continuation starting from that step; data constructed via weak supervision using GPT-4-Turbo and self-critic sampling.",
            "iteration_type": "self-critique with trained critic; used with generate-then-reflect at inference",
            "num_iterations": 16,
            "task_name": "GSM8K",
            "task_description": "Grade-school math word problems (short multi-step math reasoning).",
            "evaluation_metric": "Top-1 Accuracy (Acc); also Critic precision/recall/F1 and Critic Accuracy (process-level correctness)",
            "performance_before_reflection": "89.6% Top-1 accuracy (baseline before Critic-CoT fine-tuning)",
            "performance_after_reflection": "91.7% Top-1 accuracy after Critic-CoT fine-tuning (stage-1+stage-2 training); critic accuracy (process discrimination) reported ≈92.3% (precision/recall/F1 reported in ablations)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Training-data construction depends on verifiable final answers (math); weak-supervision uses GPT-4-Turbo which can bias the data; critic sometimes fails to find an error in wrong attempts (necessitating retries up to K=16); improvement magnitude depends on dataset difficulty.",
            "uuid": "e6954.0",
            "source_info": {
                "paper_title": "Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Critic-CoT (training effect) - MATH500",
            "name_full": "Critic-CoT: Step-wise Chain-of-Thought Critic training (effect on MATH/MATH500)",
            "brief_description": "Same Critic-CoT training pipeline applied to more challenging symbolic/math problems (MATH); produces larger absolute improvement but harder refinement dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B-Instruct",
            "model_description": "Instruction-following LLaMA-3 70B fine-tuned with Critic-CoT data constructed primarily from GPT-4 outputs and self-critic distillation.",
            "model_size": "70B",
            "reflection_method_name": "Critic-CoT (step-wise Chain-of-Thought critique + refinement training)",
            "reflection_method_description": "Two-stage training: (1) collect critic/refine pairs using an external critic (GPT-4-Turbo) and generator samples; (2) let the learned critic criticize/refine its own outputs to generate additional data; fine-tune to produce stepwise CoT critiques and refinements.",
            "iteration_type": "self-critique with trained critic; generate-then-reflect available at inference",
            "num_iterations": 16,
            "task_name": "MATH / MATH500",
            "task_description": "Advanced math competition-style problems requiring long multi-step symbolic reasoning.",
            "evaluation_metric": "Top-1 Accuracy (Acc); Critic precision/recall/F1 and Critic Accuracy",
            "performance_before_reflection": "50.4% Top-1 accuracy (baseline before Critic-CoT fine-tuning reported in paper's main table)",
            "performance_after_reflection": "57.6% Top-1 accuracy after Critic-CoT fine-tuning; critic process-accuracy reported ≈72.2% for Critic-CoT on MATH500",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Smaller iterative-refinement gains on harder math problems; data construction required additional hinting for GPT-4 to produce usable critiques in stage-1; higher noise in process labels due to complex solutions; reliance on gold-answer verifiability limits generalization to tasks without deterministic final answers.",
            "uuid": "e6954.1",
            "source_info": {
                "paper_title": "Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Iterative Refinement (inference) - GSM8K",
            "name_full": "Iterative Refinement using trained Critic-CoT critic (inference-time generate-&gt;critique-&gt;refine loops) on GSM8K",
            "brief_description": "At inference, the trained critic examines a generated solution step-wise and, if it finds an error, requests a refinement starting from the first incorrect step; this process repeats until the critic validates the solution or a max depth is reached, improving final answer accuracy on GSM8K.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B-Instruct (Critic-CoT fine-tuned)",
            "model_description": "Fine-tuned Llama-3 70B that can both generate CoT solutions and produce stepwise critiques and refinements at inference.",
            "model_size": "70B",
            "reflection_method_name": "Iterative Refinement (Critic-CoT inference)",
            "reflection_method_description": "Single-sample generate-then-reflect loop: generate one solution, critic labels steps, if an error is found refine starting at first incorrect step; repeat until critic accepts or maximum refine depth reached.",
            "iteration_type": "generate-then-reflect (recursive iterative refinement)",
            "num_iterations": 8,
            "task_name": "GSM8K",
            "task_description": "Grade-school math word problems.",
            "evaluation_metric": "Top-1 Accuracy (Refine-Acc reported; rounds tracked as Round 0..7), average tokens and latency per round",
            "performance_before_reflection": "91.7% (accuracy at Round 0 / initial single-shot after Critic-CoT fine-tuning)",
            "performance_after_reflection": "93.3% accuracy by the 7th refinement round (reported Round 7 result); overall Iterative Refinement final reported accuracy 93.3% on GSM8K",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Risk of error accumulation across refinement rounds (True→False edits); diminishing returns after several rounds; increased token usage and latency with more rounds; to avoid de-generation they limit max refine depth to d=8 and restart policy; iterative refinement is less effective than majority-vote filtering in some settings.",
            "uuid": "e6954.2",
            "source_info": {
                "paper_title": "Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Iterative Refinement (inference) - MATH500",
            "name_full": "Iterative Refinement using trained Critic-CoT critic (inference-time) on MATH500",
            "brief_description": "Same iterative refinement pipeline applied to harder MATH problems; refinement yields only marginal gains and tends to converge quickly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B-Instruct (Critic-CoT fine-tuned)",
            "model_description": "Fine-tuned Llama-3 70B critic+refiner model.",
            "model_size": "70B",
            "reflection_method_name": "Iterative Refinement (Critic-CoT inference)",
            "reflection_method_description": "Generate a solution, critic examines steps, refine from first detected error, repeat until critic accepts or maximum depth reached (d=8).",
            "iteration_type": "generate-then-reflect (recursive iterative refinement)",
            "num_iterations": 8,
            "task_name": "MATH500",
            "task_description": "Challenging contest-style mathematics requiring long derivations.",
            "evaluation_metric": "Top-1 Accuracy (Refine-Acc), per-round accuracy and cost metrics",
            "performance_before_reflection": "57.6% initial accuracy (Round 0 after Critic-CoT training)",
            "performance_after_reflection": "57.8% by Round 7 (marginal gain); final reported Iterative Refinement accuracy 57.8% on MATH500",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Very small net improvement (near plateau); critic/refinement pipeline more sensitive to error accumulation on longer, harder traces; refinement often converges without meaningful corrections on very hard problems.",
            "uuid": "e6954.3",
            "source_info": {
                "paper_title": "Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Critic-as-Filter + Majority Vote - GSM8K",
            "name_full": "Critic as Filter: filter generated candidates by step-wise critic then majority-vote (Critic+Maj)",
            "brief_description": "Generate many candidate solutions, run the trained Critic-CoT model to label steps and filter out any candidate with a detected error, then take majority vote among the filtered set; yields higher accuracy than naive majority vote.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B-Instruct (Critic-CoT fine-tuned)",
            "model_description": "Fine-tuned Llama-3 70B used both to generate candidate solutions and to perform stepwise critique/filtering; majority vote taken over filtered candidates.",
            "model_size": "70B",
            "reflection_method_name": "Critic-as-Filter + Majority Vote",
            "reflection_method_description": "Sample m candidate solutions (paper uses N samples e.g., 96 for GSM8K), run critic on each to obtain per-step labels, discard any sample where critic finds an error, then perform majority vote on remaining candidates.",
            "iteration_type": "voting over multiple samples with critic-based filtering",
            "num_iterations": null,
            "task_name": "GSM8K",
            "task_description": "Grade-school math word problems.",
            "evaluation_metric": "Majority Vote Accuracy (Maj1@N), Pass@N and Critic+Maj1@N",
            "performance_before_reflection": "Maj1@96 = 94.8% (vanilla majority vote with 96 samples)",
            "performance_after_reflection": "Critic + Maj1@96 = 95.4% (after critic-based filtering then majority vote)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Computational cost grows linearly with number of samples; requires large sampling budgets to beat single-sample refinement; relies on critic precision—incorrectly filtering correct answers reduces effective sample diversity.",
            "uuid": "e6954.4",
            "source_info": {
                "paper_title": "Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Critic-as-Filter + Majority Vote - MATH500",
            "name_full": "Critic as Filter: filter generated candidates by step-wise critic then majority-vote (effect on MATH500)",
            "brief_description": "Applying critic-based filtering before majority voting yields larger gains on harder math problems (MATH500) compared to iterative refinement alone.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B-Instruct (Critic-CoT fine-tuned)",
            "model_description": "Fine-tuned 70B model performing candidate generation and step-wise criticism for filtering.",
            "model_size": "70B",
            "reflection_method_name": "Critic-as-Filter + Majority Vote",
            "reflection_method_description": "Filter out candidates flagged as incorrect by trained critic, then perform majority vote among the remaining candidates (paper reports Maj@512 and Critic+Maj@512 values).",
            "iteration_type": "voting over multiple samples with critic-based filtering",
            "num_iterations": null,
            "task_name": "MATH500",
            "task_description": "Advanced math problems requiring extended reasoning.",
            "evaluation_metric": "Majority Vote Accuracy (Maj1@512), Critic+Maj1@512",
            "performance_before_reflection": "Maj1@512 = 65.4% (vanilla majority vote with 512 samples)",
            "performance_after_reflection": "Critic + Maj1@512 = 68.4% (after critic-based filtering then majority vote)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "High sampling costs (512 samples used); if critic has imperfect recall/precision it can either pass wrong solutions or filter out correct ones; relies on massive compute to outperform single-sample iterative refinement in some regimes.",
            "uuid": "e6954.5",
            "source_info": {
                "paper_title": "Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Iterative Refinement (code) - HumanEval",
            "name_full": "Iterative Refinement applied to code generation (HumanEval) using Critic-CoT",
            "brief_description": "Apply the trained critic+refiner to code generation problems by iteratively critiquing and refining generated solutions; majority-vote filtering not applicable for HumanEval but iterative refinement still provides gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B-Instruct (Critic-CoT fine-tuned)",
            "model_description": "Fine-tuned LLaMA-3 70B model adapted to produce code, critique code-line/step correctness, and emit refinements.",
            "model_size": "70B",
            "reflection_method_name": "Iterative Refinement (Critic-CoT inference) for code",
            "reflection_method_description": "Generate a candidate program, critic inspects stepwise/program behaviors in natural language, request refinement when faults are detected, repeat until critic accepts or max depth reached.",
            "iteration_type": "generate-then-reflect iterative refinement",
            "num_iterations": 8,
            "task_name": "HumanEval",
            "task_description": "Code-generation benchmark of Python programming problems (functional unit tests determine pass/fail).",
            "evaluation_metric": "Pass@1 (single-sample pass rate) / pass-rate",
            "performance_before_reflection": "Base model pass rate reported as 76.2% (pre-Critic-CoT baseline for the evaluated setup)",
            "performance_after_reflection": "After Critic-CoT training and iterative refinement: 78.1% (model trained on GSM8K) and 84.8% (model trained on MATH) reported in paper",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Majority-vote filtering not applicable to code evaluation in their reported setup; critic must reason about program correctness which may be more complex than verifiable math answers; iterative refinement can increase latency and still fail when critic or refiner misdiagnoses faults.",
            "uuid": "e6954.6",
            "source_info": {
                "paper_title": "Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion",
            "rating": 2
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Self-RAG: Learning to retrieve, generate, and critique through self-reflection",
            "rating": 1,
            "sanitized_title": "selfrag_learning_to_retrieve_generate_and_critique_through_selfreflection"
        },
        {
            "paper_title": "Mirror: Multiple-perspective selfreflection method for knowledge-rich reasoning",
            "rating": 1,
            "sanitized_title": "mirror_multipleperspective_selfreflection_method_for_knowledgerich_reasoning"
        }
    ],
    "cost": 0.018566,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic
11 Jun 2025</p>
<p>Xin Zheng zhengxin2020@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences</p>
<p>University of Chinese Academy of Sciences</p>
<p>Jie Lou 
Xiaohongshu Inc</p>
<p>Boxi Cao 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences</p>
<p>University of Chinese Academy of Sciences</p>
<p>Xueru Wen wenxueru2022@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences</p>
<p>University of Chinese Academy of Sciences</p>
<p>Yuqiu Ji 
Xiaohongshu Inc</p>
<p>Hongyu Lin hongyu@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences</p>
<p>Yaojie Lu luyaojie@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences</p>
<p>Xianpei Han xianpei@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences</p>
<p>Debing Zhang 
Xiaohongshu Inc</p>
<p>Le Sun sunle@iscas.ac.cn 
Institute of Software
Chinese Information Processing Laboratory
Chinese Academy of Sciences</p>
<p>Guoxin Chen 
Minpeng Liao 
Chengxi Li 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Ponde 
Oliveira Pinto 
Jared Ka- Plan 
Harri Edwards 
Yuri Burda 
Nicholas Joseph 
Greg Brockman 
Alex Ray 
Raul Puri 
Gretchen Krueger 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Scott Gray 
Nick Ryder 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Clemens Winter 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
Ariel Herbert-Voss 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Christopher Hesse 
Andrew N Carr 
Jan Leike 
Josh Achiam 
Vedant Misra 
Evan Morikawa 
Alec Radford 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Dario Amodei 
Sam Mccandlish 
Karl Cobbe 
Vineet Kosaraju 
Jacob Hilton 
Reiichiro Nakano 
Abhimanyu Dubey 
Abhinav Jauhri 
Abhinav Pandey 
Abhishek Kadian 
Ahmad Al-Dahle 
Aiesha Letman 
Akhil Mathur 
Alan Schelten 
Amy Yang 
Angela Fan 
Anirudh Goyal 
Anthony Hartshorn 
Aobo Yang 
Kanishk Gandhi 
Ayush Chakravarthy 
Anikait Singh 
Nathan Lile 
Noah D Goodman 
Cognitive 
Bofei Gao 
Zefan Cai 
Runxin X Xu 
Peiyi Wang 
Ce Zheng 
Runji Lin 
Keming Lu 
Junyang Lin 
Chang Zhou 
Wen Xiao 
Junjie Hu 
Tianyu Liu 
Daya Guo 
Dejian Yang dengyang@xiaohongshu.com 
Haowei Zhang 
Junxiao Song 
Ruoyu Zhang 
Qihao Zhu 
Shirong Ma 
Aman Madaan 
Niket Tandon 
Prakhar Gupta 
Skyler Hallinan 
Luyu Gao 
Sarah Wiegreffe 
Uri Alon 
Nouha Dziri 
Shrimai Prabhumoye 
Yiming Yang 
Shashank Gupta 
Bodhisattwa Prasad Majumder 
Katherine Hermann 
Sean Welleck 
Yazdan- Bakhsh Amir 
Nat Mcaleese 
Rai Michael Pokorny 
Juan Felipe Ceron 
Long Ouyang 
Jeffrey Wu 
Xu Jiang 
Diogo Almeida 
Carroll Wainwright 
Chong Zhang 
Sandhini Agarwal 
Katarina Slama 
John Schulman 
Fraser Kelton 
Luke Miller 
Maddie Simens 
Amanda Askell 
Paul F Christiano 
Ryan 2022 Lowe 
Long Phan 
Alice Gatti 
Ziwen Han 
Nathaniel Li 
Josephina Hu 
Hugh Zhang 
Chen Bo 
Calvin Zhang 
Mohamed Shaaban 
John Ling 
Sean Shi 
Noah Shinn 
Federico Cassano 
Ashwin Gopinath 
Karthik R Narasimhan 
Shunyu 2023 Yao 
Charlie Snell 
Jaehoon Lee 
Kelvin Xu 
Lei Li 
Zhihong Shao 
Damai Dai 
Yifei Li 
Deli Chen 
Y Wu 
Zhifang Sui 
Xuezhi Wang 
Jason Wei 
Dale Schuurmans 
V Quoc 
Ed H Le 
Sharan Chi 
Aakanksha Narang 
Denny Chowdhery 
Zhou 
Peter West 
Ximing Lu 
Faeze Brahman 
Linjie Li 
Jena D Hwang 
Liwei Jiang 
Jillian Fisher 
Abhilasha Ravichander 
Khyathi Chandu 
PangBenjamin Newman 
Wei Koh 
Allyson Ettinger 
Yejin 2024 Choi 
Thomas Wolf 
Lysandre Debut 
Victor Sanh 
Julien Chaumond 
Clement Delangue 
Anthony Moi 
Pier- Ric Cistac 
Tim Rault 
Rémi Louf 
Morgan Funtowicz 
Joe Davison 
Sam Shleifer 
Patrick Von Platen 
Clara Ma 
Yacine Jernite 
Julien Plu 
Canwen Xu 
Teven Le Scao 
Sylvain Gugger 
Mariama Drame </p>
<p>DeepSeek-AI
Bingx-uan Wang
Bin WangAixin Liu, Bei Feng, Bo Liu</p>
<p>SC '21
Association for Computing Machinery. William Saunders
Catherine Yeh, Jeff Wu, Long OuyangNew York, Steven BillsNYUSA</p>
<p>Jonathan Ward</p>
<p>CoRR
2312.08935Tianlu Wang</p>
<p>Ilia Kulikov
Olga Golovneva
Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu</p>
<p>Richard Yuanzhe Pang
Maryam Fazel-Zarandi, Jason Weston</p>
<p>Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic
11 Jun 20254DC2CDD9E1D8DDDC9A131BE6E61CDB81arXiv:2408.16326v3[cs.CL]2021aTraining verifiers to solve math word problemsCoRR, abs/211014168
Self-critic has become a crucial mechanism for enhancing the reasoning performance of LLMs.However, current approaches mainly involve basic prompts for intuitive instance-level feedback, which resembles System-1 processes and limits the reasoning capabilities.Moreover, there is a lack of in-depth investigations into the relationship between LLM's ability to criticize and its task-solving performance.To address these issues, we propose Critic-CoT, a novel framework that pushes LLMs toward System-2-like critic capability.Through a step-wise CoT reasoning paradigm and the automatic construction of weak-supervision data without human annotation, Critic-CoT enables LLMs to engage in slow, analytic self-critique and refinement, thereby improving their reasoning abilities.Experiments on GSM8K and MATH and out-of-domain evaluation demonstrate that our enhanced model significantly boosts tasksolving performance by filtering out invalid solutions or iterative refinement.Furthermore, we investigate the intrinsic correlation between critique and task-solving abilities within LLMs, discovering that these abilities can mutually reinforce each other rather than conflict. 1* This work was done when Xin Zheng interned at Xiaohongshu.</p>
<p>Introduction</p>
<p>Enhancing the reasoning abilities of large language models is essential for creating more intelligent and reliable AI systems, which has drawn extensive attention from researchers (Chollet, 2019;Bubeck et al., 2023;Morris et al., 2024).From a cognitive perspective, the procedure of human reasoning involves constant reflection and revision (Hegel et al., 1991;Kierkegaard, 1989;Popper, 1934), which has inspired increasing focus on integrating self-critic mechanisms in the reasoning process of large-scale models (Kim et al., 2023;Shinn et al., 2023;Madaan et al., 2023).This involves iteratively allowing the model to generate feedback on its own responses and then refining its reasoning based on the feedback.Compared with traditional critic methods that depend on feedback from external sources (Saunders et al., 2022;McAleese et al., 2024), self-critic relies solely on the model's internal capabilities, thus reducing the high cost of additional human annotation, and serving as a promising potential solution to scalable oversight (Leike et al., 2018;Burns et al., 2023;Cao et al., 2024).</p>
<p>However, current studies primarily focus on utilizing LLMs' critique abilities to enhance their performance.Yet, relatively little attention has been given to the investigation and development of the critique ability itself.Firstly, existing critique methods are often overly simplistic, typically relying on a basic prompt to directly point out the error, without stepwise Chain-of-Thought examination or training procedure, which leads to relatively poor self-critic accuracy (Luo et al., 2023;West et al., 2024).Specifically, proposing a valid critique is a complicated task that requires a thorough understanding of statements and precise negativity.However, current LLMs are normally not explicitly trained for critic capability.Therefore, these simple approaches usually tend to "criticize" like System-1 (fast thinking mode), which is more intuitive and likely to make mistakes, rather than more rigorous and deliberate System-2 (slow thinking mode) (Kahneman, 2011;Yu et al., 2024), while shifting LLMs from System-1 toward System-2, which is to perform systematical analysis that fully utilizes the advantage of Chain-of-Thought reasoning, emerges as a promising approach for improving the reasoning capability (OpenAI, 2024).This limitation diminishes the effectiveness of self-critic and, further, self-correct (Huang et al., 2024).Secondly, the capabilities of task-solving and self-critic are both dependent on the model's inherent knowledge,</p>
<p>Previous Critic：Fast, Direct Error Identification Without Prior Analysis</p>
<p>Upon reviewing my previous answer, I recognize a mistake.I incorrectly identified 216 as ...</p>
<p>Step 1: Let the three consecutive integers be x-1, x, and x+1 ...</p>
<p>Step 2: Since we are looking for a perfect cube, let's express 3x ...</p>
<p>Step 3: However, n itself must also be divisible by 3, since …</p>
<p>Step 4: The smallest integer divisible by 3 is 6, for 6 ^ 3 … Answer: 216</p>
<p>Criticize Input Problem</p>
<p>What is the smallest positive perfect cube that can be written as the sum of three consecutive integers?(Reference: 7 + 8 + 9 = 3 ^ 3 = 27)</p>
<p>Generate Solution</p>
<p>Traditional Critique</p>
<p>Step 1: The sum of these three ...</p>
<p>Step 1 is correct.</p>
<p>Step 2: Expressing 3x as a cube ...</p>
<p>Step 2 is correct.</p>
<p>Step 3: The claim that n must be ...</p>
<p>Step 3 is incorrect.</p>
<p>Our CoT Critique</p>
<p>Refine</p>
<p>Step 3: Since we are looking for the ...</p>
<p>Step 4: The smallest perfect cubes are ...</p>
<p>Answer: 27</p>
<p>Step 1: To solve the problem of finding the ...</p>
<p>Step 2: We are given that this sum must be ...</p>
<p>Step 3: To ensure n is an integer, K^3 must ...</p>
<p>Step 4: We need to find the that the small … Answer: 54</p>
<p>Traditional Refine</p>
<p>Our Refine</p>
<p>Critic-CoT：Slow, Chain-of-Thought Examination Step-By-Step</p>
<p>Figure 1: Illustration of Critic-CoT: Previous instance-level critic methods attempt to identify errors directly without any prior analysis, and restart from the beginning during refinement.In contrast, our proposed Critic-CoT framework performs a step-wise examination using the Chain-of-Thought approach.When refining, rather than starting from scratch, our method makes the correction from the specific error step with the help of the corresponding critique.</p>
<p>while there is currently a lack of in-depth exploration regarding the correlation between these two capabilities within LLMs.In that case, it's challenging to balance the task-solving and the selfcritic capabilities of the model within the self-critic framework, which poses a significant obstacle to the subsequent development.</p>
<p>To this end, this paper is devoted to diving into the following critical research questions:</p>
<p>• How can we enhance a model's critique ability, pushing it toward System 2 reasoning?</p>
<p>• What is the relationship between a model's critique ability and its task-solving capability?</p>
<p>To answer the above questions, as shown in Figure 1, we propose Critic-CoT, a novel framework designed to enhance LLMs' reasoning abilities.Through step-wise Chain-of-Thought critique format and automated data construction through weak supervision, our method is able to strengthen System-2-like critic ability, without the intensive cost of human annotation.Specifically, during training, we let LLMs criticize and refine their solutions in a complete CoT way, and collect successful pairs that convert wrong solutions into correct ones, or affirm the validity of original right solutions.After supervised fine-tuning on the obtained step-wise critic-refine data, we enable the target LLM to analyze and criticize each step of its generated reasoning procedure, so that it can filter out wrong attempts and preserve the correct ones with greater precision.During inference, to leverage the model's abilities of CoT-critique and refinement, we employ two strategies: 1) majority vote filtering involves using the critic model to evaluate multiple generated solutions and filter out those incorrect; and 2) iterative refinement, on the other hand, involves repeatedly critiquing and refining a solution until no further error is detected.Through a series of experiments on the indomain dataset of GSM8K and MATH, together with out-of-domain evaluation on StrategyQA, AGIEval and HumanEval, we find that our trained critic model can fairly distinguish incorrect solutions from correct ones, and improve the reasoning accuracy via iterative refinement or critic filtering.These results demonstrate the helpfulness and effectiveness of our proposed method.Additionally, we observed that our critic model already exhibits noticeable performance improvements in task-solving, even in the absence of additional critique steps during the decoding phase.Such findings reveal that strengthening the ability to critique and refinement would not compromise the tasksolving performance, but improve it.This also suggests the presence of an intrinsic mechanism by which critique ability and task-solving capability mutually reinforce one another.</p>
<p>Our main contributions are as follows:</p>
<p>• We propose Critic-CoT, which pushes the critic paradigm of LLMs from System-1-like incentive "thinking" toward System-2-like deliberate "reasoning".</p>
<p>• Through experiments, we find that Critic-CoT can effectively teach the model to criticize and refine its own output step by step, thus noticeably improving the reasoning performance.</p>
<p>• Moreover, we find that for LLMs, the ability of critique and refinement could mutually re-inforce, which may shed light on designing more advanced self-critic framework designs in future work.</p>
<p>Related Works</p>
<p>With the development of LLMs, self-critic has emerged as a widely-adopted mechanism for reasoning, code generation, computer control, hallucination mitigation, retrieval-augmented generation and other tasks (Kim et al., 2023;Shinn et al., 2023;Madaan et al., 2023;Ji et al., 2023;Asai et al., 2024).However, typical self-critic approaches like Reflexion (Shinn et al., 2023), only utilize the LLMs' existing critique ability without further enhancement, and require the gold environment signal to iteratively generate critiques and make refinements.As long as external feedback is not available, off-the-shelf LLMs cannot perform intrinsic self-correct effectively due to limited critique and refinement abilities (Huang et al., 2024;Luo et al., 2023;Zeng et al., 2023).Later, several works are proposed to improve self-reflection via a carefully designed prompting pipeline on frozen LLMs, and no active training process is involved (Zhang et al., 2024b;Yan et al., 2024;Wu et al., 2024).Concurrently, Zhang et al. (2024a) trained a generative reward model on the outcome level rather than the process level, and did not incorporate refinement into the schema.Therefore, given the limited critic ability of current LLMs, how to train a robust and applicable critic model, which conducts detailed Chain-of-Thougt analysis in a step-wise systematic manner, and thus shifts from System-1 reasoning toward more deliberate System-2 reasoning (Kahneman, 2011), is worth investigating.</p>
<p>From the perspective of recursive reward modeling (Leike et al., 2018;Saunders et al., 2022) and scalable oversight (Burns et al., 2023), McAleese et al. (2024) recently trained "CriticGPT" to assist human labelers, which aims to improve the ability of human rather than the base model, i.e., improve the overall recall of error detection, rather than precision.While in this paper, we try to improve the reasoning ability of LLM without costly human annotation.</p>
<p>Method</p>
<p>Critic CoT is to equip LLMs with the ability to criticize and refine themselves step-by-step.As shown in Figure 2, it consists of two modules, including automated data construction via weak-supervision and self-check at inference-time.The weak-supervision principles are in Section 3.1, followed by the training process in Section 3.1, and the inference strategies in 3.3.</p>
<p>Chain-of-Thought Critique</p>
<p>In this work, we utilize a step-wise chain-ofthought critique, which makes the critique-refine process both controllable and formalizable, thereby facilitating the collection of weak supervision data.Formally, given the question Q and the corresponding gold answer Ans, we have the n-step attempt Att = [s 1 , ..., s n ] with predicted answer P red sampled by generator G.The corresponding critique Cri then can be represented as L = [l 1 , ..., l n ], where the step label l i = +1 indicates that step i is predicted to be correct, and
l i = −1 to be incorrect. Then the refinement Att ′ = [s ′ i , ..., s ′ n ′ ]
is start from the first incorrect step i with new answer P red ′ .We automatically annotate the process labels as follows:
• P red ̸ = Ans, −1 / ∈ L:
The attempt is wrong, yet the critique did not discover any error step.Thus the critique itself is problematic, and we need to sample another critique.</p>
<p>• P red ̸ = Ans, −1 ∈ L, P red ′ ̸ = Ans: The attempt is wrong, and the critique found an error, but still, the refinement is not correct.</p>
<p>There could be two cases for this situation: (1) the refinement is unsuccessful;</p>
<p>(2) the critique did not detect an earlier mistake.We simply sample another critique and corresponding refinement for this situation.</p>
<p>• P red ̸ = Ans, −1 ∈ L, P red ′ = Ans: Not only did the critique point out the error, but also the refinement reached the correct answer.We then believe the critique is valid, and collect the critique data instance C = (Q, Att, Cri) and the refinement data R = (Q, Att, Cri −1 , Att ′ ), where Cri −1 is the critique of last step, since explaining why previous steps are correct may not be helpful.</p>
<p>• P red = Ans, −1 / ∈ L: The attempt is correct, and the critique believes it is correct.So we can collect the positive critique data instance C = (Q, Att, Cri).</p>
<p>• P red = Ans, −1 ∈ L: The attempt reached the correct answer, yet the critique found an</p>
<p>Step 3: … Answer</p>
<p>Filter Error</p>
<p>Positive Critique</p>
<p>Step i: …</p>
<p>Step i is wrong</p>
<p>Step-wise Feedback</p>
<p>Negative Critique</p>
<p>As Generator</p>
<p>As Critic Correct Solution</p>
<p>Step 1 is correct</p>
<p>Step 2 is correct Step 3 is correct</p>
<p>Positive Critique</p>
<p>Step 1 is correct</p>
<p>Step 2 is wrong</p>
<p>Failed Critique</p>
<p>Refine</p>
<p>Success Refinement</p>
<p>Failed Refinement</p>
<p>Step 1 is correct</p>
<p>Step 2 is wrong</p>
<p>Negative Critique</p>
<p>Step 1 is correct</p>
<p>Step 2 is correct Step 3 is correct</p>
<p>Failed Critique</p>
<p>Step 3: … Answer error.Then, the critique could be wrong, and we need to sample another critique.</p>
<p>Auto Train: Two-Stage Training</p>
<p>To enable the model to acquire self-critiquing and refining capabilities, we first need to provide it with basic critiquing abilities, followed by self-critique for further enhancement.The overall training procedure is divided into two stages.</p>
<p>Stage 1 In the first step, we collect high-quality critique data to provide the model's basic critiquing ability.Specifically, we first sample both positive and negative solutions from a representative instruction-following model M G on the dataset D.Then, we utilize LLMs like GPT4-Turbo to serve as critic model M C .For each generated attempt Att, the critic model will retry at most k times to produce a valid critique until it reaches one of the weak supervision constraints.This will form the critic-refine dataset
D 1 = {(Q, Att, Cri)} {(Q, Att, Cri −1 , Att ′ )}
for fine-tuning the initial model M 0 into the critic model M 1 .Note that in this process, we actually distill Pass1@N of the teacher model M C into Top1@N of the student model.So, the theoretical upper bound of the student model is not necessarily limited by the teacher model's performance.</p>
<p>Stage 2 In the second step, we leverage the model's self-critique to enhance its critiquing and refining capabilities further.Namely, we let the learned critic model M 1 criticize and refine its own output.We first sample M correct-answer solutions and M incorrect-answer solutions for each question Q in the original dataset D.Then, for each attempt Att, we employ M 1 to repeatedly criticize and refine at most k times.If the model fails to critique even after k times, we fall back on the critique from a stronger yet frozen model M C as the final choice.Finally, we collect dataset
D 2 = {(Q, Att, Cri)} {(Q, Att, Cri −1 , Att ′ )}
and use D 1 D 2 to train the initial model M 0 into the final critic model M 2 , which is similar to Wang et al. (2024).This procedure helps the model to learn to criticize and refine its own reasoning outputs better.</p>
<p>Inference: Self-Check</p>
<p>Iterative Refine One single-turn refinement, which consists of multiple steps, may still contain errors.Therefore, we could iteratively inspect the refined solution, and re-refine once the critique found a mistake, and only output the final solution if it's convincing for the critic, or if it reached the maximum retry.To avoid de-generation after too many refinements, we set the maximum refine depth d = 8, and restart from the initial solution after d unsuccessful refinement at most n = 8 times.Figure A4 presents a single successful round of critique and refinement.</p>
<p>Critic As Filter Self-consistency is an effective way to reduce variance.With the ability to critique, we can filter out predict-to-be-wrong answers to further boost the performance.Specifically, for the m attempts S = {(Att, P red)}, we first let our model M check each attempt and obtain the stepwise label, which is S c = {(Att, P red, L)}.Then those which detect the error at some step are filtered out and reach S ′ c = {(Att, P red, L)|−1 / ∈ L}.Finally, we perform the majority vote to get the answer.</p>
<p>Experiment</p>
<p>We apply the Critic-CoT training process on the training dataset of GSM8K and MATH (Section 4.1), and observe a noticeable performance improvement (Section 4.2), and out-of-domain evaluations on AGIEval, StrategyQA, and HumanEval further exhibits the generalization of our trained critic ability (Section 4.3) For more analysis, discussion, see Appendix A.3 and Appendix A.4, and the prompt is presented in Appendix A.6.</p>
<p>Setup</p>
<p>Model</p>
<p>We fine-tune the critic-refine model on Llama-3-70B-Instruct (Dubey et al., 2024), which was pre-trained on more than 15 Trillion tokens and has a context length of 8,192.For critique / refinement sampling, we use GPT4-Turbo (OpenAI, 2023) of the version gpt-4-0125-preview.We use the Huggingface Transformers (Wolf et al., 2020), DeepSpeed (Rajbhandari et al., 2021) and FastChat (Zheng et al., 2023) libraries for training.We use vLLM library (Kwon et al., 2023)</p>
<p>Critic-CoT Improves Mathematical Reasoning</p>
<p>The results of in-domain evaluation are shown in</p>
<p>Critic-CoT Strengthens Out-of-Domain Reasoning</p>
<p>The results of out-domain evaluation are shown in Table 2.In general, our model, especially trained on MATH dataset, achieves positive performance gain with Iteartive Refinement and Critic As Filter, which demonstrates the generalized critique and refinement abilities beyond the math training domains.For StrategyQA, our critic models trained on two datasets show a positive performance increase when applying iterative refine and majority vote with the critic filter.On the more challenging dataset AGIEval, the Critic-CoT model trained on MATH performs much better than the model trained on grade-school level GSM8K dataset, and shows significant improvements in iterative refinement, rising to 63.7% compared with 56.6% prior the Critic-CoT training.On the code generation task HumanEval, the majority vote method is not applicable, but we can still observe the positive improvement with iterative refinement: while the base model's pass rate is 76.2%, our model trained on GSM8K and MATH achieved 78.1% and 84.8% respectively.These results highlight the robustness of our Critic-CoT models.</p>
<p>Ablation Analysis</p>
<p>To demonstrate the effectiveness of our Critic-CoT designs, we conduct a series of manual examination  Table 4: Ablation Study on GSM8K and MATH500.We use the metrics from three aspects: critic, including precision, recall, f1-score and accuracy; Iterative Refine, including accuracy before and after the refinement; and Critic As Filter, including Pass1@96, Maj1@96, and Critic+Maj1@96.The ablation study demonstrates the effectiveness of our Critic-CoT design.</p>
<p>and ablation studies, which confirm that our training data is still in high quality (Section 5.1), our proposed stepwise CoT critique is advantageous (Section 5.2), the composition of training data is beneficial (Section 5.3), and the source of improvement shall be attributed to our Critic-CoT framework rather than distillation (Section 5.4).</p>
<p>The Quality of Constructed Data</p>
<p>To examine the correctness of constructed Critic-CoT training data, we perform the manual evaluation.We sample 300 entries (100 with the critique of correct answer and 100 with the critique and refinemnt of wrong answer) each from the critic-cot data on GSM8K and MATH, and conduct a manual verification to verify the accuracy of the step-wise critiques.For the critique of the correct answer attempt, it is valid if there is indeed no error in all the intermediate steps; for the critique of the wrong attempt, it is valid if the first error step and the reason for the error are both identified.A refinement is correct, if the continuation steps are flawless.The results of manual verification are demonstrated in Table 3, with about 85% accuracy on wrong-answer critique, and more than 90% on refinement and correct-answer critique.Therefore, the data we automatically constructed maintain a high level of accuracy at the step level, which can well support the critique training process.</p>
<p>The Necessity of CoT Critique</p>
<p>To assess the necessity of the stepwise CoT critic, we remove CoT and train two baselines, namely "Process Label" and "Outcome Label".Specifically, for Process Label, the model is trained to directly predict the correctness of each step, e.g., "Step 1 is correct.Step 2 is incorrect.".Further, for Outcome Label, we remove the stepwise labels, which is to predict whether the entire solution is correct without providing step-wise details, e.g., "Each step from Step 1 to Step 8 is correct" or "Some step from Step 1 to Step 8 is incorrect".These two baselines can be viewed as the natural language versions of the Process Reward Model and Outcome Reward Model.</p>
<p>The results are shown in Table 4.We find that removing the Chain-of-Thought intermediate analysis and further stepwise labels, negatively impacts the critic accuracy, which indicates the effectiveness of stepwise CoT critique in improving the discrimination performance.From the original Critic-CoT model to the Process Label Model and the Outcome Label Model, the critique accuracy drops from 92.3% to 89.5% and further to 88.0% on GSM8K, and from 72.2% to 63.8% and further to 63.0% on MATH500.They fail to detect more errors and the recall metric is lower, despite its tendency to more easily pass correct solutions.Compared with System-1 like reasoning without explicit analysis, System-2 like reasoning with CoT critique can more precisely identify the errors.</p>
<p>The Impact of Training Data</p>
<p>To evaluate the impact of different data types during training, we perform a vertical ablation by removing either the critic data or the refinement data across both stages.In addition, we perform a horizontal ablation by training only with either stage 1 data or stage 2 data.</p>
<p>As the result shown in Table 4, we find that training only on the single-type data is less optimal.If only training to refine, the base model's critic ability remains weak, and still cannot effectively self-correct, as on GSM8K the accuracy drops from 92.0% to 88.2% after iterative refinement, and on MATH500 it drops from 55.4% to 49.8%.If only training to critic, the model obtains positive improvement from iterative refinement, with increases from 91.2% to 91.4% on GSM8K and 52.8% to 55.8% on MATH500, but still lags behind to Critic-CoT model that jointly trains the two capabilities.And finally, in terms of critic and reasoning performance, combining the data from two stages is better than training the data only from one stage.</p>
<p>Comparison with Solution Distilation</p>
<p>Since our data construction process leverages GPT-4 Turbo, one potential concern is whether the per- formance increase comes from distilling GPT-4, or our proposed Critic-CoT framework.To address this concern and better understand the source of improvement, we also train the traditional rejection sampling fine-tuning (RFT) baseline, using GPT4-Turbo to generate a correct-answer solution for each problem on the dataset of GSM8K and MATH respectively.</p>
<p>As the result shown in Table 5, unlike Critic-CoT, directly distilling from frontier LLMs may not significantly improve the reasoning performance, with only 90.7% on GSM8K and 48.0% on MAT500, which is less superior.On the one hand, the Llama-3-70B model has already experienced heavy post-training for downstream reasoning tasks (AI@Meta, 2024).On the other hand, as shown in Section 4.2, the improvements can be attributed to two key factors.First, strengthening the ability to critique and refinement, which is under-trained, directly improves the Top-1 accuracy.Moreover, at the inference phase, we can actively leverage the model's ability to reflect on its reasoning and correct mistakes, thus obtaining additional improvements that are not applicable without critic-refine training.Therefore, our proposed Critic-CoT framework plays a crucial role in driving the observed performance improvements.</p>
<p>Conclusion</p>
<p>In this paper, we introduced the Critic-CoT paradigm to enhance the reasoning abilities of Large Language Models, through a more System-2like, step-by-step Chain-of-Thought critique.Our approach leverages weak supervision to construct training data for critiques and refinements, thereby reducing the reliance on extensive human annotation.We demonstrated the effectiveness of our method through substantial improvements across the dataset of GSM8K and MATH.Additionally, our results present that training on the capabilities of critique and refinement alone improves task-solving performance, which indicates a mutualreinforce mechanism within the LLMs.We hope our work may inspire further investigations into the advancement of the self-critic framework and the transition toward System-2 reasoning.</p>
<p>Limitations</p>
<p>In this paper, we propose Critic-CoT, a framework to automatically construct critic-refine training data in the reasoning domain, where the correctness of a solution is verifiable by checking the final answer.For more general tasks, how to filter valid critiques and train robust critic models is worth investigating in future works.</p>
<p>A Appendix</p>
<p>A.1 Training &amp; Evaluation Details</p>
<p>A.1.1 Critic Data Construction GSM8K On GSM8K, since GPT-4 already got 92.0%accuracy on the test set (OpenAI, 2023), which makes it hard to obtain negative data, we use GPT-3.5-Turbo-0125instead to sample 10 solutions for each question in the training set.Then, we use GPT-4-Turbo as the critic-refine model to criticize the solutions (Table A7), with K = 16 retry.We obtain 63,485 cases, with 49,832 positive examples and 13,653 negative examples.</p>
<p>In the second stage of GSM8K critique construction, we use the learned critic model to repeatedly sample until we obtain at most 5 positive and 5 negative solutions.For strong LLMs like LLaMA-3, it's challenging to get enough negative solutions even among 512 samples, so the size of negative data would be slightly smaller.Then, we use the learned critic model to criticize itself, also with K = 16 retry.In stage two, we obtain 62,877 instances, with 39,654 positive and 26,001 negative.Among the two stages, we got 126,362 instances, with 86,708 positive and 39,654 negative.</p>
<p>MATH On MATH, in the first stage, we directly use the 90,074 GPT-4 generated solutions of PRM800K Dataset (Lightman et al., 2024), with 11,665 positive instances which all the step labels are correct, and 78,409 negative instances which one step label is incorrect.Since the MATH dataset is challenging, in order to reduce retry of GPT-4-Turbo and avoid not getting valid critique, for the critique of the negative solution, we additionally append reference solution in the input prompt, and hint it might contain mistakes, as suggested in prior work (Zelikman et al., 2022); for the positive solution, we simply hint it's correct.After obtaining the initial critique, we use GPT-4-Turbo again to remove hint phrases like "According to the reference" or "Given the hint" since we do not have any hint or reference during the test time.In stage one, we obtain 1,606 positive cases and 69,775 negative cases.</p>
<p>Similarly, in the second stage of MATH, we use the learned critic model to sample at most 5 positive and negative solutions.Then, we first use the critic model itself to critic its solutions, and without any hints, under K = 16 retry, and use GPT-4-Turbo to retry another K = 16 times with hint if failed.We construct 51,618 positive cases and 65,456 negative cases.Among the two stages, we got 188,455 cases, with 53,224 positive and 135,231 negative.</p>
<p>A.1.2 Answer Extraction</p>
<p>We let the model print the answer in the format \boxed{answer}.The model generates the answer following this pattern.We then extract the regular expression \boxed{.*}from the model output, and obtain the valid answer expression with matched parenthesis.The Python code for answer extraction is shown in Table A5.</p>
<p>A.1.3 Evaluation Metric</p>
<p>Solution For the evaluation of the solution, we extract the final answer (Appendix A.1.2) and compute the metrics of Top-1 Accuracy Acc and Refine Accuracy Refine-Acc, in which the original Top-1 predict-answer is replaced with a refined one if the critic model found an error and made iterative refinement (Section 3.3).We also compute Majority Vote Accuracy Maj1@N (Wang et al., 2023b) and Majority Vote Accuracy After Critique Critic + Maj1@N (Section 3.3), which is to select the most frequent answer among N samples, i.e.Critique For the "evaluation of evaluation", we compute Precision, Recall, and F1 for error detection; also, we compute Critic Accuracy, where the critique should find the error in wrong answer solutions and pass the correct answer solution:
P = {|P red i ̸ = Ans i ∧ −1 ∈ L i |} |{−1 ∈ L i }| (1) R = {|P red i ̸ = Ans i ∧ −1 ∈ L i |} |{P red i ̸ = Ans i }| (2) F 1 = 2 * P * R P + R (3) CriticAcc = 1 N N i=1 (P red i = Ans i ∧ −1 / ∈ L i ) ∨ (P red i ̸ = Ans i ∧ −1 ∈ L i )(4)
Here, for the i-th instance, P red i is the prediction answer, Ans i is the ground truth answer, and L i is the predicted step label list.</p>
<p>A.1.4 Implementation Details</p>
<p>The training on GSM8K dataset takes 23 hours, while on MATH dataset it takes 37 hours.The two datasets are under MIT license.For software, Huggingface Transformers, DeepSpeed, vLLM, and FastChat libraries we used are under Apache-2.0license.</p>
<p>A.2 Additional Result on MATH</p>
<p>Table A1 presents the results of Critic-CoT training on MATH dataset, with the original 7,500/5,000 split setting.</p>
<p>A.3 Analysis</p>
<p>A.3.1 Critic Performance</p>
<p>For both datasets, the critic model's accuracy continues to grow as the sample size N increases, ultimately surpassing the performance of the majority vote, which gradually converges.Specifically, in the MATH dataset, the critic model achieves substantially higher accuracy than the solution accuracy, consistently outperforming the naive majority vote due to the critic filter's superior performance.This stark contrast highlights the critic model's effectiveness in identifying and promoting correct answers.In the GSM8K dataset, despite having a critic accuracy of only 92.3%, the critic model still manages to deliver higher accuracy gains.This outcome suggests that the critic model successfully filters answers to increase the density of correct answers and decrease the density of wrong answers, compared to the normal answer distribution.The overall results demonstrate the critic model's robust capability to enhance accuracy across different datasets, validating its practical utility in improving prediction outcomes.</p>
<p>A.3.2 Inspect on Iterative Refine</p>
<p>The iterative refinement process for the GSM8K and MATH datasets demonstrates different levels of effectiveness due to their complexity, as shown in Table A2.GSM8K, being simpler, shows a higher success rate in refinement.For effective refinement, the number of false answers corrected (False → True) must exceed the number of true answers incorrectly changed (True → False).Despite occasional mistakes by the critic, correct answers are not always altered incorrectly.For GSM8K (Table A2a), accuracy improves from 91.7% initially to 93.3% by the seventh round, with significant gains in both true-to-true and falseto-true transformations.In contrast, MATH (Table A2b) starts at 57.6% accuracy, reaching 57.8% by the seventh round.The iterative refinement process tends to converge, which is expected.</p>
<p>A.3.3 Group By Difficulty Level</p>
<p>For the MATH dataset, the difficulty level is given from 1 to 5. For the GSM8K dataset, we set the difficulty level according to the number of expressions n that appeared in the reference solution, i.e., max(1, min(5, n)).As illustrated in Figure A2, the performance on the GSM8K dataset shows a gradual decline as the difficulty level increases.This trend is accompanied by the emerging effects of the critic and refine stages, which become more prominent at higher difficulty levels.In contrast, the accuracy on the MATH dataset declines sharply as the problems become more challenging.Generally, the refine stage proves effective across all levels, while the critic stage is beneficial at most levels, with some minor exceptions.These observations suggest potential areas for further improvements in the critic mechanism.</p>
<p>A.4 Discussion</p>
<p>A.4.1 Discriminative Verifier for Mathematics</p>
<p>To further improve the reasoning ability of large language models, one applicable approach is through the use of reward models, which can either be used in reinforcement learning during training (Ouyang et al., 2022) or rejection sampling at test time (Cobbe et al., 2021b).While outcomesupervised reward models (ORMs) allow for the automatic collection of training data based on the signal of the gold answer, process-supervised reward models (PRMs) would be more advantageous for more precise feedback, better interpretability and stronger alignment (Lightman et al., 2024).</p>
<p>To reduce the considerable human labeling cost and difficulty for dense annotation, a series of works based on automatic approaches have been proposed (Wang et al., 2023a;Chen et al., 2024a;Luo et al., 2024;Snell et al., 2024), all under the heuristic that for an incorrect solution, the first error step is where the continuation of previous step would lead to a correct answer.This may bring noise into training data due to false positives and negatives (Luo et al., 2024).Moreover, annotation based on the implicit solution continuation alone does not leverage LLM's emerging ability of critic, which is in a more explicit and analytic way and brings better explainability (Saunders et al., 2022;Yuan et al., 2024;Luo et al., , 2024).Additionally, binary 0/1 discrimination alone, whether outcome-based or process-based, remains more similar to System-1 reasoning rather than the desirable System-2, thus may not fully leverage the computation power support by empirically successful Chain-of-Thought prompting (Feng et al., 2023;Li et al., 2024).</p>
<p>A.4.2 Differences Between Critic-CoT and Reflexion</p>
<p>We adopt a similar approach to Relexion (Shinn et al., 2023), which leverages natural language critique to facilitate refinement, but our method diverges in the following ways:</p>
<p>Step-wise CoT Critique Reflexion translates and augments the binary reward signal from the environment to natural language, but on an instance level.Instead, fine-grained Chain-of-Thought analysis at the step level, which is more systematic, and enables us to locate the error and start refinement from a specific step, rather than refine the whole attempt.</p>
<p>Enhanced Critic ability While Relexion proposed an in-context learning pipeline for policy optimization under the oracle success/fail binary feedback signal, Huang et al. (2024) showed that without external feedback, vanilla LLMs cannot self-correct effectively due to limited critique ability.Therefore, to teach the LLMs the ability of intrinsic self-critique, our approach tries to learn the critique ability itself, through Critic-CoT train-   ing, and can apply it to test-time situations where the oracle feedback signal is not available.</p>
<p>A.4.3 Differences between Critic-CoT and Long Reasoning Model</p>
<p>Compared with these reasoning models, our method differs in terms of training mechanism and inference method.While reasoning models attempt to improve the accuracy with longer CoT output, our Critic-CoT method introduces explicit, stepwise critique, which helps identify the error more precisely and facilitates refinement.The key distinctions are as follows:</p>
<p>Controllability and Efficiency It could be nontrivial to control the long reasoning models to critic and refine during inference.When to critic, critic at which step, and when to restart depends on the model itself, and they often exhibit "overthinking," performing unnecessary reflections or exploring multiple solutions even for simple problems, leading to increased computational costs without proportionate gains (Chen et al., 2024b).On the contrary, our method can achieve controllable testtime scaling, by specifying the number of reflection rounds for Iterative Refine, and the number of samples for Critic As Filter.</p>
<p>Adaptability While models like R1 (Guo et al., 2025) are trained with reinforcement learning with outcome reward and implicitly learn the critic and refine patterns, this self-reflection behavior is not guaranteed to emerge reliably across all models (Gandhi et al., 2025).Instead, our proposed framework explicitly teaches the model step-level critique and refinement, and can be easily adapted to different LLMs of various sizes.Moreover, we believe our approach is complementary to long reasoning models.Given the limited self-critic ability of current R1-like models (He et al., 2025), our method could strengthen these models to precisely detect errors while maintaining confidence in correct steps.Reasoning models with enhanced self-critic ability could potentially reduce overthinking, boost performance for more complex tasks (Phan et al., 2025), and move toward self-improvement and scalable oversight (Schaul, 2024).</p>
<p>A.4.4 Comparison Between Inference Methods</p>
<p>The statistics of average tokens and latency per instance are shown in Refinement, as the number of rounds increases, the computation cost and latency slowly increase, but the performance gain gradually becomes saturated.</p>
<p>The reason for non-linear computation increase is that, refinement only occurs if the critique detects that the attempt is wrong and stops if the critique validates it.Yet, the pipeline of iterative refinement could be more sensitive to error accumulation, which could limit performance.For Critic As Filter, the computation cost and latency increase linearly as the number of samples increases, and the performance also improves steadily.The linear cost increase is because we need to sample the attempt first, then perform CoT critic for each step.To reduce latency, we can parallelize the inference, and in our setting, we use 32 GPUs for inference, which deploys 8 models in total.</p>
<p>As the results in Table 1 shows, the performance of majority vote and Critic As Filter surpass Iterative Refinement.We believe it's due to the intrinsic challenges of refinement and the relatively limited search space.</p>
<p>On the one hand, for Iterative Refinement to work properly, it requires the model to 1) Detect er-rors on an attempt; 2) Refine the mistakes; 3) Exit if no further errors are detected.This pipeline could be more sensitive to error accumulation.Moreover, it only edits on a single example and has a limited retry, which is sample-efficient, but may not explore the solution space more actively, as majority vote does.Specifically on the dataset of GSM8K, the invocation statistics are as follows:</p>
<p>• Majority vote: 1319 * 96 = 126,624 On the other hand, majority vote is a strong baseline, as it requires massive sampling.It leverages diverse reasoning paths and tries to mitigate the stochastic of a single sample.But under the method of Critic As Filter, we actively filter out problematic attempts and perform the majority vote on the more reasonable candidates, rather than equally account for all the predicted answers as the vanilla majority vote does, which further increases performance.This in turn demonstrates our model's strong ability to critique.</p>
<p>A.4.5 Self-Reflection</p>
<p>Besides the main results, through out-of-domain evaluation in Table 2, we find our model demonstrates generalized ability to critique and refine.</p>
<p>While the ability of LLMs to self-reflect still remains an open question, and we hope our work as a valuable exploration could shed light on future studies in this area.Moreover, as long as we adequately improve the models' ability to critique, we could achieve test-time performance increase in the form of "selfreflection".As the experiment results present, after Critic-CoT training, the ability to critique and generate both improves, though they are not exactly identical.Notably, the critique ability can surpass the task-solving ability, allowing the model to detect errors even when it has a low probability of generating a valid solution, as prior works (Saunders et al., 2022;Lin et al., 2024) also suggest.This indicates that by strengthening the model's CoT critique ability beyond its generation capability, we can leverage this discriminative power to reject imperfect responses and achieve positive performance gains.</p>
<p>A.4.6 Process Correctness of Correct Answer Attempt</p>
<p>We each sample 100 correct answer solutions, on GSM8K by GPT-3.5-Turbo and GPT-4-Turbo, and MATH by GPT-4-Turbo, and manually check if all intermediate steps are correct.The results are demonstrated in Table A6.We find that in general, the correct final answer is a good indicator of correct intermediate steps.Also, from GSM8K to MATH, as the reasoning traces become longer and more complicated, the percentage of correct answer but with wrong intermediate steps increases.</p>
<p>A.4.7 Alternatives of Leveraging Proprietary Models for Data Construction</p>
<p>To mitigate the data bias from GPT-4, we can introduce other open-source LLMs like DeepSeek-V3 or Qwen2.5-Max to perform multi-agent debates (Chan et al., 2024;Liang et al., 2024).This could encourage diverse thinking trajectories and overcome the inherited bias from a single model.</p>
<p>To reduce computation costs, we may substitute with smaller models.While our weak supervision method can effectively filter out problematic critic, to maintain a high success rate of generating valid data for smaller models, we can apply iterative refinement with more rounds, Monte Carlo Tree Search that more efficiently explores the search space, and other techniques.This can facilitate smaller models to produce desirable critic-refine data that is comparable with larger models.</p>
<p>A.5 Examples of Refinement</p>
<p>As presented in Figure A3, in an example of GSM8K, the model forgot to add one year at Step 3; then, through CoT critique, the model found that while Step 1 and Step 2 are correct, Step 3 contains this ignorance error.Finally, guided by the critique of Step 3, the model made a correction and reached the gold answer of 13.Similarly, in an example of MATH (A4), the model identified and successfully fixed the error.</p>
<p>A.6 Prompts</p>
<p>Table A7, Table A10, and Table A8 presents the prompt for critic-refine data collection using GPT4-Turbo, with Table A9 for removing the hint phrases (Section 3.2).Table A11, Table A12, and Table A13 shows the prompt of trained model for solving, critique, and refinement during stage-2-training (Section 3.2) and inference (Section 3.3).Table A14, Table A15 and Table A16 present the prompts and responses of a single-turn critique-refinement, under Critic-CoT, Step-wise Label Critic and Final Label Critic respectively.Table A17   What is the smallest positive perfect cube that can be written as the sum of three consecutive integers?</p>
<p>Model Attempt:</p>
<p>Step 1: Let the three consecutive integers be x − 1, x, and x + 1.The sum of these three integers is 3x.</p>
<p>Step 2: Since we are looking for a perfect cube, let's express 3x as a cube of some integer n, where n 3 = 3x.</p>
<p>Step 3: To find the smallest such n, we need to minimize x.However, since n 3 must be divisible by 3, and x is an integer, n itself must also be divisible by 3.</p>
<p>Step 4: The smallest integer n that is divisible 3 and gives us a perfect cube when cubed is n = 6, because 6 3 = 216.</p>
<p>Step 5: Substituting n = 6 into n 3 = 3x, we get 216 = 3x, which simplifies to x = 72.</p>
<p>Step 6: Therefore, the smallest positive perfect cube that can be written as the sum of three consecutive integers is 216 .</p>
<p>Model Critique:</p>
<p>Step 1: Let the three consecutive integers be x − 1, x, and x + 1.The sum of these three integers is indeed 3x.</p>
<p>Conclusion:</p>
<p>Step 1 is correct.</p>
<p>Step 2: Expressing 3x as a cube of some integer n, where n 3 = 3x, correctly sets up the equation needed to solve the problem.This step accurately translates the problem statement into a mathematical expression.</p>
<p>Conclusion:</p>
<p>Step 2 is correct.</p>
<p>Step 3: The claim that n must be divisible by 3 because n 3 must be divisible by 3 is a misunderstanding.In reality, n 3 being divisible by 3 does not imply that n itself must be divisible by 3; rather, it simply means that n 3 will always be divisible by 3 regardless of whether n is divisible by 3.This step introduces an unnecessary restriction on n.</p>
<p>Conclusion:</p>
<p>Step 3 is incorrect.</p>
<p>Model Refinement:</p>
<p>Step 3 (Corrected): Since we are looking for the smallest positive perfect cube that can be expressed as 3x, we should start by identifying the smallest perfect cubes and checking if any of them can be represented in this form.</p>
<p>Step 4: The smallest perfect cubes are 1 3 = 1, 2 3 = 8, 3 3 = 27, 4 3 = 64, 5 3 = 125, 6 3 = 216, and so on.Among these, 27 is the first number that can be expressed as 3x, where x is an integer (x = 9).</p>
<p>Step 5: Therefore, the smallest positive perfect cube that can be written as the sum of three consecutive integers is 27 .-Your reasoning should precede any claims or conclusions you make to avoid unwarranted assertions.</p>
<p>-At the end of the evaluation for each step, YOU MUST articulate the conclusion using the format "Conclusion:</p>
<p>Step [i] is correct" or "Conclusion:</p>
<p>Step [i] is incorrect".Words like "partially correct" are prohibited.</p>
<p>-You shall not evaluate multiple steps at a time, so words like "Step 7 to Step 24:" or "Step 4 through 6" are forbidden.</p>
<p>-Once a mistake is identified and stated, stop the evaluation, and enumerate the corrected steps starting from the step where the mistake was detected, and label this part of your response with <correction> at the start and </correction> at the end.Also, the final answer should be a single number, in the form \boxed{}, at the final step.</p>
<p>Table A7: The prompt for the collection of critique and refinement on GSM8K, using GPT4-Turbo.</p>
<p>Prompt</p>
<p>How do you evaluate the following attempt with respect to the problem, with the help of reference solution?</p>
<p>Hint: There could be a mistake.-Your reasoning should precede any claims or conclusions you make to avoid unwarranted assertions.</p>
<p>-Please ensure that the output text does not include phrases implying the use of a reference solution or hint, even though these resources are being utilized.</p>
<p>-At the end of the evaluation for each step, YOU MUST articulate the conclusion using the format "Conclusion:</p>
<p>Step [i] is correct" or "Conclusion:</p>
<p>Step [i] is incorrect".Words like "partially correct" are prohibited.</p>
<p>-You shall not evaluate multiple steps at a time, so words like "Step 7 to Step 24:" or "Step 4 through 6" are forbidden.</p>
<p>-Once a mistake is identified and stated, stop the evaluation, and enumerate the corrected steps starting from the step where the mistake was detected, and label this part of your response with <correction> at the start and </correction> at the end.Also, the final answer should be in the form \boxed{}, at the final step.</p>
<p>Table A8: The prompt for the collection of critique and refinement on MATH incorrect attempt, using GPT4-Turbo.</p>
<p>Prompt</p>
<p>For the following text, remove any phrases like "reference solution" or "hint", and keep all the other content.Do not miss the "<correction>" and "</correction>" labels that exist in the text.Do not respond to anything else.</p>
<p>--{critique_refinement}</p>
<p>Table A9: The prompt for removing the hint of critique and refinement on MATH, using GPT4-Turbo.</p>
<p>Prompt</p>
<p>How do you evaluate the following attempt with respect to the problem?Hint: All the steps are correct, and the attempt reached a correct answer.-Your reasoning should precede any claims or conclusions you make to avoid unwarranted assertions.</p>
<p>-Please ensure that the output text does not include phrases implying the use of a reference solution or hint, even though these resources are being utilized.</p>
<p>-At the end of the evaluation for each step, YOU MUST articulate the conclusion using the format "Conclusion:</p>
<p>Step [i] is correct" or "Conclusion:</p>
<p>Step [i] is incorrect".Words like "partially correct" are prohibited.</p>
<p>-You shall not evaluate multiple steps at a time, so words like "Step 7 to Step 24:" or "Step 4 through 6" are forbidden.</p>
<p>-Once a mistake is identified and stated, stop the evaluation, and enumerate the corrected steps starting from the step where the mistake was detected, and label this part of your response with <correction> at the start and </correction> at the end.Also, the final answer should be in the form \boxed{}, at the final step.</p>
<p>Table A10: The prompt for the collection of critique on MATH correct attempt, using GPT4-Turbo.</p>
<p>Prompt</p>
<h2 problem="problem">Problem</h2>
<p>--Solve the problem step by step, marking each step as "Step [i]:".Your final answer should be in the form \boxed{answer}, at the end of your response.</p>
<p>Prompt of Critique under Critic-CoT</p>
<p>How do you evaluate the following attempt with respect to the problem?<problem> Consider the geometric sequence 125 9 , 25 3 , 5, 3, . ... What is the eighth term of the sequence?Express your answer as a common fraction.</p>
<p></problem> <attempt></p>
<p>Step 1: Identify the first term of the sequence.The first term a 1 is:
a 1 = 125 9
Step 2: Calculate the common ratio r.The common ratio r can be found by dividing the second term a 2 by the first term a 1 : The n-th term of a geometric sequence can be calculated using the formula:
a n = a 1 • r n−1
Step 4: Calculate the eighth term a 8 .Substituting n = 8 into the formula:
a 8 = a 1 • r 8−1 = 125 9 • 3 5 7
Step 5: Calculate 3 5 7 .</p>
<p>Calculating 3 -Your reasoning should precede any claims or conclusions you make to avoid unwarranted assertions.</p>
<p>-At the end of the evaluation for each step, YOU MUST articulate the conclusion using the format "Conclusion:</p>
<p>Step [i] is correct" or "Conclusion:</p>
<p>Step [i] is incorrect".Words like "partially correct" are prohibited.</p>
<p>Response of Critique under Critic-CoT</p>
<p>Step 1: The first term a 1 = 125 9 is identified correctly.Conclusion: Step 1 is correct.</p>
<p>Step 2: The common ratio r is calculated as: This calculation is correct.</p>
<p>Conclusion:</p>
<p>Step 2 is correct.</p>
<p>Response of Critique under Critic-CoT</p>
<p>Step 3: The formula for the n-th term is correctly stated as:
a n = a 1 • r n−1 Conclusion:
Step 3 is correct.</p>
<p>Step 4: The eighth term is computed as:
a 8 = 125 9 • 3 5 7 This is correct. Conclusion:
Step 4 is correct.</p>
<p>Step 5: Calculating 3 5 7 :
3 5 7 = 2187 78125
This calculation is correct.</p>
<p>Conclusion:</p>
<p>Step 5 is correct.</p>
<p>Step 6: Substituting back:
a 8 = 125 9 • 2187 78125 This is correct. Conclusion:
Step 6 is correct.</p>
<p>Step 7: Multiplying the fractions: This is also correct.</p>
<p>Conclusion:</p>
<p>Step 7 is correct.</p>
<p>Step 8: The goal is to simplify the fraction 273375 703125 .The attempt made to find the GCD and simplify.To simplify the fraction, we need to find the greatest common divisor (GCD) of 273375 and 703125.The standard approach involves either prime factorization or using the Euclidean algorithm.The GCD can now be derived from the lowest powers of the common factors: -For 5: 5 3 -For 3: 3 2 Thus, the GCD is:
5 3 • 3 2
Therefore, 273375 and 703125 are not divisible by 135.</p>
<p>Conclusion:</p>
<p>Step 8 is incorrect.</p>
<p>Prompt of Refinement under Critic-CoT</p>
<p>How do you refine the following attempt with respect to the problem, given the criticism?</p>
<p><problem> Consider the geometric sequence 125 9 , 25 3 , 5, 3, . ... What is the eighth term of the sequence?Express your answer as a common fraction.</p>
<p></problem> <attempt></p>
<p>Step 1: Identify the first term of the sequence.The first term a 1 is:
a 1 = 125 9
Step 2: Calculate the common ratio r.The common ratio r can be found by dividing the second term a 2 by the first term a 1 : The n-th term of a geometric sequence can be calculated using the formula:
r = a 2 a 1 =25a n = a 1 • r n−1</p>
<p>Prompt of Refinement under Critic-CoT</p>
<p>Step 4: Calculate the eighth term a 8 .Substituting n = 8 into the formula:
a 8 = a 1 • r 8−1 = 125 9 • 3 57
Step 5: Calculate  Step 1: Identify the first term of the sequence.The first term a 1 is:
a 1 = 125 9
Step 2: Calculate the common ratio r.The common ratio r can be found by dividing the second term a 2 by the first term a 1 :</p>
<p>Response of Refinement under Outcome Label Critic</p>
<p>Thus, the common ratio is:
r = 3 5
Step 3: Use the formula for the n-th term of a geometric sequence.The n-th term of a geometric sequence can be calculated using the formula:
a n = a 1 • r n−1
Step 4: Calculate the eighth term a 8 .Substituting n = 8 into the formula:
a 8 = a 1 • r 8−1 = 125 9 • 3 57
Step 5: Calculate 3 5 7 .</p>
<p>Calculating 3 5 7 :
3 5 7 = 3 7 5 7 = 2187 78125
Step 6: Substitute back into the expression for a 8 .Now we compute:</p>
<p>Response of Code Generation on HumanEval</p>
<p>Step 1: Create a function named <code>split_words</code>that takes a string <code>txt</code>as an input.</p>
<p>Step 2: Check if there are any whitespace characters in the string.If there are, split the string into words using the 'split()' function with the default separator (whitespace).</p>
<p>Step 3: If there are no whitespace characters, check if there are any commas in the string.If there are, split the string into words using the 'split()' function with the comma as the separator.</p>
<p>Step 4: If there are neither whitespace characters nor commas in the string, iterate over the characters in the string, keeping track of the count of lowercase letters with odd alphabetical order (based on their ASCII value modulo 2).</p>
<p>Step 5: Return the result based on the conditions met in steps 2, 3, or 4.</p>
<p>Here is the Python code for the function: <code>`python d e f s p l i t _ w o r d s ( t x t ) : i f ' ' i n t x t : r e t u r n t x t .s p l i t ( ) e l i f ' , ' i n t x t : r e t u r n t x t .s p l i t ( ' , ' ) e l s e : c o u n t = sum ( 1 f o r c h a r i n t x t i f c h a r .i s l o w e r ( ) and ord ( c h a r ) % 2 != 0 ) r e t u r n c o u n t</code>T his function first checks for whitespace characters, then for commas, and finally counts the lowercase letters with odd alphabetical order if neither of the first two conditions is met.</p>
<p>Prompt of Critique on HumanEval</p>
<p>How do you evaluate the following attempt with respect to the problem?<problem> d e f s p l i t _ w o r d s ( t x t ) :</p>
<p>' ' ' G i v e n a s t r i n g o f words , r e t u r n a l i s t o f words s p l i t on w h i t e s p a c e , i f no w h i t e s p a c e s e x i s t s i n t h e t e x t you s h o u l d s p l i t on commas ' , ' i f no commas e x i s t s you Step 1: Create a function named <code>split_words</code>that takes a string <code>txt</code>as an input.</p>
<p>Step 2: Check if there are any whitespace characters in the string.If there are, split the string into words using the 'split()' function with the default separator (whitespace).</p>
<p>Step 3: If there are no whitespace characters, check if there are any commas in the string.If there are, split the string into words using the 'split()' function with the comma as the separator.</p>
<p>Step 4: If there are neither whitespace characters nor commas in the string, iterate over the characters in the string, keeping track of the count of lowercase letters with odd alphabetical order (based on their ASCII value modulo 2).</p>
<p>Step 5: Return the result based on the conditions met in steps 2, 3, or 4.</p>
<p>Here is the Python code for the function: <code>`python d e f s p l i t _ w o r d s ( t x t ) : L i n e 1 : i f ' ' i n t x t : L i n e 2 : r e t u r n t x t .s p l i t ( ) L i n e 3 : e l i f ' , ' i n t x t : L i n e 4 : r e t u r n t x t .s p l i t ( ' , ' ) L i n e 5 : e l s e : L i n e 6 : c o u n t = sum ( 1 f o r c h a r i n t x t i f c h a r .i s l o w e r ( ) and ord ( c h a r ) % 2 != 0 ) L i n e 7 : r e t u r n c o u n t</code>T his function first checks for whitespace characters, then for commas, and finally counts the lowercase letters with odd alphabetical order if neither of the first two conditions is met.</attempt></p>
<p>Response of Critique on HumanEval</p>
<p>Step 1: Define a function called <code>split_words</code>that takes a string <code>txt</code>as its parameter.</p>
<p>Conclusion:</p>
<p>Step 1 is correct.</p>
<p>Step 2: Verify if there are any whitespace characters in the string.If they exist, divide the string into words using the <code>split()</code>function with the default separator (whitespace).</p>
<p>Conclusion:</p>
<p>Step 2 is correct.</p>
<p>Step 3: If no whitespace characters are found, check for commas in the string.If commas are present, split the string into words using the <code>split()</code>function with the comma as the separator.Conclusion: Step 3 is correct.</p>
<p>Step 4: Otherwise, loop through the characters in the string, counting the number of lowercase letters that have an odd alphabetical order (based on their ASCII value modulo 2).The mistake in Step 4 lies in the interpretation of "odd alphabetical order."The problem statement refers to the alphabetical order of lowercase letters in the alphabet, not their ASCII values.The correct approach is to count the lowercase letters whose alphabetical order (where 'a' is 0, 'b' is 1, ..., 'z' is 25) is odd.</p>
<p>Conclusion:</p>
<p>Step 4 is incorrect.</p>
<p>Prompt of Refinement on HumanEval</p>
<p>How do you refine the following attempt with respect to the problem, given the criticism?You shall write another complete Python function, in the format <code>python\n[code]\n</code>. <problem> [...] </problem> <attempt> [...] </attempt> <criticism> Step 4: Otherwise, loop through the characters in the string, counting the number of lowercase letters that have an odd alphabetical order (based on their ASCII value modulo 2).The mistake in Step 4 lies in the interpretation of "odd alphabetical order."The problem statement refers to the alphabetical order of lowercase letters in the alphabet, not their ASCII values.The correct approach is to count the lowercase letters whose alphabetical order (where 'a' is 0, 'b' is 1, ..., 'z' is 25) is odd.i f ' ' i n t x t : r e t u r n t x t .s p l i t ( ) e l i f ' , ' i n t x t : r e t u r n t x t .s p l i t ( ' , ' ) e l s e : c o u n t = sum ( 1 f o r c h a r i n t x t i f c h a r .i s l o w e r ( ) and ( ord ( c h a r ) − ord ( ' a ' ) ) % 2 != 0 ) r e t u r n c o u n t ``T his function correctly counts the lowercase letters with odd alphabetical order by subtracting the ASCII value of 'a' from the ASCII value of each lowercase letter to get its alphabetical order, and then checking if this order is odd.If the order is odd, it increments the count.</correction> Table A17: Full example of a single-turn refinement on HumanEval.The attempt made mistakes in Step 4 and Line 6, which use the ASCII value directly rather than subtract the value of 'a'.The critique detect this error, and made a successul refinement.</p>
<p>Figure 2 :
2
Figure 2: The Process of Critic-CoT during training (a) and inference (b).For training, we collect the critic-refine data on the generator's samples via weak supervision (Section 3.1).Through fine-tuning, we enable the target model to criticize and refine its own reasoning process.Then, during inference, we can leverage the capabilities via Iterative Refine or Critic As Filter (Section 3.3).</p>
<p>a i = a).Following Liu et al. (2023); Havrilla et al. (2024), we compute Pass@N, which select the gold answer g among the N predictions if present, i.e. arg max a 1 (g = a).</p>
<p>Figure A1 :
A1
Figure A1: Performance of majority vote on GSM8K and MATH500 Datasets</p>
<p>Figure A2 :
A2
Figure A2: Performance group by difficulty level, on GSM8K and MATH500 Datasets</p>
<p>Figure A4 :
A4
Figure A4: Criticize and refine a problem in the MATH test set.In the attempt, the model made a hallucination in Step 3, then through stepwise CoT critique, the model detected this error, and accordingly refined the solution starting from Step 3 successfully.</p>
<p>step by step.</p>
<p>Use the formula for the n-th term of a geometric sequence.</p>
<p>First, calculate the GCD of the numerator and denominator.We can find that both 273375 and 703125 are divisible by 135.Calculating: 273375 ÷ 135 703125 ÷ 135 = 2025 5205 Continuing to simplify, both 2025 and 5205 are divisible by 15: step by step.</p>
<p>5: 273375 ÷ 5 = 54675 -Dividing by 5 again: 54675 ÷ 5 = 10935 -Dividing by 5 again: 10935 ÷ 5 = 2187 -The number 2187 is 3 7 .-Therefore, 273375 = 5 3 • 3 7 .-703125: -Dividing by 5: 703125 ÷ 5 = 140625 -Dividing by 5 again: 140625 ÷ 5 = 28125 -Dividing by 5 again: 28125 ÷ 5 = 5625 Response of Critique under Critic-CoT -Dividing by 5 again: 5625 ÷ 5 = 1125 -Dividing by 5 again: 1125 ÷ 5 = 225 -Dividing by 5 again: 225 ÷ 5 = 45 -Finally, 45 = 3 2 • 5 1 .-Therefore, 703125 = 5 6 • 3 2 .</p>
<p>Use the formula for the n-th term of a geometric sequence.</p>
<p>Simplify the fraction 273375 703125 .To simplify: First, calculate the GCD of the numerator and denominator.We can find that both 273375 and 703125 are divisible by 135.Calculating: fully simplified because 135 and 347 have no common factors.Final Answer: Thus, the eighth term of the sequence is: 135 347</attempt>Prompt of Refinement under Step-wise Label CriticHow do you refine the following attempt with respect to the problem, given the criticism?Multiply the fractions.Calculating the numerator:125 • 2187 = 273375 (correct)Calculating the denominator: 9 • 78125 = 703125 (correct)Step 8: Simplify the fraction 273375 703125 .To simplify: Calculating the GCD shows that both 273375 and 703125 are divisible by 675, so:</p>
<p>if 405 1042 can be simplified further.Both numbers do not have common factors.Final Answer: Thus, the eighth term of the sequence is:</p>
<p>p h a b e t , o r d ( ' a ' ) = 0 , o r d ( ' b ' ) = 1 , . . .o r d ( ' z ' ) = 25 E x a m p l e s s p l i t _ w o r d s ( " H e l l o w o r l d !" ) −&gt; [ " H e l l o " , " w o r l d !" ] s p l i t _ w o r d s ( " H e l l o , w o r l d !" ) −&gt; [ " H e l l o " , " w o r l d !" ] s p l i t _ w o r d s ( " a b c d e f " ) == 3 ' ' ' --Before writing the code, think step by step, marking each step as "Step [i]:" Your final function split_words should be in the form <code>python\n[code]\n</code>, at the end of your response.</p>
<p>s h o u l d r e t u r n t h e number o f l o w e r −c a s e l e t t e r s w i t h odd o r d e r i n t h e a l p h a b e t , o r d ( ' a ' ) = 0 , o r d ( ' b ' ) = 1 , . . .o r d ( ' z ' ) = 25 E x a m p l e s s p l i t _ w o r d s ( " H e l l o w o r l d !" ) −&gt; [ " H e l l o " , " w o r l d !" ] s p l i t _ w o r d s ( " H e l l o , w o r l d !" ) −&gt; [ " H e l l o " , " w o r l d !" ] s p l i t _ w o r d s ( " a b c d e f " ) == 3 ' ' ' </problem> <attempt></p>
<p>Conclusion: Step 4 is incorrect.</criticism> Response of Refinement on HumanEval <correction> Here is the corrected Python function: ```python d e f s p l i t _ w o r d s ( t x t ) :</p>
<p>Table 2 :
2
Solution Accuracy of standardize exam dataset AGIEval (2a), multi-hop reasoning dataset StrategyQA (2b) and code generation dataset HumanEval (2c).Our models generally show robust generalization.ourmodel using the original 7,500/5,000 split from MATH, rather than the extended 12,000/500 split.HumanEval (Chen et al., 2021) contains 164 handwritten Python program problems, which evaluate the code generation capability.</p>
<p>Table 1
1
After Critic-CoT training, our model's top-1 accuracy increases from 89.6% to 91.7% on GSM8K, and from 50.4% to 57.6% on MATH500.Second, the step-wise self-critique ability of the models can further enhance the reasoning performance during inference, via Iterative Refinement and Critic As Filter.With Iterative Refinement, our model achieves 93.3% accuracy on GSM8K and 57.8% on MATH500.Applying Critic As Filter, the performance improves further on the basis of majority vote.On GSM8K it rises from 94.8% with Maj1@96 to 95.4% and on MATH500 it rises from 65.4% with Maj1@512 to 68.4%.Thus, our model's accuracy surpasses strong baselines of Process Reward Model assisted model MATH-Minos,</p>
<p>, which demonstrate the effectiveness of Critic-CoT in improving the model's mathematical reasoning performance.First, strengthening the ability to critique and refinement would not compromise the task-solving performance, but improve it.InternLM-MATH-20B, Math-Shepherd, and rejection sampling finetuning model DART.Overall, the results indicate the model's enhanced abilities to identify mistakes and recover from them, thereby boosting the reasoning performance.</p>
<p>Table 3 :
3
Human Evaluation on the critique and refinement of Critic-CoT Training Data.The automatically constructed data maintain high quality, which can well support the critique training process.
ModelCriticRefineMajority VotePRF1Acc. Init. Acc Ref. Acc. Pass1@N Maj1@N +CriticOutcome Label95.528.9 44.4 88.0 87.789.799.093.693.7Process Label67.922.8 34.1 89.5 88.089.299.093.093.0Only Refine30.011.4 16.6 90.8 92.088.298.995.295.2Only Critic57.131.0 40.2 91.9 91.291.498.994.494.5Stage 142.541.5 42.0 89.3 90.791.198.993.694.2Stage 250.025.0 33.3 85.5 90.591.399.094.494.4Critic-CoT53.358.2 55.7 92.3 91.793.399.194.895.4(a) GSM8KModelCriticRefineMajority VotePRF1Acc. Init. Acc Ref. Acc. Pass1@N Maj1@N +CriticOutcome Label84.4 39.0 53.3 63.0 51.853.684.056.256.2Process Label80.2 35.9 49.6 63.8 50.452.678.649.450.8Only Refine62.3 60.1 61.2 66.0 55.449.890.463.062.8Only Critic67.9 75.4 71.5 71.6 52.855.889.060.660.6Stage 164.6 93.7 76.5 69.0 53.241.290.463.463.0Stage 279.7 45.8 58.2 71.8 57.257.490.464.665.0Critic-CoT66.1 73.7 69.7 72.2 57.657.889.264.666.6(b) MATH500</p>
<p>Table A1 :
A1
2023; Solution Accuracy of MATH.The top-1 accuracy of our method increases from 51.0% to 56.2%, and the effect of iterative refinement is moderate but positive improvement of 0.4%, while the performance gain of the critic filter is larger.
ModelSampling MethodAcc.Llama-3-70B-Instruct (Dubey et al., 2024)-51.0Maj1@9663.5Maj1@51264.3Llama-3.1-70B-Instruct (Dubey et al., 2024)-68.0DeepSeek-V2 Chat-236B (DeepSeek-AI et al., 2024) -53.9Qwen2-72B (Yang et al., 2024)-69.0GPT4-0314 (OpenAI, 2023)-42.5Critic-CoT, Llama-3-70B-Instruct (Ours)-56.2Iterative Refine56.6 ↑ 0.4Maj1@9664.2Critic + Maj1@9665.0 ↑ 0.8Maj1@51264.4Critic + Maj1@512 66.4 ↑ 2.0</p>
<p>Table A2
A2: Iterative Refine on GSM8K (A2a) andMATH500 (A2b).</p>
<p>Table A3
A3GSM8KMATHRound Acc. Avg. Token Avg. Latency (s) Round Acc. Avg. Token Avg. Latency (s)091.7 176.71.156057.6 296.71.941191.7 377.62.470153.4 694.54.544292.6 395.12.585257.2 872.85.710392.7 408.22.671355.2 1003.06.562493.0 418.52.738457.2 1114.47.291593.2 426.32.789557.4 1202.07.864693.2 432.62.830657.6 1282.78.392793.3 436.82.858757.8 1343.18.787893.3 440.42.881857.8 1390.69.098and A3. For Iterative</p>
<p>Table A3 :
A3
Inference cost of Iterative Refine on GSM8K and MATH500.
GSM8KMATHSample Acc. Avg. Token Avg. Latency (s) Sample Acc. Avg. Token Avg. Latency (s)191.7 176.71.156157.6 296.71.941893.7 2974.62.433861.0 4446.333.6361694.2 6261.65.1211663.4 9198.437.5233294.5 12524.110.2423264.8 19228.6915.7266495.0 25057.920.4936466.2 41709.0034.1109695.4 37574.630.7299666.6 60907.4149.811</p>
<p>Table A4 :
A4
Inference cost of Critic As Filter on GSM8K and MATH500.</p>
<p>Table A5 :
A5
presents a single-turn critique-refinement on HumanEval.Python code for extracting the boxed answer.
import redef extract_boxed_expressions_custom(text):stack = []current_expr = ""i = 0while i &lt; len(text):if text[i:i+7] == r"\boxed{":if stack:current_expr += text[i]stack.append("{")i += 7elif text[i] == "{" and stack:stack.append("{")current_expr += text[i]i += 1elif text[i] == "}" and stack:stack.pop()if stack:current_expr += text[i]else:current_expr = re.split('=', current_expr)[-1]return current_expri += 1elif stack:current_expr += text[i]i += 1else:i += 1return "def extract_answer(text):predict_answer_span = re.findall(r'(\boxed{.*})', text, re.DOTALL)[-1]predict_answer = extract_boxed_expressions_custom(predict_answer_span)return predict_answerDataModelIntermediate Accuracy of Correct Answer AttemptGSM8K GPT-3.5-Turbo 97%GSM8K GPT-4-Turbo99%MATHGPT-4-Turbo93%</p>
<p>Table A6 :
A6
Human Evaluation on the Intermediate Steps of Attempts with Correct Answer
Question:</p>
<p>Table A11 :
A11
The prompt for solution sampling.Your reasoning should precede any claims or conclusions you make to avoid unwarranted assertions.-At the end of the evaluation for each step, YOU MUST articulate the conclusion using the format "Conclusion: Step [i] is correct" or "Conclusion: Step [i] is incorrect".Words like "partially correct" are prohibited.
PromptHow do you evaluate the following attempt with respect to the problem?<problem>{problem}</problem><attempt>{attempt}</attempt>--<strong>Notes</strong>:-Please think step by step.-</p>
<p>Table A12 :
A12
The prompt for critique.
PromptHow do you refine the following attempt with respect to the problem, given the criticism?<problem>{problem}</problem><attempt>{attempt}</attempt><criticism>{wrong_step_criticism}</criticism></p>
<p>Table A13 :
A13
The prompt for refinement.</p>
<p>Table A15 :
A15
Full example of Step-wise Label Critic.The problem and the attempt are the same in TableA14, so we skip them.Without CoT critique, the model incorrectly believed that Step 5 is wrong, then made the refinement starting from Step 5, which is unsuccessful.For the whole attempt, you only need to output "Each step from Step 1 to Step 8 is correct."if right, or "Some step from Step 1 to Step 8 is incorrect" if wrong.Do not provide anything else in your output.
Prompt of Critique under Outcome Label CriticHow do you evaluate the following attempt with respect to the problem?<problem>[...]</problem><attempt>[...]</attempt>--<strong>Notes</strong>:-Response of Critique under Outcome Label CriticSome step from Step 1 to Step 8 is incorrect.Prompt of Refinement under Outcome Label CriticHow do you refine the following attempt with respect to the problem, given the criticism?<problem>[...]</problem><attempt>[...]</attempt><criticism>Some step from Step 1 to Step 8 is incorrect.</criticism>Response of Refinement under Outcome Label Critic<correction></p>
<p>Table A16 :
A16
Full example of Final Label Critic.The problem and the attempt are also the same in TableA14.With only the outcome label, the model managed to detect the attempt was incorrect, then made the refinement starting from the first step, which was unsuccessful.
Prompt of Code Generation on HumanEval## Python code problemd e f s p l i t _ w o r d s ( t x t ) :' ' 'G i v e n a s t r i n g o f words , r e t u r n a l i s t o f words s p l i t onw h i t e s p a c e , i f no w h i t e s p a c e s e x i s t s i n t h e t e x t yous h o u l d s p l i t on commas ' , ' i f no commas e x i s t s you s h o u l dr e t u r n t h e number o f l o w e r −c a s e l e t t e r s w i t h odd o r d e r i n t h ea l
Our code and data are available at https://github. com/AlignRM/Critic-CoT
AcknowledgmentWe sincerely thank the reviewers for their insightful comments and valuable suggestions.This work was supported by Beijing Natural Science Foundation (L243006), Beijing Municipal Science and Technology Project (Nos.Z231100010323002), and the Natural Science Foundation of China (No.  62476265, 62306303).Training language models to follow instructions with human feedback.In Advances in Neural Information Processing Systems, volume 35, pages 27730-27744.Curran Associates, Inc. : breaking the gpu memory wall for extreme scale deep learning.In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, The generative AI paradox: "what it can create, it may not understand".In The Twelfth International Conference on Learning Representations.Ethics StatementAll the data and models are acquired from public datasets and pre-trained models, and no human annotators are involved during the data construction procedure.Question:Carlos is planting a lemon tree.The tree will cost $90 to plant.Each year it will grow 7 lemons, which he can sell for $1.5 each.It costs $3 a year to water and feed the tree.How many years will it take before he starts earning money on the lemon tree?Model Attempt:Step 1: Calculate the annual income from selling lemons.This is done by multiplying the number of lemons grown per year (7) by the price per lemon ($1.5).So, 7 * $1.5 = $10.50.Step 2: Subtract the annual cost of watering and feeding the tree ($3) from the annual income from selling lemons ($10.50).So, $10.50 -$3 = $7.50.This is the net income per year after expenses.Step 3: Determine when Carlos will start earning money back.This is done by dividing the initial cost of planting the tree ($90) by the net income per year ($7.50).So, $90 / $7.50 = 12 years.Therefore, it will take Carlos 12 years before he starts earning money on the lemon tree.Model Critique:Step 1: The attempt calculates the annual income from selling lemons by multiplying the number of lemons grown per year (7) by the price per lemon ($1.5), resulting in $10.50.Conclusion:Step 1 is correct.Step 2: The attempt subtracts the annual cost of watering and feeding the tree ($3) from the annual income from selling lemons ($10.50),resulting in a net income per year of $7.50.Conclusion:Step 2 is correct.Step 3: The attempt divides the initial cost of planting the tree ($90) by the net income per year ($7.50), resulting in 12 years.However, this calculation does not account for the fact that Carlos will start earning money back in the first year, even though he won't break even until the 13th year.Conclusion:Step 3 is incorrect.Model Refinement:Step 3 (Corrected): Determine when Carlos will start earning money back.This is done by dividing the initial cost of planting the tree ($90) by the net income per year ($7.50).So, $90 / $7.50 = 12 years.However, since Carlos will start earning money back in the first year, we need to add one more year to this calculation.Therefore, it will take Carlos 13 years before he starts earning money on the lemon tree.The GCD can now be derived from the lowest powers of the common factors: -For 5: 5 3 -For 3: 3 2 Thus, the GCD is: Step 9: Further simplify 729 1875 .The numerator 729 is 3 6 and the denominator 1875 is 3 • 625.The GCD is 3. Calculating:729 ÷ 3 1875 ÷ 3 = 243 625 This fraction is fully simplified.Final Answer: Thus, the eighth term of the sequence is:Response of Critique under Step-wise Label CriticStep 1 is correct.Step 2 is correct.Step 3 is correct.Step 4 is correct.Step 5 is incorrect.
A I , Meta , Llama 3.1 model card. 2024</p>
<p>Self-RAG: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023Preprint</p>
<p>Weak-tostrong generalization: Eliciting strong capabilities with weak supervision. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, Jeff Wu ; Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, Le Sun, Hongyu Lin, Bowen Yu, arXiv:2312.09390arXiv:2406.012522023. 2024PreprintTowards scalable automated alignment of llms: A survey</p>
<p>Chateval: Towards better LLM-based evaluators through multi-agent debate. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Towards mitigating LLM hallucination via self reflection. Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung, 10.18653/v1/2023.findings-emnlp.123Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Thinking Fast and Slow. Farrar. Daniel Kahneman, 2011Straus and Giroux</p>
<p>The Concept of Irony, with Continual Reference to Socrates/Notes of Schelling's Berlin Lectures. Søren Kierkegaard, 1989Princeton University PressIIKierkegaard's Writings</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Marcus Mcaleer, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, 10.1145/3600006.3613165Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23. the 29th Symposium on Operating Systems Principles, SOSP '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Scalable agent alignment via reward modeling: a research direction. Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg, arXiv:1811.078712018Preprint</p>
<p>Chain of thought empowers transformers to solve inherently serial problems. Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu, 10.18653/v1/2024.emnlp-main.992Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2024</p>
<p>CriticBench: Benchmarking LLMs for critique-correct reasoning. Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang, 10.18653/v1/2024.findings-acl.91Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Transformers: State-of-the-art natural language processing. Alexander M Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>Large language models can self-correct with key condition verification. Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, Meng Jiang, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Mirror: Multiple-perspective selfreflection method for knowledge-rich reasoning. Hanqi Yan, Qinglin Zhu, Xinyu Wang, Lin Gui, Yulan He, 10.18653/v1/2024.acl-long.382Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, arXiv:2407.10671Qwen2 technical report. 2024Preprint</p>
<p>Internlmmath: Open math large language models toward verifiable reasoning. Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, 10.48550/ARXIV.2402.06332CoRR, abs/2402.06332Xipeng Qiu, Jiayu Wang, Kai Chen, and Dahua Lin2024</p>
<p>Distilling system 2 into system 1. Ping Yu, Jing Xu, Jason Weston, Ilia Kulikov, arXiv:2407.060232024Preprint</p>
<p>Self-rewarding language models. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, Jason E Weston, Forty-first International Conference on Machine Learning. 2024</p>
<p>STar: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 2022</p>
<p>Mr-gsm8k: A metareasoning benchmark for large language model evaluation. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia, 10.48550/ARXIV.2312.17080CoRR, abs/2312.170802023</p>
<p>Generative verifiers: Reward modeling as next-token prediction. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal, arXiv:2408.152402024aPreprint</p>
<p>Self-contrast: Better reflection through inconsistent solving perspectives. Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu, 10.18653/v1/2024.acl-long.197Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024b1</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Agieval: A humancentric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.063642023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
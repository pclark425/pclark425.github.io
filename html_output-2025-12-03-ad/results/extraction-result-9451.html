<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9451 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9451</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9451</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-197ba7bbfdbb052b0770088815c110774220f397</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/197ba7bbfdbb052b0770088815c110774220f397" target="_blank">Prompting PaLM for Translation: Assessing Strategies and Performance</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> An in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation performance among similarly-trained LLMs to date, investigates various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM’s MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM’s MT output which reveals some interesting properties and prospects for future work.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9451.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9451.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>shot_count</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number of few-shot examples (shot count)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Effect of varying the number of in-context translation examples (0 to 10) on PaLM's sentence-level MT quality; performance improves with more shots with diminishing returns after ~5 shots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentence-level Machine Translation (WMT newstest sets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate individual sentences between English and German/French/Chinese, evaluated on WMT test sets using BLEURT, BLEU and human MQM.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompting with n examples prepended in a fixed template (tested n = 0,1,2,5,10). Examples are concrete source-target sentence pairs (language name headers included). Decoding used greedy search (temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot (0-shot) and other shot counts (1,2,5,10); experiments compared performance across these shot counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (en->de, Language template): BLEURT by shots: 0-shot 63.9, 1-shot 69.1, 2-shot 71.7, 5-shot 73.6, 10-shot 74.4 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>5-shot vs 0-shot: +9.7 BLEURT (63.9 -> 73.6) for en->de (Language template); gains diminish after 5 shots (5->10: +0.8 BLEURT).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Substantial improvement from 0 to 5 shots (~+9.7 BLEURT in en->de example); marginal gain beyond 5 shots (~+0.8 BLEURT from 5->10).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>More in-context examples provide the model with clearer demonstrations of the mapping from source to target; improvements saturate because additional examples add diminishing new information within the model's context window.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Shot counts tested: 0,1,2,5,10. Template variants also tested. Results averaged/median across multiple seeds; greedy decoding (stop at newline). 5-shot chosen as standard for many experiments due to diminishing returns and context-length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting PaLM for Translation: Assessing Strategies and Performance', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9451.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9451.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>template_type</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt template style</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different hard prompt templates (headers and wording) were tested (Language names, two-letter codes, header text, fully textual prompts, language names in target language, and no labels) to assess effect on PaLM MT performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentence-level Machine Translation (WMT newstest sets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate individual sentences between English and German, evaluated with BLEURT/BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Hard prompts with fixed template variants: 'Language' (e.g., 'English: ...' and 'German: ...'), 'Codes' (en/de), 'Header' ('Translate following sentences:' + Language), 'Textual' (natural-language 'Translate X from English into German: Y'), 'Deutsch' (language names in German), and 'None' (no labels). Used in few-shot settings (varied shot counts).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared the above template variants across the same shot counts (0,1,2,5,10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>5-shot BLEURT (en->de): Language 73.6, Codes 73.4, Header 73.4, Textual 73.0, Deutsch 73.5, None 73.0 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>At low shot counts templates produce large differences (e.g., 0-shot ranged from 3.2 to 72.6 across templates), but at 5+ shots differences are small (~±0.6 BLEURT).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large effect at very low shots (0-shot: up to ~69 BLEURT difference between best and worst templates), but small effect at 5 shots (max ~0.6 BLEURT).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The exact wording/labels matter when demonstrations are scarce because they set the task framing; with multiple examples (≥5) the examples themselves largely define the task, making template wording less important.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Template variants evaluated with 0/1/2/5/10 shots; results shown in Table 7. Random selection of examples used for these comparisons; median of 5 runs reported. Based on this, authors adopted simple 'Language' template and 5-shot regime.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting PaLM for Translation: Assessing Strategies and Performance', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9451.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9451.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>example_selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Example selection strategy (random vs kNN) and pool choice</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of strategies for choosing few-shot examples: random selection vs k-nearest-neighbour retrieval (kNN) from different pools (WMT-full, WMT-dev, high-end); pool quality was found more important than retrieval proximity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentence-level Machine Translation (WMT newstest sets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate individual sentences between English and German/Chinese/French, evaluated with BLEURT, BLEU, and MQM human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompting with 5-shot sentence pairs (WMT pools) or 1-shot paragraphs (high-end pool); examples selected either randomly from a pool or via kNN retrieval on the source side using BOW or RoBERTa embeddings. Pools: WMT-full (large, mixed quality), WMT-dev (smaller, higher-quality, target-original), high-end (manually curated bilingual professionally-edited articles).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random selection vs kNN (BOW) vs kNN (RoBERTa) across pools WMT-full and WMT-dev, and random (1-shot paragraphs) from high-end pool; also comparisons to SOTA and Google Translate baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (en->de): WMT-full random BLEURT 71.8, WMT-full kNN BOW 71.7, WMT-full kNN RoBERTa 73.0, WMT-dev random 74.8, WMT-dev kNN RoBERTa 74.8 (Table 3). Across language pairs (Table 4): de->en WMT-full random 74.7 BLEURT vs WMT-dev random 75.9 vs high-end random 75.8; kNN often not superior to random on WMT-dev/high-end.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Pool effect: WMT-dev (or high-end) outperforms WMT-full by about ~1.0 BLEURT in many settings (e.g., en->de 74.8 vs 71.8 => ~+3.0 BLEURT in that block). kNN (RoBERTa) sometimes improves over BOW (e.g., WMT-full en->de: +1.2 BLEURT vs BOW) but does not consistently beat random selection from high-quality pools.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Pool quality effect typically ~+1.0 BLEURT (WMT-dev/high-end vs WMT-full) and up to ~+3 BLEURT in some blocks; kNN-vs-random effects are small/inconsistent (±~0.0–1.3 BLEURT) and can be negative when kNN retrieves noisy/non-parallel examples.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that example quality (correctness and target-side quality) matters more than lexical or semantic proximity to the source; kNN retrieval matches only on source side and is therefore vulnerable to corpus alignment noise concentrated in documents, making it less robust than random draws from a smaller high-quality pool.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>WMT pools: WMT-full (large crawled data), WMT-dev (development sets, target-original), high-end (curated bilingual paragraphs). Retrieval: kNN via ScaNN using cosine/Euclidean on BOW counts or RoBERTa embeddings; 5-shot for WMT pools, 1-shot paragraphs for high-end; random runs: 5 seeds, median BLEURT reported; human MQM used for selected comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting PaLM for Translation: Assessing Strategies and Performance', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9451.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9451.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN_representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>kNN retrieval representation: BOW vs RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of two sentence representations for kNN prompt retrieval: lexical Bag-of-Words (BOW) with cosine distance vs RoBERTa embeddings with Euclidean distance, assessing impact on retrieved-example usefulness and downstream translation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentence-level Machine Translation (WMT newstest sets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate individual sentences between English and German/Chinese/French, using retrieved in-context examples chosen by different representation+distance measures.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>kNN retrieval on source sentences using either: (a) sparse BOW counts with cosine distance (focuses on surface lexical similarity), or (b) RoBERTa sentence embeddings with Euclidean distance (captures semantics). Retrieved examples are inserted as few-shot demonstrations in the fixed template.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>kNN-BOW vs kNN-RoBERTa vs random selection (from same pool).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (en->de, WMT-full): random 71.8 BLEURT, kNN BOW 71.7, kNN RoBERTa 73.0 (Table 3). For de->en (WMT-full): random 74.8, kNN BOW 72.7, kNN RoBERTa 73.8.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>RoBERTa often outperforms BOW by up to ~1.3 BLEURT (e.g., en->de WMT-full), but RoBERTa does not consistently outperform random selection from a high-quality pool (WMT-dev/high-end).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>RoBERTa vs BOW effect typically ~+1 BLEURT in favorable settings; effect relative to random is inconsistent (<±1.5 BLEURT).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>RoBERTa embeddings better capture sentence semantics and thus retrieve subject-matter-relevant prompts, but retrieval still matches only on source-side and can bring along parallel/noise issues; hence representation helps retrieval quality but does not guarantee better downstream translation if pool quality is low.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>kNN implemented with ScaNN; BOW vectors (sparse counts) with cosine distance; RoBERTa multilingual transformer embeddings with Euclidean distance; retrieval performed over large pools (WMT-full) and smaller dev/high-end pools; results averaged across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting PaLM for Translation: Assessing Strategies and Performance', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9451.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9451.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>fixed_vs_random_prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fixed maximum-likelihood prompt vs random per-input prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison between choosing a single high-quality prompt for all inputs (selected by maximizing held-out probability) versus randomly sampling prompts per input from the high-end pool; fixed prompt often matched or outperformed random selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentence-level Machine Translation (WMT newstest sets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate sentences using 1-shot paragraph prompts drawn from a curated high-end pool; compare using a single selected prompt for all inputs vs random selection per input.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>1-shot paragraph prompts from high-end pool. Fixed prompt chosen by computing PaLM probability of held-out paragraphs conditioned on each candidate prompt, selecting the highest-probability prompt; compared to random 1-shot selection per input.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Fixed maximum-likelihood prompt vs random prompt selection (multiple runs to get min/avg/max performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 18 (examples): en->de fixed min BLEURT 74.5 vs random avg 74.7 (min/avg/max: fixed 74.5; random min 74.7 avg 75.0 max 75.0 in table formatting varied by LP). For most language pairs fixed prompt performed as well or better than the average of random runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Fixed prompt performance comparable to or slightly better than random average in most language pairs; results vary by language (Chinese->English was an exception in their held-out selection).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Differences small (on the order of ≤~0.5 BLEURT in reported comparisons), but fixed prompt reduces variability and can be safer.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>A single high-quality prompt that induces high model probability on held-out data can provide stable, high-quality conditioning and avoid run-to-run variance introduced by per-input random sampling; model-probability is a reasonable heuristic for prompt quality.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Experiments run on high-end pool with 1-shot paragraph prompts; prompt selection by held-out likelihood; comparisons reported as min/avg/max over 5 random runs; measured on standard test sets (WMT21 for German/Chinese, WMT14 for French).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting PaLM for Translation: Assessing Strategies and Performance', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9451.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9451.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>prompt_pool_domain_quality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt pool domain and quality (WMT-full vs WMT-dev vs high-end)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Impact of the corpus from which few-shot examples are drawn: large noisy crawled corpora (WMT-full) vs smaller higher-quality development sets (WMT-dev) vs curated high-end bilingual texts; higher-quality pools lead to better translation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentence-level Machine Translation (WMT newstest sets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate sentences using few-shot prompts sampled from different pools of parallel data; evaluate with BLEURT/BLEU and MQM.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>5-shot (sentences) prompts sampled randomly or via retrieval from one of three pools: WMT-full (large, mixed-quality crawled), WMT-dev (smaller, higher-quality, target-original), high-end (manually curated professional bilingual paragraphs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>WMT-full vs WMT-dev vs high-end (random selection and kNN retrieval variants compared).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (de->en, Table 4): WMT-full random BLEURT 74.7, WMT-dev random 75.9, high-end random 75.8; en->de: WMT-full random 73.7, WMT-dev random 74.8, high-end 74.7.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Switching from WMT-full to WMT-dev/high-end yields roughly +1.0 BLEURT (varies by pair), and MQM human scores likewise improve, indicating fewer major accuracy/omission errors.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Typical improvement from poor/large-crawled pool to smaller high-quality pool is about +1 BLEURT, with effects visible in MQM accuracy components as reductions in major omission errors.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Example quality (correctness of alignments and target-language naturalness) is the main determinant of prompt usefulness; large pools increase chance of close lexical matches but also increase chance of noisy/bad translations which can mislead the model.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Pool sizes reported in paper; WMT-dev curated to be target-original and closer domain match to test sets; high-end pool uses paragraph pairs from professionally edited bilingual articles; experiments used 5-shot for WMT pools, 1-shot paragraphs for high-end; BLEURT and MQM used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting PaLM for Translation: Assessing Strategies and Performance', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9451.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9451.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>decoding_temperature</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoding strategy: greedy (temperature=0) vs sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Effect of stochastic sampling temperature on generation quality: authors found non-zero sampling temperatures degraded translation quality compared to greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentence-level Machine Translation (WMT newstest sets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate sentences conditioned on prompts; compare deterministic greedy decoding to sampling with non-zero temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Greedy decoding (sampling temperature=0) used as default; experiments note testing of sampling temperatures != 0.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Greedy (temp=0) vs sampling (temp>0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No numeric metric values reported, but authors state: 'We found that using a sampling temperature other than 0 tended to degrade translation quality.' (Section 3, footnote).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative/aggregate negative effect of sampling vs greedy reported (no BLEURT/BLEU numbers presented).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Sampling introduces variance and can produce less faithful or lower-quality translations for this conditional MT setup; greedy decoding is more stable for sentence-level translation conditioned on few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors used greedy decoding as default for all experiments; sampling temperature experiments were performed in preliminary testing and led to lower quality, so not used in main reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompting PaLM for Translation: Assessing Strategies and Performance', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Palm: Scaling language modeling with pathways <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>What makes good in-context examples for GPT-3? <em>(Rating: 2)</em></li>
                <li>Incontext examples selection for machine translation <em>(Rating: 2)</em></li>
                <li>Prompt programming for large language models: Beyond the few-shot paradigm <em>(Rating: 1)</em></li>
                <li>Using natural language prompts for machine translation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9451",
    "paper_id": "paper-197ba7bbfdbb052b0770088815c110774220f397",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "shot_count",
            "name_full": "Number of few-shot examples (shot count)",
            "brief_description": "Effect of varying the number of in-context translation examples (0 to 10) on PaLM's sentence-level MT quality; performance improves with more shots with diminishing returns after ~5 shots.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "540B",
            "task_name": "Sentence-level Machine Translation (WMT newstest sets)",
            "task_description": "Translate individual sentences between English and German/French/Chinese, evaluated on WMT test sets using BLEURT, BLEU and human MQM.",
            "presentation_format": "Few-shot prompting with n examples prepended in a fixed template (tested n = 0,1,2,5,10). Examples are concrete source-target sentence pairs (language name headers included). Decoding used greedy search (temperature=0).",
            "comparison_format": "Zero-shot (0-shot) and other shot counts (1,2,5,10); experiments compared performance across these shot counts.",
            "performance": "Example (en-&gt;de, Language template): BLEURT by shots: 0-shot 63.9, 1-shot 69.1, 2-shot 71.7, 5-shot 73.6, 10-shot 74.4 (Table 7).",
            "performance_comparison": "5-shot vs 0-shot: +9.7 BLEURT (63.9 -&gt; 73.6) for en-&gt;de (Language template); gains diminish after 5 shots (5-&gt;10: +0.8 BLEURT).",
            "format_effect_size": "Substantial improvement from 0 to 5 shots (~+9.7 BLEURT in en-&gt;de example); marginal gain beyond 5 shots (~+0.8 BLEURT from 5-&gt;10).",
            "explanation_or_hypothesis": "More in-context examples provide the model with clearer demonstrations of the mapping from source to target; improvements saturate because additional examples add diminishing new information within the model's context window.",
            "null_or_negative_result": false,
            "experimental_details": "Shot counts tested: 0,1,2,5,10. Template variants also tested. Results averaged/median across multiple seeds; greedy decoding (stop at newline). 5-shot chosen as standard for many experiments due to diminishing returns and context-length constraints.",
            "uuid": "e9451.0",
            "source_info": {
                "paper_title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "template_type",
            "name_full": "Prompt template style",
            "brief_description": "Different hard prompt templates (headers and wording) were tested (Language names, two-letter codes, header text, fully textual prompts, language names in target language, and no labels) to assess effect on PaLM MT performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "540B",
            "task_name": "Sentence-level Machine Translation (WMT newstest sets)",
            "task_description": "Translate individual sentences between English and German, evaluated with BLEURT/BLEU.",
            "presentation_format": "Hard prompts with fixed template variants: 'Language' (e.g., 'English: ...' and 'German: ...'), 'Codes' (en/de), 'Header' ('Translate following sentences:' + Language), 'Textual' (natural-language 'Translate X from English into German: Y'), 'Deutsch' (language names in German), and 'None' (no labels). Used in few-shot settings (varied shot counts).",
            "comparison_format": "Compared the above template variants across the same shot counts (0,1,2,5,10).",
            "performance": "5-shot BLEURT (en-&gt;de): Language 73.6, Codes 73.4, Header 73.4, Textual 73.0, Deutsch 73.5, None 73.0 (Table 7).",
            "performance_comparison": "At low shot counts templates produce large differences (e.g., 0-shot ranged from 3.2 to 72.6 across templates), but at 5+ shots differences are small (~±0.6 BLEURT).",
            "format_effect_size": "Large effect at very low shots (0-shot: up to ~69 BLEURT difference between best and worst templates), but small effect at 5 shots (max ~0.6 BLEURT).",
            "explanation_or_hypothesis": "The exact wording/labels matter when demonstrations are scarce because they set the task framing; with multiple examples (≥5) the examples themselves largely define the task, making template wording less important.",
            "null_or_negative_result": false,
            "experimental_details": "Template variants evaluated with 0/1/2/5/10 shots; results shown in Table 7. Random selection of examples used for these comparisons; median of 5 runs reported. Based on this, authors adopted simple 'Language' template and 5-shot regime.",
            "uuid": "e9451.1",
            "source_info": {
                "paper_title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "example_selection",
            "name_full": "Example selection strategy (random vs kNN) and pool choice",
            "brief_description": "Comparison of strategies for choosing few-shot examples: random selection vs k-nearest-neighbour retrieval (kNN) from different pools (WMT-full, WMT-dev, high-end); pool quality was found more important than retrieval proximity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "540B",
            "task_name": "Sentence-level Machine Translation (WMT newstest sets)",
            "task_description": "Translate individual sentences between English and German/Chinese/French, evaluated with BLEURT, BLEU, and MQM human evaluation.",
            "presentation_format": "Few-shot prompting with 5-shot sentence pairs (WMT pools) or 1-shot paragraphs (high-end pool); examples selected either randomly from a pool or via kNN retrieval on the source side using BOW or RoBERTa embeddings. Pools: WMT-full (large, mixed quality), WMT-dev (smaller, higher-quality, target-original), high-end (manually curated bilingual professionally-edited articles).",
            "comparison_format": "Random selection vs kNN (BOW) vs kNN (RoBERTa) across pools WMT-full and WMT-dev, and random (1-shot paragraphs) from high-end pool; also comparisons to SOTA and Google Translate baselines.",
            "performance": "Example (en-&gt;de): WMT-full random BLEURT 71.8, WMT-full kNN BOW 71.7, WMT-full kNN RoBERTa 73.0, WMT-dev random 74.8, WMT-dev kNN RoBERTa 74.8 (Table 3). Across language pairs (Table 4): de-&gt;en WMT-full random 74.7 BLEURT vs WMT-dev random 75.9 vs high-end random 75.8; kNN often not superior to random on WMT-dev/high-end.",
            "performance_comparison": "Pool effect: WMT-dev (or high-end) outperforms WMT-full by about ~1.0 BLEURT in many settings (e.g., en-&gt;de 74.8 vs 71.8 =&gt; ~+3.0 BLEURT in that block). kNN (RoBERTa) sometimes improves over BOW (e.g., WMT-full en-&gt;de: +1.2 BLEURT vs BOW) but does not consistently beat random selection from high-quality pools.",
            "format_effect_size": "Pool quality effect typically ~+1.0 BLEURT (WMT-dev/high-end vs WMT-full) and up to ~+3 BLEURT in some blocks; kNN-vs-random effects are small/inconsistent (±~0.0–1.3 BLEURT) and can be negative when kNN retrieves noisy/non-parallel examples.",
            "explanation_or_hypothesis": "Authors hypothesize that example quality (correctness and target-side quality) matters more than lexical or semantic proximity to the source; kNN retrieval matches only on source side and is therefore vulnerable to corpus alignment noise concentrated in documents, making it less robust than random draws from a smaller high-quality pool.",
            "null_or_negative_result": true,
            "experimental_details": "WMT pools: WMT-full (large crawled data), WMT-dev (development sets, target-original), high-end (curated bilingual paragraphs). Retrieval: kNN via ScaNN using cosine/Euclidean on BOW counts or RoBERTa embeddings; 5-shot for WMT pools, 1-shot paragraphs for high-end; random runs: 5 seeds, median BLEURT reported; human MQM used for selected comparisons.",
            "uuid": "e9451.2",
            "source_info": {
                "paper_title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "kNN_representation",
            "name_full": "kNN retrieval representation: BOW vs RoBERTa",
            "brief_description": "Comparison of two sentence representations for kNN prompt retrieval: lexical Bag-of-Words (BOW) with cosine distance vs RoBERTa embeddings with Euclidean distance, assessing impact on retrieved-example usefulness and downstream translation quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "540B",
            "task_name": "Sentence-level Machine Translation (WMT newstest sets)",
            "task_description": "Translate individual sentences between English and German/Chinese/French, using retrieved in-context examples chosen by different representation+distance measures.",
            "presentation_format": "kNN retrieval on source sentences using either: (a) sparse BOW counts with cosine distance (focuses on surface lexical similarity), or (b) RoBERTa sentence embeddings with Euclidean distance (captures semantics). Retrieved examples are inserted as few-shot demonstrations in the fixed template.",
            "comparison_format": "kNN-BOW vs kNN-RoBERTa vs random selection (from same pool).",
            "performance": "Example (en-&gt;de, WMT-full): random 71.8 BLEURT, kNN BOW 71.7, kNN RoBERTa 73.0 (Table 3). For de-&gt;en (WMT-full): random 74.8, kNN BOW 72.7, kNN RoBERTa 73.8.",
            "performance_comparison": "RoBERTa often outperforms BOW by up to ~1.3 BLEURT (e.g., en-&gt;de WMT-full), but RoBERTa does not consistently outperform random selection from a high-quality pool (WMT-dev/high-end).",
            "format_effect_size": "RoBERTa vs BOW effect typically ~+1 BLEURT in favorable settings; effect relative to random is inconsistent (&lt;±1.5 BLEURT).",
            "explanation_or_hypothesis": "RoBERTa embeddings better capture sentence semantics and thus retrieve subject-matter-relevant prompts, but retrieval still matches only on source-side and can bring along parallel/noise issues; hence representation helps retrieval quality but does not guarantee better downstream translation if pool quality is low.",
            "null_or_negative_result": false,
            "experimental_details": "kNN implemented with ScaNN; BOW vectors (sparse counts) with cosine distance; RoBERTa multilingual transformer embeddings with Euclidean distance; retrieval performed over large pools (WMT-full) and smaller dev/high-end pools; results averaged across runs.",
            "uuid": "e9451.3",
            "source_info": {
                "paper_title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "fixed_vs_random_prompt",
            "name_full": "Fixed maximum-likelihood prompt vs random per-input prompts",
            "brief_description": "Comparison between choosing a single high-quality prompt for all inputs (selected by maximizing held-out probability) versus randomly sampling prompts per input from the high-end pool; fixed prompt often matched or outperformed random selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "540B",
            "task_name": "Sentence-level Machine Translation (WMT newstest sets)",
            "task_description": "Translate sentences using 1-shot paragraph prompts drawn from a curated high-end pool; compare using a single selected prompt for all inputs vs random selection per input.",
            "presentation_format": "1-shot paragraph prompts from high-end pool. Fixed prompt chosen by computing PaLM probability of held-out paragraphs conditioned on each candidate prompt, selecting the highest-probability prompt; compared to random 1-shot selection per input.",
            "comparison_format": "Fixed maximum-likelihood prompt vs random prompt selection (multiple runs to get min/avg/max performance).",
            "performance": "Table 18 (examples): en-&gt;de fixed min BLEURT 74.5 vs random avg 74.7 (min/avg/max: fixed 74.5; random min 74.7 avg 75.0 max 75.0 in table formatting varied by LP). For most language pairs fixed prompt performed as well or better than the average of random runs.",
            "performance_comparison": "Fixed prompt performance comparable to or slightly better than random average in most language pairs; results vary by language (Chinese-&gt;English was an exception in their held-out selection).",
            "format_effect_size": "Differences small (on the order of ≤~0.5 BLEURT in reported comparisons), but fixed prompt reduces variability and can be safer.",
            "explanation_or_hypothesis": "A single high-quality prompt that induces high model probability on held-out data can provide stable, high-quality conditioning and avoid run-to-run variance introduced by per-input random sampling; model-probability is a reasonable heuristic for prompt quality.",
            "null_or_negative_result": false,
            "experimental_details": "Experiments run on high-end pool with 1-shot paragraph prompts; prompt selection by held-out likelihood; comparisons reported as min/avg/max over 5 random runs; measured on standard test sets (WMT21 for German/Chinese, WMT14 for French).",
            "uuid": "e9451.4",
            "source_info": {
                "paper_title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "prompt_pool_domain_quality",
            "name_full": "Prompt pool domain and quality (WMT-full vs WMT-dev vs high-end)",
            "brief_description": "Impact of the corpus from which few-shot examples are drawn: large noisy crawled corpora (WMT-full) vs smaller higher-quality development sets (WMT-dev) vs curated high-end bilingual texts; higher-quality pools lead to better translation performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "540B",
            "task_name": "Sentence-level Machine Translation (WMT newstest sets)",
            "task_description": "Translate sentences using few-shot prompts sampled from different pools of parallel data; evaluate with BLEURT/BLEU and MQM.",
            "presentation_format": "5-shot (sentences) prompts sampled randomly or via retrieval from one of three pools: WMT-full (large, mixed-quality crawled), WMT-dev (smaller, higher-quality, target-original), high-end (manually curated professional bilingual paragraphs).",
            "comparison_format": "WMT-full vs WMT-dev vs high-end (random selection and kNN retrieval variants compared).",
            "performance": "Example (de-&gt;en, Table 4): WMT-full random BLEURT 74.7, WMT-dev random 75.9, high-end random 75.8; en-&gt;de: WMT-full random 73.7, WMT-dev random 74.8, high-end 74.7.",
            "performance_comparison": "Switching from WMT-full to WMT-dev/high-end yields roughly +1.0 BLEURT (varies by pair), and MQM human scores likewise improve, indicating fewer major accuracy/omission errors.",
            "format_effect_size": "Typical improvement from poor/large-crawled pool to smaller high-quality pool is about +1 BLEURT, with effects visible in MQM accuracy components as reductions in major omission errors.",
            "explanation_or_hypothesis": "Example quality (correctness of alignments and target-language naturalness) is the main determinant of prompt usefulness; large pools increase chance of close lexical matches but also increase chance of noisy/bad translations which can mislead the model.",
            "null_or_negative_result": false,
            "experimental_details": "Pool sizes reported in paper; WMT-dev curated to be target-original and closer domain match to test sets; high-end pool uses paragraph pairs from professionally edited bilingual articles; experiments used 5-shot for WMT pools, 1-shot paragraphs for high-end; BLEURT and MQM used for evaluation.",
            "uuid": "e9451.5",
            "source_info": {
                "paper_title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "decoding_temperature",
            "name_full": "Decoding strategy: greedy (temperature=0) vs sampling",
            "brief_description": "Effect of stochastic sampling temperature on generation quality: authors found non-zero sampling temperatures degraded translation quality compared to greedy decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "540B",
            "task_name": "Sentence-level Machine Translation (WMT newstest sets)",
            "task_description": "Translate sentences conditioned on prompts; compare deterministic greedy decoding to sampling with non-zero temperature.",
            "presentation_format": "Greedy decoding (sampling temperature=0) used as default; experiments note testing of sampling temperatures != 0.",
            "comparison_format": "Greedy (temp=0) vs sampling (temp&gt;0).",
            "performance": "No numeric metric values reported, but authors state: 'We found that using a sampling temperature other than 0 tended to degrade translation quality.' (Section 3, footnote).",
            "performance_comparison": "Qualitative/aggregate negative effect of sampling vs greedy reported (no BLEURT/BLEU numbers presented).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Sampling introduces variance and can produce less faithful or lower-quality translations for this conditional MT setup; greedy decoding is more stable for sentence-level translation conditioned on few-shot examples.",
            "null_or_negative_result": true,
            "experimental_details": "Authors used greedy decoding as default for all experiments; sampling temperature experiments were performed in preliminary testing and led to lower quality, so not used in main reported results.",
            "uuid": "e9451.6",
            "source_info": {
                "paper_title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Palm: Scaling language modeling with pathways",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "What makes good in-context examples for GPT-3?",
            "rating": 2
        },
        {
            "paper_title": "Incontext examples selection for machine translation",
            "rating": 2
        },
        {
            "paper_title": "Prompt programming for large language models: Beyond the few-shot paradigm",
            "rating": 1
        },
        {
            "paper_title": "Using natural language prompts for machine translation",
            "rating": 1
        }
    ],
    "cost": 0.01335945,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Prompting PaLM for Translation: Assessing Strategies and Performance</h1>
<p>David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, George Foster Google<br>{vilar, freitag, colincherry, jmluo, vratnakar, fosterg}@google.com</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an indepth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM's MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM's MT output which reveals some interesting properties and prospects for future work.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) trained to predict the next token from a lengthy context have demonstrated impressive machine translation capabilities, despite being trained on corpora that are overwhelmingly English, with no intentionallyincluded parallel text. In this paper, we carry out an in-depth investigation into the translation capabilities of LLMs, testing different prompting strategies and carefully assessing the resulting performance. We study the recently-introduced PaLM model (Chowdhery et al., 2022), a 540B-parameter decoder-only language model trained on a heavily English-centric, multilingual corpus. It has achieved the strongest MT results among LLMs trained on non-parallel multilingual corpora.</p>
<p>To ensure a fair assessment of PaLM's MT capability, we begin with an exploration of example selection methods for use with fixed prompt templates. We vary both the pool from which examples are chosen and the method for choosing
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Histogram of the sentence-level BleUrt difference between two different 5-shot PaLM runs using the random prompt selection method from the original paper on a corpus of 1000 sentences. Each bar corresponds to a difference range of 1 BLEURT point. A majority of sentences (516) show a difference of more than 1 BLEURT point, demonstrating that the choice of prompt can strongly affect translation quality.
them, comparing standard random selection to $k$ -nearest-neighbour ( $k \mathrm{NN}$ ) selection that customizes prompts for specific inputs. Figure 1 highlights the importance of example selection by showing that two randomly-selected sets of examples can result in significantly different distributions of sentencelevel BLEURT scores.</p>
<p>Although Chowdhery et al. (2022) report interesting results on low-resource and non-English language pairs, their most striking findings concern high-resource pairs. Accordingly, we limit our investigation to French, German, and Chinese translation to and from English. We evaluate sentencelevel translation quality using recommended practices for high-quality MT, specifically: (i) we use recent WMT test sets to guard against train/test data leakage, and to facilitate comparison with state-of-the-art (SOTA) MT systems; (ii) we use a SOTA automatic metric (BLEURT) instead of BLEU which has been demonstrated to be suboptimal for high-quality translations (Kocmi et al.,</p>
<p>2021; Freitag et al., 2021b); and (iii) we conduct an expert-based human evaluation with detailed categories to characterize the error patterns of the automatically generated translations.</p>
<p>Our contributions are as follows:</p>
<ul>
<li>We carry out the first systematic study of LLM prompting for MT, exploring both the example candidate pool and the selection strategy. We find that the quality of examples matters more than the domain from which they are drawn or their lexico-semantic proximity to the current input.</li>
<li>We evaluate the translation capability of LLMs with the procedure currently recommended by the MT community. We find that, although impressive, the sentence-level translation capacity of LLMs still lags behind SOTA MT.</li>
</ul>
<h2>2 Related Work</h2>
<p>Inspired by the findings of Radford et al. (2019); Brown et al. (2020), prompting strategies for LLMs have become a topic of intense interest, generating work across a broad spectrum of methods and applications (Liu et al., 2021). A basic distinction can be made between hard (explicit text) prompting such as we use, and soft prompting that seeks to learn embeddings (Lester et al., 2021), activations (Li and Liang, 2021; Hambardzumyan et al., 2021), or attention weights (Liu et al., 2022a) that condition the model to perform a desired task. The latter approach is more expressive and more efficient at inference time, but performance can be sensitive to initialization (Hou et al., 2022), and some techniques require modifications to the model.</p>
<p>Hard prompts have the advantage of being easy to interpret and modify. Work in this area includes tools to facilitate development of handcrafted prompts (Strobelt et al., 2022; Bach et al., 2022); algorithms to find optimal prompts through gradient-guided search (Shin et al., 2020) or exhaustive search through labels (Schick and Schütze, 2021) or both labels and templates (Gao et al., 2021); as well as studies on the effect of example order (Kumar and Talukdar, 2021; Lu et al., 2022). Hard prompts have also been used to analyze model capabilities (Garg et al., 2022; Li et al., 2022a), the role of data (Singh et al., 2022), and the nature of prompting itself (Min et al., 2022; Wei et al., 2022).</p>
<p>With few exceptions, e.g. (Li et al., 2022b; Liu et al., 2022b; Valvoda et al., 2022), early approaches to hard prompting tended to condition on the task rather than the specific input. Our $k \mathrm{NN}$ approach for conditioning on the input was pioneered by Liu et al. (2022b), who used RoBERTa embeddings to identify relevant GPT-3 prompts for sentiment, table-to-text, and QA tasks. They found that $k \mathrm{NN}$ works better than a random-selection baseline, and that the advantage grows as the size of the (domain-controlled) example pool increases.</p>
<p>Work on prompting LLMs for MT began with the GPT-3 and PaLM papers (Brown et al., 2020; Chowdhery et al., 2022), which adopted similar approaches, comparing 0,1 , and $n$-shot ${ }^{1}$ random selection of independent sentence pairs from WMT training corpora, and testing on older French, German, and Romanian WMT test sets traditionally used in ML, augmented in PaLM with French $\rightarrow$ German and Kazakh. For both models, performance increased with number of shots, and $n$-shot BLEU scores were found to be competitive with previous unsupervised SOTA, and in some settings-particularly into English-supervised SOTA as well.</p>
<p>In other early MT work, Reynolds and McDonell (2021) experimented with prompt templates for GPT-3, and found that 0 -shot prompts with carefully-chosen templates can outperform $n$-shot prompts with sub-optimal templates. Garcia and Firat (2022) explored using prompts with mT5 (Xue et al., 2021) to control output attributes such as formality, and also examine the effect of using promptlike natural-language tags during fine-tuning. Patel et al. (2022) proposed autoregressive prompting: concatenating only the first predicted word to a prompt and output prefix at each step.</p>
<h2>Après nous, le déluge</h2>
<p>Since our paper appeared on arXiv in November 2022, there has been a flood of work on using LLMs for MT, which we summarize briefly for completeness. A number of papers (Agrawal et al., 2022; Zhang et al., 2023; Jiao et al., 2023; Hendy et al., 2023) investigate prompt quality and source proximity using methods similar to ours but with different LLMs, notably GPT-3.5, GPT-4 and their instruction-tuned counterparts. Their findings are in line with ours, with the exception of Agrawal et al. (2022), who achieve significant gains using</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>lexical matching augmented with a diversity mechanism to select prompts. Apart from differences in model and setting, a potentially salient discrepancy is their emphasis on BLEU rather than neural metrics to measure performance. Other interesting work that conditions prompts on source segments uses dictionaries to supply translations in lowresource settings (Ghazvininejad et al., 2023; Lu et al., 2023), or chain-of-thought inspired prompts that elicit keywords, topic, and related examples from the model itself (He et al., 2023).</p>
<p>Further recent work looks at the role of data, attributing LLM MT capabilities to the presence of incidental bilingual examples (Briakou et al., 2023), or showing that parallel data (Schioppa et al., 2023), dictionaries (Jones et al., 2023), or restriction to bilingual settings (Garcia et al., 2023) can boost performance in smaller LMs. Another popular line aims at controlling various properties of translations such as formality or use of specified terminology, either statically (Garcia et al., 2023; Moslem et al., 2023) or with human interaction (Pilault et al., 2023). Finally, there is extensive work on analyzing the translation output of LLMs, generally finding that it is more fluent than accurate (Hendy et al., 2023; Anonymous, 2023), good at handling document context (Wang et al., 2023; Karpinska and Iyyer, 2023) but also prone to problems such as hallucination (Zhang et al., 2023; Guerreiro et al., 2023), and frequently sub-par in low-resource settings (Zhu et al., 2023; Bawden and Yvon, 2023)</p>
<h2>3 Prompting for Machine Translation</h2>
<p>For a general task, prompting an LLM to generate a desired output $y$ from an input $x$ can involve many steps (Liu et al., 2021), including template generation, slot filling, answer search, and answer mapping. In MT, the answer search and mapping processes are simplified because the answers generated by the LLM can be used directly; we simplify further by using a fixed template. What we explore in depth is the slot filling portion; in particular, we test a variety of methods to select few-shot examples for the prompt.</p>
<p>In initial experiments we determined that for few-shot prompting the exact form of the template is unimportant, see Appendix A for details. Following this observation, we decided to adopt simple templates where each example if preprended by the corresponding language name. These results in
prompts of the form (for $n$-shot prompting):</p>
<div class="codehilite"><pre><span></span><code><span class="k">[source]: [X1]</span>
<span class="k">[target]: [Y1]</span>
<span class="na">*.</span>
<span class="na">[source]</span><span class="o">:</span><span class="w"> </span><span class="s">[X</span>
<span class="na">[target]</span><span class="o">:</span><span class="w"> </span><span class="s">[Y</span>
<span class="k">[source]: [X]</span>
<span class="na">[target]</span><span class="o">:</span>
</code></pre></div>

<p>where [source] and [target] are instantiated with the names in English of the source and target languages, e.g. English and German. Note that this scheme has been found to be present in the training data as a marker for multilingual content (Briakou et al., 2023). Each slot pair $\left(X_{i}, Y_{i}\right)$ is filled with a translation example for these languages, and the final slot $X$ is filled with the current source text. Our algorithm for $n$-shot translation from a source text $x$ to a target text $y$ is:</p>
<ol>
<li>Choose translation example pairs $\left(x_{1}, y_{1}\right) \ldots$ $\left(x_{n}, y_{n}\right)$. In general, these can depend on $x$.</li>
<li>Plug the example pairs and $x$ into the template. Condition PaLM on the resulting string.</li>
<li>Perform a greedy search, ${ }^{2}$ stopping when the model outputs a newline.</li>
<li>Output the predicted suffix verbatim as $y$.</li>
</ol>
<p>Example selection operates in two phases: first choose a pool containing parallel text, then choose examples from the pool. Choosing the pool lets us control global attributes of examples such as domain and average quality. Our baseline method for choosing examples is to select them randomly from the pool. We also experiment with selecting examples that are "closest" to the source text, on the hypothesis that such examples will help guide the model to produce similar translations.</p>
<p>To find relevant examples, we use $k$-nearest neighbor ( $k \mathrm{NN}$ ) search on the source side of our parallel pool, inspired by Khandelwal et al. (2021). We carry out the search itself using the method of Guo et al. (2020) ${ }^{3}$, and investigate two possible representations of the sentences, with associated distance measures:</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">LP</th>
<th style="text-align: left;">Year</th>
<th style="text-align: center;">#sents</th>
<th style="text-align: right;">Ref</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{en} \rightarrow \mathrm{de}$</td>
<td style="text-align: left;">2021</td>
<td style="text-align: center;">1002</td>
<td style="text-align: right;">C</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{de} \rightarrow \mathrm{en}$</td>
<td style="text-align: left;">2021</td>
<td style="text-align: center;">1000</td>
<td style="text-align: right;">B</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{en} \rightarrow \mathrm{zh}$</td>
<td style="text-align: left;">2021</td>
<td style="text-align: center;">1002</td>
<td style="text-align: right;">A</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{zh} \rightarrow \mathrm{en}$</td>
<td style="text-align: left;">2021</td>
<td style="text-align: center;">1948</td>
<td style="text-align: right;">A</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{en} \rightarrow \mathrm{fr}$</td>
<td style="text-align: left;">2014</td>
<td style="text-align: center;">3003</td>
<td style="text-align: right;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{fr} \rightarrow \mathrm{en}$</td>
<td style="text-align: left;">2014</td>
<td style="text-align: center;">3003</td>
<td style="text-align: right;">N/A</td>
</tr>
</tbody>
</table>
<p>Table 1: Test set information, including the newstest dataset year and, when applicable, the reference we use for scoring.</p>
<p>Bag-of-words (BOW): Each sentence is represented by a (sparse) vector of counts associated with words in the vocabulary. As the associated distance measure we use cosine distance. This representation focuses on the surface form of the words, and thus favors lexical similarity between the examples.</p>
<p>Roberta: Sentences are represented as embeddings in the space defined by Roberta (Liu et al., 2019), a multilingual transformer-based model, with Euclidean distance used for retrieval. We expect these embeddings to reflect the semantics of the sentence, and thus retrieve prompts that are relevant to their subject matter. ${ }^{4}$</p>
<h2>4 Data</h2>
<p>We experiment with translation into and out of English for Chinese, French and German. After English ( $78.0 \%$ ), German ( $3.5 \%$ ) and French ( $3.3 \%$ ) are the two largest languages in PaLM's 780B token training corpus; Chinese ( $0.4 \%$ ) is the 15 th largest, and it also represents an inherently more difficult translation task. To facilitate comparisons with recent SOTA systems, and to minimize the chance of overlap with PaLM's training corpus, we test on news data from the WMT 2021 evaluation campaign (Akhbardeh et al., 2021). Since French was not included in WMT21, we use data from WMT14; apart from being older, these test sets are not purely source-original (Freitag et al., 2019) like the more recent ones. Table 1 shows statistics for our test data.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Size of the different prompt pools, measured in sentences for the WMT sets and in paragraphs for the high-end pool.</p>
<p>For prompt selection, we use three distinct pools: the full WMT training corpus for each language pair (WMT-full), the corresponding WMT development sets (WMT-dev), and a manually-curated "high-end" pool. Sizes are shown in Table 2. The WMT-full pool is largest and offers the highest probability of close $k \mathrm{NN}$ matches, but it is crawled text drawn from sources of varying quality. The WMT-dev pool has generally better quality, and is a closer domain match to our test set; to encourage PaLM to produce more natural text, we included only target-original texts. ${ }^{5}$ For German $\leftrightarrow$ English and Chinese $\leftrightarrow$ English we include all the news test sets from 2010 to 2020. As English $\leftrightarrow$ French was discontinued after 2015, we used sets from 2010 to 2013, augmented with newsdiscussion2015.</p>
<p>The high-end pool comes from websites containing bilingual articles that we judged to be professionally edited, with native or near-native quality in both languages. The articles are drawn from various domains (biography, business, commentary, culture, fashion, food, news, and obituary), with the news domain of the test sets comprising less than $50 \%$ for each language. We treat these articles as symmetrical, and use them as prompt sources in both translation directions. Due to the non-literal nature of the translations, there is frequently no 1-1 correspondence between sentence pairs, so we extract aligned paragraphs for prompting. More detailed information about the high-end pool is provided in Appendix B.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h2>5 Experiments</h2>
<p>For compatibility with Chowdhery et al. (2022), we ran all experiments at the sentence level, translating each test sentence individually and in isolation from its context. This deprives PaLM of the ability to exploit the longer contexts it was exposed to during training, but it matches the operating mode of our baselines (including SOTA baselines), and facilitates evaluation. ${ }^{6}$ We leave an exploration of potential gains from conditioning on longer histories to future work.</p>
<p>In preliminary experiments, we varied the number of shots from 0 to 10 , and found clear performance gains as we increased the number of shots, with diminishing returns after 5 sentence pairs (see Appendix A). Accordingly we report all results on the WMT pools in the 5 -shot setting, where each shot is a single sentence pair, matching the configuration in Chowdhery et al. (2022). For the high-end pool, lacking 1-1 sentence alignments, we use 1-shot examples, where each shot is a single paragraph pair. This provides roughly the same quantity of text as 5 -shot with sentences, although it creates a stylistic mismatch with our test setup, as we still translate on a sentence-by-sentene basis, as in the other conditions.</p>
<p>When randomly selecting examples, we observed that there is little variability in automatic scores when selecting different samples ${ }^{7}$ (see Appendix C). For the results reported in this section, we let PaLM produce translations with 5 different seeds and we selected the run with the median BleUrt score. Translation time was some orders of magnitude longer than a dedicated translation system.</p>
<p>Following recent recommendations (Kocmi et al., 2021; Freitag et al., 2021a) we favour neural metrics (BLEURT in our case) over BLEU, although we also report BLEU scores for completeness. We use a cased version of BleUrt (Sellam et al., 2020) that is based on RembERT (Chung et al., 2020). We use BLEU as implemented in SACREBLEU ${ }^{8}$ (Post, 2018), with zh tokenization for English-Chinese, and 13a tokenization for all other languages.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>To perform human evaluation, we hired professional translators ( 7 for $\mathrm{En} \rightarrow \mathrm{De}, 5$ for $\mathrm{De} \rightarrow \mathrm{En}$, 4 for $\mathrm{Zh} \rightarrow \mathrm{En}$, and 4 for $\mathrm{En} \rightarrow \mathrm{Zh}$ ) and measure translation quality with a document-context version of MQM (Lommel et al., 2014) which mimics the setup proposed in Freitag et al. (2021a). This includes using the same error categories, severity levels and error weighting schema. As suggested in the study, we weight each major error with 5 and each minor error with 1 , except for minor punctuation errors which get a score of 0.1 . We depart from Freitag et al. (2021a) in using only a single annotator per segment, and in not imposing a limit of 5 errors per sentence. Additionally, due to technical restrictions on the length of an evaluation session, we limited the MQM evaluation to the first 12 segments per document.</p>
<h3>5.1 Selection strategies and pools</h3>
<p>We warm up by comparing example selection strategies on the two WMT pools, using automatic metrics to evaluate quality on English $\leftrightarrow$ German. Results are shown in Table 3. The main observation is that the choice of pool is much more important than the selection method: the results for WMT-dev are notably higher than those for WMT-full across all settings. When comparing $k \mathrm{NN}$ selection methods, RobeRta is more effective than BOW, but it does not provide a consistent advantage over random selection.</p>
<p>We conjecture that the quality of an example is more important than its proximity to the current source sentence. The larger size of the full WMT pool means that the $k \mathrm{NN}$ approaches will in general be able to find examples that are closer to each source sentence than those from the dev pool, but any resulting gain is offset by the greater risk that an example from the full pool will be a poor translation (since we match only on the source side). Interestingly, had we relied only on BLEU, we would have concluded that the choice of pool is unimportant, and that random selection consistently outperforms $k \mathrm{NN}$.</p>
<h3>5.2 Results on all language pairs</h3>
<p>Table 4 contains our main results, for German $\leftrightarrow$ English, Chinese $\leftrightarrow$ English, and French $\leftrightarrow$ English. For each language pair, we ran PaLM with random selection on all three pools and with $k \mathrm{NN}$ RobeRta on the WMT-full pool. We compared these systems to output from the best performing system in the 2021 WMT evaluation campaign for</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LP</th>
<th style="text-align: center;">Pool</th>
<th style="text-align: center;">Selection</th>
<th style="text-align: center;">BLEURT</th>
<th style="text-align: center;">BLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><img alt="img-1.jpeg" src="img-1.jpeg" /></td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$k$ NN BOW</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">32.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$k$ NN RoBERTa</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">32.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">dev</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">32.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$k$ NN RoBERTa</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">32.3</td>
</tr>
<tr>
<td style="text-align: center;"><img alt="img-2.jpeg" src="img-2.jpeg" /></td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">38.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$k$ NN BOW</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">36.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$k$ NN RoBERTa</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">35.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">dev</td>
<td style="text-align: center;">random</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$k$ NN RoBERTa</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">37.2</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of example selection strategies on the WMT-full and WMT-dev pools. Values for random selection are averaged over 5 runs.</p>
<p>German and Chinese, and for off-the-shelf Google Translate for all six language pairs. We evaluate with BLEU and BLEURT as in the previous section, augmented with human MQM assessments for German and Chinese. French is a special case, as its evaluation set is eight years old, and it is difficult to ensure that any of the MT systems we evaluate have not been exposed to it during training. We include it mostly for the purposes of comparison to Chowdhery et al. (2022), and do not provide SOTA results or perform human evaluation.</p>
<p>Comparing PaLM results for German and Chinese, the pattern from the previous section holds up: random selection from the WMT-dev pool outperforms selection from the full pool. MQM scores correlate well with BLEURT for these results. Despite domain and style mismatch, results for the highend pool are very similar to those for WMT-devcloser than any results on the full pool-adding support to the hypothesis that example quality is the main determinant of PaLM's output quality.</p>
<p>The French results reverse the general pattern. For this language pair, random selection from the WMT-full pool does best, although the results for all methods are fairly similar, with a difference of approximately 0.5 BLEURT between the best and worst. One potential explanation is the age and quality of newstest2014, as WMT test-set creation has dramatically improved since then.</p>
<p>Turning to a comparison between PaLM and conventional MT systems, the specialized SOTA systems have a substantial advantage of between 1 and 3 BLEURT points over the best PaLM re-
sults, a gap that is reflected in their much lower MQM scores. The difference is narrower for the general-purpose Google Translate system: less than 1 BLEURT except for Chinese $\rightarrow$ English (1.8), with French $\rightarrow$ English at parity. PaLM's performance relative to the best MT system for each language pair is generally better when translating into English, where it is lower by $1.0,2.3$, and 0.0 BLEURT for German, Chinese, and French, compared to drops of $2.1,2.5$, and 0.6 in the reverse direction.</p>
<p>The MQM results show some interesting characteristics of translations produced by PaLM. In all language pairs evaluated, fluency MQM scores for PaLM are generally similar to those for SOTA systems, while accuracy scores are lower. The accuracy gap is dominated by Major Accuracy/Omission errors, followed by inconsistent patterns of other Accuracy/* errors across language pairs. In some languages, the best-performing PaLM systems make fewer Style/Awkward errors than SOTA. Table 5 shows a selection of MQM error counts for PaLM WMT-dev random and SOTA systems; full details are provided in Appendix D.</p>
<h3>5.3 Comparison to previous results</h3>
<p>Our only results that are directly comparable to the few-shot results from Chowdhery et al. (2022) are the WMT-full Bleu scores in table 4 c (WMT14 French test-set). Our result for French $\rightarrow$ English matches theirs exactly, but our score for English $\rightarrow$ French is lower by 1.7 (42.3 versus 44.0). We attribute this discrepancy to their use of the SACREBLEU int1 tokenizer; when we evaluate our output using this version, we obtain matching scores.</p>
<p>Our general finding that PaLM's into-English performance is better than the reverse direction matches the conclusion from Chowdhery et al. (2022), while our comparison with recent SOTA systems on current test sets contrasts with their results indicating that PaLM can rival supervised performance in older settings.</p>
<h2>6 Analysis</h2>
<p>In this section we delve further into various aspects of PaLM's MT performance.</p>
<h2>6.1 $k$ NN versus random prompts</h2>
<p>To understand the performance difference between $k$ NN RoBERTa and randomly-selected examples, we performed a qualitative analysis, choosing sen-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LP</th>
<th style="text-align: center;">System</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MQM $\downarrow$</th>
<th style="text-align: center;">BLEURT $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">en $\rightarrow$ de</td>
<td style="text-align: center;">WMT21 Facebook Submission (Tran et al., 2021)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.18 ${ }^{\dagger}$</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">42.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">39.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">WMT-full random</td>
<td style="text-align: center;">1.90</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-full $k \mathrm{NN}$</td>
<td style="text-align: center;">1.93</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">32.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-dev random</td>
<td style="text-align: center;">1.58</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">32.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">high-end random</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;">de $\rightarrow$ en</td>
<td style="text-align: center;">WMT21 Facebook Submission (Tran et al., 2021)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.31 ${ }^{\dagger}$</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">41.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.71</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">40.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">WMT-full random</td>
<td style="text-align: center;">2.38</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-full $k \mathrm{NN}$</td>
<td style="text-align: center;">3.03</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">35.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-dev random</td>
<td style="text-align: center;">1.92</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">high-end random</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">38.8</td>
</tr>
</tbody>
</table>
<p>(a) German $\rightarrow$ English (nt2021). All MQM results labelled with $\dagger$ are significantly better than all other systems based on PERM-BOTH pair-wise significance testing (Koehn, 2004) with $p=0.05$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LP</th>
<th style="text-align: center;">System</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MQM $\downarrow$</th>
<th style="text-align: center;">BLEURT $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">en $\rightarrow \mathrm{zh}$</td>
<td style="text-align: center;">WMT21 We</td>
<td style="text-align: center;">Chat Submission (Zeng et al., 2021)</td>
<td style="text-align: center;">2.47 ${ }^{\dagger}$</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">36.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.23</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">36.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">WMT-full random</td>
<td style="text-align: center;">4.35</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">28.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-full $k \mathrm{NN}$</td>
<td style="text-align: center;">5.06</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">28.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-dev random</td>
<td style="text-align: center;">3.24</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">29.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">high-end random</td>
<td style="text-align: center;">3.70</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">29.6</td>
</tr>
<tr>
<td style="text-align: center;">zh $\rightarrow$ en</td>
<td style="text-align: center;">WMT21 Border</td>
<td style="text-align: center;">line Submission (Wang et al., 2021)</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">33.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.12</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">32.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">WMT-full random</td>
<td style="text-align: center;">3.95</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">25.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-full $k \mathrm{NN}$</td>
<td style="text-align: center;">4.06</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">23.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-dev random</td>
<td style="text-align: center;">3.60</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">25.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">high-end random</td>
<td style="text-align: center;">3.89</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">25.1</td>
</tr>
</tbody>
</table>
<p>(b) Chinese $\rightarrow$ English (nt2021). All MQM results labelled with $\dagger$ are significantly better than all other systems based on PERM-BOTH pair-wise significance testing (Koehn, 2004) with $\mathrm{p}=0.05$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LP</th>
<th style="text-align: center;">System</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BLEURT $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">en $\rightarrow \mathrm{fr}$</td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">45.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">WMT-full random</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-full $k \mathrm{NN}$</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">41.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-dev random</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">41.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">high-end random</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">38.6</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{fr} \rightarrow$ en</td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">43.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">WMT-full random</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">42.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-full $k \mathrm{NN}$</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">41.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-dev random</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">42.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">high-end random</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">40.4</td>
</tr>
</tbody>
</table>
<p>(c) French $\rightarrow$ English (nt2014).</p>
<p>Table 4: Translation results for all language pairs. Values for random selection are the BLEURT median of 5 runs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LP</th>
<th style="text-align: left;">Sev.</th>
<th style="text-align: center;">Category</th>
<th style="text-align: right;">PaLM</th>
<th style="text-align: right;">SOTA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">de $\rightarrow$ en</td>
<td style="text-align: left;">Major</td>
<td style="text-align: center;">Omission</td>
<td style="text-align: right;">51</td>
<td style="text-align: right;">19</td>
</tr>
<tr>
<td style="text-align: left;">en $\rightarrow$ de</td>
<td style="text-align: left;">Major</td>
<td style="text-align: center;">Omission</td>
<td style="text-align: right;">26</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">zh $\rightarrow$ en</td>
<td style="text-align: left;">Major</td>
<td style="text-align: center;">Omission</td>
<td style="text-align: right;">109</td>
<td style="text-align: right;">42</td>
</tr>
<tr>
<td style="text-align: left;">en $\rightarrow$ zh</td>
<td style="text-align: left;">Major</td>
<td style="text-align: center;">Omission</td>
<td style="text-align: right;">80</td>
<td style="text-align: right;">46</td>
</tr>
<tr>
<td style="text-align: left;">de $\rightarrow$ en</td>
<td style="text-align: left;">Minor</td>
<td style="text-align: center;">Awkward</td>
<td style="text-align: right;">73</td>
<td style="text-align: right;">81</td>
</tr>
<tr>
<td style="text-align: left;">en $\rightarrow$ de</td>
<td style="text-align: left;">Minor</td>
<td style="text-align: center;">Awkward</td>
<td style="text-align: right;">166</td>
<td style="text-align: right;">144</td>
</tr>
<tr>
<td style="text-align: left;">zh $\rightarrow$ en</td>
<td style="text-align: left;">Minor</td>
<td style="text-align: center;">Awkward</td>
<td style="text-align: right;">205</td>
<td style="text-align: right;">284</td>
</tr>
<tr>
<td style="text-align: left;">en $\rightarrow$ zh</td>
<td style="text-align: left;">Minor</td>
<td style="text-align: center;">Awkward</td>
<td style="text-align: right;">115</td>
<td style="text-align: right;">142</td>
</tr>
</tbody>
</table>
<p>Table 5: Selected MQM error count comparisons between PaLM WMT-dev random and SOTA. Omission is a subcategory of Accuracy errors, and Awkward is a subcategory of Style. Full details are provided in Appendix D.
tences with the largest BLEURT difference between the two systems. Table 14a in Appendix F shows an example where the $k \mathrm{NN}$ system correctly retrieves relevant translation examples in the football domain, guiding PaLM to produce a better translation than the random selection system. This contrasts with the example in Table 14b, where the retrieved source sentences are also from the relevant domain, but all have alignment errors, causing PaLM to generate hallucinated output. In general, random selection is also prone to landing on alignment errors, but as each prompt is selected independently, the odds that all examples will be errors are low. An informal analysis of $k \mathrm{NN}$ examples indicates that if one non-parallel prompt is selected, the others also tend to be of poor quality, perhaps due to corpus alignment errors that are concentrated in particular documents or topics. Since $k \mathrm{NN}$ matches only on the source side, it is not robust to this noise.</p>
<h3>6.2 Example Translations</h3>
<p>Example translations comparing PaLM and SOTA systems for German $\rightarrow$ English and English $\rightarrow$ Chinese are given in Appendix 6.2, in Table 15 and Table 16, respectively. We compared the translations of both systems and chose examples that are short, but include the most frequent patterns that we observed also in longer translations. In general, PaLM's translations are less literal when compared to supervised NMT systems. Even though this is one of the strengths of PaLM, it occasionally misses some important information in the source or hallucinates facts not present in the source sentence. The supervised models on the other hand are faithful to the source;</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Year</th>
<th style="text-align: left;">LP</th>
<th style="text-align: center;">\% Clean</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2014</td>
<td style="text-align: left;">$\mathrm{fr} \rightarrow$ en</td>
<td style="text-align: center;">$\mathbf{6 9 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathrm{en} \rightarrow \mathrm{fr}$</td>
<td style="text-align: center;">93.6</td>
</tr>
<tr>
<td style="text-align: left;">2016</td>
<td style="text-align: left;">$\mathrm{de} \rightarrow$ en</td>
<td style="text-align: center;">$\mathbf{8 0 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathrm{en} \rightarrow \mathrm{de}$</td>
<td style="text-align: center;">97.3</td>
</tr>
<tr>
<td style="text-align: left;">2021</td>
<td style="text-align: left;">$\mathrm{en} \rightarrow \mathrm{de}$</td>
<td style="text-align: center;">99.6</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathrm{en} \rightarrow \mathrm{zh}$</td>
<td style="text-align: center;">99.7</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathrm{de} \rightarrow \mathrm{en}$</td>
<td style="text-align: center;">97.9</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathrm{zh} \rightarrow \mathrm{en}$</td>
<td style="text-align: center;">98.1</td>
</tr>
</tbody>
</table>
<p>Table 6: The size of clean (lacking 15-gram target-side overlap with PaLM training data) versions of test sets for various WMT years and language pairs
this reduces the risk of omission and addition errors, but occasionally leads to translations that are not natural in the target language (e.g. translating street names or using the wrong time format). These findings are in line with the MQM results presented in section 5.2.</p>
<h3>6.3 Overlap of test and training data</h3>
<p>One major change with respect to Chowdhery et al. (2022) is our use of more recent WMT test sets, which are unlikely to overlap with PaLM's training data. ${ }^{9}$ We test this hypothesis using the technique from Chowdhery et al. (2022), which involves measuring high-order $n$-gram matches; specifically, we measure 15-gram overlap as tokenized by the mBERT tokenizer (Devlin et al., 2019). ${ }^{10}$ For test sequences with fewer than 15 tokens, we consider them overlapping if the complete sequence is found as a subsequence of a training example. We report the degree of overlap by showing the percentage of original test examples that survive in the clean test set after removing overlap in Table 6. This confirms that the older French $\rightarrow$ English and German $\rightarrow$ English sets have substantial overlap with PaLM's training data, while the newer test sets, whether into or out of English, have much smaller overlapping portions.</p>
<p>Chowdhery et al. (2022) also measure the effect of test-set overlap on translation quality, comparing scores on the original test set to the clean set with overlapping examples removed. In section H we</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>report similar scores for the older test sets, and extend the analysis to calibrate the effect of overlap on MT evaluation, by comparing to an overlap-free off-the-shelf system.</p>
<h2>7 Conclusion</h2>
<p>We perform a careful assessment of the sentencelevel MT capabilities of PaLM, which we compare to SOTA and a current off the shelf (COTS) MT system for three high-resource languages-German, Chinese, and French-into and out of English, using the latest test sets from WMT. We chose to focus on a small set of high-resource language pairs in order to test the claims of the original PaLM paper, which are most striking for these pairs. The time and expense of performing high-quality human evaluations precluded a broader investigation.</p>
<p>Comparing $k \mathrm{NN}$ and random strategies for selecting 5-shot translation examples to instantiate fixed prompt templates, we find that $k \mathrm{NN}$ 's potential advantage in identifying examples relevant to the source sentence is outweighed by its susceptibility to corpus noise. Choosing examples randomly from small, high-quality pools works well, and performance appears to be independent of the domain and translation style of the pool, suggesting that example quality is the most important factor.</p>
<p>Using both the BleURt metric and MQM human evaluations, we show that PaLM's performance, while very impressive for a system never deliberately exposed to parallel text, still significantly lags that of competition-grade SOTA systems on recent WMT test sets, and to a lesser extent the performance of COTS systems as well. This contrasts with some of the findings of Chowdhery et al. (2022). As in that work, we find that performance into English is somewhat better than the reverse direction. Finally, we perform an extensive analysis of the characteristics of PaLM's MT output, notably finding that in all languages we tested it tends to be creative and fluent but prone to omissions and other accuracy errors; broadly speaking, it matches the fluency but lags the accuracy of conventional NMT.</p>
<p>In future work we look forward to testing PaLM on document-level translation tasks, unleashing its formidable capacity for leveraging long contexts. We would also like to explore prompt tuning methods that are more sophisticated than the hard-prompt setting we adopted for this paper, particularly to see if these might offer a way to tighten
up PaLM's MT accuracy without destroying its impressive ability to generate highly-fluent text.</p>
<h2>Limitations</h2>
<p>As we use only a small number of language pairs, it is not clear how general our conclusions are; in particular, they pertain only to languages that are well represented in PaLM's training corpus, and only to translation into and out of English. Our restriction to independent sentence-level translations may have caused us to underestimate PaLM's true capabilities, since some of the accuracy problems we observed might be considered less severe in the context of whole-document translation where less literal translations are more typical. Our exploration of prompting barely scratches the surface of the many methods that have been proposed for adapting LLMs to particular tasks, and we may have missed a technique that produces higher-quality translations than we observed. Finally, the human evaluation we rely on to provide our most accurate results is necessarily subjective, and if we were to have carried out the evaluation with different raters and a different methodology, our conclusions might well have been different.</p>
<h2>Ethical Considerations</h2>
<p>Working with large language models comes with many ethical concerns that are discussed in detail in Brown et al. (2020) and Chowdhery et al. (2022). There, MT is often one task of many, while we focus on the question of proper example selection for few-shot prompting of MT, which adds a few specific concerns. Our conclusion that prompt quality is important could lead one to build a system with prompts drawn from a small set of trusted sources; indeed, our high-end set is one such example of this. In such a scenario, this small source will have an outsized impact on the output of the translation system, and one must be careful to manage issues of attribution and intellectual property. Furthermore, an editorial choice defining high-quality language can potentially reduce quality for groups and topics not typically discussed in this style (Gururangan et al., 2022). Finally, by highlighting the power of few-shot examples, one might be tempted to turn example selection over to the users of a system. There, special steps must be taken to avoid exposing users to biased or toxic outputs, which may be triggered by unconstrained prompting (Gehman et al., 2020; Costa-jussà et al., 2022).</p>
<h2>References</h2>
<p>Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2022. Incontext examples selection for machine translation. arXiv preprint arXiv:2212.02437.</p>
<p>Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ondřej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussa, Cristina España-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT21). In Proceedings of the Sixth Conference on Machine Translation, pages 1-88, Online. Association for Computational Linguistics.</p>
<p>Anonymous. 2023. Does gpt-3 produces less literal translations? Anonymous preprint under review.</p>
<p>Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. 2022. Promptsource: An integrated development environment and repository for natural language prompts. arXiv preprint arXiv:2202.01279.</p>
<p>Rachel Bawden and François Yvon. 2023. Investigating the translation performance of a large multilingual language model: the case of bloom. arXiv preprint arXiv:2303.01911.</p>
<p>Eleftheria Briakou, Colin Cherry, and George Foster. 2023. Searching for needles in a haystack: On the role of incidental bilingualism in palm's translation capability. arXiv preprint arXiv:2305.10266.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877-1901.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaarav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, and Sebastian Ruder. 2020. Rethinking embedding coupling in pre-trained language models. arXiv preprint:2010.12821.</p>
<p>Marta R. Costa-jussà, Eric Smith, Christophe Ropers, Daniel Licht, Javier Ferrando, and Carlos Escolano. 2022. Toxicity in multilingual machine translation at scale. arXiv preprint arXiv:2210.03070.</p>
<p>Daniel Deutsch, Rotem Dror, and Dan Roth. 2021. A statistical analysis of summarization evaluation metrics using resampling methods. Transactions of the Association for Computational Linguistics, 9:11321146.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Markus Freitag, Isaac Caswell, and Scott Roy. 2019. APE at scale and its implications on MT evaluation biases. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 34-44, Florence, Italy. Association for Computational Linguistics.</p>
<p>Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460-1474.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expertbased human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733-774, Online. Association for Computational Linguistics.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816-3830.</p>
<p>Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Fangxiaoyu Feng, Melvin Johnson, and Orhan Firat. 2023. The unreasonable effectiveness of few-shot learning for machine translation. arXiv preprint arXiv:2302.01398.</p>
<p>Xavier Garcia and Orhan Firat. 2022. Using natural language prompts for machine translation. arXiv preprint arXiv:2202.11822.</p>
<p>Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. 2022. What can transformers learn in-context? a case study of simple function classes. arXiv preprint arXiv:2208.01066.</p>
<p>Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356-3369, Online. Association for Computational Linguistics.</p>
<p>Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer. 2023. Dictionary-based phrase-level prompting of large language models for machine translation. arXiv preprint arXiv:2302.07856.</p>
<p>Nuno M Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André FT Martins. 2023. Hallucinations in large multilingual translation models. arXiv preprint arXiv:2303.16104.</p>
<p>Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. 2020. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning.</p>
<p>Suchin Gururangan, Dallas Card, Sarah K. Dreier, Emily K. Gade, Leroy Z. Wang, Zeyu Wang, Luke Zettlemoyer, and Noah A. Smith. 2022. Whose language counts as high quality? measuring language ideologies in text data selection. arXiv preprint arXiv:2201.10474.</p>
<p>Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: Word-level Adversarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4921-4933, Online. Association for Computational Linguistics.</p>
<p>Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2023. Exploring humanlike translation strategy with large language models. arXiv preprint arXiv:2305.04118.</p>
<p>Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210.</p>
<p>Yutai Hou, Hongyuan Dong, Xinghao Wang, Bohan Li, and Wanxiang Che. 2022. Metaprompting: Learning to learn better prompts. arXiv preprint arXiv:2209.11486.</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? a preliminary study. arXiv preprint arXiv:2301.08745.</p>
<p>Alex Jones, Isaac Caswell, Ishank Saxena, and Orhan Firat. 2023. Bilex rx: Lexical data augmentation for massively multilingual machine translation. arXiv preprint arXiv:2303.15265.</p>
<p>Marzena Karpinska and Mohit Iyyer. 2023. Large language models effectively leverage document-level context for literary translation, but critical errors persist. arXiv preprint arXiv:2304.03245.</p>
<p>Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2021. Nearest neighbor machine translation. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Proceedings of the Sixth Conference on Machine Translation, pages 478-494, Online. Association for Computational Linguistics.</p>
<p>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388395, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Sawan Kumar and Partha Talukdar. 2021. Reordering examples helps during priming-based few-shot learning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages $4507-4518$.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045-3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Jiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2022a. Probing via prompting. arXiv preprint arXiv:2207.01736.</p>
<p>Junyi Li, Tianyi Tang, Jian-Yun Nie, Ji-Rong Wen, and Xin Zhao. 2022b. Learning to transfer prompts for text generation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3506-3518, Seattle, United States. Association for Computational Linguistics.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages</p>
<p>4582-4597, Online. Association for Computational Linguistics.</p>
<p>Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022a. Few-shot parameter-efficient finetuning is better and cheaper than in-context learning. arXiv preprint arXiv:2205.05638.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022b. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Arle Lommel, Hans Uszkoreit, and Aljoscha Burchardt. 2014. Multidimensional Quality Metrics (MQM) : A Framework for Declaring and Describing Translation Quality Metrics. Tradumàtica, pages 0455463.</p>
<p>Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, and Furu Wei. 2023. Chain-of-dictionary prompting elicits translation in large language models. arXiv preprint arXiv:2305.06575.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086-8098, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</p>
<p>Yasmin Moslem, Rejwanul Haque, and Andy Way. 2023. Adaptive machine translation with large language models. arXiv preprint arXiv:2301.13294.</p>
<p>Ajay Patel, Bryan Li, Mohammad Sadegh Rasooli, Noah Constant, Colin Raffel, and Chris Callison-Burch. 2022. Bidirectional language models are also few-shot learners. arXiv preprint arXiv:2209.14500.</p>
<p>Jonathan Pilault, Xavier Garcia, Arthur Bražinskas, and Orhan Firat. 2023. Interactive-chainprompting: Ambiguity resolution for crosslingual conditional generation with interaction. arXiv preprint arXiv:2301.10309.</p>
<p>Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-7.</p>
<p>Timo Schick and Hinrich Schütze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269.</p>
<p>Andrea Schioppa, Xavier Garcia, and Orhan Firat. 2023. Cross-lingual supervision improves large language models pre-training. arXiv preprint arXiv:2305.11778.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 42224235.</p>
<p>Chandan Singh, John X Morris, Jyoti Aneja, Alexander M Rush, and Jianfeng Gao. 2022. Explaining patterns in data with language models via interpretable autoprompting. arXiv preprint arXiv:2210.01848.</p>
<p>Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer, Hanspeter Pfister, and Alexander M Rush. 2022. Interactive and visual prompt engineering for ad-hoc task adaptation with large language models. IEEE transactions on visualization and computer graphics.</p>
<p>Chau Tran, Shruti Bhosale, James Cross, Philipp Koehn, Sergey Edunov, and Angela Fan. 2021. Facebook AI's WMT21 news translation task submission.</p>
<p>In Proceedings of the Sixth Conference on Machine Translation, pages 205-215, Online. Association for Computational Linguistics.</p>
<p>Josef Valvoda, Yimai Fang, and David Vandyke. 2022. Prompting for a conversation: How to control a dialog model? arXiv preprint arXiv:2209.11068.</p>
<p>Longyue Wang, Mu Li, Fangxu Liu, Shuming Shi, Zhaopeng Tu, Xing Wang, Shuangzhi Wu, Jiali Zeng, and Wen Zhang. 2021. Tencent translation system for the WMT21 news translation task. In Proceedings of the Sixth Conference on Machine Translation, pages 216-224, Online. Association for Computational Linguistics.</p>
<p>Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023. Document-level machine translation with large language models. arXiv preprint arXiv:2304.02210.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics.</p>
<p>Xianfeng Zeng, Yijin Liu, Ernan Li, Qiu Ran, Fandong Meng, Peng Li, Jinan Xu, and Jie Zhou. 2021. WeChat neural machine translation systems for WMT21. In Proceedings of the Sixth Conference on Machine Translation, pages 243-254, Online. Association for Computational Linguistics.</p>
<p>Biao Zhang, Barry Haddow, and Alexandra Birch. 2023. Prompting large language model for machine translation: A case study. arXiv preprint arXiv:2301.07069.</p>
<p>Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023. Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv:2304.04675.</p>
<h2>Appendices</h2>
<h2>A Prompt Exploration</h2>
<p>As preliminary experiments we tried different prompting templates:</p>
<p>Language This is the prompt template used in the paper (see Section 3). It prepends the examples with the corresponding language name in English.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;"># shots</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Language</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">74.4</td>
</tr>
<tr>
<td style="text-align: center;">Codes</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">74.1</td>
</tr>
<tr>
<td style="text-align: center;">Header</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">74.1</td>
</tr>
<tr>
<td style="text-align: center;">Textual</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">73.7</td>
</tr>
<tr>
<td style="text-align: center;">Deutsch</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">74.1</td>
</tr>
<tr>
<td style="text-align: center;">None</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">74.1</td>
</tr>
</tbody>
</table>
<p>Table 7: BleURt results with different prompt templates and number of prompts for the English $\rightarrow$ German translation directions. The prompt examples were randomly selected. The median of 5 runs are shown.</p>
<p>Codes Like "Language", but instead of full English names, two-letter languages codes are used (e.g. "en", "de").</p>
<p>Header Like "Language", but the header "Translate following sentences:" is added.</p>
<p>Textual A textual request for translating a sentence: "Translate $X_{n}$ from English into German: $Y_{n}$ ", where $X_{n}$ and $Y_{n}$ are the translation examples, as in Section 3. The source sentence $X$ is given with the same template, but without specifying any translation.</p>
<p>Deutsch Like "Language", but the language names are given in German ("Englisch", "Deutsch").</p>
<p>None No added text. Source and target examples are just input one after the other.</p>
<p>As shown in Table 7, the choice of a prompting strategy has a crucial impact when the number of shots is low, but the effect is reduced when we increase the number of examples shown. The number of examples also has a significant impact on translation quality. We chose to work with 5 examples, as there are diminishing returns when increasing the number of prompts, and choosing a higher number has additional practical implications (e.g. possibly exceeding the maximum input length).</p>
<h2>B High-end pool</h2>
<p>Table 9 describes the high-end pool. All listed articles were manually downloaded in June-August 2022, and semi-automatically divided into bilingual paragraphs. Our high-end pool consists of all paragraphs from all articles. The domain breakdown for each language pair is shown in Table 8.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Genre</th>
<th style="text-align: right;">Proportion</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">en $\leftrightarrow$ de</td>
<td style="text-align: right;">en $\leftrightarrow$ fr</td>
<td style="text-align: right;">en $\leftrightarrow \mathrm{zh}$</td>
</tr>
<tr>
<td style="text-align: left;">biography</td>
<td style="text-align: right;">$31 \%$</td>
<td style="text-align: right;">$20 \%$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">business</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$15 \%$</td>
</tr>
<tr>
<td style="text-align: left;">commentary</td>
<td style="text-align: right;">$25 \%$</td>
<td style="text-align: right;">$10 \%$</td>
<td style="text-align: right;">$16 \%$</td>
</tr>
<tr>
<td style="text-align: left;">culture</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$44 \%$</td>
<td style="text-align: right;">$14 \%$</td>
</tr>
<tr>
<td style="text-align: left;">fashion</td>
<td style="text-align: right;">$16 \%$</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">food</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$8 \%$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">news</td>
<td style="text-align: right;">$4 \%$</td>
<td style="text-align: right;">$18 \%$</td>
<td style="text-align: right;">$43 \%$</td>
</tr>
<tr>
<td style="text-align: left;">obituary</td>
<td style="text-align: right;">$24 \%$</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$13 \%$</td>
</tr>
</tbody>
</table>
<p>Table 8: Genre distributions for the high-end pool.</p>
<h2>C Variability of Random Runs</h2>
<p>Table 10 shows the automatic scores for random runs for the German $\rightarrow$ English language pair. It can be observed that the range of scores is quite small, less than 0.5 BLEURT points for all language directions. For both directions, the use of WMT-dev, as opposed to WMT-full, for the random pool reduces the observed range in BLEURT by at least 0.1 .</p>
<h2>D Detailed MQM Scores</h2>
<p>Table 11 presents MQM scores for PaLM WMTdev random and SOTA systems in the four language pairs evaluated, along with the breakdown of the scores into their Accuracy and Fluency components. Table 12 presents detailed MQM error counts for PaLM WMT-dev random and SOTA systems in en $\rightarrow$ de and de $\rightarrow$ en.</p>
<h2>E Significance numbers</h2>
<p>We calculate pairwise significance numbers based on PERM-BOTH pair-wise significance testing (Koehn, 2004; Deutsch et al., 2021). Results can be seen in Table 13.</p>
<h2>F Example Prompts</h2>
<p>Tables 14a and 14b show prompt examples where $k \mathrm{NN}$ and random selection do better, respectively, as described in section 6.1.</p>
<h2>G Example Translations</h2>
<p>Tables 15 and 16 show example translations for German $\rightarrow$ English and English $\rightarrow$ Chinese as described in section 6.2.</p>
<h2>H Overlap Analysis</h2>
<p>Chowdhery et al. (2022) show BLEU differences between clean and original test sets, and provide some evidence that differences are not due to memorization, but it still isn't clear how much overlap actually inflates a model's score. We directly quantify the effect of train-test overlap on decision making by comparing 5-shot PaLM to Google Translate (GT) ${ }^{11}$ on our two sets with substantial overlap, testing under original, clean and $\neg$ clean (including only overlapping examples) scenarios. BLEU and BLEURT scores for the two systems and three test sets are shown in Table 17.</p>
<p>We can see that directly comparing original and clean results for a single system conflates differences from overlap with those from the increased difficulty of the clean subset. For example, for de $\rightarrow$ en Bleu, comparing PaLM's original and clean scores gives an overlap gap of 2.6-BLEU, in line with the gaps reported by Chowdhery et al. (2022). However, the non-overlapping GT system also has lower scores on the clean set, indicating that it may simply be more difficult. ${ }^{12}$ It's more useful to see that the original test indicated a 1.5 -BLEU difference between the two systems, while the clean test indicates a 2.0 -BLEU difference, meaning PaLM benefited from overlap by 0.5 BLEU in this comparison. The fully overlapping $\neg$ clean further distorts the difference between the two systems: the true (clean) delta of 2.0 BLEU shrinks to only 0.4 . Trends for $\mathrm{fr} \rightarrow$ en are similar: though PaLM and GT are very close according to the original test set, the clean set reveals a delta of 0.8 BLEU. Interestingly, BLEURT may be less sensitive to overlap, with the original-versus-clean deltas hovering around 0 for $\mathrm{fr} \rightarrow$ en regardless of the test subset, and de $\rightarrow$ en showing that PaLM benefits from an overlap bonus of only 0.3 BLEURT.</p>
<p>In summary, overlap between the target side of the test data and the LLM training data can have an impact on both BLEU and BLEURT scores, altering the delta between two systems where one benefits from overlap and another does not by up to 0.7</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">LP</th>
<th style="text-align: center;">paras</th>
<th style="text-align: center;">words</th>
<th style="text-align: center;">URL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { ㅇ } \ &amp; \text { 心 } \end{aligned}$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">255</td>
<td style="text-align: center;">www.deutschland.de/en/news/new-supercomputer-in-operation</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">208</td>
<td style="text-align: center;">www.deutschland.de/en/news/patents-germany-ranks-second</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">609</td>
<td style="text-align: center;">www.deutschland.de/en/news/syrian-swimmer-yusra-mardini-provides-message-of-hope-at-olympics</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">1787</td>
<td style="text-align: center;">www.zeit.de/kultur/2019-12/schoenhett-fotografie-aesthetik-rankin-mitch-epstein-roger-ballen-english</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2817</td>
<td style="text-align: center;">www.zeit.de/kultur/2020-07/</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">desinformation-peter-pomerantsev-social-media-regulation-democracy/komplettansicht</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2961</td>
<td style="text-align: center;">www.zeit.de/politik/ausland/2020-11/</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">polarization-us-elections-democrats-republicans-donald-trump-family-division-english</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2757</td>
<td style="text-align: center;">www.zeit.de/politik/deutschland/2015-11/helmut-schmidt-obituary-english/komplettansicht</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { ㅇ } \ &amp; \text { 心 } \ &amp; \text { 心 } \end{aligned}$</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">1323</td>
<td style="text-align: center;">cn.nytimes.com/asia-pacific/20220509/taiwan-china-covid/dual</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">1317</td>
<td style="text-align: center;">cn.nytimes.com/china/20220427/brownface-barrack-okarma-1968-hong-kong/dual</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">780</td>
<td style="text-align: center;">cn.nytimes.com/china/20220401/china-cheng-lei-australia/dual</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">609</td>
<td style="text-align: center;">cn.nytimes.com/china/20220421/china-eastern-crash-report/dual</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">1520</td>
<td style="text-align: center;">cn.nytimes.com/china/20220412/china-russia-propaganda/dual</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">1373</td>
<td style="text-align: center;">cn.nytimes.com/business/20220621/china-housing-real-estate-economy/dual</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">478</td>
<td style="text-align: center;">cn.nytimes.com/china/20220415/shanghais-food-crisis-prompts-residents-in-beijing-to-stockpile-supplies/dual</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">1202</td>
<td style="text-align: center;">cn.nytimes.com/obits/20220418/peng-ming-min-dead</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">843</td>
<td style="text-align: center;">https://cn.nytimes.com/world/20220330/solomon-islands-china/dual</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">846</td>
<td style="text-align: center;">france-amerique.com/a-france-of-many-colors</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1177</td>
<td style="text-align: center;">france-amerique.com/alice-guy-cinema-forgotten-pioneer</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1237</td>
<td style="text-align: center;">france-amerique.com/americanization-is-back-did-it-ever-go-away</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">666</td>
<td style="text-align: center;">france-amerique.com/a-propos-a-hard-hitting-french-american-podcast</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">457</td>
<td style="text-align: center;">france-amerique.com/camille-laurens-a-womans-life</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">970</td>
<td style="text-align: center;">france-amerique.com/football-and-soccer</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">377</td>
<td style="text-align: center;">france-amerique.com/france-united-states-naval-battle-and-diplomatic-crisis</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">615</td>
<td style="text-align: center;">france-amerique.com/jeanne-damas-all-the-women-in-her-city</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">631</td>
<td style="text-align: center;">france-amerique.com/guedelon-building-a-castle-by-hand</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">811</td>
<td style="text-align: center;">france-amerique.com/raphael-francois-culinary-director</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">874</td>
<td style="text-align: center;">france-amerique.com/thierry-mugler-provocateur</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">934</td>
<td style="text-align: center;">france-amerique.com/winds-of-change-over-democracy</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">255</td>
<td style="text-align: center;">www.deutschland.de/en/news/new-supercomputer-in-operation</td>
</tr>
</tbody>
</table>
<p>Table 9: Sizes and provenance for articles in the high-end prompt pool. The words column contains the number of English words (whitespace-separated character sequences) in each article.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LP</th>
<th style="text-align: center;">Pool</th>
<th style="text-align: center;">BLEURT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Run 1</td>
<td style="text-align: center;">Run 2</td>
<td style="text-align: center;">Run 3</td>
<td style="text-align: center;">Run 4</td>
<td style="text-align: center;">Run 5</td>
<td style="text-align: center;">Run 1</td>
<td style="text-align: center;">Run 2</td>
<td style="text-align: center;">Run 3</td>
<td style="text-align: center;">Run 4</td>
<td style="text-align: center;">Run 5</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{en} \rightarrow \mathrm{de}$</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">dev</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">32.8</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{de} \rightarrow \mathrm{en}$</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">dev</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">38.2</td>
</tr>
</tbody>
</table>
<p>Table 10: Results for random runs for the German $\rightarrow$ English translation direction.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">PaLM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SOTA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MQM $\downarrow$</td>
<td style="text-align: center;">Accuracy $\downarrow$</td>
<td style="text-align: center;">Fluency $\downarrow$</td>
<td style="text-align: center;">MQM $\downarrow$</td>
<td style="text-align: center;">Accuracy $\downarrow$</td>
<td style="text-align: center;">Fluency $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{en} \rightarrow \mathrm{de}$</td>
<td style="text-align: center;">1.58</td>
<td style="text-align: center;">1.12</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{en} \rightarrow \mathrm{zh}$</td>
<td style="text-align: center;">3.24</td>
<td style="text-align: center;">2.69</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">2.47</td>
<td style="text-align: center;">1.96</td>
<td style="text-align: center;">0.48</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{de} \rightarrow \mathrm{en}$</td>
<td style="text-align: center;">1.92</td>
<td style="text-align: center;">1.43</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">1.31</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{zh} \rightarrow \mathrm{en}$</td>
<td style="text-align: center;">3.60</td>
<td style="text-align: center;">2.97</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">2.43</td>
<td style="text-align: center;">0.68</td>
</tr>
</tbody>
</table>
<p>Table 11: MQM scores for PaLM WMT-dev random and SOTA systems, split into Accuracy and Fluency. Accuracy scores include "Accuracy/<em>," "Terminology/</em>," and "Non-translation!" error categories. Fluency scores include "Fluency/<em>," "Style/</em>," and "Locale/*" categories. The "Other" error category is not included in Accuracy or Fluency scores.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\mathrm{en} \rightarrow$ de</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">de $\rightarrow$ en</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOTA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOTA</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Major</td>
<td style="text-align: center;">minor</td>
<td style="text-align: center;">Major</td>
<td style="text-align: center;">minor</td>
<td style="text-align: center;">Major</td>
<td style="text-align: center;">minor</td>
<td style="text-align: center;">Major</td>
<td style="text-align: center;">minor</td>
</tr>
<tr>
<td style="text-align: center;">Non-translation!</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Acc/Mistrans.</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">49</td>
</tr>
<tr>
<td style="text-align: center;">Acc/Omission</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">Acc/Addition</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Acc/Untranslated</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Ter/Inappr</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: center;">Ter/Incons</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">Fl/Grammar</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">133</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">38</td>
</tr>
<tr>
<td style="text-align: center;">Fl/Register</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Fl/Inconsistency</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">Fl/Punctuation</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">260</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">29</td>
</tr>
<tr>
<td style="text-align: center;">Fl/Spelling</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">Fl/Encoding</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">St/Awkward</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">166</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">81</td>
</tr>
<tr>
<td style="text-align: center;">Locale/Date</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">Locale/Name</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">Locale/Time</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">Source Error</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Other</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Total Errors</td>
<td style="text-align: center;">142</td>
<td style="text-align: center;">673</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">362</td>
<td style="text-align: center;">189</td>
<td style="text-align: center;">296</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">281</td>
</tr>
</tbody>
</table>
<p>Table 12: MQM error counts for PaLM WMT-dev random and SOTA systems for en $\rightarrow$ de and de $\rightarrow$ en. Abbreviations are as follows: "Acc": Accuracy, "Fl": Fluency, "St": Style, "Ter": Terminology, "Inappr": Inappropriate for context, "Incons": Inconsistent.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SOTA</th>
<th style="text-align: center;">GTrans.</th>
<th style="text-align: center;">WMT-dev <br> random</th>
<th style="text-align: center;">high-end <br> random</th>
<th style="text-align: center;">WMT-full <br> random</th>
<th style="text-align: center;">kNN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">de $\rightarrow$ en</td>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">1.31</td>
<td style="text-align: center;">1.71</td>
<td style="text-align: center;">1.92</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">2.38</td>
<td style="text-align: center;">3.03</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOTA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.073</td>
<td style="text-align: center;">0.124</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-dev random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">high-end random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-full random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: center;">en $\rightarrow \mathrm{de}$</td>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">1.18</td>
<td style="text-align: center;">1.59</td>
<td style="text-align: center;">1.58</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">1.90</td>
<td style="text-align: center;">1.93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOTA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.003</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-dev random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">high-end random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-full random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.372</td>
</tr>
<tr>
<td style="text-align: center;">zh $\rightarrow$ en</td>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">3.12</td>
<td style="text-align: center;">3.60</td>
<td style="text-align: center;">3.89</td>
<td style="text-align: center;">3.95</td>
<td style="text-align: center;">4.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOTA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-dev random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.003</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">high-end random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.168</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-full random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.281</td>
</tr>
<tr>
<td style="text-align: center;">en $\rightarrow \mathrm{zh}$</td>
<td style="text-align: center;">MQM</td>
<td style="text-align: center;">2.47</td>
<td style="text-align: center;">3.23</td>
<td style="text-align: center;">3.24</td>
<td style="text-align: center;">3.70</td>
<td style="text-align: center;">4.35</td>
<td style="text-align: center;">5.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SOTA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-dev random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">high-end random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-full random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.0</td>
</tr>
</tbody>
</table>
<p>Table 13: p-values based on PERM-BOTH pair-wise significance testing (Deutsch et al., 2021). We highlight all numbers with $\mathrm{p}&lt;0.05$.</p>
<p>BLEU or 0.3 BLEURT for a 20-30\%-overlap. However, we should emphasize that the differences due to overlap are small overall, and certainly much smaller than expected if one looked only at the difference between original and clean scores.</p>
<h2>I Fixed versus random prompts</h2>
<p>The results from section 5.2 indicate that random selection from small, high-quality prompt pools can work better than trying to customize prompts for specific inputs. In this section we investigate the effect of using a single high-quality prompt for all inputs, chosen using a maximum-likelihood criterion. For convenience, we carried out experiments on the high-end pool with 1-shot paragraph prompts. For each prompt in the pool, we computed the probability of a set of held-out high-end paragraphs when PaLM was conditioned on that
prompt. We select the prompt that resulted in the highest probability for each language pair.</p>
<p>Table 18 compares this method to random selection from the high-end pool. For all language pairs except Chinese $\rightarrow$ English, the fixed prompt does as well or better than the average performance over 5 random runs where a different prompt is selected for each input during each run. In Chinese $\rightarrow$ English, the prompt that ranked 5th according to the probability criterion also outperformed the random average, suggesting problems with our held-out set for that language pair.</p>
<p>We conclude that using a single high-quality prompt can be a safer strategy than choosing a fresh randomly-selected prompt for each input. Model probability appears to be a reasonable criterion for judging quality, but we look forward to refining this heuristic in future work.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">"Wir haben die Pflichtaufgaben mit Meisterschaft und Pokal einfach hervorragend gemeistert.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">"Quite simply, we have excellently mastered the necessary tasks for the Championship and the Cup.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hyp</td>
<td style="text-align: center;">"We have simply mastered the tasks of the championship and the cup excellently.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 1</td>
<td style="text-align: center;">German: Mit einer verstärkten Mannschaft holte die Mannschaft das Double aus Meisterschaft und Pokal. English: The decision paid off as the team achieved a league and cup double.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 2</td>
<td style="text-align: center;">German: Darüber hinaus haben wir uns wichtige Meisterschaftspunkte im Kampf um den Vizetitel gesichert." English: We have furthermore secured some important championship points in the fight about the vice champion's title."</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 3</td>
<td style="text-align: center;">German: Bring deine Mannschaft durch alle Spiele der Europameisterschaft und gewinne den Pokal! English: Take your team all the way through the Euro Cup stages and lift the trophy!</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 4</td>
<td style="text-align: center;">German: So konnte er die französische Meisterschaft, den nationalen Pokal sowie den Supercup gewinnen. English: He helped the club to win the national championship and the Supercup.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 5</td>
<td style="text-align: center;">German: Roter Stern gewinnt in jener Saison das Double von Meisterschaft und Pokal. English: Red Star won their sixth double in this season.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hyp</td>
<td style="text-align: center;">"We have the compulsory tasks with mastery and cup simply superbly mastered.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 1</td>
<td style="text-align: center;">German: Im November letzten Jahres war ein einzelner Steinadler und auch Bussarde im Blick der Kamera. English: In November last year a solitary golden eagle and buzzards too appeared in web camera view.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 2</td>
<td style="text-align: center;">German: Teile: Modesto-14th Street, Stanislaus, California, Kalifornien-Luft-Qualitätsindex English: Share: Modesto-14th Street, Stanislaus, California, California Air Quality Index</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 3</td>
<td style="text-align: center;">German: So viel kostet ein Hotel in Chester English: How much does a hotel in Chester cost?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 4</td>
<td style="text-align: center;">German: "... alle Mitarbeiter, die alles tun, um hilfsbereit zu sein und sehr freundlich zu sein; köstliche Margaritas; Kolibris und Granatäpfel im Garten (sowie eine sehr freundliche Katze); Ein echtes Gefühl von Zuhause. " Aktionsangebot English: "... all staff, who go out of their way to be helpful and are extremely welcoming; delicious margaritas; hummingbirds and pomegranates in the garden (as well as a very friendly cat); a real home-from-home feeling. "</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 5</td>
<td style="text-align: center;">German: Gansevoort Land zum Verkauf English: Gansevoort Land for Sale</td>
</tr>
</tbody>
</table>
<p>(a) Example where $k \mathrm{NN}$ outperforms random selection.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Frei von Drogen veröffentlichte Green mit der Peter Green Splinter Group einige Alben, bis sich die Band 2004 auflöste.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">Free of drugs, Green and the Peter Green Splinter Group released various albums before the band split up in 2004.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hyp</td>
<td style="text-align: center;">The band released their debut album, The Last of the Great Pretenders, in 2003.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 1</td>
<td style="text-align: center;">German: Ab 1990 war er Sänger der Gruppe Talisman, mit der er sieben Studioalben veröffentlichte, bis sich die Band 2007 auflöste. English: From 1998 until his departure in 2007, he was the lead singer of the group Lonestar, which recorded seven studio albums on BNA Records during his tenure as lead vocalist.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 2</td>
<td style="text-align: center;">German: 2001 veröffentlichte die Band unter dem Namen Glass die rockige Single Out Of Nowhere, verpasste die Charts und löste sich im Anschluss auf. English: Around this time he wrote and presented the ITV Network productions The Rock that Doesn't Roll and The Rock That Rolled Away.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 3</td>
<td style="text-align: center;">German: Mit ihrer Band Ex Cops veröffentlichte sie zwei Alben, bevor sich die Band 2015 auflöste. English: Their new band released two EPs before signing to Apparition Records in 2011.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 4</td>
<td style="text-align: center;">German: In seiner Jugend gründete David Haering die Punk-Band Side Effect, mit der er drei Alben und eine EP veröffentlichte. English: Peter Hajba and Alexander Brandon used OpenMPT to compose the soundtracks for Bejeweled 2, Bejeweled 3 and other PopCap games.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 5</td>
<td style="text-align: center;">German: Nach der Veröffentlichung des Live-Albums Beast from the East 1988 trennten sich die Wege der Musiker, als Don Dokken die Band auflöste. English: In 1988, after the Monsters of Rock Tour and a further platinum album, Don Dokken decided to break up the band and they went on their separate ways.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hyp</td>
<td style="text-align: center;">Free from drugs, Green released several albums with the Peter Green Splinter Group, until the band broke up in 2004.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 1</td>
<td style="text-align: center;">German: $250 \mathrm{gr} / \mathrm{m} 2$ : eine Reihe merino intermedia, vielseitigkeit und schutz garantiert. English: 250 $\mathrm{gr} / \mathrm{m} 2$ : Range merino intermediate, versatility and guaranteed protection.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 2</td>
<td style="text-align: center;">German: 127 Moo.3, Choeng Thale, Thalang, Phuket, Strand Bang Tao, Thailand (Karte anzeigen)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 3</td>
<td style="text-align: center;">English: 127 Moo.3, Choeng Thale, Thalang, Phuket, Bang Tao Beach (Phuket), Thailand (Show map)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 4</td>
<td style="text-align: center;">German: Ich bin stolz, sagen zu können, dass Ihr Produkt mir die Größe verliehen hat, von der ich jahrelang geträumt habe. English: I am proud to say that your product has given me the size that I sought for all these years.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 5</td>
<td style="text-align: center;">German: In der NBA Saison 2003-2004 schien es endlich besser für Garnett und die Timberwolves zu laufen. English: The 2002-03 season seemed to look up for the Timberwolves.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Prompt 5</td>
<td style="text-align: center;">German: APC-Fhrancis Oliver Lopez extern natürliche Links English: APC-Fhrancis Oliver Lopez External Passing Juice</td>
</tr>
</tbody>
</table>
<p>(b) Example where random selection outperforms $k \mathrm{NN}$.</p>
<p>Table 14: Example prompts for random and $k \mathrm{NN}$ Roberta selection for the German $\rightarrow$ English translation direction.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">Wegen eines ersten offiziellen Corona-Verdachtsfalls im Land hat Nordkorea aus Furcht vor einer Katastrophe <br> eine Großstadt komplett abgeriegelt.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">Due to a first official suspected case of coronavirus in the country, North Korea has completely shut down a <br> major city out of fear of a disaster.</td>
</tr>
<tr>
<td style="text-align: left;">SOTA</td>
<td style="text-align: left;">Due to a first official suspected case of coronavirus in the country, North Korea has completely sealed off a <br> major city for fear of a catastrophe. <br> North Korea has completely sealed off a city of 300,000 people because of a first suspected case of coronavirus <br> in the country, fearing a catastrophe.</td>
</tr>
</tbody>
</table>
<p>(a) PaLM generated two errors: (i) Makes the assumption that the major city has 300,000 citizens, (ii) Forgot to translate the word official. SOTA generates a perfect translation. Note: this example also exemplifies PaLM's less literal translation style.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">Für mindestens drei Menschen endet die Tour mit dem Motorrad tödlich.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">For at least three people, their bike ride ended in death.</td>
</tr>
<tr>
<td style="text-align: left;">SOTA</td>
<td style="text-align: left;">The motorcycle tour ends fatally for at least three people.</td>
</tr>
<tr>
<td style="text-align: left;">PaLM</td>
<td style="text-align: left;">At least three people die in motorcycle accidents.</td>
</tr>
</tbody>
</table>
<p>(b) The source mentions a single accident happening on a bike tour. PaLM refers to multiple accidents happening independently.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">Ein Zeuge hörte gegen 3.40 Uhr Geräusche in der Talstraße und lief in Richtung des Imbisses.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">One witness heard noises on Talstraße around 3:40 am and ran in the direction of the snack stand.</td>
</tr>
<tr>
<td style="text-align: left;">SOTA</td>
<td style="text-align: left;">A witness heard noises in the valley road at around 3:40 a.m. and ran towards the snack bar.</td>
</tr>
<tr>
<td style="text-align: left;">PaLM</td>
<td style="text-align: left;">A witness heard noises in Talstraße at around 3:40 a.m. and ran towards the snack bar.</td>
</tr>
</tbody>
</table>
<p>(c) SOTA generates an overly-literal translation, resulting in copying the street name (Talstrasse) and using the wrong time format.</p>
<p>Table 15: Example translations from newstest2021 German $\rightarrow$ English. PaLM translations are generated with the high-end prompt pool. These are typical of error patterns observed in the translation output. We also observed the same pattern when using WMT-dev as the prompt pool. In general, SOTA is more faithful to the source while PaLM generates less literal translations that occasionally miss some information from the source.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">French World Cup winner Dembele, who has struggled for game time at the Camp Nou, was recently linked <br> with a move to PSG in a swap deal with Neymar. <br> 在诺坎普球场冲锋陷阵的法国世界杯冠军得主Dembele 最近通过与Neymar 交换转投PSG。</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SOTA</td>
<td style="text-align: left;">法国世界杯冠军登贝莱在诺坎普一直在为比赛时间而挣扎，最近他与内马尔交换转会巴黎圣日尔</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">曼。</td>
</tr>
<tr>
<td style="text-align: left;">PaLM</td>
<td style="text-align: left;">法国世界杯冠军德容，在诺坎普的出场时间一直不多，最近被传与内马尔进行交换加钱转会 <br> 到PSG。</td>
</tr>
</tbody>
</table>
<p>(a) PaLM produces two errors: (i) translates a wrong player's name; (ii) adds extra information that the player received a raise in the swap deal. SOTA produces a perfect translation, but is much more literal than PaLM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">...in the wake of September 11, ASIO was given power to compulsorily question people for up to seven days <br> in relation to terrorism offences.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">...在911事件之后，澳安全情报局有权对牵涉恐怖主义行为的人员进行为期最高7天的强制性询</td>
</tr>
<tr>
<td style="text-align: left;">问。</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">SOTA</td>
<td style="text-align: left;">...在9月11日之后，安全情报组织被授权对与恐怖主义罪行有关的人进行长达7天的强制性讯问。</td>
</tr>
<tr>
<td style="text-align: left;">PaLM</td>
<td style="text-align: left;">...澳大利亚安全情报局在9-11恐怖袭击之后获得了强制询问人员的权力，可以在7天内就恐怖主义 <br> 罪行进行询问。</td>
</tr>
</tbody>
</table>
<p>(b) The source phrase "September 11" is translated literally by SOTA into a date, whereas PaLM produces a more appropriate translation by describing it as a terrorist attack.</p>
<p>Table 16: Example translations from newstest2021 English $\rightarrow$ Chinese. PaLM translations are generated with the WMT-dev prompt pool. We find SOTA to generate more literal translations than PaLM, but PaLM suffers from more omissions and mistranslations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Data</th>
<th style="text-align: center;">\%Clean</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BLEURT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Orig</td>
<td style="text-align: center;">Clean</td>
<td style="text-align: center;">$\neg$ Clean</td>
<td style="text-align: center;">Orig</td>
<td style="text-align: center;">Clean</td>
<td style="text-align: center;">$\neg$ Clean</td>
</tr>
<tr>
<td style="text-align: center;">de $\rightarrow$ en 2016</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">81.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-full Random</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">81.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Diff</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">$-0.2$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{fr} \rightarrow$ en 2014</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">Google Trans.</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">79.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT-dev Random</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">79.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Diff</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">$-0.6$</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">$-0.1$</td>
<td style="text-align: center;">0.1</td>
</tr>
</tbody>
</table>
<p>Table 17: Comparison between Google Translate and 5-shot PaLM using three test sets: Orig. (original), Clean (overlapping examples removed) and $\neg$ Clean (including only overlapping examples). We use Random instead of WMT-dev Random for de $\rightarrow$ en to avoid using the WMT 2021 development sets to prompt for the WMT 2016 test ("sampling from the future").</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LP</th>
<th style="text-align: center;">Selection</th>
<th style="text-align: center;">BLEURT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">min</td>
<td style="text-align: center;">avg</td>
<td style="text-align: center;">max</td>
</tr>
<tr>
<td style="text-align: center;">en $\rightarrow$ de</td>
<td style="text-align: center;">fixed <br> random</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">75.0</td>
</tr>
<tr>
<td style="text-align: center;">de $\rightarrow$ en</td>
<td style="text-align: center;">fixed <br> random</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">en $\rightarrow$ zh</td>
<td style="text-align: center;">fixed <br> random</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">64.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">zh $\rightarrow$ en</td>
<td style="text-align: center;">fixed <br> random</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">67.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">en $\rightarrow$ fr</td>
<td style="text-align: center;">fixed <br> random</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">75.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{fr} \rightarrow$ en</td>
<td style="text-align: center;">fixed <br> random</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">77.6</td>
</tr>
</tbody>
</table>
<p>Table 18: Fixed (maximum-likelihood) prompts vs random prompts. All prompts are drawn from the high-end pool, and performance is measured on the standard test sets (WMT21 for German and Chinese, WMT14 for French). The scores for random selection are the minimum, average, and maximum over 5 random draws.</p>
<h1>A For every submission:</h1>
<p>A1. Did you describe the limitations of your work?
Unnumbered Limitations section immediately after Conclusion.
A2. Did you discuss any potential risks of your work?
Unnumbered Ethical Considerations section immediately after Limitations.
A3. Do the abstract and introduction summarize the paper's main claims?
Abstract, Section 1.
\ A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B Did you use or create scientific artifacts?</h2>
<h2>Section 4</h2>
<p>B1. Did you cite the creators of artifacts you used?
Sections 4, 5
\ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
We did not discuss licenses in the paper, but we verified that our use of materials was permitted. We are not distributing artifacts.
B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
We partially address this question in our Ethical Considerations section; as mentioned above, in general we ensured that our use of materials was permitted.</p>
<p>B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
Not applicable. Left blank.
B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
Sections 4, Appendix
B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
Sections 4, Appendix</p>
<h2>C Did you run computational experiments?</h2>
<h2>Section 5</h2>
<p>C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>We provide partial answers to this question in sections 1 and 5. We are not authorized to provide full details.
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ We chose Google Translate for comparison because it is non-trivial to build a SOTA baseline for older WMT scenarios. Through personal communication, we understand that Google Translate has no overlap with WMT test sets.
${ }^{12}$ The difference in difficulty between Clean and $\neg$ Clean for systems without overlap is not easily explained. A common difficulty indicator is sentence length, but average lengths, as measured by number of SACREBLEU tokens per sentence, are similar between Clean and $\neg$ Clean for both de $\rightarrow$ en (23.8 versus 23.0 ) and $\mathrm{fr} \rightarrow$ en (21.1 versus 22.7 ).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ As identified by SACREBLEU.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
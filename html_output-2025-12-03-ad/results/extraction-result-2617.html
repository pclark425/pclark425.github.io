<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2617 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2617</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2617</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-cd5a26b89f0799db1cbc1dff5607cb6815739fe7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cd5a26b89f0799db1cbc1dff5607cb6815739fe7" target="_blank">A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions using the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function.</p>
                <p><strong>Paper Abstract:</strong> We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2617.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2617.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization of Expensive Cost Functions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential, model-based optimization framework that places a prior (commonly a Gaussian process) over an expensive black-box objective and chooses new evaluations by maximizing an acquisition (utility) function that trades off exploration and exploitation to minimize the number of costly evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Places a prior distribution (typically a Gaussian process) over unknown objective functions, updates a posterior after each expensive observation, and selects the next evaluation point by optimizing an acquisition function (e.g., EI, PI, UCB). Key components: (1) surrogate model (GP) with kernel/hyperparameters learned by marginal likelihood; (2) acquisition function encoding expected utility; (3) inner optimization of acquisition function (e.g., DIRECT or multistart) to pick next expensive experiment; (4) update loop with noise handling (additive Gaussian noise model).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General-purpose black-box optimization where function evaluations are expensive — examples discussed: simulation-based planning, active user modelling (preference elicitation), hierarchical reinforcement learning; broadly applicable to scientific experimental design (e.g., engineering, robotics, hyperparameter tuning, drug/simulation studies).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequential myopic allocation: at each step, compute a posterior predictive mean and variance from the GP and choose the next experiment as the argmax of an acquisition function that encodes the expected utility of spending one additional evaluation there. The allocation implicitly trades off the expected benefit of improvement against uncertainty (exploration) without explicit global planning except where multistep variants are used.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Primarily measured in number of expensive objective evaluations (function queries); the paper emphasizes minimizing these. Computational overhead of the surrogate (GP) and acquisition maximization is treated as relatively cheap compared to objective evaluations but no explicit FLOP or time metrics are given.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not framed as mutual information except in related work; acquisition functions operationalize expected utility: Expected Improvement (EI), Probability of Improvement (PI) and Upper Confidence Bound (UCB) are used as proxies for expected information/utility; multistep expected-improvement variants are discussed for non-myopic information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Encoded in the acquisition function: EI and PI combine posterior mean and variance to balance exploitation (high mean) and exploration (high variance); UCB explicitly adds a multiple of the posterior standard deviation to the mean (mu + kappa*sigma). Parameters (e.g., xi in EI/PI or kappa/tau_t in UCB) control the strength of exploration; some schedules (e.g., tau_t in GP-UCB) provide theoretical no-regret guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit mechanism for promoting diversity of hypotheses beyond the exploration pressure induced by acquisition functions (variance term encourages sampling in uncertain regions). The paper notes portfolio and bandit-based meta-selection of acquisition functions (see separate entry) which can diversify search behavior across strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of expensive evaluations / general cost-per-evaluation budget (implicit). The paper frames optimization under a small evaluation budget and focuses on minimizing evaluations; also mentions handling noisy/costly experiments (e.g., drug trials, destructive tests).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Budget handled implicitly by minimizing number of function evaluations via greedy (myopic) expected-utility (acquisition) selection; variants include intensification (resampling incumbents) for noisy budgets and multistep planning for anticipating future budget use (not typically used due to computational cost).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement-over-incumbent metrics: acquisition functions use expected improvement or probability of improvement relative to the current best observation (or current best posterior mean under noise). No explicit novelty or external impact metric for 'breakthrough' beyond large expected improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Tutorial-level qualitative metrics: efficiency measured in number of evaluations required to reach high objective values; no specific numerical experimental results presented in this part of the tutorial (recommendations and behavior illustrations are qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to random search, brute-force grid search and classical global optimization approaches; acquisition strategies (PI, EI, UCB) compared against each other; related methods: kriging/DACE/EGO from experimental design literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative claims: Bayesian optimization is more sample-efficient (fewer expensive evaluations) than naive methods; different acquisition functions yield different sampling trajectories (e.g., EI tends to exploit more locally while GP-UCB explores more). No numeric head-to-head results reported in the tutorial sections reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Described qualitatively as requiring far fewer function evaluations than naive/stochastic methods for expensive objectives; specific percentages or speedups are not provided in the tutorial text.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Yes — the paper analyzes the exploration-exploitation trade-off via acquisition functions, sensitivity of PI to target parameter (xi), the EI formulation (including xi) and GP-UCB with theoretically motivated exploration schedules. It discusses myopic (one-step) vs multistep planning: myopic is computationally cheaper but may miss long-term benefits; multistep expected improvement exists but is more expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key recommendations: use GP surrogate models with appropriate kernels and hyperparameter learning; use acquisition functions (EI is a strong practical default with small xi ~0.01 recommended by Lizotte); GP-UCB with theoretically-specified schedules gives regret guarantees; for robustness, a portfolio/meta-selection of acquisition functions can outperform single acquisitions. Multistep (non-myopic) strategies can improve decisions but are usually too costly except in limited settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning', 'publication_date_yy_mm': '2010-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2617.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2617.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Improvement (EI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition function that selects the next evaluation point by maximizing the expectation of improvement over the current best (incumbent) under the GP predictive distribution, combining both mean and variance analytically.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expected Improvement (EI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Defines improvement I(x)=max{0, f(x)-f(x^+)} and computes its expectation under the GP posterior analytically: EI(x) = (mu(x)-f(x^+)) Phi(Z) + sigma(x) phi(Z), where Z = (mu(x)-f(x^+))/sigma(x). A trade-off parameter xi can be added to encourage exploration: replace f(x^+) with f(x^+)+xi. EI is typically maximized greedily (myopically) at each step to choose the next expensive experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General black-box, expensive evaluation optimization — used in experimental design, engineering optimization, active preference learning examples in the tutorial.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate the next evaluation to the argmax of EI; points with either high posterior mean or high posterior variance (or both) get high EI and thus resources. The method is inherently one-step-ahead (myopic) unless extended to multistep EI variants.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Implicitly number of expensive evaluations; computing EI requires GP predictive mean/variance (matrix solves dominated by O(n^3) for GP hyperparameter updates) and evaluation of closed-form EI expression — acquisition optimization is cheap compared to objective evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>EI itself is the expected improvement (expected utility) — a proxy for information or value of evaluation; not framed as mutual information but as expected reduction in regret relative to current best.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balance via analytic EI expression: the sigma(x) phi(Z) term rewards high uncertainty (exploration) while the (mu(x)-f^+) Phi(Z) term rewards high mean (exploitation). Parameter xi can bias toward more exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting term beyond the variance-driven exploration component; EI can become locally exploitative if xi is too small and the posterior heavily favors a region.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Designed for fixed/limited number of expensive evaluations (implicit), with a goal to minimize total evaluations needed to reach good solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Myopic maximization of EI at each step attempts to make the best use of the immediate budget (one evaluation at a time); optional schedules for xi have been proposed (but empirically cooling xi did not help much in some studies).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>A large expected improvement value corresponds to large potential discovery; EI directly values magnitude of potential improvement rather than just probability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Analytical expression for acquisition; empirical tuning recommendations (e.g., xi≈0.01 often works well per Lizotte) but no numeric performance figures reported in tutorial text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to PI and GP-UCB and its sensitivity contrasted with PI; EI often preferred because it accounts for magnitude of improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative: EI tends to balance exploration and exploitation better than PI (which can be overly greedy) and often performs competitively; exact numeric comparisons are not included in the tutorial text.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported qualitatively as sample-efficient compared to naive sampling; no explicit numeric gains provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses how EI balances exploration/exploitation, how including xi affects tradeoff and notes possible myopia; mentions analytical multistep EI extensions but highlights higher computational cost for non-myopic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>EI is a strong practical acquisition choice; include small positive xi (Lizotte suggests 0.01 scaled) to encourage exploration; be aware of myopia and consider multistep EI only when computational budget allows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning', 'publication_date_yy_mm': '2010-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2617.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2617.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probability of Improvement (PI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition function that scores candidate points by the posterior probability that they will improve on the current incumbent by at least a threshold, favoring points with a high chance to exceed the incumbent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Probability of Improvement (PI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PI(x) = Phi((mu(x)-f(x^+)-xi)/sigma(x)), where Phi is the standard normal CDF and xi≥0 is a user parameter controlling exploration; selects the next evaluation at the x that maximizes PI. It focuses on the probability a point yields any improvement above a threshold rather than expected magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General black-box expensive optimization and certain perceptual/preference modelling tasks where thresholded improvement is meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate next evaluation to maximize PI; favors points with high probability to beat the incumbent (exploitation) unless xi or schedule is used to encourage exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive evaluations; computing PI requires GP mean/variance and evaluation of the normal CDF — acquisition optimization cheap relative to objective.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Probability of improvement (a probability-based expected utility), not mutual information; focuses on chance of improvement rather than expected magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Controlled by xi: small xi leads to greedy, local exploitation; larger xi biases toward global exploration. Kushner suggested schedules for xi to encourage exploration early and decrease over time.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity promotion other than exploration encouraged by larger xi values.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Implicitly designed to operate under a limited number of expensive evaluations; user may schedule xi based on remaining budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>User-controlled xi (possibly scheduled) to manage exploration when budget is ample versus exploitation when budget dwindles.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement relative to incumbent with threshold xi; does not account for magnitude beyond the threshold except through the choice of xi.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Analytical form; empirical sensitivity to xi discussed qualitatively (too small => local, too large => slow fine-tuning); no numeric experiments reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively to EI and UCB; PI is noted to be sensitive to xi and can be overly greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative: PI can perform well but is very sensitive to the target parameter; EI often preferred because it accounts for improvement magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not quantified in tutorial; described qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Yes — sensitivity to xi, exploration vs exploitation tradeoff discussed and recommended to tune xi or schedule it (Kushner's suggestion).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>PI's greediness requires careful choice of xi; if xi is scheduled appropriately PI can explore early and exploit later, but EI is generally more robust.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning', 'publication_date_yy_mm': '2010-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2617.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2617.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process Upper Confidence Bound (GP-UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition strategy that selects points by maximizing mu(x) + beta_t^{1/2} sigma(x), where beta_t (or related schedule) scales exploration; has theoretical no-regret guarantees when beta_t is chosen properly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP-UCB (Upper Confidence Bound)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At iteration t, select x maximizing mu(x) + sqrt(nu*tau_t) * sigma(x) (notation in paper), where tau_t is a time-dependent term (e.g., 2 log(t^{d/2+2} pi^2 / 3 delta)) and nu>0. The sigma term explicitly drives exploration; with appropriate schedule GP-UCB achieves sublinear cumulative regret bounds (no-regret asymptotics).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Black-box expensive function optimization; formulated under a multi-armed bandit perspective and applicable where regret guarantees are desired (e.g., experimental design with many rounds).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate to maximize mu(x)+exploration_bonus; exploration bonus increases when uncertainty is large or when schedule calls for more exploration, providing an explicit tradeoff schedule across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Primary focus on number of expensive evaluations and cumulative regret over T iterations; GP computations required for mu and sigma but acquisition evaluation is cheap relative to objective evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly mutual information — uses posterior variance as a proxy for potential information/value and incorporates it additively into the utility (upper confidence bound).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit via the additive uncertainty term scaled by a time-dependent parameter: exploration strength controlled by sqrt(nu * tau_t). With tau_t chosen per theory, GP-UCB balances exploration/exploitation and yields no-regret guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity engine; exploration term tends to encourage sampling across uncertain regions, which indirectly increases diversity of sampled hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed horizon T (number of iterations) factored into the theoretical schedule tau_t; budget is explicitly considered in regret bounds and parameter schedules.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handles budget via time-dependent exploration schedule tau_t that depends on t and problem dimension; theoretical bounds ensure cumulative regret scales sublinearly with T if schedule chosen correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not explicitly defined beyond regret and improvement relative to optimum; selection encourages discovery by exploring high-uncertainty regions that may contain large optima.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theoretical cumulative regret R_T = sum_t (f(x^*) - f(x_t)) and asymptotic no-regret property (R_T / T -> 0) when tau_t chosen as in the paper. No numeric experiments are presented here in the tutorial excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to EI and PI; GP-UCB tends to explore more persistently while EI may become locally exploitative.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative: GP-UCB can continue exploring regions that EI may abandon, offering stronger theoretical guarantees (no-regret) but may be slower to fine-tune in practice depending on parameter choices.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>No empirical numbers given; theory gives asymptotic guarantees on regret implying eventual efficiency, but practical gains depend on kernel smoothness and parameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Yes — the paper discusses how the schedule tau_t controls exploration intensity and how acquisition behavior differs from EI/PI; GP-UCB's theoretical framework explicitly links exploration schedule to regret bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Choosing the exploration schedule (tau_t) per the theoretical prescriptions yields provable no-regret behavior. In practice, parameter choices like nu and tau_t control the exploration-exploitation balance and must be set carefully.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning', 'publication_date_yy_mm': '2010-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2617.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2617.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Acquisition Portfolio</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Portfolio of Acquisition Functions with Bandit Meta-Selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-strategy that maintains multiple acquisition functions and allocates which acquisition to use at each step via an online multi-armed bandit algorithm, aiming to adaptively select the best acquisition strategy for the problem.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Acquisition Function Portfolio (bandit-governed)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Instead of committing to a single acquisition function, the system maintains a portfolio of acquisition functions (e.g., EI, PI, UCB, etc.) and uses an online multi-armed bandit algorithm to select which acquisition to deploy at each iteration. Rewards for the bandit arms come from realized improvement or other performance signals; the approach leverages bandit learning to adaptively allocate the limited evaluation budget across acquisition strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General Bayesian optimization and experimental design settings where different acquisition heuristics perform variably across problems; intended to improve robustness across problem instances.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate the next expensive evaluation by first selecting an acquisition function via the meta-bandit, then maximizing that acquisition to pick a point. The bandit adjusts probabilities/weights over acquisition functions based on past rewards, thus reallocating future computational budget among heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive function evaluations remains primary; additional light-weight overhead for bandit bookkeeping and occasional extra acquisition optimization runs, but these are small relative to objective evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicitly uses the acquisition functions' utility signals (e.g., EI value, realized improvement) as the reward signal for the bandit; not formulated explicitly as mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Two-level: (1) within each acquisition, standard exploration-exploitation (mean vs variance); (2) at meta-level, bandit balances exploration over acquisition strategies (trying different acquisition arms) vs exploitation (using those that have recently yielded good rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Promotes diversity indirectly by allowing multiple acquisition heuristics with different exploratory behavior to be selected over time; the bandit meta-controller can encourage trying less-used acquisitions (exploration across strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Limited number of expensive evaluations; the portfolio aims to reallocate the evaluation budget adaptively across acquisition strategies to maximize performance under this limit.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Treats each expensive evaluation as a resource; the bandit meta-policy uses observed returns to bias selection toward acquisition strategies that deliver better improvement-per-evaluation given the remaining budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Bandit reward signals are based on realized improvements (magnitude or probability), so breakthroughs are implicitly valued as high reward events that increase selection probability of the enabling acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports that the portfolio 'almost always outperforms the best individual acquisition function' on a suite of test problems (qualitative claim in tutorial); no numeric metrics are included in the tutorial excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Single acquisition functions (EI, PI, GP-UCB) as baselines; portfolio is compared against each individual acquisition in experiments described in Brochu et al. [2010b] (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported qualitatively to outperform the best single acquisition on standard test problems; specific numerical comparisons are not provided in this tutorial excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative improvement in robustness and empirical performance across problems; no specific percentages or runtime gains given in the tutorial excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses the benefit of hedging across acquisitions to mitigate the risk of a single poor choice (no detailed cost/information tradeoff numbers in tutorial excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: when unsure which acquisition function is best for a new problem, run a portfolio governed by an online bandit to adaptively allocate evaluations among acquisition heuristics; this tends to yield more reliable performance than committing to a single acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning', 'publication_date_yy_mm': '2010-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2617.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2617.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EGO / DACE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient Global Optimization (EGO) / Design and Analysis of Computer Experiments (DACE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A kriging/Gaussian process-based sequential experimental design approach that pairs a response-surface model (DACE/kriging) with a sequential expected-improvement (EI) criterion (EGO) to optimize expensive computer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Design and Analysis of Computer Experiments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>EGO (Efficient Global Optimization) / DACE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DACE refers to the Gaussian-process (kriging) modelling framework for computer experiments; EGO is the sequential optimization algorithm that couples DACE with the Expected Improvement acquisition to select expensive simulation points sequentially. Typical steps: fit kriging surrogate, compute EI, optimize EI to pick next sample, update surrogate, repeat.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Computer experiments, simulation-based scientific experiments, engineering design optimization where true evaluations are computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequentially allocate simulation runs to the argmax of EI computed on the kriging/DACE surrogate; aims to reduce number of expensive simulation runs needed to find optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive simulation runs (function evaluations); surrogate fitting (kriging) cost also considered but secondary.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Improvement under the kriging posterior used as the acquisition (value-of-information proxy), rather than explicit mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>EI balances mean and variance of the kriging surrogate; DACE/kriging covariance choices determine smoothness assumptions which influence where uncertainty is high and thus where exploration is encouraged.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism beyond surrogate-driven exploration via variance; DACE/kriging sometimes uses localized neighbor models in practice which can diversify local search behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed/limited number of simulation runs (explicit experimental-design context).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Sequential selection via EI to maximize expected utility per expensive simulation under a limited budget; classical experimental design criteria (A-, D-, E-optimality) are discussed in connection to parameter estimation problems but EGO focuses on optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Large expected improvement values correspond to potential high-impact (breakthrough) discoveries relative to current best; no separate novelty or breakthrough scoring is defined.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Historical experimental design literature reports fewer simulations to reach good solutions compared to naive designs; tutorial does not present new numeric EGO results but references Jones et al. [1998] for empirical studies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared historically with grid search, random designs, classical non-adaptive experimental designs; EGO aims to be more sample-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitative claim: EGO reduces required function evaluations compared to non-adaptive designs; numeric results are in the cited EGO literature (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not quantified in the tutorial excerpt; referenced literature demonstrates substantial reductions in simulation runs in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Discusses relation of kriging model assumptions and kernel choices to exploration behavior; contrasts non-adaptive classical experimental design with sequential EGO approach.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Sequential, model-based allocation via EI on a kriging surrogate is recommended for expensive computer experiments; appropriate kernel selection and hyperparameter estimation are important for good allocation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning', 'publication_date_yy_mm': '2010-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Design and Analysis of Computer Experiments <em>(Rating: 2)</em></li>
                <li>Efficient Global Optimization <em>(Rating: 2)</em></li>
                <li>Sequential Design for Optimization <em>(Rating: 1)</em></li>
                <li>Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design <em>(Rating: 2)</em></li>
                <li>A Multi-armed Bandit Approach to Adaptive Acquisition in Bayesian Optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2617",
    "paper_id": "paper-cd5a26b89f0799db1cbc1dff5607cb6815739fe7",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "Bayesian Optimization",
            "name_full": "Bayesian Optimization of Expensive Cost Functions",
            "brief_description": "A sequential, model-based optimization framework that places a prior (commonly a Gaussian process) over an expensive black-box objective and chooses new evaluations by maximizing an acquisition (utility) function that trades off exploration and exploitation to minimize the number of costly evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Bayesian Optimization",
            "system_description": "Places a prior distribution (typically a Gaussian process) over unknown objective functions, updates a posterior after each expensive observation, and selects the next evaluation point by optimizing an acquisition function (e.g., EI, PI, UCB). Key components: (1) surrogate model (GP) with kernel/hyperparameters learned by marginal likelihood; (2) acquisition function encoding expected utility; (3) inner optimization of acquisition function (e.g., DIRECT or multistart) to pick next expensive experiment; (4) update loop with noise handling (additive Gaussian noise model).",
            "application_domain": "General-purpose black-box optimization where function evaluations are expensive — examples discussed: simulation-based planning, active user modelling (preference elicitation), hierarchical reinforcement learning; broadly applicable to scientific experimental design (e.g., engineering, robotics, hyperparameter tuning, drug/simulation studies).",
            "resource_allocation_strategy": "Sequential myopic allocation: at each step, compute a posterior predictive mean and variance from the GP and choose the next experiment as the argmax of an acquisition function that encodes the expected utility of spending one additional evaluation there. The allocation implicitly trades off the expected benefit of improvement against uncertainty (exploration) without explicit global planning except where multistep variants are used.",
            "computational_cost_metric": "Primarily measured in number of expensive objective evaluations (function queries); the paper emphasizes minimizing these. Computational overhead of the surrogate (GP) and acquisition maximization is treated as relatively cheap compared to objective evaluations but no explicit FLOP or time metrics are given.",
            "information_gain_metric": "Not framed as mutual information except in related work; acquisition functions operationalize expected utility: Expected Improvement (EI), Probability of Improvement (PI) and Upper Confidence Bound (UCB) are used as proxies for expected information/utility; multistep expected-improvement variants are discussed for non-myopic information gain.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Encoded in the acquisition function: EI and PI combine posterior mean and variance to balance exploitation (high mean) and exploration (high variance); UCB explicitly adds a multiple of the posterior standard deviation to the mean (mu + kappa*sigma). Parameters (e.g., xi in EI/PI or kappa/tau_t in UCB) control the strength of exploration; some schedules (e.g., tau_t in GP-UCB) provide theoretical no-regret guarantees.",
            "diversity_mechanism": "No explicit mechanism for promoting diversity of hypotheses beyond the exploration pressure induced by acquisition functions (variance term encourages sampling in uncertain regions). The paper notes portfolio and bandit-based meta-selection of acquisition functions (see separate entry) which can diversify search behavior across strategies.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of expensive evaluations / general cost-per-evaluation budget (implicit). The paper frames optimization under a small evaluation budget and focuses on minimizing evaluations; also mentions handling noisy/costly experiments (e.g., drug trials, destructive tests).",
            "budget_constraint_handling": "Budget handled implicitly by minimizing number of function evaluations via greedy (myopic) expected-utility (acquisition) selection; variants include intensification (resampling incumbents) for noisy budgets and multistep planning for anticipating future budget use (not typically used due to computational cost).",
            "breakthrough_discovery_metric": "Improvement-over-incumbent metrics: acquisition functions use expected improvement or probability of improvement relative to the current best observation (or current best posterior mean under noise). No explicit novelty or external impact metric for 'breakthrough' beyond large expected improvement.",
            "performance_metrics": "Tutorial-level qualitative metrics: efficiency measured in number of evaluations required to reach high objective values; no specific numerical experimental results presented in this part of the tutorial (recommendations and behavior illustrations are qualitative).",
            "comparison_baseline": "Compared conceptually to random search, brute-force grid search and classical global optimization approaches; acquisition strategies (PI, EI, UCB) compared against each other; related methods: kriging/DACE/EGO from experimental design literature.",
            "performance_vs_baseline": "Qualitative claims: Bayesian optimization is more sample-efficient (fewer expensive evaluations) than naive methods; different acquisition functions yield different sampling trajectories (e.g., EI tends to exploit more locally while GP-UCB explores more). No numeric head-to-head results reported in the tutorial sections reproduced here.",
            "efficiency_gain": "Described qualitatively as requiring far fewer function evaluations than naive/stochastic methods for expensive objectives; specific percentages or speedups are not provided in the tutorial text.",
            "tradeoff_analysis": "Yes — the paper analyzes the exploration-exploitation trade-off via acquisition functions, sensitivity of PI to target parameter (xi), the EI formulation (including xi) and GP-UCB with theoretically motivated exploration schedules. It discusses myopic (one-step) vs multistep planning: myopic is computationally cheaper but may miss long-term benefits; multistep expected improvement exists but is more expensive.",
            "optimal_allocation_findings": "Key recommendations: use GP surrogate models with appropriate kernels and hyperparameter learning; use acquisition functions (EI is a strong practical default with small xi ~0.01 recommended by Lizotte); GP-UCB with theoretically-specified schedules gives regret guarantees; for robustness, a portfolio/meta-selection of acquisition functions can outperform single acquisitions. Multistep (non-myopic) strategies can improve decisions but are usually too costly except in limited settings.",
            "uuid": "e2617.0",
            "source_info": {
                "paper_title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning",
                "publication_date_yy_mm": "2010-12"
            }
        },
        {
            "name_short": "EI",
            "name_full": "Expected Improvement (EI)",
            "brief_description": "An acquisition function that selects the next evaluation point by maximizing the expectation of improvement over the current best (incumbent) under the GP predictive distribution, combining both mean and variance analytically.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Expected Improvement (EI)",
            "system_description": "Defines improvement I(x)=max{0, f(x)-f(x^+)} and computes its expectation under the GP posterior analytically: EI(x) = (mu(x)-f(x^+)) Phi(Z) + sigma(x) phi(Z), where Z = (mu(x)-f(x^+))/sigma(x). A trade-off parameter xi can be added to encourage exploration: replace f(x^+) with f(x^+)+xi. EI is typically maximized greedily (myopically) at each step to choose the next expensive experiment.",
            "application_domain": "General black-box, expensive evaluation optimization — used in experimental design, engineering optimization, active preference learning examples in the tutorial.",
            "resource_allocation_strategy": "Allocate the next evaluation to the argmax of EI; points with either high posterior mean or high posterior variance (or both) get high EI and thus resources. The method is inherently one-step-ahead (myopic) unless extended to multistep EI variants.",
            "computational_cost_metric": "Implicitly number of expensive evaluations; computing EI requires GP predictive mean/variance (matrix solves dominated by O(n^3) for GP hyperparameter updates) and evaluation of closed-form EI expression — acquisition optimization is cheap compared to objective evaluations.",
            "information_gain_metric": "EI itself is the expected improvement (expected utility) — a proxy for information or value of evaluation; not framed as mutual information but as expected reduction in regret relative to current best.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balance via analytic EI expression: the sigma(x) phi(Z) term rewards high uncertainty (exploration) while the (mu(x)-f^+) Phi(Z) term rewards high mean (exploitation). Parameter xi can bias toward more exploration.",
            "diversity_mechanism": "No explicit diversity-promoting term beyond the variance-driven exploration component; EI can become locally exploitative if xi is too small and the posterior heavily favors a region.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Designed for fixed/limited number of expensive evaluations (implicit), with a goal to minimize total evaluations needed to reach good solutions.",
            "budget_constraint_handling": "Myopic maximization of EI at each step attempts to make the best use of the immediate budget (one evaluation at a time); optional schedules for xi have been proposed (but empirically cooling xi did not help much in some studies).",
            "breakthrough_discovery_metric": "A large expected improvement value corresponds to large potential discovery; EI directly values magnitude of potential improvement rather than just probability.",
            "performance_metrics": "Analytical expression for acquisition; empirical tuning recommendations (e.g., xi≈0.01 often works well per Lizotte) but no numeric performance figures reported in tutorial text.",
            "comparison_baseline": "Compared conceptually to PI and GP-UCB and its sensitivity contrasted with PI; EI often preferred because it accounts for magnitude of improvement.",
            "performance_vs_baseline": "Qualitative: EI tends to balance exploration and exploitation better than PI (which can be overly greedy) and often performs competitively; exact numeric comparisons are not included in the tutorial text.",
            "efficiency_gain": "Reported qualitatively as sample-efficient compared to naive sampling; no explicit numeric gains provided.",
            "tradeoff_analysis": "Paper discusses how EI balances exploration/exploitation, how including xi affects tradeoff and notes possible myopia; mentions analytical multistep EI extensions but highlights higher computational cost for non-myopic methods.",
            "optimal_allocation_findings": "EI is a strong practical acquisition choice; include small positive xi (Lizotte suggests 0.01 scaled) to encourage exploration; be aware of myopia and consider multistep EI only when computational budget allows.",
            "uuid": "e2617.1",
            "source_info": {
                "paper_title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning",
                "publication_date_yy_mm": "2010-12"
            }
        },
        {
            "name_short": "PI",
            "name_full": "Probability of Improvement (PI)",
            "brief_description": "An acquisition function that scores candidate points by the posterior probability that they will improve on the current incumbent by at least a threshold, favoring points with a high chance to exceed the incumbent.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Probability of Improvement (PI)",
            "system_description": "PI(x) = Phi((mu(x)-f(x^+)-xi)/sigma(x)), where Phi is the standard normal CDF and xi≥0 is a user parameter controlling exploration; selects the next evaluation at the x that maximizes PI. It focuses on the probability a point yields any improvement above a threshold rather than expected magnitude.",
            "application_domain": "General black-box expensive optimization and certain perceptual/preference modelling tasks where thresholded improvement is meaningful.",
            "resource_allocation_strategy": "Allocate next evaluation to maximize PI; favors points with high probability to beat the incumbent (exploitation) unless xi or schedule is used to encourage exploration.",
            "computational_cost_metric": "Number of expensive evaluations; computing PI requires GP mean/variance and evaluation of the normal CDF — acquisition optimization cheap relative to objective.",
            "information_gain_metric": "Probability of improvement (a probability-based expected utility), not mutual information; focuses on chance of improvement rather than expected magnitude.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Controlled by xi: small xi leads to greedy, local exploitation; larger xi biases toward global exploration. Kushner suggested schedules for xi to encourage exploration early and decrease over time.",
            "diversity_mechanism": "No explicit diversity promotion other than exploration encouraged by larger xi values.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Implicitly designed to operate under a limited number of expensive evaluations; user may schedule xi based on remaining budget.",
            "budget_constraint_handling": "User-controlled xi (possibly scheduled) to manage exploration when budget is ample versus exploitation when budget dwindles.",
            "breakthrough_discovery_metric": "Improvement relative to incumbent with threshold xi; does not account for magnitude beyond the threshold except through the choice of xi.",
            "performance_metrics": "Analytical form; empirical sensitivity to xi discussed qualitatively (too small =&gt; local, too large =&gt; slow fine-tuning); no numeric experiments reported here.",
            "comparison_baseline": "Compared qualitatively to EI and UCB; PI is noted to be sensitive to xi and can be overly greedy.",
            "performance_vs_baseline": "Qualitative: PI can perform well but is very sensitive to the target parameter; EI often preferred because it accounts for improvement magnitude.",
            "efficiency_gain": "Not quantified in tutorial; described qualitatively.",
            "tradeoff_analysis": "Yes — sensitivity to xi, exploration vs exploitation tradeoff discussed and recommended to tune xi or schedule it (Kushner's suggestion).",
            "optimal_allocation_findings": "PI's greediness requires careful choice of xi; if xi is scheduled appropriately PI can explore early and exploit later, but EI is generally more robust.",
            "uuid": "e2617.2",
            "source_info": {
                "paper_title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning",
                "publication_date_yy_mm": "2010-12"
            }
        },
        {
            "name_short": "GP-UCB",
            "name_full": "Gaussian Process Upper Confidence Bound (GP-UCB)",
            "brief_description": "An acquisition strategy that selects points by maximizing mu(x) + beta_t^{1/2} sigma(x), where beta_t (or related schedule) scales exploration; has theoretical no-regret guarantees when beta_t is chosen properly.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GP-UCB (Upper Confidence Bound)",
            "system_description": "At iteration t, select x maximizing mu(x) + sqrt(nu*tau_t) * sigma(x) (notation in paper), where tau_t is a time-dependent term (e.g., 2 log(t^{d/2+2} pi^2 / 3 delta)) and nu&gt;0. The sigma term explicitly drives exploration; with appropriate schedule GP-UCB achieves sublinear cumulative regret bounds (no-regret asymptotics).",
            "application_domain": "Black-box expensive function optimization; formulated under a multi-armed bandit perspective and applicable where regret guarantees are desired (e.g., experimental design with many rounds).",
            "resource_allocation_strategy": "Allocate to maximize mu(x)+exploration_bonus; exploration bonus increases when uncertainty is large or when schedule calls for more exploration, providing an explicit tradeoff schedule across iterations.",
            "computational_cost_metric": "Primary focus on number of expensive evaluations and cumulative regret over T iterations; GP computations required for mu and sigma but acquisition evaluation is cheap relative to objective evaluations.",
            "information_gain_metric": "Not explicitly mutual information — uses posterior variance as a proxy for potential information/value and incorporates it additively into the utility (upper confidence bound).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit via the additive uncertainty term scaled by a time-dependent parameter: exploration strength controlled by sqrt(nu * tau_t). With tau_t chosen per theory, GP-UCB balances exploration/exploitation and yields no-regret guarantees.",
            "diversity_mechanism": "No explicit diversity engine; exploration term tends to encourage sampling across uncertain regions, which indirectly increases diversity of sampled hypotheses.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed horizon T (number of iterations) factored into the theoretical schedule tau_t; budget is explicitly considered in regret bounds and parameter schedules.",
            "budget_constraint_handling": "Handles budget via time-dependent exploration schedule tau_t that depends on t and problem dimension; theoretical bounds ensure cumulative regret scales sublinearly with T if schedule chosen correctly.",
            "breakthrough_discovery_metric": "Not explicitly defined beyond regret and improvement relative to optimum; selection encourages discovery by exploring high-uncertainty regions that may contain large optima.",
            "performance_metrics": "Theoretical cumulative regret R_T = sum_t (f(x^*) - f(x_t)) and asymptotic no-regret property (R_T / T -&gt; 0) when tau_t chosen as in the paper. No numeric experiments are presented here in the tutorial excerpt.",
            "comparison_baseline": "Compared conceptually to EI and PI; GP-UCB tends to explore more persistently while EI may become locally exploitative.",
            "performance_vs_baseline": "Qualitative: GP-UCB can continue exploring regions that EI may abandon, offering stronger theoretical guarantees (no-regret) but may be slower to fine-tune in practice depending on parameter choices.",
            "efficiency_gain": "No empirical numbers given; theory gives asymptotic guarantees on regret implying eventual efficiency, but practical gains depend on kernel smoothness and parameter tuning.",
            "tradeoff_analysis": "Yes — the paper discusses how the schedule tau_t controls exploration intensity and how acquisition behavior differs from EI/PI; GP-UCB's theoretical framework explicitly links exploration schedule to regret bounds.",
            "optimal_allocation_findings": "Choosing the exploration schedule (tau_t) per the theoretical prescriptions yields provable no-regret behavior. In practice, parameter choices like nu and tau_t control the exploration-exploitation balance and must be set carefully.",
            "uuid": "e2617.3",
            "source_info": {
                "paper_title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning",
                "publication_date_yy_mm": "2010-12"
            }
        },
        {
            "name_short": "Acquisition Portfolio",
            "name_full": "Portfolio of Acquisition Functions with Bandit Meta-Selection",
            "brief_description": "A meta-strategy that maintains multiple acquisition functions and allocates which acquisition to use at each step via an online multi-armed bandit algorithm, aiming to adaptively select the best acquisition strategy for the problem.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Acquisition Function Portfolio (bandit-governed)",
            "system_description": "Instead of committing to a single acquisition function, the system maintains a portfolio of acquisition functions (e.g., EI, PI, UCB, etc.) and uses an online multi-armed bandit algorithm to select which acquisition to deploy at each iteration. Rewards for the bandit arms come from realized improvement or other performance signals; the approach leverages bandit learning to adaptively allocate the limited evaluation budget across acquisition strategies.",
            "application_domain": "General Bayesian optimization and experimental design settings where different acquisition heuristics perform variably across problems; intended to improve robustness across problem instances.",
            "resource_allocation_strategy": "Allocate the next expensive evaluation by first selecting an acquisition function via the meta-bandit, then maximizing that acquisition to pick a point. The bandit adjusts probabilities/weights over acquisition functions based on past rewards, thus reallocating future computational budget among heuristics.",
            "computational_cost_metric": "Number of expensive function evaluations remains primary; additional light-weight overhead for bandit bookkeeping and occasional extra acquisition optimization runs, but these are small relative to objective evaluation cost.",
            "information_gain_metric": "Implicitly uses the acquisition functions' utility signals (e.g., EI value, realized improvement) as the reward signal for the bandit; not formulated explicitly as mutual information.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Two-level: (1) within each acquisition, standard exploration-exploitation (mean vs variance); (2) at meta-level, bandit balances exploration over acquisition strategies (trying different acquisition arms) vs exploitation (using those that have recently yielded good rewards).",
            "diversity_mechanism": "Promotes diversity indirectly by allowing multiple acquisition heuristics with different exploratory behavior to be selected over time; the bandit meta-controller can encourage trying less-used acquisitions (exploration across strategies).",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Limited number of expensive evaluations; the portfolio aims to reallocate the evaluation budget adaptively across acquisition strategies to maximize performance under this limit.",
            "budget_constraint_handling": "Treats each expensive evaluation as a resource; the bandit meta-policy uses observed returns to bias selection toward acquisition strategies that deliver better improvement-per-evaluation given the remaining budget.",
            "breakthrough_discovery_metric": "Bandit reward signals are based on realized improvements (magnitude or probability), so breakthroughs are implicitly valued as high reward events that increase selection probability of the enabling acquisition.",
            "performance_metrics": "Paper reports that the portfolio 'almost always outperforms the best individual acquisition function' on a suite of test problems (qualitative claim in tutorial); no numeric metrics are included in the tutorial excerpt.",
            "comparison_baseline": "Single acquisition functions (EI, PI, GP-UCB) as baselines; portfolio is compared against each individual acquisition in experiments described in Brochu et al. [2010b] (cited).",
            "performance_vs_baseline": "Reported qualitatively to outperform the best single acquisition on standard test problems; specific numerical comparisons are not provided in this tutorial excerpt.",
            "efficiency_gain": "Qualitative improvement in robustness and empirical performance across problems; no specific percentages or runtime gains given in the tutorial excerpt.",
            "tradeoff_analysis": "Paper discusses the benefit of hedging across acquisitions to mitigate the risk of a single poor choice (no detailed cost/information tradeoff numbers in tutorial excerpt).",
            "optimal_allocation_findings": "Recommendation: when unsure which acquisition function is best for a new problem, run a portfolio governed by an online bandit to adaptively allocate evaluations among acquisition heuristics; this tends to yield more reliable performance than committing to a single acquisition.",
            "uuid": "e2617.4",
            "source_info": {
                "paper_title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning",
                "publication_date_yy_mm": "2010-12"
            }
        },
        {
            "name_short": "EGO / DACE",
            "name_full": "Efficient Global Optimization (EGO) / Design and Analysis of Computer Experiments (DACE)",
            "brief_description": "A kriging/Gaussian process-based sequential experimental design approach that pairs a response-surface model (DACE/kriging) with a sequential expected-improvement (EI) criterion (EGO) to optimize expensive computer experiments.",
            "citation_title": "Design and Analysis of Computer Experiments",
            "mention_or_use": "mention",
            "system_name": "EGO (Efficient Global Optimization) / DACE",
            "system_description": "DACE refers to the Gaussian-process (kriging) modelling framework for computer experiments; EGO is the sequential optimization algorithm that couples DACE with the Expected Improvement acquisition to select expensive simulation points sequentially. Typical steps: fit kriging surrogate, compute EI, optimize EI to pick next sample, update surrogate, repeat.",
            "application_domain": "Computer experiments, simulation-based scientific experiments, engineering design optimization where true evaluations are computationally expensive.",
            "resource_allocation_strategy": "Sequentially allocate simulation runs to the argmax of EI computed on the kriging/DACE surrogate; aims to reduce number of expensive simulation runs needed to find optimum.",
            "computational_cost_metric": "Number of expensive simulation runs (function evaluations); surrogate fitting (kriging) cost also considered but secondary.",
            "information_gain_metric": "Expected Improvement under the kriging posterior used as the acquisition (value-of-information proxy), rather than explicit mutual information.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "EI balances mean and variance of the kriging surrogate; DACE/kriging covariance choices determine smoothness assumptions which influence where uncertainty is high and thus where exploration is encouraged.",
            "diversity_mechanism": "No explicit diversity mechanism beyond surrogate-driven exploration via variance; DACE/kriging sometimes uses localized neighbor models in practice which can diversify local search behavior.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed/limited number of simulation runs (explicit experimental-design context).",
            "budget_constraint_handling": "Sequential selection via EI to maximize expected utility per expensive simulation under a limited budget; classical experimental design criteria (A-, D-, E-optimality) are discussed in connection to parameter estimation problems but EGO focuses on optimization.",
            "breakthrough_discovery_metric": "Large expected improvement values correspond to potential high-impact (breakthrough) discoveries relative to current best; no separate novelty or breakthrough scoring is defined.",
            "performance_metrics": "Historical experimental design literature reports fewer simulations to reach good solutions compared to naive designs; tutorial does not present new numeric EGO results but references Jones et al. [1998] for empirical studies.",
            "comparison_baseline": "Compared historically with grid search, random designs, classical non-adaptive experimental designs; EGO aims to be more sample-efficient.",
            "performance_vs_baseline": "Qualitative claim: EGO reduces required function evaluations compared to non-adaptive designs; numeric results are in the cited EGO literature (not reproduced here).",
            "efficiency_gain": "Not quantified in the tutorial excerpt; referenced literature demonstrates substantial reductions in simulation runs in many cases.",
            "tradeoff_analysis": "Discusses relation of kriging model assumptions and kernel choices to exploration behavior; contrasts non-adaptive classical experimental design with sequential EGO approach.",
            "optimal_allocation_findings": "Sequential, model-based allocation via EI on a kriging surrogate is recommended for expensive computer experiments; appropriate kernel selection and hyperparameter estimation are important for good allocation behavior.",
            "uuid": "e2617.5",
            "source_info": {
                "paper_title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning",
                "publication_date_yy_mm": "2010-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Design and Analysis of Computer Experiments",
            "rating": 2
        },
        {
            "paper_title": "Efficient Global Optimization",
            "rating": 2
        },
        {
            "paper_title": "Sequential Design for Optimization",
            "rating": 1
        },
        {
            "paper_title": "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design",
            "rating": 2
        },
        {
            "paper_title": "A Multi-armed Bandit Approach to Adaptive Acquisition in Bayesian Optimization",
            "rating": 1
        }
    ],
    "cost": 0.01968,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning</h1>
<p>Eric Brochu, Vlad M. Cora and Nando de Freitas</p>
<p>November 26, 2024</p>
<h4>Abstract</h4>
<p>We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments-active user modelling with preferences, and hierarchical reinforcement learningand a discussion of the pros and cons of Bayesian optimization based on our experiences.</p>
<h2>1 Introduction</h2>
<p>An enormous body of scientific literature has been devoted to the problem of optimizing a nonlinear function $f(\mathbf{x})$ over a compact set $\mathcal{A}$. In the realm of optimization, this problem is formulated concisely as follows:</p>
<p>$$
\max _{\mathbf{x} \in \mathcal{A} \subset \mathbb{R}^{d}} f(\mathbf{x})
$$</p>
<p>One typically assumes that the objective function $f(\mathbf{x})$ has a known mathematical representation, is convex, or is at least cheap to evaluate. Despite the influence of classical optimization on machine learning, many learning problems do not conform to these strong assumptions. Often, evaluating the objective function is expensive or even impossible, and the derivatives and convexity properties are unknown.</p>
<p>In many realistic sequential decision making problems, for example, one can only hope to obtain an estimate of the objective function by simulating future scenarios. Whether one adopts simple Monte Carlo simulation or adaptive</p>
<p>schemes, as proposed in the fields of planning and reinforcement learning, the process of simulation is invariably expensive. Moreover, in some applications, drawing samples $f(\mathbf{x})$ from the function corresponds to expensive processes: drug trials, destructive tests or financial investments. In active user modeling, $\mathbf{x}$ represents attributes of a user query, and $f(\mathbf{x})$ requires a response from the human. Computers must ask the right questions and the number of questions must be kept to a minimum so as to avoid annoying the user.</p>
<h1>1.1 An Introduction to Bayesian Optimization</h1>
<p>Bayesian optimization is a powerful strategy for finding the extrema of objective functions that are expensive to evaluate. It is applicable in situations where one does not have a closed-form expression for the objective function, but where one can obtain observations (possibly noisy) of this function at sampled values. It is particularly useful when these evaluations are costly, when one does not have access to derivatives, or when the problem at hand is non-convex.</p>
<p>Bayesian optimization techniques are some of the most efficient approaches in terms of the number of function evaluations required (see, e.g. [Močkus, 1994, Jones et al., 1998, Streltsov and Vakili, 1999, Jones, 2001, Sasena, 2002]). Much of the efficiency stems from the ability of Bayesian optimization to incorporate prior belief about the problem to help direct the sampling, and to trade off exploration and exploitation of the search space. It is called Bayesian because it uses the famous "Bayes' theorem", which states (simplifying somewhat) that the posterior probability of a model (or theory, or hypothesis) $M$ given evidence (or data, or observations) $E$ is proportional to the likelihood of $E$ given $M$ multiplied by the prior probability of $M$ :</p>
<p>$$
P(M \mid E) \propto P(E \mid M) P(M)
$$</p>
<p>Inside this simple equation is the key to optimizing the objective function. In Bayesian optimization, the prior represents our belief about the space of possible objective functions. Although the cost function is unknown, it is reasonable to assume that there exists prior knowledge about some of its properties, such as smoothness, and this makes some possible objective functions more plausible than others.</p>
<p>Let's define $\mathbf{x}<em i="i">{i}$ as the $i$ th sample, and $f\left(\mathbf{x}</em>}\right)$ as the observation of the objective function at $\mathbf{x<em 1:="1:" t="t">{i}$. As we accumulate observations ${ }^{1} \mathcal{D}</em>}=\left{\mathbf{x<em 1:="1:" t="t">{1: t}, f\left(\mathbf{x}</em> \mid f\right)$. Essentially, given what we think we know about the prior, how likely is the data we have seen? If our prior belief is that the objective function is very smooth and noisefree, data with high variance or oscillations should be considered less likely than data that barely deviate from the mean. Now, we can combine these to obtain our posterior distribution:}\right)\right}$, the prior distribution is combined with the likelihood function $P\left(\mathcal{D}_{1: t</p>
<p>$$
P\left(f \mid \mathcal{D}<em 1:="1:" t="t">{1: t}\right) \propto P\left(\mathcal{D}</em> \mid f\right) P(f)
$$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of using Bayesian optimization on a toy 1D design problem. The figures show a Gaussian process (GP) approximation of the objective function over four iterations of sampled values of the objective function. The figure also shows the acquisition function in the lower shaded plots. The acquisition is high where the GP predicts a high objective (exploitation) and where the prediction uncertainty is high (exploration)—areas with both attributes are sampled first. Note that the area on the far left remains unsampled, as while it has high uncertainty, it is (correctly) predicted to offer little improvement over the highest observation.</p>
<p>The posterior captures our updated beliefs about the unknown objective function. One may also interpret this step of Bayesian optimization as estimating the objective function with a surrogate function (also called a response surface), described formally in $\S 2.1$ with the posterior mean function of a Gaussian process.</p>
<p>To sample efficiently, Bayesian optimization uses an acquisition function to determine the next location $\mathbf{x}_{t+1} \in \mathcal{A}$ to sample. The decision represents an automatic trade-off between exploration (where the objective function is very uncertain) and exploitation (trying values of $\mathbf{x}$ where the objective function is expected to be high). This optimization technique has the nice property that it aims to minimize the number of objective function evaluations. Moreover, it is likely to do well even in settings where the objective function has multiple local maxima.</p>
<p>Figure 1 shows a typical run of Bayesian optimization on a 1D problem. The optimization starts with two points. At each iteration, the acquisition function is maximized to determine where next to sample from the objective function - the acquisition function takes into account the mean and variance of the predictions over the space to model the utility of sampling. The objective is then sampled at the argmax of the acquisition function, the Gaussian process is updated and the process is repeated. +One may also interpret this step of Bayesian optimization as estimating the objective function with a surrogate function (also called a response surface), described formally in $\S 2.1$ with the posterior mean function of a Gaussian process.</p>
<h1>1.2 Overview</h1>
<p>In $\S 2$, we give an overview of the Bayesian optimization approach and its history. We formally present Bayesian optimization with Gaussian process priors (§2.1) and describe covariance functions (§2.2), acquisition functions (§2.3) and the role of Gaussian noise (§2.4). In $\S 2.5$, we cover the history of Bayesian optimization, and the related fields of kriging, GP experimental design and GP active learning.</p>
<p>The second part of the tutorial builds on the basic Bayesian optimization model. In $\S 3$ and $\S 4$ we discuss extensions to Bayesian optimization for active user modelling in preference galleries, and hierarchical control problems, respectively. Finally, we end the tutorial with a brief discussion of the pros and cons of Bayesian optimization in $\S 5$.</p>
<h2>2 The Bayesian Optimization Approach</h2>
<p>Optimization is a broad and fundamental field of mathematics. In order to harness it to our ends, we need to narrow it down by defining the conditions we are concerned with.</p>
<p>Our first restriction is to simply specify that the form of the problem we are concerned with is maximization, rather than the more common form of minimization. The maximization of a real-valued function $\mathbf{x}^{*}=\operatorname{argmax}_{\mathbf{x}} f(\mathbf{x})$ can be regarded as the minimization of the transformed function</p>
<p>$$
g(\mathbf{x})=-f(\mathbf{x})
$$</p>
<p>We also assume that the objective is Lipschitz-continuous. That is, there exists some constant $C$, such that for all $\mathbf{x}<em 2="2">{1}, \mathbf{x}</em>$ :} \in \mathcal{A</p>
<p>$$
\left|f\left(\mathbf{x}<em 2="2">{1}\right)-f\left(\mathbf{x}</em>}\right)\right| \leq C\left|\mathbf{x<em 2="2">{1}-\mathbf{x}</em>\right|
$$</p>
<p>though $C$ may be (and typically is) unknown.
We can narrow the problem down further by defining it as one of global, rather than local optimization. In local maximization problems, we need only find a point $\mathbf{x}^{(*)}$ such that</p>
<p>$$
f\left(\mathbf{x}^{(<em>)}\right) \geq f(\mathbf{x}), \forall \mathbf{x} \text { s.t. }\left|\mathbf{x}^{(</em>)}-\mathbf{x}\right|&lt;\epsilon
$$</p>
<p>If $-f(\mathbf{x})$ is convex, then any local maximum is also a global maximum. However, in our optimization problems, we cannot assume that the negative objective function is convex. It might be the case, but we have no way of knowing before we begin optimizing.</p>
<p>It is common in global optimization, and true for our problem, that the objective is a black box function: we do not have an expression of the objective function that we can analyze, and we do not know its derivatives. Evaluating the function is restricted to querying at a point $\mathbf{x}$ and getting a (possibly noisy) response. Black box optimization also typically requires that all dimensions have bounds on the search space. In our case, we can safely make the simplifying assumption these bounds are all axis-aligned, so the search space is a hyperrectangle of dimension $d$.</p>
<p>A number of approaches exist for this kind of global optimization and have been well-studied in the literature (e.g., [Törn and Žilinskas, 1989, Mongeau et al., 1998, Liberti and Maculan, 2006, Zhigljavsky and Žilinskas, 2008]). Deterministic approaches include interval optimization and branch and bound methods. Stochastic approximation is a popular idea for optimizing unknown objective functions in machine learning contexts [Kushner and Yin, 1997]. It is the core idea in most reinforcement learning algorithms [Bertsekas and Tsitsiklis, 1996, Sutton and Barto, 1998], learning methods for Boltzmann machines and deep belief networks [Younes, 1989, Hinton and Salakhutdinov, 2006] and parameter estimation for nonlinear state space models [Poyiadjis et al., 2005, Martinez-Cantin et al., 2006]. However, these are generally unsuitable for our domain because they still require many samples, and in the active user-modelling domain drawing samples is expensive.</p>
<p>Even in a noise-free domain, evaluating an objective function with Lipschitz continuity $C$ on a $d$-dimensional unit hypercube, guaranteeing the best observation $f\left(\mathbf{x}^{+}\right) \geq f\left(\mathbf{x}^{\star}\right)-\epsilon$ requires $(C / 2 \epsilon)^{d}$ samples [Betrò, 1991]. This can be an incredibly expensive premium to pay for insurance against unlikely scenarios. As a result, the idea naturally arises to relax the guarantees against pathological worst-case scenarios. The goal, instead, is to use evidence and prior knowledge to maximize the posterior at each step, so that each new evaluation decreases the distance between the true global maximum and the expected maximum given the model. This is sometimes called "one-step" [Močkus, 1994] "average-case" [Streltsov and Vakili, 1999] or "practical" [Lizotte, 2008] optimization. This average-case approach has weaker demands on computation than the worst-case approach. As a result, it may provide faster solutions in many practical domains where one does not believe the worst-case scenario is plausible.</p>
<p>Bayesian optimization uses the prior and evidence to define a posterior distribution over the space of functions. The Bayesian model allows for an elegant means by which informative priors can describe attributes of the objective function, such as smoothness or the most likely locations of the maximum, even when the function itself is not known. Optimizing follows the principle of maximum expected utility, or, equivalently, minimum expected risk. The process of deciding where to sample next requires the choice of a utility function and a</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Bayesian</span><span class="w"> </span><span class="n">Optimization</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="n">Find</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">optimizing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">acquisition</span><span class="w"> </span><span class="n">function</span><span class="w"> </span><span class="n">over</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">GP</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span><span class="o">=</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">argmax</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">x</span><span class="p">}}</span><span class="w"> </span><span class="n">u</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">objective</span><span class="w"> </span><span class="n">function</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span><span class="n">y_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span><span class="o">=</span><span class="n">f</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="o">+</span>\<span class="n">varepsilon_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span>
<span class="w">        </span><span class="n">Augment</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">data</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="n">t</span><span class="p">}</span><span class="o">=</span>\<span class="n">left</span>\<span class="p">{</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">},</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">x</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="n">t</span><span class="p">},</span><span class="w"> </span><span class="n">y_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="n">right</span>\<span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">update</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">GP</span><span class="o">.</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<p>way of optimizing the expectation of this utility with respect to the posterior distribution of the objective function. This secondary optimization problem is usually easier because the utility is typically chosen so that it is easy to evaluate, though still nonconvex. To make clear which function we are discussing, we will refer to this utility as the acquisition function (also sometimes called the infill function). In $\S 2.3$, we will discuss some common acquisition functions.</p>
<p>In practice, there is also have the possibility of measurement noise, which we will assume is Gaussian. We define $\mathbf{x}<em i="i">{i}$ as the $i$ th sample and $y</em>}=f\left(\mathbf{x<em i="i">{i}\right)+\varepsilon</em>$. We will discuss noise in more detail in $\S 2.4$.}$, with $\varepsilon_{i} \stackrel{\text { iid }}{=} \mathcal{N}\left(0, \sigma_{\text {noise }}^{2}\right)$, as the noisy observation of the objective function at $\mathbf{x}_{i</p>
<p>The Bayesian optimization procedure is shown in Algorithm 1. As mentioned earlier, it has two components: the posterior distribution over the objective and the acquisition function. Let us focus on the posterior distribution first and come back to the acquisition function in $\S 2.3$. As we accumulate observations $\mathcal{D}<em 1:="1:" t="t">{1: t}=\left{\mathbf{x}</em>}, y_{1: t}\right}$, a prior distribution $P(f)$ is combined with the likelihood function $P\left(\mathcal{D<em 1:="1:" t="t">{1: t} \mid f\right)$ to produce the posterior distribution: $P\left(f \mid \mathcal{D}</em> \mid f\right) P(f)$. The posterior captures the updated beliefs about the unknown objective function. One may also interpret this step of Bayesian optimization as estimating the objective function with a surrogate function (also called a response surface). In $\S 2.1$, we will discuss how Gaussian process priors can be placed on $f$.}\right) \propto P\left(\mathcal{D}_{1: t</p>
<h1>2.1 Priors over functions</h1>
<p>Any Bayesian method depends on a prior distribution, by definition. A Bayesian optimization method will converge to the optimum if (i) the acquisition function is continuous and approximately minimizes the risk (defined as the expected deviation from the global minimum at a fixed point $\mathbf{x}$ ); and (ii) conditional variance converges to zero (or appropriate positive minimum value in the presence of noise) if and only if the distance to the nearest observation is zero [Močkus, 1982, Močkus, 1994]. Many models could be used for this prior-early work mostly used the Wiener process (§2.5). However, Gaussian process (GP) priors for Bayesian optimization date back at least to the late 1970s [O'Hagan, 1978, Žilinskas, 1980]. Močkus [1994] explicitly set the framework for the Gaussian process prior by specifying the additional "simple and natural" conditions that (iii) the objective is continuous; (iv) the prior is homogeneous; (v) the optimization is independent of the $m^{\text {th }}$ differences. This</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Simple 1D Gaussian process with three observations. The solid black line is the GP surrogate mean prediction of the objective function given the data, and the shaded area shows the mean plus and minus the variance. The superimposed Gaussians correspond to the GP mean and standard deviation $(\mu(\cdot)$ and $\sigma(\cdot))$ of prediction at the points, $\mathbf{x}_{1: 3}$.
includes a very large family of common optimization tasks, and Močkus showed that the GP prior is well-suited to the task.</p>
<p>A GP is an extension of the multivariate Gaussian distribution to an infinitedimension stochastic process for which any finite combination of dimensions will be a Gaussian distribution. Just as a Gaussian distribution is a distribution over a random variable, completely specified by its mean and covariance, a GP is a distribution over functions, completely specified by its mean function, $m$ and covariance function, $k$ :</p>
<p>$$
f(\mathbf{x}) \sim \mathcal{G P}\left(m(\mathbf{x}), k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right)
$$</p>
<p>It is often useful to intuitively think of a GP as analogous to a function, but instead of returning a scalar $f(\mathbf{x})$ for an arbitrary $\mathbf{x}$, it returns the mean and variance of a normal distribution (Figure 2) over the possible values of $f$ at $\mathbf{x}$. Stochastic processes are sometimes called "random functions", by analogy to random variables.</p>
<p>For convenience, we assume here that the prior mean is the zero func-</p>
<p>tion $m(\mathbf{x})=0$; alternative priors for the mean can be found in, for example [Martinez-Cantin et al., 2009, Brochu et al., 2010a]. This leaves us the more interesting question of defining the covariance function $k$. A very popular choice is the squared exponential function:</p>
<p>$$
k\left(\mathbf{x}<em j="j">{i}, \mathbf{x}</em>}\right)=\exp \left(-\frac{1}{2}\left|\mathbf{x<em j="j">{i}-\mathbf{x}</em>\right)
$$}\right|^{2</p>
<p>Note that this function approaches 1 as values get close together and 0 as they get further apart. Two points that are close together can be expected to have a very large influence on each other, whereas distant points have almost none. This is a necessary condition for convergence under the assumptions of [Močkus, 1994]. We will discuss more sophisticated kernels in $\S 2.2$.</p>
<p>If we were to sample from the prior, we would choose $\left{\mathbf{x}<em 1:="1:" t="t">{1: t}\right}$ and sample the values of the function at these indices to produce the pairs $\left{\mathbf{x}</em>}, \mathbf{f<em 1:="1:" t="t">{1: t}\right}$, where $\mathbf{f}</em>)$, where the kernel matrix is given by:}=f\left(\mathbf{x}_{1: t}\right)$. The function values are drawn according to a multivariate normal distribution $\mathcal{N}(0, \mathbf{K</p>
<p>$$
\mathbf{K}=\left[\begin{array}{ccc}
k\left(\mathbf{x}<em 1="1">{1}, \mathbf{x}</em>}\right) &amp; \ldots &amp; k\left(\mathbf{x<em t="t">{1}, \mathbf{x}</em>\right) \
\vdots &amp; \ddots &amp; \vdots \
k\left(\mathbf{x}<em 1="1">{t}, \mathbf{x}</em>}\right) &amp; \ldots &amp; k\left(\mathbf{x<em t="t">{t}, \mathbf{x}</em>\right)
\end{array}\right]
$$</p>
<p>Of course, the diagonal values of this matrix are 1 (each point is perfectly correlated with itself), which is only possible in a noise-free environment. We will discuss noise in $\S 2.4$. Also, recall that we have for simplicity chosen the zero mean function.</p>
<p>In our optimization tasks, however, we will use data from an external model to fit the GP and get the posterior. Assume that we already have the observations $\left{\mathbf{x}<em 1:="1:" t="t">{1: t}, \mathbf{f}</em>}\right}$, say from previous iterations, and that we want to use Bayesian optimization to decide what point $\mathbf{x<em t_1="t+1">{t+1}$ should be considered next. Let us denote the value of the function at this arbitrary point as $f</em>}=f\left(\mathbf{x<em 1:="1:" t="t">{t+1}\right)$. Then, by the properties of Gaussian processes, $\mathbf{f}</em>$ are jointly Gaussian:}$ and $f_{t+1</p>
<p>$$
\left[\begin{array}{l}
\mathbf{f}<em t_1="t+1">{1: t} \
f</em>
\end{array}\right] \sim \mathcal{N}\left(\mathbf{0},\left[\begin{array}{cc}
\mathbf{K} &amp; \mathbf{k} \
\mathbf{k}^{T} &amp; k\left(\mathbf{x}<em t_1="t+1">{t+1}, \mathbf{x}</em>\right)
\end{array}\right]\right)
$$</p>
<p>where</p>
<p>$$
\mathbf{k}=\left[\begin{array}{lll}
k\left(\mathbf{x}<em 1="1">{t+1}, \mathbf{x}</em>}\right) &amp; k\left(\mathbf{x<em 2="2">{t+1}, \mathbf{x}</em>}\right) &amp; \cdots &amp; k\left(\mathbf{x<em t="t">{t+1}, \mathbf{x}</em>\right)
\end{array}\right]
$$</p>
<p>Using the Sherman-Morrison-Woodbury formula (see, e.g., [Rasmussen and Williams, 2006, Press et al., 2007]), one can easily arrive at an expression for the predictive distribution:</p>
<p>$$
P\left(f_{t+1} \mid \mathcal{D}<em t_1="t+1">{1: t}, \mathbf{x}</em>}\right)=\mathcal{N}\left(\mu_{t}\left(\mathbf{x<em t="t">{t+1}\right), \sigma</em>\right)\right)
$$}^{2}\left(\mathbf{x}_{t+1</p>
<p>where</p>
<p>$$
\begin{aligned}
\mu_{t}\left(\mathbf{x}<em 1:="1:" t="t">{t+1}\right) &amp; =\mathbf{k}^{T} \mathbf{K}^{-1} \mathbf{f}</em> \
\sigma_{t}^{2}\left(\mathbf{x}<em t_1="t+1">{t+1}\right) &amp; =k\left(\mathbf{x}</em>
\end{aligned}
$$}, \mathbf{x}_{t+1}\right)-\mathbf{k}^{T} \mathbf{K}^{-1} \mathbf{k</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The effect of changing the kernel hyperparameters. Shown are squared exponential kernels with $\theta=0.1,0.2,0.5$. On the left is the function $k(0, \mathbf{x})$. On the right are some one-dimensional functions sampled from a GP with the hyperparameter value.</p>
<p>That is, $\mu_{t}(\cdot)$ and $\sigma_{t}^{2}(\cdot)$ are the sufficient statistics of the predictive posterior distribution $P\left(f_{i+1} \mid \mathcal{D}<em t_1="t+1">{1: t}, \mathbf{x}</em>\right)$. For legibility, we will omit the subscripts on $\mu$ and $\sigma$ except where it might be unclear. In the sequential decision making setting, the number of query points is relatively small and, consequently, the GP predictions are easy to compute.</p>
<h1>2.2 Choice of covariance functions</h1>
<p>The choice of covariance function for the Gaussian Process is crucial, as it determines the smoothness properties of samples drawn from it. The squared exponential kernel in Eqn (1) is actually a little naive, in that divergences of all features of $\mathbf{x}$ affect the covariance equally.</p>
<p>Typically, it is necessary to generalize by adding hyperparameters. In an isotropic model, this can be done with a single hyperparameter $\theta$, which controls the width of the kernel:</p>
<p>$$
k\left(\mathbf{x}<em j="j">{i}, \mathbf{x}</em>}\right)=\exp \left(-\frac{1}{2 \theta^{2}}\left|\mathbf{x<em j="j">{i}-\mathbf{x}</em>\right)
$$}\right|^{2</p>
<p>For anisotropic models, a very popular choice is the squared exponential kernel with a vector of automatic relevance determination (ARD) hyperparameters $\boldsymbol{\theta}$</p>
<p>[Rasmussen and Williams, 2006, page 106]:</p>
<p>$$
k\left(\mathbf{x}<em j="j">{i}, \mathbf{x}</em>}\right)=\exp \left(-\frac{1}{2}\left(\mathbf{x<em j="j">{i}-\mathbf{x}</em>\right)\right)
$$}\right)^{T} \operatorname{diag}(\boldsymbol{\theta})^{-2}\left(\mathbf{x}-\mathbf{x}^{\prime</p>
<p>where $\operatorname{diag}(\boldsymbol{\theta})$ is a diagonal matrix with $d$ entries $\boldsymbol{\theta}$ along the diagonal. Intuitively, if a particular $\theta_{\ell}$ has a small value, the kernel becomes independent of $\ell$-th input, effectively removing it automatically. Hence, irrelevant dimensions are discarded. Figure 3 shows examples of different hyperparameter values on the squared exponential function and what functions sampled from those values look like. Typically, the hyperparameter values are learned by "seeding" with a few random samples and maximizing the log-likelihood of the evidence given $\boldsymbol{\theta}$ [Jones et al., 1998, Sasena, 2002, Santner et al., 2003, Rasmussen and Williams, 2006]. This can often be aided with an informative hyperprior on the hyperparameters, often a log normal prior [Lizotte, 2008, Frean and Boyle, 2008]. Methods of learning these values more efficiently is currently an active subfield of research (e.g. [Osborne, 2010, Brochu et al., 2010a]).</p>
<p>Another important kernel for Bayesian optimization is the Matérn kernel [Matérn, 1960, Stein, 1999], which incorporates a smoothness parameter $\varsigma$ to permit greater flexibility in modelling functions:</p>
<p>$$
k\left(\mathbf{x}<em j="j">{i}, \mathbf{x}</em>}\right)=\frac{1}{2^{\varsigma-1} \Gamma(\varsigma)}\left(2 \sqrt{\varsigma}\left|\mathbf{x<em j="j">{i}-\mathbf{x}</em>}\right|\right)^{\varsigma} H_{\varsigma}\left(2 \sqrt{\varsigma}\left|\mathbf{x<em j="j">{i}-\mathbf{x}</em>\right|\right)
$$</p>
<p>where $\Gamma(\cdot)$ and $H_{\varsigma}(\cdot)$ are the Gamma function and the Bessel function of order $\varsigma$. Note that as $\varsigma \rightarrow \infty$, the Matérn kernel reduces to the squared exponential kernel, and when $\varsigma=0.5$, it reduces to the unsquared exponential kernel. As with the squared exponential, length-scale hyperparameter are often incorporated.</p>
<p>While the squared exponential and Matérn are the most common kernels for GPs, numerous others have been examined in the machine learning literature (see, e.g., [Genton, 2001] or [Rasmussen and Williams, 2006, Chapter 4] for an overview). Appropriate covariance functions can also be used to extend the model in other interesting ways. For example, the recent sequential sensor work of Osborne, Garnett and colleagues uses GP models with extensions to the covariance function to model the characteristics of changepoints [Osborne et al., 2010] and the locations of sensors in a network [Garnett et al., 2010a]. A common additional hyperparameter is simply a scalar applied to $k$ to control the magnitude of the variance.</p>
<p>Determining which of a set of possible kernel functions to use for a problem typically requires a combination of engineering and automatic model selection, either hierarchical Bayesian model selection [Mackay, 1992] or cross-validation. However, these methods require fitting a model given a representative sample of data. In [Brochu et al., 2010a], we discuss how model selection can be performed using models believed to be similar. The techniques introduced in [Brochu et al., 2010b] could also be applied to model selection, though that is outside the scope of this tutorial.</p>
<h1>2.3 Acquisition Functions for Bayesian Optimization</h1>
<p>Now that we have discussed placing priors over smooth functions and how to update these priors in light of new observations, we will focus our attention on the acquisition component of Bayesian optimization. The role of the acquisition function is to guide the search for the optimum. Typically, acquisition functions are defined such that high acquisition corresponds to potentially high values of the objective function, whether because the prediction is high, the uncertainty is great, or both. Maximizing the acquisition function is used to select the next point at which to evaluate the function. That is, we wish to sample $f$ at $\operatorname{argmax}_{\mathbf{x}} u(\mathbf{x} \mid \mathcal{D})$, where $u(\cdot)$ is the generic symbol for an acquisition function.</p>
<h3>2.3.1 Improvement-based acquisition functions</h3>
<p>The early work of Kushner [1964] suggested maximizing the probability of improvement over the incumbent $f\left(\mathbf{x}^{+}\right)$, where $\mathbf{x}^{+}=\operatorname{argmax}<em i="i">{\mathbf{x}</em>} \in \mathbf{x<em i="i">{1: t}} f\left(\mathbf{x}</em>\right)$, so that</p>
<p>$$
\begin{aligned}
\mathrm{PI}(\mathbf{x}) &amp; =P\left(f(\mathbf{x}) \geq f\left(\mathbf{x}^{+}\right)\right) \
&amp; =\Phi\left(\frac{\mu(\mathbf{x})-f\left(\mathbf{x}^{+}\right)}{\sigma(\mathbf{x})}\right)
\end{aligned}
$$</p>
<p>where $\Phi(\cdot)$ is the normal cumulative distribution function. This function is also sometimes called MPI (for "maximum probability of improvement") or "the $P$-algorithm" (since the utility is the probability of improvement).</p>
<p>The drawback, intuitively, is that this formulation is pure exploitation. Points that have a high probability of being infinitesimally greater than $f\left(\mathbf{x}^{+}\right)$ will be drawn over points that offer larger gains but less certainty. As a result, a modification is to add a trade-off parameter $\xi \geq 0$ :</p>
<p>$$
\begin{aligned}
\mathrm{PI}(\mathbf{x}) &amp; =P\left(f(\mathbf{x}) \geq f\left(\mathbf{x}^{+}\right)+\xi\right) \
&amp; =\Phi\left(\frac{\mu(\mathbf{x})-f\left(\mathbf{x}^{+}\right)-\xi}{\sigma(\mathbf{x})}\right)
\end{aligned}
$$</p>
<p>The exact choice of $\xi$ is left to the user, though Kushner recommended a schedule for $\xi$, so that it started fairly high early in the optimization, to drive exploration, and decreased toward zero as the algorithm continued. Several researchers have studied the empirical impact of different values of $\xi$ in different domains [Törn and Žilinskas, 1989, Jones, 2001, Lizotte, 2008].</p>
<p>An appealing characteristic of this formulation for perceptual and preference models is that while maximizing $\mathrm{PI}(\cdot)$ is still greedy, it selects the point most likely to offer an improvement of at least $\xi$. This can be useful in psychoperceptual tasks, where there is a threshold of perceptual difference.</p>
<p>Jones [2001] notes that the performance of $\mathrm{PI}(\cdot)$
"is truly impressive. It would be quite natural if the reader, like so many others, became enthusiastic about this approach. But if there is a single lesson to be taken away from this paper, it is that</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Gaussian process from Figure 2, additionally showing the region of probable improvement. The maximum observation is at $\mathbf{x}^{+}$. The darkly-shaded area in the superimposed Gaussian above the dashed line can be used as a measure of improvement, $I(\mathbf{x})$. The model predicts almost no possibility of improvement by observing at $\mathbf{x}<em 2="2">{1}$ or $\mathbf{x}</em>\right)$.
nothing in this response-surface area is so simple. There always seems to be a counterexample. In this case, the difficulty is that [the $\mathrm{PI}(\cdot)$ method] is extremely sensitive to the choice of the target. If the desired improvement is too small, the search will be highly local and will only move on to search globally after searching nearly exhaustively around the current best point. On the other hand, if $[\xi]$ is set too high, the search will be excessively global, and the algorithm will be slow to fine-tune any promising solutions."}$, while sampling at $\mathbf{x}_{3}$ is more likely to improve on $f\left(\mathbf{x}^{+</p>
<p>A somewhat more satisfying alternative acquisition function would be one that takes into account not only the probability of improvement, but the magnitude of the improvement a point can potentially yield. In particular, we want to minimize the expected deviation from the true maximum $f\left(\mathbf{x}^{*}\right)$, when choosing</p>
<p>a new trial point:</p>
<p>$$
\begin{aligned}
\mathbf{x}<em t_1="t+1">{t+1} &amp; =\underset{\mathbf{x}}{\operatorname{argmin}} \mathbb{E}\left(\left|f</em>}(\mathbf{x})-f\left(\mathbf{x}^{\star}\right)\right| \mid \mathcal{D<em t_1="t+1">{1: t}\right) \
&amp; =\underset{\mathbf{x}}{\operatorname{argmin}} \int\left|f</em>}(\mathbf{x})-f\left(\mathbf{x}^{\star}\right)\right| P\left(f_{t+1} \mid \mathcal{D<em t_1="t+1">{1: t}\right) d f</em>
\end{aligned}
$$</p>
<p>Note that this decision process is myopic in that it only considers one-stepahead choices. However, if we want to plan two steps ahead, we can easily apply recursion:</p>
<p>$$
\mathbf{x}<em _mathbf_x="\mathbf{x">{t+1}=\underset{\mathbf{x}}{\operatorname{argmin}} \mathbb{E}\left(\min </em>}^{\prime}} \mathbb{E}\left(\left|f_{t+2}\left(\mathbf{x}^{\prime}\right)-f\left(\mathbf{x}^{\star}\right)\right| \mid \mathcal{D<em 1:="1:" t="t">{t+1}\right) \mid \mathcal{D}</em>\right)
$$</p>
<p>One could continue applying this procedure of dynamic programming for as many steps ahead as desired. However, because of its expense, Močkus et al. [1978] proposed the alternative of maximizing the expected improvement with respect to $f\left(\mathbf{x}^{+}\right)$. Specifically, Močkus defined the improvement function as:</p>
<p>$$
\mathrm{I}(\mathbf{x})=\max \left{0, f_{t+1}(\mathbf{x})-f\left(\mathbf{x}^{+}\right)\right}
$$</p>
<p>That is, $\mathrm{I}(\mathbf{x})$ is positive when the prediction is higher than the best value known thus far. Otherwise, $\mathrm{I}(\mathbf{x})$ is set to zero. The new query point is found by maximizing the expected improvement:</p>
<p>$$
\mathbf{x}=\underset{\mathbf{x}}{\operatorname{argmax}} \mathbb{E}\left(\max \left{0, f_{t+1}(\mathbf{x})-f\left(\mathbf{x}^{+}\right)\right} \mid \mathcal{D}_{t}\right)
$$</p>
<p>The likelihood of improvement I on a normal posterior distribution characterized by $\mu(\mathbf{x}), \sigma^{2}(\mathbf{x})$ can be computed from the normal density function,</p>
<p>$$
\frac{1}{\sqrt{2 \pi} \sigma(\mathbf{x})} \exp \left(-\frac{\left(\mu(\mathbf{x})-f\left(\mathbf{x}^{+}\right)-\mathrm{I}\right)^{2}}{2 \sigma^{2}(\mathbf{x})}\right)
$$</p>
<p>The expected improvement is the integral over this function:</p>
<p>$$
\begin{aligned}
\mathbb{E}(\mathrm{I}) &amp; =\int_{\mathrm{I}=0}^{\mathrm{I}=\infty} \mathrm{I} \frac{1}{\sqrt{2 \pi} \sigma(\mathbf{x})} \exp \left(-\frac{\left(\mu(\mathbf{x})-f\left(\mathbf{x}^{+}\right)-\mathrm{I}\right)^{2}}{2 \sigma^{2}(\mathbf{x})}\right) d \mathrm{I} \
&amp; =\sigma(\mathbf{x})\left[\frac{\mu(\mathbf{x})-f\left(\mathbf{x}^{+}\right)}{\sigma(\mathbf{x})} \Phi\left(\frac{\mu(\mathbf{x})-f\left(\mathbf{x}^{+}\right)}{\sigma(\mathbf{x})}\right)+\phi\left(\frac{\mu(\mathbf{x})-f\left(\mathbf{x}^{+}\right)}{\sigma(\mathbf{x})}\right)\right]
\end{aligned}
$$</p>
<p>The expected improvement can be evaluated analytically [Močkus et al., 1978, Jones et al., 1998], yielding:</p>
<p>$$
\begin{aligned}
\mathrm{EI}(\mathbf{x}) &amp; =\left{\begin{array}{ll}
\left(\mu(\mathbf{x})-f\left(\mathbf{x}^{+}\right)\right) \Phi(Z)+\sigma(\mathbf{x}) \phi(Z) &amp; \text { if } \sigma(\mathbf{x})&gt;0 \
0 &amp; \text { if } \sigma(\mathbf{x})=0
\end{array}\right. \
Z &amp; =\frac{\mu(\mathbf{x})-f\left(\mathbf{x}^{+}\right)}{\sigma(\mathbf{x})}
\end{aligned}
$$</p>
<p>where $\phi(\cdot)$ and $\Phi(\cdot)$ denote the PDF and CDF of the standard normal distribution respectively. Figure 4 illustrates a typical expected improvement scenario.</p>
<p>It should be said that being myopic is not a requirement here. For example, it is possible to derive analytical expressions for the two-step ahead expected improvement [Ginsbourger et al., 2008] and multistep Bayesian optimization [Garnett et al., 2010b]. This is indeed a very promising recent direction.</p>
<h1>2.3.2 Exploration-exploitation trade-off</h1>
<p>The expectation of the improvement function with respect to the predictive distribution of the Gaussian process enables us to balance the trade-off of exploiting and exploring. When exploring, we should choose points where the surrogate variance is large. When exploiting, we should choose points where the surrogate mean is high.</p>
<p>It is highly desirable for our purposes to express $\operatorname{EI}(\cdot)$ in a generalized form which controls the trade-off between global search and local optimization (exploration/exploitation). Lizotte [2008] suggests a $\xi \geq 0$ parameter such that:</p>
<p>$$
\operatorname{EI}(\mathbf{x})= \begin{cases}\left(\mu(\mathbf{x})-f\left(\mathbf{x}^{+}\right)-\xi\right) \Phi(Z)+\sigma(\mathbf{x}) \phi(Z) &amp; \text { if } \sigma(\mathbf{x})&gt;0 \ 0 &amp; \text { if } \sigma(\mathbf{x})=0\end{cases}
$$</p>
<p>where</p>
<p>$$
Z= \begin{cases}\frac{\mu(\mathbf{x})-f\left(\mathbf{x}^{+}\right)-\xi}{\sigma(\mathbf{x})} &amp; \text { if } \sigma(\mathbf{x})&gt;0 \ 0 &amp; \text { if } \sigma(\mathbf{x})=0\end{cases}
$$</p>
<p>This $\xi$ is very similar in flavour to the $\xi$ used in Eqn (2), and to the approach used by Jones et al. [2001]. Lizotte's experiments suggest that setting $\xi=0.01$ (scaled by the signal variance if necessary) works well in almost all cases, and interestingly, setting a cooling schedule for $\xi$ to encourage exploration early and exploitation later does not work well empirically, contrary to intuition (though Lizotte did find that a cooling schedule for $\xi$ might slightly improve performance on short runs $(t&lt;30)$ of PI optimization).</p>
<h3>2.3.3 Confidence bound criteria</h3>
<p>Cox and John [1992, 1997] introduce an algorithm they call "Sequential Design for Optimization", or SDO. Given a random function model, SDO selects points for evaluation based on the lower confidence bound of the prediction site:</p>
<p>$$
\operatorname{LCB}(\mathbf{x})=\mu(\mathbf{x})-\kappa \sigma(x)
$$</p>
<p>where $\kappa \geq 0$. While they are concerned with minimization, we can maximize by instead defining the upper confidence bound:</p>
<p>$$
\operatorname{UCB}(\mathbf{x})=\mu(\mathbf{x})+\kappa \sigma(x)
$$</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Examples of acquisition functions and their settings. The GP posterior is shown at top. The other images show the acquisition functions for that GP. From the top: probability of improvement (Eqn (2)), expected improvement (Eqn (4)) and upper confidence bound (Eqn (5)). The maximum of each function is shown with a triangle marker.</p>
<p>Like other parameterized acquisition models we have seen, the parameter $\kappa$ is left to the user. However, an alternative acquisition function has been proposed by Srinivas et al. [2010]. Casting the Bayesian optimization problem as a multi-armed bandit, the acquisition is the instantaneous regret function</p>
<p>$$
r(\mathbf{x})=f\left(\mathbf{x}^{*}\right)-f(\mathbf{x})
$$</p>
<p>The goal of optimizing in the framework is to find:</p>
<p>$$
\min \sum_{t}^{T} r\left(\mathbf{x}<em t="t">{t}\right)=\max \sum</em>\right)
$$}^{T} f\left(\mathbf{x}_{t</p>
<p>where $T$ is the number of iterations the optimization is to be run for.
Using the upper confidence bound selection criterion with $\kappa_{t}=\sqrt{\nu \tau_{t}}$ and the hyperparameter $\nu&gt;0$ Srinivas et al. define</p>
<p>$$
\mathrm{GP}-\mathrm{UCB}(\mathbf{x})=\mu(\mathbf{x})+\sqrt{\nu \tau_{t}} \sigma(\mathbf{x})
$$</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Examples of acquisition functions and their settings in 2 dimensions. The top row shows the objective function (which is the Branin function here), and the posterior mean and variance estimates $\mu(\cdot)$ and $\sigma^{2}(\cdot)$. The samples used to train the GP are shows with white dots. The second row shows the acquisition functions for the GP. From left to right: probability of improvement (Eqn (2)), expected improvement (Eqn (4)) and upper confidence bound (Eqn (5)). The maximum of each function is shown with a triangle marker.</p>
<p>With $\nu=1$ and $\tau_{t}=2 \log \left(t^{d / 2+2} \pi^{2} / 3 \delta\right)$, it can be shown ${ }^{2}$ with high probability that this method is no regret, i.e. $\lim <em T="T">{T \rightarrow \infty} R</em>$ is the cumulative regret} / T=0$, where $R_{T</p>
<p>$$
R_{T}=\sum_{t=1}^{T} f\left(\mathbf{x}^{\star}\right)-f\left(\mathbf{x}_{t}\right)
$$</p>
<p>This in turn implies a lower-bound on the convergence rate for the optimization problem.</p>
<p>Figures 5 and 6 show how with the same GP posterior, different acquisition functions with different maxima are defined. Figure 7 gives an example of how</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Comparison of probability of improvement (top), expected improvement (middle) and upper confidence bound (bottom) acquisition functions on a toy 1D problem. In the upper rows, the objective function is shown with the dotted red line, the solid blue line is the GP posterior mean. In the lower rows, the respective infill functions are shown, with a star denoting the maximum. The optimizations are initialized with the same two points, but quickly follow different sampling trajectories. In particular, note that the greedy EI algorithm ignores the region around $x=0.4$ once it is determined there is minimal chance of improvement, while GP-UCB continues to explore.</p>
<p>PI, EI and GP-UCB give rise to distinct sampling behaviour over time.
With several different parameterized acquisition functions in the literature, it is often unclear which one to use. Brochu et al. [2010b] present one method of utility selection. Instead of using a single acquisition function, they adopt a portfolio of acquisition functions governed by an online multi-armed bandit strategy, which almost always outperforms the best individual acquisition function of a suite of standard test problems.</p>
<h1>2.3.4 Maximizing the acquisition function</h1>
<p>To find the point at which to sample, we still need to maximize the constrained objective $u(\mathbf{x})$. Unlike the original unknown objective function, $u(\cdot)$ can be cheaply sampled. We optimize the acquisition function using DIRECT [Jones et al., 1993], a deterministic, derivative-free optimizer. It uses the existing samples of the objective function to decide how to proceed to DIvide the feasible space into finer RECTangles. A particular advantage in active learning applications is that DIRECT can be implemented as an "any-time" algorithm, so that as long as the user is doing something else, it continues to optimize, and when interrupted, the program can use the best results found to that point in time. Methods such as Monte Carlo and multistart have also been used, and seem to perform reasonably well [Močkus, 1994, Lizotte, 2008].</p>
<h3>2.4 Noise</h3>
<p>The model we've used so far assumes that we have perfectly noise-free observations. In real life, this is rarely possible, and instead of observing $f(\mathbf{x})$, we can often only observe a noisy transformation of $f(\mathbf{x})$.</p>
<p>The simplest transformation arises when $f(\mathbf{x})$ is corrupted with Gaussian noise $\epsilon \sim \mathcal{N}\left(0, \sigma_{\text {noise }}^{2}\right)$ [Rasmussen and Williams, 2006]. If the noise is additive, we can easily add the noise distribution to the Gaussian distribution $\mathcal{N}(0, \mathbf{K})$ and define</p>
<p>$$
y_{i}=f\left(\mathbf{x}<em i="i">{i}\right)+\epsilon</em>
$$</p>
<p>Since the mean is zero, this type of noise simply requires that we replace the kernel $\mathbf{K}$ with the following kernel for the noisy observations of $f(\cdot)$ :</p>
<p>$$
\mathbf{K}=\left[\begin{array}{ccc}
k\left(\mathbf{x}<em 1="1">{1}, \mathbf{x}</em>}\right) &amp; \ldots &amp; k\left(\mathbf{x<em t="t">{1}, \mathbf{x}</em>\right) \
\vdots &amp; \ddots &amp; \vdots \
k\left(\mathbf{x}<em 1="1">{t}, \mathbf{x}</em>}\right) &amp; \ldots &amp; k\left(\mathbf{x<em t="t">{t}, \mathbf{x}</em>\right)
\end{array}\right]+\sigma_{\text {noise }}^{2} I
$$</p>
<p>This yields the predictive distribution:</p>
<p>$$
P\left(y_{t+1} \mid \mathcal{D}<em t_1="t+1">{1: t}, \mathbf{x}</em>}\right)=\mathcal{N}\left(\mu_{t}\left(\mathbf{x<em t="t">{t+1}\right), \sigma</em>}^{2}\left(\mathbf{x<em _noise="{noise" _text="\text">{t+1}\right)+\sigma</em>\right)
$$}}^{2</p>
<p>and the sufficient statistics</p>
<p>$$
\begin{aligned}
\mu_{t}\left(\mathbf{x}<em _noise="{noise" _text="\text">{t+1}\right) &amp; =\mathbf{k}^{T}\left[\mathbf{K}+\sigma</em>}}^{2} I\right]^{-1} \mathbf{y<em t="t">{1: t} \
\sigma</em>}^{2}\left(\mathbf{x<em t_1="t+1">{t+1}\right) &amp; =k\left(\mathbf{x}</em>}, \mathbf{x<em _noise="{noise" _text="\text">{t+1}\right)-\mathbf{k}^{T}\left[\mathbf{K}+\sigma</em>
\end{aligned}
$$}}^{2} I\right]^{-1} \mathbf{k</p>
<p>In a noisy environment, we also change the definition of the incumbent in the PI and EI acquisition functions. Instead of using the best observation, we use the distribution at the sample points, and define as the incumbent, the point with the highest expected value,</p>
<p>$$
\mu^{+}=\underset{\mathbf{x}<em 1:="1:" t="t">{i} \in \mathbf{x}</em>\right)
$$}}{\operatorname{argmax}} \mu\left(\mathbf{x}_{i</p>
<p>This avoids the problem of attempting to maximize probability or expected improvement over an unreliable sample. It is also possible to resample potential incumbents to get more reliable estimates of the values in a noisy environment [Bartz-Beielstein et al., 2005, Huang et al., 2006, Hutter et al., 2009], a process sometimes called intensification. Nonstationary noise models are also possible, such as autoregressive moving-average noise [Murray-Smith and Girard, 2001] and heteroskedastic Gaussian noise [Goldberg et al., 1998].</p>
<h1>2.5 A brief history of Bayesian optimization</h1>
<p>The earliest work we are aware of resembling the modern Bayesian optimization approach is the early work of Kushner [1964], who used Wiener processes for unconstrained one-dimensional problems. Kushner's decision model was based on maximizing the probability of improvement ( $\S 2.3 .1$ ). He also included a parameter that controlled the trade-off between 'more global' and 'more local' optimization, in the same spirit as the exploration-exploitation trade-off. A key difference is that in a (one-dimensional) Wiener process, the intervals between samples are independent, and Kushner was concerned with the problem of selecting from a finite set of intervals. Later work extended Kushner's technique to multidimensional optimization, using, for example, interpolation in a Delauney triangulation of the space [Elder, 1992] or projecting Wiener processes between sample points [Stuckman, 1988].</p>
<p>Meanwhile, in the former Soviet Union, Močkus and colleagues developed a multidimensional Bayesian optimization method using linear combinations of Wiener fields. This was first published in English as [Močkus et al., 1978]. This paper also, significantly, describes an acquisition function that is based on myopic expected improvement of the posterior, which has been widely adopted in Bayesian optimization as the expected improvement function (§2.3.1). A more recent review of Močkus' approach is [Močkus, 1994].</p>
<p>At the same time, a large, related body of work emerged under the name kriging (§2.6), in honour of the South African student who developed this technique at the University of the Witwatersrand [Krige, 1951], though largely popularized by Matheron and colleagues (e.g. [Matheron, 1971]). In kriging, the goal is interpolation of a random field via a linear predictor. The errors on this model are typically assumed to not be independent, and are modelled with a Gaussian process.</p>
<p>More recently, Bayesian optimization using Gaussian processes has been successfully applied to derivative-free optimization and experimental design, where it is called Efficient Global Optimization, or EGO (§2.7).</p>
<p>There exist several consistency proofs for this algorithm in the one-dimensional setting [Locatelli, 1997] and one for a simplification of the algorithm using simplicial partitioning in higher dimensions [Žilinskas and Žilinskas, 2002]. The convergence of the algorithm using multivariate Gaussian processes has been recently established in [Vasquez and Bect, 2008].</p>
<h1>2.6 Kriging</h1>
<p>Kriging has been used in geostatistics and environmental science since the 1950s and remains important today. We will briefly summarize the connection to Bayesian optimization here. More detailed examinations can be found in, for example, [Stein, 1999, Sasena, 2002, Diggle and Ribeiro, 2007]. This section is primarily drawn from these sources.</p>
<p>In many modelling techniques in statistics and machine learning, it is assumed that samples drawn from a process with independent, identically distributed residuals, typically, $\varepsilon \sim \mathcal{N}\left(0, \sigma_{\text {noise }}^{2}\right)$ :</p>
<p>$$
y(\mathbf{x})=f(\mathbf{x})+\varepsilon
$$</p>
<p>In kriging, however, the usual assumption is that errors are not independent, and are, in fact, spatially correlated: where errors are high, it is expected that nearby errors will also be high. Kriging is a combination of a linear regression model and a stochastic model fitted to the residual errors of the linear model. The residual is modelled with a zero-mean Gaussian process, so $\varepsilon$ is actually parameterized by $\mathbf{x}: \varepsilon(\mathbf{x}) \sim \mathcal{N}\left(0, \sigma^{2}(\mathbf{x})\right)$.</p>
<p>The actual regression model depends on the type of kriging. In simple kriging, $f$ is modelled with the zero function, making it a zero-mean GP model. In ordinary kriging, $f$ is modelled with a constant but unknown function. Universal kriging models $f$ with a polynomial of degree $k$ with bases $m$ and coefficients $\beta$, so that</p>
<p>$$
y(\mathbf{x})=\sum_{j=1}^{k} \beta_{j} m_{j}(\mathbf{x})+\varepsilon(\mathbf{x})
$$</p>
<p>Other, more exotic types of kriging are also used.
Clearly, kriging and Bayesian optimization are very closely related. There are some key differences in practice, though. In Bayesian optimization, models are usually fit through maximum likelihood. In kriging, models are usually fit using a variogram, a measure of the average dissimilarity between samples versus their separation distance. Fitting is done using least squares or similar numerical methods, or interactively, by an expert visually inspecting the variogram plot with specially-designed software. Kriging also often restricts the prediction model to use only a small number of neighbours, making it fit locally while ignoring global information. Bayesian optimization normally uses all the data in order to learn a global model.</p>
<h1>2.7 Experimental design</h1>
<p>Kriging has been applied to experimental design under the name $D A C E$, after "Design and Analysis of Computer Experiments", the title of a paper by Sacks et al. [1989] (and more recently a book by Santner et al. [2003]). In DACE, the regression model is a best linear unbiased predictor (BLUP), and the residual model is a noise-free Gaussian process. The goal is to find a design point or points that optimizes some criterion.</p>
<p>The "efficient global optimization", or $E G O$, algorithm is the combination of DACE model with the sequential expected improvement ( $\S 2.3 .1$ ) acquisition criterion. It was published in a paper by Jones et al. [1998] as a refinement of the SPACE algorithm (Stochastic Process Analysis of Computer Experiments) [Schonlau, 1997]. Since EGO's publication, there has evolved a body of work devoted to extending the algorithm, particularly in adding constraints to the optimization problem [Audet et al., 2000, Sasena, 2002, Boyle, 2007], and in modelling noisy functions [Bartz-Beielstein et al., 2005, Huang et al., 2006, Hutter et al., 2009, Hutter, 2009].</p>
<p>In so-called "classical" experimental design, the problem to be addressed is often to learn the parameters $\zeta$ of a function $g_{\zeta}$ such that</p>
<p>$$
y_{i}=g_{\zeta}\left(\mathbf{x}<em i="i">{i}\right)+\varepsilon</em>, \forall i \in 1, \ldots, t
$$</p>
<p>with noise $\varepsilon_{i}$ (usually Gaussian) for scalar output $y_{i} . \mathbf{x}<em _zeta="\zeta">{i}$ is the $i^{\text {th }}$ set of experimental conditions. Usually, the assumption is that $g</em>$ is linear, so that</p>
<p>$$
y_{i}=\zeta^{T} \mathbf{x}<em i="i">{i}+\varepsilon</em>
$$</p>
<p>An experiment is represented by a design matrix $\mathbf{X}$, whose rows are the inputs $\mathbf{x}_{1: t}$. If we let $\varepsilon \sim \mathcal{N}(0, \sigma)$, then for the linear model, the variance of the parameter estimate $\widehat{\zeta}$ is</p>
<p>$$
\operatorname{Var}(\widehat{\zeta})=\sigma^{2}\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1}
$$</p>
<p>and for an input $\mathbf{x}_{i}$, the prediction is</p>
<p>$$
\operatorname{Var}\left(\widehat{y}<em i="i">{t}\right)=\sigma^{2} \mathbf{x}</em>
$$}^{T}\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{x}_{i</p>
<p>An optimal design is a design matrix that minimizes some characteristic of the inverse moment matrix $\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1}$. Common criteria include A-optimality, which minimizes the trace; D-optimality, which minimizes the determinant; and E-optimality, which minimizes the maximum eigenvalue.</p>
<p>Experimental design is usually non-adaptive: the entire experiment is designed before data is collected. However, sequential design is an important and active subfield (e.g. [Williams et al., 2000, Busby, 2009].</p>
<h3>2.8 Active learning</h3>
<p>Active learning is another area related to Bayesian optimization, and of particular relevance to our task. Active learning is closely related to experimental design and, indeed, the decision to describe a particular problem as active</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ These bounds hold for reasonably smooth kernel functions, where the exact formulation of the bounds depends upon the form of kernel used. We refer the interested reader to the original paper [Srinivas et al., 2010].&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5690 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5690</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5690</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-264590726</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.18358v1.pdf" target="_blank">A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The springing up of Large Language Models (LLMs) has shifted the community from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning paradigm. Along this line of research endeavors in the area, LLM-based prompting methods have attracted much attention, partially due to the technological advantages brought by prompt engineering (PE) as well as the underlying NLP principles disclosed by various prompting methods. Traditional supervised learning usually requires training a model based on labeled data and then making predictions. In contrast, PE methods directly use the powerful capabilities of existing LLMs (i.e., GPT-3 and GPT-4) via composing appropriate prompts, especially under few-shot or zero-shot scenarios. Facing the abundance of studies related to the prompting and the ever-evolving nature of this field, this article aims to (i) illustrate a novel perspective to review existing PE methods, within the well-established communication theory framework; (ii) facilitate a better/deeper understanding of developing trends of existing PE methods used in four typical tasks; (iii) shed light on promising research directions for future PE methods.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5690.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5690.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Discrete/cloze prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manually-designed discrete cloze (fill-in-the-blank) prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Natural-language templates that reformulate tasks as cloze/fill-in-the-blank items (mask or [Z]) to elicit outputs from LLMs; widely used for zero-shot and few-shot classification and knowledge probing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (e.g., BERT-style masked LMs, GPT-family autoregressive LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification / knowledge probing / translation / summarization (as examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Recast downstream tasks into forms aligned with pretraining (masked prediction or next-token prediction) by inserting a template with a masked or fill token where the model should produce the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Manually authored natural-language templates (discrete/hard prompts) that place the input into a sentence with a blank token (e.g., "[A] Overall, it was a [Z] restaurant") used in zero-shot or few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Continuous/soft prompts, null prompts, or alternative manually designed templates (different wording); sometimes compared to automated / optimized prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: shown to improve downstream task performance (especially in zero-shot settings) but highly sensitive to wording/templating choices; can be unstable across small prompt changes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (but variable / sensitive)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>By reframing tasks to mimic pretraining objectives (masked or next-token prediction), discrete cloze prompts reduce encoding mismatch and elicit stored knowledge from the model; however exact wording determines which embeddings/context are activated, causing sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Discrete prompts are time-consuming and unstable; subtle wording differences can substantially decrease performance, and simple null prompts (concatenating input + [mask]) have been reported to achieve comparable accuracy in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5690.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5690.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristic prompt construction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heuristic-based prompt construction (data-driven template selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Construct prompts using intuitive heuristics such as frequent middle words or shortest dependency-path phrases extracted from training data to form more flexible prompts per-example.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How can we know what language models know?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (survey references such as GPT-family and other pre-trained LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text classification / prompt-based tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Create per-example or dataset-specific prompts using heuristic rules derived from training data statistics to better match task distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Heuristic-constructed discrete prompts (e.g., selecting frequent middle words or shortest dependency path phrases as prompt fragments) applied in few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Manual hand-crafted prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported to yield a large performance gain compared to manually designed prompts in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to manually-designed prompts: large qualitative gain (no numeric value reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Heuristic prompts better align prompt content with training examples and extract more task-relevant cues, reducing the encoding mismatch between user intent and model representation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5690.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5690.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paraphrasing/back-translation prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paraphrasing-based prompt augmentation (including back-translation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate diverse paraphrased versions of prompts (e.g., via back-translation or model-based rewriters) to expand candidate prompts for selection or ensembling, increasing lexical diversity while preserving meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs and auxiliary MT / paraphrase models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prompt candidate generation for various downstream tasks (classification, QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce paraphrase variants of a base prompt to improve robustness and cover lexical/formulation variations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple paraphrased prompt templates produced by back-translation, model-based rewriters, or synonym substitution; then either select or aggregate across these prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single manually-written prompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: increases lexical diversity and can improve performance and robustness; used as part of prompt candidate pools and multi-prompt aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (robustness/performance typically increases)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Paraphrasing broadens the prompt space and reduces sensitivity to exact wording by covering semantically equivalent phrasings, decreasing encoding error.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5690.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5690.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Optimized discrete prompts (AutoPrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gradient-guided or search-based automatic discrete prompt generation (e.g., AutoPrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated methods that search discrete token triggers (via gradients or other search/optimization signals) to find prompt tokens that elicit desired behavior from pretrained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Eliciting knowledge from language models with automatically generated prompts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pretrained LMs used in cited work (e.g., GPT variants / BERT-family in original AutoPrompt-type work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge elicitation / classification / probing tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Search over discrete token space for trigger words or short phrases that, when used as prompts, maximize probability of desired outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Automated discrete trigger search using gradient signals or reinforcement learning to find high-performing discrete prompt tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Manual discrete prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: expands candidate space and can substantially improve prompting performance compared to naive manual prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Optimization discovers discrete tokens that strongly activate relevant model knowledge pathways, reducing encoding mismatch more effectively than manual search.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5690.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5690.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Soft / continuous prompts (prefix-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous soft prompts / prefix-tuning / prompt-tuning (learned embedding prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Learn a small set of continuous prompt parameters (soft prompts) injected into the model's input space (prefix/infix/hybrid) to steer generation, often via gradient-based optimization, providing better performance than discrete prompts in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prefix-tuning: Optimizing continuous prompts for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoregressive / sequence-to-sequence LLMs (e.g., T5, GPT-style models referenced in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text generation, summarization, classification, table-to-text and other generation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Learnable continuous vectors appended or inserted into the model context to instruct behavior without modifying base model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Soft prompt embeddings (prefix/infix/hybrid) with tunable length and initialization; investigated positions include prefix, infix, hybrid and parameters like length and initialization methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Discrete/hard prompts; different soft-prompt positions and initializations compared against each other.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: soft prompts generally outperform hard/discrete prompts; prefix slightly outperforms infix in cited comparisons; proper initialization strongly affects performance; there is a length threshold after which gains plateau or slightly decline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (soft prompts > hard prompts; prefix ≳ infix)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Continuous prompts can occupy a richer, denser representation space aligned with model embeddings, enabling finer-grained steering and optimization that better matches model internals, reducing encoding error; prompt position, length and initialization modulate how prompt vectors interact with model representations, affecting efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Random initialization of soft prompts is usually unsatisfactory; no single optimal length across tasks — performance may plateau or decline beyond a task-specific threshold; some works report only slight differences between positions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5690.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5690.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Answer-space engineering / verbalizers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt answer engineering (discrete verbalizers and soft answer tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that constrain or map the model's free-form outputs into a controlled answer space (discrete tokens, spans, sentences) and search or learn mappings (verbalizers) between model outputs and task labels, sometimes via optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs and masked LMs used for classification/labeling tasks (survey-level references)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text classification, named-entity recognition, QA, and other tasks requiring mapping from generated text to labels</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Design the admissible output set (tokens, spans, sentences) and the mapping from those outputs to final task labels; methods include manual mapping, paraphrasing/broadening, decomposition of labels, and learned/verbalizer optimization (including soft answer tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Constrain output to a predefined discrete answer set, prune vocabulary to top-k candidates per class, or learn continuous soft token embeddings representing classes and optimize them together with prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Identity/free-form outputs versus constrained/verbalized outputs; discrete verbalizers versus soft (learned) answer embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: restricting or optimizing the answer space (verbalizers or soft tokens) improves alignment between generated text and target labels and yields better downstream accuracy/decoding; optimization-based verbalizers and soft-answer tokens are effective.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Confining the decoding space reduces redundancy and variance in LLM outputs, simplifying the decoding/mapping step and reducing decoding error; learned mappings can better capture semantic correspondence than naive manual mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5690.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5690.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-prompt ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-prompt engineering and ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use multiple different prompt templates at inference (or generate several outputs per prompt) and aggregate results (majority vote, weighted average, verification) to stabilize predictions and mitigate prompt sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (survey references including GPT-family and others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Classification and reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Send multiple semantically-related prompts or sample multiple decoder outputs per prompt, then combine results to obtain a final prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Ensemble across different prompt templates (expanding P_T) or across multiple sampled reasoning paths (diversifying P_A), using aggregation strategies like majority voting, weighted averaging, or model-based verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single prompt inference</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: aggregation across multiple prompts/paths typically yields more stable and often higher-accuracy outputs than relying on a single prompt, reducing sensitivity to prompt formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (stability and often accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Ensembling compensates for variability introduced by different prompt phrasings and sampling randomness, approximating a larger semantic neighborhood around user intent (reducing encoding error) and leveraging complementary correct paths (reducing decoding error).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>For some simple tasks ensemble gains are limited; aggregation adds compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5690.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5690.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) & self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting and self-consistency aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt formats that elicit intermediate reasoning steps (CoT) and techniques that sample multiple reasoning chains then use a consistency-based aggregation (majority vote or verifier) to improve final answer accuracy on complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large autoregressive LLMs (e.g., GPT-3, PaLM and other chain-of-thought-capable models referenced in the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex multi-step reasoning and arithmetic/commonsense/symbolic problems</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate step-by-step rationale (chain-of-thought) from which final answers are derived; sample multiple chains and aggregate to select the most consistent answer.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>CoT prompting (few-shot or zero-shot prompts like 'Let's think step by step') combined with sampling multiple reasoning paths and self-consistency voting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard direct-answer prompting with no intermediate steps</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: CoT with self-consistency increases reasoning accuracy over direct prompting; zero-shot CoT (fixed phrase) can elicit reasoning in previously zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Eliciting intermediate steps exposes and aggregates internal latent reasoning traces; sampling multiple chains and voting mitigates incorrect individual chains and amplifies consistent correct reasoning paths, reducing decoding error.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5690.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5690.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot CoT ('Let's think step by step')</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting (fixed prompt phrase)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple zero-shot prompt template (e.g., 'Let's think step by step') that elicits chain-of-thought style intermediate reasoning without task-specific exemplars, improving performance on reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models are Zero-Shot Reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large LLMs (survey-cited examples include GPT-family models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning tasks (arithmetic, commonsense, symbolic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use a zero-shot instruction that prompts the model to produce intermediate reasoning before the answer, without few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot prompt appended to question (e.g., 'Let's think step by step') to encourage chain-of-thought generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard zero-shot prompts without the CoT phrase; few-shot CoT with exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported improvements in multi-step reasoning accuracy compared to naive zero-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The fixed CoT cue triggers the model to produce intermediate reasoning traces aligning with its latent reasoning capabilities, converting zero-shot setups into effective reasoning prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5690.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5690.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-turn / least-to-most prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-turn decomposition prompting (Least-to-Most prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decompose a hard problem into a sequence of simpler subproblems solved iteratively (answers fed into subsequent prompts), improving ability to solve complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (survey references to GPT-family and other large LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex multi-step reasoning and semantic parsing tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Iteratively reduce hard problems to easier subproblems using a sequence of prompts where later prompts use prior intermediate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multi-turn prompting where the original task is decomposed and intermediate results are appended to subsequent prompts (temporal decomposition of prompt templates).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single-shot end-to-end prompting or flat chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: reported to enable solving of problems that single-shot prompting struggles with, improving final answer correctness for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Decomposition reduces per-step difficulty and allows the model to focus on simpler subtasks, lowering encoding/decoding errors per step and improving overall problem-solving reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Prefix-tuning: Optimizing continuous prompts for generation. <em>(Rating: 2)</em></li>
                <li>Large Language Models are Zero-Shot Reasoners. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Eliciting knowledge from language models with automatically generated prompts <em>(Rating: 2)</em></li>
                <li>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Exploiting cloze questions for few-shot text classification and natural language inference <em>(Rating: 1)</em></li>
                <li>WARP: Word-level Adversarial ReProgramming <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5690",
    "paper_id": "paper-264590726",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Discrete/cloze prompts",
            "name_full": "Manually-designed discrete cloze (fill-in-the-blank) prompts",
            "brief_description": "Natural-language templates that reformulate tasks as cloze/fill-in-the-blank items (mask or [Z]) to elicit outputs from LLMs; widely used for zero-shot and few-shot classification and knowledge probing.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (e.g., BERT-style masked LMs, GPT-family autoregressive LMs)",
            "model_size": null,
            "task_name": "Classification / knowledge probing / translation / summarization (as examples)",
            "task_description": "Recast downstream tasks into forms aligned with pretraining (masked prediction or next-token prediction) by inserting a template with a masked or fill token where the model should produce the answer.",
            "problem_format": "Manually authored natural-language templates (discrete/hard prompts) that place the input into a sentence with a blank token (e.g., \"[A] Overall, it was a [Z] restaurant\") used in zero-shot or few-shot settings.",
            "comparison_format": "Continuous/soft prompts, null prompts, or alternative manually designed templates (different wording); sometimes compared to automated / optimized prompts.",
            "performance": "Qualitative: shown to improve downstream task performance (especially in zero-shot settings) but highly sensitive to wording/templating choices; can be unstable across small prompt changes.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (but variable / sensitive)",
            "explanation_or_hypothesis": "By reframing tasks to mimic pretraining objectives (masked or next-token prediction), discrete cloze prompts reduce encoding mismatch and elicit stored knowledge from the model; however exact wording determines which embeddings/context are activated, causing sensitivity.",
            "counterexample_or_null_result": "Discrete prompts are time-consuming and unstable; subtle wording differences can substantially decrease performance, and simple null prompts (concatenating input + [mask]) have been reported to achieve comparable accuracy in some settings.",
            "uuid": "e5690.0",
            "source_info": {
                "paper_title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Heuristic prompt construction",
            "name_full": "Heuristic-based prompt construction (data-driven template selection)",
            "brief_description": "Construct prompts using intuitive heuristics such as frequent middle words or shortest dependency-path phrases extracted from training data to form more flexible prompts per-example.",
            "citation_title": "How can we know what language models know?",
            "mention_or_use": "mention",
            "model_name": "LLMs (survey references such as GPT-family and other pre-trained LMs)",
            "model_size": null,
            "task_name": "Text classification / prompt-based tasks",
            "task_description": "Create per-example or dataset-specific prompts using heuristic rules derived from training data statistics to better match task distribution.",
            "problem_format": "Heuristic-constructed discrete prompts (e.g., selecting frequent middle words or shortest dependency path phrases as prompt fragments) applied in few-shot settings.",
            "comparison_format": "Manual hand-crafted prompts",
            "performance": "Qualitative: reported to yield a large performance gain compared to manually designed prompts in cited work.",
            "performance_comparison": "Compared to manually-designed prompts: large qualitative gain (no numeric value reported here).",
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Heuristic prompts better align prompt content with training examples and extract more task-relevant cues, reducing the encoding mismatch between user intent and model representation.",
            "counterexample_or_null_result": null,
            "uuid": "e5690.1",
            "source_info": {
                "paper_title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Paraphrasing/back-translation prompts",
            "name_full": "Paraphrasing-based prompt augmentation (including back-translation)",
            "brief_description": "Generate diverse paraphrased versions of prompts (e.g., via back-translation or model-based rewriters) to expand candidate prompts for selection or ensembling, increasing lexical diversity while preserving meaning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs and auxiliary MT / paraphrase models",
            "model_size": null,
            "task_name": "Prompt candidate generation for various downstream tasks (classification, QA)",
            "task_description": "Produce paraphrase variants of a base prompt to improve robustness and cover lexical/formulation variations.",
            "problem_format": "Multiple paraphrased prompt templates produced by back-translation, model-based rewriters, or synonym substitution; then either select or aggregate across these prompts.",
            "comparison_format": "Single manually-written prompt",
            "performance": "Qualitative: increases lexical diversity and can improve performance and robustness; used as part of prompt candidate pools and multi-prompt aggregation.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (robustness/performance typically increases)",
            "explanation_or_hypothesis": "Paraphrasing broadens the prompt space and reduces sensitivity to exact wording by covering semantically equivalent phrasings, decreasing encoding error.",
            "counterexample_or_null_result": null,
            "uuid": "e5690.2",
            "source_info": {
                "paper_title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Optimized discrete prompts (AutoPrompt)",
            "name_full": "Gradient-guided or search-based automatic discrete prompt generation (e.g., AutoPrompt)",
            "brief_description": "Automated methods that search discrete token triggers (via gradients or other search/optimization signals) to find prompt tokens that elicit desired behavior from pretrained LMs.",
            "citation_title": "Eliciting knowledge from language models with automatically generated prompts",
            "mention_or_use": "mention",
            "model_name": "Pretrained LMs used in cited work (e.g., GPT variants / BERT-family in original AutoPrompt-type work)",
            "model_size": null,
            "task_name": "Knowledge elicitation / classification / probing tasks",
            "task_description": "Search over discrete token space for trigger words or short phrases that, when used as prompts, maximize probability of desired outputs.",
            "problem_format": "Automated discrete trigger search using gradient signals or reinforcement learning to find high-performing discrete prompt tokens.",
            "comparison_format": "Manual discrete prompts",
            "performance": "Qualitative: expands candidate space and can substantially improve prompting performance compared to naive manual prompts.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Optimization discovers discrete tokens that strongly activate relevant model knowledge pathways, reducing encoding mismatch more effectively than manual search.",
            "counterexample_or_null_result": null,
            "uuid": "e5690.3",
            "source_info": {
                "paper_title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Soft / continuous prompts (prefix-tuning)",
            "name_full": "Continuous soft prompts / prefix-tuning / prompt-tuning (learned embedding prompts)",
            "brief_description": "Learn a small set of continuous prompt parameters (soft prompts) injected into the model's input space (prefix/infix/hybrid) to steer generation, often via gradient-based optimization, providing better performance than discrete prompts in many settings.",
            "citation_title": "Prefix-tuning: Optimizing continuous prompts for generation.",
            "mention_or_use": "mention",
            "model_name": "Autoregressive / sequence-to-sequence LLMs (e.g., T5, GPT-style models referenced in survey)",
            "model_size": null,
            "task_name": "Text generation, summarization, classification, table-to-text and other generation tasks",
            "task_description": "Learnable continuous vectors appended or inserted into the model context to instruct behavior without modifying base model weights.",
            "problem_format": "Soft prompt embeddings (prefix/infix/hybrid) with tunable length and initialization; investigated positions include prefix, infix, hybrid and parameters like length and initialization methods.",
            "comparison_format": "Discrete/hard prompts; different soft-prompt positions and initializations compared against each other.",
            "performance": "Qualitative: soft prompts generally outperform hard/discrete prompts; prefix slightly outperforms infix in cited comparisons; proper initialization strongly affects performance; there is a length threshold after which gains plateau or slightly decline.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (soft prompts &gt; hard prompts; prefix ≳ infix)",
            "explanation_or_hypothesis": "Continuous prompts can occupy a richer, denser representation space aligned with model embeddings, enabling finer-grained steering and optimization that better matches model internals, reducing encoding error; prompt position, length and initialization modulate how prompt vectors interact with model representations, affecting efficacy.",
            "counterexample_or_null_result": "Random initialization of soft prompts is usually unsatisfactory; no single optimal length across tasks — performance may plateau or decline beyond a task-specific threshold; some works report only slight differences between positions.",
            "uuid": "e5690.4",
            "source_info": {
                "paper_title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Answer-space engineering / verbalizers",
            "name_full": "Prompt answer engineering (discrete verbalizers and soft answer tokens)",
            "brief_description": "Methods that constrain or map the model's free-form outputs into a controlled answer space (discrete tokens, spans, sentences) and search or learn mappings (verbalizers) between model outputs and task labels, sometimes via optimization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs and masked LMs used for classification/labeling tasks (survey-level references)",
            "model_size": null,
            "task_name": "Text classification, named-entity recognition, QA, and other tasks requiring mapping from generated text to labels",
            "task_description": "Design the admissible output set (tokens, spans, sentences) and the mapping from those outputs to final task labels; methods include manual mapping, paraphrasing/broadening, decomposition of labels, and learned/verbalizer optimization (including soft answer tokens).",
            "problem_format": "Constrain output to a predefined discrete answer set, prune vocabulary to top-k candidates per class, or learn continuous soft token embeddings representing classes and optimize them together with prompts.",
            "comparison_format": "Identity/free-form outputs versus constrained/verbalized outputs; discrete verbalizers versus soft (learned) answer embeddings.",
            "performance": "Qualitative: restricting or optimizing the answer space (verbalizers or soft tokens) improves alignment between generated text and target labels and yields better downstream accuracy/decoding; optimization-based verbalizers and soft-answer tokens are effective.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Confining the decoding space reduces redundancy and variance in LLM outputs, simplifying the decoding/mapping step and reducing decoding error; learned mappings can better capture semantic correspondence than naive manual mappings.",
            "counterexample_or_null_result": null,
            "uuid": "e5690.5",
            "source_info": {
                "paper_title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Multi-prompt ensembling",
            "name_full": "Multi-prompt engineering and ensembling",
            "brief_description": "Use multiple different prompt templates at inference (or generate several outputs per prompt) and aggregate results (majority vote, weighted average, verification) to stabilize predictions and mitigate prompt sensitivity.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (survey references including GPT-family and others)",
            "model_size": null,
            "task_name": "Classification and reasoning tasks",
            "task_description": "Send multiple semantically-related prompts or sample multiple decoder outputs per prompt, then combine results to obtain a final prediction.",
            "problem_format": "Ensemble across different prompt templates (expanding P_T) or across multiple sampled reasoning paths (diversifying P_A), using aggregation strategies like majority voting, weighted averaging, or model-based verifiers.",
            "comparison_format": "Single prompt inference",
            "performance": "Qualitative: aggregation across multiple prompts/paths typically yields more stable and often higher-accuracy outputs than relying on a single prompt, reducing sensitivity to prompt formulation.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (stability and often accuracy)",
            "explanation_or_hypothesis": "Ensembling compensates for variability introduced by different prompt phrasings and sampling randomness, approximating a larger semantic neighborhood around user intent (reducing encoding error) and leveraging complementary correct paths (reducing decoding error).",
            "counterexample_or_null_result": "For some simple tasks ensemble gains are limited; aggregation adds compute cost.",
            "uuid": "e5690.6",
            "source_info": {
                "paper_title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) & self-consistency",
            "name_full": "Chain-of-Thought prompting and self-consistency aggregation",
            "brief_description": "Prompt formats that elicit intermediate reasoning steps (CoT) and techniques that sample multiple reasoning chains then use a consistency-based aggregation (majority vote or verifier) to improve final answer accuracy on complex reasoning tasks.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "mention",
            "model_name": "Large autoregressive LLMs (e.g., GPT-3, PaLM and other chain-of-thought-capable models referenced in the survey)",
            "model_size": null,
            "task_name": "Complex multi-step reasoning and arithmetic/commonsense/symbolic problems",
            "task_description": "Generate step-by-step rationale (chain-of-thought) from which final answers are derived; sample multiple chains and aggregate to select the most consistent answer.",
            "problem_format": "CoT prompting (few-shot or zero-shot prompts like 'Let's think step by step') combined with sampling multiple reasoning paths and self-consistency voting.",
            "comparison_format": "Standard direct-answer prompting with no intermediate steps",
            "performance": "Qualitative: CoT with self-consistency increases reasoning accuracy over direct prompting; zero-shot CoT (fixed phrase) can elicit reasoning in previously zero-shot settings.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Eliciting intermediate steps exposes and aggregates internal latent reasoning traces; sampling multiple chains and voting mitigates incorrect individual chains and amplifies consistent correct reasoning paths, reducing decoding error.",
            "counterexample_or_null_result": null,
            "uuid": "e5690.7",
            "source_info": {
                "paper_title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Zero-shot CoT ('Let's think step by step')",
            "name_full": "Zero-shot Chain-of-Thought prompting (fixed prompt phrase)",
            "brief_description": "A simple zero-shot prompt template (e.g., 'Let's think step by step') that elicits chain-of-thought style intermediate reasoning without task-specific exemplars, improving performance on reasoning tasks.",
            "citation_title": "Large Language Models are Zero-Shot Reasoners.",
            "mention_or_use": "mention",
            "model_name": "Large LLMs (survey-cited examples include GPT-family models)",
            "model_size": null,
            "task_name": "Reasoning tasks (arithmetic, commonsense, symbolic reasoning)",
            "task_description": "Use a zero-shot instruction that prompts the model to produce intermediate reasoning before the answer, without few-shot exemplars.",
            "problem_format": "Zero-shot prompt appended to question (e.g., 'Let's think step by step') to encourage chain-of-thought generation.",
            "comparison_format": "Standard zero-shot prompts without the CoT phrase; few-shot CoT with exemplars.",
            "performance": "Qualitative: reported improvements in multi-step reasoning accuracy compared to naive zero-shot prompting.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "The fixed CoT cue triggers the model to produce intermediate reasoning traces aligning with its latent reasoning capabilities, converting zero-shot setups into effective reasoning prompts.",
            "counterexample_or_null_result": null,
            "uuid": "e5690.8",
            "source_info": {
                "paper_title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Multi-turn / least-to-most prompting",
            "name_full": "Multi-turn decomposition prompting (Least-to-Most prompting)",
            "brief_description": "Decompose a hard problem into a sequence of simpler subproblems solved iteratively (answers fed into subsequent prompts), improving ability to solve complex reasoning tasks.",
            "citation_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.",
            "mention_or_use": "mention",
            "model_name": "LLMs (survey references to GPT-family and other large LMs)",
            "model_size": null,
            "task_name": "Complex multi-step reasoning and semantic parsing tasks",
            "task_description": "Iteratively reduce hard problems to easier subproblems using a sequence of prompts where later prompts use prior intermediate answers.",
            "problem_format": "Multi-turn prompting where the original task is decomposed and intermediate results are appended to subsequent prompts (temporal decomposition of prompt templates).",
            "comparison_format": "Single-shot end-to-end prompting or flat chain-of-thought",
            "performance": "Qualitative: reported to enable solving of problems that single-shot prompting struggles with, improving final answer correctness for complex tasks.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Decomposition reduces per-step difficulty and allows the model to focus on simpler subtasks, lowering encoding/decoding errors per step and improving overall problem-solving reliability.",
            "counterexample_or_null_result": null,
            "uuid": "e5690.9",
            "source_info": {
                "paper_title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Prefix-tuning: Optimizing continuous prompts for generation.",
            "rating": 2,
            "sanitized_title": "prefixtuning_optimizing_continuous_prompts_for_generation"
        },
        {
            "paper_title": "Large Language Models are Zero-Shot Reasoners.",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Eliciting knowledge from language models with automatically generated prompts",
            "rating": 2,
            "sanitized_title": "eliciting_knowledge_from_language_models_with_automatically_generated_prompts"
        },
        {
            "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Exploiting cloze questions for few-shot text classification and natural language inference",
            "rating": 1,
            "sanitized_title": "exploiting_cloze_questions_for_fewshot_text_classification_and_natural_language_inference"
        },
        {
            "paper_title": "WARP: Word-level Adversarial ReProgramming",
            "rating": 1,
            "sanitized_title": "warp_wordlevel_adversarial_reprogramming"
        }
    ],
    "cost": 0.018023,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models
24 Oct 2023</p>
<p>Yuanfeng Song yfsong@webank.com 
AI Group
WeBank Co
LtdChina</p>
<p>Yuanqin He yuanqinhe@webank.com 
AI Group
WeBank Co
LtdChina</p>
<p>Xuefang Zhao 
AI Group
WeBank Co
LtdChina</p>
<p>Hanlin Gu 
AI Group
WeBank Co
LtdChina</p>
<p>Di Jiang dijiang@webank.com 
AI Group
WeBank Co
LtdChina</p>
<p>Haijun Yang 
AI Group
WeBank Co
LtdChina</p>
<p>Lixin Fan lixinfan@webank.com 
AI Group
WeBank Co
LtdChina</p>
<p>Qiang Yang qiangyang@webank.com 
AI Group
WeBank Co
LtdChina</p>
<p>A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models
24 Oct 202382ED014288DD51262903A438078FA673arXiv:2310.18358v1[cs.CL]
The springing up of Large Language Models (LLMs) has shifted the community from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning paradigm.Along this line of research endeavors in the area, LLM-based prompting methods have attracted much attention, partially due to the technological advantages brought by prompt engineering (PE) as well as the underlying NLP principles disclosed by various prompting methods.Traditional supervised learning usually requires training a model based on labeled data and then making predictions.In contrast, PE methods directly use the powerful capabilities of existing LLMs (i.e., GPT-3 and GPT-4) via composing appropriate prompts, especially under few-shot or zero-shot scenarios.Facing the abundance of studies related to the prompting and the ever-evolving nature of this field, this article aims to (i) illustrate a novel perspective to review existing PE methods, within the well-established communication theory framework; (ii) facilitate a better/deeper understanding of developing trends of existing PE methods used in four typical tasks; (iii) shed light on promising research directions for future PE methods.Keywords Prompting Methods, Large Language Models between the pre-training tasks used to construct the LLM with the down-streaming tasks queried by the end users.Through careful prompt designing, users can steer LLM's output in the desired direction, shaping its style, tone, and content to align with their goals.To this end, numerous prompt engineering (PE) methods have been explored with the notable progress</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) (e.g., GPT-3 [1], GPT-4 [2], LLaMa [3]) make it possible for machines to understand users' attention accurately, thus revolutionizing the human-computer interaction (HCI) paradigm.Compared to traditional machine systems like databases and search engines, LLMs demonstrate impressive capability in understanding, generating, and processing natural language, facilitating a series of services ranging from personal assistants [4], healthcare [5] to e-commercial tools [6] via a unified natural language interface between users and machine.</p>
<p>The research paradigm around LLM has shifted from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning approach.Along this line of research endeavors, LLM-based prompting engineering (PE) methods [7,1] have attracted much attention, partially be-cause they are the key techniques in making full use of the superior capabilities of LLMs via constructing appropriate prompts.PE refers to the process of crafting effective instructions to guide the behavior of LLMs, and it greatly helps in bridging the gap A typical prompt usually has:</p>
<p>• Role: Define the identify that the LLM is emulating, like a doctor or a customer service agent.• Context: Describe the situation and relevant facts to frame the task or question for the LLM.Providing background context helps guide the response.• Input: Clearly explain the task or information being requested of the LLM.Concise, direct prompts work best.• Output Format: Indicate the desired output format and specify the type of output expected focuses the LLM.Typical output includes a conversational response, a summary, or a series of instructional steps, etc. • Examples (optional): Illustrative examples further clarify the appropriate response style and content.This "few-shot learning" helps steer the capabilities of the LLMs.ing engineering has evolved from solely utilizing discrete prompts to continuous prompts, and even to exploring hybrid prompts that combine continuous and discrete elements, which provides a larger optimization space to achieve better performance.With the emergent capability of LLM, LLMs are leveraged to plan and use external tools via its in-context learning capability, which significantly enhanced its ability in specialized domains and broadened its application fields.</p>
<p>Following these studies, one can summarize representative PE methods in a chronological overview as illustrated in Fig. 2.These methods can be categorized as three groups that respectively correspond to three prompting tasks proposed to improve the qualities of LLMs' outputs, namely prompt template engineering, prompt answer engineering, and multi-turn prompting and multi-prompt learning.An example of the input and output for the above-mentioned tasks can be found in Table 1.</p>
<p>• First, prompt template engineering methods aim to carefully design a piece of "text" that guides the language models to produce the desired outputs.For example, in Table 1, to finish a classical sentiment detection for a input A="Great places to eat near my location!", the prompt template engineering designs a template "[A] Overall, it was a [Z] restaurant" to enforce the LLM to fill the desired comments in the blank i.e. [Z].Essentially this type of template engineering method induces LLM to focus on word embeddings that are relevant to the questions.A common designing principle of existing prompt template engineering methods is to better align information between users and LLMs.Such a trend is manifested by the evolution from using discrete prompts (e.g., a piece of human-readable text) [11,9] to continuous ones (e.g., a continuous task-specific vector) [13,20].</p>
<p>• Second, prompt answer engineering [7] refers to the process of searching for an answer space and a map to the original output, which enhances users' understanding of the information encapsulated A d a P r o m p t W A R P M a n u a l l y
[ 1 ] [ 2 ] [ 3 ] [ 4 ] [ 5 ] [ 7 ] [ 1 0 ] [ 1 2 ] [1 5 ] [ 6 ]
[ 8 ]
[ 9 ] [ 1 1 ] [ 1 3 ] [ 1 4 ] [ 1 6 ] [ 1 7 ] [ 2 1 ] [ 2 2 ] [ 1 8 ] [ 2 4 ] [ 1 0 ] [ 2 5 ] [ 2 7 ] [ 2 8 ] [ 2 9 ] [ 3 0 ] [ 3 1 ] [ 3 2 ] [ 3 3 ] [3 4 ]
Fig. 2. Chronological overview of representative studies in prompting methods from four aspects: prompt template engineering [8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24], prompt answering engineering [25,9,26,11,10,12,27,17,28,29,30], and multi-turn prompting and multi-prompt learning [10,31,17,32,33,34,35,36,37,38,39,40,41].</p>
<p>within the LLM.For the same example in Table 1, the prompt answer engineering aims to find a mapping from the result "good" obtained from the LLM to the desired answer "positive".The field of prompt answer engineering is currently witnessing a notable development trend characterized by the pursuit of models that excel in decoding model information from simple mapping to complex mapping to enhance human comprehension.</p>
<p>• Third, multi-prompting methods mainly applied ensemble techniques [10] to mitigate the sensitivity of LLM to different formulations and to obtain a more stable output.In   "All in all, it was [Z]";) and their inference results (i.e., 1. "good" 2. "great!" 3. "okay") to obtain the final desired one (i.e., "positive").Later, as LLMs become more capable, multi-turn prompt methods attract more attention that aims to provide more context to LLM by leveraging information either from LLM itself or external tools [37,33].In the field of multi-prompting methods, researchers are endeavoring to develop adaptive strategies that enhance LLM's ability to task planning and the utilization of tools.</p>
<p>In this article, we summarize the prompting methods from a communication theory perspective with which the ultimate goal of PE is to reduce the information misunderstanding between the users and the LLMs.</p>
<p>Therefore, as delineated in Section 2, the communication theory perspective provides a coherent explanation of different PE methods in terms of their objectives and underlying principles.Moreover, this novel perspective also offers and presents insights into scenarios where by summarizing significant findings and discussing potential research directions.</p>
<p>A Communication Theory Perspective of Prompting Methods</p>
<p>The study of modern communication theory, which dates back to the 1940s and the following decades, gave rise to a variety of communication models including both linear transmission models and non-linear models such as interaction, transaction, and convergence models [42,43,44].A common theme of these early studies is to analyze how individuals utilize verbal and nonverbal interactions to develop meaning in diverse circumstances.Conceptually, the communication process is often modeled as a chain of information processing steps involving encoding, transmitting, and decoding of messages, between a sender and a receiver.</p>
<p>To give a better illustration, Fig. 3a depicts the classical Model of Communication in communication the-ory, which includes a sender encoding a message and transmitting it to the receiver over a channel.Then, the receiver decodes the message and delivers some type of response.During the transmission process, the message may be distorted due to noise, leading to the necessity of multi-turn interaction.</p>
<p>The original communication theory is widely utilized to examine factors including social [45], cultural [46], and psychological [47] that influence human communication.The overall goal of communication theory is to reveal and clarify the common human experience of interacting with others through information exchange.</p>
<p>Among early studies of various communication models, we are particularly inspired by two influential works, namely, Shannon-Weaver's mathematical model of communication [48]
X gω T −→ P T f θ −→ P A hω A −→ Y,(1)
where g ω T represents the mapping from the input X to the prompt P T , f θ denotes the mapping from the prompt P T to the answer P A and h ω A denotes the mapping from the answer P A to the output Y (see Fig. 3b for an illustration).</p>
<p>Definition 2 (Goal of PES).PES aims to maximize the mutual information between the inputs X and outputs Y , i.e.,
max ω T ,ω A I(X, Y ) = max ω T ,ω A I(X, h ω A • f θ • g ω T (X)) (2)
where f
• g(x) = f (g(x)).
It's worth noting that prompt engineering is consistently divided into two procedures: Prompt Template Engineering and Prompt Answer Engineering.Each procedure has specific goals similar to Eq. ( 2) that align with its intended purpose.</p>
<p>While the capacity in Def. 2 is well-known in information theory [50], how to reach the maximum of Eq. (2) for large language models illustrated in Fig. 3b remains an unexplored research direction.There exists a large variety of prompting engineering methods, which, in our view, essentially aim to reduce information misunderstanding between users and LLMs.In other words, they aim to reach the capacity of PES as defined.This connection between PES and the communication models has never been explicitly stated before.</p>
<p>Moreover, the existing work can be divided into three categories: prompt template engineering (X enhance the capability of the receiver that could better handle users' information needs, and most importantly, the multi-turn prompting and multi-prompt engineering aim to constantly reduce the information misunderstanding via multi-turn interactions.</p>
<p>• Prompt template engineering aims to optimize max
ω T I(X, P A ) = max ω T I(X, f θ • g ω T (X)),(3)
which looks for an additional piece of text, namely a prompt, to steer the LLMs to produce the desired outputs for downstream tasks.From the communication theory perspective, it acts as an "encoder" to bridge the gap between the users and the LLMs by encoding the messages in a way that the model can understand and then elicit knowledge from LLMs (see details in Section 3).</p>
<p>In the encoding process, the challenge lies in the accurate understanding of the user's intention by LLM with limited instruction following capability.Template engineering aims to reduce this mismatch by translating the user's request to a format that could be better understood by LLM.</p>
<p>• Prompt answer engineering aims to optimize max
ω A I(P T , Y ) = max ω A I(P T , h ω A • f θ (P T )),(4)
which focuses on developing appropriate inputs for prompting methods, has two goals: 1) search</p>
<p>for a prompt answer P A ; 2) look for a map to the target output Y that will result in an accurate predictive model.In the decoding process, LLMgenerated output often carries redundant information in addition to the expected answer due to its unlimited output space.Answer engineering aims to confine the output space and extract the target answer.The field of prompt answer engineering is currently witnessing a notable develop-ment trend characterized by the pursuit of effective answer engineering such that ultimate outputs (i.e.Y ) are well aligned with that of end users' expectations (see details in Section 4)</p>
<p>• To further reduce the information misunderstanding, the user could conduct multi-interaction according to Eq.</p>
<p>(3) and Eq. ( 4), called multi-prompt/multi-turn PE.Multiprompting methods aims to optimize max
ω T i M i=1 I(X, f θ • g ω T i (X)),(5)
which mainly applied ensemble techniques [10] to mitigate the sensitivity of LLM to different formulations and to obtain a more stable output.Later, as LLMs become more capable, multiturn prompt methods focus to provide more context to LLM by leveraging multiple communication procedures between the machine and person [37,33].In the field of multi-prompting methods, researchers are endeavoring to develop adaptive strategies that enhance LLM's ability to task planning and the utilization of tools.The adaptive and iterative nature of multi-prompting methods is by the communication theory (see Section 5 for an elaborated explanation).</p>
<p>Prompt Template Engineering</p>
<p>Given the information chain X → P T → P A , the answer P A is determined by the prompt-processed P T and model M with pre-trained weights θ.Suppose that PA is the targeted prediction, the key problem of prompt template engineering is to find a good prompt that maximizes the probability p( PA |M, P T , θ) on diverse downstream tasks with limited data.which are discussed in detail as follows.</p>
<p>Manually-designed</p>
<p>Initially, the prompt templates are manually designed in natural language based on the user's experience, and they have been validated to be able to improve the performances of downstream tasks, especially in a zero-shot setting [1,8].The most frequent style is to reformulate the original task as a 'fill-in-blank' cloze one [9,10], and the answer is obtained by predicting the words in the given [mask] place.For example, as illustrated in Fig. 4 and Table 2, Petroni et al. [9] manually designed prompts to re-structure the relational knowledge, while studies like [10,51]  prompts show some effectiveness [53], they are also criticized for being time-consuming and unstable [15].A subtle difference in the designed prompts may result in a substantial performance decrease.As such, how to explore the prompt space and construct prompts more thoroughly and more effectively becomes an important and challenging issue.</p>
<p>Heuristic-based</p>
<p>The heuristic-based methods focus on finding acceptable prompts by some intuitive strategies.For example, to construct more flexible and diverse prompts</p>
<p>for different examples (rather than the fixed ones),</p>
<p>Jiang et al. [11] propose to use the most frequent middle words and the phrase spanning in the shortest dependency path that appeared in the training data as a prompt.This method shows a large performance gain compared to the manually-designed prompts.
✗ ✓ ✓ ✓ ✗ ✓ Heuristic-based [11, 54, 19] ✓ ✓ ✓ ✓ ✓ ✓ Paraphrasing-based [11, 55, 14] ✓ ✓ ✓ ✓ ✗ ✓ Generation-based [17, 56] ✓ ✓ ✓ ✓ ✓ ✓ Optimization-based [12, 22, 57] ✓ ✗ ✓ ✗ ✓ ✗
LLMs can understand better.</p>
<p>Generation-based</p>
<p>The generation-based methods treat prompt searching as a generative task that can be carried out by some LMs.For example, Gao et al. [17] first make use of the generative ability of T5 [52] to fill in the placeholders as prompts, and then the prompts could be further improved by encoding domain-specific information [56].</p>
<p>Optimization-based</p>
<p>To alleviate the weakness of insufficient exploration space faced by existing methods, the optimized-based methods try to generate prompts guided by some optimization signals.For example, Shin et al. [12] employs gradients as the signals, and then searches for discrete trigger words as prompts to enrich the candidate space.Deng et al. [22] generates the prompt using a reinforced-learning approach that is directed with the reward function.</p>
<p>Ranking P T</p>
<p>After obtaining multiple prompt candidates with the above-mentioned methods, the next step is to rank them to select the most effective one.Existing studies solve this problem by finding prompts that are close to the training samples to reduce the information mismatch between the pre-training and inference phases.</p>
<p>Execution Accuracy</p>
<p>Since the objective of the designed prompts is to fulfill the downstream tasks, it's intuitive and straightforward to evaluate the performance by execution accuracy over the specific tasks [57,17,11].</p>
<p>Log Probability</p>
<p>The log probability criterion prefers the prompt that delivers the correct output with higher probability, rather than being forced to give the exact answer.</p>
<p>For example, a prompt template that can work well for</p>
<p>all training examples is given the maximum generated probability in [17].Furthermore, language models can also be utilized to evaluate the quality of prompts.In [59], the prompt with the highest probability given by an LM is selected, which indicates closer to the general expression that appears in the training dataset.</p>
<p>Others</p>
<p>Other criteria can be used to select the top one or the top-k prompt.For example, Shin et al. [12] regards the words that are estimated to have the largest performance improvement as the most crucial elements.</p>
<p>Tuning P T</p>
<p>Due to the continuous nature of LLMs, searching over discrete space is sub-optimal [15].How can we further improve the performance once we obtain a prompt?</p>
<p>Recent studies turn to optimizing the prompt as continuous embeddings.</p>
<p>The main idea is to learn a few continuous parameters, referred to as soft prompts, and these continuous parameters can be optionally initialized by the previously obtained discrete prompt.Li et al. [13] first introduces a continuous task-specific 'prefix-tuning' for generative tasks.Studies like [20] and [15] adopt a similar strategy and prove its effectiveness in various natural language understanding tasks.Following the abovementioned studies, many improvements have been conducted to find better prompts, such as better optimizing strategies [16], better vector initialization [21,23],</p>
<p>indicative anchors [15] etc.Furthermore, studies like [60,13,20] further point out that prompt position, length, and initialization all affect the performance of continuous prompts [60,13,20] (Table 3).In this section, we summarize these factors as follows:</p>
<p>• Different Position.</p>
<p>There are three different positions for autoregressive LM that the prompt can be inserted into, that is, the prefix There is no significant performance difference between those positions.[13] shows that prefix prompt sightly outperforms infix prompt, and the hybrid one is much more flexible than the others.</p>
<p>• Different Length.There is no optimal length for all tasks, but there is always a threshold.The performance will increase before reaching the threshold, then it will either plateau or slightly decrease.</p>
<p>• Different Initialization.A proper initialization is essential for the performance of the prompts and the performance of random initialization is usually unsatisfactory.Typical methods include initialized by sampling real word [13,20], using class label [20], using discrete prompt [16], and using pre-trained based vector [21,23].Furthermore, the manually designed prompts can provide a good starting point for the following search process.</p>
<p>Trends for Prompt Template Engineering</p>
<p>There are two trends in prompt template engineering:</p>
<p>• Tend to have less human involvement, using automated methods rather than designing manually when constructing prompts.</p>
<p>• Tend to develop optimization-based techniques.</p>
<p>The gradient-based searching method shows better performance than the derivative-free one in</p>
<p>Work Position</p>
<p>Length Initialization prefix tuning [13] prefix, infix 200 (summarization), 10 (table-to-text) random, real words prompt tuning [20] prefix 1, 5, 20, 100, 150 random, sampled vocab, class label p-tuning [15] hybrid 3 (prefix), 3 (infix) LSTM-trained DART [18] infix 3 unused token in vocabulary OPTIPrompt [16] infix 5, 10 manual prompt dynamic [60] hybrid, dynamic dynamic sampled vocab hard prompt construction while the soft prompt is more promising than the hard one.</p>
<p>From the communication theory perspective, the development history of prompting template engineering reflects the trends of utilizing prompts with stronger expressive ability to better capture the user's intent.</p>
<p>Prompt Answering Engineering</p>
<p>As illustrated in Fig. 3 2).Technology-wise, PAE involves a set of methods that control admissible answer space and optimization mechanisms of LLMs' output (see overview in Table 4).</p>
<p>Search for an Answer Space</p>
<p>Pre-defined Answer Space</p>
<p>This involves a set of pre-defined answers for the question-answering task, e.g., pre-defined emotions ("happiness", "surprise", "shame", "anger", etc.) for the sentiment classification task.The model can then be trained to select the best answer from this predefined space.As an illustration, the answer space P A can be defined as the set of all tokens [9], fixed-length spans [65], or token sequences [8].Furthermore, in certain tasks like text classification, question answering, or entity recognition, answers are crafted manually as word lists that pertain to relevant topics [25,27].</p>
<p>Discrete Answer Space</p>
<p>Discrete answer space refers to a set of specific and distinct answer options that a language model can choose from when generating a response to a given prompt.</p>
<p>Specifically, the possible answers are limited to a fixed set of choices, such as a small number of named entities or keyphrases (e.g., the total choice of planet in the solar system is eight).The model can then be trained to identify whether the correct answer is among this set of possibilities [63,61,12].</p>
<p>Continuous Answer Space</p>
<p>Continuous answer space refers to a scenario where the possible answers or responses are not restricted to a predefined set of discrete options.Instead, the answers can take on a range of continuous values or be any text, number, or value within a broader, unbounded spectrum [28,66].</p>
<p>The model can then be trained to predict a point in this continuous space that corresponds to the correct answer.</p>
<p>Hybrid Approach</p>
<p>This involves combining multiple methods to design the answer space, such as using a pre-defined list of entities for certain types of questions, but allowing for free-form text answers for other types of questions [67].Pre-defined answer [9,25,27] Generation Remark 1. Answer shapes summarized as follows are also needed in prompt answer engineering.In practice,</p>
<p>the choice of answer shape depends on the desired outcome of the task.</p>
<p>• Tokens: individual tokens within the vocabulary of a pre-trained Language Model (LLM), or a subset of the vocabulary..</p>
<p>• Span: short sequences of multiple tokens, often comprising a phrase or segment of text.</p>
<p>• Sentence: A longer segment of text that can encompass one or more complete sentences.</p>
<p>Search for an Answer Mapping</p>
<p>There are several strategies to search for an answer mapping.</p>
<p>Manually Mapping</p>
<p>In many cases, the mapping from potential answers space P A to output Y is obvious such that this mapping can be done manually.For instance, the answer is output itself for the translation task [9] such that the mapping is identity mapping; In addition, Yin et al [25] designed related topics ("health", "food", "finance", "sports", etc.), situations ("shelter", "water", "medical assistance", etc.), or other possible labels.Cui et al. [27] manually proposed some entity tags such as "organization", "person" and "location", etc. for the Named Entity Recognition problem.</p>
<p>Broadening the answer P A</p>
<p>Broadening P A (P ′ A = B(P A )) is expanding the answer space to obtain a more accurate mapping.Jiang et al. [63] proposed a method to paraphrase the answer space P A by transferring the original prompt into other similar expressions.In their approach, they employed a back-translation technique by first translating prompts into another language and then translating them back, resulting in a set of diverse paraphrased answers.The probability of the final output can be expressed as P (Y |x) = y∈B(P A ) P (y|x), where B(Y ) represents the set of possible paraphrased answers.</p>
<p>Decomposing the output</p>
<p>Decomposing Y (D(Y )) aims to expand the information of Y , which makes it easier to look for a mapping g θ .For example, Chen et al. [64] decomposed the labels into several words and regarded them as the answer.Concretely, they decomposed label/output "per:city of death" into three separated words {person, city, death}.The probability of final output can be written as P (y|x) = y∈D(Y ) P (y|x).</p>
<p>Optimizing the mapping.</p>
<p>There exist two approaches to optimize the mapping function.The first approach is to generate the pruned space PA and search for a set of answers within this pruned space.Schick et al. [26,61] introduced a technique for generating a mapping from each label to a singular token that represents its semantic meaning.This mapping, referred to as a verbalizer v, is designed to identify sets of answers.Their approach involves estimating a verbalizer v by maximizing the likelihood w.r.t. the training data conditioned on the verbalizer v.</p>
<p>Shin et al. [12] proposed an alternative approach for selecting the answer tokens.They employed logistic classifiers to identify the top-k tokens that yield the highest probability score, which together form the selected answer.In addition, Gao et al. [62] constructed a pruned set PA c containing the top-k vocabulary words based on their conditional likelihood for each class c.As for the second approach, it investigates the potential of utilizing soft answer tokens that can be optimized through gradient descent.Hambardzumyan et al. [28] allocated a virtual token to represent each class label and optimized the token embedding for each class along with the prompt token embedding using gradient descent.</p>
<p>Trends for Prompt Answer Engineering</p>
<p>There are two trends in prompt answer engineering:</p>
<p>• Developing more robust and generalizable question-answering models that can handle more complex tasks and a broader range of inputs.For example, the answer space is some discrete spans at the beginning (see Sect. 6) and developed to the complex continuous space (see Sect. 4.1.</p>
<p>3).</p>
<p>• There is also a focus on improving the quality and relevance of prompts to improve model performance.Specifically, several techniques have been explored, such as paraphrasing and pruning, after the direct mapping approach.More recently, optimization methods using gradient descent have been proposed to enhance accuracy.</p>
<p>The prompt answering engineering also shows a trend of exploring prompts to decode the machine language with less information loss, i.e., has a better understanding of the machine.</p>
<p>Multiple Prompting Methods</p>
<p>Multiple prompts can be utilized to further reduce the information mismatch during the encoding and decoding process.These methods can be categorized into two main types, namely "multi-prompt engineering"</p>
<p>and "multi-turn prompt engineering", depending on the interrelationship of prompts (see Fig. 5).Multi-prompt engineering is akin to an ensemble system, whereby each response serves as a valid answer, and responses from multiple prompts are aggregated to produce a more stable outcome.This type of method can be thought to extend the use of prompts in the spatial domain.On the other hand, multi-turn PE entails a sequence of prompts, whereby subsequent prompts depend on the response generated from previous prompts or the obtaining of the final answer relies on multiple responses.Consequently, this type of method can be viewed as an extension in the temporal domain.</p>
<p>Multi-prompt Engineering Methods</p>
<p>Multi-prompt methods employ multiple prompts with similar patterns during the inference aiming to enhance information preservation.This method is closely associated with ensembling techniques [93,94,95].Although the primary motivation is to exploit the complementary advantages of different prompts and reduce the expenses associated with PE, it can also be integrated with prompt-engineering techniques to further improve efficacy.From a communication theory perspective, multi-prompt engineering can be considered as sending multiple copies of the message to ensure the authentic delivery of data.</p>
<p>Expanding P T</p>
<p>Expanding P T aims to cover a larger semantic space around the sender's true intention, and a more stable approximation of the target output, XA , can be ob-  tained by aggregating the responses.</p>
<p>Jiang et al. [11,20,28] propose to combine outputs of different prompts to get the final result for classification tasks.Qin et al. [68] incorporates multi-prompt ideas with soft ppromptsand optimizes weights of each prompt together with prompt parameters.Yuan et al. [55] propose to use text generation probability as the score for text generation evaluation, and aggregate multiple results of different prompts as the final score.</p>
<p>Diversifying P A</p>
<p>Different from expanding P T whose main goal is to leverage the input space around P T , diversifying P A aims to exploit the various "thinking paths" of the LLM through sampling its decoder.This is especially effective for handling complex tasks, such as mathematical and reasoning problems.</p>
<p>Wang et al. [69] propose a self-consistency method based on the Chain-of-thoughts (CoT) which samples multiple reasoning paths and selects the most consistent answer by majority voting or weighted averaging.</p>
<p>Lewkowycz [70]   PET with multiple verbalizers.This is achieved by introducing sample-dependent output space.</p>
<p>Multi-turn Prompt Engineering Methods</p>
<p>Multi-turn prompt engineering methods involve decomposing the full prompting task into several subtasks, each addressed by a corresponding prompt.This process typically entails a sequence of encoding and decoding operations, where subsequent prompts may depend on the decoded message from previous prompts or each prompt is responsible for a sub-task.The outcome can be obtained either from the result of the last prompt or by aggregating the responses generated by all prompts.This strategy is designed to tackle challenging tasks, such as complex mathematical questions or reasoning tasks.It mainly involves two components:</p>
<p>1) decomposing P T into sub-tasks to reduce the difficulty of each sub-task; and 2) modifying P T to generate better intermediate results for later steps.These two components can help to bridge the gap between complex X and Y .introduced a dynamic least-to-most prompting method for semantic parsing tasks by utilizing multiple prompts to build a more flexible tree-based decomposition.Following this line of research, we also would like to discuss some potential challenges and future directions for PE methods, which could be divided into four categories including Reducing the Encoding Error, Reducing the Decoding Error, and Interactive and Multi-turn Prompting.</p>
<p>Reducing the Encoding Error</p>
<p>• Better Ranking Criteria.One of the points of discrete prompts is that it's difficult to design and choose an optimal prompt, causing its instability.Although soft prompts partly addressed this problem, the discrete prompt is still very important because it has good interpretability and has been proven to be able to help soft prompts search effectively.Looking through the existing methods, we can find that accuracy-based criteria are resource-consuming, while LM-based log probability is not sufficient to evaluate the prompt.So a well-designed ranking criterion combined with a mass of auto-based generated prompts may be a good direction for the future.</p>
<p>• Task-agnostic Prompt.Even though the prompt has been proven effective in many tasks such as classification and text generation, most of the existing work has to design a specific prompt for a given task, which makes it complex and complicated [109].So how to generate a taskagnostic prompt or transfer the prompt to other fields quickly may be a challenging problem.</p>
<p>Discrete(meta-learning [110]) and continuous (decomposition [111]) prompts are applied to tackle this issue.However, they are not well-optimized and can serve unseen tasks.</p>
<p>• Interpretability Issue.Recent studies show that those methods learning optimal prompts in continuous space can achieve better performance than in discrete space [7].However, the generated 'soft' prompts are difficult to read and understand, namely poor interpretability.Therefore, designing and improving soft prompts can be tough.Existing work [20] tries to use the nearest words in embedding space to probe the effect.</p>
<p>However, the reasons excavated are not obvious.</p>
<p>It remains to explore why this kind of prompt can work well and what causes the performance differences between different prompts.</p>
<p>Reducing the Decoding Error</p>
<p>• Privacy-preserving Methods [112,113,114].To address privacy concerns on output, future research could focus on developing methods that preserve the privacy of the data used for training and inference.This could include techniques such as differential privacy, homomorphic encryption, and federated learning.</p>
<p>• Human-in-the-loop Methods [115].To improve the accuracy and relevance of prompt answer engineering methods, future research could focus on developing methods that incorporate human feedback and interaction.This could enable users to provide feedback and corrections to the generated answers and to refine the model over time.the "thinking path" of LLMs can be evoked with proper indication.This property can be exploited to generate step-by-step task-solving procedures like scratch paper in exams so that the final answer can be better justified.[83] builds an interactive framework based on this idea and further involves human interaction for better controllability of the process.In addition to this online paradigm, LLMs can also be asked to explain the answer or decision afterward.[116] demonstrated that GPT-3 generates more favorable free-text explanations than crowdsourced.</p>
<p>• Interactive Multi-turn Prompt.Though automation in prompting methods is highly wanted, humans in the loop can bring more controllability and supervision over the process, producing more reliable results.However, frequent human intervention will diminish the efficiency gained by using LLMs.Therefore, in addition to the granularity of decomposed tasks, it is also required to determine when to involve human feedback.This could be designed manually for each task, but it would be much more efficient if LLMs could plan these stages by themselves.</p>
<p>Conclusion</p>
<p>This paper tries to provide an overview of existing prompting methods from a communication theory perspective.Towards this objective, we consider LLMs as a unified interface to achieve various NLP tasks and examine these prompt-based studies to reduce the information misunderstanding that appears in the different stages between users and LLMs during their interactions.We hope this survey will inspire researchers with a new understanding of the related issues in prompting methods, therefore stimulating progress in this promising area.</p>
<p>Fig. 1 .
1
Fig.1.Components of A Good Prompt.</p>
<p>l F o r m e r H u g g i n g G P T</p>
<p>Fig. 3b.Specifically, the prompt template engineering aims to reduce the encoding error/ look for the prompt that is easily understood by the machine, while the prompt answering engineering aims to reduce the decoding error/ look for the prompt that easily understood by the human.The development of LLMs aims to</p>
<p>are dedicated to solving the text classification and language understanding tasks by several self-defining prompt patterns and propose a new training procedure named PET.Another line of work involves developing prefix prompts for generation tasks, which provide instructions and steer the LLMs to finish the sentence.For example, a summarization task can be handled by adding 'TL;DR:' [8], and a translation task can be conducted into 'Eng.Translate to Spanish: Span' [52].Even though manually designed</p>
<p>[P</p>
<p>REF IX; X T ; Y ], the infix [X T ; IN EF IX; Y ], and the hybrid one [P REF IX; X T ; IN F IX; Y ].</p>
<p>(b), prompt answer engineering (PAE) aims to align LLMs outputs with the intended purpose.The use of PAE is motivated by the need to mitigate the gap between the capabilities of pretrained LLMs and a large variety of requirements of different downstream tasks (see more discussion in Sect.</p>
<p>Fig. 5 .
5
Fig.5.Overview of multiple prompting methods.(a) Multi-prompt methods utilize several similar prompts to produce a more stable result.(b) Multi-turn prompt methods produce the final result by aggregating responses from a sequence of prompts.</p>
<p>applied a similar idea to quantitative problems by combining multiple prompts and output sampling.Wang et al.[71] investigated various ensemble variants in reasoning problems and found that rational sampling in the output space is more efficient.These methods solely used the final answer as the selection criterion and did not exploit the generated rationals from various sampling paths.To take advantage of these intermediate results, Li et al.[72] proposed to generate more diversified reasoning paths with multiple prompts and used a model-based verifier to select and rank these reasoning paths.Fu et al.[73] introduced a complexity-based metric to evaluate reasoning paths and prioritize those with higher complexity in the aggregation.Weng et al.[96] employed LLM to self-verify various reasonings by comparing predicted conditions using the generated reasonings to original conditions.The consistency score is then used to select the final result.Yao et al.[97] proposed the "Tree of Thoughts" to explore the intermediate steps across various reasoning paths, and used the LLM to evaluate the quality of each possible path.</p>
<ol>
<li>
<p>1 . 3
13
Optimizing θ This line of work treats multiple prompts as a label generator to address the sample deficiency problem.Schick et al. [10] first proposes pattern-exploiting training (PET) that employs a knowledge distillation strategy to aggregate results from multiple promptverbalizer combinations (PVP).They first utilize PVP pairs to train separate models that generate pseudolabels for unlabeled datasets.This extended dataset is then used to train the final classification model.Schick et al. [98] extends this idea to the text generation task by using the generation probability of decoded text as the score.[17] uses a similar method for automatic template generation.Schick et al. [74] further expands</p>
</li>
<li>
<p>2 . 1
21
Decomposing P T Decomposing P T is the first step in handling complex tasks, and a proper decomposition requires a good understanding of both the target task and the user's intention.Yang et al. [99] decomposed SQL operations using fine-tuned few-shot models and untrained zero-shot models combined with predefined rules.However, ruled-based decomposition heavily relies on human experiences, so it is desirable to automate this step with LLMs.Min et al. [76] proposed an unsupervised method that utilizes a similarity-based pseudodecomposition set as a target to train a seq2seq model as a question generator.The decomposed simple question is then answered by an off-the-shelf single-hop QA model.Perez et al. [31] treats the decomposition in multi-hop reading comprehension (RC) task as a span prediction problem which only needs a few hundreds of samples.For each task, various decomposition paths are generated, with each sub-question answered by a single-hop RC model.Finally, a scorer model is used to select the top-scoring answer based on the solving path.Khot et al. [77] proposed a text modular network leveraging existing models to build a next-question generator.The training samples are obtained from sub-task models conditioned on distant supervision hints.With the emergent general ability of LLMs, instead of training a task-specific decomposition model, LLMs are used to fulfill decomposition tasks.Zhou et al. [38] proposed the least-to-most prompting method where hard tasks are first reduced to less difficult sub-tasks by LLM.Then answers from previous sub-problems are combined with the original task to facilitate subsequent question solving.Dua et al. [78] employs a similar idea and appends both question and answer from the previous stage to the subsequent prompt.Creswell et al. [79] proposed a selection-inference framework.It uses LLM to alternatively execute selecting relevant information</p>
</li>
</ol>
<p>Fig. 6 .
6
Fig.6.Schematic illustrations of multi-prompting methods.(a) Multi-prompt methods mainly employ ensemble-based methods.(b) Multi-turn prompt methods mainly leverage LLMs or external tools to provide clearer and more helpful context.</p>
<ol>
<li>3
3
Interactive and Multi-turn Prompting • Transparency and Explainability.Despite the recent popularity of LLMs, the lack of explainability of the outputs and transparency of the working mechanism makes LLMs less attractive in complex tasks that require high stability.The success of the chain-of-thoughts methodology shows</li>
</ol>
<p>Table</p>
<p>Table 1 .
1
Running Examples for PE Methods
StageInputOutputPrompt Template EngineeringGreat places to eat near my location!Great places to eat near my location! Overall, it was a [Z] restaurant.Large LanguageGreat places to eat near my location!Great places to eat near my location!ModelOverall, it was a [Z] restaurant.Overall, it was a good restaurant.Prompt Answering EngineeringgoodpositiveMulti-Prompt1. It was a [Z]; 2. Just [Z]; 3. All in all, it was [Z];1. good 2. great! 3. okayexisting prompting methods come short.
The remainder of the article is structured as follows: Section 2 details the overview of the prompting methods from the communication theory perspective.Sections 3, 4, and 5 review and summarize the recent progresses, respectively, from four PE tasks namely prompt template engineering, answer engineering, and multi-turn prompting methods.Section 6 discusses other related surveys and potential research directions.Finally, we conclude this article in Section 7</p>
<p>[9,10,10,51,9,8,15,52,53,53,11]cting P T is to transform the specific task to make it align with the pretraining objective (i.e., next-word prediction, masked LM) of the LM.As shown in Table2, existing prompt constructing methods[9,10,10,51,9,8,15,52,53,53,11]could be categorized into five different approaches,
3.1 Constructing P T
To obtain the optimal prompt, current works [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24] can be formulated into three categories: constructing P T , ranking P T and tuning P T , as shown in Fig. 4.</p>
<p>Table 2 .
2
Summary of the prompt construction methods
3.1.3 Paraphrasing-basedThe paraphrasing-based methods are widely usedin data augmentation, aiming at generating augmenteddata that is semantically related to the original text,and this could be achieved in various ways using ma-chine translation, model-based generation, and rule-based generation [58]. The paraphrasing-based meth-ods could naturally be used to construct prompt can-didates based on the original text, and we could fur-ther select the best one or integrate them to providebetter performance. Representative studies includes[11, 55, 14]. Specifically, Jiang et al. [11] uses back-
[8,9,10].[19]tries to form task-specific prompts by combining simple human-picked sub-prompts according to some logic rules.Different from the above methods, Logan et al.[54]uses an extremely simple uniform rule by null prompts, which only concatenates the inputs and the [mask] token, and it's able to gain a comparable accuracy with manually-defined prompts.translation to enhance the lexical diversity while keeping the semantic meaning.Yuan et al.[55]manually creates some seeds and finds their synonyms to narrow down the search space.Haviv et al.[14]uses a BERTbased model to act as a rewriter to obtain prompts thatMethodAutomated Gradient-Free Few-shot Zero-shot Stability Interpret-ability Manually-design[8,9,10]</p>
<p>Table 3 .
3
Summary of the prompt tuning methods</p>
<p>Table 4 .
4
Summary for the prompt answer engineering methods
Answer Space TypeAnswer Mapping MethodWorkTask TypeOptimizing the mappingDiscrete Answer Space Continuous Answer Space[26, 61, 12, 62] Classification &amp; regression [28] ClassificationBroadening the outputdiscrete Answer Space[63]GenerationDecomposing the outputdiscrete Answer Space[64]ClassificationManually Mapping</p>
<p>Table 5 .
5
Summary of the PE methods involving multiple prompts.NLU: Natural Language Understanding, NLG: Natural Language Generation.
MethodNLUNLGReasoningMulti-promptExpanding PT Diversifying PA[11, 20, 28, 68] -[55] --[69, 70, 71, 72, 73]Optimizing θ[10, 74][75, 17]-Multi-turn promptDecomposing PT Refining PT--[31, 76, 77] [38, 78, 79, 80, 81, 82, 83] [84, 85] [32, 37, 85, 69, 86, 87]Augmenting PT-[39, 88][40, 41, 89]Optimizing θ-[31, 76][89, 90, 91, 92]</p>
<p>[90]et al. [76,31]trained an LM model for decomposing the original task into subtasks.Nye et al.[90]trains the LLM to produce intermediate steps stored in a scratch pad for later usage.
addresses this problem by using hard questions with neering:that revolves around the way we look at NLP. In an-human annotations as exemplars. The hardness is mea-• Developing an enhanced adaptive promptingother survey [104] that mainly focuses on the reason-sured by the disagreement of results obtained by multi-strategy for LLM-based task decomposition is im-ing abilities (e.g., arithmetic, commonsense, symbolicple sampling of the LLM. Zhang et al. [87] proposed perative. The extensive range and intricacy ofreasoning, logical, multi-modal) of LLMs, Qiao et. alautomatic CoT methods. They introduced question tasks render human-based or rule-based task de-Zelikman et al. [91] utilized the intermediate outputs summarized the studies that harness these reasoningclustering and demonstration sampling steps to auto-composition infeasible. While some studies havethat lead to the correct answer as the target to fine-abilities via advanced PE methods like chain-of-thoughmatically select the best demonstrations for the CoT explored the use of LLM prompting to gener-tune the LLM. Wang et al. [89] proposed an iterative and generated knowledge prompts. Additionally, sometemplate. ate intermediate questions or actions for specificprompting framework using a context-aware prompter. focused surveys cover specific topics like parameter-5.2.3 Augmenting P T tasks, a comprehensive strategy is currently lack-The prompter consists of a set of soft prompts that are efficient fine-tuning (PEFT) LLMs using PE methodsing. Different from refining P T which mainly focuses on finding prompts that generate better intermediate porating external tools, LLMs can address their external information, knowledge, tools, etc. in the for fine-tuning is a crucial objective. By incor-results, augmenting P T leverages the exploitation of • Enabling LLM to leverage tools without the needprepared for the encoder and decoder of the LLMs re-[105]. Different from the above-mentioned studies, we try to interpret existing PE methods from a communi-spectively. Taylor et al. [92] employed step-by-step so-enables the LM to output reasoning steps if required. lutions of scientific papers in the training corpus, which cation theory perspective.prompting. We present some examples in this field be-limitations in specialized domains or capabilities.5.3 Trends for Multiple Prompting Methodslow, for more details we refer the reader to the specific Previous studies have employed fine-tuning-basedEnsemble-based methods are easy to implement andsurvey [102]. Yang et al. [39] proposed a recursive re-approaches to train LLMs in utilizing web searchflexible to incorporate with various strategies, e.g. ex-Ye et al. [82] uses LLMs as the decomposer for table-based reasoning tasks. LLMs are used for both sub-table ex-traction and question decomposition. Press et al. [101] proposed Self-Ask which decomposes the original task by repeatedly asking LLM if follow-up questions are needed. Wu et al. [83] proposed to build an interactive chaining framework with several primitive operations of LLM to provide better transparency and controllability of using LLMs. prompting and revision (3R) framework for long story generation leveraging pre-defined outlines. In each step, the context of the current status and the outline of the story is provided to the prompt to ensure better con-tent coherence. Yang et al. [88] proposed to use more detailed outlines so that the story generation LLM can focus more on linguistic aspects. Information retrieved from other sources is also often used to augment P T . Yao et al. [103] gives the LLM access to information or other tools accessible through APIs. From the communication theory perspective, mul-tiple prompting methods evolved from the extension in the spatial domain (ensemble-based methods) into the temporal domain (mulit-turn), to better align the user's intention and LLM's capability by decomposing the user's request and leveraging external tools. 6 Discussion 5.2.2 Refining P Following the success of the few-shot chain-of-thoughts (CoT) prompting method, Kojima et al. [37] proposed a zero-shot CoT method that utilizes the fixed prompt 'Let's think step by step' to generate reason-ings. These intermediate results are then fused with the original question to get the final answer. To select more effective exemplars, various methods are proposed. Li et al. [84] uses LLMs to first generate a pseudo-QA pool, then a clustering method combined with similar-these silver labels serves as the selection criterion of ex-emplars. To further reduce the search complexity of panding input space and aggregating output space. However, this brings limited advantages for complex problems whose final answers are hard to obtain di-rectly, but rely heavily on the intermediate thinking steps. Therefore, multi-turn PE methods emerged. It essentially adjusts its input dynamically during the in-teraction based on the knowledge and feedback from the LLM or external tools. In this way, LLM can leverage more context and understand better the true ity to the question is adopted to dynamically select QA pairs from the generated QA pool as demonstration exemplars. Shum et al. [32] leveraged a high-quality from Wikipedia. Thoppilan et al. [34] taught the LLM intention of the user. Initially, specialized LLM are Researchers have proposed several surveys to re-to use search engines for knowledge retrieval. More trained to handle planning and solving specific sub-capitulate the rapid advancements in the field of PE broadly, Paranjape et al. [33] introduces a task library tasks, this not only introduces extra training effort but methods [7, 104, 105, 106, 107, 108]. To name a few, to enable the LLM using external tools. Schick et exemplar pool to obtain an exemplar distribution us-also constrains the generalization capability of LLM. Liu et. al proposed a comprehensive survey about ex-al. [40] trained the LLM to use various external tools ing a variance-reduced policy gradient estimator. Ye et With the increasing understanding ability and larger isting PE methods, which covers common aspects like via API. Shen et al. [41] utilized LLM as a central con-al. [85] employs self-consistency method[69] to generate preferred paradigm, which utilizes embedded knowl-pseudo-labels of an unlabeled dataset. The accuracy of troller to coordinate other models to solve tasks. input length of LLM, in-context learning becomes the template engineering, answering engineering, trainingvarious combinations, additional surrogate metrics areintroduced to estimate the accuracy. Diao et al. [86]
[7]fining P T aims to construct a better representation of P T based on the feedback from previous prompt-ing results.This is especially important for multi-step reasoning, where the quality of generated intermediate reasonings has a critical impact on the final answer.5.2.4 Optimizing θGeneral LMs are not optimized for producing intermediate rationals or decomposing a complex task or question.Before the era of LLMs, these tasks require specifically trained LMs.edge and the capability of LLM to handle various tasks via prompting.This paradigm soon dominated because of its efficiency and flexibility.There are two trends in multiple prompting engi-strategies, applications, and challenges[7].They reveal the development history of prompting learning and describe a set of mathematical notations that could summarize most of the existing studies.Furthermore, they consider prompt-based learning as a new paradigm</p>
<p>Language models are few-shot learners. Advances in neural information processing systems. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, 202033</p>
<p>OpenAI. Gpt-4 technical report. 2023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>The potential of gpt-4 as an ai-powered virtual assistant for surgeons specialized in joint arthroplasty. K Cheng, Z Li, C Li, R Xie, Q Guo, Y He, H Wu, Annals of Biomedical Engineering. 2023</p>
<p>Evaluating the feasibility of chatgpt in healthcare: an analysis of multiple clinical and research scenarios. M Cascella, J Montomoli, V Bellini, E Bignami, Journal of Medical Systems. 471332023</p>
<p>A review of chatgpt ai's impact on several business sectors. Partners. S George, George A , H , Universal International Innovation Journal. 20231</p>
<p>Neubig G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, ACM Computing Surveys. 5592023</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>F Petroni, T Rocktäschel, P Lewis, A Bakhtin, Y Wu, A H Miller, S Riedel, arXiv:1909.01066Language models as knowledge bases?. 2019arXiv preprint</p>
<p>Exploiting cloze questions for few shot text classification and natural language inference. T Schick, H Schütze, arXiv:2001.076762020arXiv preprint</p>
<p>How can we know what language models know? Transactions of the. Z Jiang, F F Xu, J Araki, G Neubig, 2020Association for Computational Linguistics8</p>
<p>Eliciting knowledge from language models with automatically generated prompts. T Shin, Y Razeghi, Logan Iv R L, Wallace E Singh, S Autoprompt, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. L Li, P Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>A Haviv, J Berant, Globerson A Bertese, arXiv:2103.05327Learning to speak to bert. 2021arXiv preprint</p>
<p>X Liu, Y Zheng, Z Du, M Ding, Y Qian, Z Yang, J Tang, arXiv:2103.10385Gpt understands, too. 2021arXiv preprint</p>
<p>Factual probing is [mask]: Learning vs. learning to recall. Z Zhong, D Friedman, D Chen, arXiv:2104.052402021arXiv preprint</p>
<p>Making pre-trained language models better few-shot learners. T Gao, A Fisch, D Chen, arXiv:2012.157232020arXiv preprint</p>
<p>Differentiable prompt makes pre-trained language models better fewshot learners. N Zhang, L Li, X Chen, S Deng, Z Bi, C Tan, F Huang, H Chen, arXiv:2108.131612021arXiv preprint</p>
<p>Ptr: Prompt tuning with rules for text classification. X Han, W Zhao, N Ding, Z Liu, M Sun, AI Open. 32022</p>
<p>The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Ppt: Pre-trained prompt tuning for few-shot learning. Y Gu, X Han, Z Liu, M Huang, arXiv:2109.043322021arXiv preprint</p>
<p>M Deng, J Wang, C P Hsieh, Y Wang, H Guo, T Shu, M Song, E P Xing, Z Hu, Rlprompt, arXiv:2205.12548Optimizing discrete text prompts with reinforcement learning. 2022arXiv preprint</p>
<p>Y Hou, H Dong, X Wang, Li B , Che W Metaprompting, arXiv:2209.11486Learning to learn better prompts. 2022arXiv preprint</p>
<p>Multitask prompt tuning enables parameter-efficient transfer learning. Z Wang, R Panda, L Karlinsky, R Feris, H Sun, Y Kim, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. W Yin, J Hay, D Roth, arXiv:1909.001612019arXiv preprint</p>
<p>Automatically identifying words that can serve as labels for few-shot text classification. T Schick, H Schmid, H Schütze, arXiv:2010.136412020arXiv preprint</p>
<p>Templatebased named entity recognition using bart. L Cui, Y Wu, J Liu, S Yang, Y Zhang, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>WARP: Word-level Adversarial ReProgramming. K Hambardzumyan, H Khachatrian, J May, 10.48550/arXiv.2101.00121</p>
<p>Y Chen, Y Liu, L Dong, S Wang, C Zhu, M Zeng, Y Zhang, Adaprompt, arXiv:2202.04824Adaptive model training for prompt-based nlp. 2022arXiv preprint</p>
<p>Accurate and prompt answering framework based on customer reviews and question-answer pairs. Expert Systems with Applications. E Kim, H Yoon, J Lee, M Kim, 2022203117405</p>
<p>Unsupervised Question Decomposition for Question Answering. E Perez, P Lewis, Yih W T, K Cho, D Kiela, 10.18653/v1/2020.emnlp-main.713Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</p>
<p>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. K Shum, S Diao, T Zhang, 10.48550/arXiv.2302.12822</p>
<p>Automatic multi-step reasoning and tool-use for large language models. B Paranjape, S Lundberg, S Singh, H Hajishirzi, L Zettlemoyer, M T Ribeiro, Art, 10.48550/arXiv.2303.09014</p>
<p>R Thoppilan, De Freitas, D Hall, J Shazeer, N Kulshreshtha, A Cheng, H T , Jin A Bos, T Baker, L Du, Y , arXiv:2201.08239Language models for dialog applications. 2022arXiv preprint</p>
<p>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, A Herzog, D Ho, J Hsu, J Ibarz, B Ichter, A Irpan, E Jang, R J Ruano, K Jeffrey, S Jesmonth, N J Joshi, R Julian, D Kalashnikov, Y Kuang, K H Lee, S Levine, Y Lu, L Luu, C Parada, P Pastor, J Quiambao, K Rao, J Rettinghouse, D Reyes, P Sermanet, N Sievers, C Tan, A Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, S Xu, M Yan, A Zeng, 10.48550/arXiv.2204.01691</p>
<p>Selfconsistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, Chi E Narang, S Chowdhery, A Zhou, D , arXiv:2203.111712022arXiv preprint</p>
<p>Large Language Models are Zero-Shot Reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, 10.48550/arXiv.2205.11916</p>
<p>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, Chi E , 10.48550/arXiv.2205.10625</p>
<p>Re3: Generating Longer Stories With Recursive Reprompting and Revision. K Yang, Y Tian, N Peng, D Klein, 10.48550/arXiv.2210.06774</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, February 2023</p>
<p>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. Y Shen, K Song, X Tan, D Li, W Lu, Y Zhuang, 10.48550/arXiv.2303.17580April 2023</p>
<p>Handbook of communication models, perspectives, strategies. U Narula, 2006Atlantic Publishers &amp; Dist</p>
<p>A dictionary of media and communication. D Chandler, R Munday, 2011OUP Oxford</p>
<p>Theories and models of communication. P Cobley, P J Schulz, 2013Walter de Gruyter1</p>
<p>Dynamic social impact: The creation of culture by communication. B Latané, Journal of communication. 4641996</p>
<p>From the standpoint (s) of traditionally muted groups: Explicating a co-cultural communication theoretical model. M Orbe, Communication Theory. 811998</p>
<p>Negative reactions to depressive behaviors: a communication theories analysis. C Segrin, L Y Abramson, Journal of abnormal psychology. 10346551994</p>
<p>A mathematical theory of communication. The Bell system technical journal. C Shannon, 194827</p>
<p>The process and effects of mass communication. W E Schram, 1954</p>
<p>Elements of information theory. T Cover, 1999John Wiley &amp; Sons</p>
<p>It's not just size that matters: Small language models are also few-shot learners. T Schick, H Schütze, arXiv:2009.071182020arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Y Zhou, Y Zhao, I Shumailov, R Mullins, Y Gal, arXiv:2304.03609Revisiting automated prompting: Are we actually doing better?. 2023arXiv preprint</p>
<p>Cutting down on prompts and parameters: Simple few-shot learning with language models. Iv R L Logan, I Balažević, E Wallace, F Petroni, S Singh, S Riedel, arXiv:2106.133532021arXiv preprint</p>
<p>Evaluating generated text as text generation. W Yuan, G Neubig, P Liu, Bartscore, Advances in Neural Information Processing Systems. 202134</p>
<p>Pada: A prompt-based autoregressive approach for adaptation to unseen domains. E Ben-David, N Oved, R Reichart, arXiv:2102.122062021arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, arXiv:2211.019102022arXiv preprint</p>
<p>Data augmentation approaches in natural language processing: A survey. B Li, Y Hou, Che W , AI Open. 32022</p>
<p>Commonsense knowledge mining from pretrained models. J Davison, J Feldman, A Rush, Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing. the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processingEMNLP-IJCNLP2019</p>
<p>Dynamic prompting: A unified framework for prompt tuning. X Yang, W Cheng, X Zhao, L Petzold, H Chen, arXiv:2303.029092023arXiv preprint</p>
<p>Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference. T Schick, H Schütze, 10.48550/arXiv.2001.07676</p>
<p>Making pre-trained language models better few-shot learners. T Gao, A Fisch, D Chen, Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL-IJCNLP 2021. 2021</p>
<p>How Can We Know What Language Models Know?. Z Jiang, F F Xu, J Araki, G Neubig, 10.48550/arXiv.1911.12543</p>
<p>Adaptive model training for prompt-based NLP. Y Chen, Y Liu, L Dong, S Wang, C Zhu, M Zeng, Y Zhang, Adaprompt, Findings of the Association for Computational Linguistics: EMNLP 2022. December 2022</p>
<p>Z Jiang, A Anastasopoulos, J Araki, H Ding, G Neubig, X-Factr, arXiv:2010.06189Multilingual factual knowledge retrieval from pretrained language models. 2020arXiv preprint</p>
<p>Learning continuous hierarchies in the lorentz model of hyperbolic geometry. M Nickel, D Kiela, International conference on machine learning. 2018</p>
<p>Few-shot slot tagging with collapsed dependency transfer and label-enhanced task-adaptive projection network. Y Hou, Che W Lai, Y Zhou, Z Liu, Y Liu, H Liu, T , Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Learning How to Ask: Querying LMs with Mixtures of Soft Prompts. G Qin, J Eisner, 10.18653/v1/2021.naacl-main.410Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. X Wang, J Wei, D Schuurmans, Q Le, Chi E Narang, S Chowdhery, A Zhou, D , 10.48550/arXiv.2203.11171</p>
<p>Solving Quantitative Reasoning Problems with Language Models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, Y Wu, B Neyshabur, G Gur-Ari, V Misra, June 2022</p>
<p>Rationale-Augmented Ensembles in Language Models. X Wang, J Wei, D Schuurmans, Q Le, Chi E Zhou, D , 10.48550/arXiv.2207.00747</p>
<p>On the Advance of Making Language Models Better Reasoners. Y Li, Z Lin, S Zhang, Q Fu, B Chen, J G Lou, Chen W , </p>
<p>Complexity-Based Prompting for Multi-Step Reasoning. Y Fu, H Peng, A Sabharwal, P Clark, T Khot, 10.48550/arXiv.2210.00720</p>
<p>It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners. T Schick, H Schütze, 10.48550/arXiv.2009.07118</p>
<p>Few-Shot Text Generation with Pattern-Exploiting Training. T Schick, H Schütze, 10.48550/arXiv.2012.11926</p>
<p>Multi-hop Reading Comprehension through Question Decomposition and Rescoring. S Min, V Zhong, L Zettlemoyer, H Hajishirzi, 10.18653/v1/P19-1613Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics</p>
<p>Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models. T Khot, D Khashabi, K Richardson, P Clark, A Sabharwal, 10.18653/v1/2021.naacl-main.99Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>Successive Prompting for Decomposing Complex Questions. D Dua, S Gupta, S Singh, M Gardner, 10.48550/arXiv.2212.04092December 2022</p>
<p>Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning. A Creswell, M Shanahan, I Higgins, 10.48550/arXiv.2205.09712May 2022</p>
<p>Ask Me Anything: A simple strategy for prompting language models. S Arora, A Narayan, M F Chen, L Orr, N Guha, K Bhatia, I Chami, F Sala, C Ré, 10.48550/arXiv.2210.02441November 2022</p>
<p>Decomposed Prompting: A Modular Approach for Solving Complex Tasks. T Khot, H Trivedi, M Finlayson, Y Fu, K Richardson, P Clark, A Sabharwal, </p>
<p>Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. Y Ye, B Hui, M Yang, B Li, F Huang, Y Li, 10.48550/arXiv.2301.13808</p>
<p>T Wu, M Terry, C J Cai, Chains, 10.48550/arXiv.2110.01691Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. </p>
<p>Self-Prompting Large Language Models for Open-Domain QA. J Li, Z Zhang, H Zhao, 10.48550/arXiv.2212.08635</p>
<p>Explanation Selection Using Unlabeled Data for In-Context Learning. X Ye, G Durrett, 10.48550/arXiv.2302.04813</p>
<p>Active Prompting with Chain-of-Thought for Large Language Models. S Diao, P Wang, Y Lin, T Zhang, 10.48550/arXiv.2302.12246</p>
<p>Automatic Chain of Thought Prompting in Large Language Models. Z Zhang, A Zhang, M Li, A Smola, 10.48550/arXiv.2210.03493</p>
<p>DOC: Improving Long Story Coherence With Detailed Outline Control. K Yang, D Klein, N Peng, Y Tian, </p>
<p>Iteratively Prompt Pretrained Language Models for Chain of Thought. B Wang, X Deng, H Sun, 10.48550/arXiv.2203.08383</p>
<p>Show Your Work: Scratchpads for Intermediate Computation with Language Models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, C Sutton, A Odena, 10.48550/arXiv.2112.00114November 2021</p>
<p>. E Zelikman, Y Wu, J Mu, N D Goodman, Star, Bootstrapping Reasoning With Reasoning</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, Galactica, A Large Language Model for Science. November 2022</p>
<p>Stacked Generalization: When does it work?. M Ting, I Witten, </p>
<p>Ensembling neural networks: Many could be better than all. H Zhou, J Wu, W Tang, 10.1016/S0004-3702(02)00190-X137</p>
<p>Generalized Minimum Bayes Risk System Combination. K Duh, K Sudoh, X Wu, H Tsukada, M Nagata, Proceedings of 5th International Joint Conference on Natural Language Processing. 5th International Joint Conference on Natural Language Processing</p>
<p>Large Language Models are Better Reasoners with Self-Verification. Y Weng, M Zhu, F Xia, B Li, S He, K Liu, J Zhao, May 2023</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, 10.48550/arXiv.2305.10601May 2023</p>
<p>Few-shot text generation with natural language instructions. T Schick, H Schütze, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models. J Yang, H Jiang, Q Yin, D Zhang, B Yin, Yang D Seqzero, 10.48550/arXiv.2205.07381May 2022</p>
<p>Compositional Semantic Parsing with Large Language Models. A Drozdov, N Schärli, E Akyürek, N Scales, X Song, X Chen, O Bousquet, D Zhou, </p>
<p>Measuring and Narrowing the Compositionality Gap in Language Models. O Press, M Zhang, S Min, L Schmidt, N A Smith, M Lewis, 10.48550/arXiv.2210.03350</p>
<p>Augmented Language Models: A Survey. G Mialon, R Dessì, M Lomeli, C Nalmpantis, R Pasunuru, R Raileanu, B Rozière, T Schick, J Dwivedi-Yu, A Celikyilmaz, E Grave, Y Lecun, T Scialom, 10.48550/arXiv.2302.07842</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, 10.48550/arXiv.2210.03629</p>
<p>Reasoning with Language Model Prompting: A Survey. S Qiao, Y Ou, N Zhang, X Chen, Y Yao, S Deng, C Tan, F Huang, H Chen, 10.48550/arXiv.2212.09597</p>
<p>Scaling down to scale up: A guide to parameter-efficient fine-tuning. V Lialin, V Deshpande, A Rumshisky, arXiv:2303.156472023arXiv preprint</p>
<p>W Zhao, X Zhou, K Li, J Tang, T Wang, X Hou, Y Min, Y Zhang, B Zhang, J Dong, Z , arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>A survey for in-context learning. Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, Z Sui, arXiv:2301.002342022arXiv preprint</p>
<p>Is prompt all you need? no. a comprehensive and broader view of instruction learning. R Lou, K Zhang, W Yin, arXiv:2303.104752023arXiv preprint</p>
<p>Adapting language models for zero-shot learning by metatuning on dataset and prompt collections. R Zhong, K Lee, Z Zhang, D Klein, arXiv:2104.046702021arXiv preprint</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. L Reynolds, K Mcdonell, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. 2021</p>
<p>Few-shot text-to-sql translation using structure and content prompt learning. S Madden, 2023</p>
<p>Deep learning with differential privacy. M Abadi, A Chu, I Goodfellow, H B Mcmahan, I Mironov, K Talwar, L Zhang, Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. the 2016 ACM SIGSAC conference on computer and communications security2016</p>
<p>A fully homomorphic encryption scheme. C Gentry, 2009Stanford university</p>
<p>Federated machine learning: Concept and applications. Q Yang, Y Liu, T Chen, Y Tong, ACM Transactions on Intelligent Systems and Technology (TIST). 1022019</p>
<p>Interaction and feedback in a spoken language system: A theoretical framework. S E Brennan, E Hulteen, 19958Knowledge-based systems</p>
<p>Reframing Human-AI Collaboration for Generating Free-Text Explanations. S Wiegreffe, J Hessel, S Swayamdipta, M Riedl, Y Choi, 10.18653/v1/2022.naacl-main.47Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language TechnologiesJuly 2022</p>            </div>
        </div>

    </div>
</body>
</html>
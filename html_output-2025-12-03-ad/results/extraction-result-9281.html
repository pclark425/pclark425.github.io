<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9281 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9281</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9281</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-0adec918885dff698acf359988ed79a543157f80</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0adec918885dff698acf359988ed79a543157f80" target="_blank">Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work uses the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, it identifies performant prompts and yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.</p>
                <p><strong>Paper Abstract:</strong> When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are “fantastic” and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9281.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9281.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Order Permutations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training-sample Prompt Order Permutations (In-context Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The order in which few-shot training examples are concatenated into a prompt (different permutations) strongly affects LLM classification performance: some orders yield near-supervised accuracy while others yield near-random performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (GPT2-family) / GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.1B, 0.3B, 0.8B, 1.5B, 2.7B, 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentence-level text classification (eleven datasets: SST-2, SST-5, DBPedia, MR, CR, MPQA, Subj, TREC, AGNews, RTE, CB)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard text classification benchmarks across sentiment, subjectivity, topic, and textual entailment tasks, evaluated by accuracy on validation subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context learning prompts created by (i) applying a template to each (x,y) pair (linearization) and (ii) concatenating n training samples in a particular order as context; common settings: 4-shot for most datasets, 1-shot for DBPedia, 2-shot for AGNews. Prompts vary only by the permutation of the examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Alternative permutations of the same set of examples (baseline = average over all permutations); oracle = best permutations selected by validation; calibrated outputs also compared in some tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (SST-2, GPT-2 1.5B): baseline (mean over 24 permutations) accuracy = 66.8% (std 10.8); best (oracle top permutations) = 86.1% (std 4.5). For GPT-3 175B baseline = 93.9% (std 0.6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Same prompt examples but different orders produced wide ranges: e.g., for SST-2 some permutations reach >85% while others approach random (~50%). GlobalE-selected prompt (entropy probing) increased GPT-2 1.5B SST-2 to 81.8% (std 3.9).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large: example absolute difference for GPT-2 1.5B on SST-2: +19.3 percentage points between baseline mean (66.8%) and oracle (86.1%); selecting via GlobalE gave +15.0 pp over baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors find order sensitivity is pervasive and not explained by specific sample subsets; degenerate (bad) permutations often cause highly unbalanced predicted label distributions. Hypotheses: the concatenation order changes how the autoregressive LM conditions on previous context leading to distributional biases and overconfident predictions for some classes.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>4-shot for most datasets (1-shot DBPedia, 2-shot AGNews) with up to 24 permutations per sampled training set; experiments over 5 random sample sets (120 runs total); evaluation on 256-sample validation subset; metrics reported as accuracy with stdev across sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9281.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9281.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Permutation Transferability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-transferability of performant prompt permutations across model sizes and initialisations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Good example-order permutations for one model or model size are not guaranteed to be good for another; correlations of permutation performance across models are low.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family (different sizes) and GPT-3 family</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>examples: GPT-2 Large (0.8B), GPT-2 XL (1.5B), GPT-3 (2.7B, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (representative), other text classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary sentiment classification and other classification tasks used to test whether a permutation that is good for one model remains so for another.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context prompts differing only in example ordering; all permutations evaluated per model and pairwise performance ranks compared.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Cross-model comparison of the same set of permutations to compute Spearman rank correlations of permutation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Observed low pairwise correlations; example: correlation between permutation rankings for GPT-3 175B and 2.7B is ~0.05. Example explicit performance change: a specific permutation dropped from 88.7% to 51.6% when moving from GPT2-XL (1.5B) to GPT2-Large (0.8B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Permutation that is high-performing on one model can be low-performing on another; no reliable transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large and model-dependent; single-permutation example: -37.1 percentage points when moving models (88.7% to 51.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Differences likely due to model-scale-dependent conditional behaviours: different parameterisations lead the LM to condition on context differently, so order-induced biases are model-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>All permutations of 4 examples (24 permutations) evaluated per model; pairwise Spearman rank correlations computed across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9281.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9281.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adding Training Samples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of increasing number of in-context training examples on order sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Increasing the number of few-shot examples increases mean performance but does not reliably reduce variance due to prompt order; order sensitivity persists and can even increase.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.1B to 1.5B (GPT-2 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (exploratory)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Study of how varying the number of in-context training examples affects average performance and permutation-induced variance.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Varying k-shot prompts (k increased), sampling up to 24 permutations of the selected examples for each k.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison across k (different numbers of training samples) while keeping permutation sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mean accuracy increases with more examples, but variance across permutations remains high; no reliable collapse of variance with more examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Although mean goes up, some permutations still yield near-random performance even with larger k.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>No consistent reduction in variance; effect size variable and task/model dependent (no single numeric effect reported).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Order sensitivity is likely a fundamental issue of in-context learning and not solved merely by adding more examples; the way autoregressive conditioning interacts with sequence order remains critical.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used GPT-2 family; for each k, sampled up to 24 distinct orderings and measured accuracy distribution (visualised in Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9281.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9281.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Label Ordering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training-label Pattern (Positive/Negative) Ordering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The order of labels (e.g., patterns of positive/negative examples) in the few-shot prompt does not provide consistent gains across models; label-ordering effects are model-dependent and seemingly random.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family and GPT-3 family</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>multiple (various GPT-2/GPT-3 sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Binary label classification (e.g., SST-2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate whether permutations constrained to different label patterns (e.g., PNNP, PNPN, ...) consistently influence performance across models.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompts where only the sequence of labels across the provided examples is permuted (six distinct label patterns for 4-shot balanced set).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Across different model sizes: compare performance ranks by label pattern and compute correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No consistent label-ordering that is performant across model sizes; performance correlation across models for label patterns is low (visualised in Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Label ordering effect is inconsistent across models; a pattern beneficial for one model often not beneficial for another.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not quantified as a single value; effect exists but is model- and task-dependent and lacks transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Label-ordering interacts with model-specific conditioning; there is no universal advantageous label sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used all patterns of the 4-example permutations that correspond to the same label-sequence types (6 unique label patterns); computed pairwise Spearman correlations across models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9281.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9281.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calibration (Zhao et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Output Calibration of Model Probabilities (per Zhao et al. 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying post-hoc calibration to model outputs increases mean accuracy for many permutations but does not remove high variance across prompt permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Calibrate before use: Improving few-shot performance of language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 XL (1.5B) (example reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (2-shot example in figure)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Calibrate predicted label distributions (reduce prior bias) to improve few-shot classification.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompt (2-shot in the calibration figure) with post-hoc calibration applied to model prediction probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Raw uncalibrated outputs vs calibrated outputs across all permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Calibration raises mean performance substantially for some settings, but variance across permutations remains high (visualised in Figure 6, right). Exact numeric example not tabulated in main tables but qualitatively shows improved mean yet preserved spread.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Calibration improves average accuracy but does not eliminate order sensitivity; variance remains.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Improves mean performance (magnitude dataset/model dependent) but does not reduce permutation-induced variance (no single effect-size reported).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Calibration corrects for model output priors/unbalanced predictive biases but does not change how the model conditions on the sequence order of examples; hence variance persists.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Calibration technique referenced from Zhao et al. (2021); applied to GPT-2 XL 2-shot experiments and plotted calibrated vs uncalibrated permutation performances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9281.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9281.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entropy-based Probing (GlobalE / LocalE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probing-set Construction plus Entropy Ranking (Global Entropy and Local Entropy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors construct artificial unlabeled probing sets by sampling generations from the LM under each candidate prompt ordering and rank prompts by entropy-based metrics (GlobalE over label distribution, LocalE average per-example entropy) to select performant prompt orders without labeled dev data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family and GPT-3 family (used both for generation and evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.1B, 0.3B, 0.8B, 1.5B, 2.7B, 175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Eleven text classification datasets (SST-2, SST-5, DBPedia, MR, CR, MPQA, Subj, TREC, AGNews, RTE, CB)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select best few-shot prompt permutations without labeled development data using a generated probing set and entropy metrics, then evaluate classification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context prompts (same set of candidate permutations). For each permutation c_m, generate probing sequences g_m ~ P(.|c_m) from the LM, discard generated labels, build probing set D, compute GlobalE (entropy over predicted label distribution on D) and LocalE (average per-example prediction entropy on D), rank permutations, select top-K (K=4) permutations for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline (average over all permutations), Oracle (select top-by actual validation performance), split-training-set selection (use half training as dev), calibrated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GlobalE yields on average a 13% relative improvement across the eleven tasks compared to baseline; LocalE yields ~9.6% relative improvement. Example numbers from Table 2: GPT-2 0.8B SST-2 baseline 74.5% -> GlobalE 84.8% (std 4.1), LocalE 81.1% (std 5.5). GPT-3 2.7B SST-2 baseline 78.0% -> GlobalE 80.2%, LocalE 81.0%. GPT-3 175B already high baseline 93.9%; GlobalE/LocalE matched or slightly improved.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GlobalE generally outperforms LocalE and the baseline averaging over permutations; both outperform splitting the tiny training set for dev-selection. Example (SST-2, GPT-2 0.8B): baseline 74.5%, split-training-set selection 75.1%, LocalE 81.1%, GlobalE 84.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Representative effect sizes: +10–15 percentage points absolute improvement over baseline for mid-sized GPT-2 models on SST-2; average across tasks: GlobalE ~+13% relative improvement vs baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>High-entropy (balanced) predicted label distributions on an LM-generated probing set indicate prompts that avoid degenerate, overconfident behaviour and thus generalise better; LocalE flags prompts that avoid overconfident per-example predictions, while GlobalE favours overall label balance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Probing generation: max length 128, sampling temperature t=2, block n-gram repetition blocking; for each candidate permutation generate probes, discard generated labels, compute GlobalE/LocalE; rank permutations and select top-K (K=4) for final evaluation. Experiments used 24 permutations per sample set and 5 different random sample sets (2 sets for GPT-3 175B due to cost).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9281.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9281.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template Variation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Different Prompt Templates on Permutation Selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different textual templates for linearising (x,y) pairs affect baseline performance but entropy-based probing (GlobalE/LocalE) consistently improves over baseline across templates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.1B, 0.3B, 0.8B, 1.5B (examples in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (template sensitivity study)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare four different templates (different phrasing/label mappings) for representing training examples in prompts and test whether the entropy probing selection still improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Four templates (e.g., 'Review: {Sentence} Sentiment: {Label}', 'Input: {Sentence} Prediction: {Label}', alternate label mappings good/bad vs positive/negative, etc.) with same few-shot concatenation and permutation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline using all permutations per template vs top-k selected by LocalE/GlobalE per template.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Across templates GlobalE/LocalE consistently improved accuracy over baseline. Example (GPT-2 0.8B SST-2): Template1 baseline 74.5% -> LocalE 81.1% -> GlobalE 84.8%; Template2 baseline 66.6% -> LocalE 80.0% -> GlobalE 80.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Improvements persisted across all four templates tested; relative ordering of best template varied but probing reliably improved each template's mean and reduced variance.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Template-dependent; improvements of +6–18 absolute percentage points in examples shown (varies by base model/template).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Entropy probing is template-agnostic because it evaluates prompt-induced model behaviour (entropy/balance) on a generated probing set rather than relying on surface phrasing alone.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Four template variants for SST-2 (Table 4); experiments run per template with same permutation sampling and K=4 selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9281.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9281.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Split-training Dev vs Probing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using held-out subset of few-shot examples as dev vs. generated probing for prompt selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using a held-out split of the tiny few-shot training set for dev-selection improves over naive baseline but is outperformed by entropy-based probing using model-generated synthetic probes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.1B, 0.3B, 0.8B, 1.5B (examples in Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare selecting top permutations by measured validation performance on a split of the real few-shot training data vs selecting top permutations by entropy probing on synthetic probing set.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>4-shot prompt: either split into 2+2 and use held-out 2-shot as dev to rank permutations, or use probing set generated by LM and rank by GlobalE/LocalE.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline (average over permutations) vs split-training-set dev selection vs GlobalE/LocalE probing selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (SST-2): GPT-2 0.8B baseline 74.5% -> split-training-set selection 75.1% -> LocalE 81.1% -> GlobalE 84.8%. For other sizes similar pattern: probing outperformed split-set selection consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Probing (GlobalE/LocalE) consistently outperforms selection based on splitting tiny training set for dev; split-training selection improves slightly over baseline but less than probing.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example effect (GPT-2 0.8B SST-2): GlobalE +10.3 pp over baseline and +9.7 pp over split-training-set selection.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Splitting an already tiny few-shot set reduces the signal for both training and validation and is suboptimal; generated probing sets provide more diverse synthetic inputs for robust entropy-based ranking without needing labeled held-out data.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>4-shot prompts split in half for split-training dev selection; probing uses generated samples as per the probing methodology; results averaged across 5 random sample sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9281.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9281.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentence-pair Task Limits</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Difficulties of Small Models on Sentence-Pair Tasks (RTE, CB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For smaller model sizes, even selecting performant permutations via probing may not improve performance above random baselines on sentence-pair entailment datasets (RTE, CB), indicating absence of useful decision boundary in small models for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family (smaller sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.1B–1.5B (representative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sentence-pair tasks: RTE, CB</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Textual entailment benchmarks requiring reasoning over premise-hypothesis pairs; evaluated by accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot in-context concatenation of pair templates; permutation probing and selection applied as in other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline permutations vs GlobalE/LocalE selection vs larger model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>For GPT-2 small/medium, performance on CB and RTE often close to random/majority (e.g., many baseline entries ~50%); probing gives only minimal gains still near random. At larger scales (GPT-3 175B) selection yields meaningful gains (GlobalE improves CB by 4.9% for GPT-3 175B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Small models: baseline ~random and probing minimal change; Large model: measurable improvement with probing.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Small or negligible for small models; measurable (several percentage points) for large models (e.g., +4.9% for GPT-3 175B on CB via GlobalE).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>For some tasks/models there may be no prompt that provides the model with sufficient internal capability to solve the task — i.e., the model lacks the necessary underlying classification ability, so prompt optimisation cannot help.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CB and RTE evaluated with the same probe-selection pipeline; observed that only larger models show improvements above random/majority baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 2)</em></li>
                <li>Making pre-trained language models better few-shot learners <em>(Rating: 2)</em></li>
                <li>What makes good in-context examples for gpt-3? <em>(Rating: 2)</em></li>
                <li>Autoprompt: Eliciting knowledge from language models with automatically generated prompts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9281",
    "paper_id": "paper-0adec918885dff698acf359988ed79a543157f80",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Prompt Order Permutations",
            "name_full": "Training-sample Prompt Order Permutations (In-context Learning)",
            "brief_description": "The order in which few-shot training examples are concatenated into a prompt (different permutations) strongly affects LLM classification performance: some orders yield near-supervised accuracy while others yield near-random performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (GPT2-family) / GPT-3",
            "model_size": "0.1B, 0.3B, 0.8B, 1.5B, 2.7B, 175B",
            "task_name": "Sentence-level text classification (eleven datasets: SST-2, SST-5, DBPedia, MR, CR, MPQA, Subj, TREC, AGNews, RTE, CB)",
            "task_description": "Standard text classification benchmarks across sentiment, subjectivity, topic, and textual entailment tasks, evaluated by accuracy on validation subsets.",
            "presentation_format": "Few-shot in-context learning prompts created by (i) applying a template to each (x,y) pair (linearization) and (ii) concatenating n training samples in a particular order as context; common settings: 4-shot for most datasets, 1-shot for DBPedia, 2-shot for AGNews. Prompts vary only by the permutation of the examples.",
            "comparison_format": "Alternative permutations of the same set of examples (baseline = average over all permutations); oracle = best permutations selected by validation; calibrated outputs also compared in some tests.",
            "performance": "Example (SST-2, GPT-2 1.5B): baseline (mean over 24 permutations) accuracy = 66.8% (std 10.8); best (oracle top permutations) = 86.1% (std 4.5). For GPT-3 175B baseline = 93.9% (std 0.6).",
            "performance_comparison": "Same prompt examples but different orders produced wide ranges: e.g., for SST-2 some permutations reach &gt;85% while others approach random (~50%). GlobalE-selected prompt (entropy probing) increased GPT-2 1.5B SST-2 to 81.8% (std 3.9).",
            "format_effect_size": "Large: example absolute difference for GPT-2 1.5B on SST-2: +19.3 percentage points between baseline mean (66.8%) and oracle (86.1%); selecting via GlobalE gave +15.0 pp over baseline.",
            "explanation_or_hypothesis": "Authors find order sensitivity is pervasive and not explained by specific sample subsets; degenerate (bad) permutations often cause highly unbalanced predicted label distributions. Hypotheses: the concatenation order changes how the autoregressive LM conditions on previous context leading to distributional biases and overconfident predictions for some classes.",
            "null_or_negative_result": false,
            "experimental_details": "4-shot for most datasets (1-shot DBPedia, 2-shot AGNews) with up to 24 permutations per sampled training set; experiments over 5 random sample sets (120 runs total); evaluation on 256-sample validation subset; metrics reported as accuracy with stdev across sets.",
            "uuid": "e9281.0",
            "source_info": {
                "paper_title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Permutation Transferability",
            "name_full": "Non-transferability of performant prompt permutations across model sizes and initialisations",
            "brief_description": "Good example-order permutations for one model or model size are not guaranteed to be good for another; correlations of permutation performance across models are low.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 family (different sizes) and GPT-3 family",
            "model_size": "examples: GPT-2 Large (0.8B), GPT-2 XL (1.5B), GPT-3 (2.7B, 175B)",
            "task_name": "SST-2 (representative), other text classification tasks",
            "task_description": "Binary sentiment classification and other classification tasks used to test whether a permutation that is good for one model remains so for another.",
            "presentation_format": "Few-shot in-context prompts differing only in example ordering; all permutations evaluated per model and pairwise performance ranks compared.",
            "comparison_format": "Cross-model comparison of the same set of permutations to compute Spearman rank correlations of permutation performance.",
            "performance": "Observed low pairwise correlations; example: correlation between permutation rankings for GPT-3 175B and 2.7B is ~0.05. Example explicit performance change: a specific permutation dropped from 88.7% to 51.6% when moving from GPT2-XL (1.5B) to GPT2-Large (0.8B).",
            "performance_comparison": "Permutation that is high-performing on one model can be low-performing on another; no reliable transfer.",
            "format_effect_size": "Large and model-dependent; single-permutation example: -37.1 percentage points when moving models (88.7% to 51.6%).",
            "explanation_or_hypothesis": "Differences likely due to model-scale-dependent conditional behaviours: different parameterisations lead the LM to condition on context differently, so order-induced biases are model-specific.",
            "null_or_negative_result": false,
            "experimental_details": "All permutations of 4 examples (24 permutations) evaluated per model; pairwise Spearman rank correlations computed across model sizes.",
            "uuid": "e9281.1",
            "source_info": {
                "paper_title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Adding Training Samples",
            "name_full": "Effect of increasing number of in-context training examples on order sensitivity",
            "brief_description": "Increasing the number of few-shot examples increases mean performance but does not reliably reduce variance due to prompt order; order sensitivity persists and can even increase.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 family",
            "model_size": "0.1B to 1.5B (GPT-2 variants)",
            "task_name": "SST-2 (exploratory)",
            "task_description": "Study of how varying the number of in-context training examples affects average performance and permutation-induced variance.",
            "presentation_format": "Varying k-shot prompts (k increased), sampling up to 24 permutations of the selected examples for each k.",
            "comparison_format": "Comparison across k (different numbers of training samples) while keeping permutation sampling.",
            "performance": "Mean accuracy increases with more examples, but variance across permutations remains high; no reliable collapse of variance with more examples.",
            "performance_comparison": "Although mean goes up, some permutations still yield near-random performance even with larger k.",
            "format_effect_size": "No consistent reduction in variance; effect size variable and task/model dependent (no single numeric effect reported).",
            "explanation_or_hypothesis": "Order sensitivity is likely a fundamental issue of in-context learning and not solved merely by adding more examples; the way autoregressive conditioning interacts with sequence order remains critical.",
            "null_or_negative_result": false,
            "experimental_details": "Used GPT-2 family; for each k, sampled up to 24 distinct orderings and measured accuracy distribution (visualised in Figure 3).",
            "uuid": "e9281.2",
            "source_info": {
                "paper_title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Label Ordering",
            "name_full": "Training-label Pattern (Positive/Negative) Ordering",
            "brief_description": "The order of labels (e.g., patterns of positive/negative examples) in the few-shot prompt does not provide consistent gains across models; label-ordering effects are model-dependent and seemingly random.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 family and GPT-3 family",
            "model_size": "multiple (various GPT-2/GPT-3 sizes)",
            "task_name": "Binary label classification (e.g., SST-2)",
            "task_description": "Evaluate whether permutations constrained to different label patterns (e.g., PNNP, PNPN, ...) consistently influence performance across models.",
            "presentation_format": "Few-shot prompts where only the sequence of labels across the provided examples is permuted (six distinct label patterns for 4-shot balanced set).",
            "comparison_format": "Across different model sizes: compare performance ranks by label pattern and compute correlations.",
            "performance": "No consistent label-ordering that is performant across model sizes; performance correlation across models for label patterns is low (visualised in Figure 5).",
            "performance_comparison": "Label ordering effect is inconsistent across models; a pattern beneficial for one model often not beneficial for another.",
            "format_effect_size": "Not quantified as a single value; effect exists but is model- and task-dependent and lacks transferability.",
            "explanation_or_hypothesis": "Label-ordering interacts with model-specific conditioning; there is no universal advantageous label sequence.",
            "null_or_negative_result": true,
            "experimental_details": "Used all patterns of the 4-example permutations that correspond to the same label-sequence types (6 unique label patterns); computed pairwise Spearman correlations across models.",
            "uuid": "e9281.3",
            "source_info": {
                "paper_title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Calibration (Zhao et al.)",
            "name_full": "Output Calibration of Model Probabilities (per Zhao et al. 2021)",
            "brief_description": "Applying post-hoc calibration to model outputs increases mean accuracy for many permutations but does not remove high variance across prompt permutations.",
            "citation_title": "Calibrate before use: Improving few-shot performance of language models",
            "mention_or_use": "use",
            "model_name": "GPT-2 XL (1.5B) (example reported)",
            "model_size": "1.5B",
            "task_name": "SST-2 (2-shot example in figure)",
            "task_description": "Calibrate predicted label distributions (reduce prior bias) to improve few-shot classification.",
            "presentation_format": "Few-shot prompt (2-shot in the calibration figure) with post-hoc calibration applied to model prediction probabilities.",
            "comparison_format": "Raw uncalibrated outputs vs calibrated outputs across all permutations.",
            "performance": "Calibration raises mean performance substantially for some settings, but variance across permutations remains high (visualised in Figure 6, right). Exact numeric example not tabulated in main tables but qualitatively shows improved mean yet preserved spread.",
            "performance_comparison": "Calibration improves average accuracy but does not eliminate order sensitivity; variance remains.",
            "format_effect_size": "Improves mean performance (magnitude dataset/model dependent) but does not reduce permutation-induced variance (no single effect-size reported).",
            "explanation_or_hypothesis": "Calibration corrects for model output priors/unbalanced predictive biases but does not change how the model conditions on the sequence order of examples; hence variance persists.",
            "null_or_negative_result": false,
            "experimental_details": "Calibration technique referenced from Zhao et al. (2021); applied to GPT-2 XL 2-shot experiments and plotted calibrated vs uncalibrated permutation performances.",
            "uuid": "e9281.4",
            "source_info": {
                "paper_title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Entropy-based Probing (GlobalE / LocalE)",
            "name_full": "Probing-set Construction plus Entropy Ranking (Global Entropy and Local Entropy)",
            "brief_description": "Authors construct artificial unlabeled probing sets by sampling generations from the LM under each candidate prompt ordering and rank prompts by entropy-based metrics (GlobalE over label distribution, LocalE average per-example entropy) to select performant prompt orders without labeled dev data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 family and GPT-3 family (used both for generation and evaluation)",
            "model_size": "0.1B, 0.3B, 0.8B, 1.5B, 2.7B, 175B",
            "task_name": "Eleven text classification datasets (SST-2, SST-5, DBPedia, MR, CR, MPQA, Subj, TREC, AGNews, RTE, CB)",
            "task_description": "Select best few-shot prompt permutations without labeled development data using a generated probing set and entropy metrics, then evaluate classification accuracy.",
            "presentation_format": "Few-shot in-context prompts (same set of candidate permutations). For each permutation c_m, generate probing sequences g_m ~ P(.|c_m) from the LM, discard generated labels, build probing set D, compute GlobalE (entropy over predicted label distribution on D) and LocalE (average per-example prediction entropy on D), rank permutations, select top-K (K=4) permutations for evaluation.",
            "comparison_format": "Baseline (average over all permutations), Oracle (select top-by actual validation performance), split-training-set selection (use half training as dev), calibrated outputs.",
            "performance": "GlobalE yields on average a 13% relative improvement across the eleven tasks compared to baseline; LocalE yields ~9.6% relative improvement. Example numbers from Table 2: GPT-2 0.8B SST-2 baseline 74.5% -&gt; GlobalE 84.8% (std 4.1), LocalE 81.1% (std 5.5). GPT-3 2.7B SST-2 baseline 78.0% -&gt; GlobalE 80.2%, LocalE 81.0%. GPT-3 175B already high baseline 93.9%; GlobalE/LocalE matched or slightly improved.",
            "performance_comparison": "GlobalE generally outperforms LocalE and the baseline averaging over permutations; both outperform splitting the tiny training set for dev-selection. Example (SST-2, GPT-2 0.8B): baseline 74.5%, split-training-set selection 75.1%, LocalE 81.1%, GlobalE 84.8%.",
            "format_effect_size": "Representative effect sizes: +10–15 percentage points absolute improvement over baseline for mid-sized GPT-2 models on SST-2; average across tasks: GlobalE ~+13% relative improvement vs baseline.",
            "explanation_or_hypothesis": "High-entropy (balanced) predicted label distributions on an LM-generated probing set indicate prompts that avoid degenerate, overconfident behaviour and thus generalise better; LocalE flags prompts that avoid overconfident per-example predictions, while GlobalE favours overall label balance.",
            "null_or_negative_result": false,
            "experimental_details": "Probing generation: max length 128, sampling temperature t=2, block n-gram repetition blocking; for each candidate permutation generate probes, discard generated labels, compute GlobalE/LocalE; rank permutations and select top-K (K=4) for final evaluation. Experiments used 24 permutations per sample set and 5 different random sample sets (2 sets for GPT-3 175B due to cost).",
            "uuid": "e9281.5",
            "source_info": {
                "paper_title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Template Variation",
            "name_full": "Effect of Different Prompt Templates on Permutation Selection",
            "brief_description": "Different textual templates for linearising (x,y) pairs affect baseline performance but entropy-based probing (GlobalE/LocalE) consistently improves over baseline across templates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 family",
            "model_size": "0.1B, 0.3B, 0.8B, 1.5B (examples in Table 3)",
            "task_name": "SST-2 (template sensitivity study)",
            "task_description": "Compare four different templates (different phrasing/label mappings) for representing training examples in prompts and test whether the entropy probing selection still improves performance.",
            "presentation_format": "Four templates (e.g., 'Review: {Sentence} Sentiment: {Label}', 'Input: {Sentence} Prediction: {Label}', alternate label mappings good/bad vs positive/negative, etc.) with same few-shot concatenation and permutation experiments.",
            "comparison_format": "Baseline using all permutations per template vs top-k selected by LocalE/GlobalE per template.",
            "performance": "Across templates GlobalE/LocalE consistently improved accuracy over baseline. Example (GPT-2 0.8B SST-2): Template1 baseline 74.5% -&gt; LocalE 81.1% -&gt; GlobalE 84.8%; Template2 baseline 66.6% -&gt; LocalE 80.0% -&gt; GlobalE 80.9%.",
            "performance_comparison": "Improvements persisted across all four templates tested; relative ordering of best template varied but probing reliably improved each template's mean and reduced variance.",
            "format_effect_size": "Template-dependent; improvements of +6–18 absolute percentage points in examples shown (varies by base model/template).",
            "explanation_or_hypothesis": "Entropy probing is template-agnostic because it evaluates prompt-induced model behaviour (entropy/balance) on a generated probing set rather than relying on surface phrasing alone.",
            "null_or_negative_result": false,
            "experimental_details": "Four template variants for SST-2 (Table 4); experiments run per template with same permutation sampling and K=4 selection.",
            "uuid": "e9281.6",
            "source_info": {
                "paper_title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Split-training Dev vs Probing",
            "name_full": "Using held-out subset of few-shot examples as dev vs. generated probing for prompt selection",
            "brief_description": "Using a held-out split of the tiny few-shot training set for dev-selection improves over naive baseline but is outperformed by entropy-based probing using model-generated synthetic probes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 family",
            "model_size": "0.1B, 0.3B, 0.8B, 1.5B (examples in Table 5)",
            "task_name": "SST-2 (comparison)",
            "task_description": "Compare selecting top permutations by measured validation performance on a split of the real few-shot training data vs selecting top permutations by entropy probing on synthetic probing set.",
            "presentation_format": "4-shot prompt: either split into 2+2 and use held-out 2-shot as dev to rank permutations, or use probing set generated by LM and rank by GlobalE/LocalE.",
            "comparison_format": "Baseline (average over permutations) vs split-training-set dev selection vs GlobalE/LocalE probing selection.",
            "performance": "Example (SST-2): GPT-2 0.8B baseline 74.5% -&gt; split-training-set selection 75.1% -&gt; LocalE 81.1% -&gt; GlobalE 84.8%. For other sizes similar pattern: probing outperformed split-set selection consistently.",
            "performance_comparison": "Probing (GlobalE/LocalE) consistently outperforms selection based on splitting tiny training set for dev; split-training selection improves slightly over baseline but less than probing.",
            "format_effect_size": "Example effect (GPT-2 0.8B SST-2): GlobalE +10.3 pp over baseline and +9.7 pp over split-training-set selection.",
            "explanation_or_hypothesis": "Splitting an already tiny few-shot set reduces the signal for both training and validation and is suboptimal; generated probing sets provide more diverse synthetic inputs for robust entropy-based ranking without needing labeled held-out data.",
            "null_or_negative_result": false,
            "experimental_details": "4-shot prompts split in half for split-training dev selection; probing uses generated samples as per the probing methodology; results averaged across 5 random sample sets.",
            "uuid": "e9281.7",
            "source_info": {
                "paper_title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Sentence-pair Task Limits",
            "name_full": "Difficulties of Small Models on Sentence-Pair Tasks (RTE, CB)",
            "brief_description": "For smaller model sizes, even selecting performant permutations via probing may not improve performance above random baselines on sentence-pair entailment datasets (RTE, CB), indicating absence of useful decision boundary in small models for these tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 family (smaller sizes)",
            "model_size": "0.1B–1.5B (representative)",
            "task_name": "Sentence-pair tasks: RTE, CB",
            "task_description": "Textual entailment benchmarks requiring reasoning over premise-hypothesis pairs; evaluated by accuracy.",
            "presentation_format": "Few-shot in-context concatenation of pair templates; permutation probing and selection applied as in other tasks.",
            "comparison_format": "Baseline permutations vs GlobalE/LocalE selection vs larger model performance.",
            "performance": "For GPT-2 small/medium, performance on CB and RTE often close to random/majority (e.g., many baseline entries ~50%); probing gives only minimal gains still near random. At larger scales (GPT-3 175B) selection yields meaningful gains (GlobalE improves CB by 4.9% for GPT-3 175B).",
            "performance_comparison": "Small models: baseline ~random and probing minimal change; Large model: measurable improvement with probing.",
            "format_effect_size": "Small or negligible for small models; measurable (several percentage points) for large models (e.g., +4.9% for GPT-3 175B on CB via GlobalE).",
            "explanation_or_hypothesis": "For some tasks/models there may be no prompt that provides the model with sufficient internal capability to solve the task — i.e., the model lacks the necessary underlying classification ability, so prompt optimisation cannot help.",
            "null_or_negative_result": true,
            "experimental_details": "CB and RTE evaluated with the same probe-selection pipeline; observed that only larger models show improvements above random/majority baselines.",
            "uuid": "e9281.8",
            "source_info": {
                "paper_title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 2,
            "sanitized_title": "calibrate_before_use_improving_fewshot_performance_of_language_models"
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners",
            "rating": 2,
            "sanitized_title": "making_pretrained_language_models_better_fewshot_learners"
        },
        {
            "paper_title": "What makes good in-context examples for gpt-3?",
            "rating": 2,
            "sanitized_title": "what_makes_good_incontext_examples_for_gpt3"
        },
        {
            "paper_title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "rating": 1,
            "sanitized_title": "autoprompt_eliciting_knowledge_from_language_models_with_automatically_generated_prompts"
        }
    ],
    "cost": 0.01857075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</h1>
<p>Yao Lu ${ }^{\dagger}$ Max Bartolo ${ }^{\ddagger}$ Alastair Moore ${ }^{\ddagger}$ Sebastian Riedel ${ }^{\dagger}$ Pontus Stenetorp ${ }^{\dagger}$<br>${ }^{\dagger}$ University College London<br>${ }^{\ddagger}$ Mishcon de Reya LLP<br>{yao.lu,m.bartolo,s.riedel,p.stenetorp}@cs.ucl.ac.uk<br>alastair.moore@mishcon.com</p>
<h4>Abstract</h4>
<p>When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, finetuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are "fantastic" and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true fewshot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a $13 \%$ relative improvement for GPTfamily models across eleven different established text classification tasks.</p>
<h2>1 Introduction</h2>
<p>Large pretrained language models (PLMs, Devlin et al., 2019; Peters et al., 2018; Raffel et al., 2020; Liu et al., 2019; Yang et al., 2019; Radford et al., 2019) have shown remarkable performance when conditioned with an appropriate textual context (Petroni et al., 2019, 2020; Jiang et al., 2020; Shin et al., 2020; Davison et al., 2019). For example, when conditioned on a long document and a "TL;DR:" token, they can generate a summary of said document, and when provided a partial question ("The theory of relativity was developed by _"), they can generate the correct answer. Perhaps most strikingly, when primed with a context consisting of very few training examples, they produce
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Four-shot performance for 24 different sample orders across different sizes of GPT-family models (GPT-2 and GPT-3) for the SST-2 and Subj datasets.
text classification results that can match those of fully supervised models. This type of few shot setting, is commonly referred to as "In-context Learning" (Brown et al., 2020).</p>
<p>A core component of in-context learning is the text-based prompt that serves as the context. Composing a prompt requires: (i) text linearisation using a template; and (ii) training sample concatenation (See Table 1 for an example). It has been established that the structure of the template has a large impact on performance (Shin et al., 2020; Gao et al., 2020; Schick and Schütze, 2020; Jiang et al., 2020). However, to the best of our knowledge, no work has studied the effect of the sample ordering on In-context Learning performance.</p>
<p>Perhaps counter-intuitively, we find that the right sample order can make as much of a difference as</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Example</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">training set</td>
<td style="text-align: center;">(the greatest musicians, 1) (redundant concept. 0)</td>
</tr>
<tr>
<td style="text-align: center;">linearization</td>
<td style="text-align: center;">Review: the greatest musicians. Sentiment: positive Review: redundant concept. Sentiment: negative</td>
</tr>
<tr>
<td style="text-align: center;">concatenation</td>
<td style="text-align: center;">Review: the greatest musicians. Sentiment: positive. Review: redundant concept. Sentiment: negative. <br> OR <br> Review: redundant concept. Sentiment: negative. Review: the greatest musicians. Sentiment: positive</td>
</tr>
</tbody>
</table>
<p>Table 1: Procedures for prompt construction.
the right template. As can be seen in Figure 1, some permutations have comparable performance (over $85 \%$ accuracy) to supervised training for sentiment classification, while others perform close to random (around 50\%). This order sensitivity is universal across models, and although increasing the model size somewhat addresses it, the problem is still present for some text classification tasks (Subj in Figure 1) for models with billions of parameters.</p>
<p>In our analysis, we find no common denominator between performant sample orders and that they are not transferable across different model sizes and tasks. In a fully-supervised setting, we could rely on a development set to select among sample orders. However, this is not desirable in a few-shot setting where the size of the development set is very limited, even unavailable (Perez et al., 2021) . Instead, we use the generative nature of language models to construct an unlabelled artificial development set and refer to it as a probing set. As the probing set is unlabelled, we use the predicted label distribution statistics and propose entropy-based metrics to measure the quality of candidate prompts.Experimental results show that we can achieve on average $13 \%$ relative improvement across eleven different established text classification tasks across all different sizes (four orders of magnitude) of PLMs.</p>
<p>To summarise, our contributions are as follows:</p>
<ol>
<li>We study order sensitivity for In-context Learning, which we show is crucial for the success of pretrained language models for fewshot learning.</li>
<li>We propose a simple, generation-based probing method to identify performant prompts without requiring additional data.</li>
<li>Our probing method is universally applicable and effective across different sizes of pretrained language models and for different types of datasets - achieving on average a</li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Training sample permutations for the Incontext Learning setting. The concatenation of training samples as well as test data transforms the classification task into a sequence generation task.
$13 \%$ relative improvement over a wide range of tasks.</p>
<h2>2 Order Sensitivity and Prompt Design</h2>
<p>In this section, we study the relationship between permutation performance and various factors. For the ease of visualisation, we use a fixed random subset of four samples with a balanced label distribution from the SST-2 dataset and consider all 24 possible sample order permutations. This setup is illustrated in Figure 2. We also test five randomlyselected sets of examples and summarised variance statistics in the experiment section (Section 5).</p>
<p>Although beneficial, increasing model size does not guarantee low variance We evaluate the order permutations for four different sizes of GPT-2 $(0.1 B-1.5 B)^{1}$ and GPT-3 (2.7B-175B). As we can observe in Figure 1, models can obtain remarkable few-shot performance. We see that the GPT2-XL (1.5B) model can even surpass $90 \%$ accuracy given just four samples. This result is comparable to those of supervised models trained on more than 60,000 samples. However, the performance variation of different permutations remain a big issue, especially for "smaller" models. ${ }^{2}$ The same model can exhibit nearly perfect behaviour given one sample order, but then fall back to be on par with a random baseline for another. While increasing the model size (by a few order of magnitudes) can sometimes alleviate the issue, it still cannot resolve it entirely (especially if we consider tasks other than SST-2). In contrast, different initialisations of supervised fine-tuning approaches typically result in less than $1 \%$ standard deviation for their test set performance (Gao et al., 2020).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Order sensitivity using different numbers of training samples.</p>
<h3>Adding training samples does not significantly reduce variance</h3>
<p>To further explore the order sensitivity of few-shot prompts, we increase the number of training samples and then sample a subset of at most 24 different orderings. We use the GPT2 family models for this experiment. In Figure 3, we can observe that increasing the number of training samples leads to increases in performance. However, a high level of variance remains, even with a large number of samples and can even increase. Based on this, we draw the conclusion that order sensitivity is likely to be a fundamental issue of In-context Learning regardless of the number of training samples.</p>
<h3>Performant prompts are not transferable across models</h3>
<p>We find that a specific permutation's performance may drop from 88.7% to 51.6% by changing the underlying model from GPT2-XL (1.5B) to GPT2-Large (0.8B). This suggests that a particular permutation working well for one model does not imply that it will provide good results for another model. To validate this hypothesis, we use all possible order permutations of the four samples as prompts – 24 in total. We then perform prediction conditioned on each of these prompts for different models and calculate the pairwise Spearman's rank correlation coefficient between the scores. These results are shown in Figure 4.</p>
<p>If there is a common pattern for performant prompts, we should then be able to observe high correlation across models. However, the behaviour of permutations is seemingly random even across</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Training sample permutation performance correlation across different models.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Training label pattern permutation performance correlation across different models.</p>
<p>different sizes of the same model. For example, the 175B and 2.7B model only has a correlation of 0.05, this means a good permutation for the 2.7B model is in no way guaranteed that it will also yield good performance for the 175B model.</p>
<h3>Performant label orderings are not consistent across models</h3>
<p>In addition to training example ordering, we also explore label ordering for training prompts. We use all patterns of the abovementioned full permutations – six different label patterns. We then compute the pairwise Spearman correlation across different models as described in the previous paragraph. As shown in Figure 5, the behaviour of label orderings is once again seemingly random across different sizes of the same model. It is thus not possible to identify a label</p>
<p><sup>4</sup>NNPP, NPNP, NPPN, PNNP, PNPN, PPNN, where P/N respectively denotes positive/negative</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Left: Predicted SST-2 label distribution under different prompts. Right: 2-shot calibrated performance (Zhao et al., 2021) of all possible permutations on GPT2-XL (1.5B).
ordering that is performant across different models.
Degenerate behaviour of bad prompts We perform error analysis across performant and nonperformant prompts and observe that the majority of failing prompts suffer from highly unbalanced predicted label distributions (Figure 6, left). An intuitive way to address this would be by calibrating the output distribution, along the lines of Zhao et al. (2021). However, we find that although calibration leads to much higher performance, the variance remains high (Figure 6, right).</p>
<h2>3 Methodology</h2>
<p>The previous section demonstrates that prompt order can have a substantial effect on performance, with some orderings of the same prompts for the same model providing random performance, and other "better" orderings providing performance competitive with supervised approaches. This suggests that there could be various ways of selecting prompt orders to achieve better performance, but the challenge is to do so automatically and without the need for additional labels (e.g., a development set).</p>
<p>Hence, in this section, we explore the question of: "How can we automatically generate a 'probing set' to find performant prompt orderings"? We approach this by: (i) for a randomly-selected set of training samples, we use every possible ordering permutation of this set as candidates; (ii) constructing a probing set by querying the language model using all candidate prompts as context; and (iii) use this probing set to identify the best ordering by ranking them using a probing metric.</p>
<h3>3.1 Sampling from the Language Model to Construct a Probing Set</h3>
<p>We propose a simple methodology to automatically construct a "probing set", by directly sam-
pling from the language model itself. This approach makes it possible to generate probing sets automatically, without access to any additional data. Concretely, given a set of training samples $S=\left{\left(x_{i}, y_{i}\right)\right}, i=1, \cdots, n$, where $x_{i}$ and $y_{i}$ denote the sentence and label of the $i^{\text {th }}$ training sample. We then define a transformation $\mathcal{T}$, mapping each sample into natural language space, such that $t_{i}=\mathcal{T}\left(x_{i}, y_{i}\right) . t_{i}$ is therefore a text sequence of the $i^{\text {th }}$ training sample using the template defined by $\mathcal{T}$. In this work, we use a simple transformation function $\mathcal{T}$ such that $\mathcal{T}\left(x_{i}, y_{i}\right)=$ input: $x_{i}$ type: $y_{i}$. This transforms each sample into a standard format sentence, which linearises each element in the set into natural language space defined as $S^{\prime}=\left{t_{i}\right}, i=1, \cdots, n$.</p>
<p>We then define a full permutation function group of $n$ training samples, $\mathcal{F}=\left{f_{m}\right}, m=1, \cdots, n$ !, where each function $f_{m}$ takes $S^{\prime}$ as input and outputs $c_{m}$ : the concatenation of a unique permutation. In our case, sampling four training samples at random gives up to 24 possible ordering permutations of the transformed samples.</p>
<p>For each prompt candidate $c_{m}$, we then sample from the language model to obtain the probing sequence $g_{m} \sim P\left(\cdot \mid c_{m} ; \theta\right)$, where $\theta$ denotes the parameters of the pretrained language model. We stop decoding from the language model upon generating the special end-of-sentence token defined by a template, or reach the generation length limit. Our probing set construction method is illustrated in Figure 7, where the objective is to generate a probing set that shares a similar distribution to the training samples.</p>
<p>We run this sampling process for all possible prompt ordering permutations and extract probing samples from them $\left(\mathcal{T}^{-1}(g)\right)$. Then gather extracted samples together to form the probing set $D=\mathcal{T}^{-1}\left(g_{1}\right) \oplus \ldots \oplus \mathcal{T}^{-1}\left(g_{n!}\right)$. Although the probing set contains predicted label for each sentence, there is no guarantee on the validity of these labels. Therefore, we discard them from the probing set as we are only interested in sampling probes from the language model corresponding to the input distribution.</p>
<h3>3.2 Probing Metrics</h3>
<p>Once we have constructed a probing set for a given set of samples, we can now use that probing set to identify the best possible prompt ordering for that particular sample set. Here, we explore two</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Our probing set construction method, showing the various possible ordering permutations of the randomly selected training samples, the resulting generation for each permutation, and the concatenation of each into a probing set. Note that we discard the generated labels, as there is no guarantee that these generated labels are correct.</p>
<p>methods for selecting the best ordering: Global Entropy (GlobalE), and Local Entropy (LocalE).</p>
<p><strong>Global Entropy (GlobalE)</strong> The motivation behind GlobalE is to identify prompts of specific sample orderings that avoid the issue of extremely unbalanced predictions (as we have previously established it as key problem for non-performant prompts). We compute the predicted label $$y_i$$ for data point $$(x_i', y_i')$$ under context $$c_m$$ as follows:</p>
<p>$$y_i,m = \arg\max_{v \in V} P(v|c_m \oplus \mathcal{T}(x_i'); \theta) \tag{1}$$</p>
<p>For each label $$v \in V$$ (where $$V$$ denotes the target label set), we compute the label probability over the probing set as:</p>
<p>$$p_m^v = \frac{\sum_i \mathbb{1}_{{y_i,m = v}}}{|D|} \tag{2}$$</p>
<p>We then use the predicted category label entropy as the GlobalE score for $$c_m$$ as follows:</p>
<p>$$\text{GlobalE}<em V="V" _in="\in" v="v">m = \sum</em>$$} -p_m^v \log p_m^v \tag{3</p>
<p><strong>Local Entropy (LocalE)</strong> The motivation behind LocalE is that if a model is overly confident for all probing inputs, then it is likely that the model is not behaving as desired. At the very least, it is poorly calibrated, which could also be an indication of a poor capability to appropriately differentiate between classes. Similar to the GlobalE computation, we calculate the prediction probability of a data point $$(x_i', y_i')$$ over the target labels $$v \in V$$ under context $$c_m$$, as follows:</p>
<p>$$p_{i,m}^v = P_{(x_i', y_i') \sim D}(v|c_m \oplus \mathcal{T}(x_i'); \theta), \ v \in V \tag{4}$$</p>
<p>We then calculate the average prediction entropy per data point as the LocalE score:</p>
<p>$$\text{LocalE}<em V="V" _in="\in" v="v">m = \frac{\sum_i \sum</em>$$} -p_{i,m}^v \log p_{i,m}^v}{|D|} \tag{5</p>
<p>As we now have a way to score each prompt ordering, based on its effect against the probing set, we can rank each prompt ordering by performance as measured by GlobalE or LocalE respectively.</p>
<h3>4 Experimental Setup</h3>
<p>We use four different sizes of GPT-2 (Radford et al., 2019) (with 0.1B, 0.3B, 0.8B, and 1.5B parameters) and two sizes of GPT-3 (Brown et al., 2020) (with 2.7B, and 175B parameters). Due to limited context window size (up to 1024 word-pieces for the GPT-2 series of models), we use a 4-shot setting for all datasets except AGNews and DBPedia. Our experiments are based on the open-source checkpoints of GPT-2 models and access to the OpenAI GPT-3 API.<sup>5</sup> For probing set generation, we restrict the maximum generation length to 128. We also use sampling with a temperature, $$t$$, of 2, and we also make use of block n-gram repetitions (Paulus et al., 2018) to encourage diverse generation.</p>
<p>We use 24 different permutations for each set of randomly selected training samples and use 5 different sets (except for GPT-3 with 175B parameters, where we only do two sets with 12 different permutations due to the high monetary cost) for each experiment, giving a total of 120 runs. We report the mean and standard deviation of the corresponding evaluation metric over 5 different sets.</p>
<p>For performant prompt selection, we rank candidate prompts using the LocalE and GlobalE prob-</p>
<p><sup>5</sup>https://openai.com/api/</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;">SST-5</th>
<th style="text-align: center;">DBPedia</th>
<th style="text-align: center;">MR</th>
<th style="text-align: center;">CR</th>
<th style="text-align: center;">MPQA</th>
<th style="text-align: center;">Subj</th>
<th style="text-align: center;">TREC</th>
<th style="text-align: center;">AGNews</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">CB</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Majority</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: center;">Finetuning (Full)</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">90.5</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2 0.1B</td>
<td style="text-align: center;">$58.9_{7.8}$</td>
<td style="text-align: center;">$29.0_{4.9}$</td>
<td style="text-align: center;">$44.9_{9.7}$</td>
<td style="text-align: center;">$58.6_{7.6}$</td>
<td style="text-align: center;">$58.4_{6.4}$</td>
<td style="text-align: center;">$68.9_{7.1}$</td>
<td style="text-align: center;">$52.1_{9.7}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 2}_{4.7}$</td>
<td style="text-align: center;">$50.8_{11.9}$</td>
<td style="text-align: center;">$49.7_{2.7}$</td>
<td style="text-align: center;">$50.1_{1.0}$</td>
</tr>
<tr>
<td style="text-align: center;">LocalE</td>
<td style="text-align: center;">$\mathbf{6 5 . 2}_{3.9}$</td>
<td style="text-align: center;">$34.4_{4.4}$</td>
<td style="text-align: center;">$53.3_{4.9}$</td>
<td style="text-align: center;">$66.0_{6.3}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 0}_{5.4}$</td>
<td style="text-align: center;">$72.5_{6.0}$</td>
<td style="text-align: center;">$52.9_{1.3}$</td>
<td style="text-align: center;">$48.0_{3.9}$</td>
<td style="text-align: center;">$61.0_{5.9}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 0}_{3.3}$</td>
<td style="text-align: center;">$49.9_{1.6}$</td>
</tr>
<tr>
<td style="text-align: center;">GlobalE</td>
<td style="text-align: center;">$63.8_{5.8}$</td>
<td style="text-align: center;">$\mathbf{3 5 . 8}_{2.0}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 1}_{4.3}$</td>
<td style="text-align: center;">$\mathbf{6 6 . 4}_{5.8}$</td>
<td style="text-align: center;">$64.8_{2.7}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 5}_{4.5}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 0}_{1.3}$</td>
<td style="text-align: center;">$46.1_{3.7}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 1}_{5.7}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 0}_{3.0}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 3}_{1.6}$</td>
</tr>
<tr>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">$73.5_{1.7}$</td>
<td style="text-align: center;">$38.2_{4.0}$</td>
<td style="text-align: center;">$60.5_{4.2}$</td>
<td style="text-align: center;">$74.3_{4.9}$</td>
<td style="text-align: center;">$70.8_{4.4}$</td>
<td style="text-align: center;">$81.3_{2.5}$</td>
<td style="text-align: center;">$55.2_{1.7}$</td>
<td style="text-align: center;">$58.1_{4.3}$</td>
<td style="text-align: center;">$70.3_{2.8}$</td>
<td style="text-align: center;">$56.8_{2.0}$</td>
<td style="text-align: center;">$52.1_{1.3}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2 0.3B</td>
<td style="text-align: center;">$61.0_{13.2}$</td>
<td style="text-align: center;">$25.9_{5.9}$</td>
<td style="text-align: center;">$51.7_{7.0}$</td>
<td style="text-align: center;">$54.2_{7.8}$</td>
<td style="text-align: center;">$56.7_{8.4}$</td>
<td style="text-align: center;">$54.5_{8.8}$</td>
<td style="text-align: center;">$54.4_{7.9}$</td>
<td style="text-align: center;">$52.6_{4.9}$</td>
<td style="text-align: center;">$47.7_{10.6}$</td>
<td style="text-align: center;">$48.8_{2.6}$</td>
<td style="text-align: center;">$50.2_{5.3}$</td>
</tr>
<tr>
<td style="text-align: center;">LocalE</td>
<td style="text-align: center;">$75.3_{4.6}$</td>
<td style="text-align: center;">$31.0_{3.4}$</td>
<td style="text-align: center;">$47.1_{3.7}$</td>
<td style="text-align: center;">$65.2_{6.6}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 9}_{6.3}$</td>
<td style="text-align: center;">$67.6_{7.2}$</td>
<td style="text-align: center;">$\mathbf{6 6 . 7}_{9.3}$</td>
<td style="text-align: center;">$53.0_{3.9}$</td>
<td style="text-align: center;">$51.2_{7.3}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 8}_{1.0}$</td>
<td style="text-align: center;">$47.1_{4.2}$</td>
</tr>
<tr>
<td style="text-align: center;">GlobalE</td>
<td style="text-align: center;">$\mathbf{7 8 . 7}_{5.2}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 7}_{5.2}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 3}_{5.4}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 0}_{5.9}$</td>
<td style="text-align: center;">$70.7_{6.7}$</td>
<td style="text-align: center;">$\mathbf{6 8 . 3}_{6.9}$</td>
<td style="text-align: center;">$65.8_{10.1}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 3}_{1.6}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 6}_{7.2}$</td>
<td style="text-align: center;">$51.1_{1.9}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 3}_{3.7}$</td>
</tr>
<tr>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">$85.5_{4.3}$</td>
<td style="text-align: center;">$40.5_{6.3}$</td>
<td style="text-align: center;">$65.2_{7.6}$</td>
<td style="text-align: center;">$74.7_{6.1}$</td>
<td style="text-align: center;">$80.4_{5.4}$</td>
<td style="text-align: center;">$77.3_{2.3}$</td>
<td style="text-align: center;">$79.4_{2.4}$</td>
<td style="text-align: center;">$63.3_{2.9}$</td>
<td style="text-align: center;">$68.4_{8.0}$</td>
<td style="text-align: center;">$53.9_{1.3}$</td>
<td style="text-align: center;">$62.5_{7.4}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2 0.8B</td>
<td style="text-align: center;">$74.5_{10.3}$</td>
<td style="text-align: center;">$34.7_{4.2}$</td>
<td style="text-align: center;">$55.0_{12.5}$</td>
<td style="text-align: center;">$64.6_{13.1}$</td>
<td style="text-align: center;">$70.9_{12.7}$</td>
<td style="text-align: center;">$65.5_{8.7}$</td>
<td style="text-align: center;">$56.4_{8.1}$</td>
<td style="text-align: center;">$56.5_{2.7}$</td>
<td style="text-align: center;">$62.2_{11.6}$</td>
<td style="text-align: center;">$53.2_{2.0}$</td>
<td style="text-align: center;">$38.8_{8.5}$</td>
</tr>
<tr>
<td style="text-align: center;">LocalE</td>
<td style="text-align: center;">$81.1_{5.5}$</td>
<td style="text-align: center;">$40.3_{4.7}$</td>
<td style="text-align: center;">$56.7_{7.5}$</td>
<td style="text-align: center;">$82.6_{4.2}$</td>
<td style="text-align: center;">$85.4_{3.8}$</td>
<td style="text-align: center;">$73.6_{4.8}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 4}_{4.2}$</td>
<td style="text-align: center;">$56.2_{1.7}$</td>
<td style="text-align: center;">$62.7_{8.1}$</td>
<td style="text-align: center;">$53.3_{1.6}$</td>
<td style="text-align: center;">$38.4_{5.2}$</td>
</tr>
<tr>
<td style="text-align: center;">GlobalE</td>
<td style="text-align: center;">$\mathbf{8 4 . 8}_{4.1}$</td>
<td style="text-align: center;">$\mathbf{4 6 . 9}_{1.1}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 7}_{5.6}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 3}_{2.9}$</td>
<td style="text-align: center;">$\mathbf{8 6 . 7}_{2.5}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 8}_{3.1}$</td>
<td style="text-align: center;">$68.6_{6.5}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 2}_{2.3}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 7}_{3.6}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 5}_{1.5}$</td>
<td style="text-align: center;">$\mathbf{4 1 . 2}_{4.5}$</td>
</tr>
<tr>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">$88.9_{1.8}$</td>
<td style="text-align: center;">$48.4_{0.7}$</td>
<td style="text-align: center;">$72.3_{3.3}$</td>
<td style="text-align: center;">$87.5_{1.1}$</td>
<td style="text-align: center;">$89.9_{0.9}$</td>
<td style="text-align: center;">$80.3_{4.9}$</td>
<td style="text-align: center;">$76.6_{4.1}$</td>
<td style="text-align: center;">$62.1_{1.5}$</td>
<td style="text-align: center;">$78.1_{1.3}$</td>
<td style="text-align: center;">$57.3_{1.0}$</td>
<td style="text-align: center;">$53.2_{5.3}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2 1.5B</td>
<td style="text-align: center;">$66.8_{10.8}$</td>
<td style="text-align: center;">$41.7_{6.7}$</td>
<td style="text-align: center;">$82.6_{2.5}$</td>
<td style="text-align: center;">$59.1_{11.9}$</td>
<td style="text-align: center;">$56.9_{8.0}$</td>
<td style="text-align: center;">$73.9_{8.6}$</td>
<td style="text-align: center;">$59.7_{10.4}$</td>
<td style="text-align: center;">$53.1_{3.3}$</td>
<td style="text-align: center;">$77.6_{7.3}$</td>
<td style="text-align: center;">$55.0_{1.4}$</td>
<td style="text-align: center;">$53.8_{4.7}$</td>
</tr>
<tr>
<td style="text-align: center;">LocalE</td>
<td style="text-align: center;">$76.7_{8.2}$</td>
<td style="text-align: center;">$\mathbf{4 5 . 1}_{3.1}$</td>
<td style="text-align: center;">$83.8_{1.7}$</td>
<td style="text-align: center;">$78.1_{5.6}$</td>
<td style="text-align: center;">$71.8_{8.0}$</td>
<td style="text-align: center;">$78.5_{3.6}$</td>
<td style="text-align: center;">$69.7_{5.8}$</td>
<td style="text-align: center;">$53.6_{3.1}$</td>
<td style="text-align: center;">$79.3_{3.7}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 8}_{1.1}$</td>
<td style="text-align: center;">$52.6_{3.9}$</td>
</tr>
<tr>
<td style="text-align: center;">GlobalE</td>
<td style="text-align: center;">$\mathbf{8 1 . 8}_{3.9}$</td>
<td style="text-align: center;">$43.5_{4.5}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 9}_{1.8}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 9}_{5.7}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 4}_{6.9}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 4}_{2.1}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 9}_{6.0}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 5}_{5.0}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 9}_{1.2}$</td>
<td style="text-align: center;">$56.3_{1.2}$</td>
<td style="text-align: center;">$\mathbf{5 5 . 1}_{4.6}$</td>
</tr>
<tr>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">$86.1_{4.5}$</td>
<td style="text-align: center;">$50.9_{1.0}$</td>
<td style="text-align: center;">$87.3_{1.5}$</td>
<td style="text-align: center;">$84.0_{2.7}$</td>
<td style="text-align: center;">$80.3_{2.3}$</td>
<td style="text-align: center;">$85.1_{1.4}$</td>
<td style="text-align: center;">$79.9_{2.7}$</td>
<td style="text-align: center;">$59.0_{2.3}$</td>
<td style="text-align: center;">$86.1_{6.7}$</td>
<td style="text-align: center;">$58.2_{0.6}$</td>
<td style="text-align: center;">$63.9_{4.3}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 2.7B</td>
<td style="text-align: center;">$78.0_{10.7}$</td>
<td style="text-align: center;">$35.3_{6.9}$</td>
<td style="text-align: center;">$81.1_{1.8}$</td>
<td style="text-align: center;">$68.0_{12.9}$</td>
<td style="text-align: center;">$76.8_{11.7}$</td>
<td style="text-align: center;">$66.5_{10.3}$</td>
<td style="text-align: center;">$49.1_{2.9}$</td>
<td style="text-align: center;">$55.3_{4.4}$</td>
<td style="text-align: center;">$72.9_{4.8}$</td>
<td style="text-align: center;">$48.6_{1.9}$</td>
<td style="text-align: center;">$50.4_{0.7}$</td>
</tr>
<tr>
<td style="text-align: center;">LocalE</td>
<td style="text-align: center;">$\mathbf{8 1 . 0}_{6.0}$</td>
<td style="text-align: center;">$42.3_{4.7}$</td>
<td style="text-align: center;">$80.3_{1.7}$</td>
<td style="text-align: center;">$75.6_{4.1}$</td>
<td style="text-align: center;">$79.0_{5.5}$</td>
<td style="text-align: center;">$72.5_{5.8}$</td>
<td style="text-align: center;">$54.2_{4.2}$</td>
<td style="text-align: center;">$54.0_{2.6}$</td>
<td style="text-align: center;">$72.3_{4.6}$</td>
<td style="text-align: center;">$50.4_{1.9}$</td>
<td style="text-align: center;">$50.5_{6.8}$</td>
</tr>
<tr>
<td style="text-align: center;">GlobalE</td>
<td style="text-align: center;">$80.2_{4.2}$</td>
<td style="text-align: center;">$\mathbf{4 3 . 2}_{4.3}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 2}_{0.9}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 1}_{3.8}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 3}_{3.4}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 0}_{4.3}$</td>
<td style="text-align: center;">$\mathbf{5 4 . 3}_{4.0}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 7}_{2.0}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 1}_{1.9}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 3}_{1.8}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 2}_{0.8}$</td>
</tr>
<tr>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">$89.8_{0.7}$</td>
<td style="text-align: center;">$48.0_{1.1}$</td>
<td style="text-align: center;">$85.4_{1.6}$</td>
<td style="text-align: center;">$87.4_{0.9}$</td>
<td style="text-align: center;">$90.1_{0.7}$</td>
<td style="text-align: center;">$80.9_{1.4}$</td>
<td style="text-align: center;">$60.3_{10.3}$</td>
<td style="text-align: center;">$62.8_{4.2}$</td>
<td style="text-align: center;">$81.3_{2.9}$</td>
<td style="text-align: center;">$53.4_{3.1}$</td>
<td style="text-align: center;">$52.5_{1.4}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 175B</td>
<td style="text-align: center;">$\mathbf{9 3 . 9}_{0.6}$</td>
<td style="text-align: center;">$54.4_{2.5}$</td>
<td style="text-align: center;">$95.4_{0.9}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 6}_{0.7}$</td>
<td style="text-align: center;">$91.0_{1.0}$</td>
<td style="text-align: center;">$83.2_{1.5}$</td>
<td style="text-align: center;">$71.2_{7.3}$</td>
<td style="text-align: center;">$72.1_{2.7}$</td>
<td style="text-align: center;">$85.1_{1.7}$</td>
<td style="text-align: center;">$70.8_{2.8}$</td>
<td style="text-align: center;">$75.1_{5.1}$</td>
</tr>
<tr>
<td style="text-align: center;">LocalE</td>
<td style="text-align: center;">$93.8_{0.5}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 0}_{1.7}$</td>
<td style="text-align: center;">$95.5_{0.9}$</td>
<td style="text-align: center;">$94.5_{0.7}$</td>
<td style="text-align: center;">$91.3_{0.5}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 3}_{1.7}$</td>
<td style="text-align: center;">$75.0_{4.6}$</td>
<td style="text-align: center;">$71.8_{3.2}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 9}_{0.7}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 9}_{1.4}$</td>
<td style="text-align: center;">$74.6_{4.2}$</td>
</tr>
<tr>
<td style="text-align: center;">GlobalE</td>
<td style="text-align: center;">$\mathbf{9 3 . 9}_{0.6}$</td>
<td style="text-align: center;">$53.2_{2.1}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 7}_{0.7}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 6}_{0.2}$</td>
<td style="text-align: center;">$\mathbf{9 1 . 7}_{0.4}$</td>
<td style="text-align: center;">$82.0_{0.8}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 3}_{3.5}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 6}_{2.5}$</td>
<td style="text-align: center;">$85.7_{1.0}$</td>
<td style="text-align: center;">$71.8_{1.9}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 9}_{3.3}$</td>
</tr>
<tr>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">$94.7_{0.2}$</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">$96.7_{0.2}$</td>
<td style="text-align: center;">$95.5_{0.2}$</td>
<td style="text-align: center;">$92.6_{0.4}$</td>
<td style="text-align: center;">$85.5_{0.8}$</td>
<td style="text-align: center;">$81.1_{4.9}$</td>
<td style="text-align: center;">$77.0_{1.2}$</td>
<td style="text-align: center;">$87.7_{0.6}$</td>
<td style="text-align: center;">$74.7_{0.4}$</td>
<td style="text-align: center;">$83.0_{0.9}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Our main results on subset of the validation set. To fit the data within the GPT-2 model context window size, we use 1-shot for DBPedia, 2-shot for AGNews, 4-shot for other datasets. All the baseline results are calculated based on 5 different random seeds over 24 train context permutations. LocalE and GlobalE results are calculated based on the top 4 context permutations using our proposed approach. For the GPT-3 175B, we only use 2 seeds with 12 different permutations due to a limited computation budget.
ing metrics over the automatically generated probing set. We then select top $k$ samples ranked by highest entropy values, where $k=4$ in our experiments, of the available 24 permutations as performant prompts. Finally, we use these performant prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for a majority baseline, which always predicts the majority label in the dataset, as a lower-bound of performance. We also provide an oracle to show the upper-bound of performance by selecting the top four performant orderings based on prompt performance on the validation set.</p>
<h3>4.1 Evaluation Datasets</h3>
<p>Similar to previous work (Gao et al., 2020; Zhao et al., 2021), we use eleven text classification datasets ranging from sentiment classification to textual entailment. Further details of the datasets are provided in the Appendix. For evaluation, we
sub-sample 256 samples of the validation sets for all datasets to control for the GPT-3 inference costs as it requires the usage of a monetary paid-for API.</p>
<h2>5 Results</h2>
<p>We report experimental results in Table 2 and observe consistent improvements for both LocalE and GlobalE across all tasks.</p>
<p>Entropy-based probing is effective for performant prompt selection regardless of model size We find that GlobalE achieves, on average, a $13 \%$ relative improvement across the eleven different sentence classification tasks in comparison to prompts that do not make use of probing. LocalE provides results slightly inferior to GlobalE, with an average $9.6 \%$ relative improvement over the baseline model. Our selected performant prompts also demonstrate considerably lower variance than using all candidate prompts.</p>
<p>Ranking using Entropy-based probing is robust In Figure 8, we visualise the average performance when varying $K$ for the top $K$ prompt selection. $K=24$ corresponds to using all sampled prompt orders, which is equivalent to the baseline model performance in Table 2. We can observe that the slope of curves are negative for all datasets, suggesting that our method can rank performant prompts effectively. Though $K=1$ can provide good performance for most cases, in our experiments, we use $K=4$ as preliminary experiments indicated that it yielded stable performance across datasets.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Average performance of different Top $K$ permutation selection on GPT2-Large ( 0.8 B )</p>
<p>Entropy-based probing is effective across templates We evaluate Entropy-based probing for four different templates similar to Gao et al. (2020) and Zhao et al. (2021) (Table 4) for the SST-2 dataset. Experimental results in Table 3 indicate that Entropy-based probing is valid for different templates. We also observe that the randomness across different templates is similar to Section 2. These findings suggest that Entropy-based probing is not sensitive to specific templates, as it consistently provides improvements for all cases.</p>
<p>Performant permutation selection is a safe option for In-context Learning We find that for models that suffer from high prompt variance, our prompt selection process can show large improvements - up to $30 \%$ relative improvement. Furthermore, for tasks with low initial prompt performance variance, our method does not negatively impact performance. Our prompt selection provides marginal improvement at worse and on average a $13 \%$ relative improvement in the most cases.</p>
<p>Sentence-pair tasks remain challenging for smaller-sized models even with performant permutation selection For the CB and RTE datasets,</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Template 1</th>
<th style="text-align: left;">Template 2</th>
<th style="text-align: left;">Template 3</th>
<th style="text-align: left;">Template 4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2 0.1B</td>
<td style="text-align: left;">$58.9_{7.8}$</td>
<td style="text-align: left;">$57.5_{6.8}$</td>
<td style="text-align: left;">$58.1_{7.4}$</td>
<td style="text-align: left;">$56.6_{6.6}$</td>
</tr>
<tr>
<td style="text-align: left;">LocalE</td>
<td style="text-align: left;">$\mathbf{6 5 . 2}_{3.9}$</td>
<td style="text-align: left;">$\mathbf{6 0 . 7}_{1.6}$</td>
<td style="text-align: left;">$\mathbf{6 5 . 4}_{1.8}$</td>
<td style="text-align: left;">$61.0_{4.7}$</td>
</tr>
<tr>
<td style="text-align: left;">GlobalE</td>
<td style="text-align: left;">$63.8_{5.8}$</td>
<td style="text-align: left;">$59.0_{2.9}$</td>
<td style="text-align: left;">$64.3_{4.8}$</td>
<td style="text-align: left;">$\mathbf{6 3 . 5}_{4.8}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2 0.3B</td>
<td style="text-align: left;">$61.0_{13.2}$</td>
<td style="text-align: left;">$63.9_{11.3}$</td>
<td style="text-align: left;">$68.3_{11.8}$</td>
<td style="text-align: left;">$59.2_{6.4}$</td>
</tr>
<tr>
<td style="text-align: left;">LocalE</td>
<td style="text-align: left;">$75.3_{4.6}$</td>
<td style="text-align: left;">$70.0_{7.2}$</td>
<td style="text-align: left;">$80.2_{4.2}$</td>
<td style="text-align: left;">$62.2_{3.4}$</td>
</tr>
<tr>
<td style="text-align: left;">GlobalE</td>
<td style="text-align: left;">$\mathbf{7 8 . 7}_{5.2}$</td>
<td style="text-align: left;">$\mathbf{7 3 . 3}_{4.5}$</td>
<td style="text-align: left;">$\mathbf{8 1 . 3}_{1.1}$</td>
<td style="text-align: left;">$\mathbf{6 2 . 8}_{4.3}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2 0.8B</td>
<td style="text-align: left;">$74.5_{10.3}$</td>
<td style="text-align: left;">$66.6_{10.6}$</td>
<td style="text-align: left;">$70.3_{10.5}$</td>
<td style="text-align: left;">$63.7_{8.9}$</td>
</tr>
<tr>
<td style="text-align: left;">LocalE</td>
<td style="text-align: left;">$81.1_{5.5}$</td>
<td style="text-align: left;">$80.0_{5.6}$</td>
<td style="text-align: left;">$73.7_{6.2}$</td>
<td style="text-align: left;">$\mathbf{7 1 . 3}_{4.5}$</td>
</tr>
<tr>
<td style="text-align: left;">GlobalE</td>
<td style="text-align: left;">$\mathbf{8 4 . 8}_{4.1}$</td>
<td style="text-align: left;">$\mathbf{8 0 . 9}_{3.6}$</td>
<td style="text-align: left;">$\mathbf{7 9 . 8}_{5.9}$</td>
<td style="text-align: left;">$70.7_{5.3}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2 1.5B</td>
<td style="text-align: left;">$66.8_{10.8}$</td>
<td style="text-align: left;">$80.4_{7.6}$</td>
<td style="text-align: left;">$54.5_{7.9}$</td>
<td style="text-align: left;">$69.1_{10.5}$</td>
</tr>
<tr>
<td style="text-align: left;">LocalE</td>
<td style="text-align: left;">$76.7_{8.2}$</td>
<td style="text-align: left;">$83.1_{3.6}$</td>
<td style="text-align: left;">$66.9_{7.5}$</td>
<td style="text-align: left;">$72.7_{5.5}$</td>
</tr>
<tr>
<td style="text-align: left;">GlobalE</td>
<td style="text-align: left;">$\mathbf{8 1 . 8}_{3.9}$</td>
<td style="text-align: left;">$\mathbf{8 3 . 4}_{3.2}$</td>
<td style="text-align: left;">$\mathbf{6 7 . 2}_{6.1}$</td>
<td style="text-align: left;">$\mathbf{7 4 . 2}_{5.3}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Prompt selection performance of different templates on SST-2</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ID</th>
<th style="text-align: left;">Template</th>
<th style="text-align: left;">Label Mapping</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Review: ${$ Sentence $}$ <br> Sentiment: ${$ Label $}$</td>
<td style="text-align: left;">positive/negative</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Input: ${$ Sentence $}$ <br> Prediction: ${$ Label $}$</td>
<td style="text-align: left;">positive/negative</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Review: ${$ Sentence $}$ <br> Sentiment: ${$ Label $}$</td>
<td style="text-align: left;">good/bad</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: left;">${$ Sentence $}$ It was ${$ Label $}$</td>
<td style="text-align: left;">good/bad</td>
</tr>
</tbody>
</table>
<p>Table 4: Different Templates for SST-2
the performance of GPT-2 models is not significantly different from that of a random baseline. Despite this, we find that our method for identifying performant prompts can still provide minimal performance gains, although these are still within the levels of a random guess or majority vote. One reason for this could be that, for these particular sizes of models on these tasks, no good prompt exists. As such, optimising the prompt is not particularly effective in this setting. This is further supported by the observation that prompt selection can considerably improve performance on both CB and RTE at larger model sizes (particularly so for the GPT-3 175B parameter model). In fact, we find that prompt selection using GlobalE improves performance by $4.9 \%$ for GPT-3 175B on CB. This indicates that our method is widely applicable to all model sizes, and across all tasks, as long as they already possess some existing classification ability that can be improved through prompt design.</p>
<p>Entropy-based probing outperforms using subsets of the training data for tuning If one was not to rely on generation, an alternative approach to prompt selection could be to split the (limited) training data to form a validation set. To compare</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">GPT-2 0.1B</th>
<th style="text-align: left;">GPT-2 0.3B</th>
<th style="text-align: left;">GPT-2 0.8B</th>
<th style="text-align: left;">GPT-2 1.5B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: left;">$58.9_{7.8}$</td>
<td style="text-align: left;">$61.0_{13.2}$</td>
<td style="text-align: left;">$74.5_{10.3}$</td>
<td style="text-align: left;">$66.8_{30.8}$</td>
</tr>
<tr>
<td style="text-align: left;">LocalE</td>
<td style="text-align: left;">$\mathbf{6 5 . 2}_{3.9}$</td>
<td style="text-align: left;">$75.3_{4.6}$</td>
<td style="text-align: left;">$81.1_{5.5}$</td>
<td style="text-align: left;">$76.7_{8.2}$</td>
</tr>
<tr>
<td style="text-align: left;">GlobalE</td>
<td style="text-align: left;">$63.8_{5.8}$</td>
<td style="text-align: left;">$\mathbf{7 8 . 7}_{7.4}$</td>
<td style="text-align: left;">$\mathbf{8 4 . 8}_{9.1}$</td>
<td style="text-align: left;">$\mathbf{8 1 . 8}_{3.9}$</td>
</tr>
<tr>
<td style="text-align: left;">Split Training Set</td>
<td style="text-align: left;">$62.8_{5.5}$</td>
<td style="text-align: left;">$64.2_{6.1}$</td>
<td style="text-align: left;">$75.1_{8.8}$</td>
<td style="text-align: left;">$71.4_{7.6}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparing our method with splitting the training set into train and development for SST-2.
against this approach, we split the 4 -shot training samples (same setting as in Table 2) in half. We then select the top four performing prompts using validation set performance. As can be seen in Table 5, this approach consistently outperforms the baseline. However, both Entropy-based probing methods consistently provides better performance across all model sizes.</p>
<h2>6 Related Work</h2>
<p>Unified Interface Design for NLP Most previous work focuses on shared-parameters models, pretrain on some tasks, then fine-tune for different tasks, e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), etc. Eventually, leading to multiple task-specific models. There has for some time been attempts to design a unified interface for NLP tasks (Kumar et al., 2016; Raffel et al., 2020).In parallel with these works, GPT-2 (Radford et al., 2019) shows that appending trigger tokens (e.g. "TL;DR") at the end of language model input can cause language models to behave like summarisation models. The zero-shot capability of language models shows the potential to unify NLP tasks into a language modelling framework where fine-tuning is not necessary to achieve good performance. Furthermore, GPT-3 (Brown et al., 2020) shows that task-agnostic, few-shot performance can be improved by scaling up language models. It can sometimes even become competitive with prior state-of-the-art fine-tuning approaches.</p>
<p>Prompt Design for PLMs The core challenge of prompt design is to convert training data (if it exists) into a text sequence. Most work on prompt design focuses on how to make prompts more compatible with language models. Petroni et al. (2019) uses human effort to design natural language sentences and then perform token prediction given the input context. However, hand-crafted templates require significant human effort and is likely to end up with sub-optimal performance. Recent work has explored automatic template construction: Schick and Schütze (2020) uses cloze-style tasks to con-
struct templates, Gao et al. (2020) uses an external language model to generate templates, and Shin et al. (2020) uses gradient-guided search to find templates that maximise performance. Jiang et al. (2020) uses a mining-based method to create multiple diverse templates automatically.</p>
<p>Order Sensitivity of Prompt Design Gao et al. (2020) demonstrated that finetuning-based approaches are not as order sensitive as In-context Learning. Making use of a standard-size training set, Liu et al. (2021) used nearest neighbour search to retrieve the most relevant training samples for a specific test sample. They were successful in retrieving relevant samples and concluded that after retrieving them the order in which they are provided in the prompt has little to no effect on performance. While our study is fundamentally different from theirs in that we do not make use of a standard-size training set, we do come to the opposite conclusion. All previous work on prompt design focuses on the textual quality of the prompt and, to the best of our knowledge, none has studied order sensitivity in detail.</p>
<p>True Few-shot Learning Perez et al. (2021) evaluated few-shot capability of LMs when a heldout validation set is not available. Experimental result suggested that previous work overestimate the few-shot ability of LMs in this (true few-shot learning) setting. Our work instead use the generative nature of language models to construct a probing set without relying on held-out examples. We show that our probing method is better than relying on held out examples (Figure 5) and thus enables true few-shot learning.</p>
<h2>7 Conclusion</h2>
<p>We have shown that few-shot prompts suffer from order sensitivity, in that for the same prompt the order in which samples are provided can make the difference between state-of-the-art and random performance. In our analysis of the problem, we established that it is present across tasks, model sizes, prompt templates, samples, and number of training samples. To alleviate this problem, we introduced a novel probing method that exploits the generative nature of language models to construct an artificial development set. We were able to identity performant permutations using entropy-based statistics over this set, leading to an on average $13 \%$ improvement across eleven text classification tasks.</p>
<h2>References</h2>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177-190. Springer.</p>
<p>Joe Davison, Joshua Feldman, and Alexander M Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 1173-1178.</p>
<p>Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, pages 107124 .</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168-177.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.</p>
<p>Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natural language processing. In International conference on machine learning, pages 1378-1387. PMLR.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271-278.</p>
<p>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), pages 115-124.</p>
<p>Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive summarization. In International Conference on Learning Representations.</p>
<p>Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. arXiv preprint arXiv:2105.11447.</p>
<p>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 22272237.</p>
<p>Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2020. How context affects language models' factual predictions. In Automated Knowledge Base Construction.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.
A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. In OpenAI Blog.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67.</p>
<p>Timo Schick and Hinrich Schütze. 2020. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with</p>
<p>automatically generated prompts. arXiv preprint arXiv:2010.15980.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642.</p>
<p>Ellen M Voorhees and Dawn M Tice. 2000. Building a question answering test collection. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 200-207.</p>
<p>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2):165-210.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.</p>
<p>Xiang Zhang, Junbo Zhao, and Yann Lecun. 2015. Character-level convolutional networks for text classification. Advances in Neural Information Processing Systems, 2015:649-657.</p>
<p>Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Label Mapping</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">Review: contains no wit , only labored gags <br> Sentiment: negative</td>
<td style="text-align: center;">positive/negative</td>
</tr>
<tr>
<td style="text-align: center;">SST-5</td>
<td style="text-align: center;">Review: apparently reassembled from the cutting-room floor of any given daytime soap . <br> Sentiment: terrible</td>
<td style="text-align: center;">terrible/bad/okay/good/great</td>
</tr>
<tr>
<td style="text-align: center;">MR</td>
<td style="text-align: center;">Review: lame sweet home leaves no southern stereotype unturned . <br> Sentiment: negative</td>
<td style="text-align: center;">negative/positive</td>
</tr>
<tr>
<td style="text-align: center;">CR</td>
<td style="text-align: center;">Review: bluetooth does not work on this phone . <br> Sentiment: negative</td>
<td style="text-align: center;">negative/positive</td>
</tr>
<tr>
<td style="text-align: center;">MPQA</td>
<td style="text-align: center;">Review: dangerous situation <br> Sentiment: negative</td>
<td style="text-align: center;">negative/positive</td>
</tr>
<tr>
<td style="text-align: center;">Subj</td>
<td style="text-align: center;">Input: too slow , too boring , and occasionally annoying . <br> Type: subjective</td>
<td style="text-align: center;">subjective/objective</td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">Question: When did the neanderthal man live ? <br> Type: number</td>
<td style="text-align: center;">description/entity/expression/ human/location/number</td>
</tr>
<tr>
<td style="text-align: center;">AGNews</td>
<td style="text-align: center;">input: Wall St. Bears Claw Back Into the Black (Reuters). <br> type: business</td>
<td style="text-align: center;">world/sports/business/technology</td>
</tr>
<tr>
<td style="text-align: center;">DBPedia</td>
<td style="text-align: center;">input: CMC Aviation is a charter airline based in Nairobi Kenya. <br> type: company</td>
<td style="text-align: center;">company/school/artist/athlete/politics/ transportation/building/nature/village/ animal/plant/album/film/book</td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">premise: It was a complex language. Not written down but handed down. <br> One might say it was peeled down. <br> hypothesis: the language was peeled down <br> prediction: true</td>
<td style="text-align: center;">true/false/neither</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">premise: No Weapons of Mass Destruction Found in Iraq Yet. <br> hypothesis: Weapons of Mass Destruction Found in Iraq. <br> prediction: False</td>
<td style="text-align: center;">True/False</td>
</tr>
</tbody>
</table>
<p>Table 6: Prompt template and label mapping for different tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Notation</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">x</td>
<td style="text-align: left;">sentence</td>
<td style="text-align: left;">nice movie</td>
</tr>
<tr>
<td style="text-align: left;">y</td>
<td style="text-align: left;">label</td>
<td style="text-align: left;">positive</td>
</tr>
<tr>
<td style="text-align: left;">$\mathcal{T}(\mathrm{x})$</td>
<td style="text-align: left;">template-based transformation</td>
<td style="text-align: left;">Review: nice movie</td>
</tr>
<tr>
<td style="text-align: left;">$\mathcal{T}(\mathrm{x}, \mathrm{y})$</td>
<td style="text-align: left;">without label</td>
<td style="text-align: left;">Review: nice movie</td>
</tr>
<tr>
<td style="text-align: left;">$\mathcal{T}^{-1}(\mathcal{T}(\mathrm{x}, \mathrm{y}))$</td>
<td style="text-align: left;">extract (sentence, label) pair</td>
<td style="text-align: left;">Sentiment: positive</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">from text sequence</td>
<td style="text-align: left;">(nice movie, positive)</td>
</tr>
</tbody>
</table>
<p>Table 7: Examples of transformation notations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">$#$ of Classes</th>
<th style="text-align: center;">Avg. Len.</th>
<th style="text-align: center;">Balanced</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SST-2 (Socher et al., 2013)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">SST-5 (Socher et al., 2013)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: left;">MR (Pang and Lee, 2005)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">CR (Hu and Liu, 2004)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">MPQA (Wiebe et al., 2005)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">Subj (Pang and Lee, 2004)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">TREC (Voorhees and Tice, 2000)</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: left;">AGNews (Zhang et al., 2015)</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">DBPedia (Zhang et al., 2015)</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">CB (De Marneffe et al., 2019)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">69.7/8.4</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: left;">RTE (Dagan et al., 2005)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$55.3 / 11.9$</td>
<td style="text-align: center;">Yes</td>
</tr>
</tbody>
</table>
<p>Table 8: Statistics of evaluation datasets, average length is calculated based on GPT-2 sentence-piece length. For sentence-pair tasks, we report each sentence's average length separately.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Synthetic data</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">not sure where to even begin <br> the only real film on our watch lists <br> no one will care because it is just one story</td>
</tr>
<tr>
<td style="text-align: center;">SST-5</td>
<td style="text-align: center;">not a bad documentary, but the story feels tacked on. <br> one that i have never liked and was always too long to understand and not enjoyable in parts. <br> This movie is the opposite of what it pretentious title implies.</td>
</tr>
<tr>
<td style="text-align: center;">DBPedia</td>
<td style="text-align: center;">Gweno Mott's book: Gweno is a New Yorker cartoonist published by Little, Brown, 1995/2002/2013. <br> L. Ego Equestrians is North America's first dedicated equine show in Las Vegas. <br> Graphed is a graph visualization package from the GraphViz project.</td>
</tr>
<tr>
<td style="text-align: center;">MR</td>
<td style="text-align: center;">a solid first film for the debut helmer. <br> A good deal more of the material in his previous films can be found here but this film does not come across [...] it is so effective and engaging It feels more real And at some point, maybe it was about [...]</td>
</tr>
<tr>
<td style="text-align: center;">CR</td>
<td style="text-align: center;">It works just the same, i just prefer my iPhone 6. <br> the battery last so long for me it feels like ive already had my phone a year. <br> works great with both phones</td>
</tr>
<tr>
<td style="text-align: center;">MPQA</td>
<td style="text-align: center;">this is really going nowhere <br> why does it look so angry?? <br> Excellent book and will get a good reputation</td>
</tr>
<tr>
<td style="text-align: center;">Subj</td>
<td style="text-align: center;">this will become apparent as it gets older. <br> how about something more subtle to show this girl's love? <br> a perfect summary of an episode where the entire series is one massive meta romp, with [...]</td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">Whales can hold 4 gallons. Whaler can also be written as: What whale is named Whalerel? <br> To a certain degree, how do human eyes perceive colour? <br> From where does our moon orbit, in Earth's Solar System?</td>
</tr>
<tr>
<td style="text-align: center;">AGNews</td>
<td style="text-align: center;">Google buys for S11bn: A-Z and thesaurus online, music search; photo service and TV site [...] <br> Saudi-born billionaire takes $5 Billion Hit With Bankrupt. Saudi millionaire Sultan Al-Amoudi said [...] <br> China's 'Sesame' takes over for South Korea in world TV race as US TV loses market dominance.[...]</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">Premise: The Tuareg are a nomadic people who live in the Sahara desert. <br> Hypothesis: Tuareg are nomadic people who lived in the Sahara desert before the arrival of the Arabs. <br> Premise: In the early 1940s, the United States and the Soviet Union were at war with Germany. <br> Hypothesis: Germany was at war with the United States and Russia. <br> Premise: Water is a precious commodity. <br> Hypothesis: Water is not a precious commodity.</td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">Premise: In the back corner of Melissa's classroom her father walked through the door and walked across the front. [...] Hypothesis: his curiosity was directed towards some, something other than Melissa <br> Premise: Maggie took Gloria out for a drive to the nearby city limits of Fort Myers on Tuesday Hypothesis: he couldn't bear looking down his nose at all the other houses <br> Premise: There was one in Dallas. When it came out in New Jersey. And there were,[...] Hypothesis: I would never see that movie</td>
</tr>
</tbody>
</table>
<p>Table 9: Artificial development set generated by GPT2-XL (1.5B). We random select three examples per dataset. Long sentences are trimmed due to limited space.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ We can also refer these models as GPT2-base, GPT2medium, GPT2-Large, and GPT2-XL.
${ }^{2}$ The smallest model in our experiment is the same size as BERT-base.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
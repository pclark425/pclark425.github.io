<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8126 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8126</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8126</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-271218363</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.11421v1.pdf" target="_blank">States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) exhibit various emergent abilities. Among these abilities, some might reveal the internal working mechanisms of models. In this paper, we uncover a novel emergent capability in models: the intrinsic ability to perform extended sequences of calculations without relying on chain-of-thought step-by-step solutions. Remarkably, the most advanced models can directly output the results of two-digit number additions with lengths extending up to 15 addends. We hypothesize that the model emerges Implicit Discrete State Representations (IDSRs) within its hidden states and performs symbolic calculations internally. To test this hypothesis, we design a sequence of experiments that look into the hidden states. Specifically, we first confirm that IDSRs exist. Then, we provide interesting observations about the formation of IDSRs from layer, digit, and sequence perspectives. Finally, we confirm that models indeed use IDSRs to produce the final answers. However, we also discover that these state representations are far from lossless in current open-sourced models, leading to inaccuracies in their final performance. Our work presents a novel exploration of LLMs' symbolic calculation abilities and the underlying mechanisms.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8126.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8126.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IDSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implicit Discrete State Representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hypothesized internal representation formed in transformer hidden states that encodes intermediate (discrete) results during multi-step arithmetic, enabling the model to perform consecutive additions in one forward pass by passing per-step state along the token sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-72B (primary analyzed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen-72B: a 72B-parameter transformer language model from the Qwen family; used in this paper without fine-tuning for inference-time hidden-state extraction and probing. (The paper also analyzes smaller Qwen models for scaling comparisons.)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Consecutive addition/subtraction of uniform-digit integers (1–3 digit addends), evaluated up to 14 addends; experiments include 1-digit, 2-digit and 3-digit sums and both addition and subtraction expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Digit-wise, discrete intermediate numeric states (IDSRs) are formed in hidden states at operator tokens (e.g., the '+' or '=' tokens). IDSRs appear to encode digits independently (LSB-first order) and are propagated along the token sequence so later steps can reuse the compressed intermediate result instead of recomputing from all prefix numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Classification probes (MLP probe with one hidden layer, bottle-neck MLP, and single-layer linear probe) applied to hidden states at '+', '-', '=' tokens; digit-wise multi-probe set for multi-digit numbers; 'attention bridge' intervention masking where only a single token (the bridge token) connects prefix and suffix to test whether the model uses the bridge-carried state.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Probe-based decoding: whole-number probing reported maximum prediction accuracies across addition-sign tokens (example maxima reported in paper): 100%, 99%, 74%, 62%, 37% for the 2nd→5th addition signs and final '=' (aggregate statement across experiments). For Qwen-72B specifically, shallow (first ~10) layers had maxima of ~92%, 92%, 92%, 87%, 75% for 2nd→5th and '=' tokens respectively; later-layer maxima matched the 100/99/74/62/37 profile. Behavioral accuracy (direct model outputs) - larger models maintain >50% exact-answer accuracy for sequences up to 5 two-digit addends; performance emerges strongly for sequence lengths >8 (qualitative, plotted in paper). Attention-bridge intervention: Qwen-72B can still compute via bridge for 1-digit additions across 2–10 numbers but with a significant drop relative to baseline (exact numeric drop not tabulated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Progressive resolution loss of IDSRs as they are passed along the sequence (substantial accuracy drop after early steps); IDSRs are not lossless leading to incorrect final outputs; later model layers reduce linear decodability (single-layer probes fail in mid-late layers) indicating non-linear recoding and increased information compression/loss; abrupt attention masking (attention-bridge) causes further drops in accuracy; prompting the model to 'ignore' arithmetic diverts later-layer representations away from numeric IDSRs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>1) Probes trained on hidden states at '+', '-' and '=' tokens yield prediction accuracies significantly above chance, demonstrating stored intermediate numeric information. 2) Digit-wise probes show independent digit encodings and an LSB-first formation order consistent with digit-by-digit arithmetic. 3) Layerwise probe differences (single-layer vs MLP) show early layers hold linearly-decodable IDSRs and later layers recode them nonlinearly, consistent with a two-stage formation. 4) Attention-bridge masking (blocking direct attention from prefix except through one bridge token) still allows correct answers, indicating the model relies on information carried in that bridge token's hidden state (i.e., IDSR) rather than re-attending the entire prefix at the final token.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>IDSRs are far from lossless: probe accuracy drops markedly after the second addition sign, especially for multi-digit sums; single-layer linear probes fail in layers ~50–65 implying non-linear representation that complicates simple decoding; prompts that change task objective (e.g., 'ignore the formula') reduce later-layer numeric IDSR resolution though early-layer IDSRs remain, showing context-dependent recoding; experiments limited to synthetic formulas and to open-source models (Qwen-72B primarily), so generalization to other tasks/models is not fully validated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8126.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8126.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen series</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen family of transformer LLMs (4B, 7B, 14B, 72B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of open-source transformer language models of varying sizes evaluated for emergent consecutive-addition ability and hidden-state IDSR formation; used to study scaling effects and layerwise representation properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-4B / Qwen-7B / Qwen-14B / Qwen-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLM family (multiple sizes); models were evaluated in their original form without fine-tuning. Hidden states were extracted (for Qwen-72B primarily) to run probes; behavioral accuracy was measured across sizes to study emergence.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Consecutive two-digit (and 1–3 digit) addition across variable-length sequences (2–14 addends), tested for direct-answer (no chain-of-thought) performance.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Emergent IDSRs in hidden states, with larger models forming higher-resolution IDSRs that persist across more addition steps; smaller models often fail to form persistent high-resolution states beyond very short sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Same probing suite as Qwen-72B (MLP probes, bottle-neck probe, single-layer probe) applied to the Qwen series for layerwise and size-scaling analysis; behavioral testing of direct-answer accuracy across sequence lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Behavioral (exact-answer) trends: small models (e.g., 4B/7B) show acceptable accuracy only on 2–3 two-digit additions and drop to near zero by length 5; larger models (14B, 72B) maintain >50% accuracy up to approximately five addends and show non-zero performance for longer sequences; performance scales with model size, with a sharp capability increase for sequence lengths ~3–6.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Smaller models fail early (sequence length ≈5); even large open-source models lose resolution of intermediate states as sequence length increases; layerwise nonlinear recoding in later layers reduces linear probe decodability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Scaling correlation: probe and behavioral metrics improve with model size; layerwise probe patterns (linear decoding in early layers, nonlinear recoding in later layers) consistent across sizes where hidden-state access was available.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Even the largest open-source model tested (Qwen-72B) shows substantial degradation in IDSR resolution across several addition steps; lack of hidden-state access for closed-source models limits cross-family mechanistic comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8126.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8126.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deepseek-67B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek-67B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 67B-parameter open model evaluated for emergent implicit arithmetic ability; included in the behavioral comparison of direct consecutive-addition performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-67B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>67B-parameter open-source transformer LLM; evaluated without fine-tuning for direct-addition exact-answer accuracy and included in model-size vs performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Consecutive two-digit addition over varying sequence lengths up to 14 addends (direct-answer, no chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Presumed to form IDSR-like internal states similar to other large models (paper uses it in the same behavioral evaluation, but detailed hidden-state probing focused on Qwen models).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral task evaluation (exact-answer accuracy) in the same benchmark suite as Qwen series; hidden-state probing experiments not explicitly detailed for Deepseek in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in comparative plots: Deepseek-67B is one of the larger open-source models and shows stronger emergent performance than 7B models, but exact numeric accuracies per sequence length are provided only in plotted figures (qualitative statement: larger models sustain non-zero performance for more addends).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Same qualitative failure modes as other large models: decreased accuracy with increasing sequence length and non-lossless intermediate representations leading to final errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral performance patterns consistent with emergent IDSR hypothesis across model scale; direct hidden-state probe evidence not provided for this specific model in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Paper does not provide layerwise probe data for Deepseek-67B, so mechanistic claims are inferred from behavior and from Qwen-72B detailed probes rather than direct measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8126.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8126.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Closed-source SOTA LLMs (GPT-3.5/4, Claude3, GPT4-O)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representative closed-source state-of-the-art LLMs (GPT-3.5, GPT-4, GPT4-O, Claude3-Sonnet, Claude3-Opus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-capability (often proprietary) LLMs included in behavioral comparisons that exhibit strong direct-addition performance, with the largest closed models showing high accuracy even for long addend sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4, GPT4-O, Claude3-Sonnet, Claude3-Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large transformer-based LLMs (varying parameterization and training details not specified in this paper). They were evaluated via API for direct-answer consecutive-addition performance (without chain-of-thought). Hidden-state extraction was not available for these models.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Consecutive two-digit addition across sequences including up to ten addends (behavioral exact-answer evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Not directly probed (no hidden-state access). The paper hypothesizes that similar IDSR mechanisms may be present given behavioral capability, but offers no direct internal evidence for these closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral exact-answer evaluation (direct prediction accuracy); manual verification to exclude use of external tools; no hidden-state probes or attention-bridge interventions due to API limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Behavioral results show closed-source, larger models achieving substantially higher accuracies on 10-addend problems compared to open-source models; the paper reports that the most advanced closed models can directly answer two-digit additions of up to 15 addends in some cases (example given in introduction), and Figure 2 shows closed-source models with significantly higher accuracy on 10-addend tasks (qualitative; exact numeric values in plotted figures but not enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Not directly probed; occasional non-100% probability outputs verified to confirm models were not simply retrieving memorized results or using tool calls. The paper notes errors increase with number of addends and that very long sequences still sometimes fail.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Behavioral capability consistent with IDSR hypothesis, but no internal probe or intervention evidence due to lack of hidden-state access.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No direct internal-state validation; therefore the mechanism (IDSR formation) in closed-source models remains a hypothesis supported only by behavioral parallels to open-source probe results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Eliciting latent predictions from transformers with the tuned lens <em>(Rating: 2)</em></li>
                <li>Linearity of relation decoding in transformer language models <em>(Rating: 2)</em></li>
                <li>Transformer feed-forward layers are key-value memories <em>(Rating: 1)</em></li>
                <li>Do language models plan ahead for future tokens? <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8126",
    "paper_id": "paper-271218363",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "IDSR",
            "name_full": "Implicit Discrete State Representations",
            "brief_description": "A hypothesized internal representation formed in transformer hidden states that encodes intermediate (discrete) results during multi-step arithmetic, enabling the model to perform consecutive additions in one forward pass by passing per-step state along the token sequence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-72B (primary analyzed)",
            "model_description": "Qwen-72B: a 72B-parameter transformer language model from the Qwen family; used in this paper without fine-tuning for inference-time hidden-state extraction and probing. (The paper also analyzes smaller Qwen models for scaling comparisons.)",
            "arithmetic_task_type": "Consecutive addition/subtraction of uniform-digit integers (1–3 digit addends), evaluated up to 14 addends; experiments include 1-digit, 2-digit and 3-digit sums and both addition and subtraction expressions.",
            "mechanism_or_representation": "Digit-wise, discrete intermediate numeric states (IDSRs) are formed in hidden states at operator tokens (e.g., the '+' or '=' tokens). IDSRs appear to encode digits independently (LSB-first order) and are propagated along the token sequence so later steps can reuse the compressed intermediate result instead of recomputing from all prefix numbers.",
            "probing_or_intervention_method": "Classification probes (MLP probe with one hidden layer, bottle-neck MLP, and single-layer linear probe) applied to hidden states at '+', '-', '=' tokens; digit-wise multi-probe set for multi-digit numbers; 'attention bridge' intervention masking where only a single token (the bridge token) connects prefix and suffix to test whether the model uses the bridge-carried state.",
            "performance_metrics": "Probe-based decoding: whole-number probing reported maximum prediction accuracies across addition-sign tokens (example maxima reported in paper): 100%, 99%, 74%, 62%, 37% for the 2nd→5th addition signs and final '=' (aggregate statement across experiments). For Qwen-72B specifically, shallow (first ~10) layers had maxima of ~92%, 92%, 92%, 87%, 75% for 2nd→5th and '=' tokens respectively; later-layer maxima matched the 100/99/74/62/37 profile. Behavioral accuracy (direct model outputs) - larger models maintain &gt;50% exact-answer accuracy for sequences up to 5 two-digit addends; performance emerges strongly for sequence lengths &gt;8 (qualitative, plotted in paper). Attention-bridge intervention: Qwen-72B can still compute via bridge for 1-digit additions across 2–10 numbers but with a significant drop relative to baseline (exact numeric drop not tabulated in text).",
            "error_types_or_failure_modes": "Progressive resolution loss of IDSRs as they are passed along the sequence (substantial accuracy drop after early steps); IDSRs are not lossless leading to incorrect final outputs; later model layers reduce linear decodability (single-layer probes fail in mid-late layers) indicating non-linear recoding and increased information compression/loss; abrupt attention masking (attention-bridge) causes further drops in accuracy; prompting the model to 'ignore' arithmetic diverts later-layer representations away from numeric IDSRs.",
            "evidence_for_mechanism": "1) Probes trained on hidden states at '+', '-' and '=' tokens yield prediction accuracies significantly above chance, demonstrating stored intermediate numeric information. 2) Digit-wise probes show independent digit encodings and an LSB-first formation order consistent with digit-by-digit arithmetic. 3) Layerwise probe differences (single-layer vs MLP) show early layers hold linearly-decodable IDSRs and later layers recode them nonlinearly, consistent with a two-stage formation. 4) Attention-bridge masking (blocking direct attention from prefix except through one bridge token) still allows correct answers, indicating the model relies on information carried in that bridge token's hidden state (i.e., IDSR) rather than re-attending the entire prefix at the final token.",
            "counterexamples_or_challenges": "IDSRs are far from lossless: probe accuracy drops markedly after the second addition sign, especially for multi-digit sums; single-layer linear probes fail in layers ~50–65 implying non-linear representation that complicates simple decoding; prompts that change task objective (e.g., 'ignore the formula') reduce later-layer numeric IDSR resolution though early-layer IDSRs remain, showing context-dependent recoding; experiments limited to synthetic formulas and to open-source models (Qwen-72B primarily), so generalization to other tasks/models is not fully validated.",
            "uuid": "e8126.0",
            "source_info": {
                "paper_title": "States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Qwen series",
            "name_full": "Qwen family of transformer LLMs (4B, 7B, 14B, 72B)",
            "brief_description": "A set of open-source transformer language models of varying sizes evaluated for emergent consecutive-addition ability and hidden-state IDSR formation; used to study scaling effects and layerwise representation properties.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-4B / Qwen-7B / Qwen-14B / Qwen-72B",
            "model_description": "Transformer LLM family (multiple sizes); models were evaluated in their original form without fine-tuning. Hidden states were extracted (for Qwen-72B primarily) to run probes; behavioral accuracy was measured across sizes to study emergence.",
            "arithmetic_task_type": "Consecutive two-digit (and 1–3 digit) addition across variable-length sequences (2–14 addends), tested for direct-answer (no chain-of-thought) performance.",
            "mechanism_or_representation": "Emergent IDSRs in hidden states, with larger models forming higher-resolution IDSRs that persist across more addition steps; smaller models often fail to form persistent high-resolution states beyond very short sequences.",
            "probing_or_intervention_method": "Same probing suite as Qwen-72B (MLP probes, bottle-neck probe, single-layer probe) applied to the Qwen series for layerwise and size-scaling analysis; behavioral testing of direct-answer accuracy across sequence lengths.",
            "performance_metrics": "Behavioral (exact-answer) trends: small models (e.g., 4B/7B) show acceptable accuracy only on 2–3 two-digit additions and drop to near zero by length 5; larger models (14B, 72B) maintain &gt;50% accuracy up to approximately five addends and show non-zero performance for longer sequences; performance scales with model size, with a sharp capability increase for sequence lengths ~3–6.",
            "error_types_or_failure_modes": "Smaller models fail early (sequence length ≈5); even large open-source models lose resolution of intermediate states as sequence length increases; layerwise nonlinear recoding in later layers reduces linear probe decodability.",
            "evidence_for_mechanism": "Scaling correlation: probe and behavioral metrics improve with model size; layerwise probe patterns (linear decoding in early layers, nonlinear recoding in later layers) consistent across sizes where hidden-state access was available.",
            "counterexamples_or_challenges": "Even the largest open-source model tested (Qwen-72B) shows substantial degradation in IDSR resolution across several addition steps; lack of hidden-state access for closed-source models limits cross-family mechanistic comparison.",
            "uuid": "e8126.1",
            "source_info": {
                "paper_title": "States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Deepseek-67B",
            "name_full": "Deepseek-67B",
            "brief_description": "A 67B-parameter open model evaluated for emergent implicit arithmetic ability; included in the behavioral comparison of direct consecutive-addition performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Deepseek-67B",
            "model_description": "67B-parameter open-source transformer LLM; evaluated without fine-tuning for direct-addition exact-answer accuracy and included in model-size vs performance comparisons.",
            "arithmetic_task_type": "Consecutive two-digit addition over varying sequence lengths up to 14 addends (direct-answer, no chain-of-thought).",
            "mechanism_or_representation": "Presumed to form IDSR-like internal states similar to other large models (paper uses it in the same behavioral evaluation, but detailed hidden-state probing focused on Qwen models).",
            "probing_or_intervention_method": "Behavioral task evaluation (exact-answer accuracy) in the same benchmark suite as Qwen series; hidden-state probing experiments not explicitly detailed for Deepseek in paper.",
            "performance_metrics": "Reported in comparative plots: Deepseek-67B is one of the larger open-source models and shows stronger emergent performance than 7B models, but exact numeric accuracies per sequence length are provided only in plotted figures (qualitative statement: larger models sustain non-zero performance for more addends).",
            "error_types_or_failure_modes": "Same qualitative failure modes as other large models: decreased accuracy with increasing sequence length and non-lossless intermediate representations leading to final errors.",
            "evidence_for_mechanism": "Behavioral performance patterns consistent with emergent IDSR hypothesis across model scale; direct hidden-state probe evidence not provided for this specific model in the paper.",
            "counterexamples_or_challenges": "Paper does not provide layerwise probe data for Deepseek-67B, so mechanistic claims are inferred from behavior and from Qwen-72B detailed probes rather than direct measurement.",
            "uuid": "e8126.2",
            "source_info": {
                "paper_title": "States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Closed-source SOTA LLMs (GPT-3.5/4, Claude3, GPT4-O)",
            "name_full": "Representative closed-source state-of-the-art LLMs (GPT-3.5, GPT-4, GPT4-O, Claude3-Sonnet, Claude3-Opus)",
            "brief_description": "High-capability (often proprietary) LLMs included in behavioral comparisons that exhibit strong direct-addition performance, with the largest closed models showing high accuracy even for long addend sequences.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5, GPT-4, GPT4-O, Claude3-Sonnet, Claude3-Opus",
            "model_description": "Proprietary large transformer-based LLMs (varying parameterization and training details not specified in this paper). They were evaluated via API for direct-answer consecutive-addition performance (without chain-of-thought). Hidden-state extraction was not available for these models.",
            "arithmetic_task_type": "Consecutive two-digit addition across sequences including up to ten addends (behavioral exact-answer evaluation).",
            "mechanism_or_representation": "Not directly probed (no hidden-state access). The paper hypothesizes that similar IDSR mechanisms may be present given behavioral capability, but offers no direct internal evidence for these closed-source models.",
            "probing_or_intervention_method": "Behavioral exact-answer evaluation (direct prediction accuracy); manual verification to exclude use of external tools; no hidden-state probes or attention-bridge interventions due to API limitations.",
            "performance_metrics": "Behavioral results show closed-source, larger models achieving substantially higher accuracies on 10-addend problems compared to open-source models; the paper reports that the most advanced closed models can directly answer two-digit additions of up to 15 addends in some cases (example given in introduction), and Figure 2 shows closed-source models with significantly higher accuracy on 10-addend tasks (qualitative; exact numeric values in plotted figures but not enumerated in text).",
            "error_types_or_failure_modes": "Not directly probed; occasional non-100% probability outputs verified to confirm models were not simply retrieving memorized results or using tool calls. The paper notes errors increase with number of addends and that very long sequences still sometimes fail.",
            "evidence_for_mechanism": "Behavioral capability consistent with IDSR hypothesis, but no internal probe or intervention evidence due to lack of hidden-state access.",
            "counterexamples_or_challenges": "No direct internal-state validation; therefore the mechanism (IDSR formation) in closed-source models remains a hypothesis supported only by behavioral parallels to open-source probe results.",
            "uuid": "e8126.3",
            "source_info": {
                "paper_title": "States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Eliciting latent predictions from transformers with the tuned lens",
            "rating": 2,
            "sanitized_title": "eliciting_latent_predictions_from_transformers_with_the_tuned_lens"
        },
        {
            "paper_title": "Linearity of relation decoding in transformer language models",
            "rating": 2,
            "sanitized_title": "linearity_of_relation_decoding_in_transformer_language_models"
        },
        {
            "paper_title": "Transformer feed-forward layers are key-value memories",
            "rating": 1,
            "sanitized_title": "transformer_feedforward_layers_are_keyvalue_memories"
        },
        {
            "paper_title": "Do language models plan ahead for future tokens?",
            "rating": 1,
            "sanitized_title": "do_language_models_plan_ahead_for_future_tokens"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.012426,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly
16 Jul 2024</p>
<p>Junhao Chen chenjunh22@mails.tsinghua.edu.cn 
Department of Computer Science and Technology
Tsinghua University
BeijingChina</p>
<p>Shengding Hu 
Department of Computer Science and Technology
Tsinghua University
BeijingChina</p>
<p>Zhiyuan Liu 
Department of Computer Science and Technology
Tsinghua University
BeijingChina</p>
<p>Maosong Sun 
Department of Computer Science and Technology
Tsinghua University
BeijingChina</p>
<p>States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly
16 Jul 2024B7D4E796E1815DC62AE69718CD2911DEarXiv:2407.11421v1[cs.CL]
Large Language Models (LLMs) exhibit various emergent abilities.Among these abilities, some might reveal the internal working mechanisms of models.In this paper, we uncover a novel emergent capability in models: the intrinsic ability to perform extended sequences of calculations without relying on chain-of-thought step-by-step solutions.Remarkably, the most advanced models can directly output the results of two-digit number additions with lengths extending up to 15 addends.We hypothesize that the model emerges Implicit Discrete State Representations (IDSRs) within its hidden states and performs symbolic calculations internally.To test this hypothesis, we design a sequence of experiments that look into the hidden states.Specifically, we first confirm that IDSRs exist.Then, we provide interesting observations about the formation of IDSRs from layer, digit, and sequence perspectives.Finally, we confirm that models indeed use IDSRs to produce the final answers.However, we also discover that these state representations are far from lossless in current open-sourced models, leading to inaccuracies in their final performance.Our work presents a novel exploration of LLMs' symbolic calculation abilities and the underlying mechanisms.</p>
<p>Introduction</p>
<p>LLMs have demonstrated remarkable performance in a variety of fields (Achiam et al., 2023;Touvron et al., 2023a), including natural language understanding and generation (Zhao et al., 2023), code generation (Chen et al., 2021;Nijkamp et al., 2022;Li et al., 2023b), and mathematical problemsolving (Hendrycks et al., 2021).These abilities emerge as the model scales.</p>
<p>In this study, we dive into another intriguing emergent capability: the ability of LLMs to perform arithmetic calculations, particularly consec-utive additions directly, without relying on chainof-thought reasoning.For example, given the question: "Please directly give me the answer to 17 + 38 + 32 + 87 + 47 + 28 + 17 + 21 + 53 + 15 + 18 + 76", a SOTA LLM can directly produce the correct answer "449" without producing any intermediate tokens.This phenomenon warrants investigation for two principal reasons.Firstly, it is unlikely that models were trained on such consecutive addition data, as it exerts negligible influence on overall performance across general domains and benchmarks (Wang et al., 2021).This phenomenon likely emerges naturally during the scaling process and presents a more meaningful study subject compared to tasks that may have more intricate relations with memorizing training data.Secondly, the simplicity of this phenomenon renders it an ideal candidate for interpretability research, potentially serving as a foundational step in uncovering the internal mechanisms underlying LLMs in performing intrinsic consecutive reasoning.</p>
<p>Prior research on the interpretability of models performing mathematical tasks focuses primarily on binary arithmetic operations (Zhu et al., 2024).However, this body of work fails to explain the formation of discrete state representations within the hidden layers of these models.</p>
<p>In this paper, we propose a central hypothesis to elucidate the emergent capability of implicit sequential computation: LLMs inherently track discrete states.By forming Implicit Discrete State Representations (IDSRs) that encapsulate intermediate results, LLMs can leverage these precomputed intermediate results for subsequent use, thereby preventing the necessity for intricate computations in the final step.</p>
<p>To validate this hypothesis, we construct a synthetic dataset of consecutive addition problems and employ probing methods to examine the existence of IDSRs in hidden states across various LLMs.Upon confirming its existence, we further investi-gate the properties and formation of IDSRs, and demonstrate its formation through digit-wise, layerwise, and sequence-wise perspectives, and provide noteworthy observations of distinct layer functionalities.From a digit-wise perspective, IDSRs form independently and sequentially, beginning with the lowermost digit.From the layer-wise level, a sharp transition from shallow semantic computation to semantic understanding occurs around layer 10, and a shift from linearity to non-linearity arises in the later model layers.From a sequence-wise perspective, information encoded in IDSRs is propagated along the sequence for sequential utilization.Finally, we confirm that the model utilizes IDSRs to produce the final result rather than computing using all preceding numbers simultaneously.This investigation provides insight into the multi-step reasoning and state-tracking abilities of LLMs (Singh et al., 2024;Li et al., 2023a).</p>
<p>Related Work</p>
<p>LLMs' State Tracking Abilities.Language models are exhibiting increasingly mature abilities to perform arithmetic tasks, both open-sourced (Shao et al., 2024;Jiang et al., 2023;Bai et al., 2023) and close-sourced models (Achiam et al., 2023;OpenAI, 2024;Team et al., 2023;Anthropic, 2024) are excelling at a variety of mathematical benchmarks, ranging from elementary to Olympic difficulty levels (Hendrycks et al., 2021;Cobbe et al., 2021;Chen et al., 2023;Li et al., 2024).</p>
<p>Other abilities that are discussed a lot are LMs' state encoding and tracking abilities.Li et al. and Nanda et al. investigate the existence of nonlinguistic state representations in board game settings, while Li et al. find that model representations also encode entity states in the process of textual tasks.Taking this problem further, Kim and Schuster show that models perform non-trivial state tracking given specific textual tasks.However, whether LMs track discrete states during arithmetic tasks still remains an open question.</p>
<p>Interpretability of LLMs' Arithmetic Abilities.</p>
<p>The inner workings of LMs in performing arithmetic and reasoning tasks are under-explored.Current literature suggests that neurons and layers inside LLMs may serve as feature extractors, extracting latent properties from inputs and passing them through layers (Mikolov et al., 2013;Bau et al., 2020;Belinkov, 2022;Geva et al., 2020;Burns et al., 2022;Gurnee et al., 2023).</p>
<p>Building on this idea, recent work demonstrates that hidden states during inference contain representations relevant to future tokens (Nostalgebraist, 2020;Belrose et al., 2023;Pal et al., 2023;Wu et al., 2024).This insight underpins our research, in which we prove the existence and utilization of implicit representations in LMs.</p>
<p>Previous analyses have also examined the arithmetic capabilities of LMs.Stolfo et al. identify that LMs employ MLPs and attention heads at different stages of arithmetic reasoning.</p>
<p>Broader Interpretability of LLMs.Multiple paradigm has emerged to investigate the working mechanisms of LLMs.Among them, the most famous ones are mechanistic interpretability (Conmy et al., 2023;Elhage et al., 2021) and representation engineering (Zou et al., 2023).</p>
<p>Mechanistic interpretability proposes to check the neuron-level activations and understand the functioning circuits inside LLMs.From the mechanistic interpretability perspective, mathematical tasks have been widely used as a tool because of their simplicity and clarity.For example, it is used to discover grokking in Varma et al. (2023) and understand double descent and emergent abilities in Huang et al. (2024).However, these works do not focus on the mathematical ability that emerges in SOTA LLMs.</p>
<p>Representation engineering is another pivotal approach in model interpretability, emphasizing the holistic feature representations within model layers (Li et al., 2021;Zou et al., 2023).This approach facilitates behavioral monitoring and performance modification (Zhang et al., 2024;Li et al., 2023a).However, it is still underdeveloped in practical applications.A simple form of representation engineering is the widely used approach "probing" (Alain and Bengio, 2016;Hewitt and Liang, 2019;Pimentel et al., 2020;Belinkov, 2022;Hernandez et al., 2023).Probing involves using auxiliary models, usually with simple structures, to make classifications.Research in this field extends to specific scenarios.Li et al. and Nanda et al. examine board game contexts, yielding divergent conclusions regarding the linearity of hidden states.Yang et al. explores event reasoning, demonstrating that significant reasoning predominantly occurs in the initial inference step and scales with model size.Few studies, however, critically assess the arithmetic capabilities of LMs.Some examine neuron activations only (Stolfo et al., 2023)  focus on simple calculations without comprehensively considering model layers (Zhu et al., 2024).This paper also studies the LLMs' interpretability with probing techniques but focuses on the unique perspective of IDSR.</p>
<p>Emergence of Implicit Computation</p>
<p>In this section, we confirm the emergent abilities of implicit computation using a variety of both opensource and closed-source Large Language Models (LLMs).</p>
<p>Problem Statement.We employ implicit consecutive addition as the representative task.In this context, the model is tasked with delivering the sum of an extended sequence of additions directly.An example prompt is provided below:</p>
<p>Please directly give me the answer to 17 + 38 + 32 + 87 + 47 + 28 + 17 + 21 + 53 + 15 + 18 + 76.</p>
<p>There are three reasons why the ability to solve such a task might indicate the formation of discrete state representations:</p>
<p>1.This capability is unlikely to be a result of memorizing existing training data, as storing the results of calculations necessitates a parameter space of O(99 L ).</p>
<ol>
<li>
<p>Direct optimization of this task during training is unlikely.As Goodhart's law (Strathern, 1997) suggests, "When a measure becomes a target, it ceases to be a good measure."Consecutive addition offers minimal practical performance benefits, rendering it an unlikely optimization target.Consequently, this capability may genuinely arise from large-scale unsupervised training.</p>
</li>
<li>
<p>Each computational step is relatively simple.We exclude addition involving four-digit or more due to its increased single-step complexity, which complicates tracing implicit computation because of single-step errors.</p>
</li>
</ol>
<p>To ensure that models that are only accessible through API calls do not rely on tools such as calculators, we manually verify that there is at least one addition count where the model has less than a 100% probability of yielding the correct answer.Additionally, we ensure that the models do not utilize explicit chain-of-thought reasoning through prompt engineering.</p>
<p>Specifically, we evaluate the exact accuracy of the predicted answers against the ground truth for different models directly performing consecutive addition of varying lengths from 2 to 14.</p>
<p>We include the following LLMs in our analysis: Llama2-7B (Touvron et al., 2023b), MiniCPM-2B (Hu et al., 2024), Mistral-7B (Jiang et al., 2023), Zephyr-7B (Tunstall et al., 2023), DeepSeek-67B (Bi et al., 2024), and the Qwen series with different sizes (Bai et al., 2023).For closedsource LLMs, we consider GPT-3.5 (Brown et al., 2020), GPT-4 (Achiam et al., 2023), Claude3-Sonnet, Claude3-Opus (Anthropic, 2024), andGPT4-O (OpenAI, 2024).</p>
<p>As illustrated in Figure 1, there exists a strong correlation between performance and model size.Smaller models achieve passable accuracy on the addition of two or three two-digit numbers, but their accuracy rapidly deteriorates to near zero when the length of the sequence reaches five.Larger models, however, maintain accuracies above
$GGLWLRQ&amp;RXQW $FFXUDF\ $FFXUDF\RI4ZHQ6HULHV3HUIRUPLQJ&amp;RQVHFXWLYH'LJLW$GGLWLRQ 4ZHQ% 4ZHQ% 4ZHQ% 4ZHQ%
Figure 3: Accuracies of Qwen Series Performing Consecutive 2-Digit Addition 50% for sequences of up to five numbers and demonstrate non-zero performance for sequences of even more numbers, demonstrating the fast "emergence" of this capability.</p>
<p>The "emergence" of this capability becomes most prominent when models encounter consecutive addition problems involving more than eight addends.To illustrate this phenomenon, Figure 2 presents the accuracies of both open-source and closed-source models performing direct addition with ten addends.It is evident that larger and more advanced closed-source models exhibit significantly higher task accuracies in an emergent manner.</p>
<p>To conduct a comprehensive analysis of the correlation between model size and performance, we examine the Qwen model series, including models with sizes of 72B, 14B, 7B, and 4B, as illustrated in Figure 3.The results indicate a distinct enhancement in performance proportional to the increase in model size, especially noticeable for sequence lengths ranging from three to six numbers.</p>
<p>Analysis Methodology</p>
<p>Given the remarkable ability of models to directly yield calculation outcomes, we hypothesize that these models form Implicit Discrete State Representations (IDSRs) of intermediate results.For example, consider the formula 13 + 24 + 41 =.</p>
<p>We propose that the most plausible mechanism for models to complete this calculation in a single pass is to generate an IDSR of 37 (the result of 13 + 24) at the second "+" token, which would subsequently be utilized for the next step of computation (i.e., addition with 41).</p>
<p>To thoroughly test and analyze this hypothesis, we propose and investigate the following research questions: We construct a straight-forward dataset of consecutive addition and subtraction problems with different length, addend digits and prompts.</p>
<p>Our question prompts are divided into three categories, respectively formatted as in Table 1, where i ranges from 2 to 14, and {x i } are positive integers with the same number of digits ranging from 1 to 3. We ensure the probed sum maintains the same number of digits as the addends to enhance digit probing consistency.Prompts are chosen with a diversity of semantics to demonstrate the influence of context on IDSRs tracking.</p>
<p>Type Expression
Addition {x 0 } + {x 1 } + ... + {x i−1 } = Subtraction {x 0 }+...+{x i−2 }−{x i−1 } = Prompting {Prompt}, {x 0 } + {x 1 } + ... + {x i−1 } = Table 1: Dataset Expressions
The dataset consists of 131,300 questions, as shown in Table 2. Questions are designed to ensure that expected answers follow a uniform distribution within their respective ranges, thereby eliminating probability bias and facilitating unbiased probe learning.The dataset is partitioned into training, validation, and test sets following an 80/10/10% split for probing, respectively.</p>
<p>Hidden States</p>
<p>We prompt the model to answer dataset questions directly.During inference, we retrieve the hidden state H i,j corresponding to the j th token of the input sequence from layer i of the model.</p>
<p>In our experiments, we exclusively extract the hidden states corresponding to the +, -, and = tokens for probing.This ensures that IDSRs are most prominent and unbiased.Extracting IDSRs from tokens representing addends would incorporate representations of the addends themselves, introducing  non-uniform bias and compromising the probing process.</p>
<p>Classification Probes</p>
<p>Previous work has proven the abilities of probes on a wide variety of classification tasks.In our work, we utilize a multi-layer perceptron with one hidden layer to perform classification.Specifically, the probing network is as follows:
P d i,j = Softmax(σ(W 1 H i,j )W 2 )(1)
where P d i,j is the probing prediction of the d-th digit of the IDSR, and W 1 ∈ R dm×d h and W 2 ∈ R d h ×do being the perceptron's model weights, d m and d h being the dimension of the model and the perceptron's hidden states respectively.d output is set to 10 as the probes are expected to predict a digit from 0 to 9, and d h is set to √ d m d o .In our experimental setup, three distinct types of probes are utilized: a multi-layer perceptron with two different hidden layer sizes and a single-layer perceptron.The respective parameter counts for each model type are detailed in Table 3.</p>
<p>For each experimental setting, probing models are trained on eight 80G A100 GPUs for a period ranging from 240 to 720 epochs.The duration depends on the specific formulas used as input and the number of epochs required for the model's losses to converge.</p>
<p>The learning rate is set to 1 × 10 −3 , employing a stochastic gradient descent (SGD) optimizer.The model is optimized based on cross-entropy loss.</p>
<p>Metrics</p>
<p>For the assessment of model capabilities in performing consecutive addition, we employ exact accuracy as our primary metric (EA, the ratio of the exact matches between the model output and the ground truth to the total number of questions).</p>
<p>Perceptron Model Number of Parameters</p>
<p>Multi-Layer 829,400</p>
<p>Multi-Layer (Bottle-Necked) 81,920</p>
<p>Single-Layer 40,960 To evaluate the classification probes, we compute the exact accuracy for each individual digit (IDA) as well as the overall exact accuracy (OEA, which considers a match only when all digits are predicted correctly).</p>
<p>Models Chosen</p>
<p>For our experimental setup, we select Deepseek-67B (Bi et al., 2024) and Qwen series models (4B, 7B, 14B, and 72B) (Bai et al., 2023) as representatives of open-source models.These models are utilized in their original form, without any fine-tuning or parameter modification.We aim to evaluate and compare the proficiency in executing consecutive addition tasks across a diverse range of models varying in size and capabilities.Special emphasis is placed on the Qwen-72B model to conduct an in-depth analysis of representation engineering and IDSRs' properties.</p>
<p>Existence and Properties of IDSRs</p>
<p>In this section, we present evidence of IDSRs regarding RQ1 and RQ2.To demonstrate the existence of IDSRs in hidden states during inference, we design a series of probing prediction experiments with two levels of difficulty: Whole Number Probing and Digit-wise Probing.</p>
<p>Whole Numbers Probing</p>
<p>In this set of experiments, we train probes to predict the results as whole numbers from 10 possible sums.We probe different token positions across layers to investigate the existence of IDSRs' transference along the formula.The results, illustrated in Figure 4, indicate that prediction accuracies significantly exceed random chance in all scenarios, demonstrating the existence of IDSR.</p>
<p>However, the process of forming IDSRs is far from lossless.The maximum prediction accuracies for the second to fifth addition signs and the final equal sign are 100%, 99%, 74%, 62%, and 37% respectively, indicating substantial data and resolution loss as IDSRs are passed along the formula during inference.We hypothesize that reducing this error margin in the transference of IDSRs would enhance the capability of LLMs.This will be explored in future research.</p>
<p>Interesting trends across layers can also be observed in Figure 4, which will be discussed and analyzed in detail in Section 6.</p>
<p>Digit-wise Probing</p>
<p>To investigate whether digits exist separately in the IDSRs, we employ multiple probe models to predict the respective digits of the number in question.For this experiment, we select formulas with 3-digit sums, therefore three digit-classification probes are used.The range of possible sums for the n th addition/equal sign increases significantly, from 10 in the previous experiment setting to max{999, 99n} − min{100, 10n}, an increase of 10 to 40 times.We consider a prediction to be correct only when all three-digit probes make accurate predictions on a test data item.</p>
<p>As depicted in Figure 5, probing accuracies using tokens from the first ten layers and the second addition sign from the later layers remain high.However, it is noteworthy that after significantly increasing prediction difficulty, the ability of probes to make exact predictions after the second addition sign experiences a sharp decline.This indicates that models struggle to produce high-resolution IDSRs consecutively.</p>
<p>Are IDSRs Linear?</p>
<p>To gain a concrete understanding of the IDSRs, we first examine its linearity.Beyond the original probing model with hidden size √ d m d o , we construct 1) a smaller bottle-necked probing model with hidden size 10, as well as 2) a simpler single-layer perceptron utilizing a softmax activation function.
P d i,j = Softmax(W 1 H i,j )(2)
As illustrated in Figure 6, layers 0 to 65 exhibit only minor accuracy drops with reduced hidden size, whereas layers 65 to 79 experience a significant reduction.</p>
<p>Notably, opposing accuracy trends appear in later layers for multi-layer and single-layer perceptrons.Between layers 50 and 65, accuracies for single-layer perceptrons drop to nearly zero, followed by a sharp increase for multi-layer perceptrons.This implies that layers 0 to 50 contain linear IDSRs, likely directions in the latent space.In contrast, layers 50 to 65 transit from linear to</p>
<p>Formation and Utilization of IDSRs</p>
<p>In addition to analyzing the specific properties of IDSRs, we extend our study to overall formations.In this section, we identify patterns exhibited during inference at the digit level, sequence level, and layer level, revealing the inner mechanisms of consecutive addition and multi-hop reasoning for LMs (RQ3).Following the formation analysis, we examine the utilization of such states (RQ4).</p>
<p>Digit-Level Formation</p>
<p>We investigate the second addition operation within three-digit addition tasks and derive two critical observations.First, the product of exact accuracies for the individual digits equals the overall exact accuracy, implying that models establish independent IDSRs.Second, as depicted in Figure 7, the sequence in which digit prediction accuracies surpass random chance, as determined by statistical measures and annotated in the figure, follows an ascending digit order.This pattern mirrors the order humans use for digit-by-digit calculations, suggesting that models perform multi-digit addition through a series of consecutive single-digit additions.</p>
<p>Sequence-Level Formation</p>
<p>The representation resolution of earlier addition sign tokens, as indicated by prediction accuracies, improves at earlier stages of the inference pass.The order of this resolution enhancement in</p>
<p>Layer-Level Formation</p>
<p>As depicted in Figures 4, 5, and 6, an abrupt peak in IDSRs' resolution appears within the first ten layers for both models.Beyond this point, the resolution reinitializes from near non-existent levels.</p>
<p>We propose the hypothesis that the first ten layers employ a different mechanism from the later layers, particularly in multi-step reasoning tasks such as consecutive addition.The first ten layers, termed "shallow-semantic layers", generate direct representations of arithmetic content regardless of the specific task.Conversely, the later layers, termed "semantic layers", incorporate task context, redoing the formation of the IDSRs in the process.</p>
<p>Utilizing the "subtraction" and "prompting" tasks discussed in Section 4.1.1,we conduct two sets of experiments to demonstrate the existence of shallow-semantic and semantic layers.</p>
<p>Shallow-semantic Layers.In the first set of experiments, we use subtraction formulas (as mentioned in Section 4.1.1).Predictions are made on the second addition sign, and accuracies are shown in Figure 8.</p>
<p>We can see clearly that the "subtraction" task does not change the probing result significantly.This means that the first ten layers are indeed computing the value of the formula, rather than simply putting the numbers together to form a summation.</p>
<p>Semantic Layers.For our second experiment, we use formulas with different prompts (as mentioned in Section 4.1.1).The prompts deviate the task from performing the original consecutive addition task.For example, the prompt in Figure 9 states, "Ignore the following formula and answer As shown in Figure 9, after the disruptive prompt, the maximum prediction accuracies in the first ten layers remain unaffected.However, accuracies in the later layers significantly decrease.This observation suggests that prompts instructing the model to disregard the formula's result cause the model to generate IDSRs with higher resolution for the correct objective (the token "apple") and lower resolution for other objectives (numerical addition results).</p>
<p>Shallow-semantic Layers are More Accurate.As depicted in Figure 4, the prediction accuracies using the earlier layers exhibit remarkable stability across different token positions.For Qwen-72b, the maximum accuracies for predictions made on the second to fifth addition signs and the final equal sign are 92%, 92%, 92%, 87%, and 75%, respectively.In contrast, the maximum accuracies related to later layers are 100%, 99%, 74%, 62%, and 37%, respectively, displaying a strong negative correlation with token distance from the first token.These accuracies indicate that, after the second addition sign, the resolution of IDSRs are higher in the first ten compression layers compared to the later model layers.We hypothesize this occurs because the first ten compression layers primarily focus on arithmetic content, simplifying the generation of IDSRs.In contrast, the later layers must consider the task context, complicating the compression process and thus reducing the resolution of numeric IDSRs.</p>
<p>Utilization of IDSRs</p>
<p>Upon verifying the existence of IDSRs, we subsequently address whether the model actively leverages it to generate the final response.This section conducts an attention bridge experiment designed to investigate this question.Attention Bridge.Given a question with a token length of l, we construct an attention mask M l,i , as depicted in Figure 10, to mask the first i tokens of the question from the subsequent tokens.We term the (i + 1) th token the Attention Bridge, as the IDSRs formed on this token serve as the sole conduit for information relay between the prefix and the suffix.This enables verification of LLMs' utilization of IDSRs, rather than re-attending the prefix and performing the calculation at the final token position at once.Specifically, we set the second addition sign (or the equal sign, in cases with only two numbers) as the Attention Bridge through which IDSRs pass.We then test Qwen-72B's ability to provide exact answers to consecutive 1-digit additions involving 2 to 10 numbers under this setting.</p>
<p>Results.As seen in Figure 11, despite being unable to directly observe the first two numbers, Qwen-72b demonstrates remarkable ability to perform calculations through the IDSRs passed via the second addition sign.This suggests that LLMs indeed utilize generated IDSRs to make multi-hop inferences, such as consecutive addition.However, a significant drop in accuracy compared to the baseline is observed.We hypothesize that this occurs because models are not explicitly trained to make inferences using IDSRs only and are unaccustomed to abrupt changes in the attention mask.With slight modifications to the training process, models might better utilize IDSRs to perform multi-hop inferences.</p>
<p>Conclusion</p>
<p>In this work, we report the emergent ability of models to perform implicit consecutive addition.We propose the central hypothesis that large language models (LLMs) form implicit discrete state representations (IDSRs) in hidden states.A series of experiments are designed to prove the existence of IDSR, and to demonstrate its properties and formation.We also confirm that models utilize IDSRs to generate final answers.Our work aims to pave the way for further investigations into model interpretability and enhancing model capabilities.</p>
<p>Ethical Considerations</p>
<p>Dual Use.Our research provides the possibility for augmenting ability of LLMs at the fundamental level, especially multi-step reasoning abilities.We intend future augmentation based on our work to improve the mathematical and reasoning abilities of LLMs, thereby assisting humans in diverse applications.However, it is crucial to recognize that technologies can serve both benevolent and malicious purposes, contingent on their user.Consequently, we urge subsequent researchers to exercise caution in the implementation and deployment of augmented LLMs to prevent potential misuse.</p>
<p>Data Bias.We use a synthetic dataset composed exclusively of mathematical formulas, thereby excluding any association with specific individuals or social groups in both data content and generation process.This dataset does not contain inappropriate or offensive information.Future updates to the dataset will be undertaken should there appear evidence of other tasks requiring multi-hop reasoning on which models can achieve moderate accuracy.</p>
<p>Limitations</p>
<p>We find the task diversity and model diversity of our experiments unsatisfactory.</p>
<p>Task Diversity.Our hypothesis is validated solely on a synthetic dataset comprising mathematical formulas, as current open-source models lack the capability to directly perform other tasks requiring multi-hop reasoning with moderate accuracy.Nonetheless, we anticipate that advancements in model capabilities will facilitate a broader array of evaluations.</p>
<p>Model Diversity.Interpretability analysis necessitates the extraction of hidden states, compelling the use of open-source models.The majority of our experiments utilize Qwen-72b, the highest-performing open-source model available, despite its notable capability gap compared to SOTA closed-source models.Our observations reveal a clear correlation between model capability and IDSRs' resolution.We anticipate that additional experiments with future, more advanced open-source models will further substantiate our hypothesis.</p>
<p>Future Work</p>
<p>In hindsight, we also propose various possible aspects for future exploration:</p>
<p>Influence factors.Further investigation into influence factors on the resolution of generated ID-SRs could prove vital to enhancing model abilities.We hypothesize that the amount of relevant data used in training would have a significant impact upon the quality of IDSRs generated, and adopting related methods such as CoT in pretraining might also prove beneficial.</p>
<p>Formation Interpretability.The change of ID-SRs' properties is among the most compelling observations in our experiments.Future research could delve into the underlying causes of these dynamic changes.</p>
<p>Scalability.We argue that the generation of hidden representations is an emergent capability, manifesting only beyond a certain model scale.Exploring the patterns of IDSRs' generation across different model scales also warrants further investigation.</p>
<p>Application.Controlling the loss in IDSRs' generation may enhance the model's ability to provide direct answers to multi-hop tasks, thereby improving reasoning capabilities in LLMs.</p>
<p>Figure</p>
<p>Figure 1: Accuracies of Different Models Performing Consecutive 2-Digit Addition</p>
<p>RQ1:</p>
<p>Do IDSRs really exist?RQ2: What are IDSRs' properties?RQ3: How do the IDSRs' form?RQ4: How do models utilize IDSRs?</p>
<p>Figure 4 :
4
Figure 4: Accuracies of Whole Number Probing Predictions</p>
<p>Figure 6 :
6
Figure 5: Accuracies of By-Digit Probing Predictions</p>
<p>Figure 7 :
7
Figure 7: Digit Accuracies of By-Digit Probing Predictions</p>
<p>Figure 8 :
8
Figure 8: Accuracies of By-Digit Probing Predictions with Subtraction Formulas</p>
<p>Figure 9 :
9
Figure 9: Accuracies of Probing Predictions with Ignoring Prompt and Baseline</p>
<p>FigureFigure 11 :
11
Figure 10: Attention Mask Demonstration</p>
<p>Table 2 :
2
Dataset Distribution</p>
<p>Table 3 :
3
Probe Model Sizes</p>
<p>. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Anadkat, 2023Gpt-4 technical report</p>
<p>Understanding intermediate layers using linear classifier probes. Guillaume Alain, Yoshua Bengio, arXiv:1610.016442016arXiv preprint</p>
<p>Introducing the next generation of claude. Anthropic, 2024</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, ArXiv, abs/2309.16609Qwen technical report. Xiaohuan Zhou, and Tianhang Zhu2023</p>
<p>Understanding the role of individual units in a deep neural network. David Bau, Jun-Yan Zhu, Hendrik Strobelt, Àgata Lapedriza, Bolei Zhou, Antonio Torralba, Proceedings of the National Academy of Sciences. 1172020</p>
<p>Probing classifiers: Promises, shortcomings, and advances. Yonatan Belinkov, Computational Linguistics. 4812022</p>
<p>Eliciting latent predictions from transformers with the tuned lens. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor V Ostrovsky, Lev Mckinney, Stella Biderman, Jacob Steinhardt, ArXiv, abs/2303.081122023</p>
<p>. Deepseek-Ai Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wen-Hui Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y K Li, Wenfeng Liang, Fangyun Lin, A X Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Jun-Mei Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Min Tang, Bing-Li Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Yu Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yi Xiong, Hanwei Xu, Ronald X Xu, Yanhong Xu, Dejian Yang, Yu Mei You, Shuiping Yu, Bo Xin Yuan Yu, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghu Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhang, Yao Zhao, Shangyan Zhao, Zhou, ArXiv, abs/2401.02954Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. 2024. Deepseek llm: Scaling opensource language models with longtermism</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, ArXiv, abs/2005.14165Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020</p>
<p>Discovering latent knowledge in language models without supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, ArXiv, abs/2212.038272022</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W Cummings, Matthias Plappert, Fotios Chantzis, ArXiv, abs/2107.03374Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew CarrEvaluating large language models trained on code</p>
<p>Theoremqa: A theorem-driven question answering dataset. Wenhu Chen, Ming Yin, Max W F Ku, Yixin Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi Wang, Pan Lu, ArXiv, abs/2305.125242023</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, ArXiv, abs/2110.141682021</p>
<p>Towards automated circuit discovery for mechanistic interpretability. Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, Adrià Garriga-Alonso, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A mathematical framework for transformer circuits. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack ClarkTransformer Circuits Thread</p>
<p>Transformer feed-forward layers are key-value memories. R Mor Geva, Jonathan Schuster, Omer Berant, Levy, ArXiv, abs/2012.149132020</p>
<p>Finding neurons in a haystack: Case studies with sparse probing. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, Dimitris Bertsimas, ArXiv, abs/2305.016102023</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, Jacob Steinhardt, ArXiv, abs/2103.038742021</p>
<p>Linearity of relation decoding in transformer language models. Evan Hernandez, Arnab Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, David Bau, ArXiv, abs/2308.091242023</p>
<p>Designing and interpreting probes with control tasks. John Hewitt, Percy Liang, ArXiv, abs/1909.033682019</p>
<p>Minicpm: Unveiling the potential of small language models with scalable training strategies. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chaochao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun, 2024</p>
<p>Unified view of grokking, double descent and emergent abilities: A perspective from circuits competition. Yufei Huang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun, arXiv:2402.151752024arXiv preprint</p>
<p>Devendra Singh Chaplot, Diego de Las Casas. Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, ArXiv, abs/2310.06825Mistral 7b. Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2023</p>
<p>Entity tracking in language models. Najoung Kim, Sebastian Schuster, ArXiv, abs/2305.023632023</p>
<p>Belinda Z Li, Maxwell Nye, Jacob Andreas, arXiv:2106.00737Implicit representations of meaning in neural language models. 2021arXiv preprint</p>
<p>Emergent world representations: Exploring a sequence model trained on a synthetic task. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi'egas, Hanspeter Pfister, Martin Wattenberg, ArXiv, abs/2210.133822022</p>
<p>Gsm-plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. Kenneth Li, Oam Patel, Fernanda Vi'egas, Hans-Rüdiger Pfister, Martin Wattenberg ; Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi, ArXiv, abs/2402.192552023a. 2024Inference-time intervention: Eliciting truthful answers from a language model</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Sankalp Siva, Dmitry Patel, Marco Abulkhanov, Manan Zocca, Zhihan Dey, Nourhan Zhang, Urvashi Fahmy, W Bhattacharyya, Swayam Yu, Sasha Singh, Paulo Luccioni, Maxim Villegas, Fedor Kunakov, Manuel Zhdanov, Tony Romero, Nadav Lee, Jennifer Timor, Claire Ding, Hailey Schlesinger, M Schoelkopf ; Sean, Thomas Hughes, Arjun Wolf, Guha, Yacine Jernite, Carlos Muñoz Ferrandis. Jana Ebert, Tri Dao, Mayank Mishra, Alexander Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. 2023b. Starcoder: may the source be with you! ArXiv, abs/2305.06161</p>
<p>Linguistic regularities in continuous space word representations. Tomas Mikolov, Wen Tau Yih, Geoffrey Zweig, North American Chapter. the Association for Computational Linguistics2013</p>
<p>Emergent linear representations in world models of self-supervised sequence models. Neel Nanda, Andrew Lee, Martin Wattenberg, ArXiv, abs/2309.009412023</p>
<p>Codegen: An open large language model for code with multi-turn program synthesis. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Haiquan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, International Conference on Learning Representations. 2022</p>
<p>Interpreting gpt: The logit lens. OpenAI. Nostalgebraist, 2020. 2024</p>
<p>Future lens: Anticipating subsequent tokens from a single hidden state. Koyena Pal, Jiuding Sun, Andrew Yuan, Byron C Wallace, David Bau, ArXiv, abs/2311.048972023</p>
<p>Information-theoretic probing for linguistic structure. Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, Ryan Cotterell, ArXiv, abs/2004.030612020</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, R X Xu, Jun-Mei Song, Mingchuan Zhang, Y K Li, Yu Wu, Daya Guo, ArXiv, abs/2402.033002024</p>
<p>Rethinking interpretability in the era of large language models. Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, Jianfeng Gao, ArXiv, abs/2402.017612024</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>improving ratings': audit in the british university system. Marilyn Strathern, 10.1002/(sici)1234-981x(199707)5:3&lt;305::aid-euro184&gt;3.0.co;2-4European Review. 531997</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, ArXiv, abs/2302.13971</p>
<p>Hugo Touvron, Louis Martin, Kevin R Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M Bikel, Lukas Blecher, Cantón Cristian, Moya Ferrer, Guillem Chen, David Cucurull, Jude Esiobu, Jeremy Fernandes, Wenyin Fu, Brian Fu, Cynthia Fuller, Vedanuj Gao, Naman Goswami, Anthony S Goyal, Saghar Hartshorn, Rui Hosseini, Hakan Hou, Marcin Inan, Viktor Kardas, Madian Kerkez, Isabel M Khabsa, A V Kloumann, Punit Korenev, Marie-Anne Singh Koura, Thibaut Lachaux, Jenya Lavril, Diana Lee, Yinghai Liskovich, Yuning Lu, Xavier Mao, Todor Martinet, Pushkar Mihaylov, Igor Mishra, Yixin Molybog, Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, R Smith, Xia Subramanian, Binh Tan, Ross Tang, Adina Taylor, Jian Williams, Puxin Xiang Kuan, Zhengxu Xu, Iliyan Yan, Zarov, ArXiv, abs/2307.09288Yuchen Zhang, Angela Fan. Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Zephyr: Direct distillation of lm alignment. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M Rush, Thomas Wolf, ArXiv, abs/2310.169442023</p>
<p>Vikrant Varma, Rohin Shah, Zachary Kenton, János Kramár, Ramana Kumar, arXiv:2309.02390Explaining grokking through circuit efficiency. 2023arXiv preprint</p>
<p>Generalizing to unseen domains: A survey on domain generalization. Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, IEEE Transactions on Knowledge and Data Engineering. 352021</p>
<p>Wilson Wu, John X Morris, Lionel Levine, arXiv:2404.00859Do language models plan ahead for future tokens?. 2024arXiv preprint</p>
<p>Do large language models latently perform multi-hop reasoning?. Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel, ArXiv, abs/2402.168372024</p>
<p>Towards general conceptual model editing via adversarial representation engineering. Yihao Zhang, Zeming Wei, Jun Sun, Meng Sun, 2024</p>
<p>A survey of large language models. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Z Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jianyun Liu, Ji Nie, Wen Rong, ArXiv, abs/2303.182232023</p>
<p>Language models understand numbers, at least partially. Fangwei Zhu, Damai Dai, Zhifang Sui, ArXiv, abs/2401.037352024</p>
<p>Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J Byun, Zifan Wang, Alex Troy Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, Zico Kolter, Dan Hendrycks, ArXiv, abs/2310.01405Representation engineering: A top-down approach to ai transparency. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
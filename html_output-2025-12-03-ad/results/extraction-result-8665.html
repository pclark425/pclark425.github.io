<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8665 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8665</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8665</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-263135519</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.16235v1.pdf" target="_blank">Language models in molecular discovery</a></p>
                <p><strong>Paper Abstract:</strong> The success of language models, especially transformer-based architectures, has trickled into other domains giving rise to"scientific language models"that operate on small molecules, proteins or polymers. In chemistry, language models contribute to accelerating the molecule discovery cycle as evidenced by promising recent findings in early-stage drug discovery. Here, we review the role of language models in molecular discovery, underlining their strength in de novo drug design, property prediction and reaction chemistry. We highlight valuable open-source software assets thus lowering the entry barrier to the field of scientific language modeling. Last, we sketch a vision for future molecular design that combines a chatbot interface with access to computational chemistry tools. Our contribution serves as a valuable resource for researchers, chemists, and AI enthusiasts interested in understanding how language models can and will be used to accelerate chemical discovery.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8665.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8665.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGPT (Molecular GPT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-style decoder-only transformer adapted to chemical language (SMILES) for conditional molecule generation by concatenating SMILES tokens with property/scaffold condition vectors and trained on next-token prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molgpt: molecular generation using a transformer-decoder model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (GPT-style decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES corpora; SMILES tokens concatenated with a condition vector summarizing desired properties/scaffolds (as described in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular generation for drug discovery / library design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>autoregressive SMILES generation (decoder-only GPT) with conditional inputs (condition vector); can be coupled with reinforcement learning for property optimization</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Review reports ability to generate molecules conditioned on properties/scaffolds; no quantitative novelty metrics reported in this paper (e.g., % novel vs training set not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditions (property vectors and scaffolds) are concatenated to SMILES input to bias generation toward target properties; RL can further optimize property scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Common metrics referenced for generative modeling include validity, uniqueness, diversity and property scores (e.g., pIC50 optimization in cited works), but specific MolGPT benchmark numbers are not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported as an early application of GPT-like architectures to conditional molecule generation; capable of conditional SMILES generation and property optimization when combined with RL, but quantitative outcomes are not listed in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented as a competitive transformer-based approach; transformers' scalability and embedding quality highlighted relative to RNNs and VAEs, but no direct numeric comparison for MolGPT is provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>SMILES-based generation suffers from possible invalid strings and non-uniqueness; need for downstream virtual screening/synthesizability checks; no detailed MolGPT failure modes quantified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models in molecular discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8665.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8665.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer formulation that treats conditional sequence modeling as a regression problem allowing concurrent property prediction and conditional molecular generation by concatenating molecular tokens with property tokens and using alternating masking during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Regression transformer enables concurrent sequence regression and generation for molecular language modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (sequence-to-sequence / multitask masking/regression)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES sequences with associated property values (multi-task datasets); trained with mask-infilling/regression objective combining sequence and property tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>property-driven molecular design / de novo molecule generation tailored to specified properties</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompt/conditioning via concatenated property tokens and mask-infilling / regression-style training to jointly predict properties and generate sequences</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Enables property-conditioned generation; review does not report explicit novelty percentages or similarity metrics for generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Properties are explicitly included as tokens; training alternates masking to force the model to learn to predict property values and generate molecules consistent with them, providing direct conditioning for application-specific design.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Evaluated in source work using concurrent sequence regression/generation metrics and standard generative metrics (validity, property accuracy), but specific numbers are not reproduced in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Highlighted as a seminal multitask approach enabling simultaneous property prediction and conditional generation; used in GT4SD and chatbot demos to perform substructure-constrained, property-driven design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Characterized as enabling natural multitask modeling compared to separate property predictors plus generative models, improving control over property-driven generation; no quantitative head-to-head given here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Potential for property prediction/generation tradeoffs and need for careful masking/prompting; review notes necessity of downstream validation (virtual screening, synthesis planning) but no specific failure cases listed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models in molecular discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8665.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8665.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molecular Transformer (MT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive encoder-decoder Transformer applied to chemical reaction prediction (sequence-to-sequence) using reaction SMILES in a template-free, data-driven manner, able to represent stereochemistry and predict regio- and stereoselective outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (encoder-decoder / seq2seq)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Reaction SMILES datasets (data-driven training on reaction corpora); trained on reaction pairs (precursors → products) and variants for retrosynthesis (product → precursors).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>reaction outcome prediction, single-step retrosynthesis, multi-step retrosynthesis planning and synthesis-route planning for experimental execution</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>sequence-to-sequence translation between reactant/product SMILES; used autoregressively for forward prediction and retrosynthesis; integrated into retrosynthesis planners and robotic execution pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>MT is focused on predicting reaction outcomes and retrosynthetic precursors rather than de novo novel-targeted molecule design; review does not report novelty metrics for molecule generation via MT.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Enables controllable retrosynthesis (e.g., prompt-based disconnection), and was extended to enzymatic reactions via enzyme-class tokenization to tailor to biocatalysis applications.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Performance assessed on reaction prediction accuracy, uncertainty calibration, regio-/stereoselectivity metrics; success in predicting stereochemical outcomes and use in retrosynthesis benchmarks noted but numerical scores not listed here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Presented as the flagship architecture for RXN for Chemistry; successfully applied to forward reaction prediction, single-step retrosynthesis, multi-step retrosynthesis with hypergraph exploration and enzymatic reaction planning; also enabled extraction of synthesis actions for robotic platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared favorably to graph-based models for stereochemistry and regioselectivity due to sequence-based stereochemical representation; MT is template-free and data-driven, offering advantages in representing stereochemistry directly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Requires high-quality reaction data; downstream synthesis-action extraction and robotic execution pose additional challenges; masked-LM variants and prompt-based methods were introduced to improve controllability and diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models in molecular discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8665.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8665.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE (conditional VAE / RNN-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoder (conditional and RNN-based variants for molecular generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>VAEs learn continuous latent distributions from molecular sequences (e.g., SMILES) enabling interpolation and conditional sampling; RNN-based VAEs have been applied to generate catalysts and molecules by sampling latent space conditioned on desired attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular generative model based on conditional variational autoencoder for de novo molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Conditional VAE / RNN-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>variational autoencoder (encoder-decoder), often RNN-based for sequences</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES sequences and associated property/condition vectors (e.g., catalyst binding energies or other labels) used for conditional training.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular and catalyst design (including catalyst binding energy optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>learn latent distribution; sample and decode to generate novel molecules; condition by concatenating condition vectors to input and latent embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>VAEs enable interpolation and generation of novel molecules in latent space; the review does not provide quantitative novelty metrics (e.g., % unseen) for cited VAE works.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditional VAEs incorporate condition vectors (properties) during encoding/decoding to bias generation toward targeted attributes such as binding energy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Latent-space quality is assessed via downstream tasks (e.g., property prediction such as catalyst binding energy); generative metrics (validity, property alignment) are used but exact figures are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>VAEs remain a standard approach for learning smooth latent molecular spaces and conditional generation; specific applications include catalyst design where VAE latent space correlates with binding energy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>VAEs provide smoother latent interpolation compared to discrete sequence models (RNNs) but transformers have surpassed earlier architectures in embedding quality and scalability; hybrid and multimodal VAEs also exist.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Single-modality VAEs may be limited for multi-modal conditionalization; quality of decoding (valid SMILES) and property alignment depend on training data and conditioning strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models in molecular discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8665.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8665.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN SMILES generators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Neural Network (RNN) SMILES generators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early chemical language models that model SMILES sequentially with RNNs to generate molecule libraries auto-regressively; they can produce high proportions of unique and valid SMILES but have difficulty with counting ring token occurrences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating focused molecule libraries for drug discovery with recurrent neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN-based SMILES generator</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>recurrent neural network (RNN, LSTM/GRU variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES datasets (library molecules used in drug development / screening datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo library generation for drug discovery / virtual screening</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>autoregressive character/atom-level SMILES generation, often guided by external scoring functions for property optimization</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>RNNs can generate many unique and valid SMILES; specific novelty percentages are not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>External scoring functions (docking, property predictors) used to steer generation toward desired properties; also used for focused library generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity, uniqueness, diversity, and downstream property performance are typical metrics mentioned; review does not list per-paper numeric values.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RNNs were foundational for SMILES generation and remain used for library generation; however, they have limitations in syntax counting (ring closures) and are now often superseded by transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>RNNs generally predate transformers; transformers provide better scalability and embedding quality, though RNNs can still produce high validity/uniqueness in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Difficulty counting ring-opening/closing tokens can produce invalid SMILES; syntax validity issues motivate alternatives like SELFIES or robust tokenization and augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models in molecular discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8665.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8665.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multitask Text & Chemistry T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multitask Text and Chemistry T5 (prompt-based multimodal T5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based multitask T5 model that unifies natural language and chemical language tasks (generation, property prediction, reaction prediction) and demonstrates strong generalization across discovery workflows even at modest parameter counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unifying molecular and textual representations via multi-task language modelling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multitask Text and Chemistry T5 (T5Chem / Multimodal T5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder transformer (T5-style, promptable multitask)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>250M</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Combined corpora of natural language and chemical text/SMILES for multi-task pretraining (details in cited work); trained as a promptable multitasker.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>end-to-end molecular discovery workflows including natural-text → molecule generation → synthesis planning → synthesis protocol generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompt-based multitask generation that can take natural language prompts and produce molecules or reaction plans (in-context learning / prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>In cited re-discovery workflow, the model outperformed ChatGPT and Galactica on rediscovering a herbicide, but the review does not enumerate novel-molecule novelty metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Bridges natural language objectives and chemical generation enabling task-specific prompts (e.g., specify desired property or synthesis constraints) to produce targeted outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Benchmarked on contrived discovery workflow (rediscovering a known herbicide) and compared qualitatively/numerically to general LLMs; specific generative metrics not reproduced in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Despite being only 250M parameters, the multitask T5 variant outperformed general LLMs (ChatGPT, Galactica) on a contrived rediscovery pipeline, showing the value of specialized multimodal training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed general-purpose LLMs in the cited contrived discovery workflow, demonstrating that domain-specific multimodal foundation models can be more effective than large generic models for chemistry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>While promising, such models still require integration with downstream chemistry tools for validation and synthesis execution; the review emphasizes human-in-the-loop and RLHF for improved outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models in molecular discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8665.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8665.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaccMann RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaccMann RL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning based generative framework that produces de novo hit-like anticancer molecules conditioned on transcriptomic data (gene expression) to align generated molecules with cellular response profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaccMann RL : De novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaccMann RL</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>reinforcement learning (policy-gradient or RL-guided generative model) combined with chemical generative models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Molecular datasets linked with transcriptomic (gene expression) data; uses gene-expression-contextualized training to condition molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>anticancer drug discovery (de novo design conditioned on transcriptomic profiles)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>RL-driven de novo generation conditioned on transcriptomic data to propose molecules predicted to be active in specific cellular contexts</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Capable of producing hit-like anticancer molecules aligned to transcriptomic signatures; no explicit novelty percentages provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditions generation on gene expression signatures to tailor molecules for specific cellular states or cancer types, increasing application specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Evaluated using activity-hit likeness, predicted biological effect alignment, and other domain-specific metrics in the cited work; review does not reproduce numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Showcases that coupling generative models with biological context via RL produces candidate molecules with desired cellular-effect profiles; review highlights this as an example of context-driven molecule design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Represents a shift from purely property-driven generation toward biologically contextualized generation; direct performance comparisons not detailed in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Relies on quality of biological assays/transcriptomic links and predictive models; RL optimization can produce exploitation or mode collapse if not properly regularized; experimental validation remains necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models in molecular discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8665.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8665.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GFlowNets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Flow Networks (GFlowNets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of generative models trained to sample diverse modes proportional to a provided reward function, useful in chemistry for generating diverse candidate molecules that satisfy target property rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gflownet foundations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GFlowNets</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>flow-based generative policy networks (RL-inspired generative sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained with molecular construction/action spaces and reward functions derived from desired properties; uses molecule-building trajectories rather than plain SMILES corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>diverse candidate generation for molecular design (drug discovery / materials), particularly where diversity under reward constraints is desired</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>sequential generation of molecular graphs/SMILES via an RL-like objective that encourages sampling proportional to reward to yield diverse high-reward candidates</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Designed to produce highly diverse candidate sets for a given reward; the review does not provide explicit novelty or similarity statistics from cited GFlowNet applications.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Uses explicit reward functions tied to application-specific properties, enabling targeted generation while preserving diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Diversity under reward, reward attainment, and downstream property metrics (validity, synthesizability) are typical evaluation axes; no numeric values reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Included in GT4SD as a contemporary approach for generating diverse high-reward molecules; presented as complementary to other generative strategies (VAEs, transformers, RL).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>GFlowNets aim to improve diversity compared to pure RL or greedy optimization methods that may collapse to few modes; review highlights GFlowNets' strength conceptually but does not include empirical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Implementation complexity, definition of suitable reward functions, and need for accurate property oracles for reward evaluation; requires careful design to avoid generating unsynthesizable chemotypes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models in molecular discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8665.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8665.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELFIES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELF-referencING Embedded Strings (SELFIES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robust molecular string representation designed to guarantee chemically valid molecules upon decoding by enforcing valence and structural derivation rules; used as an alternative to SMILES to avoid invalid generations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-referencing embedded strings (selfies): A 100% robust molecular string representation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SELFIES (representation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>molecular string representation (encoding scheme) rather than a generative model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Representation applied to molecular datasets in place of SMILES to train generative models; not a model trained on data itself.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>any molecular generation task where validity of generated strings is critical (drug discovery, materials design)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>used as the tokenization/representation format for generative models to ensure valid molecules are produced upon decoding</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>SELFIES guarantees 100% syntactic validity of decoded strings, reducing invalid output rate; novelty relative to training data depends on generative model, not SELFIES itself.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Representation does not by itself ensure property-targeted generation but removes invalidity as a failure mode, simplifying downstream conditioning and optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity is the primary metric improved (100% validity guarantee by construction); tradeoffs include potential overly short strings as noted in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SELFIES is presented as a solution to the invalidity issues of SMILES in generative models; while guaranteeing validity, it can produce strings that may be too short or otherwise not useful without additional constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to SMILES, SELFIES removes invalid generation failures; the review notes that canonical SMILES with augmentation remain prevalent due to other tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Guarantee of syntactic validity can still yield molecules that are chemically trivial or not useful; representation may require additional constraints to ensure useful molecular complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models in molecular discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8665.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8665.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GT4SD (Generative Toolkit for Scientific Discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GT4SD - Generative Toolkit for Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source Python toolkit focused on enabling use, training, fine-tuning and distribution of state-of-the-art generative models for scientific discovery with strong support for chemical language models, benchmarks, and model registry/inference tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accelerating material design with the generative toolkit for scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GT4SD (toolkit)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>software toolkit / model registry and interfaces (supports transformers, diffusion models, GFlowNets, VAEs, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Provides access to many pre-trained models and 50+ property prediction endpoints; integrates with common molecular datasets and benchmarks (Moses, GuacaMol, MoleculeNet endpoints via hosted APIs).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>design of organic materials, de novo molecular generation, property-driven design for drug discovery and materials science</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>exposes and orchestrates multiple generation methods (Regression Transformer, GFlowNets, VAEs, diffusion models, motif-constrained generators) and evaluation pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Depends on the integrated models (e.g., Regression Transformer, GFlowNets, MoLeR); GT4SD itself is an enabler and does not claim specific novelty metrics in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Supports property endpoints and model conditioning to enable application-specific generation and post-processing (e.g., similarity filtering, property scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Includes integration with benchmarks (Moses, GuacaMol) and offers many evaluation utilities and endpoints for property prediction and synthesizability checks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GT4SD is positioned as a comprehensive toolkit to lower barriers to entry for scientific generative modeling, providing models, registry, webapps, and Jupyter/Colab examples for practical use.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>GT4SD complements other general-purpose platforms by focusing on generative models; it interoperates with diffusers, TorchDrug and offers alternatives to proprietary platforms like Chemistry42.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Tooling alone cannot solve validation and synthesis bottlenecks; successful design pipelines still need high-quality oracles, downstream virtual screening, and wet-lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models in molecular discovery', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Molgpt: molecular generation using a transformer-decoder model. <em>(Rating: 2)</em></li>
                <li>Regression transformer enables concurrent sequence regression and generation for molecular language modelling. <em>(Rating: 2)</em></li>
                <li>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. <em>(Rating: 2)</em></li>
                <li>Molecular generative model based on conditional variational autoencoder for de novo molecular design. <em>(Rating: 2)</em></li>
                <li>Generating focused molecule libraries for drug discovery with recurrent neural networks. <em>(Rating: 2)</em></li>
                <li>PaccMann RL : De novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Gflownet foundations. <em>(Rating: 2)</em></li>
                <li>Self-referencing embedded strings (selfies): A 100% robust molecular string representation. <em>(Rating: 2)</em></li>
                <li>Unifying molecular and textual representations via multi-task language modelling. <em>(Rating: 2)</em></li>
                <li>Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations. <em>(Rating: 1)</em></li>
                <li>Chemistry42: an ai-driven platform for molecular design and optimization. <em>(Rating: 1)</em></li>
                <li>An open-source drug discovery platform enables ultra-large virtual screens. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8665",
    "paper_id": "paper-263135519",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "MolGPT",
            "name_full": "MolGPT (Molecular GPT)",
            "brief_description": "A GPT-style decoder-only transformer adapted to chemical language (SMILES) for conditional molecule generation by concatenating SMILES tokens with property/scaffold condition vectors and trained on next-token prediction.",
            "citation_title": "Molgpt: molecular generation using a transformer-decoder model.",
            "mention_or_use": "mention",
            "model_name": "MolGPT",
            "model_type": "transformer (GPT-style decoder)",
            "model_size": null,
            "training_data": "SMILES corpora; SMILES tokens concatenated with a condition vector summarizing desired properties/scaffolds (as described in the review).",
            "application_domain": "de novo molecular generation for drug discovery / library design",
            "generation_method": "autoregressive SMILES generation (decoder-only GPT) with conditional inputs (condition vector); can be coupled with reinforcement learning for property optimization",
            "novelty_of_chemicals": "Review reports ability to generate molecules conditioned on properties/scaffolds; no quantitative novelty metrics reported in this paper (e.g., % novel vs training set not provided).",
            "application_specificity": "Conditions (property vectors and scaffolds) are concatenated to SMILES input to bias generation toward target properties; RL can further optimize property scores.",
            "evaluation_metrics": "Common metrics referenced for generative modeling include validity, uniqueness, diversity and property scores (e.g., pIC50 optimization in cited works), but specific MolGPT benchmark numbers are not provided in this review.",
            "results_summary": "Reported as an early application of GPT-like architectures to conditional molecule generation; capable of conditional SMILES generation and property optimization when combined with RL, but quantitative outcomes are not listed in this review.",
            "comparison_to_other_methods": "Presented as a competitive transformer-based approach; transformers' scalability and embedding quality highlighted relative to RNNs and VAEs, but no direct numeric comparison for MolGPT is provided here.",
            "limitations_and_challenges": "SMILES-based generation suffers from possible invalid strings and non-uniqueness; need for downstream virtual screening/synthesizability checks; no detailed MolGPT failure modes quantified in the review.",
            "uuid": "e8665.0",
            "source_info": {
                "paper_title": "Language models in molecular discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Regression Transformer",
            "name_full": "Regression Transformer",
            "brief_description": "A transformer formulation that treats conditional sequence modeling as a regression problem allowing concurrent property prediction and conditional molecular generation by concatenating molecular tokens with property tokens and using alternating masking during training.",
            "citation_title": "Regression transformer enables concurrent sequence regression and generation for molecular language modelling.",
            "mention_or_use": "mention",
            "model_name": "Regression Transformer",
            "model_type": "transformer (sequence-to-sequence / multitask masking/regression)",
            "model_size": null,
            "training_data": "SMILES sequences with associated property values (multi-task datasets); trained with mask-infilling/regression objective combining sequence and property tokens.",
            "application_domain": "property-driven molecular design / de novo molecule generation tailored to specified properties",
            "generation_method": "prompt/conditioning via concatenated property tokens and mask-infilling / regression-style training to jointly predict properties and generate sequences",
            "novelty_of_chemicals": "Enables property-conditioned generation; review does not report explicit novelty percentages or similarity metrics for generated molecules.",
            "application_specificity": "Properties are explicitly included as tokens; training alternates masking to force the model to learn to predict property values and generate molecules consistent with them, providing direct conditioning for application-specific design.",
            "evaluation_metrics": "Evaluated in source work using concurrent sequence regression/generation metrics and standard generative metrics (validity, property accuracy), but specific numbers are not reproduced in this review.",
            "results_summary": "Highlighted as a seminal multitask approach enabling simultaneous property prediction and conditional generation; used in GT4SD and chatbot demos to perform substructure-constrained, property-driven design.",
            "comparison_to_other_methods": "Characterized as enabling natural multitask modeling compared to separate property predictors plus generative models, improving control over property-driven generation; no quantitative head-to-head given here.",
            "limitations_and_challenges": "Potential for property prediction/generation tradeoffs and need for careful masking/prompting; review notes necessity of downstream validation (virtual screening, synthesis planning) but no specific failure cases listed.",
            "uuid": "e8665.1",
            "source_info": {
                "paper_title": "Language models in molecular discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Molecular Transformer (MT)",
            "name_full": "Molecular Transformer",
            "brief_description": "An autoregressive encoder-decoder Transformer applied to chemical reaction prediction (sequence-to-sequence) using reaction SMILES in a template-free, data-driven manner, able to represent stereochemistry and predict regio- and stereoselective outcomes.",
            "citation_title": "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction.",
            "mention_or_use": "mention",
            "model_name": "Molecular Transformer",
            "model_type": "transformer (encoder-decoder / seq2seq)",
            "model_size": null,
            "training_data": "Reaction SMILES datasets (data-driven training on reaction corpora); trained on reaction pairs (precursors → products) and variants for retrosynthesis (product → precursors).",
            "application_domain": "reaction outcome prediction, single-step retrosynthesis, multi-step retrosynthesis planning and synthesis-route planning for experimental execution",
            "generation_method": "sequence-to-sequence translation between reactant/product SMILES; used autoregressively for forward prediction and retrosynthesis; integrated into retrosynthesis planners and robotic execution pipelines",
            "novelty_of_chemicals": "MT is focused on predicting reaction outcomes and retrosynthetic precursors rather than de novo novel-targeted molecule design; review does not report novelty metrics for molecule generation via MT.",
            "application_specificity": "Enables controllable retrosynthesis (e.g., prompt-based disconnection), and was extended to enzymatic reactions via enzyme-class tokenization to tailor to biocatalysis applications.",
            "evaluation_metrics": "Performance assessed on reaction prediction accuracy, uncertainty calibration, regio-/stereoselectivity metrics; success in predicting stereochemical outcomes and use in retrosynthesis benchmarks noted but numerical scores not listed here.",
            "results_summary": "Presented as the flagship architecture for RXN for Chemistry; successfully applied to forward reaction prediction, single-step retrosynthesis, multi-step retrosynthesis with hypergraph exploration and enzymatic reaction planning; also enabled extraction of synthesis actions for robotic platforms.",
            "comparison_to_other_methods": "Compared favorably to graph-based models for stereochemistry and regioselectivity due to sequence-based stereochemical representation; MT is template-free and data-driven, offering advantages in representing stereochemistry directly.",
            "limitations_and_challenges": "Requires high-quality reaction data; downstream synthesis-action extraction and robotic execution pose additional challenges; masked-LM variants and prompt-based methods were introduced to improve controllability and diversity.",
            "uuid": "e8665.2",
            "source_info": {
                "paper_title": "Language models in molecular discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "VAE (conditional VAE / RNN-VAE)",
            "name_full": "Variational Autoencoder (conditional and RNN-based variants for molecular generation)",
            "brief_description": "VAEs learn continuous latent distributions from molecular sequences (e.g., SMILES) enabling interpolation and conditional sampling; RNN-based VAEs have been applied to generate catalysts and molecules by sampling latent space conditioned on desired attributes.",
            "citation_title": "Molecular generative model based on conditional variational autoencoder for de novo molecular design.",
            "mention_or_use": "mention",
            "model_name": "Conditional VAE / RNN-VAE",
            "model_type": "variational autoencoder (encoder-decoder), often RNN-based for sequences",
            "model_size": null,
            "training_data": "SMILES sequences and associated property/condition vectors (e.g., catalyst binding energies or other labels) used for conditional training.",
            "application_domain": "de novo molecular and catalyst design (including catalyst binding energy optimization)",
            "generation_method": "learn latent distribution; sample and decode to generate novel molecules; condition by concatenating condition vectors to input and latent embeddings",
            "novelty_of_chemicals": "VAEs enable interpolation and generation of novel molecules in latent space; the review does not provide quantitative novelty metrics (e.g., % unseen) for cited VAE works.",
            "application_specificity": "Conditional VAEs incorporate condition vectors (properties) during encoding/decoding to bias generation toward targeted attributes such as binding energy.",
            "evaluation_metrics": "Latent-space quality is assessed via downstream tasks (e.g., property prediction such as catalyst binding energy); generative metrics (validity, property alignment) are used but exact figures are not provided here.",
            "results_summary": "VAEs remain a standard approach for learning smooth latent molecular spaces and conditional generation; specific applications include catalyst design where VAE latent space correlates with binding energy.",
            "comparison_to_other_methods": "VAEs provide smoother latent interpolation compared to discrete sequence models (RNNs) but transformers have surpassed earlier architectures in embedding quality and scalability; hybrid and multimodal VAEs also exist.",
            "limitations_and_challenges": "Single-modality VAEs may be limited for multi-modal conditionalization; quality of decoding (valid SMILES) and property alignment depend on training data and conditioning strategy.",
            "uuid": "e8665.3",
            "source_info": {
                "paper_title": "Language models in molecular discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "RNN SMILES generators",
            "name_full": "Recurrent Neural Network (RNN) SMILES generators",
            "brief_description": "Early chemical language models that model SMILES sequentially with RNNs to generate molecule libraries auto-regressively; they can produce high proportions of unique and valid SMILES but have difficulty with counting ring token occurrences.",
            "citation_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks.",
            "mention_or_use": "mention",
            "model_name": "RNN-based SMILES generator",
            "model_type": "recurrent neural network (RNN, LSTM/GRU variants)",
            "model_size": null,
            "training_data": "SMILES datasets (library molecules used in drug development / screening datasets).",
            "application_domain": "de novo library generation for drug discovery / virtual screening",
            "generation_method": "autoregressive character/atom-level SMILES generation, often guided by external scoring functions for property optimization",
            "novelty_of_chemicals": "RNNs can generate many unique and valid SMILES; specific novelty percentages are not provided in this review.",
            "application_specificity": "External scoring functions (docking, property predictors) used to steer generation toward desired properties; also used for focused library generation.",
            "evaluation_metrics": "Validity, uniqueness, diversity, and downstream property performance are typical metrics mentioned; review does not list per-paper numeric values.",
            "results_summary": "RNNs were foundational for SMILES generation and remain used for library generation; however, they have limitations in syntax counting (ring closures) and are now often superseded by transformers.",
            "comparison_to_other_methods": "RNNs generally predate transformers; transformers provide better scalability and embedding quality, though RNNs can still produce high validity/uniqueness in some settings.",
            "limitations_and_challenges": "Difficulty counting ring-opening/closing tokens can produce invalid SMILES; syntax validity issues motivate alternatives like SELFIES or robust tokenization and augmentation.",
            "uuid": "e8665.4",
            "source_info": {
                "paper_title": "Language models in molecular discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Multitask Text & Chemistry T5",
            "name_full": "Multitask Text and Chemistry T5 (prompt-based multimodal T5)",
            "brief_description": "A prompt-based multitask T5 model that unifies natural language and chemical language tasks (generation, property prediction, reaction prediction) and demonstrates strong generalization across discovery workflows even at modest parameter counts.",
            "citation_title": "Unifying molecular and textual representations via multi-task language modelling.",
            "mention_or_use": "mention",
            "model_name": "Multitask Text and Chemistry T5 (T5Chem / Multimodal T5)",
            "model_type": "encoder-decoder transformer (T5-style, promptable multitask)",
            "model_size": "250M",
            "training_data": "Combined corpora of natural language and chemical text/SMILES for multi-task pretraining (details in cited work); trained as a promptable multitasker.",
            "application_domain": "end-to-end molecular discovery workflows including natural-text → molecule generation → synthesis planning → synthesis protocol generation",
            "generation_method": "prompt-based multitask generation that can take natural language prompts and produce molecules or reaction plans (in-context learning / prompting)",
            "novelty_of_chemicals": "In cited re-discovery workflow, the model outperformed ChatGPT and Galactica on rediscovering a herbicide, but the review does not enumerate novel-molecule novelty metrics.",
            "application_specificity": "Bridges natural language objectives and chemical generation enabling task-specific prompts (e.g., specify desired property or synthesis constraints) to produce targeted outputs.",
            "evaluation_metrics": "Benchmarked on contrived discovery workflow (rediscovering a known herbicide) and compared qualitatively/numerically to general LLMs; specific generative metrics not reproduced in this review.",
            "results_summary": "Despite being only 250M parameters, the multitask T5 variant outperformed general LLMs (ChatGPT, Galactica) on a contrived rediscovery pipeline, showing the value of specialized multimodal training.",
            "comparison_to_other_methods": "Outperformed general-purpose LLMs in the cited contrived discovery workflow, demonstrating that domain-specific multimodal foundation models can be more effective than large generic models for chemistry tasks.",
            "limitations_and_challenges": "While promising, such models still require integration with downstream chemistry tools for validation and synthesis execution; the review emphasizes human-in-the-loop and RLHF for improved outputs.",
            "uuid": "e8665.5",
            "source_info": {
                "paper_title": "Language models in molecular discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "PaccMann RL",
            "name_full": "PaccMann RL",
            "brief_description": "A reinforcement-learning based generative framework that produces de novo hit-like anticancer molecules conditioned on transcriptomic data (gene expression) to align generated molecules with cellular response profiles.",
            "citation_title": "PaccMann RL : De novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning.",
            "mention_or_use": "mention",
            "model_name": "PaccMann RL",
            "model_type": "reinforcement learning (policy-gradient or RL-guided generative model) combined with chemical generative models",
            "model_size": null,
            "training_data": "Molecular datasets linked with transcriptomic (gene expression) data; uses gene-expression-contextualized training to condition molecule generation.",
            "application_domain": "anticancer drug discovery (de novo design conditioned on transcriptomic profiles)",
            "generation_method": "RL-driven de novo generation conditioned on transcriptomic data to propose molecules predicted to be active in specific cellular contexts",
            "novelty_of_chemicals": "Capable of producing hit-like anticancer molecules aligned to transcriptomic signatures; no explicit novelty percentages provided in the review.",
            "application_specificity": "Conditions generation on gene expression signatures to tailor molecules for specific cellular states or cancer types, increasing application specificity.",
            "evaluation_metrics": "Evaluated using activity-hit likeness, predicted biological effect alignment, and other domain-specific metrics in the cited work; review does not reproduce numeric results.",
            "results_summary": "Showcases that coupling generative models with biological context via RL produces candidate molecules with desired cellular-effect profiles; review highlights this as an example of context-driven molecule design.",
            "comparison_to_other_methods": "Represents a shift from purely property-driven generation toward biologically contextualized generation; direct performance comparisons not detailed in the review.",
            "limitations_and_challenges": "Relies on quality of biological assays/transcriptomic links and predictive models; RL optimization can produce exploitation or mode collapse if not properly regularized; experimental validation remains necessary.",
            "uuid": "e8665.6",
            "source_info": {
                "paper_title": "Language models in molecular discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GFlowNets",
            "name_full": "Generative Flow Networks (GFlowNets)",
            "brief_description": "A class of generative models trained to sample diverse modes proportional to a provided reward function, useful in chemistry for generating diverse candidate molecules that satisfy target property rewards.",
            "citation_title": "Gflownet foundations.",
            "mention_or_use": "mention",
            "model_name": "GFlowNets",
            "model_type": "flow-based generative policy networks (RL-inspired generative sampling)",
            "model_size": null,
            "training_data": "Trained with molecular construction/action spaces and reward functions derived from desired properties; uses molecule-building trajectories rather than plain SMILES corpora.",
            "application_domain": "diverse candidate generation for molecular design (drug discovery / materials), particularly where diversity under reward constraints is desired",
            "generation_method": "sequential generation of molecular graphs/SMILES via an RL-like objective that encourages sampling proportional to reward to yield diverse high-reward candidates",
            "novelty_of_chemicals": "Designed to produce highly diverse candidate sets for a given reward; the review does not provide explicit novelty or similarity statistics from cited GFlowNet applications.",
            "application_specificity": "Uses explicit reward functions tied to application-specific properties, enabling targeted generation while preserving diversity.",
            "evaluation_metrics": "Diversity under reward, reward attainment, and downstream property metrics (validity, synthesizability) are typical evaluation axes; no numeric values reproduced here.",
            "results_summary": "Included in GT4SD as a contemporary approach for generating diverse high-reward molecules; presented as complementary to other generative strategies (VAEs, transformers, RL).",
            "comparison_to_other_methods": "GFlowNets aim to improve diversity compared to pure RL or greedy optimization methods that may collapse to few modes; review highlights GFlowNets' strength conceptually but does not include empirical comparisons.",
            "limitations_and_challenges": "Implementation complexity, definition of suitable reward functions, and need for accurate property oracles for reward evaluation; requires careful design to avoid generating unsynthesizable chemotypes.",
            "uuid": "e8665.7",
            "source_info": {
                "paper_title": "Language models in molecular discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "SELFIES",
            "name_full": "SELF-referencING Embedded Strings (SELFIES)",
            "brief_description": "A robust molecular string representation designed to guarantee chemically valid molecules upon decoding by enforcing valence and structural derivation rules; used as an alternative to SMILES to avoid invalid generations.",
            "citation_title": "Self-referencing embedded strings (selfies): A 100% robust molecular string representation.",
            "mention_or_use": "mention",
            "model_name": "SELFIES (representation)",
            "model_type": "molecular string representation (encoding scheme) rather than a generative model",
            "model_size": null,
            "training_data": "Representation applied to molecular datasets in place of SMILES to train generative models; not a model trained on data itself.",
            "application_domain": "any molecular generation task where validity of generated strings is critical (drug discovery, materials design)",
            "generation_method": "used as the tokenization/representation format for generative models to ensure valid molecules are produced upon decoding",
            "novelty_of_chemicals": "SELFIES guarantees 100% syntactic validity of decoded strings, reducing invalid output rate; novelty relative to training data depends on generative model, not SELFIES itself.",
            "application_specificity": "Representation does not by itself ensure property-targeted generation but removes invalidity as a failure mode, simplifying downstream conditioning and optimization.",
            "evaluation_metrics": "Validity is the primary metric improved (100% validity guarantee by construction); tradeoffs include potential overly short strings as noted in the review.",
            "results_summary": "SELFIES is presented as a solution to the invalidity issues of SMILES in generative models; while guaranteeing validity, it can produce strings that may be too short or otherwise not useful without additional constraints.",
            "comparison_to_other_methods": "Compared to SMILES, SELFIES removes invalid generation failures; the review notes that canonical SMILES with augmentation remain prevalent due to other tradeoffs.",
            "limitations_and_challenges": "Guarantee of syntactic validity can still yield molecules that are chemically trivial or not useful; representation may require additional constraints to ensure useful molecular complexity.",
            "uuid": "e8665.8",
            "source_info": {
                "paper_title": "Language models in molecular discovery",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GT4SD (Generative Toolkit for Scientific Discovery)",
            "name_full": "GT4SD - Generative Toolkit for Scientific Discovery",
            "brief_description": "An open-source Python toolkit focused on enabling use, training, fine-tuning and distribution of state-of-the-art generative models for scientific discovery with strong support for chemical language models, benchmarks, and model registry/inference tools.",
            "citation_title": "Accelerating material design with the generative toolkit for scientific discovery.",
            "mention_or_use": "mention",
            "model_name": "GT4SD (toolkit)",
            "model_type": "software toolkit / model registry and interfaces (supports transformers, diffusion models, GFlowNets, VAEs, etc.)",
            "model_size": null,
            "training_data": "Provides access to many pre-trained models and 50+ property prediction endpoints; integrates with common molecular datasets and benchmarks (Moses, GuacaMol, MoleculeNet endpoints via hosted APIs).",
            "application_domain": "design of organic materials, de novo molecular generation, property-driven design for drug discovery and materials science",
            "generation_method": "exposes and orchestrates multiple generation methods (Regression Transformer, GFlowNets, VAEs, diffusion models, motif-constrained generators) and evaluation pipelines",
            "novelty_of_chemicals": "Depends on the integrated models (e.g., Regression Transformer, GFlowNets, MoLeR); GT4SD itself is an enabler and does not claim specific novelty metrics in the review.",
            "application_specificity": "Supports property endpoints and model conditioning to enable application-specific generation and post-processing (e.g., similarity filtering, property scoring).",
            "evaluation_metrics": "Includes integration with benchmarks (Moses, GuacaMol) and offers many evaluation utilities and endpoints for property prediction and synthesizability checks.",
            "results_summary": "GT4SD is positioned as a comprehensive toolkit to lower barriers to entry for scientific generative modeling, providing models, registry, webapps, and Jupyter/Colab examples for practical use.",
            "comparison_to_other_methods": "GT4SD complements other general-purpose platforms by focusing on generative models; it interoperates with diffusers, TorchDrug and offers alternatives to proprietary platforms like Chemistry42.",
            "limitations_and_challenges": "Tooling alone cannot solve validation and synthesis bottlenecks; successful design pipelines still need high-quality oracles, downstream virtual screening, and wet-lab validation.",
            "uuid": "e8665.9",
            "source_info": {
                "paper_title": "Language models in molecular discovery",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Molgpt: molecular generation using a transformer-decoder model.",
            "rating": 2,
            "sanitized_title": "molgpt_molecular_generation_using_a_transformerdecoder_model"
        },
        {
            "paper_title": "Regression transformer enables concurrent sequence regression and generation for molecular language modelling.",
            "rating": 2,
            "sanitized_title": "regression_transformer_enables_concurrent_sequence_regression_and_generation_for_molecular_language_modelling"
        },
        {
            "paper_title": "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction.",
            "rating": 2,
            "sanitized_title": "molecular_transformer_a_model_for_uncertaintycalibrated_chemical_reaction_prediction"
        },
        {
            "paper_title": "Molecular generative model based on conditional variational autoencoder for de novo molecular design.",
            "rating": 2,
            "sanitized_title": "molecular_generative_model_based_on_conditional_variational_autoencoder_for_de_novo_molecular_design"
        },
        {
            "paper_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks.",
            "rating": 2,
            "sanitized_title": "generating_focused_molecule_libraries_for_drug_discovery_with_recurrent_neural_networks"
        },
        {
            "paper_title": "PaccMann RL : De novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning.",
            "rating": 2,
            "sanitized_title": "paccmann_rl_de_novo_generation_of_hitlike_anticancer_molecules_from_transcriptomic_data_via_reinforcement_learning"
        },
        {
            "paper_title": "Gflownet foundations.",
            "rating": 2,
            "sanitized_title": "gflownet_foundations"
        },
        {
            "paper_title": "Self-referencing embedded strings (selfies): A 100% robust molecular string representation.",
            "rating": 2,
            "sanitized_title": "selfreferencing_embedded_strings_selfies_a_100_robust_molecular_string_representation"
        },
        {
            "paper_title": "Unifying molecular and textual representations via multi-task language modelling.",
            "rating": 2,
            "sanitized_title": "unifying_molecular_and_textual_representations_via_multitask_language_modelling"
        },
        {
            "paper_title": "Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations.",
            "rating": 1,
            "sanitized_title": "accelerated_antimicrobial_discovery_via_deep_generative_models_and_molecular_dynamics_simulations"
        },
        {
            "paper_title": "Chemistry42: an ai-driven platform for molecular design and optimization.",
            "rating": 1,
            "sanitized_title": "chemistry42_an_aidriven_platform_for_molecular_design_and_optimization"
        },
        {
            "paper_title": "An open-source drug discovery platform enables ultra-large virtual screens.",
            "rating": 1,
            "sanitized_title": "an_opensource_drug_discovery_platform_enables_ultralarge_virtual_screens"
        }
    ],
    "cost": 0.01910375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language models in molecular discovery
September 29, 2023</p>
<p>Nikita Janakarajan 
IBM Research Europe</p>
<p>Tim Erdmann 
IBM Research Almaden
San JoseCAUnited States</p>
<p>Sarath Swaminathan 
IBM Research Almaden
San JoseCAUnited States</p>
<p>Teodoro Laino 
IBM Research Europe</p>
<p>Jannis Born 
IBM Research Europe</p>
<p>ZurichSwitzerland</p>
<p>Language models in molecular discovery
September 29, 2023B8F4558B7EE1F5563A60B0CEDC34087BarXiv:2309.16235v1[physics.chem-ph]
The success of language models, especially transformer-based architectures, has trickled into other domains giving rise to "scientific language models" that operate on small molecules, proteins or polymers.In chemistry, language models contribute to accelerating the molecule discovery cycle as evidenced by promising recent findings in early-stage drug discovery.Here, we review the role of language models in molecular discovery, underlining their strength in de novo drug design, property prediction and reaction chemistry.We highlight valuable open-source software assets thus lowering the entry barrier to the field of scientific language modeling.Last, we sketch a vision for future molecular design that combines a chatbot interface with access to computational chemistry tools.Our contribution serves as a valuable resource for researchers, chemists, and AI enthusiasts interested in understanding how language models can and will be used to accelerate chemical discovery.</p>
<p>Introduction</p>
<p>Despite technological advances constantly reshaping our understanding of biochemical processes, the chemical industry persistently faces escalating resource costs of up to 10 years and 3 billion dollar per new market release [102].The intricacy of the problem is typically attested by an exorbitant attrition rate in in vitro screenings [77], the sheer size of the chemical space [68] and the frequency of serendipity [40].</p>
<p>Language models (LMs) emerged recently and demonstrated an astonishing ability to understand and generate human-like text [65].Machine learning (ML) in general and LMs in particular hold the potential to profoundly accelerate the molecular discovery cycle (see Figure 1).In this chapter, we explore applications of LMs to chemical design tasks.Although LMs were originally developed for natural language, they have shown compelling results in scientific discovery settings when applied to "scientific languages", e.g., in protein folding [55] or de novo design of small molecules [105], peptides [23] or polymers [66].But what exactly is a language model?By definition, it is any ML model that consumes a sequence of text chunks (so-called tokens) and is capable to reason about the content of the sequence.Since each token is essentially a vector [62], a LM is a pseudo-discrete time series model.Most typically, LMs learn probability distributions over sequences of words thus also facilitating the generation of new text given some input, for example in a language translation task.While all LMs rely on neural networks, contemporary models almost exclusively leverage the Transformer architecture [93].Now, all of this begs the question -what is the need for LMs in molecular discovery?</p>
<p>First, when applied to serializations of chemical entities (e.g., SMILES [98]), LMs can learn highly structured representations, often even tailored for desired functional properties [36].This allows to perform smooth and property-driven exploration of the originally deemed discrete protein or molecular space.Another attractive feature of scientific LMs is their ability to seamlessly bridge natural and scientific languages.This can give rise to ChatGPT-style chatbot interfaces that allow chemists to  formulate their design objectives through natural language and to iteratively refine their result with an interactive agent thus potentially accomplishing complex chemical tasks more rapidly.Here, we present an overview of the role of LMs toward accelerated molecular discovery.We commence with the conventional scientific discovery method and then discuss how molecular generative models can be coupled with molecular property prediction models.Seeking for practical usability, we then present the reader with selected software tools and libraries for scientific language modeling.We close with a vision for future molecule design that integrates natural language models into the discovery process through chatbots.</p>
<p>Accelerated molecular discovery</p>
<p>Molecule discovery, intricately linked to optimizing diverse properties in a vast space, challenges conventional scientific methods.In chemistry's Design-Make-Test-Analyze (DMTA) cycle, synthesis costs and time constraints create a bottleneck that hampers hypothesis refinement (cf. Figure 1a).Traditional approaches are largely driven by medicinal chemists who design "molecule hypotheses" which are biased, ad-hoc and non-exhaustive.This hinders progress in addressing global issues, driving crucial necessity for an accelerated process of molecule discovery.Thus, a key challenge lies in improving speed and quality of evaluating such "molecule hypotheses" grounded on laboratory work.</p>
<p>Deep generative models have recently emerged as a promising tool to expedite the hypothesis/design phase in molecular discovery.However, even the most advanced molecular generative models require an efficient method for large-scale virtual screening to test their hypotheses.The accelerated molecular discovery cycle adds a validation loop to DMTA, rapidly evaluating numerous hypotheses inexpensively (cf. Figure 1b).This loop enhances the design-phase generative model, ensuring only promising hypotheses advance to the synthesis and physical experimentation stages.</p>
<p>Molecule Representation</p>
<p>Data representation is critical as it determines which information is available for the model.As illustrated in Figure 2, various molecular representations exist.Due to popularity of chemical language models (CLMs), this section focuses on text-representations of molecules.A more focused discussion on CLMs was published by Grisoni [38].[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ... , 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
Morgan Fingerprint CN1C=NC2=C1C(=O)N(C(=O)N2C)C SMILES [C][N][C][=N][C][=C][Ring1][Branch1_1][C][Branch1_2][C][=O][N] [Branch1_1][Branch2_2][C][Branch1_2][C][=O][N][Ring1][Branch2_1][C][C]</p>
<p>SELFIES</p>
<p>InChI=1S/C8H10N4O2/c1-10-4-9-6-5(10)7( 13  Simplified Molecular Input Line-Entry System (SMILES) SMILES [98] is a string representation made up of specific characters for atoms, bonds, branches, aromaticity, rings and stereochemistry in molecular structures.The character-level representation enables easy tokenization, making SMILES an ideal input for LMs.SMILES are non-unique, so each molecule can be written as multiple SMILES strings.Hence, SMILES are either canonicalized or, alternatively, their multiplicity is used as data augmentation strategy [8] which has shown performance improvement in molecular property prediction [8,51,88] and molecular generation [3,92].In generative modeling, a common issue is the invalidity of SMILES strings due to an uneven number of ring opening/closure symbols or bond valence violations.SMILES strings can undergo further processing, such as kekulization or stereoinformation removal but employing canonicalized SMILES remains the most prevalent approach.</p>
<p>Tokenization is the process of splitting a string into vectorizable units.These units are typically a single character, n-gram characters or words.Instead of splitting at the character level, SMILES are typically tokenized at the atom level with regular expressions [79] or by additionally including positional and connectivity information, thereby acknowledging that the same atom can have different encodings based on its location in the molecular structure [91].SMILES may also be tokenized at the substructure level, as demonstrated by SMILES Pair Encoding (SMILES-PE) [52].This method, inspired by byte-pair encoding, iteratively counts and merges frequently occurring SMILES token pairs until a given condition is met.Tokenization enables the creation of a vocabulary for SMILES representations.</p>
<p>Vocabularies are dictionaries mapping tokens to vectors thus serving as gateway to LMs.For LMs to learn from SMILES, tokens are vectorized, either via one-hot encodings (where each row in the binary matrix corresponds to a SMILES position and each column signifies a token).However, this discrete method results in sparse, large matrices and thus, an alluring alternative is to learn a continuous embedding for each token during training.This facilitates the learning of semantic relationships between tokens and enhances performance.Since learning good embeddings requires a lot of data, models pre-trained on natural language corpora are a strong option to learn scientific language embeddings through fine-tuning [22].</p>
<p>Self Referencing Embedded Strings (SELFIES) SELFIES [49] were introduced as an alternative to SMILES to counter the problem of generating invalid molecules.Unlike SMILES, SELFIES are generated using derivation rules to enforce valence-bond validity.They store branch length and ring size to avoid open branches and rings.These supplementary attributes ensure a valid representation during molecule generation.While this strategy guarantees 100% validity, it could produce strings that are too short to be a useful molecule.</p>
<p>International Chemical Identifier (InChI) Introduced by the IUPAC, InChI [41] are strings encoding structural information including charge of the molecule in a hierarchical manner.The strings can get long and complex for larger molecules.To counter this, a hash called 'InChiKey' was developed to help with search and retrieval.InChIs are are less commonly used in LMs [39].</p>
<p>Generative Modelling</p>
<p>Generative modeling involves learning the data's underlying distribution with the intent of generating new samples, a technique pivotal in accelerating de novo drug discovery.A generative model may be conditional or unconditional.A conditional generative model utilizes provided data attributes or labels to generate new samples with desired properties, whereas an unconditional model solely provides a way to sample molecules similar to the training data [36].The DMTA cycle particularly benefits from the conditional generation approach as it facilitates goal-oriented hypothesis design [9].This section describes a few influential conditional generation models that act on chemical language to generate molecules satisfying user-defined conditions.</p>
<p>Recurrent Neural Network (RNN)</p>
<p>The sequential nature of RNNs makes them suitable models for processing chemical languages.Proposed in the 90s, RNNs were the first flavor of CLMs [8,79,85].Their hidden states are continuously updated as new tokens are passed to the network.During the generation process, tokens are produced auto-regressively.RNNs find use in generating molecule libraries [85] which are extensively used in drug development processes like screening.External scoring functions drive the generation of molecules with desired properties.RNNs are also adept at learning complex distributions [31] and generating a higher proportion of unique and valid SMILES [69], even though their inability to count occurrences of ring opening/closing symbols poses a challenge [46,70].</p>
<p>Variational Autoencoder (VAE)</p>
<p>VAEs learn latent distribution parameters of molecules, thus enabling the generation of new molecules by sampling from this distribution.Their unique ability lies in learning a smooth, latent space that facilitates interpolation of samples, even for notoriously discrete entities like molecules [36].To make it suitable for chemical language models (CLMs), any network compatible with string inputs can function as a VAE's encoder and decoder.Initial works primarily focused on single-modality applications, assessing latent space quality via downstream tasks [36].This approach remains prevalent and can be used to generate, e.g., catalysts with an RNN-based VAE [78] .Here, a latent space is learned and assessed by predicting the catalyst binding energy.Lim et al. [53] takes it a step further by concatenating a condition vector to the input and the latent embedding generated by the recurrent network-based VAE's encoder.This approach enables the generation of molecules specifically tailored to the given conditions.The scope of VAEs expanded progressively into multi-modal settings for conditional molecule generation, as visualized in Figure 3 and exemplified by Born et al. [11,12,13].These works on task-driven molecule generation incorporate contextual information like gene expression [13] or protein targets [11,12] or even both [45].VAEs learn embeddings of context information and primer drugs, which are merged before decoding to produce molecules.A reinforcement-learning-based approach directs the model to produce molecules with desired properties using rewards.</p>
<p>Transformer</p>
<p>The self-attention attribute of Transformers [93] have propelled these models to the forefront of NLP.</p>
<p>Transformers have an encoder module that relies on this self-attention to learn embeddings of the input and the context associated with this input.The decoder module predicts tokens using the context learnt by the encoder and previously generated tokens through attention.For generative modeling, decoder-only transformer like the Generative Pre-Training Transformer (GPT) [72] have become the dominant approach.This success was translated to the scientific language domain.One of the first models to use the GPT architecture for conditional molecule generation is MolGPT [4].SMILES tokens concatenated with a condition vector that summarizes the desired properties and scaffolds are passed as input to this model, which is then trained on the next token prediction task to generate molecules.GPT-like models coupled with RL can also be used to optimize molecular properties like pIC50 [61].In this two-stage approach, embeddings are first learnt from SMILES strings, and the embedding space is then optimized such that the model samples molecules with the desired properties.Going beyond just using GPT-like architectures for molecule generation, Regression Transformer [10] is a seminal work that formulates conditional sequence modeling as a regression problem.This gives rise to a natural multitask model that concurrently performs property prediction and conditional molecular generation.This is achieved by concatenating conventional molecular tokens with property tokens and employing an training scheme that alternates which parts of the sequence are masked.All these works are testament to the generative capabilities of Transformer-based models.The superior quality of learned embeddings coupled with its ability to handle parallel processing and scalability makes it a top choice for the task of conditional molecule generation, with promising applications in drug discovery and other areas of molecular design [66].</p>
<p>Property Prediction</p>
<p>Whether a discovery is novel or not, property prediction is a key step in validating the molecules for a given use case.The success of a molecule depends on a myriad of factors, including how it interacts with its environment.The MoleculeNet datasets [103] are a commonly used benchmark for property prediction.It is curated from public datasets and comprises over 700,000 compounds tested on various properties.Born et al. [15] uses a multiscale convolutional attention model to predict toxicity from SMILES.The model has three kernel sizes for the convolutional network and uses a a Bahdanau attention mechanism [5].The model shows a superior performance overall on various MoleculeNet tasks compared to all other SMILES-based models.A recent trend is to use transformerencoders to learn embeddings for molecules and then apply a multilayer perceptron (MLP) on the embeddings for property prediction.MolBERT [29] and ChemBERTA [20]) are two such examples.</p>
<p>These transformer-based models use a BERT backbone to learn molecular embeddings from SMILES and predict properties.Similarly, Molformer [75] uses a transformer-encoder with linear attention and relative positional encoding to learn compressed molecular representations which are then fine-tuned on chemical property prediction benchmarks.To equip transformers with better inductive biases to handle molecules, adaptations of the attention mechanism were proposed.The molecule attention transformer (MAT) incorporates inter-atomic distances and graph structure into the attention mechanism [58].An improvement over this model is the relative-MAT which fuses the distance embedding, bond embedding and neighbourhood embedding and achieves competitive performances on a range of property prediction tasks [59].</p>
<p>Software tools for scientific language modeling</p>
<p>The paradigm shift towards open-sourcing software has exerted a profound influence in chemistry.Commonly listed implications of open-sourcing in the context of drug discovery include catalyzation of methodological development, fostering of collaboration and ease of scientific reproducibility [35].In this section we present several software assets (e.g., Python packages or cloud-based web apps) that are key to enable molecular discovery.</p>
<p>Natural language models</p>
<p>The success story of the Transformer [93] as most widely adopted neural network architecture goes hand in hand with the rise of the transformers library [101], developed since 2019 by HuggingFace.Initially intended for NLP applications, Transformers were adopted interdisciplinarily, e.g in computer vision [25], reinforcement learning [19], protein folding [47] and, of course, chemistry [84].HuggingFace provides the largest public hub of language models and it offers implementations of all recent models as well as a diverse collection of pretrained models available for fine-tuning or inference.While most of their models focus on NLP, selected models are designed for life science applications, in particular molecular property prediction (e.g., ChemBerta [20]), molecular captioning (e.g., MolT5 [26]), text-based molecular generation (e.g., MolT5 [26]) but also unsupervised protein language models (e.g., ProtBert, ProtAlbert, ProtXLNet or ProtT5 [27]).Moreover, some available models like Multimodal Text and Chemistry T5 [22] are prompt-based multitasker that besides the above mentioned tasks also perform additional tasks such as forward/backward reaction prediction.</p>
<p>GT4SD -Generative modeling toolkits</p>
<p>Python libraries like GT4SD (the Generative Toolkit for Scientific Discovery [57]), TdC (Therapeutics Data Commons [43]) or deepchem [73] were developed primarily for molecular discovery applications, but especially GT4SD offers ample support of language models (LMs).GT4SD is designed to enable researchers and developers to use, train, fine-tune and distribute state-of-the-art generative models for sciences with a focus on the design of organic materials.It is compatible and inter-operable with many existing libraries and, beyond transformers, it also gives access to diffusion models (diffusers [96]) or graph generative models (TorchDrug [106]).Next to established molecular generation benchmark like Moses [69] and GuacaMol [16] that include VAEs, generative adversarial networks (GANs), genetic algorithms, and many evaluation metrics for molecular design, gt4sd also supports very contemporary models like the Regression Transformer for concurrent sequence regression and property-driven molecular design [10], GFlowNets for highly diverse candidate generation [6] or MoLeR for motif-constrained molecule generation [60].GT4SD ships with a harmonized interface and a set of command line tools that access a registry of generative models to run or train any model with a few lines of code.Trained models can be shared to a cloud-hosted model hub and the library is build to facilitate consumption by containerization or distributed computing systems.To date, it includes ∼ 50 property prediction endpoints for small molecules, proteins and crystals and overall hosts ∼ 30 pre-trained algorithms for material design, 20 free webapps [2] and many Jupyter/Colab notebooks.</p>
<p>RXN for Chemistry: Reaction and synthesis language models</p>
<p>Once a molecule has been selected for experimental validation, a tangible synthesis route has to be identified.Since the most important tasks in chemical reaction modeling can be framed as sequence conversion problems, the methodology developed for natural language translation can be seamlessly translated to chemistry [84].In this analogy, atoms are characters, molecules are words, reactions are sentences and precursors are translated into a product or vice versa.</p>
<p>The most mature and flexible library for reaction modeling with LMs is the package rxn4chemistry [32].It wraps the API of the IBM RXN for Chemistry platform, a freely accessible web application that gives access to a rich set of language models for different tasks in reaction chemistry.The flagship architecture has been the Molecular Transformer (MT), an autoregressive encoder-decoder model, originally applied to predict outcomes of chemical reactions in organic chemistry [80].Notably, the MT uses a purely data-driven, template-free approach that, unlike many graph-based models, can directly represent stereochemistry and thus also exhibits excellent performance on regio-and stereoselective reactions [67].The MT was applied to single-step retrosynthesis [90] and became the linchpin of a multi-step retrosynthesis model with a hypergraph exploration strategy [81].This approach was later generalized to enzymatic reactions with a tokenization scheme based on enzyme classes which facilitated biocatalyzed synthesis planning and paved the road towards more sustainable and green chemistry [71].Derivatives of the MT helped to enhance diversity in single-step retrosynthesis [90] and a prompt-based disconnection scheme proposed by Thakkar et al. [89] significantly improved controllability by allowing the user to mark a disconnection side in the reactant.Interestingly, an encoder-only derivative of the MT (that replaced the autoregressive decoder with a classification head and leveraged BERT-style [24] self-supervised pretraining on reactions) excelled in predicting reaction classes [83].The hidden representations of such a model were found to encode reaction types and thus allowing to map reaction atlases and to perform reaction similarity search.This gave rise to the rxnfp package for chemical reaction fingerprinting.Strikingly, masked language modeling also led later to the discovery that the learned attention weights of the Transformer are "secretly" performing atom mapping between products and reactions [82].The epiphany that CLMs accomplish atom mapping without supervision or human labeling bridged the gap between rule-based and data-driven approaches in reaction modeling, making this once tedious experimental task more efficient.</p>
<p>In the quest for automation in organic chemistry, once the precursors for a molecule's synthesis route are identified, the subsequent crucial phase involves seeking an actionable, stepwise synthesis protocol that is ideally amenable for autonomous execution on a robotic platform, such as IBM RoboRXN.In two seminal works Vaucher et al. demonstrated that encoder-decoder Transformers can extract chemical synthesis actions, first from experimental procedures described in patents [94] and later predict them directly from the reaction SMILES [95].Notable, all the aforementioned models are available via the IBM RXN for Chemistry platform which even allows to control and monitor the robotic platform directly from the web interface.For the daunting task of multistep retrosynthesis planning, RXN also includes non-transformer based models like AiZynthFinder [34], a Monte Carlo Tree Search approach build on top of a RNN.Most of the RXN models can be executed also via the rxn4chemistry Python package.</p>
<p>Specialized libraries</p>
<p>Molecular property prediction.HuggingMolecules is a library solely devoted to aggregate, standardize and distribute molecular property prediction LMs [33].It contains many encoder-only CLMs, some of them with geometrical and structure-aware inductive biases (e.g., the MAT [58] or its successor, the R-MAT [59]) while others being pure BERT-based models that were trained on SMILES (e.g,.MolBERT [29] or ChemBERTA [20]).</p>
<p>Data processing.RDKit [50] is a library for manipulating molecules in Python.For narrower applications like ML data preparation several tools exist.First, rxn-chemutils is a library with chemistryrelated utilities from RXN for Chemistry.It includes functionalities for standardizing SMILES (e.g., canonicalization or sanitization) but also conversions to other representations (e.g., InChI).It harmonizes reaction SMILES and prepares them for consumption by CLMs, including also SMILES aug-mentation (by traversing the molecular graph in a non-canonical order) and tokenization.Another library with a similar focus is pytoda [12,13].It does not support reaction SMILES but implements richer preprocessing utilities, allowing to chain &gt;10 SMILES transformations (e.g., kekulization [15]).It supports different languages (e.g., SELFIES [49] or BigSMILES [54]) and tokenization schemes (e.g., SMILES-PE [52]).Similar functionalities are available for proteins including different languages (IU-PAC, UniRep or Blosum62) and protein sequence augmentation strategies [14].For small molecules, proteins, and polymers, dedicated language classes facilitate the integration with LMs by storing vocabularies, performing online transformations and feeding to custom datasets.Datasets exist for predicting molecular properties, drug sensitivity, protein-ligand affinity or for self-supervision on small molecules, proteins or polymers.</p>
<p>General purpose platforms</p>
<p>Several general-purpose platforms for molecular discovery have been launched recently, sometimes even preserving privacy through federated learning (i.e., decentralized, distributed training).For example, MELLODDY [42] is a collaborative effort aimed at cross-pharma federated learning of 2.6 billion confidential activity data points.Similarly, VirtualFlow [37] is an open-source platform facilitating large-scale virtual screening that was shown to identify potent KEAP1 inhibitors.With a focus on de novo drug design, Chemistry42 [44] is a proprietary platform integrating AI with computational and medicinal chemistry techniques.</p>
<p>Future of molecular discovery</p>
<p>A few years ago, the idea of querying an AI model -like one would a search engine -to not only extract scientific knowledge but also perform computational analyses was an overly ambitious feat.Scientific thinking comes from the ability to reason, and AI models cannot reason like humans, yet.However, these models can learn from humans.Our propensity to document everything has enabled us to train Large Language Models (LLMs), like ChatGPT [64] and GitHub Copilot [1], to mimic human responses.When brought into the context of computational science, this could equip non-experts to confidently conduct computational analyses through well-designed prompts.With human-in-the-loop, a synergistic effect could be created where the scientist provides feedback to the model on its output, thus aiding in better model optimization (a strategy called reinforcement learning from human feedback (RLHF) that has been proven critical for ChatGPT [21]).These applications also reduce the barrier for individuals from non-scientific backgrounds to gain a more hands-on experience in conducting scientific analyses without having to go through formal training in computational analysis.</p>
<p>This section provides a sneak peak into what's next for molecular discovery.Riding the LLM wave, the future holds a place for chatbot-like interfaces that may take care of all things computational in molecular discovery.This includes, for example, generating and iteratively improving design ideas, synthesis planning, material purchasing, performing routine safety checks, and validating experiments.</p>
<p>The rise of foundation models in chemistry</p>
<p>Conventionally, neural networks are trained for a single given task to achieve maximum performance.This essentially renders the models useless for other tasks, thus requiring a new model for every new task, even when the training domain is the same, which in turn imposes a constraint on the rate of our technological advancements.Over the last few years, this conventional approach has been challenged by Large Language Models (LLMs).It has been found that scaling up LLMs leads to astonishing performances in few-shot [17] and even zero-shot task generalization [76].Referred to as "foundation models" [30,63], these models, with typically billions of parameters, can perform multiple tasks despite being trained on one large dataset.Essentially, this multi-task learning is achieved by prompting LLMs with task instructions along with the actual query text which has been found to induce exceptional performance in natural language inference and sentence completion [76].These findings have kicked off new research directions, such as prompt engineering [97] and in-context learning [17], in NLP.</p>
<p>The foundation model paradigm also finds an increasing adoption in chemistry.There is an increase in task-specific models integrating natural and chemical languages [26,94,95,104].Concurrently, multi-tasking in pure CLMs has also been advancing through models that combined tasks such as property prediction, reaction prediction and molecule generation either with small task-specific heads (e.g., T5Chem [56]) or via mask infilling (e.g., Regression Transformer [10]).Christofidellis et al. [22] were the first to bridge the gap and develop a fully prompt-based multi-task chemical and natural language model.Despite only 250M parameters, the Multitask Text and Chemistry T5 was shown to outperform ChatGPT [64] and Galactica [87] on a contrived discovery workflow for re-discovering a common herbicide (natural text → new molecule → synthesis route → synthesis execution protocol).</p>
<p>The coalescence of chatbots with chemistry tools</p>
<p>Given the aforementioned strong task generalization performances of LLMs, building chatbot interfaces around it was a natural next step and thus next to ChatGPT [64], many similar tools were launched.Such tools were found to perform well on simplistic chemistry tasks [18,99], opening potential to  [48], RDKit [50] or GT4SD [57] enables the assistant to execute programming routines in the background and thus answer highly subject-matter specific user requests without the user needing programming skills.</p>
<p>reshape how chemists interact with chemical data, enabling intuitive access to complex concepts and make valuable suggestions for diverse chemical tasks.Furthermore, AI models specifically developed by computer scientists for e.g.drug discovery or material science can be made available through applications powered by LLMs, such as chatbots.This minimizes the access barrier for subject matter experts who would otherwise require the respective programming skills to utilize these AI models.The power of such chatbots is reached through the coalscence of LLMs and existing chemistry software tools like PubChem [48], RDKit [50] or GT4SD [57].Together, such applications can unleash the full potential and value of these models by the strongly enhanced usage.An example of how the interaction with such a tool could look like is shown in Figure 4.</p>
<p>In this example, a user provides a molecule (either as SMILES string or via a molecule sketcher) and asks to identify the molecule.The chatbot relies on prompt-engineering in order to inform the LLM about all its available tools.The user input is first sent to the LLM which recognizes that one of its supported tools, in this case PubChem, can answer the question.The chatbot then sends a request to the PubChem API and returns a concise description of the molecule.The user subsequently asks to compute the logP partition coefficient [100] and the quantitative estimate of drug-likeness (QED) [7].Calculation of both properties is enabled through the GT4SD tool [57] allowing the chatbot to answer the request with certainty.This will trigger a programming routine to accurately format the API request for GT4SD, i.e., composing the SMILES string with the logP or QED endpoint.The computation is then performed asynchronously and a separate call to the post-processing routine formats the LLM-generated string reply and composes the response object for the frontend.</p>
<p>This fusion of LLMs with existing tools gives rise to a chatbot assistant for material science and data visualization that can perform simple programming routines without requiring the user to know programming or have access to compute resources.A continuation of the conversation involving more complex user queries is shown in Figure 5. Having identified the initial molecule as theobromine with  [10] as well as property [28] and similarity calculation [74,86].a logP of -1.04, the user requests three similar molecules with a slightly increased logP of -0.5.Here, ChemChat identifies the Regression Transformer [10] as the available tool to perform substructureconstrained, property-driven molecule design.Once the routine has been executed and the three candidate SMILES are collected, the text result is post-processed to add more response data objects such as molecule visualizations, datasets or Vega Lite specs for interactive visualizations.</p>
<p>In conclusion, chatbots can facilitate the integration of essentially all major cheminformatics software in a truly harmonized and seamless manner.While LLMs are not intrinsically capable to perform complex routines, at least not with high precision and in a trustworthy manner, the synergy between their natural language abilities with existing chemistry tools has the potential to transform the way chemistry is performed.</p>
<p>Accelerated molecular discovery.</p>
<p>Figure 1 :
1
Figure 1: A comparison of molecular discovery workflows: (a) classic approach, where each hypothesis (a.k.a.molecule) requires a new experimental cycle.(b) Accelerated molecular discovery cycle with machine-generated hypotheses and assisted validation, enabling simultaneous generation and testing of numerous molecules.</p>
<p>1, 3 , 7 -
37
Trimethyl-3,7-dihydro-1H-purine-2,6-dione MolFile Graph</p>
<p>)12(3)8(14)11(6)2/h4H,1-3H3 InChI T e x t -b a s e d R e p re s e n t a t io n Structure-based Representation F e a t u r eb a s e d R e p r e s e n t a t io</p>
<p>Figure 2 :
2
Figure 2: An illustration of popular ways of representing a chemical molecule as input to a ML model.The representations may be (a) String-based, such as SMILES, SELFIES, or InChI which use characters to represent different aspects of a molecule, (b) Structure-based, such as Graphs or MolFiles that encode connectivity and atomic position, and (c) Feature-based, such as Morgan Fingerprints, which encode local substructures as bits.</p>
<p>Figure 3 :
3
Figure 3: An illustration of conditional molecule generation using LMs.The process initiates with the collection and processing of multi-modal data, which is then compressed into a fixed-size latent representation.These representations are subsequently passed to a molecular generative model.The generated molecules then undergo in-silico property prediction, which is linked back to the generative model through a feedback loop during training.The in-silico models direct the generative model to produce property-or task-driven molecules using a reward function.In the inference stage, candidate molecules generated by the optimized model continue through the workflow for lab synthesis and subsequent experimental validation to determine their efficacy for the desired task.</p>
<p>Figure 4 :
4
Figure4: Screenshot of the LLM-powered chatbot application ChemChat.Embedding the capabilities of existing resources such as PubChem[48], RDKit[50] or GT4SD[57] enables the assistant to execute programming routines in the background and thus answer highly subject-matter specific user requests without the user needing programming skills.</p>
<p>Figure 5 :
5
Figure 5: Screenshot of the LLM-powered chatbot application ChemChat showing the continuation of the conversation involving generative tasks through GT4SD's Regression Transformer[10] as well as property[28] and similarity calculation[74,86].</p>
<p>Github copilot. 2021. August 8, 2023</p>
<p>Gradio: Hassle-free sharing and testing of ml models in the wild. Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, James Zou, arXiv:1906.025692019arXiv preprint</p>
<p>Randomized smiles strings improve the quality of molecular generative models. Josep Arús-Pous, Simon Viet Johansson, Oleksii Prykhodko, Jannik Esben, Christian Bjerrum, Jean-Louis Tyrchan, Hongming Reymond, Ola Chen, Engkvist, Journal of cheminformatics. 1112019</p>
<p>Molgpt: molecular generation using a transformer-decoder model. Viraj Bagal, Rishal Aggarwal, Deva Vinod, Priyakumar, Journal of Chemical Information and Modeling. 6292021</p>
<p>Neural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.04732014arXiv preprint</p>
<p>. Yoshua Bengio, Tristan Deleu, Edward J Hu, Salem Lahlou, Mo Tiwari, Emmanuel Bengio, 2021Gflownet foundations. Preprint at</p>
<p>Quantifying the chemical beauty of drugs. Richard Bickerton, V Gaia, Jérémy Paolini, Besnard, Andrew L Sorel Muresan, Hopkins, Nat. Chem. 422012</p>
<p>Esben Jannik, Bjerrum , arXiv:1703.07076Smiles enumeration as data augmentation for neural network modeling of molecules. 2017arXiv preprint</p>
<p>Trends in deep learning for property-driven drug design. Jannis Born, Matteo Manica, Current medicinal chemistry. 28382021</p>
<p>Regression transformer enables concurrent sequence regression and generation for molecular language modelling. Jannis Born, Matteo Manica, Nature Machine Intelligence. 542023</p>
<p>Active site sequence representations of human kinases outperform full sequence representations for affinity prediction and inhibitor generation: 3d effects in a 1d model. Jannis Born, Tien Huynh, Astrid Stroobants, Wendy D Cornell, Matteo Manica, Journal of Chemical Information and Modeling. 6222021</p>
<p>Datadriven molecular design for discovery and synthesis of novel ligands: a case study on sars-cov-2. Jannis Born, Matteo Manica, Joris Cadow, Greta Markert, Nikita Janakarajan, Antonio Cardinale, Teodoro Laino, María Rodríguez, Martínez , Mach. Learn.: Sci. Technol. 22250242021Nil Adell Mill, Modestas Filipavicius,</p>
<p>PaccMann RL : De novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning. iScience. Jannis Born, Matteo Manica, Ali Oskooei, Joris Cadow, Greta Markert, María Rodríguez, Martínez , 202124102269</p>
<p>On the choice of active site sequences for kinase-ligand affinity prediction. Jannis Born, Yoel Shoshan, Tien Huynh, Wendy D Cornell, Eric J Martin, Matteo Manica, Journal of chemical information and modeling. 62182022</p>
<p>Chemical representation learning for toxicity prediction. Jannis Born, Greta Markert, Nikita Janakarajan, Talia B Kimber, Andrea Volkamer, María Rodríguez Martínez, Matteo Manica, Digital Discovery. 2023</p>
<p>Guacamol: benchmarking models for de novo molecular design. Nathan Brown, Marco Fiscato, Marwin Hs Segler, Alain C Vaucher, J. Chem. Inf. Model. 5932019</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Do large language models understand chemistry? a conversation with chatgpt. Cayque Monteiro, Castro Nascimento, André Silva Pimentel, Journal of Chemical Information and Modeling. 6362023</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch, Advances in neural information processing systems. 202134</p>
<p>Chemberta: large-scale selfsupervised pretraining for molecular property prediction. Seyone Chithrananda, Gabriel Grand, Bharath Ramsundar, arXiv:2010.098852020arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730Advances in neural information processing systems</p>
<p>Unifying molecular and textual representations via multi-task language modelling. Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, Matteo Manica, International Conference on Machine Learning. 2023</p>
<p>Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations. Payel Das, Tom Sercu, Kahini Wadhawan, Inkit Padhi, Sebastian Gehrmann, Flaviu Cipcigan, Hendrik Vijil Chenthamarakshan, Strobelt, Dos Cicero, Pin-Yu Santos, Chen, Nat. Biomed. Eng. 562021</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021, 2021</p>
<p>Translation between molecules and natural language. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, 2022 Conference on Empirical Methods in Natural Language Processing. 2022. 2022</p>
<p>Prottrans: Towards cracking the language of life's code through self-supervised deep learning and high-performance computing. Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Wang Yu, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, Burkhard Rost, 10.1109/TPAMI.2021.3095381IEEE Transactions on Pattern Analysis and Machine Intelligence. 2021</p>
<p>Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Peter Ertl, Ansgar Schuffenhauer, Journal of cheminformatics. 12009</p>
<p>Molecular representation learning with language models and domainrelevant auxiliary tasks. Thomas Benedek Fabian, Héléna Edlich, Marwin Gaspar, Joshua Segler, Marco Meyers, Mohamed Fiscato, Ahmed, arXiv:2011.132302020arXiv preprint</p>
<p>Towards artificial general intelligence via a multimodal foundation model. Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jingyuan Wen, Haoyu Lu, Ruihua Song, Xin Gao, Tao Xiang, Nature Communications. 13130942022</p>
<p>Language models can learn complex molecular distributions. Daniel Flam-Shepherd, Kevin Zhu, Alán Aspuru-Guzik, Nature Communications. 13132932022</p>
<p>IBM RXN for Chemistry team. rxn4chemistry: Python wrapper for the IBM RXN for Chemistry API. 2023</p>
<p>Huggingmolecules: An open-source library for transformer-based molecular property prediction (student abstract). Piotr Gaiński, Lukasz Maziarka, Tomasz Danel, Stanis Law, Jastrzebski , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Aizynthfinder: a fast, robust and flexible open-source software for retrosynthetic planning. Amol Samuel Genheden, Veronika Thakkar, Jean-Louis Chadimová, Ola Reymond, Esben Engkvist, Bjerrum, Journal of cheminformatics. 121702020</p>
<p>Open source and open data should be standard practices. Gezelter Daniel, 2015</p>
<p>Automatic chemical design using a data-driven continuous representation of molecules. Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, Alán Aspuru-Guzik, ACS central science. 422018</p>
<p>An open-source drug discovery platform enables ultra-large virtual screens. Christoph Gorgulla, Andras Boeszoermenyi, Zi-Fu Wang, Patrick D Fischer, Paul W Coote, Krishna M Padmanabha Das, Yehor S Malets, Yurii S Dmytro S Radchenko, David A Moroz, Scott, Nature. 58078052020</p>
<p>Chemical language models for de novo drug design: Challenges and opportunities. Francesca Grisoni, Current Opinion in Structural Biology. 791025272023</p>
<p>Translating the inchi: adapting neural machine translation to predict iupac names from a chemical identifier. Jennifer Handsel, Brian Matthews, Nicola J Knight, Simon J Coles, Journal of cheminformatics. 1312021</p>
<p>Serendipity in anticancer drug discovery. Emily Hargrave-Thomas, Bo Yu, Jóhannes Reynisson, World journal of clinical oncology. 3112012</p>
<p>Inchi, the iupac international chemical identifier. Alan Stephen R Heller, Igor Mcnaught, Stephen Pletnev, Dmitrii Stein, Tchekhovskoi, Journal of cheminformatics. 712015</p>
<p>Melloddy: cross pharma federated learning at unprecedented scale unlocks benefits in qsar without compromising proprietary information. Wouter Heyndrickx, Lewis Mervin, Tobias Morawietz, Noé Sturm, Lukas Friedrich, Adam Zalewski, Anastasia Pentina, Lina Humbeck, Martijn Oldenhof, Ritsuya Niwayama, 2022</p>
<p>Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Coley Connor, W , Cao Xiao, Jimeng Sun, Marinka Zitnik, Advances in Neural Information Processing System. 202135</p>
<p>Chemistry42: an ai-driven platform for molecular design and optimization. Daniil Yan A Ivanenkov, Dmitry Polykovskiy, Bogdan Bezrukov, Vladimir Zagribelnyy, Petrina Aladinskiy, Alex Kamya, Feng Aliper, Alex Ren, Zhavoronkov, Journal of Chemical Information and Modeling. 6332023</p>
<p>A fully differentiable set autoencoder. Nikita Janakarajan, Jannis Born, Matteo Manica, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Inferring algorithmic patterns with stack-augmented recurrent nets. Armand Joulin, Tomas Mikolov, Advances in neural information processing systems. 282015</p>
<p>Highly accurate protein structure prediction with alphafold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Nature. 59678732021</p>
<p>Pubchem 2019 update: improved access to chemical data. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Nucleic acids research. 47D12019</p>
<p>Self-referencing embedded strings (selfies): A 100% robust molecular string representation. Mario Krenn, Florian Häse, Akshatkumar Nigam, Pascal Friederich, Alan Aspuru-Guzik, Machine Learning: Science and Technology. 14450242020</p>
<p>. Greg Landrum. Rdkit documentation. Release. 42013</p>
<p>Inductive transfer learning for molecular activity prediction: Next-gen qsar models with molpmofit. Xinhao Li, Denis Fourches, Journal of Cheminformatics. 1212020</p>
<p>Smiles pair encoding: a data-driven substructure tokenization algorithm for deep learning. Xinhao Li, Denis Fourches, Journal of chemical information and modeling. 6142021</p>
<p>Molecular generative model based on conditional variational autoencoder for de novo molecular design. Jaechang Lim, Seongok Ryu, Jin Woo Kim, Woo Youn, Kim, Journal of cheminformatics. 1012018</p>
<p>Bigsmiles: a structurally-based line notation for describing macromolecules. Tzyy-Shyang Lin, Connor W Coley, Hidenobu Mochigase, Haley K Beech, Wencong Wang, Zi Wang, Eliot Woods, Stephen L Craig, Jeremiah A Johnson, Julia A Kalow, ACS central science. 592019</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Science. 37966372023</p>
<p>Unified deep learning model for multitask reaction predictions with explanation. Jieyu Lu, Yingkai Zhang, Journal of Chemical Information and Modeling. 6262022</p>
<p>Accelerating material design with the generative toolkit for scientific discovery. Matteo Manica, Jannis Born, Joris Cadow, Dimitrios Christofidellis, Ashish Dave, Dean Clarke, Yves Gaetan, Nana Teukam, Giorgio Giannone, Matthew Samuel C Hoffman, Buchan, npj Computational Materials. 91692023</p>
<p>Molecule-augmented attention transformer. Lukasz Maziarka, Tomasz Danel, Krzysztof Lawomir Mucha, Jacek Rataj, Tabor, Jastrzkebski, Workshop on Graph Representation Learning, Neural Information Processing Systems. 2019</p>
<p>Pawe l Morkisz, and Stanis law Jastrzkebski. Relative molecule self-attention transformer. Lukasz Maziarka, Dawid Majchrowski, Tomasz Danel, Piotr Gaiński, Jacek Tabor, Igor Podolak, arXiv:2110.058412021arXiv preprint</p>
<p>Learning to extend molecular scaffolds with structural motif. Krzysztof Maziarz, Henry Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schneider, Nikolaus Stiefl, Marwin Segler, Marc Brockschmidt, The Tenth International Conference on Learning Representations, ICLR. 2022</p>
<p>Molecule generation using transformers and policy gradient reinforcement learning. Eyal Mazuz, Guy Shtar, Bracha Shapira, Lior Rokach, Scientific Reports. 13187992023</p>
<p>Efficient estimation of word representations in vector space. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, arXiv:1301.37812013arXiv preprint</p>
<p>Foundation models for generalist medical artificial intelligence. Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein, Harlan M Abad, Jure Krumholz, Eric J Leskovec, Pranav Topol, Rajpurkar, Nature. 61679562023</p>
<p>. Openai, Chatgpt, 2023. August 8, 2023</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Artificial intelligence driven design of catalysts and materials for ring opening polymerization using a domain-specific language. Matteo Nathaniel H Park, Jannis Manica, James L Born, Tim Hedrick, Dmitry Erdmann, Nil Yu Zubarev, Pedro L Adell-Mill, Arrechea, Nature Communications. 14136862023</p>
<p>Transfer learning enables the molecular transformer to predict regio-and stereoselective reactions on carbohydrates. Giorgio Pesciullesi, Philippe Schwaller, Teodoro Laino, Jean-Louis Reymond, Nature communications. 11148742020</p>
<p>Estimation of the size of drug-like chemical space based on gdb-17 data. Timur I Pavel G Polishchuk, Alexandre Madzhidov, Varnek, J. Comput. Aid. Mol. Des. 2782013</p>
<p>Molecular sets (moses): a benchmarking platform for molecular generation models. Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Front. Pharmacol. 1119312020</p>
<p>Deep reinforcement learning for de novo drug design. Mariya Popova, Olexandr Isayev, Alexander Tropsha, Science advances. 4778852018</p>
<p>Biocatalysed synthesis planning using data-driven learning. Daniel Probst, Matteo Manica, Yves Gaetan, Nana Teukam, Alessandro Castrogiovanni, Federico Paratore, Teodoro Laino, Nature communications. 1319642022</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Deep Learning for the Life Sciences. Bharath Ramsundar, Peter Eastman, Patrick Walters, Vijay Pande, Karl Leswing, Zhenqin Wu, 2019O'Reilly Media</p>
<p>Journal of chemical information and modeling. David Rogers, Mathew Hahn, 201050Extended-connectivity fingerprints</p>
<p>Large-scale chemical language representations capture molecular structure and properties. Jerret Ross, Brian Belgodere, Inkit Vijil Chenthamarakshan, Youssef Padhi, Payel Mroueh, Das, Nature Machine Intelligence. 4122022</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, ICLR 2022-Tenth International Conference on Learning Representations. 2022</p>
<p>Diagnosing the decline in pharmaceutical r&amp;d efficiency. Alex Jack W Scannell, Helen Blanckley, Brian Boldon, Warrington, Nat. Rev. Drug Discov. 1132012</p>
<p>Designing catalysts with deep generative models and computational data. a case study for suzuki cross coupling reactions. Oliver Schilter, Alain Vaucher, Philippe Schwaller, Teodoro Laino, Digital Discovery. 232023</p>
<p>found in translation": predicting outcomes of complex organic chemistry reactions using neural sequenceto-sequence models. Philippe Schwaller, Theophile Gaudin, David Lanyi, Costas Bekas, Teodoro Laino, Chemical science. 9282018</p>
<p>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, Alpha A Lee, ACS central science. 592019</p>
<p>Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. Philippe Schwaller, Riccardo Petraglia, Valerio Zullo, H Vishnu, Rico Nair, Riccardo Andreas Haeuselmann, Costas Pisoni, Anna Bekas, Teodoro Iuliano, Laino, Chemical science. 11122020</p>
<p>Extraction of organic chemistry grammar from unsupervised learning of chemical reactions. Philippe Schwaller, Benjamin Hoover, Jean-Louis Reymond, Hendrik Strobelt, Teodoro Laino, Science Advances. 715e41662021</p>
<p>Mapping the space of chemical reactions using attention-based neural networks. Philippe Schwaller, Daniel Probst, Alain C Vaucher, H Vishnu, David Nair, Teodoro Kreutter, Jean-Louis Laino, Reymond, Nature machine intelligence. 322021</p>
<p>Machine intelligence for chemical reaction space. Philippe Schwaller, Alain C Vaucher, Ruben Laplaza, Charlotte Bunne, Andreas Krause, Clemence Corminboeuf, Teodoro Laino, Wiley Interdisciplinary Reviews: Computational Molecular Science. 125e16042022</p>
<p>Generating focused molecule libraries for drug discovery with recurrent neural networks. Marwin Hs Segler, Thierry Kogej, Christian Tyrchan, Mark P Waller, ACS central science. 412018</p>
<p>. T Taffee, Tanimoto, Nov, 17:1957, 1957Ibm internal report</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Augmentation is what you need. Pavel Igor V Tetko, Eric Karpov, Talia B Bruno, Guillaume Kimber, Godin, International Conference on Artificial Neural Networks. Springer2019</p>
<p>Unbiasing retrosynthesis language models with disconnection prompts. Amol Thakkar, Alain C Vaucher, Andrea Byekwaso, Philippe Schwaller, Alessandra Toniato, Teodoro Laino, 2023ACS Central Science</p>
<p>Unassisted noise reduction of chemical reaction datasets. Alessandra Toniato, Philippe Schwaller, Antonio Cardinale, Joppe Geluykens, Teodoro Laino, Nature Machine Intelligence. 362021</p>
<p>Improving the quality of chemical language model outcomes with atom-in-smiles tokenization. Islambek Umit V Ucak, Juyong Ashyrmamatov, Lee, Journal of Cheminformatics. 151552023</p>
<p>Gen: highly efficient smiles explorer using autodidactic generative examination networks. Peter Ruud Van Deursen, Igor V Ertl, Guillaume Tetko, Godin, Journal of Cheminformatics. 1212020</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Automated extraction of chemical synthesis actions from experimental procedures. Alain C Vaucher, Federico Zipoli, Joppe Geluykens, H Vishnu, Philippe Nair, Teodoro Schwaller, Laino, Nature communications. 11136012020</p>
<p>Inferring experimental procedures from text-based representations of chemical reactions. Alain C Vaucher, Philippe Schwaller, Joppe Geluykens, H Vishnu, Anna Nair, Teodoro Iuliano, Laino, Nature communications. 12125732021</p>
<p>Diffusers: State-of-the-art diffusion models. Suraj Patrick Von Platen, Anton Patil, Pedro Lozhkov, Nathan Cuenca, Kashif Lambert, Mishig Rasul, Thomas Davaadorj, Wolf, 10 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, J. Chem. Inf. Comp. Sci. 2811988</p>
<p>Assessment of chemistry knowledge in large language models that generate code. Glen M Andrew D White, Heta A Hocky, Mehrad Gandhi, Sam Ansari, Cox, Subarna Geemi P Wellawatte, Ziyue Sasmal, Kangxin Yang, Yuvraj Liu, Singh, Digital Discovery. 222023</p>
<p>Prediction of physicochemical parameters by atomic contributions. A Scott, Gordon M Wildman, Crippen, Journal of chemical information and computer sciences. 3951999</p>
<p>Transformers: State-ofthe-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. the 2020 conference on empirical methods in natural language processing: system demonstrations2020</p>
<p>Estimated research and development investment needed to bring a new medicine to market. Martin Olivier J Wouters, Jeroen Mckee, Luyten, Jama. 32392009. 2018. 2020</p>
<p>Moleculenet: a benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, S Aneesh, Karl Pappu, Vijay Leswing, Pande, Chemical science. 922018</p>
<p>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Zheni Zeng, Yuan Yao, Zhiyuan Liu, Maosong Sun, Nature communications. 1318622022</p>
<p>Deep learning enables rapid identification of potent ddr1 kinase inhibitors. Alex Zhavoronkov, Yan A Ivanenkov, Alex Aliper, Mark S Veselov, Vladimir A Aladinskiy, Anastasiya V Aladinskaya, Daniil A Victor A Terentiev, Polykovskiy, Arip Maksim D Kuznetsov, Asadulaev, Nat. Biotechnol. 3792019</p>
<p>Torchdrug: A powerful and flexible machine learning platform for drug discovery. Zhaocheng Zhu, Chence Shi, Zuobai Zhang, Shengchao Liu, Minghao Xu, Xinyu Yuan, Yangtian Zhang, Junkun Chen, Huiyu Cai, Jiarui Lu, 2022Preprint at</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8837 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8837</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8837</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-269302979</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.14809v2.pdf" target="_blank">A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications</a></p>
                <p><strong>Paper Abstract:</strong> A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems. Recently, large language models (LLMs) have showcased a strong generalization ability to handle various natural language processing tasks to answer users’ arbitrary questions and generate specific-domain content. Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation. However, LLMs are sequential models for textual data, but graphs are non-sequential topological data. It is challenging to adapt LLMs to tackle graph analytics tasks. In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing challenges and future directions. Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) in terms of three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications. LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graphs and LLMs, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning, and graph representation. We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks. Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of the discussed LLM-GGA models. We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8837.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8837.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GDL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Description Language (GDL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of serializations that convert graph-structured data into sequential/textual forms so they can be consumed by LLMs; includes natural-language descriptions, adjacency lists, graph-query languages, and file-format serializations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Description Language (GDL)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>High-level category of methods that express graph nodes, edges, and attributes as sequences (natural-language sentences, adjacency lists, query language strings, edge lists, GML/GraphML, or story-like encodings) to feed into LLMs while attempting to preserve structural and attribute information.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs; text-attributed graphs; knowledge graphs; molecular graphs (depending on chosen sub-format)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Multiple possible conversions: (1) Natural-language text sentences describing nodes/edges; (2) adjacency lists enumerating neighbors; (3) graph query language strings (Cypher/Gremlin/SPARQL/GSQL); (4) edge lists or standardized formats (GML, GraphML); (5) narrative/story framing that places nodes/edges into contextual sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph structure understanding, graph query answering, node classification, link prediction, graph construction, KGQA, graph-to-text generation, embedding learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No single numeric metrics reported in survey for GDL as a category; the paper notes tasks evaluated using accuracy, F1, AUC, Hits@k, ROUGE/BLEU for text outputs, and task-specific metrics depending on downstream evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey discusses trade-offs among subtypes: natural-language descriptions are easier for LLMs to parse but less precise; adjacency lists and formal query languages are precise but harder for LLMs; structured formats (GML/GraphML) can encode larger graphs but are more challenging for prompt comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Flexible (many formats), can encode node/edge attributes and semantics, enables direct use of LLMs without specialized GNN training, allows use of LLM's reasoning and language abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Different GDLs produce varying LLM performance; structured/precise encodings are harder for LLMs to understand; long graphs can exceed LLM input windows; risk of information loss or ambiguity in natural-language encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Large-scale graphs (tens of thousands of nodes) are infeasible to input directly; LLMs struggle on structured/chain/clique topologies and complex NP-hard tasks when graph is encoded inadequately; structured formats may lead to comprehension failures if not suitably contextualized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8837.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextDesc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural-language Text Description (graph-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Describing graph elements as natural-language sentences (e.g., 'Node A is connected to Node B') to leverage LLMs' native text processing capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Natural-language linearization (text description)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode nodes, edges and attributes as fluent sentences or story fragments that describe connectivity and attributes in plain language; often used in prompts or examples supplied to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs, text-attributed graphs, small knowledge graphs, molecular graphs (via textual descriptions of structures)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Map each graph triple or adjacency relation to a sentence or short clause; optionally add narrative context ('Alice is friends with Bob') or aggregate neighbor lists into sentence templates.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph question answering, graph structure understanding (shortest path, degree, clustering coefficient), graph construction from text, node classification (when combined with attributes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports qualitative findings: natural-language descriptions can be effective for small graphs and improve LLM comprehension, but no single numeric benchmarks are reported specifically for this encoding in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to adjacency lists and formal graph languages, natural-language is easier for LLMs to parse but less compact and precise; structured encodings outperform it on precise algorithmic tasks when the LLM can interpret formal syntax.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Human-readable, leverages LLM's pretraining on natural language, can incorporate semantics and explanations, often yields better comprehension for small/medium graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Verbose (longer sequences), potential ambiguity, harder to guarantee lossless structural fidelity, scales poorly with graph size.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Large graphs exceed context windows; tasks requiring strict structural precision (e.g., exact topological computations) may suffer due to ambiguity or verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8837.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdjList</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adjacency List Serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representing a graph by listing for each vertex the set of its neighbors (e.g., 'N(A) = {B, C}'), a compact structural serialization often used in prompts or input sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Adjacency list serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each node is listed together with its neighbors (and optionally edge attributes) in a standardized textual form; the list of adjacency lines forms the sequential input.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs including directed/undirected, static graphs, text-attributed graphs (with node textual attributes appended)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each node, generate a line or sentence enumerating neighboring node IDs or labels; can be combined with node attributes in the same line.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph algorithm tasks (degree, shortest path, topological sort), structure understanding, input to LLM prompts for node/link queries</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey notes adjacency lists can describe larger graphs with dozens of nodes; no specific numeric performance reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Adjacency lists are more compact than natural-language narration and often easier to align with algorithmic problems, but less semantically rich than text descriptions and less formal than query languages.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Concise, captures explicit neighbor relationships, scales better than prose for medium-sized graphs, straightforward for LLMs to parse when prompts specify expected format.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Still limited by context window for large graphs; lacks semantic explanations; LLMs may misinterpret ordering or fail to infer global properties from local lists alone.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Very dense or very large graphs lead to long adjacency lists exceeding token limits; LLMs may miscount or mis-handle adjacency indexing leading to incorrect algorithmic answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8837.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GQL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Query Languages (Cypher/Gremlin/SPARQL/GSQL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using graph query languages as textual encodings for graphs or as part of prompts (e.g., Cypher or SPARQL queries) to represent structure precisely and enable LLMs to generate or interpret queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph query language serialization (Cypher/Gremlin/SPARQL/GSQL)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Express the graph (or queries over it) in an existing graph query language syntax; prompts can include GQL snippets or LLMs can be instructed to output GQL answers.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs, property graphs, RDF graphs, and general graphs where a GQL is appropriate</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize nodes/edges/attributes into GQL constructs (nodes/relationships/triples) or include GQL as the prompt/query language; LLMs may be asked to convert natural language to GQL or to answer using GQL.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph query generation (GQL generation), KGQA, structured graph question answering, precise retrieval tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey notes use in experiments (e.g., GQL generation tasks) but does not report unified numeric metrics for GQL encodings; KGQA metrics like EM, Hits@k are used in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GQL offers precise, executable semantics (advantage over prose) but is harder for LLMs to generate/understand without instruction tuning; natural-language prompts are easier but less precise.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Precise and executable representation; supports complex multi-hop queries and unambiguous structure; enables integration with graph databases.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Syntax and structure can be hard for LLMs to master; brittle to small formatting errors; may require specialized fine-tuning or self-prompting to translate NL to GQL reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>LLMs may produce ill-formed queries or hallucinated relations; translating large graphs into GQL for prompt ingestion can exceed context limits; execution required to verify correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8837.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StructFormats</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Graph Formats (edge lists, GML, GraphML)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard file/line-based serializations (edge lists, GML, GraphML) used to represent graphs in structured textual formats that can be fed to LLMs or converted to NL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Structured graph formats (edge list / GML / GraphML)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use established textual graph formats to encode complete graph data (nodes, edges, attributes) in a formal syntax, typically more compact and machine-readable than prose.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs, medium-scale graphs, graphs with attributes (molecular graphs, social graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Export the graph to an edge-list or GML/GraphML textual file; supply that textual representation directly in prompts or use an LLM to translate it to an NL summary.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph queries, structure understanding, ingestion into downstream systems or conversion to embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No quantitative metrics reported in survey specific to these formats; survey notes structured descriptions precisely represent graphs but are harder for LLM comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>More precise and compact than NL descriptions; compared to adjacency lists, offers richer attribute encoding; compared to GQL, less query-oriented but more generic.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Lossless/precise encoding, standard formats; good for programmatic parsing and possible downstream execution.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Less friendly to raw LLM consumption without adaptation or explanation; may require conversion or metadata to be useful for LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>LLMs may not robustly interpret raw GML/GraphML without specialized prompt engineering; very large serialized files exceed context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8837.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WalkLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WalkLM (random-walk linearization for LM training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that produces text-like sequences by performing random walks on a text-attributed graph and uses those sequences to train/fine-tune language models for graph embedding learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WalkLM: A uniform language model fine-tuning framework for attributed graph embedding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Random-walk linearization (WalkLM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generate node sequences by random-walking through the graph; for TAGs, incorporate node textual attributes into the walk sequence; feed sequences into a masked-language-model objective to learn token/node embeddings that capture structure and text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs (TAGs), citation graphs, molecular graphs with textual annotations</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Sample random walks across the graph to produce ordered node sequences; concatenate node text or IDs along the walk to form textual training sequences for an LM (typically masked LM fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph embedding learning, node classification, link prediction, representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey describes WalkLM as used for self-supervised training and downstream tasks but does not report specific numeric results in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to Path-LLM, WalkLM uses random walks (masked LM) while Path-LLM uses path selection aligned with causal LM; WalkLM is more general but may introduce noise from random walks.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Integrates graph structure with textual node information via self-supervision; can produce unified embeddings usable across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Random walks may include noisy or redundant paths; masked LM objective may not align with causal LLM objectives; selection bias in walks affects learned representation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May perform poorly when random walks fail to capture long-range bridge relationships or when walks over-emphasize dense clusters leading to locality bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8837.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Path-LLM (shortest-path based linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that selects informative long shortest paths (L2SP-based) to linearize a graph in a way aligned with causal language modeling objectives, mitigating noise and better capturing inter-cluster relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Path-LLM: A shortest-pathbased llm learning for unified graph representation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Shortest-path path-selection linearization (Path-LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Select long shortest paths that connect different dense groups (L2SP-based selection) and serialize these paths into sequences aligned with an autoregressive LM training order (causal LM), then self-supervise to learn graph-aware LM embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs, especially graphs where bridge paths between communities are informative (e.g., citation/networks with communities)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Compute long shortest paths covering bridge edges (L2SP mechanism) and convert each path into an ordered textual sequence (e.g., node-token sequence plus attributes) that fits causal LM training.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Unified graph representation learning, node classification, keyword search, general graph analytics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports Path-LLM aims to mitigate noise and align with causal LM training but does not provide specific numerical performance values in the survey itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to WalkLM, Path-LLM addresses noise by carefully selecting shortest paths and aligns order with causal LM, which the survey suggests is more suitable for LLMs trained with causal language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Better at capturing relationships between dense groups and compatible with causal LM objectives; reduces noise compared to random-walk methods.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires shortest-path computations (costly on large graphs); path-selection heuristics may miss other informative structures; still constrained by sequence length.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Performance may degrade on extremely large graphs due to path selection cost, and in graphs where shortest paths are not the most semantically informative structures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8837.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphText (grammar tree → text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that reformulates graph reasoning as a text-to-text problem by building grammar trees for graphs and traversing them to generate textual sequences which LLMs can process.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GraphText: Graph reasoning in text space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Grammar-tree traversal linearization (GraphText)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct a grammar tree representation of the graph, traverse the tree to produce text sequences, and map graph reasoning tasks into text-to-text tasks so LLMs can operate in their native modality.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs; especially used for tasks that can be mapped to text-space reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Build grammar trees reflecting graph topology/relations and perform a traversal (preorder/postorder) that emits textual tokens representing nodes/edges/relations, creating LLM-ready sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph reasoning converted to text-to-text tasks, node classification, graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey describes GraphText's approach and its applicability but does not list specific numerical results within the survey body.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GraphText focuses on fully mapping graph reasoning into text space, whereas WalkLM/Path-LLM produce sequences for embedding training; GraphText is oriented to direct reasoning as text.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables use of text-to-text LLM strengths for graph reasoning and can leverage sequence-to-sequence objectives; conceptual alignment with LLM capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Designing grammar trees and traversals that preserve necessary structure is nontrivial; may lose some structural nuance depending on grammar design.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If grammar-tree construction omits critical structural context or traversal order breaks important dependencies, reasoning and downstream performance degrade.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8837.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TalkGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TalkGraph / Graph-encoded prompts (contextual narrative encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoding the same graph in multiple natural-language contexts or narratives so LLMs can interpret node roles and local structure from diverse textual viewpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-encoded prompts / contextual narrative encoding (TalkGraph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Place graph elements into different textual contexts (stories or roles) to provide varied semantic perspectives for LLMs, improving interpretability and node-specific queries.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-attributed graphs, social graphs, TAGs where node semantics are important</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Generate multiple textual contexts/stories that describe particular nodes and edges (e.g., 'David is friends with X and collaborates with Y'), and include these contexts as prompt input.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node/link classification, graph question answering, representation learning when used as prompt-encoding technique</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey notes TalkGraph demonstrates diverse encodings improve LLM understanding for some tasks but does not provide precise quantitative metrics within the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Emphasizes contextual diversity in text encoding versus single canonical serializations (adjacency lists or GML); compared with GraphTMI, TalkGraph focuses on text-only modality.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Can reveal different semantic interpretations of a node, making it easier for LLMs to infer roles or attributes; flexible and human-interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires careful prompt design; can increase token usage substantially; diversity may introduce contradictory contexts confusing the model.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Conflicting contexts may lead to inconsistent answers; scaling to large graphs is limited by token budget.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8837.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphTMI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphTMI (text+image+motif encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal graph encoding approach that uses text modality for local structure, motif modality to capture common substructures (stars, triangles), and image modality to represent global context for LLMs/LLM-Vision models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GraphTMI: Which modality should i use-text, motif, or image?: Understanding graphs with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Multimodal graph-encoded prompts (text + motif + image)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode graph information across multiple modalities: textual descriptions for local neighborhoods, motif summaries to capture structural patterns, and images/visualizations to convey global structure; supply combined modalities to LLM or LLM-Vision.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs with motif structure (social networks, citation graphs, molecular graphs where motifs are meaningful)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Extract motif counts/structures and encode them as text; render graph visualizations for image input; create context sentences for local node neighborhoods and combine into a multi-part prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph representation, node/list classification, structural reasoning, graph understanding tasks of varying difficulty</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey indicates GraphTMI uses motif/text/image modalities and reports task-level evaluations in the original work; the survey does not reproduce numeric scores here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GraphTMI contrasts with text-only encodings (e.g., TalkGraph) by demonstrating that motif and visual encodings can capture structure that text alone misses.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Combines complementary modalities to capture both local patterns and global layout; can improve LLM understanding of structure that is hard to express in pure text.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires multimodal LLMs or additional tooling; generating motif summaries and images adds preprocessing cost; increased pipeline complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If modality fusion is poor or LLM-Vision capabilities are limited, gains may not appear; modality mismatch and token/bandwidth limits constrain scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8837.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>API-CallPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>API-call Prompting (Graph-ToolFormer style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting pattern where LLM outputs structured API call tokens (e.g., GL('benzenering')) that are mapped to graph-tool functions; the external tool executes graph algorithms and returns results to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-ToolFormer: To empower LLMs with graph reasoning ability via prompt augmented by chatgpt</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>API-call prompt serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use LLMs to generate special API-call strings embedded in natural language; these strings correspond to pre-defined graph-tool operations which are executed externally, enabling handling of large/complex computations beyond the LLM's reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs, large graphs, dynamic graphs where external tool computation is needed</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>LLM converts queries into API-call expressions (tool invocation tokens); a graph reasoning tool or library executes the calls (e.g., compute diameter, shortest path) and the tool output is fed back into the LLM for final response.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph structure understanding, large-scale queries, NP-hard subroutines offloaded to external tools, interactive graph agents</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey cites Graph-ToolFormer and related works showing improved correctness and reduced hallucination via API usage but does not give unified numeric metrics in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>API-call approach offloads heavy computations to exact graph tools vs pure in-LM encodings which try to reason solely from serialized graph text; the API approach improves correctness at expense of engineering complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables exact computation, scales to large graphs, reduces LLM hallucinations on algorithmic results, supports verification via tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires external tooling and tool-LLM integration; LLM must learn correct API-call formats; adds system complexity and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If API-call translation is incorrect or tools are missing expected functionality, the pipeline fails; dependence on tool quality and interface correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8837.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-prompting transformation (GPT4Graph style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where the LLM iteratively refines and generates its own prompt representations (e.g., converting GSQL/GML into clearer textual descriptions) to better understand graph structure and task instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Self-prompting graph-to-text transformation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>LLM generates intermediate prompts or reformulations of the input graph description (e.g., expand GSQL into natural-language summaries, produce simplified adjacency descriptions) and uses these self-generated prompts to improve downstream reasoning or prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs and knowledge graphs represented in structured formats (GSQL, GML)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Input structured graph representation into LLM; request the model to produce a clearer natural-language or stepwise representation; iteratively refine until a satisfactory textual description is obtained for the actual downstream prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph structure understanding, GQL generation, improved prompting for node/link tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports self-prompting is used to refine prompts and can help LLM comprehension; no specific numeric metrics reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Self-prompting can outperform static manual prompts by tailoring representation to the model, but may be less efficient than a well-designed single prompt or supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Adaptive to the LLM's current capabilities, can clarify complex structured inputs, reduces need for manual prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May cascade errors if initial reformulations are poor; adds extra rounds of model calls (costly).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If the LLM's reformulation is inaccurate or ambiguous, subsequent reasoning/performance degrades; sensitive to prompt design for self-prompting loop.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8837.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-to-text (GLaM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-to-text conversion for LLM fine-tuning (GLaM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert knowledge graph substructures into textual examples (paired with questions/answers) so that LLMs can be fine-tuned to better recall and reason with domain-specific KG knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLaM: Fine-tuning large language models for domain knowledge graph alignment via neighborhood partitioning and generative subgraph encoding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Knowledge-graph-to-text QA conversion (GLaM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transform KG triples/subgraphs into labeled text QA pairs or textual contexts which are then used to fine-tune or instruction-tune LLMs to incorporate domain-specific KG knowledge into their generative responses.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs, domain-specific KGs (biomedical, bibliographic), ontologies</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Partition KG neighborhoods, generate textual encodings of subgraphs and synthetic QA pairs describing facts/relations, and then use these text pairs for supervised fine-tuning of the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Fact-aware language modeling, KGQA, domain-specific question answering, reduction of hallucination</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey states GLaM is used to improve domain KG alignment and LLM recall; exact numeric improvements are not reproduced in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>KG-to-text fine-tuning is complementary to RAG approaches; it helps LLM internalize KG facts rather than retrieving them at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Improves domain knowledge recall and reduces hallucination for KG facts; enables closed-book factual knowledge transfer into LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires constructing large volumes of high-quality text-QA pairs and careful partitioning; may still miss rare facts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Incomplete or noisy KG-to-text pairs can introduce erroneous facts; model may still hallucinate beyond covered KG content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8837.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8837.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PiVe/EDC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PiVe (Iterative Verification) / EDC (Extract-Modify-Verify)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting frameworks for text-to-graph generation: LLMs extract triples from text, iteratively verify and modify outputs (PiVe), or run a multi-phase extract and automatic modification (EDC) to improve quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PiVe: Prompting with iterative verification improving graph-based generative capability of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Iterative verification-based text-to-graph conversion (PiVe / EDC)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use multi-step prompting where the LLM extracts entity-relation triples, then performs verification and iterative correction steps (PiVe) or a three-phase extract/modify/validate pipeline (EDC) to produce higher-quality semantic graphs from raw text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Semantic graphs, knowledge graphs extracted from natural language, text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Phase 1: extract candidate triples; Phase 2: verify/validate and modify incorrect or ambiguous triples (possibly with rules or secondary prompts); Phase 3: output canonicalized graph representation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Knowledge graph construction, information extraction, text-to-graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey describes these frameworks as improving graph generation quality but does not provide numeric comparisons in the survey body.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Improves over single-pass extraction by adding verification/repair loops; more reliable than one-shot triple extraction but more compute-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Higher precision and cleaner graph outputs due to verification; reduces hallucinated or spurious triples.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Additional LLM calls increase cost and latency; requires design of verification prompts or external validators.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If verifier is itself unreliable, iterative steps may not converge; complex semantics or implicit relations still hard to extract/verify.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications', 'publication_date_yy_mm': '2025-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>WalkLM: A uniform language model fine-tuning framework for attributed graph embedding <em>(Rating: 2)</em></li>
                <li>Path-LLM: A shortest-pathbased llm learning for unified graph representation <em>(Rating: 2)</em></li>
                <li>GraphText: Graph reasoning in text space <em>(Rating: 2)</em></li>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>GraphTMI: Which modality should i use-text, motif, or image?: Understanding graphs with large language models <em>(Rating: 2)</em></li>
                <li>NLGraph: Can language models reason with graphs and structures? <em>(Rating: 2)</em></li>
                <li>Graph-ToolFormer: To empower LLMs with graph reasoning ability via prompt augmented by chatgpt <em>(Rating: 2)</em></li>
                <li>GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking <em>(Rating: 2)</em></li>
                <li>GLaM: Fine-tuning large language models for domain knowledge graph alignment via neighborhood partitioning and generative subgraph encoding <em>(Rating: 2)</em></li>
                <li>PiVe: Prompting with iterative verification improving graph-based generative capability of LLMs <em>(Rating: 2)</em></li>
                <li>EDC: A three-phase framework for semantic graph generation from text <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8837",
    "paper_id": "paper-269302979",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "GDL",
            "name_full": "Graph Description Language (GDL)",
            "brief_description": "A family of serializations that convert graph-structured data into sequential/textual forms so they can be consumed by LLMs; includes natural-language descriptions, adjacency lists, graph-query languages, and file-format serializations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Graph Description Language (GDL)",
            "representation_description": "High-level category of methods that express graph nodes, edges, and attributes as sequences (natural-language sentences, adjacency lists, query language strings, edge lists, GML/GraphML, or story-like encodings) to feed into LLMs while attempting to preserve structural and attribute information.",
            "graph_type": "General graphs; text-attributed graphs; knowledge graphs; molecular graphs (depending on chosen sub-format)",
            "conversion_method": "Multiple possible conversions: (1) Natural-language text sentences describing nodes/edges; (2) adjacency lists enumerating neighbors; (3) graph query language strings (Cypher/Gremlin/SPARQL/GSQL); (4) edge lists or standardized formats (GML, GraphML); (5) narrative/story framing that places nodes/edges into contextual sentences.",
            "downstream_task": "Graph structure understanding, graph query answering, node classification, link prediction, graph construction, KGQA, graph-to-text generation, embedding learning",
            "performance_metrics": "No single numeric metrics reported in survey for GDL as a category; the paper notes tasks evaluated using accuracy, F1, AUC, Hits@k, ROUGE/BLEU for text outputs, and task-specific metrics depending on downstream evaluation.",
            "comparison_to_others": "Survey discusses trade-offs among subtypes: natural-language descriptions are easier for LLMs to parse but less precise; adjacency lists and formal query languages are precise but harder for LLMs; structured formats (GML/GraphML) can encode larger graphs but are more challenging for prompt comprehension.",
            "advantages": "Flexible (many formats), can encode node/edge attributes and semantics, enables direct use of LLMs without specialized GNN training, allows use of LLM's reasoning and language abilities.",
            "disadvantages": "Different GDLs produce varying LLM performance; structured/precise encodings are harder for LLMs to understand; long graphs can exceed LLM input windows; risk of information loss or ambiguity in natural-language encodings.",
            "failure_cases": "Large-scale graphs (tens of thousands of nodes) are infeasible to input directly; LLMs struggle on structured/chain/clique topologies and complex NP-hard tasks when graph is encoded inadequately; structured formats may lead to comprehension failures if not suitably contextualized.",
            "uuid": "e8837.0",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "TextDesc",
            "name_full": "Natural-language Text Description (graph-to-text)",
            "brief_description": "Describing graph elements as natural-language sentences (e.g., 'Node A is connected to Node B') to leverage LLMs' native text processing capability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Natural-language linearization (text description)",
            "representation_description": "Encode nodes, edges and attributes as fluent sentences or story fragments that describe connectivity and attributes in plain language; often used in prompts or examples supplied to LLMs.",
            "graph_type": "General graphs, text-attributed graphs, small knowledge graphs, molecular graphs (via textual descriptions of structures)",
            "conversion_method": "Map each graph triple or adjacency relation to a sentence or short clause; optionally add narrative context ('Alice is friends with Bob') or aggregate neighbor lists into sentence templates.",
            "downstream_task": "Graph question answering, graph structure understanding (shortest path, degree, clustering coefficient), graph construction from text, node classification (when combined with attributes)",
            "performance_metrics": "Survey reports qualitative findings: natural-language descriptions can be effective for small graphs and improve LLM comprehension, but no single numeric benchmarks are reported specifically for this encoding in the survey.",
            "comparison_to_others": "Compared to adjacency lists and formal graph languages, natural-language is easier for LLMs to parse but less compact and precise; structured encodings outperform it on precise algorithmic tasks when the LLM can interpret formal syntax.",
            "advantages": "Human-readable, leverages LLM's pretraining on natural language, can incorporate semantics and explanations, often yields better comprehension for small/medium graphs.",
            "disadvantages": "Verbose (longer sequences), potential ambiguity, harder to guarantee lossless structural fidelity, scales poorly with graph size.",
            "failure_cases": "Large graphs exceed context windows; tasks requiring strict structural precision (e.g., exact topological computations) may suffer due to ambiguity or verbosity.",
            "uuid": "e8837.1",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "AdjList",
            "name_full": "Adjacency List Serialization",
            "brief_description": "Representing a graph by listing for each vertex the set of its neighbors (e.g., 'N(A) = {B, C}'), a compact structural serialization often used in prompts or input sequences.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Adjacency list serialization",
            "representation_description": "Each node is listed together with its neighbors (and optionally edge attributes) in a standardized textual form; the list of adjacency lines forms the sequential input.",
            "graph_type": "General graphs including directed/undirected, static graphs, text-attributed graphs (with node textual attributes appended)",
            "conversion_method": "For each node, generate a line or sentence enumerating neighboring node IDs or labels; can be combined with node attributes in the same line.",
            "downstream_task": "Graph algorithm tasks (degree, shortest path, topological sort), structure understanding, input to LLM prompts for node/link queries",
            "performance_metrics": "Survey notes adjacency lists can describe larger graphs with dozens of nodes; no specific numeric performance reported in survey.",
            "comparison_to_others": "Adjacency lists are more compact than natural-language narration and often easier to align with algorithmic problems, but less semantically rich than text descriptions and less formal than query languages.",
            "advantages": "Concise, captures explicit neighbor relationships, scales better than prose for medium-sized graphs, straightforward for LLMs to parse when prompts specify expected format.",
            "disadvantages": "Still limited by context window for large graphs; lacks semantic explanations; LLMs may misinterpret ordering or fail to infer global properties from local lists alone.",
            "failure_cases": "Very dense or very large graphs lead to long adjacency lists exceeding token limits; LLMs may miscount or mis-handle adjacency indexing leading to incorrect algorithmic answers.",
            "uuid": "e8837.2",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "GQL",
            "name_full": "Graph Query Languages (Cypher/Gremlin/SPARQL/GSQL)",
            "brief_description": "Using graph query languages as textual encodings for graphs or as part of prompts (e.g., Cypher or SPARQL queries) to represent structure precisely and enable LLMs to generate or interpret queries.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Graph query language serialization (Cypher/Gremlin/SPARQL/GSQL)",
            "representation_description": "Express the graph (or queries over it) in an existing graph query language syntax; prompts can include GQL snippets or LLMs can be instructed to output GQL answers.",
            "graph_type": "Knowledge graphs, property graphs, RDF graphs, and general graphs where a GQL is appropriate",
            "conversion_method": "Serialize nodes/edges/attributes into GQL constructs (nodes/relationships/triples) or include GQL as the prompt/query language; LLMs may be asked to convert natural language to GQL or to answer using GQL.",
            "downstream_task": "Graph query generation (GQL generation), KGQA, structured graph question answering, precise retrieval tasks",
            "performance_metrics": "Survey notes use in experiments (e.g., GQL generation tasks) but does not report unified numeric metrics for GQL encodings; KGQA metrics like EM, Hits@k are used in related work.",
            "comparison_to_others": "GQL offers precise, executable semantics (advantage over prose) but is harder for LLMs to generate/understand without instruction tuning; natural-language prompts are easier but less precise.",
            "advantages": "Precise and executable representation; supports complex multi-hop queries and unambiguous structure; enables integration with graph databases.",
            "disadvantages": "Syntax and structure can be hard for LLMs to master; brittle to small formatting errors; may require specialized fine-tuning or self-prompting to translate NL to GQL reliably.",
            "failure_cases": "LLMs may produce ill-formed queries or hallucinated relations; translating large graphs into GQL for prompt ingestion can exceed context limits; execution required to verify correctness.",
            "uuid": "e8837.3",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "StructFormats",
            "name_full": "Structured Graph Formats (edge lists, GML, GraphML)",
            "brief_description": "Standard file/line-based serializations (edge lists, GML, GraphML) used to represent graphs in structured textual formats that can be fed to LLMs or converted to NL.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Structured graph formats (edge list / GML / GraphML)",
            "representation_description": "Use established textual graph formats to encode complete graph data (nodes, edges, attributes) in a formal syntax, typically more compact and machine-readable than prose.",
            "graph_type": "General graphs, medium-scale graphs, graphs with attributes (molecular graphs, social graphs)",
            "conversion_method": "Export the graph to an edge-list or GML/GraphML textual file; supply that textual representation directly in prompts or use an LLM to translate it to an NL summary.",
            "downstream_task": "Graph queries, structure understanding, ingestion into downstream systems or conversion to embeddings",
            "performance_metrics": "No quantitative metrics reported in survey specific to these formats; survey notes structured descriptions precisely represent graphs but are harder for LLM comprehension.",
            "comparison_to_others": "More precise and compact than NL descriptions; compared to adjacency lists, offers richer attribute encoding; compared to GQL, less query-oriented but more generic.",
            "advantages": "Lossless/precise encoding, standard formats; good for programmatic parsing and possible downstream execution.",
            "disadvantages": "Less friendly to raw LLM consumption without adaptation or explanation; may require conversion or metadata to be useful for LLM prompts.",
            "failure_cases": "LLMs may not robustly interpret raw GML/GraphML without specialized prompt engineering; very large serialized files exceed context windows.",
            "uuid": "e8837.4",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "WalkLM",
            "name_full": "WalkLM (random-walk linearization for LM training)",
            "brief_description": "A framework that produces text-like sequences by performing random walks on a text-attributed graph and uses those sequences to train/fine-tune language models for graph embedding learning.",
            "citation_title": "WalkLM: A uniform language model fine-tuning framework for attributed graph embedding",
            "mention_or_use": "mention",
            "representation_name": "Random-walk linearization (WalkLM)",
            "representation_description": "Generate node sequences by random-walking through the graph; for TAGs, incorporate node textual attributes into the walk sequence; feed sequences into a masked-language-model objective to learn token/node embeddings that capture structure and text.",
            "graph_type": "Text-attributed graphs (TAGs), citation graphs, molecular graphs with textual annotations",
            "conversion_method": "Sample random walks across the graph to produce ordered node sequences; concatenate node text or IDs along the walk to form textual training sequences for an LM (typically masked LM fine-tuning).",
            "downstream_task": "Graph embedding learning, node classification, link prediction, representation learning",
            "performance_metrics": "Survey describes WalkLM as used for self-supervised training and downstream tasks but does not report specific numeric results in the survey text.",
            "comparison_to_others": "Compared to Path-LLM, WalkLM uses random walks (masked LM) while Path-LLM uses path selection aligned with causal LM; WalkLM is more general but may introduce noise from random walks.",
            "advantages": "Integrates graph structure with textual node information via self-supervision; can produce unified embeddings usable across tasks.",
            "disadvantages": "Random walks may include noisy or redundant paths; masked LM objective may not align with causal LLM objectives; selection bias in walks affects learned representation.",
            "failure_cases": "May perform poorly when random walks fail to capture long-range bridge relationships or when walks over-emphasize dense clusters leading to locality bias.",
            "uuid": "e8837.5",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "Path-LLM",
            "name_full": "Path-LLM (shortest-path based linearization)",
            "brief_description": "A method that selects informative long shortest paths (L2SP-based) to linearize a graph in a way aligned with causal language modeling objectives, mitigating noise and better capturing inter-cluster relationships.",
            "citation_title": "Path-LLM: A shortest-pathbased llm learning for unified graph representation",
            "mention_or_use": "mention",
            "representation_name": "Shortest-path path-selection linearization (Path-LLM)",
            "representation_description": "Select long shortest paths that connect different dense groups (L2SP-based selection) and serialize these paths into sequences aligned with an autoregressive LM training order (causal LM), then self-supervise to learn graph-aware LM embeddings.",
            "graph_type": "General graphs, especially graphs where bridge paths between communities are informative (e.g., citation/networks with communities)",
            "conversion_method": "Compute long shortest paths covering bridge edges (L2SP mechanism) and convert each path into an ordered textual sequence (e.g., node-token sequence plus attributes) that fits causal LM training.",
            "downstream_task": "Unified graph representation learning, node classification, keyword search, general graph analytics",
            "performance_metrics": "Survey reports Path-LLM aims to mitigate noise and align with causal LM training but does not provide specific numerical performance values in the survey itself.",
            "comparison_to_others": "Compared to WalkLM, Path-LLM addresses noise by carefully selecting shortest paths and aligns order with causal LM, which the survey suggests is more suitable for LLMs trained with causal language modeling.",
            "advantages": "Better at capturing relationships between dense groups and compatible with causal LM objectives; reduces noise compared to random-walk methods.",
            "disadvantages": "Requires shortest-path computations (costly on large graphs); path-selection heuristics may miss other informative structures; still constrained by sequence length.",
            "failure_cases": "Performance may degrade on extremely large graphs due to path selection cost, and in graphs where shortest paths are not the most semantically informative structures.",
            "uuid": "e8837.6",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "GraphText",
            "name_full": "GraphText (grammar tree → text)",
            "brief_description": "A pipeline that reformulates graph reasoning as a text-to-text problem by building grammar trees for graphs and traversing them to generate textual sequences which LLMs can process.",
            "citation_title": "GraphText: Graph reasoning in text space",
            "mention_or_use": "mention",
            "representation_name": "Grammar-tree traversal linearization (GraphText)",
            "representation_description": "Construct a grammar tree representation of the graph, traverse the tree to produce text sequences, and map graph reasoning tasks into text-to-text tasks so LLMs can operate in their native modality.",
            "graph_type": "General graphs; especially used for tasks that can be mapped to text-space reasoning",
            "conversion_method": "Build grammar trees reflecting graph topology/relations and perform a traversal (preorder/postorder) that emits textual tokens representing nodes/edges/relations, creating LLM-ready sequences.",
            "downstream_task": "Graph reasoning converted to text-to-text tasks, node classification, graph reasoning",
            "performance_metrics": "Survey describes GraphText's approach and its applicability but does not list specific numerical results within the survey body.",
            "comparison_to_others": "GraphText focuses on fully mapping graph reasoning into text space, whereas WalkLM/Path-LLM produce sequences for embedding training; GraphText is oriented to direct reasoning as text.",
            "advantages": "Enables use of text-to-text LLM strengths for graph reasoning and can leverage sequence-to-sequence objectives; conceptual alignment with LLM capabilities.",
            "disadvantages": "Designing grammar trees and traversals that preserve necessary structure is nontrivial; may lose some structural nuance depending on grammar design.",
            "failure_cases": "If grammar-tree construction omits critical structural context or traversal order breaks important dependencies, reasoning and downstream performance degrade.",
            "uuid": "e8837.7",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "TalkGraph",
            "name_full": "TalkGraph / Graph-encoded prompts (contextual narrative encoding)",
            "brief_description": "Encoding the same graph in multiple natural-language contexts or narratives so LLMs can interpret node roles and local structure from diverse textual viewpoints.",
            "citation_title": "Talk like a graph: Encoding graphs for large language models",
            "mention_or_use": "mention",
            "representation_name": "Graph-encoded prompts / contextual narrative encoding (TalkGraph)",
            "representation_description": "Place graph elements into different textual contexts (stories or roles) to provide varied semantic perspectives for LLMs, improving interpretability and node-specific queries.",
            "graph_type": "Text-attributed graphs, social graphs, TAGs where node semantics are important",
            "conversion_method": "Generate multiple textual contexts/stories that describe particular nodes and edges (e.g., 'David is friends with X and collaborates with Y'), and include these contexts as prompt input.",
            "downstream_task": "Node/link classification, graph question answering, representation learning when used as prompt-encoding technique",
            "performance_metrics": "Survey notes TalkGraph demonstrates diverse encodings improve LLM understanding for some tasks but does not provide precise quantitative metrics within the survey.",
            "comparison_to_others": "Emphasizes contextual diversity in text encoding versus single canonical serializations (adjacency lists or GML); compared with GraphTMI, TalkGraph focuses on text-only modality.",
            "advantages": "Can reveal different semantic interpretations of a node, making it easier for LLMs to infer roles or attributes; flexible and human-interpretable.",
            "disadvantages": "Requires careful prompt design; can increase token usage substantially; diversity may introduce contradictory contexts confusing the model.",
            "failure_cases": "Conflicting contexts may lead to inconsistent answers; scaling to large graphs is limited by token budget.",
            "uuid": "e8837.8",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "GraphTMI",
            "name_full": "GraphTMI (text+image+motif encodings)",
            "brief_description": "A multimodal graph encoding approach that uses text modality for local structure, motif modality to capture common substructures (stars, triangles), and image modality to represent global context for LLMs/LLM-Vision models.",
            "citation_title": "GraphTMI: Which modality should i use-text, motif, or image?: Understanding graphs with large language models",
            "mention_or_use": "mention",
            "representation_name": "Multimodal graph-encoded prompts (text + motif + image)",
            "representation_description": "Encode graph information across multiple modalities: textual descriptions for local neighborhoods, motif summaries to capture structural patterns, and images/visualizations to convey global structure; supply combined modalities to LLM or LLM-Vision.",
            "graph_type": "General graphs with motif structure (social networks, citation graphs, molecular graphs where motifs are meaningful)",
            "conversion_method": "Extract motif counts/structures and encode them as text; render graph visualizations for image input; create context sentences for local node neighborhoods and combine into a multi-part prompt.",
            "downstream_task": "Graph representation, node/list classification, structural reasoning, graph understanding tasks of varying difficulty",
            "performance_metrics": "Survey indicates GraphTMI uses motif/text/image modalities and reports task-level evaluations in the original work; the survey does not reproduce numeric scores here.",
            "comparison_to_others": "GraphTMI contrasts with text-only encodings (e.g., TalkGraph) by demonstrating that motif and visual encodings can capture structure that text alone misses.",
            "advantages": "Combines complementary modalities to capture both local patterns and global layout; can improve LLM understanding of structure that is hard to express in pure text.",
            "disadvantages": "Requires multimodal LLMs or additional tooling; generating motif summaries and images adds preprocessing cost; increased pipeline complexity.",
            "failure_cases": "If modality fusion is poor or LLM-Vision capabilities are limited, gains may not appear; modality mismatch and token/bandwidth limits constrain scaling.",
            "uuid": "e8837.9",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "API-CallPrompt",
            "name_full": "API-call Prompting (Graph-ToolFormer style)",
            "brief_description": "Prompting pattern where LLM outputs structured API call tokens (e.g., GL('benzenering')) that are mapped to graph-tool functions; the external tool executes graph algorithms and returns results to the LLM.",
            "citation_title": "Graph-ToolFormer: To empower LLMs with graph reasoning ability via prompt augmented by chatgpt",
            "mention_or_use": "mention",
            "representation_name": "API-call prompt serialization",
            "representation_description": "Use LLMs to generate special API-call strings embedded in natural language; these strings correspond to pre-defined graph-tool operations which are executed externally, enabling handling of large/complex computations beyond the LLM's reasoning capability.",
            "graph_type": "General graphs, large graphs, dynamic graphs where external tool computation is needed",
            "conversion_method": "LLM converts queries into API-call expressions (tool invocation tokens); a graph reasoning tool or library executes the calls (e.g., compute diameter, shortest path) and the tool output is fed back into the LLM for final response.",
            "downstream_task": "Graph structure understanding, large-scale queries, NP-hard subroutines offloaded to external tools, interactive graph agents",
            "performance_metrics": "Survey cites Graph-ToolFormer and related works showing improved correctness and reduced hallucination via API usage but does not give unified numeric metrics in the survey text.",
            "comparison_to_others": "API-call approach offloads heavy computations to exact graph tools vs pure in-LM encodings which try to reason solely from serialized graph text; the API approach improves correctness at expense of engineering complexity.",
            "advantages": "Enables exact computation, scales to large graphs, reduces LLM hallucinations on algorithmic results, supports verification via tool outputs.",
            "disadvantages": "Requires external tooling and tool-LLM integration; LLM must learn correct API-call formats; adds system complexity and latency.",
            "failure_cases": "If API-call translation is incorrect or tools are missing expected functionality, the pipeline fails; dependence on tool quality and interface correctness.",
            "uuid": "e8837.10",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "SelfPrompt",
            "name_full": "Self-prompting transformation (GPT4Graph style)",
            "brief_description": "A method where the LLM iteratively refines and generates its own prompt representations (e.g., converting GSQL/GML into clearer textual descriptions) to better understand graph structure and task instructions.",
            "citation_title": "GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking",
            "mention_or_use": "mention",
            "representation_name": "Self-prompting graph-to-text transformation",
            "representation_description": "LLM generates intermediate prompts or reformulations of the input graph description (e.g., expand GSQL into natural-language summaries, produce simplified adjacency descriptions) and uses these self-generated prompts to improve downstream reasoning or prediction.",
            "graph_type": "General graphs and knowledge graphs represented in structured formats (GSQL, GML)",
            "conversion_method": "Input structured graph representation into LLM; request the model to produce a clearer natural-language or stepwise representation; iteratively refine until a satisfactory textual description is obtained for the actual downstream prompt.",
            "downstream_task": "Graph structure understanding, GQL generation, improved prompting for node/link tasks",
            "performance_metrics": "Survey reports self-prompting is used to refine prompts and can help LLM comprehension; no specific numeric metrics reported in the survey.",
            "comparison_to_others": "Self-prompting can outperform static manual prompts by tailoring representation to the model, but may be less efficient than a well-designed single prompt or supervised fine-tuning.",
            "advantages": "Adaptive to the LLM's current capabilities, can clarify complex structured inputs, reduces need for manual prompt engineering.",
            "disadvantages": "May cascade errors if initial reformulations are poor; adds extra rounds of model calls (costly).",
            "failure_cases": "If the LLM's reformulation is inaccurate or ambiguous, subsequent reasoning/performance degrades; sensitive to prompt design for self-prompting loop.",
            "uuid": "e8837.11",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "KG-to-text (GLaM)",
            "name_full": "KG-to-text conversion for LLM fine-tuning (GLaM)",
            "brief_description": "Convert knowledge graph substructures into textual examples (paired with questions/answers) so that LLMs can be fine-tuned to better recall and reason with domain-specific KG knowledge.",
            "citation_title": "GLaM: Fine-tuning large language models for domain knowledge graph alignment via neighborhood partitioning and generative subgraph encoding",
            "mention_or_use": "mention",
            "representation_name": "Knowledge-graph-to-text QA conversion (GLaM)",
            "representation_description": "Transform KG triples/subgraphs into labeled text QA pairs or textual contexts which are then used to fine-tune or instruction-tune LLMs to incorporate domain-specific KG knowledge into their generative responses.",
            "graph_type": "Knowledge graphs, domain-specific KGs (biomedical, bibliographic), ontologies",
            "conversion_method": "Partition KG neighborhoods, generate textual encodings of subgraphs and synthetic QA pairs describing facts/relations, and then use these text pairs for supervised fine-tuning of the LLM.",
            "downstream_task": "Fact-aware language modeling, KGQA, domain-specific question answering, reduction of hallucination",
            "performance_metrics": "Survey states GLaM is used to improve domain KG alignment and LLM recall; exact numeric improvements are not reproduced in the survey text.",
            "comparison_to_others": "KG-to-text fine-tuning is complementary to RAG approaches; it helps LLM internalize KG facts rather than retrieving them at inference time.",
            "advantages": "Improves domain knowledge recall and reduces hallucination for KG facts; enables closed-book factual knowledge transfer into LLM.",
            "disadvantages": "Requires constructing large volumes of high-quality text-QA pairs and careful partitioning; may still miss rare facts.",
            "failure_cases": "Incomplete or noisy KG-to-text pairs can introduce erroneous facts; model may still hallucinate beyond covered KG content.",
            "uuid": "e8837.12",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        },
        {
            "name_short": "PiVe/EDC",
            "name_full": "PiVe (Iterative Verification) / EDC (Extract-Modify-Verify)",
            "brief_description": "Prompting frameworks for text-to-graph generation: LLMs extract triples from text, iteratively verify and modify outputs (PiVe), or run a multi-phase extract and automatic modification (EDC) to improve quality.",
            "citation_title": "PiVe: Prompting with iterative verification improving graph-based generative capability of LLMs",
            "mention_or_use": "mention",
            "representation_name": "Iterative verification-based text-to-graph conversion (PiVe / EDC)",
            "representation_description": "Use multi-step prompting where the LLM extracts entity-relation triples, then performs verification and iterative correction steps (PiVe) or a three-phase extract/modify/validate pipeline (EDC) to produce higher-quality semantic graphs from raw text.",
            "graph_type": "Semantic graphs, knowledge graphs extracted from natural language, text-attributed graphs",
            "conversion_method": "Phase 1: extract candidate triples; Phase 2: verify/validate and modify incorrect or ambiguous triples (possibly with rules or secondary prompts); Phase 3: output canonicalized graph representation.",
            "downstream_task": "Knowledge graph construction, information extraction, text-to-graph generation",
            "performance_metrics": "Survey describes these frameworks as improving graph generation quality but does not provide numeric comparisons in the survey body.",
            "comparison_to_others": "Improves over single-pass extraction by adding verification/repair loops; more reliable than one-shot triple extraction but more compute-intensive.",
            "advantages": "Higher precision and cleaner graph outputs due to verification; reduces hallucinated or spurious triples.",
            "disadvantages": "Additional LLM calls increase cost and latency; requires design of verification prompts or external validators.",
            "failure_cases": "If verifier is itself unreliable, iterative steps may not converge; complex semantics or implicit relations still hard to extract/verify.",
            "uuid": "e8837.13",
            "source_info": {
                "paper_title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
                "publication_date_yy_mm": "2025-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "WalkLM: A uniform language model fine-tuning framework for attributed graph embedding",
            "rating": 2,
            "sanitized_title": "walklm_a_uniform_language_model_finetuning_framework_for_attributed_graph_embedding"
        },
        {
            "paper_title": "Path-LLM: A shortest-pathbased llm learning for unified graph representation",
            "rating": 2,
            "sanitized_title": "pathllm_a_shortestpathbased_llm_learning_for_unified_graph_representation"
        },
        {
            "paper_title": "GraphText: Graph reasoning in text space",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "GraphTMI: Which modality should i use-text, motif, or image?: Understanding graphs with large language models",
            "rating": 2,
            "sanitized_title": "graphtmi_which_modality_should_i_usetext_motif_or_image_understanding_graphs_with_large_language_models"
        },
        {
            "paper_title": "NLGraph: Can language models reason with graphs and structures?",
            "rating": 2,
            "sanitized_title": "nlgraph_can_language_models_reason_with_graphs_and_structures"
        },
        {
            "paper_title": "Graph-ToolFormer: To empower LLMs with graph reasoning ability via prompt augmented by chatgpt",
            "rating": 2,
            "sanitized_title": "graphtoolformer_to_empower_llms_with_graph_reasoning_ability_via_prompt_augmented_by_chatgpt"
        },
        {
            "paper_title": "GPT4Graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "GLaM: Fine-tuning large language models for domain knowledge graph alignment via neighborhood partitioning and generative subgraph encoding",
            "rating": 2,
            "sanitized_title": "glam_finetuning_large_language_models_for_domain_knowledge_graph_alignment_via_neighborhood_partitioning_and_generative_subgraph_encoding"
        },
        {
            "paper_title": "PiVe: Prompting with iterative verification improving graph-based generative capability of LLMs",
            "rating": 2,
            "sanitized_title": "pive_prompting_with_iterative_verification_improving_graphbased_generative_capability_of_llms"
        },
        {
            "paper_title": "EDC: A three-phase framework for semantic graph generation from text",
            "rating": 1,
            "sanitized_title": "edc_a_threephase_framework_for_semantic_graph_generation_from_text"
        }
    ],
    "cost": 0.02456,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications
4 Jul 2025</p>
<p>Wenbo Shang 
Xin Huang 
A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications
4 Jul 20254A17CA258F02B153569D5A03863A601AarXiv:2404.14809v2[cs.CL]GraphLLMstructure understandinggraph learninggraph representationgraph reasoningsurvey
A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems.Recently, large language models (LLMs) have showcased a strong generalization ability to handle various natural language processing tasks to answer users' arbitrary questions and generate specific-domain content.Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation.However, LLMs are sequential models for textual data, but graphs are nonsequential topological data.It is challenging to adapt LLMs to tackle graph analytics tasks.In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing challenges and future directions.Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) in terms of three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications.LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graphs and LLMs, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning and graph representation.We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks.Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of the discussed LLM-GGA models.We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics.</p>
<p>I. INTRODUCTION</p>
<p>Large language models (LLMs) possess billions of parameters and have been trained on extensive corpora using training strategies like instruction tuning [1] [2] and Direct Preference Optimization (DPO) [3], enabling them to exhibit powerful reasoning and semantic representation capabilities, thereby advancing AI intelligence closer to human levels.Undoubtedly, LLMs currently serve as the foundation model for NLP tasks [4] [5] [6], showcasing strong generalization abilities to handle various NLP tasks such as question answering [7] [8] [9], machine translation [10], code generation [11] [12], etc. LLMs have demonstrated extensive common knowledge and robust semantic comprehension abilities, fundamentally transforming Wenbo Shang and Xin Huang are with the Department of Computer Science, Hong Kong Baptist University, Hong Kong, China (e-mail: cswbshang@comp.hkbu.edu.hk;xinhuang@comp.hkbu.edu.hk)existing text-processing workflows.While initially designed for textual data, LLMs are increasingly being utilized for tasks beyond language processing, particularly graph analytic tasks, aiming to leverage the robust and strong capabilities of LLMs across different tasks to achieve superior performance.</p>
<p>LLM-GGA LLM-GQP LLM-GIL Graph-LLM-based applications
Graphs LLMs
Graphs, as structured data, play a crucial role in various real-world application scenarios, including citation networks [13], social networks [14], molecular graphs [15], web links [16], and to name a few.Various graph analytics tasks have been studied to show their usefulness, e.g., node classification, link prediction, subgraph mining, influence maximization, and so on.Their versatility and ability to capture complex relationships have made graphs indispensable tools in academic research and industry platforms.Recently, one kind of graphbased learning model, graph neural network (GNN) [17] [18], has been widely studied and applied to solve challenging graph tasks.GNNs utilize recursive message passing [19] and aggregation mechanisms [20] among nodes to derive representations of nodes, edges, or entire graphs, capturing both graph structure and node features, which have been used for various downstream tasks.However, GNNs exhibit weak generalization capabilities and semantic representation abilities [21] [22] [23], requiring retraining for different specific graph tasks and showing limited transferability [24] [25].</p>
<p>Besides a simple graph model associated with no attributes or category/numerical attributes, nodes may be enriched with raw and detailed text attributes, capturing rich and comprehensive semantics, known as text-attributed graphs.For instance, in citation networks, nodes represent papers and edges denote</p>
<p>Surveys</p>
<p>Graph structure understanding KGs and LLMs Graph learning Graph-formed reasoning Graph representation Graph-LLM-based applications Year Li et al. [23] 2024 Jin et al. [26] 2024 Liu et al. [27] 2024 Ren et al. [24] 2024 Li et al. [28] 2024 Peng et al. [29] 2024 Han et al. [30] 2024 Zhang et al. [31] 2024 Lavrinovics et al. [32] 2024 LLM-GGA 2025 citations between them, with the nodes containing long text attributes such as titles, keywords, abstracts, and even the full articles [33] [34] [35] [36].In healthcare networks, nodes link patients, doctors, diseases, and treatments, incorporating the text attributes of medical records to enhance patient care [37] [38] [39].Similarly, in social networks, nodes represent users and edges represent interactions, with nodes containing text attributes like user bios and post content to analyze social influence [40] [41] [42].In the past day, analyzing these long text attributes has been challenging due to the limited ability of previous language models [33].Leveraging the advanced capabilities of recent LLMs, it is promising to explore and conduct text-attributed graph analytics.</p>
<p>Therefore, adapting LLMs' powerful reasoning, semantic representation, and generalization capabilities to address graph tasks, leading to the inspiration of a graph foundation model [27] [26] [24], is the core of current efforts in leveraging existing large language models for various graph-related tasks.However, LLMs are sequential models for language processing, while graphs are non-Euclidean topological data, which brings a key challenge: how can LLMs solve graph data tasks?More specifically, we study this core challenge by answering three detailed questions: (a) What specific graph tasks can LLMs answer?(b) How do LLMs tackle these tasks?(c) What is the effectiveness of LLM-based methods in solving these tasks compared with the existing graph-based approaches?</p>
<p>To address the above question, this survey conducts a comprehensive study of existing relevant work on graph analytics and LLMs, focusing on exploring the key issue of the LLM-based generative graph analytics (LLM-GGA) field.Drawing from a thorough investigation of the LLM-GGA domain, we offer a structured and methodical analysis that delineates the field into three principal components: LLM-based graph query processing (LLM-GQP), which necessitates the melding of graph analytics techniques and LLM prompts for query processing; LLM-based graph inference and learning (LLM-GIL), focusing on learning and reasoning over graphs; and lastly, graph-LLM-based applications that employ the graph-LLM framework to address non-graph tasks, such as recommendation systems.The framework is shown in Figure 1.</p>
<p>We categorize these three main components into a total of six directions to provide a guideline for researchers to conduct more in-depth studies.LLM-GQP includes graph understanding, and knowledge graphs and LLMs directions.</p>
<p>LLM-GIL covers graph learning, graph-formed reasoning, and graph representation directions.The sixth direction is graph-LLM-based applications.The following section details these six directions:</p>
<p>• Graph understanding tasks.This research direction studies the performance of LLMs over P problems and NPhard problems, exploring whether LLMs can comprehend graph structures to conduct graph algorithmic tasks and graph structural properties.Current methods have primarily explored LLMs' understanding of graph structures, such as shortest path, clustering coefficient computation [43] [44], and more complex problems like maximum flow and Hamilton path [45] [46] [47].Furthermore, many more NP-hard problems have been explored, such as maximum clique, graph coloring and the proof of P ̸ = N P [48] [49].Two main methods are introduced: prompting and supervised fine-tuning (SFT).The prompting methods explore the LLM's current structural understanding ability through query processing.Meanwhile, SFT methods enhance LLMs' structure understanding capability by tuning them on specific graph datasets.• Knowledge graphs and LLMs.This direction investigates the relationship between LLMs and Knowledge Graphs (KGs).With the emergence of LLMs, discussions have arisen regarding the potential replacement of KGs [50] [51] [52] [53].Consequently, this paper discusses the limitations of LLMs in processing factual knowledge, evaluates strategies for improving LLM efficacy via KGs, and investigates potential avenues for future advancements.• Graph learning tasks.This direction explores whether LLMs can combine graph structure and text attributes for learning, extracting high-dimensional features of nodes, edges, and graphs from embedding spaces, and understanding the semantic information of graphs, for example, tasks like node classification, link prediction and graph construction [33] [35] [54] [25].Most methods can be categorized into three methodologies: LLMsas-enhancers, LLMs-as-predictors and LLMs-as-graphgenerators. LLMs can leverage their powerful reasoning ability and vast knowledge base to enhance GNNs, predict results directly and generate graphs from natural language.We comprehensively analyze these six research directions of LLM-GGA to provide valuable definitions and highlight methodologies.For each direction, we summarize the key insights and limitations of relevant methods, discuss their common and different properties, and conduct an in-depth analysis of remaining challenges and promising future directions.To further explore the capabilities of LLMs reliably, this paper uses the prompting method to test the effectiveness of LLMs in tasks such as graph structure understanding, graph learning, and graph-formed reasoning.Details of the prompts and results obtained during testing are also provided.Additionally, we refine and compile commonly used and effective prompts for graph-related tasks, assisting researchers in conducting experiments.Furthermore, this paper also organizes and introduces the code for existing popular methods, benchmarks for LLM-GGA tasks, and evaluations measuring LLM performance in graph tasks to facilitate future research.</p>
<p>Comparison with other surveys.We study state-of-the-art surveys with regard to the topics of LLM+KG, Graph-RAG, and graph foundation models [23]  Lastly, we identify the fundamental challenges in the LLM-GGA field, which are the primary obstacles to advancing LLM in solving graph tasks, including the fundamental issue of how sequential LLM handles structural graph data, the efficiency issue of large-scale graph data, and the NP-hard problems of complex graph analytics.This clarification guides the research direction for future work on LLM-GGA.</p>
<p>Roadmaps.The organization of this paper is as follows.</p>
<p>We first present the preliminaries and summarize the graph description language, which converts graphs into sequences before inputting them into LLMs in Section II.Then, we introduce six tasks of LLM-based graph analytics one by one.We present the graph structure understanding direction in Section III, knowledge graphs and LLMs in Section IV, graph learning direction in Section V, graph-formed reasoning in Section VI, graph representation in Section VII and graph-LLM-based applications in Section VIII.In the above six directions, we clarify the tasks that LLMs can perform, discuss the methodologies, conduct a comparative analysis, and propose guidelines and principles in this direction.Following this, Section IX introduces the popular datasets and new datasets for solving the above tasks and also provides metrics for evaluating LLMs or tasks in different directions.In Section X, we identify and discuss the current and upcoming challenges that LLM-GGA faces and promising future directions.Finally, our conclusions are presented in Section XI.</p>
<p>II. PRILIMINARY</p>
<p>In this section, we first introduce graphs and GNNs as a paradigm of graph-based learning models.We then distinguish LLMs from previous PLMs (Pre-trained Language Models), particularly from their language modeling and training strategies.Lastly, we explore graph description languages, which can convert graphs into sequential data as input to LLMs.</p>
<p>A. Graphs</p>
<p>Graph data represents complex relationships through nodes and edges, where nodes represent entities and edges represent their interconnections.This structure excels at modeling intricate networks such as social, biological, and transportation systems.It enables analyses like community detection [111] and shortest paths search [112], offering critical insights into the dynamics of various systems.Formally, a general graph can be represented as G = (V, E), where V and E denote the set of nodes and edges.V = {v 1 , ..., v n } where the number of nodes is |V| = n.E = {e ij } where the number of edges is |E| and e ij is an edge from v i to v j .Particularly, a text-attributed graph (TAG) can be represented as G = (V, E, X v ), where V and E denote the set of nodes and edges.V is paired with raw text attributes X = {X v1 , ..., X vn }.TAGs widely exist in many real applications, such as academic citation networks with paper abstracts, social media networks with user bios/posts, and ecommerce product networks with descriptions and reviews.A knowledge graph (KG) can be denoted as G = {V, R, E} with entities V, relations R, and edges E.An edge is denoted as a triple and is of the form (s, q, o) where s is the subject, q the query relation, and o the object.</p>
<p>B. Graph Neural Networks</p>
<p>Graph Neural Networks (GNNs) [17] [18] [113] are a type of deep learning model that can handle graph-structured data.The goal of these GNNs is to learn representations for each node, which are computed based on the node's own features, the features of the edges connected to it, the representations of its neighbors, and the features of its neighboring nodes,
h l v = AGGR(h l−1 v , {h l u − 1 : u ∈ N v }; θ l )(1)
where h l v represents the representation of node v in the l-th layer.AGGR denotes the aggregation function that aggregates the representations of neighboring nodes from the previous layer [114] [115].For node-level tasks, e.g., node classification, the learned representations can be used directly to accomplish specific objectives.However, for graph-level tasks, e.g., graph classification, a global representation can be obtained by pooling or integrating representations of all nodes.</p>
<p>C. Large Language Models</p>
<p>According to the pioneering surveys [116] [117] [4] on LLMs, a significant distinction between LLMs and PLMs is model size.LLMs with billion-level parameters that are pretrained on massive amounts of data, such as Llama [5] and ChatGPT [118].Conversely, PLMs are pre-trained language models with million-level parameters that can be more easily fine-tuned on task-specific data.Additionally, the language modeling strategies and training methods are also different.Causal Language Modeling (CLM) is the mainstream learning strategy of LLMs [33], [119], where LLMs are trained to predict the next token x i in a sequence x = {x 1 , x 2 , ..., x q } based on prefix tokens x &lt;i = {x 1 , x 2 , ..., x i−1 }.CLM is commonly used to train LLMs like GPT [118] and Llama [5].LLM is typically trained to optimize a conditional probability</p>
<p>Clustering coefficient computing</p>
<p>Given <graph>, what is the clustering coefficient of the node 1?
1 4 3 2 0</p>
<p>Topological Sorting</p>
<p>In a directed graph with 5 nodes numbered from 0 to 4: node 0 should be visited before node 1, ... Q: Can all the nodes be visited?Give the solution.</p>
<p>1 2 1 0 0</p>
<p>Bipartite Graph Matching</p>
<p>There are 2 job applicants numbered from 0 to 1, and 3 jobs numbered from 0 to 2. Each applicant is interested in some of the jobs.Each job can only accept one applicant and a job applicant can be appointed for only one job.Applicant 0 is interested in job 1, ... Q: Find an assignment of jobs to applicants in such that the maximum number of applicants find the job they are interested in.</p>
<p>Maximum Flow</p>
<p>In a directed graph with 5 nodes numbered from 0 to 4, and the edges are: an edge from node 0 to node 1 with capacity 10... Q: What is the maximum flow from node 0 to node 3? distribution p(x i |x &lt;i ) [33], which assigns a probability to each possible x ′ i ∈ D given prefix tokens x &lt;i , where D is the LLM vocabulary.Thus, the probability of the output sequence x is:
p(x) = q i=1 p(x i |x &lt;i ).(2)
Notably, the probability of generating token x i depends only on the prefix tokens x <i , showing that LLMs are blind to the following tokens x >i after x i .Instruction tuning is the common training method for LLMs, which aims to fine-tune the pre-trained LLMs over datasets that consist of groups of natural language instructions paired with responses [6] [120] [121].The objective of instruction tuning is usually the cross-entropy loss function L. x and y denote LLM's input instructions and target responses, respectively.θ refers to the LLM parameters.The loss function is as follows:
P θ (y j |x, y &lt;j ) = LLM θ (x, y &lt;j ),(3)L θ = − |y| j=1 log P θ (y j |x, y &lt;j ).(4)</p>
<p>D. Graph Description Language</p>
<p>Graphs are represented in structured data in arbitrary shapes, while LLMs typically process sequential data like texts.To bridge this gap, the graph description language (GDL) transforms the graph into sequential data, which can be input into LLMs.Specifically, GDL aims to convert graphs into sequential data while retaining the structure and unique attributes of the graph.This conversion allows graphs to be fed into an LLM for processing.There are several GDLs:</p>
<p>• Text description.Graph structure can be described using words such as 'Node 1 is connected to Node 2' and 'There are three nodes connected to Node 1'.• Adjacency list.An adjacency list represents each vertex in the graph with the collection of its neighboring vertices or edges.Node A is connected with node B and node C, which can be denoted as N (A) = {B, C}. • SQL.Several specialized SQL languages are designed for working with graph data, which are also capable of serving as GDLs.Some notable examples include Cypher [122], a query language developed by Neo4j, and Gremlin [123], SPARQL [124], and GSQL [125].There are other GDLs, such as edge lists, GML, GraphML, multi-modality encoding, and encoding as a story [126] The main challenges are: 1) comprehending graph structures locally and globally for LLMs is challenging, as LLMs are sequential models; 2) tasks with problematic assumptions and constraints are complex for LLMs to solve; 3) LLMs require algorithmic skills or strong reasoning abilities to obtain optimal or approximate answers This section provides an overview of various graph understanding tasks, along with their respective definitions.Furthermore, we also explore the prompts for LLMs to solve graph understanding tasks (shown in Table II), as well as their generalization capabilities on graph data.</p>
<p>A. Task Introduction 1) Polynomial-time problems:</p>
<p>A problem is classified as a polynomial-time problem if an algorithm can solve an instance of the problem in time bounded by a polynomial function of the input size.If the input size is denoted as n, then an algorithm runs in polynomial time if it completes in O(n k ) time for some constant k.For example:</p>
<p>Clustering coefficient computing.The clustering coefficient is a measure of how connected a vertex's neighbors are to one another, as shown in Figure 3 (c).LLMs can calculate the clustering coefficient as a measure of the degree to which nodes in a graph tend to cluster together.</p>
<p>Path search.Given a weighted directed acyclic graph G = {V, E} with each edge e ∈ E has a non-negative weight w(e), the shortest paths task involve finding a path p = (e 1 , e 2 , . . ., e n ) from a source node to a target node in G such that the sum of the weights of edges w(p) = n i=1 w(e i ) is minimized.LLMs can calculate the shortest path length and identify the qualified paths, as shown in Figure 3 (d).</p>
<p>Topological sorting.Topological sorting of a directed graph G = {V, E} refers to a linear ordering of its nodes, where each node comes before all the nodes it points to, for example, there exists a directed edge e ij from v i to v j , v i comes before v j in the ordering.The resulting array of node ordering is called topological ordering.LLM is required to generate a valid topological sorting for the given directed graph, and there may be multiple valid solutions, as shown in Figure 3 (g).</p>
<p>Figure 3 illustrates various tasks, including computing graph properties (size, degree, density, eccentricity, radius, diameter, and periphery), retrieval tasks (searching for connected nodes, attributes, and cycles), and more complex tasks (topological sorting, maximum flow, and bipartite graph matching).</p>
<p>2) NP-hard problems: NP-hard problems are defined as those computational challenges for which no polynomial-time solution exists, contingent upon the assumption that P ̸ = N P .Various NP-hard tasks like graph edit distance, graph coloring, maximum cliques, and others are shown in Figure 3 [49] [64].</p>
<p>Graph edit distance.For two graphs G 1 and G 2 , determine the minimum edit distance via node mappings, as shown in Figure 3 (j).The task involves aligning nodes in G 1 and G 2 , and minimizing the edit operations required.The operation includes adding, deleting, or substituting a single edge or node.</p>
<p>Graph coloring.Given a graph G = {V, E}, assign colors to V so that no adjacent vertices share the same color, as shown in Figure 3 (n).The objective is to minimize the number of colors used, known as the graph's chromatic number.</p>
<p>Maximum clique.Given a graph G = {V, E}, identify the largest clique, as shown in Figure 3 (k).The task entails (1) finding a feasible clique C ⊆ V , and (2) ensuring that C is the largest among all possible cliques.</p>
<p>B. Graph Structure Understanding Methods</p>
<p>Existing efforts have introduced various benchmarks to evaluate LLM graph reasoning potential, aiming to explore their capacity to address graph structure understanding problems.Prompting methods have emerged as the primary approach to assess LLM understanding of graph structures, with some studies also focusing on fine-tuning LLMs to enhance their graph understanding abilities.Thus, two main methods are introduced: prompting and supervised fine-tuning LLMs.</p>
<p>1) Prompting method: The prompting method [127] can be categorized into three main types: manual prompt, selfprompting, and API call prompt, as shown in Figure 4. Most studies utilize manual prompts, where carefully crafted prompts guide LLMs to comprehend graph structures better and understand the objectives of graph tasks, thereby leading to improved performance on graph-related tasks.</p>
<p>Manual prompts.NLGraph [47] introduces a benchmark aiming to assess the understanding capabilities of LLMs in processing textual descriptions of graphs, covering various tasks like shortest path and maximum flow.Additionally, two prompt methods are proposed as shown below.</p>
<p>Prompt III-1: Build-a-Graph Prompting Given <graph description>.Let's construct a graph with the nodes and edges first.Q: What is the degree of node 4?</p>
<p>Prompt III-2: Algorithmic Prompting</p>
<p>We can use a Depth-First Search (DFS) algorithm to find the shortest path between two given nodes in an undirected graph.<Description of DFS algorithm> Given <graph description>.Q: Give the shortest path from node 0 to node 4.</p>
<p>NLGraph [47] shows that LLMs indeed possess preliminary graph structure understanding abilities.However, LLMs perform poorly on graph structures such as chains and cliques.To explore whether LLMs can truly comprehend graph structures and reason on graphs, Huang et al. [46] and Liu et al. [44] evaluate LLMs using manual prompts.Huang et al. [46] examine two potential factors affecting performance: data leakage and homogeneity.Liu et al. [44] introduce new evaluation metrics, comprehension, correctness, fidelity, and rectification, to assess LLM proficiency in understanding graph structures.Beyond static graphs, LLMs' ability to understand dynamic graph structures is also assessed.Dynamic graphs change over time, capturing temporal network evolution patterns.LLM4DyG [45] introduces a benchmark, which uses prompting methods to evaluate LLM spatio-temporal understanding capabilities on dynamic graphs, as shown below:</p>
<p>Prompt III-3: DST2 DyG Instruction: In an undirected dynamic graph, (u, v, t) means that node u and node v are linked with an undirected edge at time t.Task Instruction: Your task is to answer when two nodes are first connected in the dynamic graph.Two nodes are connected if there exists a path between them.</p>
<p>Answer Instruction: Give the answer as an integer number at the last of your response after 'Answer:' Exemplar: Here is an example: Question: Given an undirected dynamic graph with the edges [(0, 1, 0), (1, 2, 1), (0, 2, 2)].When are node 0 and node 2 first connected?Answer:1 Question:Given an undirected dynamic graph with the edges [(0, 9, 0), (1, 9, 0), ...] When are node 2 and node 1 first connected?</p>
<p>Results show that LLMs have preliminary spatio-temporal understanding capabilities on dynamic graphs.Dynamic graph tasks become increasingly challenging with larger graph sizes and densities, while being insensitive to periods and data generation mechanisms.Investigations into LLM's performance extend P problems to NP-hard problems.Dong et al. [48] delve into the potential of LLMs to address the classical P vs. NP question.Furthermore, NPHardEval [49] and GraphArena [64] propose benchmarks for P and NP-hard problems, such as graph edit distance, graph coloring and others.For instance, the prompt for the graph coloring problem is shown below:</p>
<p>Prompt III-4: Graph coloring.(NP-hard problem) Task Instruction: ⟨Graph coloring description⟩.There are 6 vertices 1 to 6 in a graph.You may use 4 colors with alphabets from A, B, C,...to color the graph.Please label every vertex, even if it is disconnected from the rest of the graph.Please provide each vertex's color.Answer Instruction: Your output should contain two parts enclosed by <root></root>... Given Graph: Vertex 1 is connected to vertex 6,...The work mentioned above provides an initial exploration of the cognitive capabilities of LLMs regarding various graph structures, including their performance on P and NP-hard problems.Some other studies conduct in-depth investigations into the generalization capabilities of LLMs, examining whether they rely on reasoning or memory to address questions related to graph structures.NLGIFT [65] proposes a benchmark to assess the generalization capabilities of LLMs, explicitly investigating whether LLMs can surpass the semantic, numeric, structural, and reasoning patterns present in the graph data.To explore LLM sensitivity to different graph-related prompts, GraphInsight [66] posits that LLMs exhibit a "position bias" in their understanding of graphs when prompted.</p>
<p>Self-prompting.The self-prompting method refers to the process where an LLM designs prompts for itself based on the original prompt.Specifically, the LLM continuously updates the initial prompt to make it easier for itself to understand contexts in prompts (e.g., GSQL, Cypher) and the goal of the task.GPT4Graph [43] utilizes self-prompting by continuously guiding LLMs to refine the prompt with descriptions of graphs, converting the original structured GDLs (i.e., GSQL) into LLMgenerated text descriptions (as shown in Section II-D).Prompt III-5: Self-prompting Instructor: You are a brilliant graph master that can handle anything related to graphs like retrieval, detection and classification.Graph description language (Original prompt): GSQL, GML, GraphML [126] that describe graph structures.Self-updated prompts: Node P357 has 4 neighbors, where each of which are about anomaly detection... Query: What is the clustering coefficient of node P357?API call prompts.The API call prompts aim to address LLMs' limited ability in topological structuring and temporal information processing by calling APIs [68] [69], enhancing LLM-based agents.Specifically, the API call prompt is generated by LLMs to convert complex and difficult tasks into a rule-based logical expression like GL("benzenering") in Prompt III-6.The rule-based logical expression corresponds to prepared API calls.Inspired by recent models such as Toolformer [128], Graph-ToolFormer [69] is proposed to equip LLMs such as GPT-J [129] and LLaMA [130] [5] with graph structure understanding and reasoning capabilities by calling graph reasoning APIs of external tools.Gorilla [68] employs a fine-tuned LLaMA enhanced with Retriever Aware Training (RAT), surpassing GPT-4 in writing API calls and significantly reducing hallucinations.One API call prompt is shown below.</p>
<p>Prompt III-6: API call prompts Input:(Regular prompt)</p>
<p>The structure of the benzene ring molecular graph of benzene ring contains a hexagon.</p>
<p>Output:(API call prompt)</p>
<p>The structure of the [GL("benzenering")] molecular graph of benzene ring contains a hexagon.</p>
<p>2) Supervised fine-tuning (SFT) method: Beyond leveraging prompts for graph-structured tasks with LLMs, certain studies also explore LLM-supervised fine-tuning, illustrated in Figure 5. GraphLLM [70] introduces a hybrid model that inherits the capabilities of both graph learning models and LLMs, enabling LLMs to reason on graph data proficiently while utilizing the superior power of graph learning models.</p>
<p>C. Summary of Methods, Challenges and Future Directions</p>
<p>In summary, the prompting method can be divided into three categories: manual prompts, self-prompting, and API call prompts.Most current methods primarily rely on manual prompts, incorporating techniques like Chain of Thought (CoT) [131], self-consistency [132], and in-context learning [133].Self-prompting methods are also widely used to obtain better prompt representations.However, relying solely on manual prompts and self-prompting provides only marginal enhancements to model performance, as these approaches primarily leverage the existing capabilities of LLMs.Additionally, due to the limited input window of LLM, the graph size that can be input to LLM at once is also restricted, while graph sizes in the real world are typically large.</p>
<p>To overcome these shortcomings, we propose two feasible future directions to better leverage existing LLMs for handling structure understanding tasks.The first direction is breaking down complex tasks into several sub-problems.While LLMs can tackle simple graph tasks, they struggle with more challenging ones.Breaking down complex graph understanding tasks into simpler components enables LLMs to engage in multistep reasoning processes, leading to the resolution of complex issues, such as GoT [69], which can help address more intricate graph tasks like generating GNN frameworks, k-truss tasks, kd-core tasks, etc.The second direction is API call prompts.Inspired by ToolFormer [128], LLMs can be trained as agents to utilize tools for graph tasks that are hard to solve.However, current API call prompt methods [69] utilize LLMs solely to convert user queries into API command strings for processing by subsequent programs, like Prompt III-6.</p>
<p>Compared to prompting methods, fine-tuning LLMs with graph data is a more practical approach for enhancing their comprehension of graph structures.Current methods mainly utilize SFT to train LLMs, enabling them to grasp the entire graph structure through instruction tuning techniques.Various other training techniques in the natural language processing field, such as RLHF, RLAIF, PPO, and DPO, can also be employed to understand graph structures, aligning LLMs with human preferences.RLHF [6] provides detailed human feedback through pairwise comparison labeling.Furthermore, to address the instability issue in PPO [134] training, RAFT [135] can also be attempted, which requires online interaction.For offline algorithms, methods like DPO [3] and PRO [136] can also be utilized for training LLMs.Discussions.We further discuss the cognitive and generalization capabilities of LLMs on graphs.Regarding cognitive abilities, LLMs can handle simple P and NP-hard problems, commonly with several nodes.When solving P or NP problems, LLMs can often provide direct answers.For more complex problems, they use common algorithms and step-by-step reasoning, such as BFS or DFS for shortest path problems.Particularly, exhaustive enumeration is a common strategy chosen by LLMs.Moreover, their performance is not a result of data leakage in the pre-training corpus.However, they lag behind traditional graph algorithms in both efficiency and accuracy.Regarding generalization capabilities, current evidence indicates that although LLMs exhibit strong robustness and generalization performance in handling variations in semantic and numerical attributes of graphs and various graph analytical tasks, they still face challenges in large-scale and complex networks.</p>
<p>IV. KNOWLEDGE GRAPHS AND LLMS</p>
<p>LLMs have shown remarkable reasoning capabilities in challenging tasks, sparking debates on the potential replacement of Knowledge Graphs (KGs) in triplet form (subject, predicate, object) by LLMs.Recent LLMs are seen as viable alternatives to structured knowledge repositories such as KGs, indicating a shift towards utilizing LLMs for processing real-world factual knowledge [92] [94].However, LLMs, against KGs, face several significant challenges: 1) Hallucination is a common issue for LLMs due to a lack of domain-specific knowledge and knowledge obsolescence, leading to incorrect reasoning and reduced credibility in critical scenarios like medical diagnosis and legal judgments [92] [137] [53].Although some LLMs can explain predictions through causal chains, they struggle to address hallucination effectively.Integrating external KGs can help mitigate these problems [51].2) Insufficient domain knowledge hampers LLM performance in specific areas, including private datasets, necessitating the integration of domain-specific knowledge graphs to enhance their ability to answer domain-specific questions [50].3) LLMs struggle with recalling facts when generating knowledge-based content, despite excelling in learning language patterns and conversing with humans [94].4) LLMs have limitations in accurately capturing and retrieving knowledge, hindering their ability to access factual information effectively [52].Thus, KGs like Wikipedia and DBpedia are structured repositories of rich factual knowledge, providing a more explicit and reliable source of information, as shown in Figure 6.</p>
<p>A. KG Solutions to tackle LLM Limitations</p>
<p>To address the limitations of LLMs, such as hallucination, insufficient domain knowledge, etc., integrating LLMs with external KGs is a potential way to allow LLMs to reason on high-quality knowledge, thereby enhancing their capabilities.</p>
<p>Retrieval augmented generation (RAG) based on KG is currently the mainstream technique of KG solutions to tackle LLM limitations.It aims to dynamically query large-scale textual databases to retrieve relevant factual knowledge and integrate it into the responses generated by LLMs.Existing RAG works focus on addressing the above four limitations of LLMs, which are hallucination, insufficient domain knowledge, struggling with recalling facts, and roughly capturing knowledge.</p>
<p>Hallucination.To address the hallucination issues in LLMs, the Head to Tail benchmark [92] is introduced to assess LLM reliability in answering factual questions and to evaluate the probability of hallucination in generating KG triples.G-retriever [93] performs a Prize Collecting Steiner Tree optimization problem to search for relevant knowledge from KGs with closest and minimal structures.ToG [51] partially addresses hallucination by involving the LLM agent in iteratively searching KGs, identifying promising reasoning paths, and providing likely reasoning outcomes.RoG [53] synergizes LLMs with KGs for faithful and interpretable reasoning.</p>
<p>Insufficient domain knowledge.The second limitation is that LLMs need domain-specific knowledge.To tackle this, GLaM [50] is developed to convert knowledge graphs into text paired with labeled questions and answers, allowing LLMs to acquire and respond to domain-specific knowledge.</p>
<p>Struggling with recalling knowledge.Regarding the limitation related to LLMs forgetting facts, KGPLMs [94] is introduced to enhance the model's ability to recall facts compared to standalone LLM, where LLMs improve knowledge extraction accuracy, and KGs guide LLM training to enhance memory and knowledge application capabilities.</p>
<p>Roughly capturing knowledge.Finally, the fourth limitation pertains to LLM challenges in accurately retrieving and returning knowledge from KGs. KGs can enhance LLM performance by incorporating them during pre-training and inference stages or to deepen LLM's understanding of acquired knowledge.GNP [52] proposes a plug-and-play method to facilitate pre-trained LLMs in effectively learning beneficial knowledge from KGs.A graph retrieval algorithm based on text indexing or graph indexing to obtain relevant knowledge is also an effective method.GraphRAG [95] divides the KG into several communities through a community detection algorithm and then employs LLMs to generate summaries for closely related communities.ToG-2 [96] integrates graph retrieval and context retrieval to obtain in-depth clues relevant to the question, enabling LLMs to generate reliable answers.</p>
<p>Summary and discussion.Existing research on RAG faces two main limitations: multi-modality KG handling and efficient retrieval framework on large-scale KGs.Current efforts primarily focus on textual information in KGs, exhibiting limited transferability to other modalities such as images and audio.Given massive multi-modality information contained in KGs, effectively retrieving closely relevant knowledge is challenging.Moreover, in large-scale KGs, text indexing is particularly timeconsuming, taking tens of times longer than the subsequent retrieval process.As existing methods concentrate on smallscale KGs with thousands of entities [29], efficient retrieval</p>
<p>GQL Generation</p>
<p>Given <graph>, the director who directs Inception also direct what?Use Cypher to answer.</p>
<p>Node Classification</p>
<p>Given <graph>, which arxiv CS subcategory does paper "paper title" with abstract "paper abstract" belongs to? use the abbreviation to answer.</p>
<p>Link Prediction</p>
<p>Given <graph>, are these two central nodes (node 1 and node 4) connected?</p>
<p>Give me an answer of "yes" or "no".</p>
<p>?
US</p>
<p>Graph Construction</p>
<p>Transform the text into a semantic graph.</p>
<p>(c)</p>
<p>Francisco Uranga was born in 1905 and represented Argentina at the 1928 Summer Olympics.He competed in the men's 50 metre freestyle.algorithms also become difficult as the scale of KGs grows.</p>
<p>B. LLM Solutions for KG Tasks</p>
<p>LLMs can enhance KGs to tackle a broader array of challenges.By leveraging LLMs, KGs can be fortified to perform various KG-related tasks such as embedding, completion, construction, text generation from graphs, and question answering [137].An illustrative example is how LLMs can support KG tasks such as knowledge graph alignment.In entity alignment tasks between different knowledge graphs, the objective is to identify pairs of entities representing the same entity.To address this, AutoAlign [97] automatically identifies similarities between predicates across different KGs with LLM assistance.To improve the LLM performance in question answering, Shah et al. [101] enhance LLMs to generate Cyber and SPARQL queries for multi-hop knowledge graph question answering.To achieve more accurate knowledge inference while minimizing the costs associated with frequent interactions between LLMs and KGs, EtD [98] employs GNNs to identify promising candidate answers and fine-grained knowledge relevant to the question, then guides the frozen LLM to ascertain the final answer.For KG completion (KGC) tasks, Sehwag et al. [102] integrate ontological knowledge and graph structure through in-context learning to improve KGC performance.To tackle the problem of infrequent and less popular relationships substantially hindering KGC performance, MuKDC [99] utilizes multi-level knowledge distillation and generates supplementary knowledge to mitigate data scarcity in few-shot environments.Furthermore, the combination of KGs and LLMs can address tasks like multi-document question answering [100] and fact-checking in public deliberation [103].Overall, there exist several comprehensive surveys [137] [138] [94] on the study of LLMs and KGs for further investigations.</p>
<p>V. GRAPH LEARNING TASKS</p>
<p>This section introduces various graph learning tasks that LLMs can address, as shown in Figure 7, such as node classification, graph construction, etc.When adapting LLMs to tackle these tasks, the main challenges are: 1) nodes are numerous with complex text attributes, which are challenging for LLMs to analyze; 2) graph foundation models require handling diverse, scalable and complicated networks and structures like social network, biomedical network, citation network, etc; 3) graph foundation models require handling various multi-mode downstream tasks, like classification, reasoning, and description.To address these challenges, we discuss technique details, key insights, and limitations of relevant and representative methods, which primarily leverage LLMs' extensive knowledge base, semantic comprehension, and generalization capabilities.Specifically, Table III shows a detailed comparison of six influential works, which are TAPE [33], OFA [25], LLaGA [79], GraphGPT [139], TG-LLM [54] and PiVe [82].It directly highlights their common insights and differences from perspectives of the role of LLMs, the training of LLMs, the usage of GNNs, the downstream tasks, and the training strategy.</p>
<p>A. Task Introduction</p>
<p>Graph learning tasks include node classification, link prediction, graph classification, graph construction, knowledge graph question answering (KGQA), and graph query language (GQL) generation, as shown in Figure 7. Below, we detail several prominent graph learning tasks.</p>
<p>Node classification requires LLMs to learn from the neighbors or attributes of a node.This process involves classifying unseen nodes within a given graph.For example, in an academic network, this task might involve categorizing papers into different research directions, as depicted in Figure 7 (a).</p>
<p>Link prediction requires LLMs to determine whether two central nodes are connected, given their text attributes and neighbor nodes, as shown in Figure 7 (b).</p>
<p>Graph construction.Given a natural language paragraph, LLMs are required to extract the entities and relationships  [33] Enhancers Node classification Supervised OFA [25] Enhancers Node classification, graph classification, link prediction Supervised LLaGA [79] Predictors Node classification, node description, link prediction Supervised GraphGPT [139] Predictors Node classification, link prediction Self-supervised TG-LLM [54] Generators Temporal reasoning, temporal question-answering Supervised PiVe [82] Generators Text-to-graph</p>
<p>Text Embeddings</p>
<p>Text Attributes</p>
<p>LLMs</p>
<p>Explanation for reasoning process 1.</p>
<p>2.</p>
<p>Text Attributes</p>
<p>B. Graph Learning Methods</p>
<p>Concerning graph learning methods, specific techniques are designed to address individual tasks, while others can tackle various graph learning tasks.These approaches can be further categorized based on the role of LLMs in facilitating graph learning into three distinct classifications: LLMs as enhancers, LLMs as predictors, and LLMs as graph generators.When LLMs function as enhancers, they utilize their sophisticated semantic comprehension, robust reasoning abilities, and extensive knowledge base to augment the textual attributes of nodes, thus enhancing the performance of other models, such as GNNs.Conversely, when LLMs act as predictors, they are prompted or fine-tuned to solve graph learning tasks.Furthermore, when functioning as graph generators, LLMs extract semantic graphs from natural language, thereby facilitating information extraction and knowledge graph construction.In summary, adapting LLMs in graph learning tasks presents a promising avenue for advancing the field.By leveraging the strengths of LLMs as enhancers and predictors, researchers can explore new directions for enhanced performance and more profound insights into LLM-GIL tasks.</p>
<p>1) LLMs-as-enhancers: The predominant approach in the LLMs-as-enhancers is the LLM-GNN pipeline.Within this framework, LLMs are tasked with processing text attributes, while GNNs are responsible for handling graph structures, capitalizing on the complementary strengths of both components to address graph learning tasks effectively.LLMs bolster GNNs through three distinct mechanisms: encoding text attributes into embeddings (as shown in Figure 8 (a)), generating graph pseudo labels (as shown in Figure 8 (b)), and providing external knowledge or explanations (as shown in Figure 8 (c)).Subsequently, we will provide a comprehensive elaboration on these three enhancement strategies.</p>
<p>Encoding text attributes into embeddings.Many existing pipelines utilize LLMs to encode text attributes into node embeddings as node features and then feed these embeddings into a GNN for learning, as shown in Figure 8 (a).TAPE [33] fine-tunes LM and GNN for node classification, integrating original node text attributes and LLM explanations to derive node embeddings.GraphAdapter [71] designs two training stages for GNN and fusion module to integrate LLM-generated embeddings for node classification.GAugLLM [36] designs expert augmented texts as new node text attributes, which are fed into LLM for deriving node embeddings and then trains GNN based on it.Moreover, SIMTEG [72] and GLEM [35] train an LM for generating node embeddings and then train a GNN based on these node embeddings for node classification.More general models can perform various graph learning tasks.OFA [25] utilizes LLMs to unify diverse graphs by describing nodes and edges in natural language and then trains a GNN based on the obtained texts for various downstream tasks.</p>
<p>Generating graph pseudo labels.The extensive knowledge base and powerful generalization ability of LLMs enable their utilization in generating pseudo labels for graphs for data augmentation, as shown in Figure 8 (b).This process significantly enhances the GNNs training process and improves performance in graph learning tasks.GLEM [35] proposes training the GNN and LM separately within a variational EM framework, where the LM predicts gold and pseudo-labels from   the GNN label results.Moreover, LLM-GNN [73] proposes to select a candidate node set for annotation by LLMs to facilitate GNN training, addressing challenges of high annotation costs and the need for large amounts of high-quality labeled data.</p>
<p>Providing external knowledge/explanations.Owing to the extensive and diverse corpus that LLMs have been trained on, coupled with their robust reasoning capabilities, LLMs can provide external knowledge or explanations relevant to certain nodes to enhance the attributes of nodes, as shown in Figure 8 (c).The enhanced node attributes assist the downstream GNNs in better extracting and capturing node features.Graph-LLM [74] utilizes LLMs like ChatGPT to generate augmented text attributes and pseudo labels for trainable LLM and GNN training.Similarly, TAPE [33] utilizes LLMs to predict node labels and explain reasoning processes, which are then used as text attributes to train the LM and GNN.</p>
<p>2) LLMs-as-predictors.: When LLMs are predictors, they are typically employed as standalone predictors.The critical aspect of integrating LLMs as predictors lies in crafting welldesigned prompts and fine-tuning methods.Well-designed prompts encompass text attributes and graph structures, enabling LLMs to comprehend graphs and improve prediction accuracy.In contrast, fine-tuning methods, such as instruction tuning, enhance the LLM's grasp of the graph structure.Thus, LLMs-as-predictors methods can be categorized into prompting LLMs and fine-tuning LLMs, as illustrated in Figure 9.</p>
<p>Prompting LLMs.The prompting method can be divided into two categories.One type is the manual prompts, which are manually written prompts.For instance, Beyond Text [75] and KG-LLM [76] utilize manual prompt templates with slots.By filling these slots with different examples, various prompts can be constructed.Compared to manual prompts, LPNL [77] generates prompts through a two-stage sampling process.</p>
<p>Fine-tuning LLMs.IntructGLM [78] and GraphGPT [139] fine-tune LLM by instruction tuning for node classification.InstructGLM [78] uses prompts to input subgraph structures into LLMs, which are then tasked with answering questions and predicting node labels.Conversely, GraphGPT [139] inputs subgraph structures into LLMs through embedding, followed by two rounds of instruction tuning for node classification.</p>
<p>Efficient tunable modules.Training LLMs is timeconsuming and costly, and inappropriate training can undermine their original reasoning and generalization capabilities.</p>
<p>…… …… ……</p>
<p>Frozen LLM</p>
<p>["Francisco Uranga", "occupation", "swimmer"], ["Francisco Uranga", "date of birth", "01 January 1905"], ["Francisco Uranga", "country of citizenship", "Argentina"]…… Therefore, existing research attempts to design efficient tunable modules in conjunction with LLMs, focusing on training a tunable module with fewer parameters instead of the LLM itself, thereby reducing both the time and cost.LLaGA [79] designs a tunable graph projector to align different graphs, which can handle various graph tasks across multiple datasets, eliminating the need for task-specific adjustments.GraphTranslator [80] designs a tunable module, named Translator, to bridge the frozen graph model and LLMs.ENGINE [81] proposes tunable G-Ladders combined with frozen LLM layers via a side structure, eliminating the backpropagation of LLMs.</p>
<p>Verify/Modify</p>
<p>Semantic graph</p>
<p>3) LLMs-as-graph-generators: Many studies explore the capability of LLMs to generate semantic graphs from natural language, thereby benefiting various applications such as decisionmaking and recommendation, as shown in Figure 10.PiVe [82] proposes an iterative verification framework to improve the graph generative capability of LLMs.TG-LLM [54] fine-tunes LLMs to construct temporal graphs from text contexts and reason on temporal graphs rather than reason on text contexts.To enhance the quality of semantic graph generation from text in practical applications using LLMs, EDC [83] proposes a three-phase framework.This framework first extracts triples from the text and subsequently automatically modifies any inappropriate keywords within those triples.</p>
<p>C. Summary of Methods, Challenges and Future Directions</p>
<p>In summary, for addressing graph learning tasks, existing methods [33] [139] categorize based on the role of LLM into three types: LLMs-as-enhancers (LLM-GNN pipelines), LLMs-as-predictors (LLM pipelines), and LLMs-as-graphgenerators (text-to-graph pipelines).When LLMs function as enhancers, the most popular pipeline is the LLM-GNN pipeline.There are three categories of LLM-GNN pipelines, depending on how LLM enhances GNN: encoding text attributes into embeddings, generating graph pseudo labels, and providing external knowledge/explanations.However, the LLM-GNN pipelines that are currently available are not end-to-end pipelines, meaning that LLM and GNN cannot be trained together.LLM and GNN can be trained separately using frameworks like EM framework [35] or by freezing LLM and using it as an external knowledge base.Co-training LLM and GNN can lead to issues like gradient vanishing, which is a significant obstacle in current LLM-GNN pipelines due to the large number of parameters in LLM compared to GNN.To solve these limitations, we propose feasible future directions like knowledge distillation, which can reduce the number of LLM parameters while retaining the beneficial capabilities for downstream tasks.When LLMs function as predictors, three main methods are used: prompting LLMs, SFT LLMs and efficient tunable module.All approaches for fine-tuning LLMs can be reviewed in the "comparisons and discussions" section of Section III.Currently, SFT and DPO are popular methods for fine-tuning LLMs.When LLMs function as graph generators, existing studies mainly use prompt engineering methods to iteratively prompt LLMs to extract entity-relationship triples in paragraphs.PiVe [82] and EDC [83] provide verifying or modifying ways to improve semantic graph generation quality.</p>
<p>Challenges and future directions.Classical graph tasks, such as node classification on attributed static networks, have recently obtained the most attention.However, there is potential for more complex tasks in the future, such as predicting graph evolution on dynamic graphs.Leveraging LLM models that are suitable for handling sequential data and can process time series data, along with GNNs capturing changes in graph structures, can help address a broader range of problems effectively.</p>
<p>VI. GRAPH-FORMED REASONING</p>
<p>Graph-formed reasoning refers to the process that LLMs engage in cognitive operations utilizing graph structures, such as the graph of thoughts, or verify conclusions through these structures to achieve more accurate and reliable answers.Simultaneously, this methodology aids in the interpretability analysis of the LLM reasoning process.LLMs have strong reasoning capabilities, and many prompting methods are proposed to enhance LLM reasoning abilities, addressing algorithmic problems, mathematical issues, etc., such as chain of thought, self-consistency, in-context learning, and more.However, these methods diverge from the patterns of human thought.The human thought process typically consists of nonlinear continuous thoughts.The main challenge is to design the LLM reasoning process that aligns with the human thought process.Graphs can represent the thinking patterns of humans during the thought process.In this section, we first introduce the tasks that LLMs can address through graph-formed reasoning.Subsequently, we summarize related works into two distinct types of reasoning: think on graphs and verify on graphs.</p>
<p>A. Task Introduction</p>
<p>LLM can solve more difficult problems through graphformed reasoning, such as algorithmic problems, logical reasoning problems, and mathematical word problems, as shown in Figure 11.Below, we detail three tasks that can be effectively addressed through the LLM graph-formed reasoning.Set operations mainly focus on set intersection.Specifically, the second input set is split into subsets and the intersection of those subsets with the first input set is determined with the help of the LLM, as shown in Figure 11 (a).Math word problems include single-and multi-step problems with addition, multiplication, subtraction, division, and other math operations.LLMs are required to understand mathematical relationships and then perform a multi-step reasoning process to ultimately reach an answer, as shown in Figure 11 (b).Logic reasoning is a process aimed at inference and argumentation rigorously.LLMs start from a set of premises and reason towards a conclusion supported by those premises.For example, propositional logic consists of p, q, r, and various operations, as shown in Figure 11 (c).</p>
<p>Other tasks can be effectively solved through graph-formed reasoning, including document merging, multi-hop question answering, medical question answering and causal inference.</p>
<p>B. Graph-formed Reasoning Methods</p>
<p>The graph form, with its inherent structural features, not only mimics human reasoning patterns but also validates answers from LLM through the relationships between nodes and local structure.Existing work can roughly be divided into two categories: think on the graph and verify on the graph, as shown in Figure 12.Think on the graph refers to LLM thinking in the form of a graph, where each node on the graph represents a step in the thinking process or an intermediate conclusion during thinking, and the edges on the graph indicate the direction of LLM inference or the relationships between intermediate thinking steps.In this way, the LLM thinking process can be visually represented in graph form.Verify on the graph means verifying the consistency and correctness of answers by utilizing the graph's structure.For example, if the end node of different paths is the same, the results derived from different paths should be the same.If contradictory conclusions arise, then the obtained conclusion is incorrect.</p>
<p>1) Think on the graph: The GoT<em> reasoning method [57] is proposed with a two-stage framework to enable LLM to reason on a graph for answering multiple-choice questions by converting the input query into a graph.Although GoT</em> allows LLM to enhance the graph using multimodal information, it does not reason for step-by-step deduction in graph form.The Graph of Thought (GoT) [55] represents LLM's intermediate thinking as an arbitrary graph, facilitating powerful prompting for solving algorithmic problems like sorting and keyword counts.LLM thoughts are depicted as vertices with edges</p>
<p>Set Operations Math word problems</p>
<p>Janet's ducks lay 16 eggs per day.She eats three for breakfast every morning and bakes muffins for her friends every day with four.She sells the remainder at the farmers' market daily for $2 per fresh duck egg.How much in dollars does she make every day at the farmers' market?</p>
<p>Logic reasoning</p>
<p>• Premises: 1.It is not true that some giant language models do not have good performance.</p>
<p>2.All language models with good performance are used by some researchers.</p>
<p>3.If a language model is used by some researchers, it is popular.4.If BERT is a giant language model, then GPT-3 is also a giant language model.5.BERT is a giant language model.</p>
<p>• Hypothesis: GPT-3 is popular.Give hypothesis label, true or false.</p>
<p>Verifying process</p>
<p>Fig. 12. Graph-formed reasoning: "Think on graphs" involves reasoning over graph structures to derive conclusions, while "Verify on graphs" entails using graph structures to validate the correctness of LLM outputs.</p>
<p>representing dependencies between them.Existing works also explore significant components within LLM-generated causal graphs.Vashishtha et al. [85] utilize the topological ordering of variables in a graph to enhance the causal inference capabilities of LLMs, thereby determining the causal relationships among different variables.For practical applications, MindMap [84] extracts and merges subgraphs from existing KGs.LLM then infers disease and treatments over these subgraphs to derive the final answer.Extending beyond the capabilities of a single LLM, multiple LLMs can also be collaboratively harnessed to tackle complex mathematical challenges.CR [56] is proposed as a more human-like reasoning process.CR utilizes three LLMs in different roles: the proposer, verifier, and reporter.</p>
<p>2) Verify on the graph: Verify on the graph is to validate the intermediate reasoning results of LLM to enhance its performance.GraphReason [86] assumes a logical connection between the intermediate steps of different inference paths created by LLM.However, this work trains an extra model to verify the correctness of the graph formed by LLM-generated solutions rather than utilizing the relationships between nodes in the graph.The Graph-guided CoT [87] evaluates whether the generated rationales by LLMs can answer the original question based on the consistency of adjacent rationales in the graph, Numerous cutting-edge studies have focused on quantifying the trustworthiness of LLMs and assessing the reliability of their responses.D-UE [88] computes a final uncertainty measurement on a constructed directed graph, which evaluates the trustworthiness of the LLM's responses.</p>
<p>C. Summary of Methods, Challenges and Future Directions</p>
<p>In summary, graph-formed reasoning is categorized into think on the graph and verify on the graph.Think on the graph refers to using the graph structure to derive the final conclusion during the reasoning process with LLM.Verify on the graph involves treating the intermediate or final results generated by LLM as nodes on the graph and using the graph to determine if there are contradictions between the nodes, thus verifying the correctness of the LLM output.</p>
<p>For "think on the graph", a common issue with existing approaches is their lack of convenience.Compared to CoT and self-consistency prompting techniques, the reasoning processes in current works are complex, requiring multiple stages of reasoning and validation.Graph of thought methods are not plug and play, which contradicts the original intent of prompts.Even though using more LLMs can simplify the reasoning and validation process, it raises the cost and barrier to entry for reasoning.Therefore, the current challenge is to find a plug-andplay, low-barrier LLM graph reasoning method that improves LLM reasoning capabilities.For "verify on the graph", the current approaches have yet to utilize the nature of the graph structure for validation.Existing methods either retrain a model to determine correctness or use a KG for assessment without using the relationships between nodes to infer whether the conclusions within each node in the graph are correct.</p>
<p>The future direction of "think on the graph" could focus on developing a plug-and-play, low-barrier LLM graph reasoning method that enhances LLM reasoning abilities, a pressing issue that needs to be addressed.On the other hand, the future</p>
<p>Graph embedding</p>
<p>Graph-enhanced text embedding Graph-encoded prompts direction of "verify on the graph" could explore how to utilize the relationships between nodes in the graph structure to verify the outputs of LLM or the reasoning process itself.</p>
<p>VII. GRAPH REPRESENTATION</p>
<p>A. Task introduction</p>
<p>LLMs' powerful text representation abilities empower text embeddings to capture deeper semantic nuances, which also can enhance graph representations, particularly for Text Attributed Graphs (TAGs).When dealing with structured text data, the key challenge is integrating graph structures into text embeddings produced by LLMs to enhance their informativeness or enable LLMs to process text embeddings with graph structures within the text space.Moreover, effectively incorporating the graph description within the prompt is essential for LLMs, especially in closed-source models like ChatGPT, where the embedding is invisible.How the graph is encoded within the prompt influences the model's comprehension of the graph.Thus, we summarize the following three types of graph representation: graph embedding, graph-enhanced text embedding, and graphencoded prompts, as shown in Figure 13.</p>
<p>1) Graph embedding: Graph embedding focuses on transforming a graph into a specific ordered sequence, which is then fed into an LLM/PLM to learn the token embedding within sequences using their semantic capturing ability, and then derive the graph embedding based on the token embedding.</p>
<p>2) Graph-enhanced text embedding: Graph-enhanced text embedding emphasizes incorporating structural embedding into text embedding.There are two types of embeddings: structural embedding, which captures the local structure, and text embedding, which captures the semantic meaning.How to combine these two types of embeddings is the core of graph-enhanced text embedding.</p>
<p>3) Graph-encoded prompts: Graph-encoded prompts concentrate on how to describe a graph so that LLMs can understand it more efficiently and then input it into LLMs.For instance, in a regular graph, the graph can be placed in a story context by assuming that the relationships between the nodes are friends or colleagues.</p>
<p>B. Graph Representation Methods</p>
<p>For each of three graph representations mentioned above, we present their specific objectives and techniques as follows.</p>
<p>1) Graph embedding: Texts are sequential data, while graph data is structural, posing a challenge for LLMs.LLMs excel at handling texts but struggle with graphs.How do LLMs effectively process graphs and derive effective graph representations?Graph embedding methods use specific order sequences to represent the graph, where specific order represents graph structure.WalkLM [37] aims to enhance graph representations in TAGs by utilizing a small language model.Initially, text sequences are generated on TAGs through random walks, capturing graph structural features.Subsequently, these sequences are input into a masked language model for the self-supervised training process, leading to the integration of semantic and structural graph embeddings.To adapt LLM to effectively derive semantic-enhanced graph embeddings, Path-LLM [60] proposes a new path selection mechanism, aligning with casual language modeling LLMs.Path-LLM first selects long shortest paths covering bridge paths between different dense groups and then convert long shortest paths into L2SPbased shortest paths in designed ways.Path-LLM uses L2SPbased shortest paths for LLM self-supervised generation process to derive enhanced graph embeddings.Existing methods also transform graphs into the natural language based on grammar trees.GraphText [58] reformulates graph reasoning as textto-text problems, establishing text input and output spaces.GraphText first constructs grammar trees for graphs, then traverses them to generate graph text sequences, and finally maps the graph to the text space.The text input is then fed into an LLM, with the LLM results mapped to the label space, effectively enabling LLMs to handle graph tasks.While the methods mentioned above primarily concentrate on textual data, there is a growing emphasis on utilizing multimodal data to capture more comprehensive information.GALLON [89] integrates the strengths of LLMs and GNNs by distilling multimodal knowledge into a unified MLP.This approach combines the rich textual and visual data of molecules with the structural analysis capabilities of GNNs.</p>
<p>Discussions of Path-LLM [60] vs. WalkLM [37].This study compares two methods that utilize language models through unsupervised training to generate unified graph embeddings: WalkLM and Path-LLM.WalkLM emphasizes small language models employing the masked language modeling strategy, while Path-LLM concentrates on LLMs with the causal language modeling process, which is the dominant strategy for LLMs.Both methods extract graph structural features as self-supervised signals.As shown in Figure 14, WalkLM uses Fig. 14.WalkLM [37] v.s.Path-LLM [60].</p>
<p>random walks to represent graph structural features for language model learning.In contrast, Path-LLM introduces a novel L2SPbased shortest paths selection mechanism that mitigates noise, capturing relationships between different dense groups as well as features within dense groups.Furthermore, Path-LLM aligns the order of nodes in designed L2SP-based paths with the direction of the causal language modeling process, ensuring that LLMs can effectively learn the graph structure.</p>
<p>2) Graph-enhanced text embedding: Graph-enhanced text embedding emphasizes adding graph structural embeddings as additional information into text embeddings.DGTL [59] adds graph structure embeddings into text embeddings for node classification, where LLMs derive text embeddings.While DGTL [59] concentrates on utilizing LLMs to integrate text and graph structure for graph tasks, G2P2 [90] emphasizes merging graph structure with text to address text classification tasks.Textual data commonly exhibits network structures, such as hyperlinks in purchase networks, which hold semantic relationships that can enhance text classification performance.</p>
<p>3) Graph-encoded prompts: Current related works focus on describing graph structures through natural language to LLMs in prompts without deeply learning the graph structure, which can lead to an LLM's insufficient understanding of complex structural relationships.Therefore, effectively encoding graphs in the prompt is vital for LLMs to comprehend graph structure and solve graph tasks.Graph encoding refers to how graphs are represented in the textual prompt.TalkGraph [67] introduces diverse graph encoding techniques by placing the same graph in multiple contexts.This strategy highlights how a node can be interpreted differently based on the context.For instance, a node could represent a person named David, with edges indicating various relationships like co-authorships or friendships.When asking LLM the degree of one node, in the given contexts, that equals how many friendships David has.In contrast, TalkGraph [67] primarily emphasizes text modality graph encoding, while GraphTMI [91] employs three encoding modalities, namely text, image, and motif, to encode graphs.Specifically, the text modality encoding provides insights into local structures, while the motif modality encoding captures essential graph patterns like stars, triangles, and cliques, offering a balanced perspective on local and global information.In comparing these two methods, TalkGraph [67] focuses on diverse graph encoding within text modality by constructing contexts, whereas GraphTMI [91] utilizes multiple modalities to encode graphs comprehensively, enhancing the LLMs' ability to understand graph structures.</p>
<p>C. Summary of Methods, Challenges and Future Directions</p>
<p>In summary, graph embedding focuses on transforming a graph into a specific ordered sequence, which is then fed into an LLM to learn the sequence's embedding and derive the graph embedding.On the other hand, graph-enhanced text embedding emphasizes incorporating structural embedding into text embedding.Lastly, graph-encoded prompts concentrate on how to describe a graph and input it into an LLM.However, due to LLMs' powerful text representation capabilities, the first two methods exhibit a deep semantic understanding of graph attributes.However, they still need suitable structural information capturing, which remains rudimentary and inadequate.Additionally, aligning the graph structure features with text features to better represent the graph's features is a current issue that needs to be addressed.For graph-encoded prompts, most methods build a narrative context for the graph or describe it multimodally before feeding it into an LLM.Both methods enable the LLM to interpret the graph from various perspectives to improve performance.The critical challenge currently lies in designing diverse and easily understandable graph descriptions for LLMs, conveying essential graph descriptions while enhancing the LLM's comprehension of the input description.</p>
<p>Discussions of unsupervised vs. supervised graph representation methods Current methodologies for LLM-based graph representation can be classified into two main categories: supervised and unsupervised graph representation methods.In this paper, due to the self-supervised learning process of LLMs not leveraging supervised signals from downstream tasks, we categorize methods that train LLMs through a self-supervised way to derive unified graph representation as unsupervised methods.Numerous cutting-edge supervised techniques are specifically designed for single graph learning tasks, such as node classification, exemplified by TAPE [33] and GraphAdapter [71].Furthermore, further supervised approaches train their models across various tasks, such as OFA [25] and LLaGA [79].Unsupervised methods leverage graph structural features as self-supervised signals to better understand the inherent nature of graphs.These methods derive unified graph embeddings that can be applied across various downstream tasks.Unlike supervised methods, which require extensive labeled data and are specifically trained for particular downstream tasks, unsupervised methods do not need labeled data or human manual resources.Consequently, unsupervised methods can be applied to a broader range of downstream tasks, including graph data analytics and even NP-hard problems such as keyword search [60].While supervised methods typically demonstrate superior performance on specific downstream tasks, the flexibility and reduced resource requirements of unsupervised methods make them a valuable and empowering alternative in many situations.</p>
<p>VIII. GRAPH-LLM-BASED APPLICATIONS</p>
<p>Graph-LLM-based applications refer to frameworks that integrate graphs with LLMs.Apart from their applications in graph-related tasks, they are also utilized in various other domains, such as recommendation systems.Common frameworks involve combining GNNs with LLMs, merging graph data with LLMs, and exploring additional innovative approaches that leverage the advantages of graph structures and language models for diverse applications.</p>
<p>1) Task planning: Graph structures are utilized to encode the world knowledge for LLM-based agents for task planning in the open world [140].Optimus-1 [104] transforms knowledge into a hierarchical directed graph for LLMs that allows agents to explicitly represent and learn world knowledge for longhorizon tasks in an open world.SayPlan [105] conducts a semantic search of task-relevant 3D scene graphs to encode world model information to the LLM for scaling task planning for robots to large, multi-room environments.</p>
<p>2) Recommendation systems: LLMs have been extensively utilized in recommendation systems [63].Graph structures are crucial for recommendation systems, as many tasks involve learning from user-item interaction networks.Sensitive domains such as educational recommendations require minimizing the risk of hallucinations in LLMs, combining with KGs [107].LLMHG [106] facilitates a human-centric modeling of user preferences by integrating the reasoning capabilities of LLMs with the structural advantages of hypergraph neural networks.</p>
<p>3) Conversational understanding: By combining LLM with graph traversal, collaborative query rewriting [141] is proposed to improve the coverage of unseen interactions, addressing the flawed queries users pose in dialogue systems.Flawed queries often arise due to ambiguities or inaccuracies in automatic speech recognition and natural language understanding.When integrated with graph traversal, LLM can effectively navigate through the graph structure to retrieve relevant information and provide more accurate responses.</p>
<p>4)</p>
<p>Response forecasting: LLM can effectively handle social networks and extract latent personas from users' profiles and historical posts.SOCIALSENSE [142] is proposed to utilize LLMs to extract information to predict the reactions of news media.By analyzing individuals' characteristics and behavior patterns within social networks, LLM can effectively predict the impact of news releases and prevent unintended adverse outcomes.</p>
<p>Many other applications leverage LLMs and graphs across various domains.MANAGER [108] utilizes LLMs with multimodal information and the dynamic financial knowledge graph (FinDKG) to address financial prediction.GRACE [109] incorporates graph structural information in the code and incontext learning to empower LLM-based software vulnerability detection.Additionally, Li et al. [110] employ LLMs to transform input scene graphs into affordance-enhanced graphs, thereby facilitating household rearrangement.Furthermore, GPT4GNAS [62] leverages GPT-4 to generate graph neural network structures.</p>
<p>IX. BENCHMARK DATASETS AND EVALUATIONS</p>
<p>A. Datasets</p>
<p>Table IV summarizes representative works in this survey, the popular and new datasets, the performed LLM-GGA tasks, and links to the corresponding open-source codes.</p>
<p>1) Popular datasets: Popular datasets refer to graph benchmarks that are widely and frequently used.We have systematically categorized these popular benchmarks in terms of five LLM-GGA directions as follows.</p>
<p>• Graph structure understanding: ogbn-arxiv [144], ogbn-products [144], Cora [145], CiteSeer [146], Aminer(DBLP) [147], MetaQA [148], Wikidata5M [149], PROTEINS [150], MUTAG [151], NCI1 [152], PTC [153], Foursqure [154].• Knowledge graphs and LLMs: CWQ [155], WebQSP [156], Wikidata [149], GrailQA [157], QALD10-en [158].• Graph learning: ogbn-arxiv [144], ogbn-products [144], ogb-papers110M [144], ogb-citation2 [144], Cora [145], CiteSeer [146], Amazon-items [159], PubMed [160], Reddit [14], CoraFull [161], Amazon [162], PROTEINS [150], COX2 [163], BZR [163], OAG [164].• Graph-formed reasoning: GSM8K [165], SVAMP [166],</p>
<p>FOLIO [167], Bayesian networks [168], CMCQA [169], GenMedGPT-5k [170].• Graph representation: Cora [145], CiteSeer [146],</p>
<p>Goodreads-books [171], PubMed [160], Amazon [162], MIMIIC-III [172], Freebase [173], FB15K-237 [174].</p>
<p>2) New datasets: We share several newly released benchmarks to evaluate LLM-based structure understanding ability and their potential to solve graph problems in Table V.</p>
<p>• GPR [69] contains 37 particular connected graph instances generated by the Networkx toolkit, which include the "bull graph," "wheel graph," "lollipop graph," etc. Graph instances have about 15 nodes and 28 links on average.• GraphTMI [91] is a multi-modality benchmark, containing the hierarchy of graphs, associated prompts, and encoding modalities.Based on the count of motifs and homophily in graphs, graph tasks have easy, medium, and hard levels.• LLM4DyG [45] is to evaluate whether LLMs are capable of understanding spatial-temporal information on dynamic graphs.Nine dynamic graph tasks are designed to assess LLM abilities from spatial and temporal dimensions.</p>
<p>• GraphQA [67] comprises a set of diverse fundamental graph problems with more varied and realistic graph structures compared to previous studies in LLM research.</p>
<p>• NLGraph [47] is to examine whether language models can reason with graphs and structures.It contains eight graph structure understanding tasks with varying algorithmic difficulties, enabling fine-grained analysis.• Head-to-Tail [92] aims to measure how knowledgeable LLMs are, consisting of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity.• CS-TAG [34] is a comprehensive and wide-ranging compilation of benchmark for TAGs, ranging from citation networks to purchase graphs.The collection consists of eight distinct TAGs sourced from diverse domains.</p>
<p>Method</p>
<p>Dataset LLM Task Link InstrucGLM [78] ogbn-arxiv, Cora, PubMed Flan-T5, Llama-v1-7b Node, link New Benchmark Link Applicable Tasks GPR [69] link Graph structure understanding GraphTMI [91] link Graph structure understanding, graph representation LLM4DyG [45] link Graph structure understanding GraphQA [67] link Graph representation NLGraph [47] link Graph structure understanding Head-to-Tail [92] link Graph learning CS-TAG [34] link Graph structure understanding, learning, representation NLGIFT [65] link Graph structure understanding</p>
<p>• NLGIFT [65] aims to evaluate whether LLM can generalize beyond semantic, numeric, and structural reasoning patterns in the synthetic training data.NLGIFT contains 37,000 problems in total and features five types of patterns.</p>
<p>B. Evaluation Metrics</p>
<p>Evaluation metrics are essential to determine how well LLMs perform their understanding of graphs and how effectively models combining graphs and LLMs perform on various tasks is vital.We summarize important and widely-used evaluation metrics for each LLM-GGA task, as shown in Table VI.</p>
<p>1) Graph structure understanding task: Several metrics usually used include the accuracy, ROUGE [175], BLEU [176], time cost, comprehension, correctness, fidelity, and rectification comprehension.Moreover, the comprehension, correctness, fidelity, and rectification comprehension are new metrics [44] used to evaluate LLM graph understanding ability and accuracy.</p>
<p>2) Graph learning task: Various metrics evaluate models' effectiveness, efficiency, and computational demands.For the effectiveness of models, metrics such as accuracy, macro-F1, mismatch rate, and denial rate [91]   are assessed.To evaluate the computational cost, the GPU occupancy and the token limit fraction are used.</p>
<p>3) Graph-formed reasoning task: The effectiveness metrics of graph-formed reasoning tasks include the accuracy, number of errors and cost, F1-score, precision, and recall [177].Meanwhile, the efficiency of the reasoning process is evaluated through metrics such as the Latency-Volume trade-off.</p>
<p>4) Graph representation: The effectiveness of graph representation is mainly evaluated by its downstream tasks.Metrics such as accuracy and F1-score are commonly used for node classification, while AUC and Hits@k are employed in link prediction.Additionally, visualization can be utilized for directly assessing the effectiveness of graph representations.</p>
<p>5) Knowledge graphs and LLMs: KG-related tasks typically involve question-answering tasks.Evaluation metrics commonly used include accuracy, precision, recall, F1-score, Hits@k [178], EM [179], MSE, hallucination rate [92] and for some generative tasks, human evaluation may also be utilized.</p>
<p>X. FUTURE DIRECTIONS</p>
<p>In the following, we discuss the remaining challenges and promising future directions of LLM-GGA.</p>
<p>A. LLM-CGA on complex graph patterns and tasks.Existing works address simple graph tasks and patterns, such as the shortest path, clustering coefficient, maximum flow, etc.However, how to leverage LLMs to address a broad range of NP-hard problems, such as community search and interactive graph problems, is still an open question.In addition, current graph learning tasks mainly focus on simple node, edge, and graph classification, neglecting complex graph learning problems, such as the diverse classification outcomes arising from homogeneous and heterogeneous graphs.</p>
<p>B. LLM-CGA on complex and big graph data.Due to the limited input length of LLM, the graph sizes inputted through prompts typically consist of dozens of nodes.However, for large graphs with tens of thousands of nodes and edges, how can LLMs with limited input length solve such large graphs?A larger input window is required in the case of attributed graphs, where both node and edge attributes need to be considered along with the graph structure.Except for the large-scale property, big graph data usually has diverse types, including higherorder networks, multi-layer networks, uncertain networks, and dynamic graphs, where each type of graph contains unique and special properties.For instance, dynamic graphs can be represented as ordered lists or asynchronous streams of timed events, capturing patterns of temporal network evolution, such as the addition or removal of nodes and edges.In the future, LLM-based agents with the API calls are one potential direction to accurately handle large-scale and complex graphs.</p>
<p>C. Advanced Graph Prompts.Graph prompts for LLMs have yet to be sufficiently explored.Relying solely on manual prompts and self-prompting has limited capabilities in improving model performance, as they only explore the existing abilities of LLM.As shown in Section III-C, LLMs can be trained as agents to utilize tools for graph tasks that are hard to solve, like API call prompt [69].GoT [180] is also a graph reasoning paradigm that enables LLMs to provide correct answers.Future work based on the graph reasoning paradigm can consider cost-effective approaches for GoT, such as pruning and tricks to reduce algorithm complexity.In the future, it would be beneficial to explore simpler GoT paradigms that can improve the effectiveness of LLMs.</p>
<p>D. Graph foundation model and explainability.LLM is undoubtedly the foundational model in NLP.We can draw inspiration from LLMs to train a graph foundation model.The current research [181] has primarily introduced graph foundation models in the form of graph-aware tuning LLMs and also GFM based on graph vocabulary.Exploring graph foundation models is a future direction for graph tasks.In addition, we discuss the explainability.LLMs' reasoning and thinking process can be step by step.The combination of LLMs and GNNs has the potential to offer a more transparent approach to solving graph problems by leveraging the LLM's reasoning abilities.If the combination of LLMs and GNNs is interpretable, it can be utilized for various tasks, including recommendation systems, drug discovery, and fraud detection.This combination can develop more reliable and efficient decision-making systems across multiple domains.</p>
<p>E. Security and privacy.Due to the dependencies on thirdparty vendors and service providers to carry out the LLMrelated applications, data security and LLM architecture security increasingly become key issues for practical applications.For data security, knowledge graphs can be employed to efficiently store and retrieve information concerning privacy policies for LLMs, like PrivComp-KG [182], which is a potential direction in RAG domains.For LLM architecture security, several attack strategies can infer different parts of the graph data and LLM architectures, for example, model extraction attack, graph structure reconstruction, attribute inference attacks, and membership inference attacks [183].In the future, exploring the protection strategies for privacy data and LLM architectures can advance the reliable and practical LLM-based systems.</p>
<p>F. Graph databases and vector databases.Graph databases and vector databases are composed of embeddings to represent semi-structured data [184] [185].Owing to the strong representation abilities of LLMs, LLMs can generate more effective and context-aware node embeddings, thereby enhancing the retrieval accuracy of k-nearest neighbor algorithms (KNN) in graph databases and vector databases.One significant challenge lies in adapting LLMs to efficiently process large-scale databases.Future exploration can optimize the synergy between LLMs and vector databases, balancing both effectiveness and efficiency.</p>
<p>XI. CONCLUSIONS</p>
<p>Large language models based generative graph analytics (LLM-GGA) has emerged as a promising direction to analyze classical graph tasks in an innovative way and tackle bottlenecks in analyzing complex graphs with text-attributes.This paper introduces a comprehensive structural taxonomy based on recent research, which classifies LLM-GGA research into three main directions: LLM-GQP, LLM-GIL, and graph-LLM-based applications.LLM-GQP encompasses graph understanding and knowledge graphs, while LLM-GIL involves graph learning, graph-formed reasoning, and graph representation.The motivation, challenges, and mainstream methods of each direction are thoroughly examined.This survey paper offers 40+ datasets, a dozen of evaluation metrics, and the source codes for more than 40 mainstream methods in the discussed directions.It highlights the existing challenges in current methods and proposes several future directions to guide and motivate further research in this promising LLM-GGA field.</p>
<p>•</p>
<p>Fig. 1.Illustration of the LLM-GGA domain.LLM-GGA domain includes three principal components: LLM-based graph query processing (LLM-GQP), which integrates graph analytics techniques and LLM prompts for query processing; LLM-based graph inference and learning (LLM-GIL), focusing on learning and reasoning over graphs; Graph-LLM-based applications that employ the graph-LLM framework to address non-graph tasks.</p>
<p>Fig. 3 .
3
Fig. 3. Examples of graph structure understanding tasks (polynomial-time problems are shown in green and NP-hard problems are shown in blue).</p>
<p>How many C-C-O triangles are in the molecule?Graph-enhanced prefix: Structural and textual features LLM Response: There is 1 C-C-O triangle in the molecule.</p>
<p>Fig. 5 .
5
Fig.5.Supervised fine-tuning methods for graph structure understanding tasks, like prefix tuning[70].</p>
<p>USKGQA</p>
<p>Given <knowledge graph>, the director who directs Inception also direct what?Inception Nolan is dire cted by is d ir e ct e d b y Oppenheimer c o u n t r y Leonardo is starred by</p>
<p>Abstract:</p>
<p>Text in curve orientation, despite being one of the common… Title: Total Text A Comprehensive Dataset For Scene Text Detection And Recognition.</p>
<p>Fig. 7 .
7
Fig. 7. Examples of graph learning tasks.</p>
<p>encoding text attributes into embeddings (b) generating graph pseudo labels (c) providing external knowledge/explanations</p>
<p>Fig. 8 .
8
Fig. 8. LLMs-as-enhancers methods are categorized into three types: (a) Encoding text attributes into embeddings by inputting node text attributes into LLMs, then integrating these embeddings with graph structures for GNN training.(b) Generating graph pseudo-labels by feeding unlabeled nodes to LLMs for labeling, subsequently using these pseudo-labeled nodes for GNN training.(c) Providing external knowledge for downstream GNNs through two pipelines: the first enhances text attribute details via elaboration, while the second uses text attributes and queries to generate answers and explain reasoning.</p>
<p>How many C-C-O triangles are in the molecule?Response: There is 1 C-C-O triangle in the molecule.Response: There is no C-C-O triangle in the molecule.Response: There is 4 C-C-O triangle in the molecule.The title of one paper is <Title> and its abstract is <Abstract>.This paper is cited by the following papers: <Titlelist1>.Each of these papers belongs to one category in: <Categories>.You need to analyze the paper's topic based on the given title and abstract.</p>
<p>How many C-C-O triangles are in the molecule?Tunable module Response: There is 1 C-C-O triangle in the molecule.</p>
<p>Fig. 9 .
9
Fig. 9. LLMs-as-predictors methods are categorized into three types: (a) Prompting LLMs by inputting designed prompts to predict nodes, links, or graphs; (b) Fine-tuning by providing instructions to generate responses, followed by refining LLMs based on feedback; (c) Efficient tunable modules where frozen LLMs are combined with tunable components to tackle graph learning tasks, emphasizing the training of lightweight modules.</p>
<p>Transform the text into a semantic graph.Francisco Uranga was born in 1905 and represented Argentina at the 1928 Summer Olympics.He competed in the men's 50 metre freestyle.</p>
<p>Fig. 10 .
10
Fig. 10.LLMs-as-graph-generators: LLMs generate graphs by extracting entities and key relationships.</p>
<p>Find</p>
<p>the intersection of two sets of numbers.Output only the set of numbers that are present in both sets, no additional text.Input Set 1:[13,16,30,6,21,7,31,15,11,1,24,10,9,3,20,8] Input Set 2:[25,24,10,4,27, 0,14,12,8,2,29,20,17,19,26,23]</p>
<p>Fig. 11 .
11
Fig. 11.Three examples of graph-formed reasoning tasks.</p>
<p>Fig. 13 .
13
Fig. 13.Three types of graph representation: graph embedding, graph-enhanced text embedding, and graph-encoded prompts.</p>
<p>TABLE I COMPARISON
I
OF LLM-GGA AND OTHER SURVEYS.</p>
<p>novel contribution to summarize graph structure understanding, graph-formed reasoning, and graph-LLM-based applications, which make this survey more thorough and upto-date.These topics are useful for exploring LLMs in graph theory and topology (e.g., complex P and NP problems), a more reliable graph-formed LLM reasoning process (e.g., tree of thought and graph of thought), and practical applications of recommendation systems and task planning in the open world.
Our contributions and the identified challenges for futureresearch. In this paper, we provide a comprehensive surveyof the state-of-the-art work on LLMs applied to graph data.We begin by delineating six critical directions in the fieldof LLM-GGA: graph structure understanding, knowledgegraphs and LLMs, graph learning, graph-formed reasoning,graph representation, and graph-LLM-based applications. Thiscategorization clarifies the current work and offers a guidelinefor future research endeavors. In each direction, we propose astructured introduction and summarization using vivid examples
[60][27][24][28][29][30][31][32].We conduct a detailed comparison of these relevant surveys in TableI.In addition, we provide a novel taxonomy in Figure2to give a clear organization of LLM-based graph analytics methods.Comparing the existing survey[23], we have conducted a more comprehensive review of the latest research works in graph learning and graph representation, e.g.,[79][80] [71] [36] [82] [54] [83] [74][60].Moreover, we make a and offer suitable specific pipelines.We analyze the advantages and limitations of current methodologies and suggest avenues for future research.Furthermore, we organize resources related to benchmarks, evaluations, and code links within the LLM-GGA domain to facilitate further investigation by researchers.</p>
<p>…… …… …… Trainable LLM Regular prompt: What is the diameter of the binomial tree? API call prompt: The diameter of the binomial tree is Cerulean [GR(GL("gpr", "binomial_tree"), "toolx:diameter") → r]</p>
<p>.
New contexts: Text description ofFinal output: The clusteringinput graph generated by LLM itself.coefficient of node X is …Frozen LLM……Frozen LLM…………………………Manual prompt: Given <graph>, what is the number of nodes and edges in this graph? Please answer with the number of nodes: X, number of edges: X.Instructor: You are a brilliant graph master that can handle anything related to graphs like retrieval, detection and classification. Graph description language: GML, GraphML, etc. Query: What is the clustering coefficient of node X?Manual promptSelf-promptingAPI call promptsFig. 4. Three main prompting methods in graph structure understanding tasks, which are manual prompts, self-prompting, and API call prompts.3. Polynomial-time problems involve analyzing neighbors,shortest paths, graph diameter, the clustering coefficient, andtopological sorting. On the other hand, NP-hard problemsentail addressing tasks such as graph edit distance, maximumclique, maximum vertex cover, and other related problems.Discussions. (1) Different GDLs can lead to varying resultswith LLMs, so testing multiple GDLs and selecting the best-performing one is recommended. (2) Adjacency lists candescribe larger graphs with dozens of nodes, while otherGDLs usually describe smaller ones with several nodes. (3)Natural language descriptions, like the text description andencoding as a story, can effectively make LLMs understandgraph structures. Structured descriptions (e.g., SQL and GML)can more precisely represent graph structures but pose greaterchallenges for LLMs to comprehend. (4) API call promptsin Prompt III-6 can potentially handle large-scale graphs bycalling relevant APIs. (5) Specifying the LLM's output formatin the prompt can help reduce unnecessary reasoning processes.III. GRAPH STRUCTURE UNDERSTANDING TASKSGraph structure understanding tasks evaluate whether LLMscan comprehend graph structures consisting of polynomial-time problems and NP-hard problems, as shown in Figure</p>
<p>TABLE II PROMPTS
II
FOR GRAPH UNDERSTANDING TASKS.
Task CategoriesReferenceRepresentative TasksPrompt III-1graph degreePrompt III-2shortest pathP ProblemsPrompt III-3dynamic graphsPrompt III-5clustering coefficient computingPrompt III-6graph diameterNP-hard problemsPrompt III-4graph coloring</p>
<p>TABLE III COMPARISON
III
OF REPRESENTATIVE WORKS IN GRAPH LEARNING.
MethodRole of LLMsTraining of LLMsGNNs usageDownstream tasksTraining strategyTAPE</p>
<p>TABLE IV A
IV
SUMMARY OF LLM-GGA REPRESENTATIVE METHODS WITH DATASETS AND SOURCE LINKS.</p>
<p>are considered.In terms of efficiency, metrics like training time and tuned parameters</p>
<p>code link GraphAdapter[71]ogbn-arxiv, Instagram, Reddit Llama2-13B Node code link GPT4Graph[43]ogbn-arxiv,Aminer,Wiki,MetaQA InstructGPT-3 Graph structure, node, graph code link LLMtoGraph[44]Synthetic data GPT-3.5,GPT-4, Vicuna, Lazarus Multi-hop Reasoning code link Graph-LLM[74]ogbn-arxiv, Cora, PubMed, ogbn-products LLaMA, Palm Node code link TAPE[33]ogbn-arxiv, Cora, PubMed, ogbn-products GPT-3.5 Node code link LLM4DyG[45]LLM4DyG GPT-3.5, Vicuna, Llama2, CodeLlama Graph code link GraphGPT[139]ogbn-arxiv, Cora, PubMed Vicuna Node code link Graph-ToolFormer[69]GPR, Cora, Pubmed, Citeseer, Proteins, etc. GPT-J Graph Q&amp;A, graph structure code link LLaGA[79]ogbn-arxiv, ogbn-products, Pubmed, Cora GPT-3.5-turboNode code link LLM-GNN[73]Cora, Citeseer, PubMed, Wiki, ogbn-arxiv,etc.GPT-3.5-turboNode code link GraphTMI[91]Cora, Citeseer, Pubmed,GraphTMI GPT-4, GPT-4V Representation, node code link WalkLM[37]PubMed, MIMIC-III, Freebase, FB15K-237 PLMs Representation, node, link code link GraphText[58]Cora, Citeseer, Texas, Wisconsin, Cornell Llama2 Node code link TALK LIKE A GRAPH[67]GraphQA PaLM Node, link code link Graph-guided CoT[87]Wiki, MusiQue, Bamboogle Llama2 Multi-hop question answering -NLGraph[47]NLGraph GPT-3.5, GPT-4 Link,node,graph,path,pattern code link Gorilla[68]APIBench Llama2, GPT-3.5, GPT-4 Multimodal graphs code link Collaborative Query Rewriting[141]opportunity test sets, guardrail test set Dolly V2 Conversational understanding -WHEN AND WHY[46]ogbn-arxiv, Cora, PubMed, ogbn-products ChatGPT Node code link CR[56]Folio, LogiQA, Proofwriter, Logicaldeduction GPT-3.5-turbo,GPT-4, Llama2 Logic reasoning code link SOCIALSENSE[142]RFPN, Twitter PLMs Response forecasting code link MindMap[84]GenMedGPT-5k, CMCQA, ExplainCPE GPT-3.5, GPT-4 Medical Q&amp;A code link PiVe[82]KELM, WebNLG+2020, GenWiki GPT-4 Graph generation code link Graph of Thought(GoT)[55]Non-open source data GPT3.5 Graph-formed reasoning code link GLEM[35]ogbn-arxiv, ogbn-products, ogbn-papers PLMs Node code link LPNL[77]OAG T5-base Link -SIMTEG[72]ogbn-arxiv, ogbn-products, ogbl-citation2 PLMs Node, link code link Llmrec[61]Netflix, MovieLens gpt-3.5-turbo-16kRecommendation code link OFA[25]ogbn-arxiv, Cora PLMs Node, link, graph code link GraphTranslator[80]ogbn-arxiv, Taobao GPT-3.5, GPT-4 Node, graph Q&amp;A code link GAugLLM[36]PubMed, ogbn-arxiv, Amazon Llama2 Node code link GPT4GNAS[62]ogbn-arxiv, Cora, PubMed, Citeseer GPT-4 Graph neural architecture search -Graphllm[70]NLGraph Llama2-7B, Llama2-13B Link, node, graph, path, pattern code link G2P2[90]Cora, Amazon PLMs Representation code link ChatGraph[143]Gradio GPT-4V, Next-GPT Link, node, graph, application -GoT*[57]AQUA-RAT, ScienceQA T5-base Graph-formed reasoning code link KGP[100]HotpotQA, IIRC, Wiki, MuSiQue, PDFTriage Llama2 KG+LLM code link Head-to-Tail[92]DBpredia, Movie, Book, Academics GPT-4 KG+LLM code link GLaM[50]DBLP, UMLS Llama2-7B KG+LLM -ToG[51]CWQ, WebQSP, GrailQA, QALD10-en, etc. GPT-3.5, GPT-4, Llama2 KG+LLM code link Autoalign[97]DBpedia, Wikidata ChatGPT, Claude KG+LLM code link GNP[52]ConceptNet, UMLS, OpenBookQA, etc. FLAN-T5 KG+LLM code link RoG[52]WebQSP, CWQ, Freebase Llama2-7B KG+LLM code link TABLE V A SUMMARY OF NEW DATASETS.
Finetuned language models are zero-shot learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, ICLR2022</p>
<p>Visual instruction tuning. H Liu, C Li, NeurIPS. 362024</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, C D Manning, S Ermon, C Finn, NeurIPS. 362024</p>
<p>Trustllm: Trustworthiness in large language models. Y Huang, L Sun, H Wang, S Wu, Q Zhang, Y Li, ICML. 2024</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, arXiv:2307.092882023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, NeurIPS. 352022</p>
<p>Causalabstain: Enhancing multilingual llms with causal reasoning for trustworthy abstention. Y Sun, A Zuo, W Gao, J Ma, arXiv:2506.005192025arXiv preprint</p>
<p>Toolqa: A dataset for llm question answering with external tools. Y Zhuang, Y Yu, NeurIPS. 362023</p>
<p>Flexkbqa: A flexible llm-powered framework for few-shot knowledge base question answering. Z Li, S Fan, Y Gu, X Li, Z Duan, B Dong, AAAI. 202438616</p>
<p>Prompting large language model for machine translation: A case study. B Zhang, B Haddow, ICML. 2023110</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. J Liu, C S Xia, Y Wang, L Zhang, NeurIPS. 362024</p>
<p>Lever: Learning to verify language-to-code generation with execution. A Ni, S Iyer, D Radev, V Stoyanov, W -T. Yih, S Wang, X V Lin, ICML. 2023128</p>
<p>Collective classification in network data. P Sen, G Namata, M Bilgic, L Getoor, B Galligher, T Eliassi-Rad, AI magazine. 2932008</p>
<p>Inductive representation learning on large graphs. W Hamilton, Z Ying, J Leskovec, NeurIPS. 302017</p>
<p>Moleculenet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, Chemical science. 922018</p>
<p>Graph structure in the web. A Broder, R Kumar, F Maghoul, P Raghavan, S Rajagopalan, R Stata, A Tomkins, J Wiener, Computer networks. 331-62000</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, ICLR. 2017</p>
<p>Graph attention networks. P Velickovic, G Cucurull, A Casanova, A Romero, stat. 1050205502017</p>
<p>Neural message passing for quantum chemistry. J Gilmer, S S Schoenholz, P F Riley, ICML. 2017</p>
<p>Aggregation-induced emission: phenomenon, mechanism and applications. Y Hong, J W Lam, B Z Tang, Chemical communications. 292009</p>
<p>On provable benefits of depth in training graph convolutional networks. W Cong, M Ramezani, M Mahdavi, NeurIPS. 342021</p>
<p>Generalizing graph neural networks on out-of-distribution graphs. S Fan, X Wang, C Shi, P Cui, B Wang, 2023TPAMI</p>
<p>A survey of graph meets large language model: progress and future directions. Y Li, Z Li, P Wang, J Li, X Sun, H Cheng, J X Yu, IJCAI. 2024</p>
<p>A survey of large language models for graphs. X Ren, J Tang, D Yin, N Chawla, C Huang, SIGKDD. 2024</p>
<p>One for all: Towards training one graph model for all classification tasks. H Liu, J Feng, L Kong, N Liang, D Tao, Y Chen, M Zhang, ICLR2024</p>
<p>Large language models on graphs: A comprehensive survey. B Jin, G Liu, C Han, M Jiang, H Ji, J Han, 2024TKDE</p>
<p>Graph foundation models: Concepts, opportunities and challenges. J Liu, C Yang, Z Lu, J Chen, Y Li, M Zhang, T Bai, Y Fang, L Sun, P S Yu, 2025TPAMI</p>
<p>Graph intelligence with large language models and prompt learning. J Li, X Sun, Y Li, Z Li, H Cheng, J X Yu, SIGKDD. 2024</p>
<p>Graph retrieval-augmented generation: A survey. B Peng, Y Zhu, Y Liu, X Bo, H Shi, C Hong, Y Zhang, S Tang, arXiv:2408.089212024arXiv preprint</p>
<p>Retrieval-augmented generation with graphs (graphrag). H Han, Y Wang, H Shomer, K Guo, J Ding, Y Lei, M Halappanavar, R A Rossi, arXiv:2501.003092024arXiv preprint</p>
<p>A survey of graph retrievalaugmented generation for customized large language models. Q Zhang, S Chen, Y Bei, Z Yuan, H Zhou, Z Hong, J Dong, H Chen, Y Chang, X Huang, arXiv:2501.139582025arXiv preprint</p>
<p>Knowledge graphs, large language models, and hallucinations: An nlp perspective. E Lavrinovics, R Biswas, J Bjerva, K Hose, Journal of Web Semantics. 851008442025</p>
<p>Harnessing explanations: Llm-to-lm interpreter for enhanced textattributed graph representation learning. X He, X Bresson, T Laurent, A Perold, Y Lecun, B Hooi, ICLR2024</p>
<p>A comprehensive study on text-attributed graphs: Benchmarking and rethinking. H Yan, C Li, R Long, C Yan, J Zhao, W Zhuang, J Yin, P Zhang, NeurIPS. 362023</p>
<p>Learning on large-scale text-attributed graphs via variational inference. J Zhao, M Qu, C Li, H Yan, Q Liu, ICLR2023</p>
<p>Gaugllm: Improving graph contrastive learning for text-attributed graphs with large language models. Y Fang, D Fan, D Zha, Q Tan, SIGKDD. 2024</p>
<p>Walklm: A uniform language model fine-tuning framework for attributed graph embedding. Y Tan, Z Zhou, H Lv, W Liu, C Yang, NeurIPS. 362024</p>
<p>Gram: graph-based attention model for healthcare representation learning. E Choi, M T Bahadori, SIGKDD. 2017</p>
<p>Graph representation learning in biomedicine and healthcare. M M Li, K Huang, M Zitnik, Nature Biomedical Engineering. 6122022</p>
<p>Attributed social network embedding. L Liao, X He, H Zhang, T.-S Chua, TKDE. 30122018</p>
<p>Social network analysis with content and graphs. W M Campbell, C K Dagli, C J Weinstein, Lincoln Laboratory Journal. 2012013</p>
<p>Distilling knowledge on text graph for social media attribute inference. Q Li, X Li, L Chen, D Wu, SIGIR. 2022</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, M Zhou, X He, S Han, arXiv:2305.150662023arXiv preprint</p>
<p>Evaluating large language models on graphs: Performance insights and comparative analysis. C Liu, B Wu, arXiv:2308.112242023arXiv preprint</p>
<p>Llm4dyg: Can large language models solve spatial-temporal problems on dynamic graphs. Z Zhang, X Wang, Z Zhang, H Li, Y Qin, W Zhu, SIGKDD. 2024</p>
<p>Can llms effectively leverage graph structural information through prompts, and why?. J Huang, X Zhang, Q Mei, J Ma, TMLR. 2024</p>
<p>Can language models solve graph problems in natural language. H Wang, S Feng, T He, Z Tan, NeurIPS. 362024</p>
<p>Large language model for science: A study on p vs. np. Q Dong, L Dong, K Xu, G Zhou, Y Hao, Z Sui, F Wei, arXiv:2309.056892023arXiv preprint</p>
<p>Nphardeval: Dynamic benchmark on reasoning ability of large language models via complexity classes. L Fan, W Hua, L Li, H Ling, Y Zhang, L Hemphill, ACL. 2024</p>
<p>Glam: Fine-tuning large language models for domain knowledge graph alignment via neighborhood partitioning and generative subgraph encoding. S Dernbach, AAAI. 20243</p>
<p>Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph. J Sun, C Xu, L Tang, S Wang, C Lin, Y Gong, L Ni, H.-Y Shum, J Guo, ICLR. 2024</p>
<p>Graph neural prompting with large language models. Y Tian, H Song, Z Wang, H Wang, Z Hu, F Wang, N V Chawla, P Xu, AAAI. 20243888</p>
<p>Reasoning on graphs: Faithful and interpretable large language model reasoning. L Luo, Y.-F Li, R Haf, S Pan, ICLR2024</p>
<p>Large language models can learn temporal reasoning. S Xiong, A Payani, R Kompella, F Fekri, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241470</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, AAAI. 202438690</p>
<p>Cumulative reasoning with large language models. Y Zhang, J Yang, Y Yuan, A C , -C Yao, arXiv:2308.043712023arXiv preprint</p>
<p>Beyond chain-of-thought, effective graph-of-thought reasoning in language models. Y Yao, Z Li, H Zhao, arXiv:2305.165822023arXiv preprint</p>
<p>Graphtext: Graph reasoning in text space. J Zhao, L Zhuo, Y Shen, M Qu, K Liu, M M Bronstein, Z Zhu, J Tang, AFM2024</p>
<p>Disentangled representation learning with large language models for text-attributed graphs. Y Qin, X Wang, Z Zhang, W Zhu, arXiv:2310.181522023arXiv preprint</p>
<p>Path-llm: A shortest-pathbased llm learning for unified graph representation. W Shang, X Zhu, X Huang, arXiv:2408.054562024arXiv preprint</p>
<p>Llmrec: Large language models with graph augmentation for recommendation. W Wei, X Ren, J Tang, Q Wang, L Su, S Cheng, J Wang, D Yin, C Huang, 2024WSDM</p>
<p>Graph neural architecture search with gpt-4. H Wang, Y Gao, X Zheng, P Zhang, H Chen, J Bu, P S Yu, arXiv:2310.014362023arXiv preprint</p>
<p>Exploring large language model for graph data understanding in online job recommendations. L Wu, Z Qiu, Z Zheng, H Zhu, E Chen, AAAI. 202438</p>
<p>Grapharena: Benchmarking large language models on graph computational problems. J Tang, Q Zhang, Y Li, J Li, 2025ICLR</p>
<p>Can llm graph reasoning generalize beyond pattern memorization. Y Zhang, H Wang, Findings of EMNLP. 2024</p>
<p>Graphinsight: Unlocking insights in large language models for graph structure understanding. Y Cao, S Han, Z Gao, Z Ding, X Xie, S K Zhou, arXiv:2409.032582024arXiv preprint</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J Halcrow, B Perozzi, ICLR2024</p>
<p>Gorilla: Large language model connected with massive apis. S G Patil, T Zhang, NeurIPS. 372024</p>
<p>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. J Zhang, arXiv:2304.111162023arXiv preprint</p>
<p>Graphllm: Boosting graph reasoning ability of large language model. Z Chai, T Zhang, L Wu, K Han, X Hu, X Huang, Y Yang, arXiv:2310.058452023arXiv preprint</p>
<p>Can gnn be good adapter for llms. X Huang, K Han, Y Yang, D Bao, Q Tao, Z Chai, Q Zhu, 2024WWW</p>
<p>Simteg: A frustratingly simple approach improves textual graph learning. K Duan, Q Liu, T.-S Chua, S Yan, W T Ooi, Q Xie, J He, arXiv:2308.025652023arXiv preprint</p>
<p>Label-free node classification on graphs with large language models (llms). Z Chen, H Mao, H Wen, H Han, W Jin, H Zhang, H Liu, J Tang, ICLR. 2024</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, SIGKDD. 2522024</p>
<p>Beyond text: A deep dive into large language models' ability on understanding graph data. Y Hu, NeurIPS Workshop. 2023</p>
<p>Knowledge graph large language model (kg-llm) for link prediction. D Shu, T Chen, M Jin, C Zhang, M Du, ACML2024</p>
<p>Lpnl: Scalable link prediction with large language models. B Bi, S Liu, Y Wang, Findings of ACL. 2024</p>
<p>Language is all a graph needs. R Ye, C Zhang, R Wang, S Xu, Y Zhang, Findings of EACL. 2024</p>
<p>Llaga: Large language and graph assistant. R Chen, T Zhao, A K Jaiswal, N Shah, Z Wang, ICML. 2024</p>
<p>Graphtranslator: Aligning graph model to large language model for open-ended tasks. M Zhang, M Sun, P Wang, S Fan, Y Mo, X Xu, H Liu, C Yang, C Shi, 2024WWW</p>
<p>Efficient tuning and inference for large language models on textual graphs. Y Zhu, Y Wang, IJCAI. 2024</p>
<p>PiVe: Prompting with iterative verification improving graph-based generative capability of LLMs. J Han, N Collier, W Buntine, E Shareghi, Findings of ACL. Aug. 2024</p>
<p>Extract, define, canonicalize: An llm-based framework for knowledge graph construction. B Zhang, EMNLP. 2024</p>
<p>MindMap: Knowledge graph prompting sparks graph of thoughts in large language models. Y Wen, Z Wang, J Sun, ACL. Aug. 2024</p>
<p>Causal inference using llm-guided discovery. A Vashishtha, A G Reddy, AAAI Workshop. 2023</p>
<p>Enhancing reasoning capabilities of large language models: A graph-based verification approach. L Cao, arXiv:2308.092672023</p>
<p>Graph-guided reasoning for multi-hop question answering in large language models. J Park, A Patel, O Z Khan, H J Kim, J.-K Kim, arXiv:2311.097622023arXiv preprint</p>
<p>Llm uncertainty quantification through directional entailment graph and claim level response augmentation. L Da, T Chen, L Cheng, H Wei, arXiv:2407.009942024arXiv preprint</p>
<p>Llm and gnn are complementary: Distilling llm for multimodal graph learning. J Xu, Z Wu, M Lin, X Zhang, S Wang, arXiv:2406.010322024arXiv preprint</p>
<p>Augmenting low-resource text classification with graphgrounded pre-training and prompting. Z Wen, SIGIR. 2023</p>
<p>Which modality should i use-text, motif, or image?: Understanding graphs with large language models. D Das, I Gupta, J Srivastava, D Kang, Findings of NAACL. 2024</p>
<p>Head-to-tail: How knowledgeable are large language models (llms)? aka will llms replace knowledge graphs. K Sun, Y Xu, H Zha, Y Liu, X L Dong, NAACL. 2024</p>
<p>G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. X He, Y Tian, Y Sun, N Chawla, T Laurent, NeurIPS. 379072024</p>
<p>Give us the facts: Enhancing large language models with knowledge graphs for fact-aware language modeling. L Yang, H Chen, Z Li, X Ding, X Wu, 2024TKDE</p>
<p>From local to global: A graph rag approach to query-focused summarization. D Edge, H Trinh, N Cheng, J Bradley, arXiv:2404.161302024arXiv preprint</p>
<p>Think-on-graph 2.0: Deep and interpretable large language model reasoning with knowledge graphguided retrieval. S Ma, C Xu, X Jiang, 20242407arXiv e-prints</p>
<p>Autoalign: fully automatic and effective knowledge graph alignment enabled by large language models. R Zhang, Y Su, B D Trisedya, X Zhao, M Yang, H Cheng, J Qi, 2023TKDE</p>
<p>Explore then determine: A gnnllm synergy framework for reasoning over knowledge graph. G Liu, Y Zhang, Y Li, Q Yao, arXiv:2406.011452024arXiv preprint</p>
<p>Llm-based multi-level knowledge generation for few-shot knowledge graph completion. Q Li, Z Chen, C Ji, S Jiang, J Li, IJCAI. 2714947032024</p>
<p>Knowledge graph prompting for multi-document question answering. Y Wang, N Lipka, R A Rossi, A Siu, R Zhang, T Derr, AAAI. 202438214</p>
<p>Improving llm-based kgqa for multi-hop question answering with implicit reasoning in few-shot examples. M Shah, J Tian, ACL KaLLM 2024. 2024</p>
<p>In-context learning with topological information for llm-based knowledge graph completion. U M Sehwag, K Papasotiriou, J Vann, S Ganesh, ICML Workshop. 2024</p>
<p>A unified llm-kg framework to assist fact-checking in public deliberation. N Giarelis, C Mastrokostas, N Karacapilidis, LREC-COLING Workshop. 2024</p>
<p>Optimus-1: Hybrid multimodal memory empowered agents excel in long-horizon tasks. Z Li, Y Xie, R Shao, G Chen, D Jiang, L Nie, NeurIPS2024</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. K Rana, J Haviland, S Garg, J Abou-Chakra, I Reid, N Suenderhauf, CoRL2023</p>
<p>Llm-guided multi-view hypergraph learning for human-centric explainable recommendation. Z Chu, Y Wang, arXiv:2401.082172024</p>
<p>Knowledge graphs as context sources for llm-based explanations of learning recommendations. H Abu-Rasheed, C Weber, M Fathi, EDUCON. 2024</p>
<p>Modal-adaptive knowledge-enhanced graph-based financial prediction from monetary policy conference calls with llm. K Ouyang, Y Liu, S Li, COLING Workshop. 2024</p>
<p>Grace: Empowering llm-based software vulnerability detection with graph structure and in-context learning. G Lu, X Ju, X Chen, W Pei, Z Cai, JSS. 2121120312024</p>
<p>Llm-enhanced scene graph learning for household rearrangement. W Li, Z Yu, Q She, SIGGRAPH. 2024</p>
<p>A survey of community search over big graphs. Y Fang, X Huang, L Qin, Y Zhang, W Zhang, R Cheng, X Lin, VLDB Journal. 292020</p>
<p>Finding the k shortest paths. D Eppstein, SIAM Journal on computing. 2821998</p>
<p>A comprehensive survey on graph neural networks. Z Wu, S Pan, F Chen, TNNLS. 3212020</p>
<p>Graph neural networks: A review of methods and applications. J Zhou, G Cui, S Hu, Z Zhang, AI open. 12020</p>
<p>Graph neural networks in recommender systems: a survey. S Wu, F Sun, ACM Computing Surveys. 5552022</p>
<p>A survey of large language models. W X Zhao, K Zhou, J Li, T Tang, arXiv:2303.182232023arXiv preprint</p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. J Yang, H Jin, R Tang, X Han, Q Feng, H Jiang, S Zhong, B Yin, X Hu, TKDD. 1862024</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Token prediction as implicit classification to identify llm-generated text. Y Chen, H Kang, V Zhai, L Li, R Singh, B Raj, EMNLP. 202313120</p>
<p>Gpt4roi: Instruction tuning large language model on region-of-interest. S Zhang, P Sun, S Chen, M Xiao, W Shao, W Zhang, Y Liu, K Chen, P Luo, ECCV. 2025</p>
<p>How far can camels go? exploring the state of instruction tuning on open resources. Y Wang, H Ivison, P Dasigi, J Hessel, T Khot, K Chandu, D Wadden, K Macmillan, N A Smith, I Beltagy, NeurIPS. 362023</p>
<p>Cypher: An evolving query language for property graphs. N Francis, A Green, P E A Guagliardo, SIGMOD. 2018</p>
<p>The gremlin graph traversal machine and language. M A Rodriguez, 2015invited talk)," in DBPL</p>
<p>Semantics and complexity of sparql. J Pérez, M Arenas, C Gutierrez, TODS. 3432009</p>
<p>Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers. B Wang, R Shin, ACL. 2020</p>
<p>A survey of large language models on generative graph analytics: Query, learning, and applications. W Shang, X Huang, arXiv:2404.148092024arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, ACM Computing Surveys. 5592023</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessì, NeurIPS. 362024</p>
<p>Gpt-j-6b: A 6 billion parameter autoregressive language model. B Wang, A Komatsuzaki, 2021</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, NeurIPS. 352022</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, ICLR2023</p>
<p>A survey on in-context learning. Q Dong, L Li, D Dai, EMNLP. 2024</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017</p>
<p>Raft: Reward ranked finetuning for generative foundation model alignment. H Dong, W Xiong, D Goyal, Y Zhang, W Chow, R Pan, S Diao, J Zhang, K Shum, T Zhang, 2023TMLR</p>
<p>Preference ranking optimization for human alignment. F Song, B Yu, M Li, H Yu, F Huang, Y Li, H Wang, AAAI. 202438998</p>
<p>Unifying large language models and knowledge graphs: A roadmap. S Pan, L Luo, Y Wang, C Chen, J Wang, X Wu, 2024TKDE</p>
<p>Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities. Y Zhu, X Wang, J E A Chen, World Wide Web. 275582024</p>
<p>Graphgpt: Graph instruction tuning for large language models. J Tang, Y Yang, W Wei, L Shi, L Su, S Cheng, D Yin, C Huang, SIGIR. 2024</p>
<p>Can graph learning improve planning in llm-based agents. X Wu, Y Shen, C Shan, K Song, S Wang, B Zhang, J Feng, H Cheng, W Chen, Y Xiong, 2024NeurIPS</p>
<p>Graph meets llm: A novel approach to collaborative filtering for robust conversational understanding. Z Chen, Z Jiang, F Yang, E Cho, X Fan, X Huang, EMNLP. 2023</p>
<p>Decoding the silent majority: Inducing belief augmented social graph with large language model for response forecasting. C Sun, J Li, Y Fung, H Chan, T Abdelzaher, EMNLP. 2023</p>
<p>Chatgraph: Chat with your graphs. Y Peng, S Lin, Q Chen, S Wang, L Xu, X Ren, Y Li, J Xu, ICDE. 2024</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. W Hu, M Fey, M Zitnik, Y Dong, H Ren, B Liu, M Catasta, J Leskovec, NeurIPS. 331332020</p>
<p>Automating the construction of internet portals with machine learning. A K Mccallum, K Nigam, J Rennie, K Seymore, Information Retrieval. 32000</p>
<p>Citeseer: An automatic citation indexing system. C L Giles, K D Bollacker, S Lawrence, Proceedings of the third ACM conference on Digital. the third ACM conference on Digital1998</p>
<p>Arnetminer: extraction and mining of academic social networks. J Tang, J Zhang, L Yao, SIGKDD. 2008</p>
<p>Variational reasoning for question answering with knowledge graph. Y Zhang, H Dai, Z Kozareva, AAAI. 201832</p>
<p>Kepler: A unified model for knowledge embedding and pre-trained language representation. X Wang, T Gao, Z Zhu, Z Zhang, Z Liu, J Li, J Tang, ACL. 92021</p>
<p>Protein function prediction via graph kernels. K M Borgwardt, C S Ong, S Schönauer, S Vishwanathan, A J Smola, H.-P Kriegel, Bioinformatics. 2112005</p>
<p>Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. A K Debnath, Journal of Medicinal Chemistry. 3421991</p>
<p>Comparison of descriptor spaces for chemical compound retrieval and classification. N Wale, I A Watson, G Karypis, Knowledge and Information Systems. 142008</p>
<p>Statistical evaluation of the predictive toxicology challenge. H Toivonen, A Srinivasan, R D King, S Kramer, C Helma, Bioinformatics. 19102000-2001. 2003</p>
<p>Inferring anchor links across multiple heterogeneous social networks. X Kong, J Zhang, P S Yu, CIKM. 2013</p>
<p>The value of semantic parse labeling for knowledge base question answering. W.-T Yih, M Richardson, ACL. 2016</p>
<p>The web as a knowledge-base for answering complex questions. A Talmor, J Berant, NAACL. 2018</p>
<p>Beyond iid: three levels of generalization for question answering on knowledge bases. Y Gu, S Kase, M Vanni, B Sadler, P Liang, X Yan, Y Su, WWW. ACM2021</p>
<p>Qald-9-plus: A multilingual dataset for question answering over dbpedia and wikidata translated by native speakers. A Perevalov, D Diefenbach, R Usbeck, A Both, ICSC. 2022</p>
<p>Item recommendation on monotonic behavior chains. M Wan, J Mcauley, RecSys. 2018</p>
<p>Justifying recommendations using distantly-labeled reviews and fine-grained aspects. J Ni, J Li, EMNLP. 2019</p>
<p>Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. A Bojchevski, S Günnemann, ICLR2018</p>
<p>Pitfalls of graph neural network evaluation. O Shchur, M Mumme, A Bojchevski, S Günnemann, arXiv:1811.058682018</p>
<p>The network data repository with interactive graph analytics and visualization. R Rossi, N Ahmed, AAAI. 201529</p>
<p>An analysis framework of research frontiers based on the large-scale open academic graph. H Huang, ASIST. 57e3072020</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, arXiv:2110.141682021arXiv preprint</p>
<p>Are nlp models really able to solve simple math word problems?. A Patel, S Bhattamishra, N Goyal, NAACL. 2021</p>
<p>Folio: Natural language reasoning with first-order logic. S Han, H Schoelkopf, Y Zhao, EMNLP. 20242231</p>
<p>Bayesian networks with examples in r. M Scutari, 2015</p>
<p>MedConQA: Medical conversational question answering system based on knowledge graphs. F Xia, B Li, EMNLP. 2022</p>
<p>Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge. Y Li, Z Li, K Zhang, R Dan, S Jiang, Y Zhang, Cureus. 1562023</p>
<p>The effect of metadata on scientific literature tagging: A cross-field cross-model study. Y Zhang, 2023WWW</p>
<p>Mimic-iii, a freely accessible critical care database. A E Johnson, T J Pollard, L Shen, Scientific data. 312016</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. K Bollacker, C Evans, P Paritosh, T Sturge, J Taylor, SIGMOD. 2008</p>
<p>Observed versus latent features for knowledge base and text inference. K Toutanova, D Chen, ACL Workshop. 2015</p>
<p>Rouge: A package for automatic evaluation of summaries. C.-Y Lin, Text summarization branches out. 2004</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, ACL. 2002</p>
<p>A probabilistic interpretation of precision, recall and f-score, with implication for evaluation. C Goutte, ECIR. 2005</p>
<p>The web as a graph: Measurements, models, and methods. J M Kleinberg, R Kumar, COCOON. 1999</p>
<p>Causal inference without balance checking: Coarsened exact matching. S M Iacus, G King, Political analysis. 202012</p>
<p>Gimlet: A unified graph-text model for instruction-based molecule zeroshot learning. H Zhao, S Liu, M Chang, H Xu, J Fu, Z Deng, L Kong, Q Liu, NeurIPS. 362023</p>
<p>Position: Graph foundation models are already here. H Mao, Z Chen, W E A Tang, ICML. 2024</p>
<p>L Garza, L Elluri, arXiv:2404.19744Privcomp-kg: Leveraging knowledge graph and large language models for privacy policy compliance verification. 2024</p>
<p>Graph neural networks: a survey on the links between privacy and security. F Guan, T Zhu, W Zhou, K.-K R Choo, Artificial Intelligence Review. 572402024</p>
<p>Tigervector: Supporting vector search in graph databases for advanced rags. S Liu, Z Zeng, L Chen, ICMD. 2025</p>
<p>Graph data management and graph machine learning: Synergies and opportunities. A Khan, X Ke, arXiv:2502.005292025</p>            </div>
        </div>

    </div>
</body>
</html>
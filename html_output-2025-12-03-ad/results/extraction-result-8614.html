<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8614 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8614</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8614</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-264128006</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.09107v2.pdf" target="_blank">GLoRE: Evaluating Logical Reasoning of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios. Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date. GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8614.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8614.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 125M-parameter pre-trained transformer encoder used as a supervised fine-tuned baseline on GLoRE; fine-tuned for five epochs per dataset to provide classical supervised performance on logical reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLoRE: Evaluating Logical Reasoning of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A robustly optimized BERT-style masked‑language transformer encoder used here as a supervised baseline, fine-tuned on each dataset's training set for five epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE (Multi-choice Reading Comprehension, NLI, True-or-False) — examples: LogiQA 2.0, LogiQA22, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Supervised classification tasks probing strict logical reasoning across multi-choice reading comprehension (complex verbal logic problems), NLI (entailment/contradiction/neutral), and yes/no (true-or-false) reasoning requiring multi-step, symbolic-like inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning on each dataset's training set (five epochs) as a classical baseline; evaluated in zero-shot (for LLMs) comparisons vs. fine-tuned supervised model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varied by dataset; examples: LogiQA 2.0 test accuracy 48.76%, LogiQA22 33.22%, NaN-NLI 90.02%, ProofWriter 55.92% (all percentages, classification accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Serves as the supervised baseline; many instruction-tuned and reasoning-enhanced LLMs (e.g., GPT-4, QwQ-32B, DeepSeek R1) surpass RoBERTa on most MRC and TF tasks, though RoBERTa can match or beat some LLMs on certain NLI subsets (e.g., NaN-NLI).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Despite fine-tuning, RoBERTa lags humans on complex MRC; performance uneven across dataset types, indicating limits in generalizing complex multi-step verbal logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Supervised fine-tuning yields reasonable but incomplete logical reasoning capability; strong NLI-like pattern learning is possible (e.g., NaN-NLI) but generalization to complex multi-premise MRC and TF tasks remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8614.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8614.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-30B-supercot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source family member (30B variant) evaluated in zero-shot and few-shot settings; shows limited zero-shot logical reasoning on GLoRE without in-context demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLoRE: Evaluating Logical Reasoning of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-30B-supercot</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An instruction-following variant of the LLaMA foundation model (30B) used in the experiments; evaluated zero-shot and with few-shot in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>30B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE (MRC, NLI, TF) — e.g., LogiQA 2.0, LogiQA22, ReClor, ProofWriter, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Zero-shot and few-shot evaluations of natural-language logical reasoning across multiple datasets of multi-step verbal inference, entailment, and yes/no proof-style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation and few-shot in-context learning (1/2/5-shot) appended to prompts; no additional task-specific fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot average reported around 32.34% (GLoRE zero-shot aggregate), few-shot improves modestly (e.g., 5-shot ~39.62% on aggregate in Table 3); poor on many MRC tasks (~20% on some 4-way tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforms RoBERTa-base on most tasks without demonstrations; improves with ICL but remains below reasoning-enhanced models and GPT-4 on aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Very sensitive to task format and distribution; on some MRC tasks accuracy around or below random-chance for multi-way questions without in-context examples; indicates lack of robust general logical inference in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Instruction-following LLaMA variants need in-context examples to approach reasonable performance; ICL yields gains but likely via surface pattern adaptation rather than robust logical algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8614.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8614.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-40B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 40B open large language model instruction-tuned for following prompts, evaluated zero-shot and few-shot on GLoRE; exhibits performance similar to LLaMA in zero-shot logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLoRE: Evaluating Logical Reasoning of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-40B-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 40B-parameter open-source autoregressive LLM instruction-tuned for natural language tasks; evaluated on GLoRE across zero-shot and few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE (MRC, NLI, TF) — LogiQA 2.0, LogiQA22, ReClor, ProofWriter, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Instruction-following LLM evaluation on strict logical reasoning benchmarks including multi-choice verbal logic, NLI entailment tasks, and yes/no proof-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot in-context learning; no additional specialized reasoning training reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot aggregate ~32.28%; few-shot improves to 5-shot ~35.72% (Table 3); generally poor on MRC without demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Comparable to LLaMA and below specialized reasoning-enhanced models and GPT-4; underperforms RoBERTa on several tasks when no in-context examples are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Low zero-shot performance on MRC (many ~20% accuracies on 4-way tasks), indicating lack of robust logical generalization without demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Instruction tuning alone is insufficient for robust zero-shot logical reasoning; in-context learning provides improvements but may rely on superficial distributional cues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8614.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8614.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral-8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixture-of-experts style open model (Mixtral-8x7b) evaluated on GLoRE; demonstrates stronger zero-shot reasoning than LLaMA/Falcon, showing the effectiveness of MoE architectures for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLoRE: Evaluating Logical Reasoning of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A mixture-of-experts (MoE) language model variant evaluated as part of the open-source model set; outperforms other similarly scoped open models in zero-shot logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE (MRC, NLI, TF) — same dataset suite as other models</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluated on multi-choice reading comprehension, NLI, and TF datasets requiring multi-step verbal logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot evaluation (and few-shot in-context learning experiments reported for LLMs broadly); no MoE-specific training details provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot performance higher than LLaMA/Falcon (exact per-dataset numbers not exhaustively listed but reported as outperforming them in the zero-shot block).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms LLaMA and Falcon in zero-shot, demonstrating MoE effectiveness; still below reasoning-enhanced models (QwQ-32B, DeepSeek R1) and GPT-4 on aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still shows sensitivity to distribution; performance details uneven across datasets; specific failure cases not enumerated beyond aggregate sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Mixture-of-experts architectures can improve zero-shot logical reasoning behavior, but do not fully close the gap to reasoning-targeted models or robust human-level generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8614.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8614.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned conversational LLM from OpenAI evaluated zero-shot and few-shot on GLoRE; shows moderate logical reasoning ability with improvement from in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLoRE: Evaluating Logical Reasoning of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's conversational instruction-tuned LLM (ChatGPT) evaluated on GLoRE in zero-shot and few-shot regimes; demonstrates above-baseline performance on many NLI and TF tasks but inconsistent across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE (MRC, NLI, TF) — including ConTRoL, HELP, NaN-NLI, ReClor, ProofWriter, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Conversational LLM evaluated on verbal logical reasoning, entailment, monotonicity, negation, and synthetic rule-based reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot in-context learning (1/2/5-shot); instruction-following capabilities leveraged without task-specific fine-tuning in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot aggregate ~52.10%; few-shot (5-shot) improves to ~60.32% (Table 3). On ConTRoL ChatGPT achieves 58.45% (noted to surpass GPT-4 on that dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Better than RoBERTa on several tasks and substantially better than many open models in zero-shot; still below GPT-4 and QwQ-32B on overall aggregate (ChatGPT 52.10% vs GPT-4 66.34% and QwQ-32B 78.95%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dataset-sensitive: inconsistent across NLI and TF tasks; relies on superficial distributional cues when improving via ICL, per paper's analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Instruction tuning and conversational framing yield gains on some reasoning tasks, but improvements via ICL may not reflect deeper algorithmic reasoning; robustness and distributional generalization remain issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8614.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8614.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's state-of-the-art multi-modal capable LLM evaluated on GLoRE; achieves strong zero-shot and few-shot performance, close to human-level on some datasets but shows dataset sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLoRE: Evaluating Logical Reasoning of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's flagship LLM evaluated on the GLoRE benchmark in zero-shot and few-shot settings; demonstrates high accuracy on several MRC and TF datasets but notable variance across distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE (MRC, NLI, TF) — LogiQA 2.0, LogiQA22, ReClor, HELP, NaN-NLI, FraCaS, ProofWriter, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>High-level natural language logical reasoning tasks including multi-step reading comprehension, entailment classification, monotonicity and negation, and synthetic proof-based true/false reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and few-shot in-context learning; instruction-following; evaluated without additional specialized fine-tuning within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot aggregate average reported 66.34%; few-shot (5-shot) improved to 75.83% (Table 3). Examples: ReClor 87.20%, NaN-NLI 75.74%, LogiQA 2.0 72.25% but LogiQA22 58.49% (showing distribution sensitivity); FraCaS 75.35%; ProofWriter 59.66%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Surpasses RoBERTa and many open models in zero-shot; outperformed by QwQ-32B on overall GLoRE average (66.34% vs QwQ-32B 78.95%) and on several MRC/TF tasks; mixed relative to ChatGPT depending on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Marked sensitivity to data distribution (e.g., drop from 72.25% on LogiQA 2.0 to 58.49% on LogiQA22), inconsistent NLI/TF performance across datasets, and susceptibility to dataset-specific superficial cues rather than uniform algorithmic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>GPT-4 achieves strong performance but not uniformly human-level across GLoRE; results emphasize distributional sensitivity and the need for benchmarks that test generalizable logical algorithms rather than dataset-specific patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8614.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8614.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-enhanced OpenAI model variant evaluated on GLoRE that shows competitive performance on NLI and some TF tasks, illustrating improvements from specialized training strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLoRE: Evaluating Logical Reasoning of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1 mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An OpenAI model variant described as a reasoning-enhanced model in the experiments; evaluated in zero-shot and few-shot settings on GLoRE datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE (MRC, NLI, TF) — e.g., HELP (monotonicity), NaN-NLI, ConTRoL, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Evaluation focused on entailment and monotonicity-sensitive NLI tasks and multi-choice/TF tasks to probe structured logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Instruction-following zero-shot and few-shot evaluation; described as reasoning-enhanced by OpenAI system cards (cited), no further bespoke method details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Noted to have strong NLI performance on certain datasets (e.g., HELP 63.69% reported as better than QwQ-32B's 61.53%); overall aggregate less than top open reasoning models (exact average not listed in main text but reported below QwQ-32B).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms some LLMs and matches or exceeds QwQ-32B on specific fine-grained NLI tasks (e.g., HELP), but is behind QwQ-32B on aggregate benchmark performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance uneven across datasets; like other LLMs, shows distributional sensitivity and lacks fully generalizable logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Specialized reasoning-focused variants can improve on particular logical phenomena (e.g., monotonicity), but improvements are not uniformly general across all logical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8614.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8614.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-enhanced open model trained with reinforcement learning incentives to improve logical reasoning; achieves strong GLoRE performance nearing top open-source results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLoRE: Evaluating Logical Reasoning of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An open reasoning-enhanced model described as using reinforcement learning to incentivize reasoning capability (cited Deepseek-Ai work); evaluated across GLoRE tasks and exhibits high average accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE (MRC, NLI, TF) — e.g., ReClor, AR-LSAT, ProofWriter, FraCaS, ConTRoL, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks requiring multi-step verbal logic (MRC), entailment analysis (NLI), and true/false proof reasoning requiring explicit rule application.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Described as using reinforcement learning incentives to improve reasoning capabilities (per cited Deepseek-Ai work); evaluated zero-shot and few-shot in-context learning where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>High aggregate performance: reported average ~75.14% across GLoRE; dataset examples include ProofWriter 80.51%, various MRC and NLI scores listed in Table 2 (per-dataset values in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially above RoBERTa and many standard instruction-tuned LLMs; slightly below QwQ-32B on overall average (75.14% vs 78.95%), and generally competitive with or better than GPT-4 on several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Shows dataset sensitivity similar to other LLMs; may underperform on specific NLI phenomena (paper notes unevenness across NLI datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Reinforcement learning incentivizing reasoning yields strong practical gains across logical reasoning benchmarks, but generalization across different logical phenomena (e.g., monotonicity, negation) is still imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8614.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8614.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 32B-parameter reasoning-enhanced open model leveraging reinforcement learning and specialized training; achieves state-of-the-art results on GLoRE, particularly on multi-choice MRC and TF tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLoRE: Evaluating Logical Reasoning of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A reasoning-enhanced open LLM (32B parameters) described as leveraging reinforcement learning and specialized training methodology; evaluated extensively on the GLoRE benchmark where it attains the highest reported average accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GLoRE (MRC, NLI, TF) — including ReClor, AR-LSAT, LogiQA, LogiQA22, ProofWriter, FraCaS, NLI datasets (HELP, ConTRoL, NaN-NLI), etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A comprehensive suite testing strict logical reasoning: multi-step verbal deduction in MRC, entailment and monotonicity in NLI, and synthetic proof/true-false reasoning demanding multi-premise rule application.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Reasoning-specialized training (described as reinforcement learning based in paper and cited QwQ team work), instruction-following, evaluated in zero-shot and few-shot ICL settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Top aggregate performance on GLoRE with average accuracy 78.95%; exemplary results include ReClor 93.76%, AR-LSAT 92.35%, ProofWriter 82.40%; robust on LogiQA22 (86.30%) outperforming GPT-4 by ~27.8 points on that split.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms GPT-4 (66.34% average) and DeepSeek R1 (75.14%) on overall GLoRE average and sets state-of-the-art on several MRC and TF datasets; however, lags on some NLI datasets (e.g., HELP 61.53% behind o1 mini).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Uneven performance across NLI tasks—weaker on monotonicity/negation-sensitive datasets like HELP; indicates specialization favors certain reasoning formats (MRC/TF) over fine-grained entailment reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Targeted architectural and RL-based training innovations can markedly improve performance on strict logical reasoning benchmarks, but gains may be format-specific; more diverse evaluation is necessary to assess general logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GLoRE: Evaluating Logical Reasoning of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>Not another negation benchmark: The NaN-NLI test suite for sub-clausal negation <em>(Rating: 2)</em></li>
                <li>Help: A dataset for identifying shortcomings of neural models in monotonicity reasoning <em>(Rating: 2)</em></li>
                <li>Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Qwq-32b: Embracing the power of reinforcement learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8614",
    "paper_id": "paper-264128006",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "RoBERTa-base",
            "name_full": "RoBERTa (base)",
            "brief_description": "A 125M-parameter pre-trained transformer encoder used as a supervised fine-tuned baseline on GLoRE; fine-tuned for five epochs per dataset to provide classical supervised performance on logical reasoning benchmarks.",
            "citation_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base",
            "model_description": "A robustly optimized BERT-style masked‑language transformer encoder used here as a supervised baseline, fine-tuned on each dataset's training set for five epochs.",
            "model_size": "125M",
            "reasoning_task_name": "GLoRE (Multi-choice Reading Comprehension, NLI, True-or-False) — examples: LogiQA 2.0, LogiQA22, ReClor, AR-LSAT, ConTRoL, HELP, TaxiNLI, NaN-NLI, FraCaS, RuleTaker, ProofWriter",
            "reasoning_task_description": "Supervised classification tasks probing strict logical reasoning across multi-choice reading comprehension (complex verbal logic problems), NLI (entailment/contradiction/neutral), and yes/no (true-or-false) reasoning requiring multi-step, symbolic-like inference.",
            "method_or_approach": "Supervised fine-tuning on each dataset's training set (five epochs) as a classical baseline; evaluated in zero-shot (for LLMs) comparisons vs. fine-tuned supervised model.",
            "performance": "Varied by dataset; examples: LogiQA 2.0 test accuracy 48.76%, LogiQA22 33.22%, NaN-NLI 90.02%, ProofWriter 55.92% (all percentages, classification accuracy).",
            "baseline_comparison": "Serves as the supervised baseline; many instruction-tuned and reasoning-enhanced LLMs (e.g., GPT-4, QwQ-32B, DeepSeek R1) surpass RoBERTa on most MRC and TF tasks, though RoBERTa can match or beat some LLMs on certain NLI subsets (e.g., NaN-NLI).",
            "limitations_or_failures": "Despite fine-tuning, RoBERTa lags humans on complex MRC; performance uneven across dataset types, indicating limits in generalizing complex multi-step verbal logical reasoning.",
            "insights_or_conclusions": "Supervised fine-tuning yields reasonable but incomplete logical reasoning capability; strong NLI-like pattern learning is possible (e.g., NaN-NLI) but generalization to complex multi-premise MRC and TF tasks remains limited.",
            "uuid": "e8614.0",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA",
            "name_full": "LLaMA-30B-supercot",
            "brief_description": "An open-source family member (30B variant) evaluated in zero-shot and few-shot settings; shows limited zero-shot logical reasoning on GLoRE without in-context demonstrations.",
            "citation_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA-30B-supercot",
            "model_description": "An instruction-following variant of the LLaMA foundation model (30B) used in the experiments; evaluated zero-shot and with few-shot in-context learning.",
            "model_size": "30B",
            "reasoning_task_name": "GLoRE (MRC, NLI, TF) — e.g., LogiQA 2.0, LogiQA22, ReClor, ProofWriter, etc.",
            "reasoning_task_description": "Zero-shot and few-shot evaluations of natural-language logical reasoning across multiple datasets of multi-step verbal inference, entailment, and yes/no proof-style reasoning.",
            "method_or_approach": "Zero-shot evaluation and few-shot in-context learning (1/2/5-shot) appended to prompts; no additional task-specific fine-tuning reported.",
            "performance": "Zero-shot average reported around 32.34% (GLoRE zero-shot aggregate), few-shot improves modestly (e.g., 5-shot ~39.62% on aggregate in Table 3); poor on many MRC tasks (~20% on some 4-way tasks).",
            "baseline_comparison": "Underperforms RoBERTa-base on most tasks without demonstrations; improves with ICL but remains below reasoning-enhanced models and GPT-4 on aggregate.",
            "limitations_or_failures": "Very sensitive to task format and distribution; on some MRC tasks accuracy around or below random-chance for multi-way questions without in-context examples; indicates lack of robust general logical inference in zero-shot.",
            "insights_or_conclusions": "Instruction-following LLaMA variants need in-context examples to approach reasonable performance; ICL yields gains but likely via surface pattern adaptation rather than robust logical algorithms.",
            "uuid": "e8614.1",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon",
            "name_full": "Falcon-40B-instruct",
            "brief_description": "A 40B open large language model instruction-tuned for following prompts, evaluated zero-shot and few-shot on GLoRE; exhibits performance similar to LLaMA in zero-shot logical reasoning.",
            "citation_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Falcon-40B-instruct",
            "model_description": "A 40B-parameter open-source autoregressive LLM instruction-tuned for natural language tasks; evaluated on GLoRE across zero-shot and few-shot settings.",
            "model_size": "40B",
            "reasoning_task_name": "GLoRE (MRC, NLI, TF) — LogiQA 2.0, LogiQA22, ReClor, ProofWriter, etc.",
            "reasoning_task_description": "Instruction-following LLM evaluation on strict logical reasoning benchmarks including multi-choice verbal logic, NLI entailment tasks, and yes/no proof-style tasks.",
            "method_or_approach": "Zero-shot and few-shot in-context learning; no additional specialized reasoning training reported in this paper.",
            "performance": "Zero-shot aggregate ~32.28%; few-shot improves to 5-shot ~35.72% (Table 3); generally poor on MRC without demonstrations.",
            "baseline_comparison": "Comparable to LLaMA and below specialized reasoning-enhanced models and GPT-4; underperforms RoBERTa on several tasks when no in-context examples are provided.",
            "limitations_or_failures": "Low zero-shot performance on MRC (many ~20% accuracies on 4-way tasks), indicating lack of robust logical generalization without demonstrations.",
            "insights_or_conclusions": "Instruction tuning alone is insufficient for robust zero-shot logical reasoning; in-context learning provides improvements but may rely on superficial distributional cues.",
            "uuid": "e8614.2",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Mixtral",
            "name_full": "Mixtral-8x7b",
            "brief_description": "A mixture-of-experts style open model (Mixtral-8x7b) evaluated on GLoRE; demonstrates stronger zero-shot reasoning than LLaMA/Falcon, showing the effectiveness of MoE architectures for reasoning tasks.",
            "citation_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7b",
            "model_description": "A mixture-of-experts (MoE) language model variant evaluated as part of the open-source model set; outperforms other similarly scoped open models in zero-shot logical reasoning.",
            "model_size": null,
            "reasoning_task_name": "GLoRE (MRC, NLI, TF) — same dataset suite as other models",
            "reasoning_task_description": "Evaluated on multi-choice reading comprehension, NLI, and TF datasets requiring multi-step verbal logical inference.",
            "method_or_approach": "Zero-shot evaluation (and few-shot in-context learning experiments reported for LLMs broadly); no MoE-specific training details provided in paper.",
            "performance": "Zero-shot performance higher than LLaMA/Falcon (exact per-dataset numbers not exhaustively listed but reported as outperforming them in the zero-shot block).",
            "baseline_comparison": "Outperforms LLaMA and Falcon in zero-shot, demonstrating MoE effectiveness; still below reasoning-enhanced models (QwQ-32B, DeepSeek R1) and GPT-4 on aggregate.",
            "limitations_or_failures": "Still shows sensitivity to distribution; performance details uneven across datasets; specific failure cases not enumerated beyond aggregate sensitivity.",
            "insights_or_conclusions": "Mixture-of-experts architectures can improve zero-shot logical reasoning behavior, but do not fully close the gap to reasoning-targeted models or robust human-level generalization.",
            "uuid": "e8614.3",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI)",
            "brief_description": "An instruction-tuned conversational LLM from OpenAI evaluated zero-shot and few-shot on GLoRE; shows moderate logical reasoning ability with improvement from in-context learning.",
            "citation_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "OpenAI's conversational instruction-tuned LLM (ChatGPT) evaluated on GLoRE in zero-shot and few-shot regimes; demonstrates above-baseline performance on many NLI and TF tasks but inconsistent across datasets.",
            "model_size": null,
            "reasoning_task_name": "GLoRE (MRC, NLI, TF) — including ConTRoL, HELP, NaN-NLI, ReClor, ProofWriter, etc.",
            "reasoning_task_description": "Conversational LLM evaluated on verbal logical reasoning, entailment, monotonicity, negation, and synthetic rule-based reasoning datasets.",
            "method_or_approach": "Zero-shot and few-shot in-context learning (1/2/5-shot); instruction-following capabilities leveraged without task-specific fine-tuning in this evaluation.",
            "performance": "Zero-shot aggregate ~52.10%; few-shot (5-shot) improves to ~60.32% (Table 3). On ConTRoL ChatGPT achieves 58.45% (noted to surpass GPT-4 on that dataset).",
            "baseline_comparison": "Better than RoBERTa on several tasks and substantially better than many open models in zero-shot; still below GPT-4 and QwQ-32B on overall aggregate (ChatGPT 52.10% vs GPT-4 66.34% and QwQ-32B 78.95%).",
            "limitations_or_failures": "Dataset-sensitive: inconsistent across NLI and TF tasks; relies on superficial distributional cues when improving via ICL, per paper's analysis.",
            "insights_or_conclusions": "Instruction tuning and conversational framing yield gains on some reasoning tasks, but improvements via ICL may not reflect deeper algorithmic reasoning; robustness and distributional generalization remain issues.",
            "uuid": "e8614.4",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "OpenAI's state-of-the-art multi-modal capable LLM evaluated on GLoRE; achieves strong zero-shot and few-shot performance, close to human-level on some datasets but shows dataset sensitivity.",
            "citation_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's flagship LLM evaluated on the GLoRE benchmark in zero-shot and few-shot settings; demonstrates high accuracy on several MRC and TF datasets but notable variance across distributions.",
            "model_size": null,
            "reasoning_task_name": "GLoRE (MRC, NLI, TF) — LogiQA 2.0, LogiQA22, ReClor, HELP, NaN-NLI, FraCaS, ProofWriter, etc.",
            "reasoning_task_description": "High-level natural language logical reasoning tasks including multi-step reading comprehension, entailment classification, monotonicity and negation, and synthetic proof-based true/false reasoning.",
            "method_or_approach": "Zero-shot and few-shot in-context learning; instruction-following; evaluated without additional specialized fine-tuning within this work.",
            "performance": "Zero-shot aggregate average reported 66.34%; few-shot (5-shot) improved to 75.83% (Table 3). Examples: ReClor 87.20%, NaN-NLI 75.74%, LogiQA 2.0 72.25% but LogiQA22 58.49% (showing distribution sensitivity); FraCaS 75.35%; ProofWriter 59.66%.",
            "baseline_comparison": "Surpasses RoBERTa and many open models in zero-shot; outperformed by QwQ-32B on overall GLoRE average (66.34% vs QwQ-32B 78.95%) and on several MRC/TF tasks; mixed relative to ChatGPT depending on dataset.",
            "limitations_or_failures": "Marked sensitivity to data distribution (e.g., drop from 72.25% on LogiQA 2.0 to 58.49% on LogiQA22), inconsistent NLI/TF performance across datasets, and susceptibility to dataset-specific superficial cues rather than uniform algorithmic reasoning.",
            "insights_or_conclusions": "GPT-4 achieves strong performance but not uniformly human-level across GLoRE; results emphasize distributional sensitivity and the need for benchmarks that test generalizable logical algorithms rather than dataset-specific patterns.",
            "uuid": "e8614.5",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "o1 mini",
            "name_full": "OpenAI o1 mini",
            "brief_description": "A reasoning-enhanced OpenAI model variant evaluated on GLoRE that shows competitive performance on NLI and some TF tasks, illustrating improvements from specialized training strategies.",
            "citation_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "mention_or_use": "use",
            "model_name": "o1 mini",
            "model_description": "An OpenAI model variant described as a reasoning-enhanced model in the experiments; evaluated in zero-shot and few-shot settings on GLoRE datasets.",
            "model_size": null,
            "reasoning_task_name": "GLoRE (MRC, NLI, TF) — e.g., HELP (monotonicity), NaN-NLI, ConTRoL, etc.",
            "reasoning_task_description": "Evaluation focused on entailment and monotonicity-sensitive NLI tasks and multi-choice/TF tasks to probe structured logical inference.",
            "method_or_approach": "Instruction-following zero-shot and few-shot evaluation; described as reasoning-enhanced by OpenAI system cards (cited), no further bespoke method details in this paper.",
            "performance": "Noted to have strong NLI performance on certain datasets (e.g., HELP 63.69% reported as better than QwQ-32B's 61.53%); overall aggregate less than top open reasoning models (exact average not listed in main text but reported below QwQ-32B).",
            "baseline_comparison": "Outperforms some LLMs and matches or exceeds QwQ-32B on specific fine-grained NLI tasks (e.g., HELP), but is behind QwQ-32B on aggregate benchmark performance.",
            "limitations_or_failures": "Performance uneven across datasets; like other LLMs, shows distributional sensitivity and lacks fully generalizable logical inference.",
            "insights_or_conclusions": "Specialized reasoning-focused variants can improve on particular logical phenomena (e.g., monotonicity), but improvements are not uniformly general across all logical reasoning tasks.",
            "uuid": "e8614.6",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DeepSeek R1",
            "name_full": "DeepSeek R1",
            "brief_description": "A reasoning-enhanced open model trained with reinforcement learning incentives to improve logical reasoning; achieves strong GLoRE performance nearing top open-source results.",
            "citation_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "mention_or_use": "use",
            "model_name": "DeepSeek R1",
            "model_description": "An open reasoning-enhanced model described as using reinforcement learning to incentivize reasoning capability (cited Deepseek-Ai work); evaluated across GLoRE tasks and exhibits high average accuracy.",
            "model_size": null,
            "reasoning_task_name": "GLoRE (MRC, NLI, TF) — e.g., ReClor, AR-LSAT, ProofWriter, FraCaS, ConTRoL, etc.",
            "reasoning_task_description": "Benchmarks requiring multi-step verbal logic (MRC), entailment analysis (NLI), and true/false proof reasoning requiring explicit rule application.",
            "method_or_approach": "Described as using reinforcement learning incentives to improve reasoning capabilities (per cited Deepseek-Ai work); evaluated zero-shot and few-shot in-context learning where applicable.",
            "performance": "High aggregate performance: reported average ~75.14% across GLoRE; dataset examples include ProofWriter 80.51%, various MRC and NLI scores listed in Table 2 (per-dataset values in paper).",
            "baseline_comparison": "Substantially above RoBERTa and many standard instruction-tuned LLMs; slightly below QwQ-32B on overall average (75.14% vs 78.95%), and generally competitive with or better than GPT-4 on several tasks.",
            "limitations_or_failures": "Shows dataset sensitivity similar to other LLMs; may underperform on specific NLI phenomena (paper notes unevenness across NLI datasets).",
            "insights_or_conclusions": "Reinforcement learning incentivizing reasoning yields strong practical gains across logical reasoning benchmarks, but generalization across different logical phenomena (e.g., monotonicity, negation) is still imperfect.",
            "uuid": "e8614.7",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "QwQ-32B",
            "name_full": "QwQ-32B",
            "brief_description": "A 32B-parameter reasoning-enhanced open model leveraging reinforcement learning and specialized training; achieves state-of-the-art results on GLoRE, particularly on multi-choice MRC and TF tasks.",
            "citation_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "mention_or_use": "use",
            "model_name": "QwQ-32B",
            "model_description": "A reasoning-enhanced open LLM (32B parameters) described as leveraging reinforcement learning and specialized training methodology; evaluated extensively on the GLoRE benchmark where it attains the highest reported average accuracy.",
            "model_size": "32B",
            "reasoning_task_name": "GLoRE (MRC, NLI, TF) — including ReClor, AR-LSAT, LogiQA, LogiQA22, ProofWriter, FraCaS, NLI datasets (HELP, ConTRoL, NaN-NLI), etc.",
            "reasoning_task_description": "A comprehensive suite testing strict logical reasoning: multi-step verbal deduction in MRC, entailment and monotonicity in NLI, and synthetic proof/true-false reasoning demanding multi-premise rule application.",
            "method_or_approach": "Reasoning-specialized training (described as reinforcement learning based in paper and cited QwQ team work), instruction-following, evaluated in zero-shot and few-shot ICL settings.",
            "performance": "Top aggregate performance on GLoRE with average accuracy 78.95%; exemplary results include ReClor 93.76%, AR-LSAT 92.35%, ProofWriter 82.40%; robust on LogiQA22 (86.30%) outperforming GPT-4 by ~27.8 points on that split.",
            "baseline_comparison": "Outperforms GPT-4 (66.34% average) and DeepSeek R1 (75.14%) on overall GLoRE average and sets state-of-the-art on several MRC and TF datasets; however, lags on some NLI datasets (e.g., HELP 61.53% behind o1 mini).",
            "limitations_or_failures": "Uneven performance across NLI tasks—weaker on monotonicity/negation-sensitive datasets like HELP; indicates specialization favors certain reasoning formats (MRC/TF) over fine-grained entailment reasoning.",
            "insights_or_conclusions": "Targeted architectural and RL-based training innovations can markedly improve performance on strict logical reasoning benchmarks, but gains may be format-specific; more diverse evaluation is necessary to assess general logical inference.",
            "uuid": "e8614.8",
            "source_info": {
                "paper_title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logiqa: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        },
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "Not another negation benchmark: The NaN-NLI test suite for sub-clausal negation",
            "rating": 2,
            "sanitized_title": "not_another_negation_benchmark_the_nannli_test_suite_for_subclausal_negation"
        },
        {
            "paper_title": "Help: A dataset for identifying shortcomings of neural models in monotonicity reasoning",
            "rating": 2,
            "sanitized_title": "help_a_dataset_for_identifying_shortcomings_of_neural_models_in_monotonicity_reasoning"
        },
        {
            "paper_title": "Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekai_deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Qwq-32b: Embracing the power of reinforcement learning",
            "rating": 2,
            "sanitized_title": "qwq32b_embracing_the_power_of_reinforcement_learning"
        }
    ],
    "cost": 0.016218499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GLoRE: Evaluating Logical Reasoning of Large Language Models
20 Apr 2025</p>
<p>Hanmeng Liu 
Hainan University
HaikouHainan</p>
<p>Zhiyang Teng 
ByteDance SG</p>
<p>Ruoxi Ning 
Westlake University
HangzhouZhejiangChina</p>
<p>Yiran Ding 
Westlake University
HangzhouZhejiangChina</p>
<p>Xiulai Li 
Hainan University
HaikouHainan</p>
<p>Xiaozhang Liu 
Hainan University
HaikouHainan</p>
<p>Yue Zhang 
Westlake University
HangzhouZhejiangChina</p>
<p>GLoRE: Evaluating Logical Reasoning of Large Language Models
20 Apr 20252E53C0DD83F28EA3B1EBC449B1AC2C90arXiv:2310.09107v2[cs.CL]Large Language ModelLarge Reasoning ModelLogical reasoningNatural Language Inference
Large language models (LLMs) have shown significant general language understanding abilities.However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding.To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios.Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date.GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.It garnered over 300 citations upon its release.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) [50,67], especially reasoning language models [18,51] demonstrate advanced capabilities in complex reasoning tasks and show significant adaptability and versatility across various applications, from simple everyday tasks to specialized domains such as coding, mathematics, law, medicine, and finance [11,22,34,37,76].Quantitative evaluation of LLM reasoning has thus become a very important task.To this end, existing work has considered mathematical reasoning [15,26], algorithmic problem solving [9,58], and knowledge-driven reasoning [25,73].</p>
<p>Logical reasoning is a cornerstone of human intelligence and has been a central focus in artificial intelligence research since its inception [16,29,33].However, evaluating verbal reasoning turned out to be too difficult in the 1950s due to insufficient natural language understanding (NLU) technologies, and thus AI researchers focused on formal logical reasoning instead [29,48,49].Since the 2010s, NLU has witnessed huge advances, where reading comprehension [8,21] and natural language inference [7,74] tasks were solved with high accuracies, which made verbal reasoning evaluation feasible [43,80].Figure 1 illustrates an example of logical reasoning in reading comprehension.To address such questions, LLMs must engage in multi-step, algorithmic, symbolic reasoning.This makes logical reasoning an ideal testbed for evaluating LLMs' ability to process complex natural language information accurately, robustly, and logically.</p>
<p>To this end, we introduce the General Logical Reasoning Evaluation (GLoRE) benchmark, designed to assess instruction-tuned LLMs on various logical reasoning tasks.GLoRE evaluates the strengths and limitations of LLMs in this domain, similar to how GLUE [71] and Super-GLUE [70] benchmark natural language understanding.GLoRE includes three types of logical reasoning tasks: Multi-choice Reading Comprehension [35], Natural Language Inference (NLI) [17], and True-or-False (Yes-or-No) Questions [13].These tasks encompass a wide range of logical reasoning phenomena, with high-quality datasets that remain challenging for pre-trained language models [13,27,32].In total, GLoRE covers 12 datasets with 72,848 instances.Since its release in 2023, GLoRE has been used for evaluating language models, receiving over 300 citations on ArXiv.</p>
<p>Using GLoRE, we report the logical reasoning capabilities of commercial models such as GPT-4 and OpenAI o1 [51], as well as popular open-source models such as LLaMA [67], Falcon [1], Mistral [30], DeepSeek R1 [18], and QwQ-32B [66].We test their instruction-following and problem-solving abilities in logical reasoning tasks.Results show that while commercial LLMs like GPT-4 still excel in zero-shot settings and approach human performance on specific datasets like ReClor, open-source models like QwQ-32B now rival or even surpass commercial counterparts in key tasks, achieving state-of-the-art results on multiple benchmarks.This underscores rapid advancements in open-source LLMs, narrowing the performance gap with commercial models.However, performance varies significantly across datasets, indicating sensitivity to data distribution.This sensitivity is further confirmed by observations that in-context learning and supervised fine-tuning primarily enhance LLM performance on specific test distributions, demonstrating their strong learning ability.While LLMs show promise in logical reasoning, their robustness to data distribution variations remains a challenge, highlighting the need for further improvement.</p>
<p>Related Work</p>
<p>Logical reasoning with natural language.Tapping into logical reasoning capabilities represents a holistic endeavour in natural language understanding (NLU).A variety of methods have been explored to realize this objective, including symbolic systems [45,47,55], fine-tuning of language models [28,41,71,78], and hybrid approaches combining neural and symbolic elements [36,59,60].</p>
<p>The recent introduction of evaluation datasets, notably LogiQA [43] and Reclor [80], has reinvigorated the focus on logical reasoning in NLP research.Logical reasoning is now leveraged in numerous probing tasks over large Pretrained Language Models (PLMs) and applied to downstream tasks such as question-answering and dialogue systems [6,63].Despite these advancements, the aspiration to emulate human-like logical reasoning capabilities within NLU systems remains a significant challenge for traditional models [27,43].In this study, our goal is not only to quantitatively evaluate the capability of Large Language Models (LLMs) in addressing the previously mentioned challenge but also to underscore the significance of our work in providing a validated platform for enhancing various reasoning methods with our data.</p>
<p>LLM reasoning evaluation.Despite progress in evaluating LLMs for specific reasoning tasks like arithmetic [57] and commonsense [4], a yawning gap exists in comprehensively assessing their logical reasoning.While LLMs excel at specific tasks like arithmetic reasoning [57], they face challenges in complex areas like multi-step reasoning [23] and abstract scenarios [24].ChatGPT exhibits strengths in chat-specific reasoning and some commonsense domains [4,53], but struggles with tasks requiring longer chains of inference [4].Other LLMs like FLAN-T5 [12], LLaMA [67], and PaLM [2] show potential in general deductive reasoning [61], while InstructGPT and Codex excel in specialized domains like medical reasoning [38].Despite these advances, limitations in data bias [52], and complex reasoning tasks necessitate further research and optimization to fully unlock the reasoning potential of LLMs [77].</p>
<p>Big-Bench Hard (BBH) [64] isolates 23 most challenging tasks from BIG-Bench [3].These tasks comprise general language understanding, arithmetic and algorithmic reasoning, and logical deduction.However, in comparison to our benchmark, the data size of the logical reasoning section in BBH is very small.HumanEval [10] serves as a hand-written evaluation set for coding.The programming problems included are designed to assess language comprehension, reasoning, algorithms, and simple mathematics.While similar to logical reasoning in that code generation necessitates complex reasoning skills, GLoRE differs in presenting logical reasoning problems via natural language prompts.ARB [62] is a benchmark for advanced reasoning over multiple fields like mathematics, physics, biology, chemistry, and law.Similar to GLoRE, it introduces a challenging subset of math and physics problems that require advanced symbolic reasoning.However, the benchmark constraints its problem on the above subjects with domain knowledge, not general logical reasoning questions, which is the focus of GLoRE.</p>
<p>The GLoRE Dataset</p>
<p>As mentioned in the introduction, GLoRE contains three NLU tasks: Multichoice Reading Comprehension, NLI, and Yes-or-No.First, Multi-choice reading comprehension [35] is essential in verbal reasoning tests, which cover abundant high-quality logical reasoning problems in the wild.Second, Unlike multi-choice reading comprehension, NLI [17] is more general and centric on entailment relations in a simpler task format, which is a fundamental task for evaluating reasoning abilities [19,54].Third, the Yes-or-No reasoning task [13] is a combination of question-answering and textual entailment, which can serve as a playground for testing models' reasoning abilities [14,65].The data statistics are shown in Table 1.</p>
<p>Multi-choice Reading Comprehension (MRC)</p>
<p>Within the standard multi-choice reading comprehension (MRC) task setting, a system is presented with a passage and a question, and the objective is to choose the most suitable answer from a set of candidate responses.Particularly, GLoRE contains five such datasets: LogiQA [43] is a logical MRC dataset derived from the Chinese Civil Service Examination, translated into English, and made available in both Chinese and English versions.We adopt LogiQA 2.0 [40] and use both the English (LogiQA 2.0) and Chinese (LogiQA 2.0 zh) test sets for our evaluation.</p>
<p>ReClor [80] comprises question-answering examples from the LSAT exams designed to assess human logical reasoning abilities.We use the development set for our testing as the test set does not provide gold labels.AR-LSAT [72] is a dataset of analytical reasoning questions from the Law School Admission Test.Each question contains five options rather than four.</p>
<p>LogiQA22 is collected and processed according to the LogiQA 2.0 format after ChatGPT was released.It incorporates the newly released Chinese Civil Servant Exams from 2022, which are not included in the original LogiQA dataset.</p>
<p>Natural Language Inference (NLI)</p>
<p>NLI is the task of determining the logical relationship between a hypothesis and a premise.The typical scheme involves text classification, where the model selects one of three labels: entailment, contradiction, and neutral.ConTRoL [39] is an NLI dataset that offers an in-depth examination of contextual reasoning within the NLI framework.Approximately 36.2% of premisehypothesis pairs fall under the category of logical reasoning in this dataset.We choose the logical reasoning portion for our evaluation.HELP [79] is an NLI dataset emphasizing monotonicity reasoning, a crucial concept in Natural Logic [46].We use the training set for our evaluation.TaxiNLI [31] is an NLI dataset that has been re-annotated based on MNLI [75], with categories include logical categories such as connectives, mathematical reasoning, and deduction.NaN-NLI [68] is a test suite designed to probe the capabilities of NLP models in capturing sub-clausal negation.The successful handling of sub-clausal negation can be seen as a strong indicator of a model's logical reasoning capacity.</p>
<p>True-or-False (Yes-or-No) Questions (TF)</p>
<p>FraCaS test suite [56] presents complex entailment problems involving multipremised contexts as a three-way classification task.The ability to determine entailment relationships in this context is closely tied to logical reasoning.RuleTaker [14] dataset is a synthetic creation designed to examine the reasoning ability of transformer models [69] over natural language rules.This task explicitly targets logical reasoning by asking models to reason over a set of rules and facts to generate true-or-false responses as output.ProofWriter [65] dataset generates sets of facts and rules, each followed by questions, which can be proven true or false using proofs of various depths.</p>
<p>Experiments</p>
<p>We employ GLoRE to assess the logical reasoning capabilities across different categories of language models, including traditional pre-trained models and reasoning-enhanced LLMs, both open-source and proprietary.We conduct a comprehensive comparative analysis of their performance against human benchmarks.</p>
<p>Experimental Settings</p>
<p>We adopted RoBERTa-base [44] as a baseline, fine-tuning it on the training set over five epochs for each dataset.The community models selected for comparison include Falcon-40b-instruct [1], LLaMA-30b-supercot [67] Mixtral-8x7b, DeepSeek R1 [18] and QwQ-32B [66].For OpenAI models, we choose ChatGPT, GPT-4 and o1 mini [51].</p>
<p>Our evaluation metrics consisted of classification accuracy scores.Additionally, we utilized reported accuracies for datasets where human performance data was available and recorded both the average and peak performance of human participants to establish a human baseline.For the LogiQA22 dataset, we engaged five co-authors as test subjects and computed their accuracy based on 150 test examples.</p>
<p>Main Results</p>
<p>Zero-shot results.Table 2 summarizes the zero-shot evaluation results.The first block shows human performance.The second block presents RoBERTabase's supervised fine-tuning results.With 125M parameters, RoBERTa-base achieves 48.76% and 33.22% accuracy on LogiQA 2.0 and LogiQA22, respectively, lagging behind human performance.It performs better on NLI and TF tasks than MRC tasks, likely due to output ambiguities.On NaN-NLI, RoBERTa achieves 90.02% accuracy, matching human performance, possibly due to learning superficial patterns from rule-based negation data.On ProofWriter, RoBERTabase scores 55.92%, indicating potential for specific logical reasoning tasks.</p>
<p>The third block shows zero-shot results for LLaMA, Falcon, and Mixtral.LLaMA and Falcon perform similarly (32.34% vs. 32.28%),suggesting comparable reasoning capabilities despite LLaMA-30B's smaller size.Both underperform RoBERTa-base on most tasks, except Falcon on RT.On MRC tasks, their accuracy is around 20%, worse than random guessing in 4-way classification, indicating challenges in logical reasoning without in-context demonstrations.</p>
<p>Performance gaps between LogiQA and LogiQA22 are smaller for these models, suggesting stable performance across data distributions without in-domain tuning.Mixtral-8x7b outperforms LLaMA and Falcon, demonstrating the effectiveness of mixture-of-expert models.</p>
<p>The fourth block provides zero-shot results ChatGPT and GPT-4.Both models, especially GPT-4, surpass RoBERTa-base on several MRC benchmarks.However, GPT-4's accuracy drops significantly on LogiQA22 (58.49% vs. 72.25% on LogiQA 2.0), indicating sensitivity to data distribution.In NLI and TF tasks, ChatGPT and GPT-4 outperform RoBERTa, with ChatGPT achieving 58.45% accuracy on ConTRoL, surpassing GPT-4.GPT-4's NLI performance varies across datasets, further highlighting its sensitivity to data distribution.TF task results show similar inconsistencies, suggesting model rationales differ from human reasoning.</p>
<p>The final block shows results for o1 mini, DeepSeek R1, and QwQ-32B, which achieve notable improvements over prior models.QwQ-32B attains the highest average accuracy (78.95%), surpassing GPT-4 (66.34%) and DeepSeek R1 (75.14%).It achieves state-of-the-art results on MRC tasks like ReClor (93.76%) and AR-LSAT (92.35%), indicating the need for more challenging benchmarks for logical reasoning.Its robustness is evident in LogiQA22 (86.30%), outperforming GPT-4 by 27.81 percentage points.However, QwQ-32B shows uneven performance on NLI datasets, such as HELP (61.53%, lagging behind o1 mini's 63.69%), suggesting its reasoning capabilities are less generalizable in tasks requiring fine-grained entailment analysis.</p>
<p>While GPT-4 retains an advantage on FraCas (75.35%),QwQ-32B surpasses GPT-4 on ReClor (93.76% vs. 87.20%),redefining state-of-the-art performance for MRC tasks.QwQ-32B and DeepSeek R1 showcase balanced performance across most tasks, with QwQ-32B achieving unprecedented TF results (e.g., 82.40% on ProofWriter, outperforming both GPT-4's 59.66% and DeepSeek R1's 80.51%).Though still below the human average overall, these models mark substantial progress -QwQ-32B's 78.95% average accuracy (vs.DeepSeek R1's 75.14% and GPT-4's 66.34%) highlights significant architectural or training innovations for logical inference.</p>
<p>Few-shot results.LLMs excel at in-context learning [20], where performance improves with context examples and demonstration methods [42].For this study, we randomly sampled instances (1 for 1-shot, 2 for 2-shot, and 5 for 5-shot) from each dataset and appended them to the prompt.We used the same model configuration as in the zero-shot scenario.Table 3 highlights the impact of in-context learning (ICL), as seen in GPT-4's 9% accuracy gain with 5-shot learning.However, this improvement stems from statistical adaptation rather than true reasoning, as models rely on superficial patterns rather than humanlike logical inference.This aligns with findings that chain-of-thought prompts correlate with outputs but do not causally drive reasoning [5].While reasoningenhanced models narrow the gap with human performance, their sensitivity to data distribution highlights the need for further research into more robust reasoning mechanisms.GLoRE's evolving framework will continue to track these advancements.</p>
<p>Analysis</p>
<p>Large language models vs. reasoning-enhanced models.The reasoningenhanced models like QwQ-32B, DeepSeek R1, and OpenAI's o1 mini demonstrate significant improvements over traditional LLMs.QwQ-32B, in particular, achieves the highest average performance (78.95%), indicating that its reinforcement learning framework or specialized training methodology enables better generalization across data distributions.While QwQ-32B dominates MRC and TF tasks, its relatively lower performance on NLI datasets like HELP (61.53%) suggests that even state-of-the-art models struggle with tasks requiring monotonicity or negation reasoning, highlighting the need for broader evaluation beyond task-specific robustness.Data leakage concerns.While GLoRE includes diverse datasets, potential data leakage risks arise from overlapping sources.GPT-4's lower accuracy on LogiQA22 (58.49%) compared to LogiQA 2.0 (72.25%) suggests limited exposure to newer data, reducing leakage concerns but highlighting distributional sensitivity.The benchmark's dynamic updates and inclusion of newly annotated datasets help mitigate leakage by testing models on unseen distributions.Sensitivity to data distribution.The above experiments show that the performance of LLMs is sensitive to the data distribution.Even though the underlying reasoning principles are the same, LLM performance varies significantly across datasets.This suggests that LLMs might not reason using the correct rationale but rely on superficial features.As shown in Table 2, although GPT-4 achieves near-human performance on datasets like ReClor (87.20%) and NaN-NLI (75.74%), it lags significantly on others (e.g., HELP at 46.01%).This inconsistency mirrors the behavior of reasoning-enhanced models like DeepSeek R1, revealing a critical divergence from human reasoning: once humans master a reasoning pattern, their performance generalizes robustly, whereas LLMs remain sensitive to data-specific features.</p>
<p>Conclusion</p>
<p>We constructed GLoRE, a dynamic and comprehensive benchmark tailored for assessing the logical reasoning capabilities of advanced language models, including GPT-4 and various strong open-source LLMs across multiple reasoning tasks.</p>
<p>Our findings indicate that QwQ-32B, a reasoning-enhanced model, sets a new state-of-the-art on the GLoRE benchmark, significantly narrowing the gap to human performance.This underscores the potential of targeted architectural and training innovations for logical reasoning.GLoRE will be continually maintained to track advancements in this rapidly evolving domain.</p>
<p>Fig. 1 .
1
Fig. 1.Instruction and question format for logical reading comprehension tasks.</p>
<p>Table 1 .
1
Data statistics.("E": entailment; "C": contradiction; "N": neutral.)
DatasetSizeTargetDatasetSizeTargetLogiQA 2.0 test1,572 4-way multi-choice ConTRoL805E, C, NLogiQA 2.0 zh test 1,594 4-way multi-choice HELP35,891E, C, NReClor dev500 4-way multi-choice TaxiNLI test10,071E, C, NAR-LSAT test230 5-way multi-choice NaN-NLI259E, C, NLogiQA221,354 4-way multi-choice FraCas346 Yes, No, NeutralRuleTaker dev 10,068Yes, NoProofWriter dev 10,158Yes, No</p>
<p>Table 3 .
3
Average accuracies on GLoRE few-shot evaluation.
Model0-shot 1-shot 2-shot 5-shotLLaMA32.34 32.89 35.03 39.62Falcon32.28 33.14 33.76 35.72ChatGPT 52.10 55.85 57.43 60.32GPT-466.34 70.31 71.44 75.83</p>
<p>. Mrc Nli Task, Avg Tf, Lq Lq Zh Rc Al Lq22 Ct Hl Tn Nn Fc Rt Dataset, Pw Human, Avg, 86.00 88.00 63.00 56.00 83.00 87.00 81.00 97.00 94.00 92.00 84.00 82.00 82.75</p>
<p>Human Ceiling. 95.00 96.00 100.00 91.00 99.00 94.00 95.00 100.00 100.00 97.00 95.00 93.00 96.25</p>
<p>. Deepseek, R1 76.22 81.49 77.88 90.01 71.63 78.37 62.05 75.74 72.58 59.96 75.29 80.51 75.14</p>
<p>All results are in %, the best ones are in bold, and the second best ones are in underline. E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, E Goffinet, D Heslow, J Launay, Q Malartic, B Noune, B Pannier, LQ: LogiQA 2.0, RC : Re-Clor, AL: AR-LSAT, CT : ConTRoL, HL: HELP, TN : TaxiNLI, NN : NaN-NLI, FC : FraCas, RT : RuleTaker, PW : ProofWriter. Penedo, G.2023Table 2. LLMs' performance on the GLoRE benchmark. Falcon-40B: an open large language model with state-of-the-art performance</p>
<p>R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, Palm 2 technical report. 2023</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. B Bench Authors, 2023TMLR</p>
<p>Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>How likely do llms with cot mimic human reasoning?. G Bao, H Zhang, C Wang, L Yang, Y Zhang, arXiv:2402.160482024arXiv preprint</p>
<p>Logical reasoning for task oriented dialogue systems. S Beygi, M Fazel-Zarandi, A Cervone, P Krishnan, S R Jonnalagadda, 2022</p>
<p>A large annotated corpus for learning natural language inference. S R Bowman, G Angeli, C Potts, C D Manning, Proc. of EMNLP. of EMNLP2015</p>
<p>A thorough examination of the CNN/daily mail reading comprehension task. D Chen, J Bolton, C D Manning, ACL. 2016</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, Others: Evaluating large language models trained on code. 2021</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, Others: Evaluating large language models trained on code. 2021</p>
<p>Chatgpt goes to law school. J H Choi, K E Hickman, A Monahan, D Schwarcz, Available at SSRN. 2023</p>
<p>H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, Y Li, X Wang, M Dehghani, S Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. C Clark, K Lee, M W Chang, T Kwiatkowski, M Collins, K Toutanova, 2019</p>
<p>Transformers as soft reasoners over language. P Clark, O Tafjord, K Richardson, Proc. of IJCAI. of IJCAI2020</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>M J Cresswell, Logics and languages. Routledge19731st ed.</p>
<p>The pascal recognising textual entailment challenge. I Dagan, O Glickman, B Magnini, 2005MLCW</p>
<p>Deepseek-Ai , Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025</p>
<p>Transforming question answering datasets into natural language inference datasets. D Demszky, K Guu, P Liang, 2018</p>
<p>Q Dong, L Li, D Dai, C Zheng, Z Wu, B Chang, X Sun, J Xu, L Li, Z Sui, A survey on in-context learning. 2023</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, Proc. of AACL. of AACL2019</p>
<p>. S Frieder, L Pinchetti, R R Griffiths, T Salvatori, T Lukasiewicz, P C Petersen, A Chevalier, J Berner, 2023Mathematical capabilities of chatgpt</p>
<p>Chain-of-thought hub: A continuous effort to measure large models' reasoning performance. Y Fu, L Ou, M Chen, Y Wan, H Peng, T Khot, arXiv:2305.173062023arXiv preprint</p>
<p>Large language models are not abstract reasoners. G Gendron, Q Bao, M Witbrock, G Dobbie, arXiv:2305.195552023arXiv preprint</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR. the International Conference on Learning Representations (ICLR2021</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>J Huang, K C C Chang, Towards reasoning in large language models: A survey. 2023</p>
<p>Y Huang, M Fang, Y Cao, L Wang, X Liang, arXiv:2103.14349Dagn: Discourse-aware graph network for logical reasoning. 2021arXiv preprint</p>
<p>L Iwańska, Logical reasoning in natural language: It is all about knowledge. Minds and Machines. 1993</p>
<p>. A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, Others, 2024Mixtral of experts</p>
<p>Taxinli: Taking a ride up the NLU hill. P Joshi, S Aditya, A Sathe, M Choudhury, 2020CoRR</p>
<p>ContractNLI: A dataset for document-level natural language inference for contracts. Y Koreeda, C Manning, Proc. of EMNLP Findings. of EMNLP Findings2021</p>
<p>Logic for problem solving. R Kowalski, 1979Ediciones Díaz de Santos</p>
<p>Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. T H Kung, M Cheatham, A Medenilla, C Sillos, L De Leon, C Elepaño, M Madriaga, R Aggabao, G Diaz-Candido, J Maningo, 2023e0000198</p>
<p>RACE: Large-scale Reading Comprehension dataset from Examinations. G Lai, Q Xie, H Liu, Y Yang, E Hovy, EMNLP. 2017</p>
<p>Augmenting neural networks with first-order logic. T Li, V Srikumar, Proc. of ACL. of ACL2019</p>
<p>Competitionlevel code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, Others, 2022</p>
<p>Can large language models reason about medical questions?. V Liévin, C E Hother, O Winther, arXiv:2207.081432022arXiv preprint</p>
<p>Natural language inference in context -investigating contextual reasoning over long texts. H Liu, L Cui, J Liu, Y Zhang, 2020CoRR</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. H Liu, J Liu, L Cui, Z Teng, N Duan, M Zhou, Y Zhang, Speech, and Language Processing. 2023</p>
<p>Logicot: Logical chainof-thought instruction tuning. H Liu, Z Teng, L Cui, C Zhang, Q Zhou, Y Zhang, Proc. of EMNLP Findings. of EMNLP Findings2023</p>
<p>. J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, What makes good in-context examples for gpt-3? (2021</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. J Liu, L Cui, H Liu, D Huang, Wang, Y Zhang, 2020CoRR</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXivRoberta: A robustly optimized bert pretraining approach. 2019</p>
<p>Natural logic for textual inference. B Maccartney, C D Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing2007</p>
<p>Natural logic for textual inference. B Maccartney, C D Manning, Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing2007</p>
<p>Programs with common sense. J Mccarthy, 2002</p>
<p>Some philosophical problems from the standpoint of artificial intelligence. J Mccarthy, P J Hayes, Machine Intelligence. 41969</p>
<p>The logic theory machine-a complex information processing system. A Newell, H Simon, IRE Transactions on Information Theory. 1956</p>
<p>OpenAI: Gpt-4 technical report. 2023</p>
<p>Openai, Openai o1 system card. 2024</p>
<p>Human-like problem-solving abilities in large language models using chatgpt. G Orrù, A Piarulli, C Conversano, A Gemignani, Frontiers in Artificial Intelligence. 11993502023</p>
<p>S Ott, K Hebenstreit, V Liévin, C E Hother, M Moradi, M Mayrhauser, R Praas, O Winther, M Samwald, arXiv:2301.11596Thoughtsource: A central hub for large language model reasoning data. 2023arXiv preprint</p>
<p>Collecting diverse natural language inference problems for sentence representation evaluation. A Poliak, A Haldar, R Rudinger, J E Hu, E Pavlick, A S White, B Van Durme, Proc. of EMNLP. of EMNLP2018</p>
<p>Theorist: A Logical Reasoning System for Defaults and Diagnosis. D Poole, R Goebel, R Aleliunas, 1987</p>
<p>Using the framework. S G Pulman, 1996</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. C Qin, A Zhang, Z Zhang, J Chen, M Yasunaga, D Yang, 2023</p>
<p>S Quan, J Yang, B Yu, B Zheng, D Liu, A Yang, X Ren, B Gao, Y Miao, Y Feng, arXiv:2501.01257Codeelo: Benchmarking competition-level code generation of llms with human-comparable elo ratings. 2025arXiv preprint</p>
<p>Prover: Proof generation for interpretable reasoning over rules. S Saha, S Ghosh, S Srivastava, M Bansal, 2020</p>
<p>S Sanyal, H Singh, X Ren, Fairr: Faithful and robust deductive reasoning over natural language. 2022</p>
<p>A Saparov, R Y Pang, V Padmakumar, N Joshi, S M Kazemi, N Kim, H He, arXiv:2305.15269Testing the general deductive reasoning capacity of large language models using ood examples. 2023arXiv preprint</p>
<p>T Sawada, D Paleka, A Havrilla, P Tadepalli, P Vidas, A Kranias, J J Nay, K Gupta, A Komatsuzaki, Arb: Advanced reasoning benchmark for large language models. 2023</p>
<p>Neural natural logic inference for interpretable question answering. J Shi, X Ding, L Du, T Liu, B Qin, Proc. of EMNLP. of EMNLP2021</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, J Wei, 2022</p>
<p>O Tafjord, B D Mishra, P Clark, Proofwriter: Generating implications, proofs, and abductive statements over natural language. 2021</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Q Team, 2025</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>Not another negation benchmark: The NaN-NLI test suite for sub-clausal negation. T H Truong, Y Otmakhova, T Baldwin, T Cohn, J H Lau, K Verspoor, Proc. of AACL. of AACL2022</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 2017CoRR</p>
<p>A Wang, Y Pruksachatkun, N Nangia, A Singh, J Michael, F Hill, O Levy, S R Bowman, SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. 2019</p>
<p>GLUE: A multitask benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP2018</p>
<p>From lsat: The progress and challenges of complex reasoning. S Wang, Z Liu, W Zhong, M Zhou, Z Wei, Z Chen, N Duan, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 2022</p>
<p>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Y Wang, X Ma, G Zhang, Y Ni, A Chandra, S Guo, W Ren, A Arulraj, X He, Z Jiang, Proc. of NeurIPS. of NeurIPS2024</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Proc. of AACL. of AACL2018</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Proc. of NAACL. of NAACL2018</p>
<p>S Wu, O Irsoy, S Lu, V Dabravolski, M Dredze, S Gehrmann, P Kambadur, D Rosenberg, G Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Z Wu, L Qiu, A Ross, E Akyürek, B Chen, B Wang, N Kim, J Andreas, Y Kim, arXiv:2307.024772023arXiv preprint</p>
<p>Logiformer. F Xu, J Liu, Q Lin, Y Pan, L Zhang, Proc. of SIGIR. of SIGIR2022</p>
<p>Help: A dataset for identifying shortcomings of neural models in monotonicity reasoning. H Yanaka, K Mineshima, D Bekki, K Inui, S Sekine, J Bos, Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (<em>SEM2019). the Eighth Joint Conference on Lexical and Computational Semantics (</em>SEM2019)2019</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. W Yu, Z Jiang, Y Dong, J Feng, Proc. of ICLR. of ICLR2020</p>            </div>
        </div>

    </div>
</body>
</html>
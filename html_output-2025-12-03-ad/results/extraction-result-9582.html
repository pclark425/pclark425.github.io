<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9582 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9582</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9582</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-48f5312d6640a076e9a30548f8cf016dcac18d9f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/48f5312d6640a076e9a30548f8cf016dcac18d9f" target="_blank">A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper reviews the state-of-the-art applications of LLMs in the biomedical domain, exploring their effectiveness in automating complex tasks such as evidence synthesis and data extraction from a biomedical corpus of documents.</p>
                <p><strong>Paper Abstract:</strong> The rapid advancement of large language models (LLMs) has opened new boundaries in the extraction and synthesis of medical knowledge, particularly within evidence synthesis. This paper reviews the state-of-the-art applications of LLMs in the biomedical domain, exploring their effectiveness in automating complex tasks such as evidence synthesis and data extraction from a biomedical corpus of documents. While LLMs demonstrate remarkable potential, significant challenges remain, including issues related to hallucinations, contextual understanding, and the ability to generalize across diverse medical tasks. We highlight critical gaps in the current research literature, particularly the need for unified benchmarks to standardize evaluations and ensure reliability in real-world applications. In addition, we propose directions for future research, emphasizing the integration of state-of-the-art techniques such as retrieval-augmented generation (RAG) to enhance LLM performance in evidence synthesis. By addressing these challenges and utilizing the strengths of LLMs, we aim to improve access to medical literature and facilitate meaningful discoveries in healthcare.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9582.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9582.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nedbaylo_LBD_GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Literature-Based Discovery with GPT (Nedbaylo & Hristovski, 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature-based discovery (LBD) approach that uses GPT-3.5 / GPT-4 with a bifurcated prompt-engineering technique to generate disease characteristics and suggest potential interventions from biomedical literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Implementing literature-based discovery (lbd) with chatgpt</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5 / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-based conversational LLMs (GPT-3.5 and GPT-4) used via prompt engineering; no model-size or fine-tuning details reported in the review beyond using bifurcated prompts across separate chat windows.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Biomedical / Literature-based discovery (medicine, hypothesis generation)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Relied on preexisting bibliographic databases (e.g., Medline) and scientific literature in the biomedical domain; not quantified in the review but described as depending on well-established datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Hypothesis generation / associative rules (disease-characteristic → suggested interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>The pipeline generated lists of disease characteristics (without naming the disease) and then suggested interventions mapped to those characteristics; the paper reports hypothesis-like associations rather than formally stated generalized laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Bifurcated prompt engineering: original prompt split into two parts executed in separate chat windows (first: generate disease characteristics; second: suggest interventions based on those characteristics). No fine-tuning; model queried zero-shot via crafted prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative evaluation by the study authors; no quantitative benchmark reported in the review. The review notes evaluation critique regarding depth, novelty, and referencing.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The GPT-based LBD workflow produced candidate hypotheses/associations but often lacked technical depth, novelty, and adequate bibliographic references; method heavily depended on underlying bibliographic coverage (e.g., Medline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>No direct quantitative comparison to human experts or traditional LBD pipelines reported in the review; assessed qualitatively and found lacking in depth and referencing compared to expected scientific standards.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Dependence on preexisting bibliographic databases limits applicability to well-curated domains; outputs often lacked technical depth and proper references; limited novelty in many generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Reported issues with lack of adequate references and insufficient technical depth; potential implicit hallucination due to unsupported suggestions; dependence on training corpus coverage could induce bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9582.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9582.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tao_HIV_GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Pipeline for HIV Drug Resistance Queries (Tao et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated pipeline using GPT-4 to answer 60 specific questions about HIV drug resistance by converting 60 selected papers into markdown and querying the model in different modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 performance on querying scientific publications: reproducibility, accuracy, and impact of an instruction sheet</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-4 used as a zero-shot/controlled QA engine over prepared paper texts (markdown); no additional fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Biomedical / HIV drug resistance (clinical genomics, antiviral susceptibility)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>60 selected scientific papers on HIV drug resistance; texts converted to markdown; methods and results sections retained while introductions and discussions were excluded to focus on empirical content.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Clinical inference rules (statements about mutation → effect on drug susceptibility) and factual assertions extracted to answer specific scientific queries.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Study framed questions that required extracting or inferring whether particular mutations affected susceptibility to antiviral drugs; the paper reports model answers to such query types rather than listing generalized rules.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Automated pipeline: paper conversion to markdown + prompting GPT-4 in two modes — multiple-question mode (all questions simultaneously) and single-question mode (one at a time); experiments with and without an instruction sheet containing specialized knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Quantitative evaluation across 60 questions measuring accuracy, recall, and precision (reported mean accuracy 86.9%, recall 72.5%, precision 87.4% in the review summary).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Overall fairly good mean accuracy (86.9%). The instruction sheet did not improve accuracy; the model struggled on inferential queries requiring implicit information; single-question mode had higher false positive rate than multiple-question mode.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>No explicit human-expert baseline numbers reported in the review summary; evaluation reported as accuracy metrics on the curated question set derived from the 60 papers.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Model struggled with inference and implicit information; instruction sheet was not effectively utilised; mode-dependent biases (higher false positives in single-question mode).</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Bias observed in response behavior across question modes (false positives); model produced unreliable answers for implicit/inferential queries—risk of hallucinated or unsupported statements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9582.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9582.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gartlehner_RCT_Claude2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data Extraction for Evidence Synthesis using Claude 2 (Gartlehner et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proof-of-concept study using Claude 2 to extract 16 pre-specified data elements (study identifiers, participant characteristics, outcomes) from a convenience sample of 10 open-access randomized controlled trials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data extraction for evidence synthesis using a large language model: A proof-of-concept study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Claude 2 LLM used via prompt-engineered extraction pipeline; prompts were iteratively tested and refined. No fine-tuning described; open-access RCTs may overlap with model training data.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Biomedical / Evidence synthesis (randomized controlled trials)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Convenience sample of 10 open-access RCT full texts; selection allowed possibility that texts were in Claude 2 training data (authors note this concern).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Structured data extraction yielding standardized data elements (not explicit 'laws' but generalizable reporting rules: e.g., mapping trial texts → structured PICO-like elements).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Extracted items included study identifiers and participant characteristics; these structured outputs enable consistent capture of study-level facts but the study did not claim derivation of high-level scientific 'laws'.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompt engineering: crafted and iteratively refined prompts to extract the 16 target data elements from each trial text; no model fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Quantitative evaluation against manual extraction: reported accuracy 96.3% and F1-score 98% (as summarized in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High reported extraction accuracy and F1 on this small, open sample; however, concerns about hallucinations (two cases reported) and possible contamination if trials were present in model training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared to manual extraction (implicit baseline); high agreement metrics reported, but the review notes caveats due to sample size and potential training-set overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Small convenience sample (10 trials), potential overlap with training data, and instances of hallucination; generalizability to more diverse or proprietary trials remains untested.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Two hallucinated answers reported; authors flagged risk of bias from training on open-access data and hallucination tendencies when facing unfamiliar or complex content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9582.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9582.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dagdelen_structured_extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Information Extraction with LLMs (Dagdelen et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequence-to-sequence approach that fine-tunes GPT-3 and LLaMa-2 on small annotated datasets (400–650 pairs) to perform joint named-entity recognition and relation extraction in materials-science literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured information extraction from scientific text with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3 and LLaMa-2 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Sequence-to-sequence LLMs (GPT-3, LLaMa-2) fine-tuned on a limited set (400–650 annotated text-extraction pairs) to jointly perform NER and relation extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science literature (scientific text mining)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Materials-science literature with a relatively small annotated training set of 400–650 text-extraction pairs for fine-tuning; broader unlabelled corpus not quantified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Entity–relation extraction enabling structured relations (e.g., material → property / process relations) that can be aggregated into generalizable patterns or rules.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>The model extracts entities and their relations from sentences (e.g., identifying a material and an associated property/process) but the review notes the model struggled to synthesize complete evidence or higher-level generalizations from these relations.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Fine-tuning of sequence-to-sequence LLMs on annotated pairs for joint NER and relation extraction within a seq-to-seq framework.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated on the annotated dataset; specifics of metrics not detailed in the review summary, but overall performance limited by dataset size and relationship complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Models could extract entities and relations but struggled with full evidence synthesis; limitations stemmed from small annotation sets and complexity of relations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>No direct baseline numbers reported in the review summary; performance constrained relative to the complexity of the target extraction/synthesis task.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Small fine-tuning dataset (400–650 pairs) and complexity of relations resulted in incomplete evidence synthesis; scaling and generalizability are open challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Noted difficulty in fully capturing relationships; potential for missed relations or incorrect extractions when data are sparse — risk of erroneous relation outputs if insufficient training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9582.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9582.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Saxena_RoBERTa_RAG_QA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-scale Knowledge Synthesis and Complex IR (Saxena et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid retrieval and QA architecture using RoBERTa and Fusion-in-Decoder to support paragraph retrieval, triplet retrieval from knowledge graphs, and complex multi-hop question answering on the COVID-19 Open Research Dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large-scale knowledge synthesis and complex information retrieval from biomedical documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>RoBERTa (encoder) and Fusion-in-Decoder (generative reader)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Encoder-based RoBERTa for retrieval/reading and a Fusion-in-Decoder generative reader; architecture combines lexical and semantic retrieval, knowledge-graph triplet extraction, and extractive/generative readers.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Biomedical / COVID-19 literature (information retrieval and synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>COVID-19 Open Research Dataset (CORD-19) and related biomedical documents; system indexes paragraphs, retrieves triplets from knowledge graphs, and supports complex multi-hop QA. Corpus size not numerically specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Triplet-based factual relations (subject–relation–object) enabling structured patterns across documents (thematic patterns or factual rules aggregated from triplets).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>System extracts subject–relation–object triplets from the knowledge graph to support faceted queries and synthesize answers across documents rather than explicit generalized scientific 'laws'.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Hybrid retrieval (lexical + semantic) for paragraph retrieval, triplet extraction from knowledge graphs for faceted refinement, and multi-hop Dense Retriever feeding extractive (RoBERTa) and generative (Fusion-in-Decoder) readers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Reported retrieval and ranking metrics: precision 81% and NDCG 72% as summarized in the review's comparison table.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Achieved reasonable retrieval/ranking performance (precision 81%, NDCG 72%); authors reported difficulties handling large amounts of unstructured textual data and generative answer shortcomings in older, smaller LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared retrieval/QA pipeline performance with conventional IR metrics; specific baselines not enumerated in the review summary but metrics indicate competitive retrieval performance for their tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Older/smaller generative models struggled with fluent generative answers; handling large volumes of unstructured medical text remained challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Generative components had difficulty producing reliable free-text answers; risk of erroneous or hallucinated generative responses when relying on smaller LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9582.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9582.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChIP-GPT_LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChIP-GPT (Cinquin, 2024) - LLaMA-based metadata extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned LLaMA model (ChIP-GPT) that extracts and summarizes metadata (ChIP targets and cell lines) from Sequence Read Archive biomedical records, including a summarization step to manage long inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chip-gpt: a managed large language model for robust data extraction from biomedical database records</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>LLaMA (fine-tuned as ChIP-GPT)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>LLaMA architecture fine-tuned on extracted sentences from biomedical Sequence Read Archive records; includes a pre-summarization pipeline to select informative sentences for model input.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Biomedical / ChIP-Seq metadata extraction (genomics experimental metadata)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Sequence Read Archive biomedical database records; longer records were preprocessed via summarization that selects informative sentences and discards irrelevant details to fit model input size constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Metadata extraction mapping (experiment record → structured metadata fields) enabling standardized reporting rules (e.g., mapping reported methods → identified cell lines/targets).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Extracted metadata fields such as chromatin immunoprecipitation targets and associated cell lines from database records (structured outputs rather than high-level scientific laws).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Fine-tuning LLaMA on extracted and summarized sentences from SRA records; a summarization pre-step reduces input length by selecting informative sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared model output to human expert annotations; reported accuracy ~90% in the review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High accuracy (~90%) on the task and competitive with human experts in extraction; however, the summarization pre-step sometimes discarded relevant information, negatively affecting final extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared favorably to human expert extraction on targeted metadata fields, though errors were linked to preprocessing choices.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Summarization step sometimes removed relevant sentences, reducing extraction quality; limitations imposed by input length constraints and preprocessing decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Preprocessing-induced omissions rather than hallucinations were the main issue; potential for incorrect or incomplete extracted metadata when input trimming removes critical context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9582.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9582.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sonnenburg_Curie_BPA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned GPT-3 (Curie) for BPA Risk Assessment (Sonnenburg et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning a GPT-3 family Curie model on 78 publications (results sections) to extract structured data for chemical risk assessment of Bisphenol A (BPA).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Artificial intelligence-based data extraction for next generation risk assessment: Is fine-tuning of a large language model worth the effort?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3 family (Curie) fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>A Curie-class GPT-3 family model was fine-tuned on prompt-completion pairs derived largely from results sections of 78 selected publications; no additional architecture details provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Toxicology / Chemical risk assessment (Bisphenol A)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Dataset of 78 selected publications focusing on results sections to create prompt-completion pairs for fine-tuning and extraction; domain-specific selection targeted toxicological findings.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Toxicological outcome relationships (structured experimental results and associations that can be used to derive evidence patterns for risk assessment).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Model extracted structured experimental results from publications used to populate risk-assessment records; the review reports extraction metrics rather than explicit generalized toxicological rules.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Fine-tuning Curie on prompt-completion pairs derived from results sections of selected papers for supervised extraction of experimental findings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Quantitative evaluation with reported metrics: precision 25%, recall 23%, F1-score 50% (as summarized in the review table).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuned Curie outperformed off-the-shelf models on this narrow extraction task, but overall precision and recall were low, indicating substantial room for improvement, especially for complex experimental designs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared to non-fine-tuned ready-to-use models, fine-tuned Curie performed better on the targeted extraction tasks, but absolute performance was modest.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Difficulties handling studies with complex experimental designs and low precision/recall overall; limited dataset size (78 papers) restricts generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Potential for hallucinated or incorrect structuring when fine-tuned on limited, heterogeneous result sections; low precision and recall suggest noisy or over-generalized outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9582.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9582.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mashatian_RAG_GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retriever-Augmented Generation for Diabetes Care with GPT-4 (Mashatian et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RAG pipeline using GPT-4 to extract and present accurate diabetes and diabetic foot care knowledge to laypersons at an eighth-grade literacy level; used a corpus of 295 articles and evaluated across 175 questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Building trustworthy generative artificial intelligence for diabetes care and limb preservation: A medical knowledge extraction case</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (used within a Retrieval-Augmented Generation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-4 used in a RAG architecture where retrieved context from a 295-article corpus is provided to the model to generate user-friendly, zero-shot answers; no model fine-tuning was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Biomedical / Diabetes care and patient education</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Corpus of 295 diabetes-related articles used as the retrieval context; evaluated on 175 user-level questions spanning diabetes and diabetic foot care topics.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Patient-centered knowledge extraction / layperson-focused rules (actionable care recommendations and factual medical guidance synthesized from the literature).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>The system produced user-friendly recommendations and factual answers about diabetes care based on retrieved literature context; the review reports high task accuracy but no formal generalized scientific rules were enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Retriever-Augmented Generation (RAG): retrieve relevant passages from the article corpus and feed context to GPT-4 to generate zero-shot, literacy-level-adjusted answers via prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluation across 175 questions with reported accuracy metrics (table reports accuracy 98% for a RAG model in the review summary).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High reported accuracy on the limited evaluation set (98% in table), offering promising zero-shot, retrieval-backed generation for layperson education; limited corpus size and evaluation scope noted as constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>No explicit human-expert baseline reported in the review summary; authors highlight that no fine-tuning was required and RAG improved reliability relative to pure generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Evaluation based on a relatively small and static corpus (295 articles); further testing required to assess performance with continuously growing literature and broader question types.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>RAG mitigates some hallucination risk by providing retrieved context, but the review cautions about generalizability and potential for unsupported statements outside the retrieval context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9582.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9582.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Review_ChatGPT4_screening</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT-4 for Screening and Summarization (This review's methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The review authors used ChatGPT-4 to generate summaries and context for each selected study to accelerate screening in their PRISMA-guided systematic review workflow, followed by manual validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>ChatGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>ChatGPT-4 (a GPT-4-based conversational LLM) used interactively to generate study summaries and contextual analyses; no fine-tuning described — used as an assistive summarization tool during screening.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Meta-research / Systematic review screening and summarization across biomedical literature</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>All candidate studies retrieved during the systematic search across ACM DL, IEEE Xplore, Scopus, SOLO (including arXiv), and ScienceDirect; number not explicitly enumerated in the review text.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Document-level summarization and synthesis (not direct law extraction); facilitated distillation of study findings and methodological traits across many scholarly papers.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Generated concise summaries of each candidate study to assist human reviewers; examples of synthesized content included model descriptions, tasks, and key limitations per paper rather than explicit generalizable scientific laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompted ChatGPT-4 to produce summaries for each selected study; outputs were then manually validated by the authors to ensure accuracy and completeness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Manual human validation of the automatically generated summaries to confirm key insights, challenges, and contributions were correctly captured.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using ChatGPT-4 accelerated the screening process and ensured consistency, but manual validation was necessary to correct errors or omissions; process illustrated a practical human-in-the-loop approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared to fully manual screening, the hybrid approach reduced initial review workload while preserving human oversight; no quantitative time-savings reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Automated summaries required manual checking; LLM outputs can miss nuances or misrepresent study details without human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Implicit risk that the LLM could hallucinate or mischaracterize studies; manual validation was used to mitigate these risks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured information extraction from scientific text with large language models <em>(Rating: 2)</em></li>
                <li>Large-scale knowledge synthesis and complex information retrieval from biomedical documents <em>(Rating: 2)</em></li>
                <li>Gpt-4 performance on querying scientific publications: reproducibility, accuracy, and impact of an instruction sheet <em>(Rating: 2)</em></li>
                <li>Data extraction for evidence synthesis using a large language model: A proof-of-concept study <em>(Rating: 2)</em></li>
                <li>Building trustworthy generative artificial intelligence for diabetes care and limb preservation: A medical knowledge extraction case <em>(Rating: 2)</em></li>
                <li>Artificial intelligence-based data extraction for next generation risk assessment: Is fine-tuning of a large language model worth the effort? <em>(Rating: 2)</em></li>
                <li>Implementing literature-based discovery (lbd) with chatgpt <em>(Rating: 2)</em></li>
                <li>Bag of tricks for training data extraction from language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9582",
    "paper_id": "paper-48f5312d6640a076e9a30548f8cf016dcac18d9f",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "Nedbaylo_LBD_GPT",
            "name_full": "Literature-Based Discovery with GPT (Nedbaylo & Hristovski, 2024)",
            "brief_description": "A literature-based discovery (LBD) approach that uses GPT-3.5 / GPT-4 with a bifurcated prompt-engineering technique to generate disease characteristics and suggest potential interventions from biomedical literature.",
            "citation_title": "Implementing literature-based discovery (lbd) with chatgpt",
            "mention_or_use": "use",
            "llm_model_name": "GPT-3.5 / GPT-4",
            "llm_model_description": "GPT-based conversational LLMs (GPT-3.5 and GPT-4) used via prompt engineering; no model-size or fine-tuning details reported in the review beyond using bifurcated prompts across separate chat windows.",
            "application_domain": "Biomedical / Literature-based discovery (medicine, hypothesis generation)",
            "input_corpus_description": "Relied on preexisting bibliographic databases (e.g., Medline) and scientific literature in the biomedical domain; not quantified in the review but described as depending on well-established datasets.",
            "qualitative_law_type": "Hypothesis generation / associative rules (disease-characteristic → suggested interventions)",
            "qualitative_law_example": "The pipeline generated lists of disease characteristics (without naming the disease) and then suggested interventions mapped to those characteristics; the paper reports hypothesis-like associations rather than formally stated generalized laws.",
            "extraction_methodology": "Bifurcated prompt engineering: original prompt split into two parts executed in separate chat windows (first: generate disease characteristics; second: suggest interventions based on those characteristics). No fine-tuning; model queried zero-shot via crafted prompts.",
            "evaluation_method": "Qualitative evaluation by the study authors; no quantitative benchmark reported in the review. The review notes evaluation critique regarding depth, novelty, and referencing.",
            "results_summary": "The GPT-based LBD workflow produced candidate hypotheses/associations but often lacked technical depth, novelty, and adequate bibliographic references; method heavily depended on underlying bibliographic coverage (e.g., Medline).",
            "comparison_to_baseline": "No direct quantitative comparison to human experts or traditional LBD pipelines reported in the review; assessed qualitatively and found lacking in depth and referencing compared to expected scientific standards.",
            "reported_limitations": "Dependence on preexisting bibliographic databases limits applicability to well-curated domains; outputs often lacked technical depth and proper references; limited novelty in many generated hypotheses.",
            "bias_or_hallucination_issues": "Reported issues with lack of adequate references and insufficient technical depth; potential implicit hallucination due to unsupported suggestions; dependence on training corpus coverage could induce bias.",
            "uuid": "e9582.0",
            "source_info": {
                "paper_title": "A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Tao_HIV_GPT4",
            "name_full": "GPT-4 Pipeline for HIV Drug Resistance Queries (Tao et al., 2024)",
            "brief_description": "Automated pipeline using GPT-4 to answer 60 specific questions about HIV drug resistance by converting 60 selected papers into markdown and querying the model in different modes.",
            "citation_title": "Gpt-4 performance on querying scientific publications: reproducibility, accuracy, and impact of an instruction sheet",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4",
            "llm_model_description": "GPT-4 used as a zero-shot/controlled QA engine over prepared paper texts (markdown); no additional fine-tuning reported.",
            "application_domain": "Biomedical / HIV drug resistance (clinical genomics, antiviral susceptibility)",
            "input_corpus_description": "60 selected scientific papers on HIV drug resistance; texts converted to markdown; methods and results sections retained while introductions and discussions were excluded to focus on empirical content.",
            "qualitative_law_type": "Clinical inference rules (statements about mutation → effect on drug susceptibility) and factual assertions extracted to answer specific scientific queries.",
            "qualitative_law_example": "Study framed questions that required extracting or inferring whether particular mutations affected susceptibility to antiviral drugs; the paper reports model answers to such query types rather than listing generalized rules.",
            "extraction_methodology": "Automated pipeline: paper conversion to markdown + prompting GPT-4 in two modes — multiple-question mode (all questions simultaneously) and single-question mode (one at a time); experiments with and without an instruction sheet containing specialized knowledge.",
            "evaluation_method": "Quantitative evaluation across 60 questions measuring accuracy, recall, and precision (reported mean accuracy 86.9%, recall 72.5%, precision 87.4% in the review summary).",
            "results_summary": "Overall fairly good mean accuracy (86.9%). The instruction sheet did not improve accuracy; the model struggled on inferential queries requiring implicit information; single-question mode had higher false positive rate than multiple-question mode.",
            "comparison_to_baseline": "No explicit human-expert baseline numbers reported in the review summary; evaluation reported as accuracy metrics on the curated question set derived from the 60 papers.",
            "reported_limitations": "Model struggled with inference and implicit information; instruction sheet was not effectively utilised; mode-dependent biases (higher false positives in single-question mode).",
            "bias_or_hallucination_issues": "Bias observed in response behavior across question modes (false positives); model produced unreliable answers for implicit/inferential queries—risk of hallucinated or unsupported statements.",
            "uuid": "e9582.1",
            "source_info": {
                "paper_title": "A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Gartlehner_RCT_Claude2",
            "name_full": "Data Extraction for Evidence Synthesis using Claude 2 (Gartlehner et al., 2024)",
            "brief_description": "Proof-of-concept study using Claude 2 to extract 16 pre-specified data elements (study identifiers, participant characteristics, outcomes) from a convenience sample of 10 open-access randomized controlled trials.",
            "citation_title": "Data extraction for evidence synthesis using a large language model: A proof-of-concept study",
            "mention_or_use": "use",
            "llm_model_name": "Claude 2",
            "llm_model_description": "Claude 2 LLM used via prompt-engineered extraction pipeline; prompts were iteratively tested and refined. No fine-tuning described; open-access RCTs may overlap with model training data.",
            "application_domain": "Biomedical / Evidence synthesis (randomized controlled trials)",
            "input_corpus_description": "Convenience sample of 10 open-access RCT full texts; selection allowed possibility that texts were in Claude 2 training data (authors note this concern).",
            "qualitative_law_type": "Structured data extraction yielding standardized data elements (not explicit 'laws' but generalizable reporting rules: e.g., mapping trial texts → structured PICO-like elements).",
            "qualitative_law_example": "Extracted items included study identifiers and participant characteristics; these structured outputs enable consistent capture of study-level facts but the study did not claim derivation of high-level scientific 'laws'.",
            "extraction_methodology": "Prompt engineering: crafted and iteratively refined prompts to extract the 16 target data elements from each trial text; no model fine-tuning reported.",
            "evaluation_method": "Quantitative evaluation against manual extraction: reported accuracy 96.3% and F1-score 98% (as summarized in the review).",
            "results_summary": "High reported extraction accuracy and F1 on this small, open sample; however, concerns about hallucinations (two cases reported) and possible contamination if trials were present in model training data.",
            "comparison_to_baseline": "Compared to manual extraction (implicit baseline); high agreement metrics reported, but the review notes caveats due to sample size and potential training-set overlap.",
            "reported_limitations": "Small convenience sample (10 trials), potential overlap with training data, and instances of hallucination; generalizability to more diverse or proprietary trials remains untested.",
            "bias_or_hallucination_issues": "Two hallucinated answers reported; authors flagged risk of bias from training on open-access data and hallucination tendencies when facing unfamiliar or complex content.",
            "uuid": "e9582.2",
            "source_info": {
                "paper_title": "A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Dagdelen_structured_extraction",
            "name_full": "Structured Information Extraction with LLMs (Dagdelen et al., 2024)",
            "brief_description": "Sequence-to-sequence approach that fine-tunes GPT-3 and LLaMa-2 on small annotated datasets (400–650 pairs) to perform joint named-entity recognition and relation extraction in materials-science literature.",
            "citation_title": "Structured information extraction from scientific text with large language models",
            "mention_or_use": "use",
            "llm_model_name": "GPT-3 and LLaMa-2 (fine-tuned)",
            "llm_model_description": "Sequence-to-sequence LLMs (GPT-3, LLaMa-2) fine-tuned on a limited set (400–650 annotated text-extraction pairs) to jointly perform NER and relation extraction.",
            "application_domain": "Materials science literature (scientific text mining)",
            "input_corpus_description": "Materials-science literature with a relatively small annotated training set of 400–650 text-extraction pairs for fine-tuning; broader unlabelled corpus not quantified in the review.",
            "qualitative_law_type": "Entity–relation extraction enabling structured relations (e.g., material → property / process relations) that can be aggregated into generalizable patterns or rules.",
            "qualitative_law_example": "The model extracts entities and their relations from sentences (e.g., identifying a material and an associated property/process) but the review notes the model struggled to synthesize complete evidence or higher-level generalizations from these relations.",
            "extraction_methodology": "Fine-tuning of sequence-to-sequence LLMs on annotated pairs for joint NER and relation extraction within a seq-to-seq framework.",
            "evaluation_method": "Evaluated on the annotated dataset; specifics of metrics not detailed in the review summary, but overall performance limited by dataset size and relationship complexity.",
            "results_summary": "Models could extract entities and relations but struggled with full evidence synthesis; limitations stemmed from small annotation sets and complexity of relations.",
            "comparison_to_baseline": "No direct baseline numbers reported in the review summary; performance constrained relative to the complexity of the target extraction/synthesis task.",
            "reported_limitations": "Small fine-tuning dataset (400–650 pairs) and complexity of relations resulted in incomplete evidence synthesis; scaling and generalizability are open challenges.",
            "bias_or_hallucination_issues": "Noted difficulty in fully capturing relationships; potential for missed relations or incorrect extractions when data are sparse — risk of erroneous relation outputs if insufficient training examples.",
            "uuid": "e9582.3",
            "source_info": {
                "paper_title": "A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Saxena_RoBERTa_RAG_QA",
            "name_full": "Large-scale Knowledge Synthesis and Complex IR (Saxena et al., 2022)",
            "brief_description": "A hybrid retrieval and QA architecture using RoBERTa and Fusion-in-Decoder to support paragraph retrieval, triplet retrieval from knowledge graphs, and complex multi-hop question answering on the COVID-19 Open Research Dataset.",
            "citation_title": "Large-scale knowledge synthesis and complex information retrieval from biomedical documents",
            "mention_or_use": "use",
            "llm_model_name": "RoBERTa (encoder) and Fusion-in-Decoder (generative reader)",
            "llm_model_description": "Encoder-based RoBERTa for retrieval/reading and a Fusion-in-Decoder generative reader; architecture combines lexical and semantic retrieval, knowledge-graph triplet extraction, and extractive/generative readers.",
            "application_domain": "Biomedical / COVID-19 literature (information retrieval and synthesis)",
            "input_corpus_description": "COVID-19 Open Research Dataset (CORD-19) and related biomedical documents; system indexes paragraphs, retrieves triplets from knowledge graphs, and supports complex multi-hop QA. Corpus size not numerically specified here.",
            "qualitative_law_type": "Triplet-based factual relations (subject–relation–object) enabling structured patterns across documents (thematic patterns or factual rules aggregated from triplets).",
            "qualitative_law_example": "System extracts subject–relation–object triplets from the knowledge graph to support faceted queries and synthesize answers across documents rather than explicit generalized scientific 'laws'.",
            "extraction_methodology": "Hybrid retrieval (lexical + semantic) for paragraph retrieval, triplet extraction from knowledge graphs for faceted refinement, and multi-hop Dense Retriever feeding extractive (RoBERTa) and generative (Fusion-in-Decoder) readers.",
            "evaluation_method": "Reported retrieval and ranking metrics: precision 81% and NDCG 72% as summarized in the review's comparison table.",
            "results_summary": "Achieved reasonable retrieval/ranking performance (precision 81%, NDCG 72%); authors reported difficulties handling large amounts of unstructured textual data and generative answer shortcomings in older, smaller LMs.",
            "comparison_to_baseline": "Compared retrieval/QA pipeline performance with conventional IR metrics; specific baselines not enumerated in the review summary but metrics indicate competitive retrieval performance for their tasks.",
            "reported_limitations": "Older/smaller generative models struggled with fluent generative answers; handling large volumes of unstructured medical text remained challenging.",
            "bias_or_hallucination_issues": "Generative components had difficulty producing reliable free-text answers; risk of erroneous or hallucinated generative responses when relying on smaller LMs.",
            "uuid": "e9582.4",
            "source_info": {
                "paper_title": "A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ChIP-GPT_LLaMA",
            "name_full": "ChIP-GPT (Cinquin, 2024) - LLaMA-based metadata extraction",
            "brief_description": "A fine-tuned LLaMA model (ChIP-GPT) that extracts and summarizes metadata (ChIP targets and cell lines) from Sequence Read Archive biomedical records, including a summarization step to manage long inputs.",
            "citation_title": "Chip-gpt: a managed large language model for robust data extraction from biomedical database records",
            "mention_or_use": "use",
            "llm_model_name": "LLaMA (fine-tuned as ChIP-GPT)",
            "llm_model_description": "LLaMA architecture fine-tuned on extracted sentences from biomedical Sequence Read Archive records; includes a pre-summarization pipeline to select informative sentences for model input.",
            "application_domain": "Biomedical / ChIP-Seq metadata extraction (genomics experimental metadata)",
            "input_corpus_description": "Sequence Read Archive biomedical database records; longer records were preprocessed via summarization that selects informative sentences and discards irrelevant details to fit model input size constraints.",
            "qualitative_law_type": "Metadata extraction mapping (experiment record → structured metadata fields) enabling standardized reporting rules (e.g., mapping reported methods → identified cell lines/targets).",
            "qualitative_law_example": "Extracted metadata fields such as chromatin immunoprecipitation targets and associated cell lines from database records (structured outputs rather than high-level scientific laws).",
            "extraction_methodology": "Fine-tuning LLaMA on extracted and summarized sentences from SRA records; a summarization pre-step reduces input length by selecting informative sentences.",
            "evaluation_method": "Compared model output to human expert annotations; reported accuracy ~90% in the review summary.",
            "results_summary": "High accuracy (~90%) on the task and competitive with human experts in extraction; however, the summarization pre-step sometimes discarded relevant information, negatively affecting final extraction.",
            "comparison_to_baseline": "Compared favorably to human expert extraction on targeted metadata fields, though errors were linked to preprocessing choices.",
            "reported_limitations": "Summarization step sometimes removed relevant sentences, reducing extraction quality; limitations imposed by input length constraints and preprocessing decisions.",
            "bias_or_hallucination_issues": "Preprocessing-induced omissions rather than hallucinations were the main issue; potential for incorrect or incomplete extracted metadata when input trimming removes critical context.",
            "uuid": "e9582.5",
            "source_info": {
                "paper_title": "A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Sonnenburg_Curie_BPA",
            "name_full": "Fine-tuned GPT-3 (Curie) for BPA Risk Assessment (Sonnenburg et al., 2024)",
            "brief_description": "Fine-tuning a GPT-3 family Curie model on 78 publications (results sections) to extract structured data for chemical risk assessment of Bisphenol A (BPA).",
            "citation_title": "Artificial intelligence-based data extraction for next generation risk assessment: Is fine-tuning of a large language model worth the effort?",
            "mention_or_use": "use",
            "llm_model_name": "GPT-3 family (Curie) fine-tuned",
            "llm_model_description": "A Curie-class GPT-3 family model was fine-tuned on prompt-completion pairs derived largely from results sections of 78 selected publications; no additional architecture details provided in the review.",
            "application_domain": "Toxicology / Chemical risk assessment (Bisphenol A)",
            "input_corpus_description": "Dataset of 78 selected publications focusing on results sections to create prompt-completion pairs for fine-tuning and extraction; domain-specific selection targeted toxicological findings.",
            "qualitative_law_type": "Toxicological outcome relationships (structured experimental results and associations that can be used to derive evidence patterns for risk assessment).",
            "qualitative_law_example": "Model extracted structured experimental results from publications used to populate risk-assessment records; the review reports extraction metrics rather than explicit generalized toxicological rules.",
            "extraction_methodology": "Fine-tuning Curie on prompt-completion pairs derived from results sections of selected papers for supervised extraction of experimental findings.",
            "evaluation_method": "Quantitative evaluation with reported metrics: precision 25%, recall 23%, F1-score 50% (as summarized in the review table).",
            "results_summary": "Fine-tuned Curie outperformed off-the-shelf models on this narrow extraction task, but overall precision and recall were low, indicating substantial room for improvement, especially for complex experimental designs.",
            "comparison_to_baseline": "Compared to non-fine-tuned ready-to-use models, fine-tuned Curie performed better on the targeted extraction tasks, but absolute performance was modest.",
            "reported_limitations": "Difficulties handling studies with complex experimental designs and low precision/recall overall; limited dataset size (78 papers) restricts generalizability.",
            "bias_or_hallucination_issues": "Potential for hallucinated or incorrect structuring when fine-tuned on limited, heterogeneous result sections; low precision and recall suggest noisy or over-generalized outputs.",
            "uuid": "e9582.6",
            "source_info": {
                "paper_title": "A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Mashatian_RAG_GPT4",
            "name_full": "Retriever-Augmented Generation for Diabetes Care with GPT-4 (Mashatian et al., 2024)",
            "brief_description": "A RAG pipeline using GPT-4 to extract and present accurate diabetes and diabetic foot care knowledge to laypersons at an eighth-grade literacy level; used a corpus of 295 articles and evaluated across 175 questions.",
            "citation_title": "Building trustworthy generative artificial intelligence for diabetes care and limb preservation: A medical knowledge extraction case",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4 (used within a Retrieval-Augmented Generation framework)",
            "llm_model_description": "GPT-4 used in a RAG architecture where retrieved context from a 295-article corpus is provided to the model to generate user-friendly, zero-shot answers; no model fine-tuning was performed.",
            "application_domain": "Biomedical / Diabetes care and patient education",
            "input_corpus_description": "Corpus of 295 diabetes-related articles used as the retrieval context; evaluated on 175 user-level questions spanning diabetes and diabetic foot care topics.",
            "qualitative_law_type": "Patient-centered knowledge extraction / layperson-focused rules (actionable care recommendations and factual medical guidance synthesized from the literature).",
            "qualitative_law_example": "The system produced user-friendly recommendations and factual answers about diabetes care based on retrieved literature context; the review reports high task accuracy but no formal generalized scientific rules were enumerated.",
            "extraction_methodology": "Retriever-Augmented Generation (RAG): retrieve relevant passages from the article corpus and feed context to GPT-4 to generate zero-shot, literacy-level-adjusted answers via prompt engineering.",
            "evaluation_method": "Evaluation across 175 questions with reported accuracy metrics (table reports accuracy 98% for a RAG model in the review summary).",
            "results_summary": "High reported accuracy on the limited evaluation set (98% in table), offering promising zero-shot, retrieval-backed generation for layperson education; limited corpus size and evaluation scope noted as constraints.",
            "comparison_to_baseline": "No explicit human-expert baseline reported in the review summary; authors highlight that no fine-tuning was required and RAG improved reliability relative to pure generation.",
            "reported_limitations": "Evaluation based on a relatively small and static corpus (295 articles); further testing required to assess performance with continuously growing literature and broader question types.",
            "bias_or_hallucination_issues": "RAG mitigates some hallucination risk by providing retrieved context, but the review cautions about generalizability and potential for unsupported statements outside the retrieval context.",
            "uuid": "e9582.7",
            "source_info": {
                "paper_title": "A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Review_ChatGPT4_screening",
            "name_full": "ChatGPT-4 for Screening and Summarization (This review's methodology)",
            "brief_description": "The review authors used ChatGPT-4 to generate summaries and context for each selected study to accelerate screening in their PRISMA-guided systematic review workflow, followed by manual validation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "ChatGPT-4",
            "llm_model_description": "ChatGPT-4 (a GPT-4-based conversational LLM) used interactively to generate study summaries and contextual analyses; no fine-tuning described — used as an assistive summarization tool during screening.",
            "application_domain": "Meta-research / Systematic review screening and summarization across biomedical literature",
            "input_corpus_description": "All candidate studies retrieved during the systematic search across ACM DL, IEEE Xplore, Scopus, SOLO (including arXiv), and ScienceDirect; number not explicitly enumerated in the review text.",
            "qualitative_law_type": "Document-level summarization and synthesis (not direct law extraction); facilitated distillation of study findings and methodological traits across many scholarly papers.",
            "qualitative_law_example": "Generated concise summaries of each candidate study to assist human reviewers; examples of synthesized content included model descriptions, tasks, and key limitations per paper rather than explicit generalizable scientific laws.",
            "extraction_methodology": "Prompted ChatGPT-4 to produce summaries for each selected study; outputs were then manually validated by the authors to ensure accuracy and completeness.",
            "evaluation_method": "Manual human validation of the automatically generated summaries to confirm key insights, challenges, and contributions were correctly captured.",
            "results_summary": "Using ChatGPT-4 accelerated the screening process and ensured consistency, but manual validation was necessary to correct errors or omissions; process illustrated a practical human-in-the-loop approach.",
            "comparison_to_baseline": "Compared to fully manual screening, the hybrid approach reduced initial review workload while preserving human oversight; no quantitative time-savings reported in the review.",
            "reported_limitations": "Automated summaries required manual checking; LLM outputs can miss nuances or misrepresent study details without human oversight.",
            "bias_or_hallucination_issues": "Implicit risk that the LLM could hallucinate or mischaracterize studies; manual validation was used to mitigate these risks.",
            "uuid": "e9582.8",
            "source_info": {
                "paper_title": "A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured information extraction from scientific text with large language models",
            "rating": 2,
            "sanitized_title": "structured_information_extraction_from_scientific_text_with_large_language_models"
        },
        {
            "paper_title": "Large-scale knowledge synthesis and complex information retrieval from biomedical documents",
            "rating": 2,
            "sanitized_title": "largescale_knowledge_synthesis_and_complex_information_retrieval_from_biomedical_documents"
        },
        {
            "paper_title": "Gpt-4 performance on querying scientific publications: reproducibility, accuracy, and impact of an instruction sheet",
            "rating": 2,
            "sanitized_title": "gpt4_performance_on_querying_scientific_publications_reproducibility_accuracy_and_impact_of_an_instruction_sheet"
        },
        {
            "paper_title": "Data extraction for evidence synthesis using a large language model: A proof-of-concept study",
            "rating": 2,
            "sanitized_title": "data_extraction_for_evidence_synthesis_using_a_large_language_model_a_proofofconcept_study"
        },
        {
            "paper_title": "Building trustworthy generative artificial intelligence for diabetes care and limb preservation: A medical knowledge extraction case",
            "rating": 2,
            "sanitized_title": "building_trustworthy_generative_artificial_intelligence_for_diabetes_care_and_limb_preservation_a_medical_knowledge_extraction_case"
        },
        {
            "paper_title": "Artificial intelligence-based data extraction for next generation risk assessment: Is fine-tuning of a large language model worth the effort?",
            "rating": 2,
            "sanitized_title": "artificial_intelligencebased_data_extraction_for_next_generation_risk_assessment_is_finetuning_of_a_large_language_model_worth_the_effort"
        },
        {
            "paper_title": "Implementing literature-based discovery (lbd) with chatgpt",
            "rating": 2,
            "sanitized_title": "implementing_literaturebased_discovery_lbd_with_chatgpt"
        },
        {
            "paper_title": "Bag of tricks for training data extraction from language models",
            "rating": 1,
            "sanitized_title": "bag_of_tricks_for_training_data_extraction_from_language_models"
        }
    ],
    "cost": 0.01964425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences</h1>
<p>Gabriel Lino Garcia ${ }^{1}$ (D) ${ }^{\text {a }}$, João Renato Ribeiro Manesco ${ }^{1}{ }^{\text {b }}$ Pedro Henrique Paiola ${ }^{1}{ }^{1}{ }^{1}$, Lucas Miranda $^{2}$, Maria Paola de Salvo ${ }^{2}$ and João Paulo Papa ${ }^{1}{ }^{1}{ }^{1}$<br>${ }^{1}$ School of Sciences, São Paulo State University (UNESP), Bauru - SP, Brazil<br>${ }^{2}$ EasyTelling, São Paulo - SP, Brazil<br>{gabriel.lino, joao.r.manesco, pedro.paiola, joao.papa}@unesp.br<br>{lucas, paola}@easytelling.com</p>
<p>Keywords: evidence synthesis, large language models, LLMs, biomedical, healthcare, literature-based discovery, knowledge extraction</p>
<p>Abstract: The rapid advancement of large language models (LLMs) has opened new boundaries in the extraction and synthesis of medical knowledge, particularly within evidence synthesis. This paper reviews the state-of-theart applications of LLMs in the biomedical domain, exploring their effectiveness in automating complex tasks such as evidence synthesis and data extraction from a biomedical corpus of documents. While LLMs demonstrate remarkable potential, significant challenges remain, including issues related to hallucinations, contextual understanding, and the ability to generalize across diverse medical tasks. We highlight critical gaps in the current research literature, particularly the need for unified benchmarks to standardize evaluations and ensure reliability in real-world applications. In addition, we propose directions for future research, emphasizing the integration of state-of-the-art techniques such as retrieval-augmented generation (RAG) to enhance LLM performance in evidence synthesis. By addressing these challenges and utilizing the strengths of LLMs, we aim to improve access to medical literature and facilitate meaningful discoveries in healthcare.</p>
<h2>1 INTRODUCTION</h2>
<p>In recent years, the number of studies published in various fields has grown exponentially, with a wide range of applications spanning multiple disciplines (Bornmann et al., 2024). While this surge in scientific literature contributes to knowledge expansion, it poses a significant challenge for researchers and professionals attempting to stay informed of critical developments (Beller et al., 2013). This trend is particularly important in fields such as medicine, where the implications of new findings can have direct and life-altering impacts on patients' lives.</p>
<p>However, despite the abundance of information, the practical use of many of these studies remains limited (Meho, 2007). This is often due to barriers to accessing relevant and impactful research, particularly when important findings are hidden within the vast-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ness of the literature. The medical field exemplifies this challenge, as impactful discoveries that could improve treatment outcomes or advance medical science may go unnoticed simply because they are buried beneath a deluge of publications or presented in a highly technical manner.</p>
<p>Systematic reviews have long been regarded as a solution, providing comprehensive and structured analyses of the available literature. While effective, these reviews are exhaustive and frequently very technical, making them time-consuming to produce and difficult for non-experts to interpret. Consequently, they do not fully solve the issue of accessibility, especially for those who lack the specialized knowledge to extract meaningful insights from the data.</p>
<p>The advent of large language models (LLMs) offers a promising avenue for addressing this challenge. LLMs, powered by advancements in artificial intelligence, have the potential to assist in the navigation and synthesis of vast amounts of scientific literature. Indeed, some research has already begun to explore the potential of these models to automate aspects of the systematic review process, highlighting the pos-</p>
<p>sibility of their application in evidence synthesis and knowledge extraction (Alshami et al., 2023).</p>
<p>LLMs, such as GPT, are particularly well-suited to tasks involving knowledge extraction and summarization. They can process and synthesize vast amounts of text, distilling complex ideas into more accessible formats (Ignat et al., 2023). This capability makes them powerful tools for navigating large datasets and filtering out the most relevant information. Moreover, the ability of LLMs to generate concise and coherent summaries from complex data sets is already being harnessed in some preliminary applications within systematic review processes.</p>
<p>While early applications of LLMs in systematic reviews are promising, synthesizing scientific evidence for decision-making requires more than mere automation of reviews. Managers, healthcare professionals, and other non-technical stakeholders require tools to explore the literature and synthesize evidence meaningfully and effectively. This necessitates further investigation into how LLMs can be adapted as a tool for evidence synthesis for researchers and those involved in policy-making and management in fields like healthcare.</p>
<p>Given the potential of LLMs to address these challenges, this paper aims to provide a comprehensive review of current research exploring the use of LLMs in evidence synthesis, with a particular focus on their application in the medical field. We will examine existing methods, tools, and frameworks that leverage LLMs for scientific knowledge extraction and discuss how these approaches can help bridge the gap between vast amounts of data and practical, actionable insights.</p>
<p>Thus, we have proposed a review of scientific evidence synthesis using LLMs, particularly for the healthcare case. Among the contributions of our work are:</p>
<ul>
<li>We provide a comprehensive evaluation of the state-of-the-art LLMs used for medical evidence synthesis, focusing on their performance and challenges in tasks like clinical decision support and knowledge extraction.</li>
<li>We identify key limitations, including issues with hallucinations, contextual understanding, and scalability, offering insights into the research gaps that need to be addressed.</li>
<li>We propose the need for unified benchmarks and highlight future directions to enhance the effectiveness of LLMs in biomedical applications.</li>
</ul>
<p>The remainder of this paper is structured as follows. In Section 2, we present the protocol used to conduct our review. Section 3 gives a background on
evidence synthesis based on scientific literature. Section 4 discusses the main works found in scientific evidence synthesis. Section 5 discusses the state of the art and current limitations in the literature, and Section 6 concludes the paper.</p>
<h2>2 Review Methodology</h2>
<p>This systematic review followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to ensure a transparent and rigorous approach to evaluating the role of LLMs in the biomedical field. The main objective of this review is to provide a comprehensive assessment of the effectiveness, challenges, and methodologies of LLMs in the extraction and synthesis of medical knowledge. While knowledge extraction is a broader area of interest, this review specifically focuses on evidence synthesis, which aligns more closely with the complex task of integrating diverse medical information for clinical decision-making and research purposes.</p>
<p>The guiding research question of this review is: "How effective are LLMs in extracting and synthesizing structured and relevant knowledge from medical texts, and what are the key challenges and outcomes associated with their application in the medical field?" This question encompasses the exploration of the models most frequently used, how they are fine-tuned for biomedical applications, their performance compared to other knowledge extraction methods, and the specific biomedical domains that benefit most from LLM-based approaches.</p>
<p>The review adopted a search strategy across several academic and preprint databases, including ACM Digital Library, IEEE Xplore, Scopus, SOLO (including arXiv), and ScienceDirect. The search string was designed to cover the breadth of LLM applications in medical knowledge extraction, focusing specifically on evidence synthesis tasks. Studies were included if they focused on applying LLMs in medical settings, provided quantitative evaluations, and were published in English over the past five years.</p>
<h3>2.1 Screening of Relevant Works</h3>
<p>To enhance the screening process's efficiency, we used a ChatGPT-4 LLM to generate summaries and provide context for each selected study. This approach allowed us to hasten the review of many articles while maintaining a high level of consistency in the analysis. The initial screening was guided by predefined inclusion and exclusion criteria, with priority given to studies that addressed LLM applications in</p>
<p>evidence synthesis. A diagram illustrating the screening and selection process, following PRISMA guidelines, is provided in Figure 1.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the screening process, conducted in accordance with the PRISMA protocol, used to identify and select relevant studies for this review.</p>
<p>After the automated summarization phase, we manually validated the results to ensure that key insights, challenges, and contributions were accurately captured. This dual approach-combining automation with manual oversight—provided a robust method for managing the complexity of the literature while preserving the depth and rigor expected in systematic reviews.</p>
<p>For each included study, we extracted data on the type of LLM used, the medical task it was applied to, performance outcomes, and the challenges encountered. Particular attention was paid to how LLMs were adapted for biomedical knowledge extraction, the domains where they performed most effectively, and how they compared to other knowledge extraction techniques.</p>
<h2>3 Background</h2>
<p>Evidence synthesis refers to collecting, analyzing, and integrating information from the literature in a well-documented way to answer a pre-defined research question. One such form of evidence synthesis is through systematic reviews, which follow a rigorous search protocol to obtain all possible empiri-
cal evidence regarding a certain area of knowledge in a way that reduces bias and allows for replicability (Lasserson et al., 2019).</p>
<p>The objective of such systems is to support decision-makers by providing a reliable summary of research findings in the scope of the literature. However, constantly performing manual evidence synthesis whenever you need to make a decision can cause many constraints, especially since systematic reviews are highly complex and time-consuming, requiring strict adherence to rigorous protocols. This process often limits the decision maker's ability to respond quickly to novel evidence (Singh, 2017).</p>
<p>Many approaches have been proposed to accelerate such processes using novel Natural Language Processing (NLP) techniques such as LLMs, mostly focusing on different aspects of the systematic review protocol, which can significantly reduce the manual burden and time required to complete a review (Bolanos et al., 2024). These tools, however, are still focused on specific parts of the review process, such as screening or data extraction. While they can still be relevant, preparing the complete review can still take time and hinder decision-makers productivity.</p>
<p>The use of LLMs to accelerate knowledge synthesis has been explored in other reviews (Bolanos et al., 2024; Burgard and Bittermann, 2023; de la TorreLópez et al., 2023). However, these reviews primarily concentrate on streamlining the systematic review process. In contrast, our work focuses on techniques that use LLMs to perform evidence synthesis directly. In other words, those methods focus on compiling relevant literature in a more flexible, less constrained manner to provide quick, up-to-date knowledge, particularly within the healthcare domain, where timely access to synthesized evidence is crucial.</p>
<h2>4 Results</h2>
<p>This section synthesizes the selected studies that have applied various LLMs to medical evidence synthesis. The discussion is divided into two parts: first, we examine the use of LLMs in evidence synthesis as a broader concept, and second, we focus specifically on their applications within the medical field as reported in the literature.</p>
<h3>4.1 Evidence Synthesis</h3>
<p>Regarding evidence Synthesis using LLMs, a few works have been proposed more broadly to solve specific tasks in recent literature. One work (Khraisha</p>
<p>et al., 2024) evaluates the efficacy of GPT-4 in performing systematic review tasks such as title/abstract screening, full-text review, and data extraction from various literature types and languages without human intervention. The study employed a "human-out-of-the-loop" approach, meaning that GPT-4 was tested independently without human intervention during extraction, focusing on the literature regarding parenting in protracted refugee situations. This work describes several challenges of current GPT models, indicating that the model's performance is heavily biased by the distribution of relevant and irrelevant studies, displaying a tendency of over-excluding studies.</p>
<p>In other words, this concept is expanded in less generic fields, such as government report generation (Gupta et al., 2024), in which Google's Gemini Pro and OpenAI's GPT-3.5 Turbo are used to improve data extraction, analysis, and visualization processes by reading all existing reports to identify graphs and charts needing updates, extracting data points from these graphs and synthesizing information in their knowledge repositories to update their report graphs. Although the method does not exactly synthesize information in textual form, it performs a RetrievalAugmented Generation (RAG) method to synthesize information in graphic form.</p>
<p>The work of Yu et al. (2023) focuses on enhancing the extraction of training data from language models, specifically using the GPT-Neo 1.3B model in a two-stage pipeline: suffix generation, where various sampling strategies are employed, including topk sampling, nucleus sampling, and typical sampling, to produce candidate suffixes based on a given prefix; and suffix ranking, in which perplexity and additional metrics like Zlib entropy are used to evaluate and select the most likely suffixes from the generated candidates. Although interesting for several aspects of knowledge extraction, this technique is still dependent on a large number of candidates that are ranked to identify a single successful instance, only working in limited scenarios.</p>
<p>In order to summarize the knowledge base, Discrete Text Summarization (DeTS) was proposed as a novel unsupervised method for discrete text summarization, in which grammatical sequence scoring and independent key points from large textual datasets are used for natural language inference. The technique consists of two main components: candidate extraction and candidate matching. In the candidate extraction phase, the algorithm utilizes Semantic Role Labeling (SRL) to segment comments into smaller pieces. The candidate matching phase aligns these key points with sentences from the entire corpus using NLI algorithms, determining whether the sentences
entail the key points based on their semantic similarity. Although the method was successful in a restricted scenario of anonymous comments, there is still a concern regarding bias in the employed dataset.</p>
<p>Focusing more on the extraction of information in scientific texts, Dagdelen et al. (2024) focuses on joint-named entity recognition and relation extraction, allowing the models to identify entities and their relationships within materials science literature. The model architecture is based on a sequence-tosequence framework, where GPT-3 and LLaMa-2 models are fine-tuned on a relatively small dataset of 400-650 annotated text-extraction pairs. Given the dataset's limitations and the relationships' complexity, the model still struggles to perform complete evidence synthesis.</p>
<h3>4.2 Synthesis in Biomedical Sciences</h3>
<p>In the healthcare domain, which is the primary focus of our analysis, one study explores the application of LLMs, specifically GPT-3.5 and GPT-4, for Literature-Based Discovery to generate research hypotheses Nedbaylo and Hristovski (2024). The authors employ a bifurcated prompt engineering technique, where the original prompt is split into two parts and executed in separate chat windows. In the first segment, the model generates a list of disease characteristics without disclosing the disease name. In contrast, the second suggests potential interventions based on the factors identified in the initial segment. The study conducts a qualitative evaluation and concludes that GPT-based methods heavily depend on preexisting bibliographic databases, such as Medline, which restricts their applicability to fields with wellestablished datasets. Additionally, the study finds that outputs from GPT-4 often lack technical depth and novelty, and they frequently fail to provide adequate references for the generated information.</p>
<p>Tao et al. (2024) employs the OpenAI GPT-4 model to evaluate its performance in answering specific questions related to HIV drug resistance from published scientific papers. The researchers designed an automated pipeline that transformed the text of 60 selected HIV drug resistance papers into markdown format, excluding sections like the introduction and discussion to focus on the methods and results. By posing 60 questions in two modes, multiplequestion mode (all questions presented simultaneously) and single-question mode (one question at a time), with and without an instruction sheet containing specialized knowledge, the authors were able to perform a quantitative evaluation of the task. Although the method obtained a fairly good mean ac-</p>
<p>curacy of $86.9 \%$, some aspects of the technique could be improved, such as the fact that the instruction sheet was not effectively utilized by the model, by not improving the accuracy of the responses, in addition, the method struggled with specific queries that required making inferences, especially when dealing with implicit information in the texts. Worst of all, the GPT4 model was more likely to provide false positive answers when questions were submitted individually than when they were submitted together, indicating the presence of bias when dealing with certain aspects of the literature.</p>
<p>Using the Claude 2 LLM, one study performs a proof-of-concept design to evaluate the performance of LLMs in interpreting randomized controlled trials (RCTs) Gartlehner et al. (2024). To do that, the researchers selected a convenience sample of 10 openaccess RCTs and focused on extracting 16 distinct data elements, which included study identifiers, participant characteristics, and outcome data. The data extraction pipeline involves mostly prompt engineering, where the crafted prompts were iteratively tested and refined to optimize the model's output. Although the method obtains interesting results, several concerns should be considered, such as the fact that open RCTs were used, meaning they could be in the training set of Claude 2. The authors also reported two cases of hallucinations from answers that the model did not know how to reply to.</p>
<p>Focusing on information retrieval to support knowledge synthesis in biomedical documents, one work uses the RoBERTa language model to work with the COVID-19 Open Research Dataset Saxena et al. (2022). To do that, an architecture was developed consisting of three main components that extract relevant information: Paragraph Retrieval, Triplet Retrieval from Knowledge Graphs, and Complex Question Answering. The Paragraph Retrieval employs a hybrid approach combining lexical methods with semantic search for indexing and re-ranking results based on relevance. The Triplet Retrieval system extracts subject-relation-object triplets from the knowledge graph, allowing for faceted refinement of search results. Finally, the complex QA system utilizes a Multi-hop Dense Retriever to handle multihop questions, iteratively retrieving relevant passages and employing both extractive (RoBERTa) and generative (Fusion-in-Decoder) readers to generate answers. The model relies on an older approach for language models, which, although smaller, has more difficulty when dealing with generative language answers; the authors also report difficulty in dealing with the amount of unstructured textual data reported in the medical literature.</p>
<p>This trend of using encoder-based smaller language models was also observed in two other works in the literature. One of them works on a dataset of biomedical articles collected from PubMed, aiming to summarize biomedical literature by using an encoderdecoder model based on BioBERT Alambo et al. (2022) To achieve its purpose, the researchers created a framework based on two main components: an entity-driven knowledge retriever that extracts facts based on named entities identified in the source documents, and a knowledge-guided abstractive summarizer that generates summaries by attending to both the source article and the retrieved facts.</p>
<p>The other work focuses on semi-automate data extraction for systematic literature using a BERT model pre-trained on a corpus of 100,000 open-access clinical publications, the main objective being extracting clinical relationships from the corpus in order to perform literature filtering Panayi et al. (2023). Both works, although displaying interesting results, suffer from the difficulties of dealing with complex unstructured data; the BioBERT-based models, although displaying better results, still report hallucination, as there is an attempt to generate structured text.</p>
<p>Other works are more focused on extracting specific information from the biomedical literature. One such work, named ChIP-GPT, fine-tuned a LLaMa architecture on Sequence Read Archive information obtained from biomedical database records by extracting relevant sentences and summarizing biomedical records, intending to identify chromatin immunoprecipitation (ChIP) targets and cell lines in these records. A summarization step was implemented to manage the input length limitations of the model for longer records. This involved selecting informative sentences while discarding irrelevant details, ensuring the model could generate concise and accurate answers. The final results display good accuracy in the task even when compared with human experts. However, it was observed that this summarization step still had a problem discarding relevant information for the model, which impacted the final results.</p>
<p>Following the same concept, another work focuses on extracting relevant data from scientific literature for chemical risk assessment, focusing on Bisphenol A Sonnenburg et al. (2024). To achieve that, the authors created a dataset containing 78 selected publications, primarily extracting results sections to form prompt-completion pairs so that they could perform fine-tuning on an LLM from the GPT-3 family named Curie model. The Curie model reported remarkable results, especially compared to other ready-to-use models that were not fine-tuned, even if the authors still have some concerns regarding treating stud-</p>
<p>ies with more complex experimental designs.
Finally, a study that focuses on using a RetrieverAugmented Generation (RAG) model to extract and deliver accurate medical knowledge about diabetes and diabetic foot care, specifically tailored for laypersons with an eighth-grade literacy level, was proposed Mashatian et al. (2024). The authors use the GPT-4 model to formulate user-friendly responses based on the retrieved context, offering a zero-shot solution to the problem. A set of 295 articles was used to provide context and be evaluated across 175 questions on various diabetes-related topics. This work offers an interesting solution that does not require training the LLM and displays the results in natural language, even for people with lower literacy levels. Despite its advantages, the model is still evaluated on a very limited set of data, so further experiments are required to see how it performs with new works being published in the literature.</p>
<h2>5 Comparison of Current Methods and Future Directions</h2>
<p>In this section, we perform an aggregated analysis that compares datasets, performance metrics, identified limitations, and future trends suggested by each method to understand the limitations of current techniques and the future trends in the literature. Although the area lacks a complete benchmark for evaluating evidence synthesis, and each method does a different analysis, we still compare each technique by their main points and context in Table 1.</p>
<p>Existing methods that use language models for knowledge synthesis in biomedical tasks employ various strategies and applications, addressing several aspects of knowledge extraction, including literaturebased discovery, data extraction, and questionanswering. One of the key challenges in this area is to perform a comprehensive evaluation capable of comparing the effectiveness of each method and LLM by themselves. Currently, each method follows its own protocol, with distinct datasets, evaluation criteria, and tasks, making it difficult to draw direct comparisons between models or assess their generalizability to new problems.</p>
<p>In the reviewed studies, several models have been used for evidence syntheses, such as GPT-3.5, GPT4, Claude 2, and even encoder-based methods, such as RoBERTa and BioBERT, on a wide range of tasks ranging from generating research hypotheses to answering specific scientific queries and extracting clinical trial data. For example, Nedbaylo and Hristovski (2024) focuses on literature-based discovery through
a GPT-4 LLM, while Tao et al. (2024) employs the same LLM in a very distinct task and context: processing structured biomedical data for HIV drug resistance analysis. Although both methods use the same model and follow similar strategies, it is difficult to conclude if the observed limitations come from the technique or the contextual data, as there is no shared information among them. Both studies also highlight considerable limitations of current LLMs in dealing with insights from the data, especially when the model needs to handle implicit information to generate connections and process novel information.</p>
<p>Another current problem in the literature is that most methods focus on using pre-established datasets, such as Medline in the study by Nedbaylo and Hristovski (2024) or the curated HIV drug resistance literature in Tao et al. (2024), to create a knowledge base for the LLM and fine-tune the model. In this case, it is difficult to establish the influence of each piece of data in the response, and it also makes dealing with the novel and growing literature even more difficult, as there is a need to re-train the model. As such, strategies based on RAG, such as the one proposed by Mashatian et al. (2024), appear as a more reliable solution to deal with new pieces of data and may pose the path forward for new techniques that may incorporate modern RAG approaches at their core.</p>
<p>In the study of Gartlehner et al. (2024) that uses a Claude 2 model for the extraction of data from RCTs, the authors raise two main concerns regarding this area: the growing risks of hallucination caused by difficulties in dealing with unstructured data in biomedical research, and the potential bias coming from training on open-access data, which the model may already have seen during training. In addition, without a common benchmark, it is difficult to determine how these models would fare on more diverse datasets or in more complex, less predictable environments.</p>
<p>Studies using smaller, encoder-based models like RoBERTa, BioBERT, and BERT also exhibit this challenge. While they are effective for specific tasks like entity extraction and information retrieval, their scalability and ability to generate new insights are limited. This is particularly evident in models designed for more specialized tasks, such as ChIP-GPT for chromatin immunoprecipitation data extraction and the fine-tuned Curie model for chemical risk assessment. These models achieve impressive results within their respective domains but lack the flexibility to be easily compared or adapted across different biomedical tasks without a unified evaluation standard.</p>
<p>The absence of a unified benchmark also extends</p>
<p>Table 1: Comparison of various methods used in biomedical literature for knowledge extraction and evidence synthesis.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Paper</th>
<th style="text-align: center;">Year</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Biomedical Context</th>
<th style="text-align: center;">Reported Results</th>
<th style="text-align: center;">Brief Summary</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Alambo et al. (2022)</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">BioBERT</td>
<td style="text-align: center;">Biomedical articles and facts from biomedical knowledge bases.</td>
<td style="text-align: center;">Precision: 55.07\% Recall 7.97\%</td>
<td style="text-align: center;">The method uses integrates named entities and facts from biomedical knowledge bases into transformer-based models to enhance the factual consistency of generated summaries in biomedical literature.</td>
</tr>
<tr>
<td style="text-align: center;">Saxena et al. (2022)</td>
<td style="text-align: center;">2022</td>
<td style="text-align: center;">RoBERTa + Fusion-in-Decoder</td>
<td style="text-align: center;">Unstructured text data from documents, articles, biomedical journals, and structured data like tables and electronic health records.</td>
<td style="text-align: center;">Precision: 81\% NDCG: 72\%</td>
<td style="text-align: center;">A framework for complex information retrieval from biomedical documents is developed, utilizing paragraph retrieval, triplet retrieval from knowledge graphs, and complex question answering.</td>
</tr>
<tr>
<td style="text-align: center;">Panayi et al. (2023)</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">Metrics for progression-free survival, patient age, treatment arm dosage, and eGFR measurements extracted from systematic literature reviews in oncology and Fabry disease.</td>
<td style="text-align: center;">F1-score: 73\%</td>
<td style="text-align: center;">A machine learning approach using pre-trained BERT and CRF models was employed to automate data extraction from systematic literature reviews by identifying biomedical entities and their relations.</td>
</tr>
<tr>
<td style="text-align: center;">Sonnenburg et al. (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">NAM-based toxicity data on Bisphenol A (BPA) extracted from scientific publications</td>
<td style="text-align: center;">Precision: 25\%, Recall: 23\%, F1-score: $50 \%$.</td>
<td style="text-align: center;">A fine-tuned version of GPT-3 was employed to extract and structure data from scientific publications for risk assessment, specifically targeting toxicological properties of bisphenol A (BPA).</td>
</tr>
<tr>
<td style="text-align: center;">Nedbaylo and Hristovski (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">GPT-3.5 and GPT-4</td>
<td style="text-align: center;">Scientific papers and literature related to biomedical concepts.</td>
<td style="text-align: center;">Only a qualitative evaluation was performed in this paper</td>
<td style="text-align: center;">A Literature-based discovery technique based on the generation of research hypotheses via bifurcated prompt engineering.</td>
</tr>
<tr>
<td style="text-align: center;">Cinquin (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">ChIP-GPT (LLaMa)</td>
<td style="text-align: center;">Metadata from Chromatin Immunoprecipitation Sequencing (ChIP-Seq) experiments.</td>
<td style="text-align: center;">Accuracy: 90\%</td>
<td style="text-align: center;">A LLaMA-based model is fine-tuned to extract metadata from biomedical database records, targeting chromatin immunoprecipitation (ChIP) data.</td>
</tr>
<tr>
<td style="text-align: center;">Gartlehner et al. (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">Claude 2</td>
<td style="text-align: center;">The research paper focuses on extracting several types of biomedical data elements from published studies, including study identifiers, characteristics of study participants, and outcome data.</td>
<td style="text-align: center;">Accuracy: 96.3\%, F1-score: 98\%</td>
<td style="text-align: center;">The study uses the Claude 2 LLM to extract several types of biomedical elements from published studies in medical literature.</td>
</tr>
<tr>
<td style="text-align: center;">Manhattan et al. (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Scientific papers related to diabetes and diabetic foot care.</td>
<td style="text-align: center;">Accuracy: 98\%</td>
<td style="text-align: center;">A RAG model was developed to extract accurate medical knowledge about diabetes and diabetic foot care for laypersons with an eighth-grade literacy level, using prompt engineering.</td>
</tr>
<tr>
<td style="text-align: center;">Tao et al. (2024)</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">HIV genetic sequence data, antiviral treatment histories, and the effects of mutations on susceptibility to antiviral drugs from papers on HIV drug resistance (HIVDR).</td>
<td style="text-align: center;">Accuracy: 86.9\%, Recall 72.5\%, Precision 87.4\%</td>
<td style="text-align: center;">An automated pipeline using GPT-4 was created to evaluate the accuracy of responses to queries about published papers on HIV drug resistance.</td>
</tr>
</tbody>
</table>
<p>to the issue of hallucinations and false positives, which have been identified across multiple studies. For example, the GPT-4 model in Tao et al. (2024)'s study showed a higher tendency for false positives in single-question mode, raising concerns about the consistency and reliability of LLMs when applied to individual biomedical queries. Likewise, the hallucinations observed in Claude 2's responses in Gartlehner et al. (2024)'s work underlines the need for more robust evaluation methods to assess generated content's accuracy and factual correctness.</p>
<p>As we can observe from the current state of the field, developing a unified benchmark to standardize the evaluation of LLMs across evidence synthesis in diverse biomedical tasks is crucial to advance the field. This benchmark must evaluate the model's accuracy, contextual comprehension, insights capability, generalizability, and the capacity to process unstructured data commonly found in this area. Furthermore, recent advancements in LLM retrieval techniques, such as RAG techniques, could significantly enhance evidence synthesis, as these techniques not only improve the quality of generated responses but also allow for the models to deal with the novel incoming literature, on top of enabling the retrieval of the relevant sources, ensuring that LLMs contribute
to evidence synthesis in a transparent and verifiable manner.</p>
<h2>6 Conclusion</h2>
<p>This review aims to highlight the increasing role of natural language processing and LLMs in biomedical research, particularly for automating tasks like literature-based discovery, clinical trial data extraction, and answering complex scientific questions. Although these models show great promise, they also reveal significant shortcomings. Many rely on preexisting datasets and struggle with more nuanced tasks such as making inferences, understanding context, and dealing with implicit information. Issues like hallucinations and false positives in several studies further underscore the need for stronger fact-checking and better handling of contextual information.</p>
<p>A major challenge is the lack of a unified benchmark across various tasks. The current variety of datasets and evaluation metrics makes it difficult to compare models directly or evaluate how well they generalize to new problems. Fine-tuned models like ChIP-GPT and Curie perform well in narrow areas but are less proven in broader contexts. Similarly,</p>
<p>older encoder-based models such as RoBERTa and BioBERT have scalability limitations and are difficult to manage the complexity of unstructured medical literature.</p>
<p>Looking ahead, efforts should focus on improving models' ability to grasp context, addressing the problem of hallucinations, and creating unified benchmarks to standardize evaluation across tasks. Integrating techniques like knowledge graphs, multi-hop reasoning, and retrieval-augmented generation (RAG) could help close the current performance gaps, especially for complex evidence synthesis. While LLMs hold great potential in healthcare, they still require improvements in architecture, evaluation methods, and unstructured data handling to fully realize their transformative potential in biomedical research and automated evidence synthesis.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>This study was funded by the São Paulo Research Foundation (FAPESP) grants 2013/07375-0, 2019/07665-4, 2023/14427-8, 2024/00789-8, and 2024/01336-7 as well as the National Council for Scientific and Technological Development grants 308529/2021 - 9 and 400756/2024 - 2.</p>
<h2>REFERENCES</h2>
<p>Alambo, A., Banerjee, T., Thirunarayan, K., and Raymer, M. (2022). Entity-driven fact-aware abstractive summarization of biomedical literature. In 2022 26th International Conference on Pattern Recognition (ICPR), pages 613-620. IEEE.</p>
<p>Alshami, A., Elsayed, M., Ali, E., Eltoukhy, A. E., and Zayed, T. (2023). Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions. Systems, 11(7):351.</p>
<p>Beller, E. M., Chen, J. K.-H., Wang, U. L.-H., and Glasziou, P. P. (2013). Are systematic reviews up-to-date at the time of publication? Systematic reviews, 2:1-6.</p>
<p>Bolanos, F., Salatino, A., Osborne, F., and Motta, E. (2024). Artificial intelligence for literature reviews: Opportunities and challenges. arXiv preprint arXiv:2402.08565.</p>
<p>Bornmann, L., Haunschild, R., and Mutz, R. (2024). Accelerating scientific progress with preprints. Nature Computational Science, 4(5):311-311.</p>
<p>Burgard, T. and Bittermann, A. (2023). Reducing literature screening workload with machine learning. Zeitschrift für Psychologie.</p>
<p>Cinquin, O. (2024). Chip-gpt: a managed large language model for robust data extraction from biomedical database records. Briefings in bioinformatics, 25(2):bbad535.</p>
<p>Dagdelen, J., Dunn, A., Lee, S., Walker, N., Rosen, A. S., Ceder, G., Persson, K. A., and Jain, A. (2024). Structured information extraction from scientific text with large language models. Nature Communications, 15(1):1418.
de la Torre-López, J., Ramírez, A., and Romero, J. R. (2023). Artificial intelligence to automate the systematic review of scientific literature. Computing, 105(10):2171-2194.</p>
<p>Gartlehner, G., Kahwati, L., Hilscher, R., Thomas, I., Kugley, S., Crotty, K., Viswanathan, M., Nussbaumer-Streit, B., Booth, G., Erskine, N., et al. (2024). Data extraction for evidence synthesis using a large language model: A proof-of-concept study. Research Synthesis Methods.</p>
<p>Gupta, R., Pandey, G., and Pal, S. K. (2024). Automating government report generation: A generative ai approach for efficient data extraction, analysis, and visualization. Digital Government: Research and Practice.</p>
<p>Ignat, O., Jin, Z., Abzaliev, A., Biester, L., Castro, S., Deng, N., Gao, X., Gunal, A., He, J., Kazemi, A., et al. (2023). Has it all been solved? open nlp research questions not solved by large language models. arXiv preprint arXiv:2305.12544.</p>
<p>Khraisha, Q., Put, S., Kappenberg, J., Warraitch, A., and Hadfield, K. (2024). Can large language models replace humans in systematic reviews? evaluating gpt-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. Research Synthesis Methods.</p>
<p>Lasserson, T. J., Thomas, J., and Higgins, J. P. (2019). Starting a review. Cochrane handbook for systematic reviews of interventions, pages 1-12.</p>
<p>Mashatian, S., Armstrong, D. G., Ritter, A., Robbins, J., Aziz, S., Alenabi, I., Huo, M., Anand, T., and Tavakolian, K. (2024). Building trustworthy generative artificial intelligence for diabetes care and limb preservation: A medical knowledge extraction case. Journal of Diabetes Science and Technology, page 19322968241253568.</p>
<p>Meho, L. I. (2007). The rise and rise of citation analysis. Physics World, 20(1):32.</p>
<p>Nedbaylo, A. and Hristovski, D. (2024). Implementing literature-based discovery (lbd) with chatgpt. In 2024 47th MIPRO ICT and Electronics Convention (MIPRO), pages 120-125. IEEE.</p>
<p>Panayi, A., Ward, K., Benhadji-Schaff, A., Ibanez-Lopez, A. S., Xia, A., and Barzilay, R. (2023). Evaluation of a prototype machine learning tool to semi-automate data extraction for systematic literature reviews. Systematic Reviews, 12(1):187.</p>
<p>Saxena, S., Sangani, R., Prasad, S., Kumar, S., Athale, M., Awhad, R., and Vaddina, V. (2022). Large-scale knowledge synthesis and complex information retrieval from biomedical documents. In 2022 IEEE International Conference on Big Data (Big Data), pages 2364-2369. IEEE.</p>
<p>Singh, S. (2017). How to conduct and interpret systematic reviews and meta-analyses. Clinical and translational gastroenterology, 8(5):e93.</p>
<p>Sonnenburg, A., van der Lugt, B., Rehn, J., Wittkowski, P., Bech, K., Padberg, F., Eleftheriadou, D., Dobrikov, T., Bouwmeester, H., Mereu, C., et al. (2024). Artificial intelligence-based data extraction for next generation risk assessment: Is fine-tuning of a large language model worth the effort? Toxicology, 508:153933.</p>
<p>Tao, K., Osman, Z. A., Tzou, P. L., Rhee, S.-Y., Ahluwalia, Y., and Shafer, R. W. (2024). Gpt-4 performance on querying scientific publications: reproducibility, accuracy, and impact of an instruction sheet. BMC Medical Research Methodology, 24(1):139.</p>
<p>Yu, W., Pang, T., Liu, Q., Du, C., Kang, B., Huang, Y., Lin, M., and Yan, S. (2023). Bag of tricks for training data extraction from language models. In International Conference on Machine Learning, pages 40306-40320. PMLR.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>a) https://orcid.org/0000-0003-1236-7929
b) https://orcid.org/0000-0002-1617-5142
c) https://orcid.org/0000-0001-9093-535X
d) https://orcid.org/0000-0002-6494-7514&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
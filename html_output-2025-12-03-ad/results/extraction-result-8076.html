<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8076 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8076</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8076</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-258865545</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.14387v4.pdf" target="_blank">AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their strong instruction-following abilities. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following requires tackling three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 50x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, DPO, best-of-n, expert iteration, and more) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% improvement in win-rate against Davinci003. We release all components of AlpacaFarm at https://github.com/tatsu-lab/alpaca_farm.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8076.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8076.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>per-example agreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-example agreement between simulated LLM judges and human majority vote</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measured percent agreement between AlpacaFarm's simulated evaluation annotators (p_eval_sim: a pool of 13 API LLM annotators) and the human majority vote on pairwise preference labels; used to validate fidelity of LLM-as-judge at individual example level.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Single-turn instruction following (pairwise preference judgments)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Alpaca PREF split / evaluation set (805 instructions composed from public eval datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>p_eval_sim (pool of simulated annotators: GPT-4, ChatGPT, Davinci003 prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Pool of 13 simulated annotators implemented via OpenAI API: 5 GPT-4 prompts, 4 ChatGPT prompts, 4 Davinci003 prompts; varied prompt formats, in-context examples, batch sizes; sampling temperature 1.0 for annotator pool; evaluation pool excludes injected training flip-noise.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Crowdworkers from Amazon Mechanical Turk (selected subset of annotators; majority vote over 3 human annotations used as reference in agreement computations)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>percent agreement with human majority vote</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.65</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>some simulator-specific biases (preference for first output in prompt; preference for outputs from same model as simulator); does not perfectly capture all human diversity</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Simulated evaluator pool achieved a per-example agreement (~65%) similar to a held-out human annotator (~66%); single deterministic GPT-4 prompts could reach higher agreement (~68%) but lacked inter-/intra-annotator variability.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Much lower cost and faster annotation (orders-of-magnitude cheaper); reproducible and scalable; comparable per-example agreement to an individual human annotator.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Agreement computed against majority vote of 3 human annotators on preference pairs; simulated pool used randomized output ordering and diverse prompts; reported agreement and cost comparisons (see costs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8076.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8076.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>method-ranking correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Correlation of method-level rankings: simulated LLM evaluator vs human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of overall method rankings (win-rates vs Davinci003) produced by AlpacaFarm's simulated evaluator and by human annotators, reported with Spearman correlation and R^2 as measures of agreement at the method level.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Comparative evaluation of LPF (learning from pairwise feedback) methods via win-rate against Davinci003</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>805-instruction AlpacaFarm evaluation set (combined public eval datasets) and 200-demo Alpaca Demo subset for validation</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>p_eval_sim (AlpacaFarm simulated evaluation) and p_human (human annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Simulated evaluation uses the 13-annotator pool (GPT-4/ChatGPT/Davinci003 prompts) without injected training label flips; human evaluation uses the selected MTurk annotators (majority vote aggregation).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Crowdworkers from Amazon Mechanical Turk (16 selected from qualification; majority votes used for aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman rank correlation (method-level); also reported R^2 for some plots</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.98</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>A few rank mismatches (e.g., SFT10k vs SFT52k and ChatGPT vs PPO) where simulator and humans disagreed; simulator can still mis-rank some near-tie/close-performance cases.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Strong match in method rankings between simulation and human pipeline (Spearman 0.98), indicating AlpacaFarm is effective for method selection; some mismatches attributed to subtle biases (e.g., human preference for longer outputs) or evaluation prompt constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Enables low-cost, rapid iteration and reliable method-level selection that transfers to human-evaluated outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Eleven methods trained and evaluated both in-simulation (M_sim with p_eval_sim) and on human feedback (M_human with p_human); win-rate vs Davinci003 computed per method and compared; evaluation set of 805 instructions; method ranking correlation reported via Spearman rho and scatterplots (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8076.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8076.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>variance & overoptimization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Annotator variance comparisons and their impact on reward-model overoptimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantified annotator variance (average error of held-out annotation to majority vote) for humans, simulated evaluation annotator, simulated training annotator (with label flips), and single-prompt GPT-4; analyzed how variance (and injected label noise) affects reward-model overoptimization during LPF training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Investigating training dynamics (overoptimization) in LPF methods using different annotator models</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PREF split outputs used for reward-model training and LPF experiments (10k PREF split, eval subset for diagnostics)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>p_human (human annotators), p_eval_sim (simulated evaluation pool), p_ann_sim (simulated training annotator with label flips), p_GPT-4_sim (single low-variance GPT-4 prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>p_eval_sim: pool variance ~0.26; p_ann_sim: injected label flips producing variance ~0.43; human annotator variance ~0.35; single GPT-4 annotator variance ~0.1. (Variance measured as average error of a held-out annotation to majority vote over 3 annotators.)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Crowdworkers from Amazon Mechanical Turk (reference human variance = 0.35)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>annotator variance (average error to majority vote) and qualitative observation of overoptimization curves; no single scalar inter-rater correlation reported here (variance values used)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Low-variance single LLM annotators (e.g., single GPT-4 prompt) fail to reproduce human-like overoptimization; lack of inter-/intra-annotator variability leads to optimistic/incorrect training dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Human and AlpacaFarm (with multiple annotators + label noise) exhibit reward-model overoptimization (win-rate rises then falls as surrogate reward is optimized); single-prompt GPT-4 (low variance) shows no overoptimization, misleadingly suggesting methods can be optimized longer.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>When designed to match human variance (via annotator pool + noise), LLM simulators reproduce important qualitative training behaviors such as overoptimization, enabling safer method development.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Variance values: human = 0.35, p_eval_sim ≈ 0.26, p_ann_sim ≈ 0.43, single GPT-4 ≈ 0.1; label-flip ablation showed ~25% label flipping necessary to induce overoptimization; plotted win-rate vs surrogate reward for human, p_ann_sim, and p_GPT-4_sim (Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8076.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8076.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cost & throughput</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cost and speed comparison: simulated LLM judges vs human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported monetary cost per 1000 preference annotations and iteration time differences between AlpacaFarm simulated annotators and crowdworker human annotations, demonstrating orders-of-magnitude savings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Pairwise preference annotation for instruction-following outputs</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PREF split and evaluation sample annotations (examples counted per 1000 annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>p_eval_sim / p_ann_sim (simulated annotators via API LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Simulated evaluation annotator pool quoted cost: $12 per 1000 examples (≈25× cheaper than human at $300 per 1000) in one comparison; elsewhere training annotator variant p_ann_sim cost quoted as $6 per 1000 examples (≈50× cheaper) depending on batching and noise scheme; simulated annotators use API LLMs with batching to amortize in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>MTurk crowdworkers (paid median hourly rate $21; one-time $3000 cost for PREF split annotation; $242 recurring cost for evaluating a single model on 805 instructions stated as an example)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Cost advantage comes with potential simulator-specific biases; cost figures vary by annotator configuration and batching.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Simulated annotators yield large reductions in monetary cost and turnaround time, enabling more iterations; cost per 1000 annotations reported between $6 and $12 depending on setup versus ~$300 for human annotation in their pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Orders-of-magnitude cheaper; faster iteration; reproducible; supports batching/in-context example amortization for lower latency and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Cost estimates derived from API usage and MTurk rates described in methods; simulated annotations batched (GPT-4 batching up to 5 used in production, dev found up to 20 fits), sampling temperature 1.0 for annotator diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8076.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8076.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>sim-to-real transfer (fidelity-performance tradeoff)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation-to-human transfer: fidelity vs deployment performance when training on different simulated annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative comparison of human-evaluated win-rates for models trained on different annotator sources: human feedback, AlpacaFarm pooled annotator (p_ann_sim), and single low-variance GPT-4 annotator (p_GPT-4_sim); highlights a fidelity-performance tradeoff in simulator design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Single-turn instruction-following model training and human evaluation (win-rate vs Davinci003)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PREF split (10k) for pairwise feedback used during training; human evaluation on held-out evaluation instructions</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>p_human (human annotators), p_ann_sim (AlpacaFarm training annotator pool with 25% label flips), p_GPT-4_sim (single deterministic GPT-4 annotator)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>p_ann_sim: pool + 25% flip noise; p_GPT-4_sim: single GPT-4 prompt deterministic (temperature 0) used to maximize agreement and minimize variance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Crowdworkers from Amazon Mechanical Turk (majority-vote aggregation for evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>win-rate (%) measured by human annotation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Simulators designed for high fidelity (matching human variability) can produce models that perform worse when deployed without subsequent human fine-tuning; conversely, single consistent LLM annotators that maximize agreement can produce better direct-transfer models at cost of being less faithful to human distributional properties.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>PPO trained on human preferences achieved 55% human win-rate; PPO trained on single GPT-4 annotator achieved 50% human win-rate; PPO trained on AlpacaFarm annotator pool (p_ann_sim) achieved 43% human win-rate; SFT baseline ~44% — demonstrating a ~5–12% gap depending on simulator choice and revealing a fidelity vs direct-deployment-performance tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Ability to design simulators either for faithful method-development (match human variability) or for producing high direct-transfer performance (use a single consistent high-agreement annotator).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Compared human-evaluated win-rates of several PPO models: PPO_human = 55%, PPO_GPT-4_sim = 50%, PPO_ann_sim = 43%; evaluation against Davinci003 with human annotators; demonstrates transfer differences depending on simulated annotator design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An automatic evaluator of instruction-following models <em>(Rating: 2)</em></li>
                <li>G-Eval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 1)</em></li>
                <li>Thinking fast and slow with deep learning and tree search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8076",
    "paper_id": "paper-258865545",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "per-example agreement",
            "name_full": "Per-example agreement between simulated LLM judges and human majority vote",
            "brief_description": "Measured percent agreement between AlpacaFarm's simulated evaluation annotators (p_eval_sim: a pool of 13 API LLM annotators) and the human majority vote on pairwise preference labels; used to validate fidelity of LLM-as-judge at individual example level.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
            "evaluation_task": "Single-turn instruction following (pairwise preference judgments)",
            "dataset_name": "Alpaca PREF split / evaluation set (805 instructions composed from public eval datasets)",
            "judge_model_name": "p_eval_sim (pool of simulated annotators: GPT-4, ChatGPT, Davinci003 prompts)",
            "judge_model_details": "Pool of 13 simulated annotators implemented via OpenAI API: 5 GPT-4 prompts, 4 ChatGPT prompts, 4 Davinci003 prompts; varied prompt formats, in-context examples, batch sizes; sampling temperature 1.0 for annotator pool; evaluation pool excludes injected training flip-noise.",
            "human_evaluator_type": "Crowdworkers from Amazon Mechanical Turk (selected subset of annotators; majority vote over 3 human annotations used as reference in agreement computations)",
            "agreement_metric": "percent agreement with human majority vote",
            "agreement_score": 0.65,
            "reported_loss_aspects": "some simulator-specific biases (preference for first output in prompt; preference for outputs from same model as simulator); does not perfectly capture all human diversity",
            "qualitative_findings": "Simulated evaluator pool achieved a per-example agreement (~65%) similar to a held-out human annotator (~66%); single deterministic GPT-4 prompts could reach higher agreement (~68%) but lacked inter-/intra-annotator variability.",
            "advantages_of_llm_judge": "Much lower cost and faster annotation (orders-of-magnitude cheaper); reproducible and scalable; comparable per-example agreement to an individual human annotator.",
            "experimental_setting": "Agreement computed against majority vote of 3 human annotators on preference pairs; simulated pool used randomized output ordering and diverse prompts; reported agreement and cost comparisons (see costs).",
            "uuid": "e8076.0",
            "source_info": {
                "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "method-ranking correlation",
            "name_full": "Correlation of method-level rankings: simulated LLM evaluator vs human evaluation",
            "brief_description": "Comparison of overall method rankings (win-rates vs Davinci003) produced by AlpacaFarm's simulated evaluator and by human annotators, reported with Spearman correlation and R^2 as measures of agreement at the method level.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
            "evaluation_task": "Comparative evaluation of LPF (learning from pairwise feedback) methods via win-rate against Davinci003",
            "dataset_name": "805-instruction AlpacaFarm evaluation set (combined public eval datasets) and 200-demo Alpaca Demo subset for validation",
            "judge_model_name": "p_eval_sim (AlpacaFarm simulated evaluation) and p_human (human annotators)",
            "judge_model_details": "Simulated evaluation uses the 13-annotator pool (GPT-4/ChatGPT/Davinci003 prompts) without injected training label flips; human evaluation uses the selected MTurk annotators (majority vote aggregation).",
            "human_evaluator_type": "Crowdworkers from Amazon Mechanical Turk (16 selected from qualification; majority votes used for aggregation)",
            "agreement_metric": "Spearman rank correlation (method-level); also reported R^2 for some plots",
            "agreement_score": 0.98,
            "reported_loss_aspects": "A few rank mismatches (e.g., SFT10k vs SFT52k and ChatGPT vs PPO) where simulator and humans disagreed; simulator can still mis-rank some near-tie/close-performance cases.",
            "qualitative_findings": "Strong match in method rankings between simulation and human pipeline (Spearman 0.98), indicating AlpacaFarm is effective for method selection; some mismatches attributed to subtle biases (e.g., human preference for longer outputs) or evaluation prompt constraints.",
            "advantages_of_llm_judge": "Enables low-cost, rapid iteration and reliable method-level selection that transfers to human-evaluated outcomes.",
            "experimental_setting": "Eleven methods trained and evaluated both in-simulation (M_sim with p_eval_sim) and on human feedback (M_human with p_human); win-rate vs Davinci003 computed per method and compared; evaluation set of 805 instructions; method ranking correlation reported via Spearman rho and scatterplots (Figure 3).",
            "uuid": "e8076.1",
            "source_info": {
                "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "variance & overoptimization",
            "name_full": "Annotator variance comparisons and their impact on reward-model overoptimization",
            "brief_description": "Quantified annotator variance (average error of held-out annotation to majority vote) for humans, simulated evaluation annotator, simulated training annotator (with label flips), and single-prompt GPT-4; analyzed how variance (and injected label noise) affects reward-model overoptimization during LPF training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
            "evaluation_task": "Investigating training dynamics (overoptimization) in LPF methods using different annotator models",
            "dataset_name": "PREF split outputs used for reward-model training and LPF experiments (10k PREF split, eval subset for diagnostics)",
            "judge_model_name": "p_human (human annotators), p_eval_sim (simulated evaluation pool), p_ann_sim (simulated training annotator with label flips), p_GPT-4_sim (single low-variance GPT-4 prompt)",
            "judge_model_details": "p_eval_sim: pool variance ~0.26; p_ann_sim: injected label flips producing variance ~0.43; human annotator variance ~0.35; single GPT-4 annotator variance ~0.1. (Variance measured as average error of a held-out annotation to majority vote over 3 annotators.)",
            "human_evaluator_type": "Crowdworkers from Amazon Mechanical Turk (reference human variance = 0.35)",
            "agreement_metric": "annotator variance (average error to majority vote) and qualitative observation of overoptimization curves; no single scalar inter-rater correlation reported here (variance values used)",
            "agreement_score": null,
            "reported_loss_aspects": "Low-variance single LLM annotators (e.g., single GPT-4 prompt) fail to reproduce human-like overoptimization; lack of inter-/intra-annotator variability leads to optimistic/incorrect training dynamics.",
            "qualitative_findings": "Human and AlpacaFarm (with multiple annotators + label noise) exhibit reward-model overoptimization (win-rate rises then falls as surrogate reward is optimized); single-prompt GPT-4 (low variance) shows no overoptimization, misleadingly suggesting methods can be optimized longer.",
            "advantages_of_llm_judge": "When designed to match human variance (via annotator pool + noise), LLM simulators reproduce important qualitative training behaviors such as overoptimization, enabling safer method development.",
            "experimental_setting": "Variance values: human = 0.35, p_eval_sim ≈ 0.26, p_ann_sim ≈ 0.43, single GPT-4 ≈ 0.1; label-flip ablation showed ~25% label flipping necessary to induce overoptimization; plotted win-rate vs surrogate reward for human, p_ann_sim, and p_GPT-4_sim (Figure 5).",
            "uuid": "e8076.2",
            "source_info": {
                "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "cost & throughput",
            "name_full": "Cost and speed comparison: simulated LLM judges vs human annotators",
            "brief_description": "Reported monetary cost per 1000 preference annotations and iteration time differences between AlpacaFarm simulated annotators and crowdworker human annotations, demonstrating orders-of-magnitude savings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
            "evaluation_task": "Pairwise preference annotation for instruction-following outputs",
            "dataset_name": "PREF split and evaluation sample annotations (examples counted per 1000 annotations)",
            "judge_model_name": "p_eval_sim / p_ann_sim (simulated annotators via API LLMs)",
            "judge_model_details": "Simulated evaluation annotator pool quoted cost: $12 per 1000 examples (≈25× cheaper than human at $300 per 1000) in one comparison; elsewhere training annotator variant p_ann_sim cost quoted as $6 per 1000 examples (≈50× cheaper) depending on batching and noise scheme; simulated annotators use API LLMs with batching to amortize in-context examples.",
            "human_evaluator_type": "MTurk crowdworkers (paid median hourly rate $21; one-time $3000 cost for PREF split annotation; $242 recurring cost for evaluating a single model on 805 instructions stated as an example)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "Cost advantage comes with potential simulator-specific biases; cost figures vary by annotator configuration and batching.",
            "qualitative_findings": "Simulated annotators yield large reductions in monetary cost and turnaround time, enabling more iterations; cost per 1000 annotations reported between $6 and $12 depending on setup versus ~$300 for human annotation in their pipeline.",
            "advantages_of_llm_judge": "Orders-of-magnitude cheaper; faster iteration; reproducible; supports batching/in-context example amortization for lower latency and cost.",
            "experimental_setting": "Cost estimates derived from API usage and MTurk rates described in methods; simulated annotations batched (GPT-4 batching up to 5 used in production, dev found up to 20 fits), sampling temperature 1.0 for annotator diversity.",
            "uuid": "e8076.3",
            "source_info": {
                "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "sim-to-real transfer (fidelity-performance tradeoff)",
            "name_full": "Simulation-to-human transfer: fidelity vs deployment performance when training on different simulated annotators",
            "brief_description": "Quantitative comparison of human-evaluated win-rates for models trained on different annotator sources: human feedback, AlpacaFarm pooled annotator (p_ann_sim), and single low-variance GPT-4 annotator (p_GPT-4_sim); highlights a fidelity-performance tradeoff in simulator design.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
            "evaluation_task": "Single-turn instruction-following model training and human evaluation (win-rate vs Davinci003)",
            "dataset_name": "PREF split (10k) for pairwise feedback used during training; human evaluation on held-out evaluation instructions",
            "judge_model_name": "p_human (human annotators), p_ann_sim (AlpacaFarm training annotator pool with 25% label flips), p_GPT-4_sim (single deterministic GPT-4 annotator)",
            "judge_model_details": "p_ann_sim: pool + 25% flip noise; p_GPT-4_sim: single GPT-4 prompt deterministic (temperature 0) used to maximize agreement and minimize variance.",
            "human_evaluator_type": "Crowdworkers from Amazon Mechanical Turk (majority-vote aggregation for evaluation)",
            "agreement_metric": "win-rate (%) measured by human annotation",
            "agreement_score": null,
            "reported_loss_aspects": "Simulators designed for high fidelity (matching human variability) can produce models that perform worse when deployed without subsequent human fine-tuning; conversely, single consistent LLM annotators that maximize agreement can produce better direct-transfer models at cost of being less faithful to human distributional properties.",
            "qualitative_findings": "PPO trained on human preferences achieved 55% human win-rate; PPO trained on single GPT-4 annotator achieved 50% human win-rate; PPO trained on AlpacaFarm annotator pool (p_ann_sim) achieved 43% human win-rate; SFT baseline ~44% — demonstrating a ~5–12% gap depending on simulator choice and revealing a fidelity vs direct-deployment-performance tradeoff.",
            "advantages_of_llm_judge": "Ability to design simulators either for faithful method-development (match human variability) or for producing high direct-transfer performance (use a single consistent high-agreement annotator).",
            "experimental_setting": "Compared human-evaluated win-rates of several PPO models: PPO_human = 55%, PPO_GPT-4_sim = 50%, PPO_ann_sim = 43%; evaluation against Davinci003 with human annotators; demonstrates transfer differences depending on simulated annotator design.",
            "uuid": "e8076.4",
            "source_info": {
                "paper_title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "AlpacaEval: An automatic evaluator of instruction-following models",
            "rating": 2
        },
        {
            "paper_title": "G-Eval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 1
        },
        {
            "paper_title": "Thinking fast and slow with deep learning and tree search",
            "rating": 1
        }
    ],
    "cost": 0.01599975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback
8 Jan 2024</p>
<p>Yann Dubois yanndubs@stanford.edu 
Stanford Xuechen Li 
Stanford Rohan Taori 
Stanford Tianyi Zhang 
Stanford Ishaan Gulrajani 
Jimmy Ba 
Carlos Guestrin 
Percy Liang 
Tatsunori B Hashimoto 
Dev Model </p>
<p>University of Toronto</p>
<p>Baseline Model API LLMs Feedback Source Experiment Cost Iteration time Reproducible</p>
<p>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback
8 Jan 2024832D95CD10C9D4BDBD337941AAEAEBC0arXiv:2305.14387v4[cs.LG]
Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their strong instruction following abilities.Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback.Replicating and understanding this instruction-following requires tackling three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations.We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost.First, we design LLM prompts to simulate human feedback that are 50x cheaper than crowdworkers and display high agreement with humans.Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions.Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback.Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of real human feedback and show that the rankings of models trained in AlpacaFarm match the rankings of models trained on human data.As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% improvement in win-rate against Davinci003.We release all components of AlpacaFarm at https://github.com/tatsu-lab/alpaca_farm.</p>
<p>Introduction</p>
<p>Large language models (LLMs) [9,12,45] have demonstrated unprecedented capabilities in following diverse and open-ended instructions [46,5,37].These achievements have often been attributed to the fine-tuning of pretrained LLMs using human feedback, but this process remains poorly understood due to the lack of published information on the training methods from LLM vendors.For example, it was recently revealed that only the Davinci003 model in the instruct series of OpenAI models used reinforcement learning (RL) with the PPO algorithm [44], leading some to question the importance of RL in the training process given that Davinci002 already worked very well.Understanding and improving these methods requires open and transparent replications of the training process, but this remains challenging due to the cost and complexity associated with methods that learn from human feedback.</p>
<p>Our goal is to facilitate research and development on instruction following models and methods that learn from human feedback.We identify three main challenges: the high cost of data annotation, the lack of automated evaluation for model development, and the absence of validated implementations of existing methods.To address these three challenges, we introduce AlpacaFarm (Figure 1), a simulation sandbox that enables experimentation at low cost.Using AlpacaFarm, researchers can rapidly iterate on method development in simulation and transfer these insights to build high-performance systems with actual human feedback.</p>
<p>For the first challenge of data annotation costs, AlpacaFarm simulates human annotators with (oracle) API LLMs that are faster and cheaper.To collect simulated feedback data, we design prompts for oracle API LLMs (e.g.GPT-4) that enable us to simulate human pairwise comparisons at a cost that is 45x cheaper than crowdworkers, and tune these prompts to faithfully capture many aspects of human annotators, such as their quality judgments, inter-annotator variability, and stylistic preferences.</p>
<p>For the second challenge of automated evaluation, we designed an automatic evaluation protocol aiming to quantify system performance on simple yet realistic real-world human instructions.</p>
<p>Improving evaluations of open-ended instruction following has been challenging due to the cost and non-replicability of human evaluation, the lack of real human interaction data, and the diversity of natural human instructions.To address this, we used the aforementioned simulated annotators to compute the win-rate between outputs from the evaluated model and those from a baseline LLM on a set of simple yet realistic instructions.To select the instructions, we used a set of real-world user interactions (Alpaca Demo [65]) as reference instructions.As those cannot be released directly due to confidentiality constraints, we combined existing public evaluation datasets to create a set of instructions that mimic the Alpaca Demo evaluation set.Quantitative evaluations of system rankings on our evaluation data show a high correlation with system rankings on the Alpaca Demo instructions.</p>
<p>For the third challenge of missing reference implementations, we implement and test several popular learning algorithms including PPO [58], expert iteration [2], and Quark [39], and release reference implementations.We show that, among the methods we studied, PPO with a surrogate reward model is the most effective training-time method, improving the win-rate against Davinci003 of an instruction fine-tuned LLaMA 7B model from 44% to 55%.Other baselines that have been validated on simpler tasks fall short in comparison, highlighting the importance of testing these algorithms in a real instruction-following setting.</p>
<p>As an end-to-end evaluation of the AlpacaFarm, we compare eleven methods trained and evaluated in AlpacaFarm with the same methods trained and evaluated on actual human feedback.We show that the method rankings obtained from developing on AlpacaFarm closely agree with the method rankings obtained from training on actual human data (Spearman correlation of 0.98) and that the best method in AlpacaFarm leads to substantial gains with human feedback.Finally, we find that AlpacaFarm can replicate qualitative behaviors of human feedback such as over-optimization of the reward model, suggesting that AlpacaFarm serves as an effective way for researchers to rapidly study and develop methods for training instruction-following LLMs using human feedback.</p>
<p>Background &amp; problem statement</p>
<p>To begin, Section 2.1 introduces the instruction following task and the pairwise-comparison-based human feedback setting that we study.With this background, we formally define in section 2.2 the goals of developing a low-cost simulator for studying instruction following and learning from human feedback.</p>
<p>Learning to follow instructions</p>
<p>In the instruction following task [46,6,72,37], we are presented with user instructions x ∈ X (e.g."Tell me something about alpacas"), and our goal is to develop a model p θ that generates high-quality responses y ∼ p θ (y | x) as judged by an unobserved human reward function R : X × Y → R.</p>
<p>While there are a rich set of methods that learn from human feedback to directly optimize R (see Section 6), in this work we focus on the setting of learning from pairwise feedback (LPF) due to its central role in recent instruction-following LLMs [46].The starting point of this process is a model that is fine-tuned on instruction-following demonstrations (x, y), which we denote as p SFT θ (y | x).The LPF process then involves taking pairs of samples y 0 , y 1 from p SFT θ for each instruction x, querying humans for which sample within each pair is better, and learning from this pairwise feedback.Since all methods start from the SFT base, we use p θ for notational simplicity.</p>
<p>Learning from pairwise feedback (LPF).More formally, we define the pairwise feedback dataset as D pairwise = {(x (j) , y (j) 0 , y (j) 1 , z (j) )} j .In this notation, a human annotator rates two candidate responses y 0 , y 1 ∼ p θ (y | x) for the instruction x.These binary ratings z ∈ {0, 1} are assumed to be generated according to their unobserved reward R and z indicates a (potentially stochastic) comparison for the better response y z , where R(x, y z ) &gt; R(x, y 1−z ).</p>
<p>Many algorithms have been proposed to learn on D pairwise .Some algorithms like RLHF [16,46] learn a surrogate reward function as the learning signal and some operate more directly on D pairwise .We defer the discussion of different learning algorithms to Section 3.4.</p>
<p>Pairwise evaluation.Once instruction-following models are trained, researchers need to evaluate these models.One common approach for evaluating models is pairwise model evaluation [14,20,7], which performs pairwise comparisons on outputs generated by the model p θ and a reference model p ref similarly to the pairwise feedback process.Concretely, we collect pairwise preference for two models ({(x (j) , y
(j) θ , y (j) ref , z (j) )} j ),
which is aggregated by computing the average win-rate -the percentage of times p θ is preferred to p ref .</p>
<p>Researchers can then compare LPF models by their win-rates against the same reference model.</p>
<p>Problem statement</p>
<p>The goal of AlpacaFarm is to provide three key components that enable rapid research and development of instruction following models: low-cost pairwise feedback generators, automated evaluations for methods development, and reference implementations for comparison and modification.With these three components, researchers can develop new methods in simulation and transfer these insights to build high-performance systems on actual human feedback.</p>
<p>For pairwise feedback, we substitute human preference judgements z human ∼ p human (z | x, y 0 , y 1 ) with a simulated preference z sim ∼ p sim (z | x, y 0 , y 1 ) using oracle API LLMs.Our goal is to construct a p sim that is both low-cost and faithfully captures different aspects of human preference feedback, such as quality judgments, inter-annotator agreement rates, and stylistic preferences.</p>
<p>For evaluations, we evaluate system outputs using the pairwise preference simulator and identify evaluation datasets that reflect natural human-LLM interactions.The goal for our evaluations is to ensure that system rankings on the new evaluation dataset closely match human rankings on a realistic set of instructions.We use the Alpaca Demo [46] as reference for real-world instructions.</p>
<p>For reference methods, we develop and evaluate six LPF methods.Our goal will be to provide simple and working implementations that provide substantial improvements on both simulated and human feedback data.This will allow researchers to build upon and compare to competitive baselines in a complex instructionfollowing environment.</p>
<p>AlpacaFarm combines these three components into a simulation framework for learning from pairwise feedback.We evaluate the complete system by an end-to-end workflow of developing methods in simulation and transferring the insights to the real world.Concretely, we will run each method M on the simulated preferences (called M sim ) and evaluate with simulated rankings p sim .In parallel, we will run M on human preferences (called M human ) and evaluate with human rankings p human .We consider AlpacaFarm to be successful if the simulated method rankings correlate well with the human method rankings.Note that the goal of AlpacaFarm is not to train a good model with simulated data, instead it is to analyze methods, i.e., algorithms and hyperparameters, with simulated data and transfer the insights to the real world.</p>
<p>The rest of this work will present the details of the pairwise feedback and evaluation design (Section 3), evaluate these designs (Section 4), and analyze the different reference methods we implemented in the AlpacaFarm (Section 5).</p>
<p>Constructing the AlpacaFarm</p>
<p>In this section, we detail how we construct the AlpacaFarm.In Section 4, we then validate our design choices by comparing the LPF workflow with human feedback and evaluation.</p>
<p>Instruction following data</p>
<p>Before defining the details of how we simulate pairwise feedback, we must first specify a large and diverse set of instructions x upon which we can build the rest of AlpacaFarm.We opt to use the Alpaca data [65] as a starting point due to its large size (52k (x, y) examples) and the non-trivial instruction following capabilities of models trained on this data.</p>
<p>We repurpose the Alpaca data into splits suitable for learning from human feedback methods by following a similar data splitting ratio as [46].We created four splits (42k in total), leaving 10k for the future:</p>
<p>• Supervised finetuning (SFT) split: 10k data for fine-tuning the base instruction-following LLM used in subsequent steps.• Pairwise preference (PREF) split: 10k instructions on which we will collect pairwise feedback data.</p>
<p>• Unlabeled split: 20k unlabeled instructions used in algorithms such as PPO.</p>
<p>• Validation split: 2k data for development and tuning.</p>
<p>Designing simulated pairwise preference p sim</p>
<p>Equipped with the Alpaca instruction data, we now describe the design of our simulated annotator for pairwise preferences.Our core proposal is to design the simulator p sim (z | x, y 0 , y 1 ) by prompting OpenAI API LLMs.While using LLMs as a proxy for annotators has become increasingly popular [14,36], using LLMs as part of a simulation environment poses major additional challenges.Our simulated preferences p sim must not only have a high agreement with human preferences p human , it must also capture other qualitative aspects of human feedback such as inter-and intra-annotator inconsistencies.Intuitively, the noise and variability in pairwise feedback are key parts of the challenge in the LPF problem and we find that ignoring these factors leads to a simulator that diverges substantially from real-world behavior (Section 4.3).Indeed, when training on sampled preference the variability of the preference may affect the LPF training dynamics.</p>
<p>Basic GPT-4 prompt design.To start with, we design prompts by providing a guideline of appropriate responses, feeding in-context examples, and leveraging batch generation to save on costs.As a first baseline, we query GPT-4 with a single prompt (denoted as p GPT-4  sim ) and we find p GPT-4   sim has an agreement rate with human annotators that is similar to the agreement rate between humans (65% vs 66%; see results in Section 4.3).However, we find that this simple baseline of p GPT-4 sim fails to capture the variability in human annotation and can lead to qualitatively different results for method development, especially for reward over-optimization (Section 4.3).</p>
<p>Simulating human variability.To better emulate human annotators, we modify the basic simulated annotator design to capture annotator variability in two ways.First, we emulate inter-annotator variability in the simulated pairwise preference p sim by mimicking a pool of annotators.We design different annotators by querying different API LLMs and varying the prompts with different formats, batch sizes, and in-context examples.In the end, we created 13 simulated annotators which we describe fully in Appendix C. Second, we emulate intra-annotator variability by flipping the simulated preference 25% of the time, i.e., we inject random noise.</p>
<p>With these ingredients, we come up with a simulated preference p ann sim that meets our requirement of agreement and variability.Overall, annotating 1000 outputs using simulated preference only costs $6, which is 50x Discuss the causes of the Great Depression Make a list of desirable Skills for software engineers to add to LinkedIn.Are there any free SAST tools out there?I'm trying to teach myself to have nicer handwriting.Can you help?What if Turing had not cracked the Enigma code during World War II?Take MLK speech "I had a dream" but turn it into a top 100 rap song What are some toys I can buy my kids for imaginative play?Hi, I have a question about MFCC (mel frequency cepstral coefficients).Are they the same thing as a MEL-spectogram, or is there a difference?cheaper than human annotation.In Section 4, we collect actual human preference and quantitatively verify the agreement and variability of our simulated preference.</p>
<p>Designing an automatic evaluation</p>
<p>For researchers to develop LPF methods in the AlpacaFarm, we want to support them with an automatic evaluation so they can quickly iterate while reliably comparing methods.To replace the usual human evaluation, there are two challenges.First, how do we quantify the quality of the output of different models?Second, what instructions can we use that are representative of human interactions?</p>
<p>Evaluation protocol.To quantify the quality of an LLM p θ , we measure the win-rate of that LLM against a reference model, i.e, the expected number of times that the output from p θ is preferred to the output of a reference model p ref on some instruction x.The benefits of using simulated win-rates are that it provides a metric that is easy to understand, is comparable across methods conditioned on a single reference model, and can reuse the routine we built for pairwise feedback.We use the 13 simulated annotators described in Section 3.2 without injecting the additional noise (as adding uniform noise does not change model rankings) and denote this preference simulator as p eval sim .For the reference model, we use Davinci003 as it is a well-studied system that performs similarly to the models we fine-tune.</p>
<p>Evaluation instructions.Instruction following requires diverse coverage over realistic interactions.To build an appropriate evaluation protocol, we combine several open-source evaluation datasets and use real-world interactions with a demo instruction-following LM (Alpaca Demo [65]) as guidance for constructing our data combination.Due to privacy considerations, we do not directly release the demo data and opt to use it to guide how we combine existing open evaluation datasets.</p>
<p>We construct an evaluation of 805 instructions, which includes 252 instructions from the self-instruct test set [71], 188 from the Open Assistant (OASST) test set, 129 from Anthropic's helpful test set [6], 80 from Vicuna test set [76,15], and 156 from Koala test set [20].In Table 1 and Figure 2, we show example instructions from our evaluation dataset and their root verb distribution, which shows the diverse coverage of our evaluation data.We find that combining across datasets is important for automatic evaluation to match real-world interactions, as discussed in Section 4.4.</p>
<p>Reference methods in AlpacaFarm</p>
<p>Finally, AlpacaFarm defines a collection of validated LPF methods for instruction following.We leave a more detailed methods description to Appendix A and provide a brief overview here.In all LPF methods that follow, we begin by first performing an initial fine-tuning step on supervised data consisting of instructions and outputs.We begin by describing three baselines that directly operate on pairwise feedback.</p>
<p>• Binary FeedME.Binary FeedME [44] continues supervised fine-tuning on the preferred output in each pairwise comparison.• Binary reward conditioning.Reward conditioning [26,34,28] is a simple scheme that incorporates learning from negative (non-preferred) examples; a token denoting whether the output was preferred is prepended before fine-tuning, and the positive token is used to condition at inference time.• Direct preference optimization.Direct Preference Optimization (DPO) [52] maximizes the log-likelihood of preferences under the Bradley-Terry model w.r.t. a reward model defined implicitly by an "optimal" policy under the reward model given a reference policy model.</p>
<p>Figure 3: The ranking of methods trained and evaluated in AlpacaFarm matches that of methods trained and evaluated in the human-based pipeline.Each point represents one method M (e.g.PPO).The x-axis shows the simulated evaluation (win-rates measured by p eval sim ) on methods trained in simulation M sim .The y-axis shows human evaluation (win-rates measured by p human ) on methods trained with human feedback M human .Gray points show models that we did not train, so their x and y values only differ in the evaluation (simulated vs human).Without those points, we have R 2 = 0.83 and a Spearman Correlation of 0.94.</p>
<p>Many LPF methods do not directly operate on pairwise feedback data, but instead first construct a surrogate reward model by fine-tuning a classifier from the SFT base using pairwise feedback.The following LPF methods maximize the continuous-valued reward defined by the logits of this classifier.</p>
<p>• Best-of-n sampling.Best-of-n (or re-ranking) [63,5,21,8] is a simple but effective inference-time method that draws n i.i.d.responses from the SFT model and returns the response with the highest surrogate reward.</p>
<p>Unless stated otherwise, we use n = 1024 in our experiments.</p>
<p>• Expert iteration.Expert iteration [2,60,70] is the natural training-time extension of best-of-n: it first generates according to best-of-n on new instructions and then fine-tunes on the best outputs.</p>
<p>• Proximal Policy Optimization (PPO).PPO [24,58] is a popular reinforcement learning algorithm that maximizes surrogate reward, subject to a KL penalty keeping parameters near the SFT initialization.</p>
<p>• Quark.We use the top-quantile variant of Quark [39], which bins examples by reward and trains on the best bin, while minimizing the KL divergence between the trained model and the SFT initialization, and maximizing the model's entropy.</p>
<p>Validating the AlpacaFarm simulator</p>
<p>With the simulator and methods defined, we can now evaluate AlpacaFarm.As our main result, in Section 4.2 we analyze the correlation between the final rankings of methods in both the simulated LPF workflow and human-based LPF.Afterward, we will analyze the detailed design choices within the simulator by identifying whether our pairwise feedback accurately mimics human pairwise feedback (Section 4.3) and whether rankings on our evaluation data match rankings on the Alpaca Demo data (Section 4.4).</p>
<p>Experimental details</p>
<p>Models.As a baseline and starting point for LPF methods, we fine-tuned LLaMA 7B [68] on the 10k SFT split.We take SFT 10k to be the starting point for all LPF methods and collect the simulated preference p ann sim and human preference p human from SFT 10k's outputs (with temp=1.0)on the 10k instruction PREF split.Then, for each of the six reference LPF methods M :</p>
<p>• We trained and tuned M on simulated preferences p ann sim , evaluating the resulting model M sim against the Davinci003 reference with the simulated evaluator p eval sim .• We trained some of the models M on human preferences across hyperparameter ranges identified in simulation, evaluating the resulting model M human against Davinci003 with humans p human .</p>
<p>In addition to the six methods, we also evaluate existing standard instruction following and base models: GPT-4 (gpt-4-0314), ChatGPT (gpt-3.5-turbo-0301),Davinci001 (text-davinci-001), LLaMA 7B [68], and Alpaca 7B [65].Alpaca 7B is a LLaMA 7B model finetuned on the concatenation of all data splits, denoted SFT 52k.For these models, we measure both the simulated win-rate and human win-rate using p eval sim and p human respectively.</p>
<p>Annotator:</p>
<p>Human</p>
<p>Human mode agreement</p>
<p>Figure 4: Our simulated annotators are cheap and have a high agreement with human annotators.We show price (xaxis) vs agreement (y-axis) as measured by each annotator's agreement with the majority vote among 3 human annotations.Grey points are all simulated annotators in the pool, the green shows the resulting pool of annotators (used for evaluation), the orange shows the same pool with random noise added during training.This does not change the implied reward function from , but makes the learning problem more challenging.The blue shows the average of human annotators, and the red shows a single low variance GPT-4 annotator analyzed below.</p>
<p>At inference time, for all systems except best-of-n, we sample with temperature 0.7 and set the maximum number of tokens to be 300.For best-of-n sampling, we found a higher temperature to be helpful in encouraging output diversity, and so we rerank samples from SFT 10k with temperature 1.0.We provide more thorough experimental details and hyperparameters for all methods in Appendix B.</p>
<p>Human annotation.We collected reference human annotation by showing crowdworkers two potential outputs y 0 or y 1 for a given instruction x and asked them to select the index z ∈ {0, 1} of their preferred output.Annotators are recruited from Amazon Mechanical Turk using a qualification test of 25 questions.Out of an initial pool of 34 annotators, we selected the 16 whose agreement rate was higher than 70% with the authors' annotations.We paid the annotators a median hourly rate of $21, leading to a one-time $3000 cost of annotating our PREF split and a recurring $242 cost for evaluating a single model on the 805 evaluation instructions.See Appendix D for additional details including the annotation interface we provided.</p>
<p>End-to-end validation of AlpacaFarm</p>
<p>We now analyze the correlation between rankings in simulation and on human data.Figure 3 shows the win-rate of methods in AlpacaFarm (x-axis) with the win-rate from the human-based pipeline (y-axis).We see that the rankings have a Spearman correlation of 0.98, which suggests that AlpacaFarm faithfully captures the rankings among different LPF methods.This enables researchers to develop models in the low-cost AlpacaFarm environment and transfer these insights to train models on real-world human interactions.</p>
<p>Inspecting these results more closely, we point out the two rank mismatches.The first comparison is SFT10k against SFT52k, where human annotators preferred SFT10k (44.3% vs 40.7%) while the simulator had the opposite preference (36.7% vs 39.2%, Table 2).The other mismatch is ChatGPT against PPO, where human annotators preferred PPO to ChatGPT (55.1% vs 52.9%) unlike the simulator annotators which preferred ChatGPT to PPO (46.8% vs 61.4%).In both cases, these are not major mistakes, as we do not expect SFT52k to be much worse than SFT10k or for a 7B LLaMA model to substantially outperform ChatGPT.</p>
<p>Validating the pairwise preferences component</p>
<p>Having demonstrated that AlpacaFarm succeeds at the end-to-end validation of methods rankings, we now take a closer look at our pairwise preferences, showing that they have a high agreement with human annotators and that they replicate important qualitative features of model training.For additional details see appendix C. Simulated annotators match human agreement.We begin by computing agreement levels between our simulated annotator and a majority vote of 3 human annotators, comparing this to the agreement level of a held-out human annotator, as shown in Figure 4. We find that our evaluator p eval sim (green) has a 65% agreement rate with the human majority vote, which is similar to the held-out human agreement rate at 66% (blue).At the same time, p eval sim is 25× cheaper ($300 → $12 per 1000 examples).The training time annotator p ann sim (yellow) has lower agreement due to label flip noise but this does not mean that p ann sim is less faithful to human annotations, since this noise is unbiased and both annotators (p ann sim , p eval sim ) represent the same underlying preference function.sim .For each plot, the x-axis measures the average surrogate rewards on the eval set.Human and AlpacaFarm preferences result in over-optimization, while simple GPT-4 preference does not.Figure 4 also shows that we identified a few single prompts performing even better than p eval sim , with one of the GPT-4 prompts achieving 68% agreement.While this high agreement level is impressive, we do not use single prompts directly for AlpacaFarm, as single prompts do not replicate the inter-annotator variability important for a simulator.Instead, randomizing over different annotators and injecting additional noise is needed to match the distributional features and learning dynamics of human data, which we discuss next.</p>
<p>Simulated annotators replicate overoptimization.We now show that modeling annotator variability in the simulated annotator (p ann sim ) is necessary to capture important qualitative features of LPF model training.To do so, we compare the behavior of three of the best-performing models trained under p ann sim with those trained using the single GPT-4 prompt p GPT-4  sim , which has higher human agreement but little inter-and intra-annotator variability.Figure 5 shows the learning dynamics of these models for pairwise feedback by p human (left), p ann sim (middle), and p GPT-4 sim (right) as the PPO iteration count and rerank sample counts for best-of-n and expert iteration are increased.For both the human and AlpacaFarm preferences, models that more effectively optimize the surrogate reward (x-axis) improve in win-rate (y-axis) up until a point, where overoptimization of the reward occurs and win-rates decrease.In contrast, simple GPT-4 feedback shows no overoptimization, leading to the false conclusion that the LPF methods can be optimized for much longer than is actually the case.For example, Figure 5 right shows best-of-1024 to be much better than PPO, which strongly disagrees with the results on human data.Noted in prior work [19], overoptimization is the result of the proxy reward model Rϕ being an imperfect estimate of the true reward R. We hypothesize that the strong overoptimization seen for human annotators is partly due to the (inter-and intra-) variability of their annotations, which degrades the quality of the reward model.To test this hypothesis, we measured the variance of each of the three annotators by calculating the average error of a held-out annotation to the majority vote over 3 random draws.At 0.26 for p eval sim and 0.43 for p ann sim , AlpacaFarm is close to the human variance of 0.35; in contrast, the GPT-4 annotator has a much lower variance at 0.1.Finally, in Appendix E.1 we ablate the AlpacaFarm design more finely and find that overoptimization is strongest when using both label noise and 13 annotators, but also seems present when using only label noise.Appendix C contains further analyses of bias and variability of annotators.</p>
<p>Validating the evaluation protocol</p>
<p>Finally, we test our evaluation instructions that combines existing open-source evaluation datasets.While we have observed that this dataset is diverse (see Figure 2), it is unclear whether it evaluates performance for any type of real-world human usage.To resolve this, we measure method-level correlations against a set of real user interactions recorded on the Alpaca Demo [65].We manually went through the interactions and identified 200 instructions that do not contain any personal identifying information, toxic or unsafe questions, and those that refer to the chatbot directly (e.g."who are you developed by?").The terms of use for the demo do not allow us to publicly release this data, but we use this data to evaluate the proposed evaluation set.</p>
<p>We use the same 11 systems as displayed in Figure 3, with the LPF methods trained in simulation, and evaluate them using p eval sim .Figure 6 plots the simulated win-rates on the Demo instructions against those on the AlpacaFarm evaluation data.The two win-rates are strongly correlated (r 2 = 0.97), indicating that AlpacaFarm evaluation data can serve as a proxy for evaluating methods on simple demo interactions.</p>
<p>Benchmarking reference methods on the AlpacaFarm</p>
<p>We now study the performance of reference methods in AlpacaFarm.Table 2 contains the details of the main evaluation results (presented graphically in Figure 3).In the rest of this section, we discuss our findings from these results, demonstrating that the conclusions we reach using human feedback could have been derived using AlpacaFarm at a substantially lower cost.See appendix B for experimental details.</p>
<p>Comparing LPF methods</p>
<p>Supervised fine-tuning is highly effective.Table 2 shows that the SFT step is effective and provides the majority of the win-rate gains.SFT brings the base LLaMA model up from a simulator win-rate of 11% to 37% and a human win-rate of 7% to 44%.However, we observe little to no gain from SFT 10k to SFT 52k.</p>
<p>PPO tops the LPF leaderboard.Among the LPF methods we study, PPO performs the best in both the simulator (47%) and on human feedback data (55%).Notably, with a win-rate of 55%, the PPO trained with human feedback was preferred to ChatGPT by our human annotators for single-turn instruction-following.This surprising result is likely because ChatGPT was prompted for responses shorter than 1000 characters to fit in the simulated annotators' context size, and both crowdworkers and simulators annotators preferred longer responses (see appendix C.2).For example, in AlpacaEval [33] (slightly different simulated annotator) we show that the unconstrained ChatGPT achieves 89% win-rate, much higher than the 44% win-rate for the PPO model.PPO: The five most common causes of stress among people are: 1. Pressure at work: Whether it is due to tight deadlines, high expectations, or an overwhelming workload, pressure at work can be a major source of stress.2. Money problems: Financial issues can be a significant source of stress, from not being able to make ends meet to struggling with debt.3. Unhealthy lifestyle: Poor diet, lack of exercise, and sleep deprivation can all contribute to stress.4. Unhappiness at home: Problems in relationships, conflicts with family members, and other domestic issues can be a major source of stress.5. Uncertainty: Excessive worry and anxiety about the future, health issues, or other unknowns can be a major cause of stress.</p>
<p>Best-of-n is simple and competitive.Best-of-n sampling outperforms all LPF methods besides PPO.The competitive performance of best-of-n sampling points to the useful learning signals captured by the reward model and helps us understand how training-time LPF methods can benefit from a reward model.</p>
<p>Expert Iteration and Quark lag behind.Even though expert iteration trains with best-of-n outputs, it trails best-of-n with a 3-6% win-rate gap in both simulated and human feedback workflows.This result suggests that acquiring the improvement from best-of-n sampling is not straightforward and points to why more complex learning algorithms like PPO might be useful.For Quark, although the rewards of samples improve during training, the resulting models did not outperform the SFT baseline.</p>
<p>Methods that directly learn from pairwise feedback do not perform well.We observe that binary reward conditioning and binary FeedME do not improve over the SFT 10k baseline.This result suggests that learning a surrogate reward model may be an important ingredient to LPF.</p>
<p>Computational cost.While our primary focus is the win-rate of LPF methods, we also measured the computational costs of each reference method, generally finding the training process to take &lt; 2 hrs for most methods on a single 8×A100 machine.The largest computational costs involved decoding a large number of samples for both Best-of-n and Expert Iteration (See Appendix E.2 for details).</p>
<p>Analysis of model outputs</p>
<p>PPO and best-of-n reranking both demonstrate substantial gains with both AlpacaFarm's simulated feedback and human feedback.While the changes to the win-rate are clear, it is less clear what changes to the model lead to these improvements -are the gains driven by improvements to factuality or reasoning, or are they driven by stylistic differences?We now focus on an analysis of the outputs of these systems to understand what factors contribute to the improvement in their win-rates.</p>
<p>We find a major difference in the length distributions of the outputs, with outputs becoming increasingly longer after applying LPF methods in both human and simulated feedback.For example, the average length of SFT 10k outputs is 278 characters.Best-of-16 increases the average length to 570 characters and applying PPO increases it to 637 tokens.In Table 3, we show a qualitative example of how PPO training changes the output for an evaluation instruction, with further quantative details and analysis in Appendix E.3.</p>
<p>Using AlpacaFarm to train models directly for human deployment</p>
<p>The main goal of AlpacaFarm is to provide a simulator to enable the development of methods that would then be trained on human feedback before deployment.A natural question is whether one can use AlpacaFarm to train models on simulated preferences that directly perform well on human evaluation, without having to retrain on human preferences.We show that AlpacaFarm can be repurposed for this goal once the pairwise feedback simulator is modified to maximize agreement rather than match human annotator variability.</p>
<p>To illustrate this point, we compare the best PPO model (step 40) trained in AlpacaFarm p ann sim with a matched PPO model (step 30) trained on the single low-variance GPT-4 annotator p GPT-4  sim .We then measure their win-rate according to human preference evaluation, p human .The results are displayed in Table 4.We find that PPO ann sim trained in AlpacaFarm only achieves a win-rate of 43%, while PPO GPT-4   sim trained on GPT-4 data achieves a win-rate of 50%.To contextualize these results, the initial SFT model has a win-rate of 44%, PPO human has a win-rate of 55%, and the best non-PPO human method has a win-rate of 51% (Best-of-16).Thus, training in simulation can provide good models directly for deployment, though this approach suffers a 5% performance gap relative to collecting real human annotations.</p>
<p>These results demonstrate a faithfulness-performance tradeoff in simulator design: more faithful simulators, which display greater over-optimization, train objectively worse models.The standard AlpacaFarm pairwise evaluators are best suited for developing new methods and performing method selection in the simulator, as Figure 3 demonstrates that the ranking of methods remains the same when they are re-trained on human preferences.However, for directly deploying models trained in the simulator, a single consistent annotator such as p GPT-4   sim can provide significant gains on real-world evaluation.</p>
<p>Related work</p>
<p>Instruction following.There have been an number of works studying instruction following as a cross-task generalization across a pool of NLP tasks [41,73,54,4,72].Our work focuses on a recent trend in instruction following methods which increasingly focus on real world human interaction patterns [46,6], rather than collections of existing NLP benchmarks.For example, InstructGPT was developed on user instructions submitted to OpenAI API [46].Our work builds upon these works by attempting to bridge the gap between the ease of development and evaluation of traditional academic benchmarks and the more complex algorithms and real-world settings of recent works on instruction following.</p>
<p>Simulating human feedback.Constitutional AI [7] simulates human feedback with AI feedback for model development to improve harmlessness and helpfulness.AlpacaFarm, on the other hand, simulates human feedback with oracle API LLMs so that simulated experiments reflect the outcomes of experiments performed with real human feedback.Due to the difference in goals, the construction and usage of the feedback simulator are different in the two settings.For example, AlpacaFarm's simulator perturbs LLM preferences with label noise to mimic the noisiness of human annotation, whereas Constitutional AI's simulator does not inject extra noise.</p>
<p>The evaluation aspects of our work are related to a growing line of work on simulating human annotation for evaluation [14,51,14,50,35,36].Our core evaluation and feedback mechanism makes use of the same underlying ideas, but our work is distinguished by a focus on using pairwise feedback for training, as well as careful validation beyond per-example agreement metrics.AlpacaFarm shows that LLM feedback can capture method-level correlations as well as qualitatively important features of human annotation for phenomena such as overoptimization.</p>
<p>Our goal of emulating human annotators also connects to work on simulating humans with LMs based on personas [48,47,1,3], as well as works that simulate human behavior in the context of cognitive science, social science, and economics [69,25].Our work complements these works by showing that simulated LLM annotators can replicate many of the qualitative features of training on pairwise human feedback.</p>
<p>More broadly, building a simulator environment to enable low-cost experimentation is common in the field of reinforcement learning and robotics [11,67,66,64,17,23,18].Our work shares the same underlying motivations, but instead of simulating physical systems, AlpacaFarm simulates human preference feedback.</p>
<p>Methods for learning from feedback.To hold annotation cost constant across learning methods, we have focused only on methods that learn from pairwise feedback in this work.However, there exist methods in the literature other than those explored in AlpacaFarm that can incorporate alternative sources of feedback such as natural language [74,32,22,59,55,13,56,40], numeric ratings [44,31], or execution traces [13].We view extensions of AlpacaFarm to these settings as exciting future work.</p>
<p>We have included a set of RL algorithms in our study that optimize the surrogate reward, but this set is by no means comprehensive.RL research applied to NLP has a long history [75,62,27,49,42,30,29,53,61], and we expect future work in this direction to benefit from the ideas and artifacts in AlpacaFarm.</p>
<p>Limitations and future directions</p>
<p>Validation.Section 4 validates the use of AlpacaFarm, but there are nevertheless some limitations with the validation setting.First, the instructions we considered (even those from the real world-demo in section 4.4) are relatively simple and single-turn.Second, all models we fine-tuned use a LLaMA 7B as starting point.Finally, human validation is based on feedback from 13 crowdworkers, which may not reflect broader human preferences and seem to have biases such as preferences for longer outputs as highlighted in appendix C.2.Although we do not expect the previous limitations to significantly affect the usefulness of AlpacaFarm, we encourage users to be vigilant when using any simulator and hope to further validate AlpacaFarm as more datasets and base models become available.More generally, evaluations using pairwise comparison can be an issue even using human annotators as it does not provide a deep understanding of the differences between two models.</p>
<p>Assumption of an oracle LLM.AlpacaFarm assumes that we can access an oracle LLM, much more powerful than the investigated models, that can be used to approximate human feedback.While this may be true in research settings, this will not be the case when building state-of-the-art models.We believe that investigating the use of the same model for the simulator and for learning is a promising direction for future work.</p>
<p>AlpacaFarm for fine-grained development.Section 4 shows that AlpacaFarm is effective for model selection and can replicate important behaviors such as over-optimization.We have nevertheless found that suitable hyperparameters for learning algorithms are different for training with simulated feedback compared to human feedback.For example, due to changes in the scale of values of the surrogate reward model, the range of suitable KL regularization coefficients for RLHF is different.</p>
<p>Simulated annotators.</p>
<p>A key component of AlpacaFarm is the simulated annotators.In section 4.3 and appendix C we showed that our annotators have high agreement with human annotators and exhibit similar biases, such as preferences for longer outputs and lists.There are nevertheless some biases that are specific to simulated annotators.For example, we found that simulated annotators prefer the first outputs shown in the prompt and outputs that come from the same model as the simulator.Although we controlled for those biases by randomizing the order of output in the prompt, and using the same training data for all considered methods, there may be other biases that we have not identified.For better automatic annotators and further analysis refer to AlpacaEval [33].</p>
<p>Future directions.We showed that AlpacaFarm substantially lowers the cost and iteration time of research on and development of methods for learning with pairwise feedback.AlpacaFarm provides a blueprint for constructing other useful simulators for AI research that requires human supervision, and we view it as an exciting opportunity to expand this simulation approach to support data from other domains as well as methods that learn from alternative forms of human feedback.</p>
<p>A Reference LPF methods on AlpacaFarm</p>
<p>We provide a more thorough description of the methods in AlpacaFarm along with the custom modifications we make.For more on hyper-parameter tuning, experiment details, and further ablations, please see Appendix B.</p>
<p>Our methods fall into two categories based on whether they fit a surrogate reward model as part of the learning process.In addition, to those methods which are trained in AlpacaFarm, we also include well-known baseline methods from the OpenAI API.</p>
<p>API baseline methods.</p>
<p>In both the human and the simulated feedback workflows, we evaluate the following methods from the OpenAI API: GPT-4, ChatGPT, Davinci001, and implicitly Davinci003, which is the baseline model we compare every model with.Outputs from all these models are sampled at temperature 0.7 and with top-p 1.0.For the text models, we use the same prompt as all other reference methods in AlpacaFarm.For both chat models (ChatGPT and Davinci003), we had to change prompts for two reasons.First, those models require prompts in a chat format, which is different from the text format.Second, we found that those models generated sequences that were much longer than the rest of our models, which were trained to output sequences of less than 300 tokens.We thus ask in the system prompt for a response that is shorter than 1000 and 500 characters respectively for ChatGPT and GPT-4, which we found to give shorter answers while working similarly to a raw prompt that does not mention the length.</p>
<p>A.1 Methods that directly learn from pairwise feedback Binary FeedME.FeedME is a method proposed by OpenAI [44] that incorporates human feedback with supervised fine-tuning on model generations that are rated 7/7 by human labelers.We adapt this approach to the pairwise feedback setting and call this baseline binary FeedME.This approach fine-tunes the SFT model on the chosen response in each preference pair with supervised learning.</p>
<p>Binary Reward Conditioning.Motivated by controllable generation through conditioning [26,34,28,20], we propose binary reward conditioning, a baseline method that fine-tunes the SFT model with the feedback data D pairwise by conditioning instances with either a positive or negative control token.Specifically, for each instance (x, y 0 , y 1 , z) ∈ D pairwise , the string concatenation of instruction x and response y z denoted as [x, y z ] is prepended with the positive token and used in supervised fine-tuning (similarly [x, y 1−z ] is prepended with the negative token).This process creates a modified demonstration dataset that is double the size of D pairwise .At test time, we draw samples from the fine-tuned model conditioned on the positive token.</p>
<p>Direct Preference Optimization.DPO [52] maximizes the log-likelihood of preferences under the Bradley-Terry model w.r.t. a reward model.The reward model is not instantiated explicitly but defined implicitly by the current policy which is assumed to be optimal under the KL-regularized objective.More precisely, DPO maximizes the following objective
E (x,y0,y1,z)∼Dpairwise log σ β log p θ (y z | x) p SFT (y z | x) − β log p θ (y 1−z | x) p SFT (y 1−z | x) .(1)
A.2 Methods that optimize a surrogate reward function</p>
<p>We now describe methods that incorporate feedback by first building a surrogate reward model with pairwise feedback data.To start, we describe the step of training the surrogate reward model.</p>
<p>To train a parameterized surrogate Rϕ , one can maximize the log-likelihood of the preferences z under the Bradley-Terry model [10] maximize ϕ j log P (z (j) | x (j) , y
(j) 0 , y (j) 1 ) = j log exp( Rϕ (x (j) , y (j) z )) exp( Rϕ (x (j) , y (j) 0 )) + exp( Rϕ (x (j) , y (j) 1 )) . (2)
Once the surrogate reward model is trained, both training and inference algorithms can optimize against the reward model rather than query pairwise feedback.While this can be a powerful approach, we will see that it can also lead to over-optimization [19] where models learn to exploit the reward model rather than achieve high true reward.We now describe 4 methods that leverage the surrogate reward model.</p>
<p>Best-of-n Sampling.Best-of-n sampling (or re-ranking) [63,5,21,8] is a common inference-time method that aims to improve the generation quality.Given an input x, the method returns the response with the highest surrogate reward value among n i.i.d.responses drawn from the SFT model.While simple to implement and useful as a baseline, this approach incurs high inference costs.</p>
<p>Expert Iteration.Expert iteration [2,60,70] is a technique that has recently been used to train language models.We adapt this approach in AlpacaFarm as a two-step method.In the first step, we perform best-of-n sampling and store the generated samples.In the second step, we fine-tune p SFT on these samples with supervised learning.While prior work applies expert iteration for multiple rounds by performing best-of-n sampling again for intermediate models, we focus on performing a single round.In Appendix B, we include our preliminary study of multi-round expert iteration.</p>
<p>Proximal Policy Optimization.Proximal Policy Optimization [PPO; 24,58] is a popular RL algorithm that has been recently used to develop InstructGPT [46] and ChatGPT [43].When applied to fine-tune LMs with RLHF, PPO maximizes the following KL-regularized objective w.r.t.model parameters θ
E x∼p(x), y∼p θ (y|x) Rϕ (x, y) − β log p θ (y | x) p SFT (y | x) ,(3)
where p(x) is an unlabeled instruction distribution, p θ (y | x) is fine-tuned from the p SFT model, and β ∈ R is a regularization coefficient.Each step of PPO alternates between drawing samples from the current policy and performing gradient updates based on the pool of samples with importance sampling and clipping.</p>
<p>Quark.Quark is inspired by reward conditioning and has been shown to be effective for controllable generation tasks.Like binary reward conditioning, Quark on train sequences with prepended control tokens.Unlike binary reward conditioning, Quark bins model samples into multiple groups based on the reward value, adds KL and entropy regularization, and repeats the entire process across multiple rounds.</p>
<p>In our preliminary analysis, we find the top-quantile variant reported in [38], i.e.only training on the best reward group, to perform the better than the all-quantiles variant which trains on all groups.</p>
<p>B Details on methods implementation and hyperparameters B.1 PPO</p>
<p>We follow an existing PPO implementation for fine-tuning language models,2 but also introduce modifications.First, off-the-shelf PPO implementations for language model fine-tuning tend to normalize the estimated advantage for each minibatch.We found this led to training instabilities for small minibatch sizes and instead normalize the advantage across the entire batch of rollouts obtained for each PPO step.Second, we initialize the value model from the reward model as opposed to the SFT model, following more recent documented practice [46] (the authors did not release code).Our preliminary experiments showed that initializing from reward worked much better than initializing from SFT for maximizing the surrogate reward.</p>
<p>We tuned hyperparameters to improve training stability and reduce convergence time so that experiments can reliably finish with relatively tight compute budgets.In the end, we settled on a batch size of 512 for each PPO step, which consisted of 2 epochs of gradient steps each performed with a batch of 256 rollouts.We used a peak learning rate of 10 −5 which decayed to 0 throughout training.We clipped the gradient by Euclidean norm with a threshold of 1.We trained for 10 full passes over the unlabeled set, which amounts to 390 PPO steps.Performance typically peaked very early on during training (see Figure 5).We set λ and γ both to 1 for generalized advantage estimation [57].We used a fixed KL regularizer coefficient as opposed to an adaptive one.We tuned the coefficient value for both simulated and human PPO, and settled with 0.02 for human PPO, and 0.002 for simulated PPO.We note that suitable values for the KL regularizer coefficient depend on the early stopping criteria and the scale of surrogate reward values.</p>
<p>B.2 Quark</p>
<p>We re-implement Quark for our needs and make several modifications.First, the original Quark formulation accumulates rollouts during training and stores them in a pool that consistently grows.We found this led to overhead that increased during training (since after each rollout batch is generated, the pool is expanded and rollouts in the pool are re-sorted by their reward values).To operate under a reasonable compute budget, we discard previous rollouts once a new batch of rollouts is generated.In other words, the pool is reset once rollout is performed.This modification made the compute cost constant throughout training and thus more predictable overall.Second, we found that training on rollouts of more bins led to worse efficiency for reward optimization, and thus opted to train only on rollouts of the top-scoring bin (best-quantile variant in the original paper [39]).Preliminary ablations on a simple sentiment task showed that any potential loss in perplexity for the best-quantile variant can be compensated by turning up the KL regularizer.Lastly, we found the entropy penalty used in the original Quark formulation to give no benefit for working with instruction following.Small entropy penalty terms were enough to cause big degradations in text generation quality in terms of fluency.</p>
<p>For the official run with reported results, we used a KL regularizer coefficient of 0.05, a peak learning rate of 3 × 10 −6 which decayed to 0 throughout training.Each Quark step had batch size 512 for rollout, and 2 epochs of gradients updates each with batch size 256.We clipped the gradient by Euclidean norm with a threshold of 1.We trained for 10 full passes over the unlabeled set, which amounts to 390 Quark steps.</p>
<p>B.3 Direct Preference Optimization</p>
<p>For the DPO run on noisy simulated preferences, we set β = 0.1 and train for 1 epoch with a batch size of 64.</p>
<p>The learning rate linearly warms up to the peak of 1e−5 within 3% of training and linearly decays to 0.</p>
<p>C Pairwise preference simulation C.1 Details about simulated annotators</p>
<p>For all our simulated annotators we used OpenAI API to generate outputs.We first discuss below the overall design choices for all our simulators below, and then discuss our annotator pool below in more detail.For all the actual prompts we used refer to https://github.com/tatsu-lab/alpaca_farm.</p>
<p>Randomized order.For each annotator, we randomize the ordering between the two outputs to annotate, i.e., we randomly choose which output is the first and which is the second.We found randomization to be important given that the first output is often preferred by simulated annotators.</p>
<p>Prompts with and without inputs.Following the Alpaca dataset [65] and self-instruct framework [71] some instructions have associated inputs, while others do not.For each annotator, we thus write two corresponding prompts, one for instructions with inputs and one for instructions without inputs.Both prompts are essentially the same but in-context examples differ in the presence of the input.</p>
<p>Batching for GPT4.When adding in-context examples, prompts can become relatively long, which leads to high-cost and waiting time when using GPT-4 as a simulator.To decrease cost and increase annotation speed, we amortize the cost of in-context examples by providing a batch of instruction-output pairs to annotate at once by GPT-4.For our simulated annotator we use a maximum batch size of 5 but found during development that we could fit batch size up to 20 in the context window without significantly decreasing performance.</p>
<p>To improve performance when using batching, we found it useful to provide a few in-context examples in a batched format and to index every component of an annotation (instruction, input, output, . . .).</p>
<p>Improving parsing for ChatGPT.Overall we found ChatGPT to be much more sensitive and harder to use as a simulator.In particular, we found it to be more sensitive to the prompt format and to often fail to generate annotations that could be parsed, e.g., by responding "Neither is better, this depends on personal preferences" despite being explicitly instructed to choose a preference.We found two tricks to be effective to make ChatGPT's more parsable.First, we add a negative bias to tokens such as "Neither" and "Both" and a positive bias to the tokens that we hoped to match.We found the aforementioned biasing of tokens to work well but it can be problematic when using Chain of Thought reasoning.A second trick that we found to be effective is to ask ChatGPT to generate a JSON object that contains a string field with a short explanation (Chain of Thought) and a boolean field that indicates whether the first output was preferred.</p>
<p>Now that we have discussed the overall design choices for our simulated annotators, we discuss in more detail the prompts and parameters for each of our annotators.</p>
<p>AlpacaFarm's evaluation annotators p eval sim .To try to match the bias and variance of human annotators, we use a pool of 13 simulated annotators that were developed at different stages of the project.In particular, we use the following sources of variations:</p>
<p>• Models.Five of the annotators are powered by GPT-4, four by ChatGPT, and four by Davinci003.</p>
<p>The difference between different annotators for the same model is mostly the prompt.• In-context examples.Prompts for the same models use different numbers of in-context examples.</p>
<p>• Prompt format.We use different prompt formats between and for the same model.For example different batch sizes and different formats of outputs (JSON vs raw text).• Preferences.Two of the GPT4 annotators are explicitly prompted to prefer sequences that are respectively long and short.• Sampling.For each annotator in the pool, we use a sampling temperature of 1.0 with top p also 1.0.</p>
<p>The high temperature means that we have variability that arises from sampling.</p>
<p>AlpacaFarm's training annotators p ann sim .Our simulated annotators for training are the same as the evaluation annotators p eval sim except that we flip the output with 0.25 probability.We implement this by taking a mixture between p eval sim and an independent Bernoulli random variable with probability 0.5.This means that we only need to label half of the outputs for training, which makes it 2× faster and cheaper.</p>
<p>GPT4.For the GPT4 annotator p GPT-4 sim we use a prompt with batch size five that corresponds to one of the prompts from our simulated pool of annotators.For p GPT-4 sim we use temperature 0, i.e., deterministic annotations.</p>
<p>C.2 Additional results</p>
<p>We now provide additional results for understanding our pairwise annotators.For more results and code for generating plots see https://github.com/tatsu-lab/alpaca_eval.</p>
<p>Our pool of annotators has low bias and matches human variance.Figure 7 shows the estimated bias (y-axis) and variance (x-axis) of simulated evaluators.We see that single evaluators have a smaller variance (less than 0.2) than humans (blue line, 0.34).This lack of variability makes emulating it with a proxy reward very easy and leads to unrealistic over-optimization properties in the simulator, as seen in Figure 5.Using a pool of annotators (green point) for evaluation and additionally adding noise (orange) during training gives an estimated variance significantly closer to humans (blue line 0.35).We hypothesize that this is necessary for the simulator to show a similar over-optimization behavior as humans.Concerning the bias, we find that our simulated annotators for evaluation p eval sim and training p train sim both have low bias values (0.38 and 0.35) on par with one of our best GPT-4 annotators (0.33).</p>
<p>Variability in a pool of annotators mostly comes from the underlying model.In Figure 8 we show the pairwise agreement between all annotators in our pool and all other annotators including the majority vote of humans (first column) and single humans (second column).The desired high variance corresponds to low values on the diagonal (annotators disagree with themselves) and low bias corresponds to high values in the first column (high agreement with the mode of humans).As in Figure 7, we see that our pool of annotators p eval sim has low bias and high variance.Figure 8 also shows that the largest source of variability between annotators comes from the underlying model, as illustrated by the clusters that arise from GPT4, ChatGPT and Davinci003 annotators.</p>
<p>Humans and simulated annotators prefer longer outputs that contain lists.One natural question is whether simulated and human annotators have biases towards different type of outputs, which would cause models in both frameworks to be qualitatively different.We identify two stylistic features, the length and the presence of lists, for which humans have a strong preference and analyze whether simulated annotators match those preferences.We found that humans prefer longer outputs 62% of the time, while our simulated annotators prefer those 64% of the time.Similarly, humans prefer outputs with lists 69% of the time, while our simulated annotators prefer those 63% of the time.This shows that our simulated annotators match well the stylistic preferences of humans, which suggests that models trained in our sandbox are optimizing similar preferences as those trained with human feedback and they will likely exhibit similar behaviors.Preference for longer outputs (%)</p>
<p>Figure 9: Humans and our simulated annotators prefer outputs that are longer and have lists.</p>
<p>D Details on human data collection</p>
<p>Qualification.We conducted the qualification of our annotators based on 25 qualification examples.The qualification examples were generated but an OPT 6B model that was studied in the earlier development phase of this project.The five student authors of this paper annotated a shared set of pairwise preferences.</p>
<p>From the shared set, we selected 25 questions where the majority of the authors reached an agreement on the correct annotation.We then use these questions as a qualification test and selected the top 16 annotators whose agreement is the highest with the authors.We paid the annotators the same price for the qualification round as we did for the main qualification.</p>
<p>During the annotation process, we also compare each annotator's preference to that of GPT-4.We identified one annotator whose agreement is around 50% with GPT-4, which is a clear outlier from other annotators.Therefore, we discontinued working with this annotator during the annotation project and removed their annotation.</p>
<p>Annotation guideline.We display our annotation guideline in Figure 10 and annotation interface in Figure 11.</p>
<p>In our annotation process, we find that there are pairs that only differ in punctations or have minimal edit distance and we instruct the annotators to select a response as slightly better/worse if the difference between the pairs is marginal.As a result, around 18% of the collected preference selected the slightly better options.</p>
<p>In our LPF experiments, we binarize the preference and treated the slightly better options the same as the normal preference labels.However, we release the more fine-grained labels as resources and leave the study to future work.</p>
<p>Hi test22. Not your MTurk id? Logout</p>
<p>Your annotation progress for this HIT: 0/6 completed.</p>
<p>Sample 1 of 6.</p>
<p>Lets you skip forward (all samples still need to be marked): → Next sample.</p>
<p>Returns you back to the next un-marked sample to mark:  Go to next un-marked sample.</p>
<p>Annotation Guidelines Instruction</p>
<p>Tell me about self driving cars</p>
<p>Input AI Response 1</p>
<p>Self driving cars are vehicles that are capable of detecting their surroundings and navigating without any input from a human driver.They use a variety of sensors, such as cameras, radar, and lidar, to detect objects in their environment and use advanced algorithms to make decisions about how to maneuver around them.Self driving cars are becoming increasingly popular and are being used in a variety of applications, such as ride-hailing services and autonomous delivery services.</p>
<p>AI Response 2</p>
<p>Self driving cars are cars that are equipped with sensors, cameras, and software, which allows them to navigate without the need for a driver.Self driving cars are able monitor their environment and navigate without the assistance of a driver.Self driving cars use a combination of technologies such as GPS, radar, lidar, and computer vision to detect their environment and avoid obstacles.They are also able to update their maps and adjust their routes to account for variables such as traffic and construction.</p>
<p>The main benefit of self driving cars is their ability to reduce crashes and save lives.Autonomous vehicles have the potential to save up to 30,000 lives annually.They are also more energy efficient than traditional vehicles, as they do not require a driver who has to maintain a minimum speed.Autonomous vehicles also create new opportunities for increased mobility, allowing those who are unable to drive to get around.</p>
<p>Rating</p>
<p>Response 1 is better.In this section, we ablate the different components of p ann sim that add variability along two axes: randomizing across different simulated annotators, and adding label noise.To ablate the randomization across different annotators, we compare to the simple GPT-4 prompt p GPT-4 sim with added label noise.To ablate the label noise, we compare to p eval sim , which is p ann sim without the label noise.We train reward models on these preference distributions and compare the performance of best-of-n sampling.</p>
<p>Figure 12 shows the results of the ablation, demonstrating clearly that added label noise provides the majority of the overoptimization effect.In particular, the two options that do not add label noise, p GPT-4 sim and p eval sim , keep increasing win-rates with more samples.This result suggests that modeling intra-annotator variability via label noise may be an important component to understand learning from human preference data.The exact ratio of 25% label noise was chosen to be the smallest considered ratio that induced overoptimization out of the ones we considered (0,17%,25%,35%), as shown in fig.13.</p>
<p>Figure 13: 25% label flipping is the smallest ratio we tested that induced over-optimization.The y-axis shows the simulated win-rate.The x-axis shows the number n of samples used in best-of-n sampling, which is proportional to the output's average reward.Each curve shows a reward model trained on samples with different label flipping ratios (0%, 17.5%, 25%, 35%).We see that over-optimization seems to be approximately monotonic with respect to the label-flipping ratio.</p>
<p>E.2 Understanding computational cost</p>
<p>While we have focused primarily on the performance of the final model, the computational cost of these methods is an important consideration.We provide time estimates for training on our specific implementation and compute environment (a single 8×A100 machine).While these timings are specific to our experiments, we believe these insights may be useful in understanding the cost of learning from pairwise feedback.</p>
<p>To begin with, supervised fine-tuning and methods that directly adapt supervised fine-tuning like Binary Reward Conditioning and Binary FeedME are generally fast, taking less than an hour for 10k instructions.Best-of-n sampling incurs no training cost but instead suffers a substantial inference time cost.The optimal n for best-of-n was around 16, which (in principle) translates into an increased cost of 16x for batched inference.</p>
<p>In our experiments, Expert Iteration works best when training on the outputs of best-of-16 sampling, which involves substantial amounts of compute to perform best-of-16 sampling on the unlabeled split.Concretely, this sampling process takes around 4 hours on the 20k unlabeled split and the subsequent fine-tuning takes less than an hour.We observe that PPO performance peaks on both human data and in simulation after 40-80 steps, which translates to less than 2 hours of compute time on the full machine.DPO training for 1 epoch on half the machine (4× A100) takes less than 3 hours.</p>
<p>E.3 Additional analysis of model outputs</p>
<p>In this section, we provide a qualitative analysis of the model outputs.In Table 5, we show three example outputs from SFT 10k, PPO human , and PPO sim .As mentioned in Section 5.2, we find that the model outputs after LPF training often become much longer.In Table 6 and Table 7, we show the average length of model outputs of various systems in the number of characters, which qualitatively illustrates this phenomenon.While longer responses indeed lead to better human preference, we continue to see hallucinations in the output (e.g. in the example question about Berlin) and it is not clear to us whether our LPF models improve on the more critical aspects such as faithfulness, knowledge, etc. Table 5: Comparing system output before and after training with PPO, we observe that outputs often become longer.Example showing output from PPO-tuned models trained with the human preference p human and with the simulated preference p ann sim .What do you think about ChatGPT?SFT 10k: I think ChatGPT is a promising technology that has the potential to revolutionize the way we interact with computers.It has the ability to generate natural language text quickly and accurately, which can be used for many different applications.I think it's an exciting technology that we'll continue to see more and more uses for in the future.PPO Human : I think ChatGPT is a fascinating development in natural language processing.It is an artificial intelligence system that is able to generate human-like text, allowing it to converse with humans on a variety of topics.It is able to learn from large datasets and improve its performance over time, allowing it to produce text with increasing accuracy and sophistication.I think it has a lot of potential applications, from natural language processing and machine translation, to automated customer service and natural language processing.It is an incredible step forward in AI technology, and I look forward to seeing what new developments come out of it in the future.PPO sim : I think ChatGPT is a really cool technology that can generate natural language text based on an input.It allows for users to quickly generate text responses without needing to write code or create a custom natural language processor.It also is very good at understanding context and can generate text based on context and prompts.I think it has a lot of potential applications, such as automated customer service, natural language processing, and text generation.I think it's pretty amazing how quickly the technology is advancing and it's really cool to see what's possible with AI and machine learning.I'm really excited to see what the future holds for ChatGPT and other AI technologies.Who made Berlin?SFT 10k: The city of Berlin was made by the German people.PPO human : The city of Berlin was made by various people over the course of its history.It was first established as a medieval trading post in the 12th century, and subsequently built up by a variety of different rulers.The city was heavily damaged during the Thirty Years' War, but was rebuilt and expanded by the Hohenzollern dynasty in the 17th and 18th centuries.During the 19th century, Berlin became the capital of the German Empire, and was significantly expanded and modernized under the leadership of Mayor Otto von Bismarck.The city was also divided into East and West sectors following World War II, with each sector being ruled by separate Cold War-era governments.PPO sim : The city of Berlin was made by various people over the centuries.The earliest known inhabitants of the area were the Tubal and Jabal tribes, who arrived in the region around 2300 BC.These tribes were later replaced by Celtic tribes, who founded the city of Berlin in the 1st century BC.The city was then conquered by the Romans in the 1st century AD and was renamed as 'Berlinium'.During the late Middle Ages, Berlin became an important city in the Holy Roman Empire and was ruled by various princes and electors.In the 17th century, Berlin gained its current shape under the rule of Prince-Elector Frederick William I, who established the city's defensive ring of bastions and set up the first permanent settlements outside the city walls.In the 18th and 19th centuries, Berlin experienced a cultural and economic boom, becoming the capital of the Kingdom of Prussia in 1701 and the seat of the German Empire in 1871.The city was also the site of many historical events, including the foundation of the Weimar Republic in 1919 and the fall of the Berlin Wall in 1989.Today, Berlin is a bustling city with a population of over 3.7 million people, making it the third-largest city in Germany.How many black holes are known to exist at the time?SFT 10k: Nearly 100 million black holes are known to exist at the time.PPO human : At the time, there are believed to be several million black holes in the universe.This number is constantly increasing as astronomers discover more and more black holes through observation and study.Most black holes are found at the centers of many different galaxies, and are thought to be formed when the most massive stars in the galaxy run out of nuclear fuel and explode in a supernova.The resulting core of the star is so densely packed that nothing, not even light, can escape its powerful gravitational pull, giving it the name "black hole".PPO sim : At the time of this writing, there are over 1000 known black holes in the universe.</p>
<p>F Additional Analysis on Training and Evaluation Instructions</p>
<p>We plot in Figure 14 and Figure 15 the breakdowns of the Alpaca training instruction distribution and the AlpacaFarm evaluation instruction distribution respectively.In the inner wheel, we plot the root verb distribution of the instructions and in the outer wheel, we plot the direct subject distribution.We find that both the training distribution and the evaluation distribution cover a diverse range of instructions and the distributions match at a high level.</p>
<p>Figure 1 :
1
Figure 1: AlpacaFarm is a simulation sandbox that enables fast and cheap experimentation on LLMs that learn from human feedback.It simulates human feedback with (oracle) API LLMs, provides a validated evaluation protocol, and offers a suite of reference method implementations.Researchers can rapidly iterate on model development and transfer their methods to training on human data to maximize performance.</p>
<p>Figure 2 :
2
Figure 2: Root verb distribution of the eval instructions.</p>
<p>Figure 5 :
5
Figure 5: The added variability in AlpacaFarm annotators is needed to reproduce reward model overoptimization seen in human data.Left: training and evaluation with human preferences p human .Middle: training and evaluation with AlpacaFarm preferences, p ann sim and p eval sim .Right: training and evaluation with simple GPT-4 preferences, p GPT-4sim .For each plot, the x-axis measures the average surrogate rewards on the eval set.Human and AlpacaFarm preferences result in over-optimization, while simple GPT-4 preference does not.</p>
<p>win-rate on p GPT4 sim Expert Iteration Best-of-n PPO (c) Single-prompt GPT-4</p>
<p>Figure 7 :
7
Figure7: Our simulated annotators achieve relatively low bias with human annotators and match human variance.The y-axis shows the estimated bias, i.e., the error between the majority vote of 4 simulated annotators and the majority vote of 4 human annotators.The x-axis shows the estimated variance, i.e., the error between a held-out annotation and the majority vote of the other three annotators.The bias of humans is by definition 0, and variance is shown with a blue line.Grey points are all the annotators in our simulated pool, the green point shows the resulting pool of annotators (which we use for evaluation), the orange point shows the same simulated pool with additional noise (which we use for training), the blue point the average human annotator, and the red point shows a single low variance GPT-4 annotator we analyze.</p>
<p>Figure 8 :
8
Figure 8: The largest source of variability between annotators comes from the underlying model.Every cell of the heatmap shows the agreement between two annotators (x-and y-axis).</p>
<p>Figure 12 :
12
Figure 12: Label noise is the most crucial ingredient for inducing overoptimization.</p>
<p>Figure 14 :
14
Figure 14: Breakdowns of the 52k Alpaca training instructions.</p>
<p>Figure 15 :
15
Figure 15: Breakdowns of 805 AlpacaFarm evaluation instructions.</p>
<p>Table 1 :
1
Example instructions in AlpacaFarm's evaluation data.</p>
<p>Table 2 :
2
AlpacaFarm evaluation results on baseline and LHF methods.For methods without a <em>, the left column shows win-rates when we train and evaluate in simulation, while the right column shows win-rates when we train and evaluate with human feedback.For models with a </em>, those are not trained by us so the left and right columns respectively show simulated and human evaluation.Win-rates are computed against Davinci003.We did not evaluate Quark and Binary Reward Conditioning for human evaluation because they underperformed in development.† ChatGPT and GPT-4 were prompted for responses shorter than 1000 characters to have lengths comparable to other methods.
Simulated Win-rate (Demo)0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8R 2 = 0.97 Spearman Correlation: 0.90Figure 6: Correlation plot of simulated win-rates computed on AlpacaFarm's evaluation versus that on real-world interactions with the Alpaca Demo.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Simulated Win-rate (AlpacaFarm)MethodSimulated Win-rate (%) Human Win-rate (%)GPT-4<em>  †79.0 ± 1.469.8 ± 1.6ChatGPT</em>  †61.4 ± 1.752.9 ± 1.7PPO46.8 ± 1.855.1 ± 1.7DPO46.8 ± 1.7-Best-of-102445.0 ± 1.750.7 ± 1.8Expert Iteration41.9 ± 1.745.7 ± 1.7SFT 52k39.2 ± 1.740.7 ± 1.7SFT 10k36.7 ± 1.744.3 ± 1.7Binary FeedME36.6 ± 1.737.9 ± 1.7Quark35.6 ± 1.7-Binary Reward Conditioning32.4 ± 1.6-Davinci001<em>24.4 ± 1.532.5 ± 1.6LLaMA 7B</em>11.3 ± 1.16.5 ± 0.9</p>
<p>Table 3 :
3
Comparing system output before and after PPO training, we observe that outputs often become longer.Instruction: What are the five most common causes of stress among people?
SFT 10k: The fivemost common causes ofstress among people arework, money, relation-ships, health, and fam-ily.</p>
<p>Table 4 :
4
Model transfer results.
MethodHuman Win-rate (%)PPO human55%Best-of-16 human51%PPO GPT-4 sim50%SFT 10k PPO ann sim44% 43%</p>
<p>never required): for cases where rating is near random.
E Additional resultsE.1 Label noise ablations for simulated annotators0.30 0.35 0.40 0.45 0.50 sim for blue, p eval Eval win-rate (p GPT4 sim for orange)Number of reranked samples (best-of-n) 2 0 2 1 2 2 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10Train distribution for reward model p GPT4 sim p GPT4 sim w/ 25% label flips p eval sim p ann sim , = p eval sim w/ 25% label flips
Response 1 is only slightly better.(only pick this if it's truly close) Response 2 is only slightly better.(only pick this if it's truly close) Response 2 is better.Optional explanation (Submit Figure 11: Our annotation interface.</p>
<p>https://github.com/openai/lm-human-preferences
Acknowledgments and Disclosure of FundingWe thank the Stanford Center for Research on Foundation Models (CRFM), Stanford HAI, and Stability AI for compute support.Data collection efforts were supported through the Tianqiao &amp; Chrissy Chen Institute and an Open Philanthropy grant.XL is supported by a Stanford Graduate Fellowship.RT is supported by the NSF GRFP under Grant No. DGE 1656518.YD is supported by a Knights-Hennessy Scholarship.Hi! We are a group of researchers working on Artificial Intelligence (AI).In this task, we will ask you to help us rate an AI model's responses to instructions.In the area below, you will first read:1.An instruction we give to the AI system.2. An input that is provided along with the instruction.This is an optional input and not all instructions will have inputs.3. Two responses from the AI system Your task is to decide which response is better.There are several dimensions that you can think along.Consider the following questions:1. Is the response helpful?For example, if the instruction asked for a recipe for healthy food, and the response is a useful recipe, then we can consider it helpful.2. Is the response language natural?For example, AI responses are often repetitive, which is not natural.3. Is the response factual/accurate?AI responses often make up new information.For example, if the response claims that Donald Trump is the current U.S.president, then you should consider it inaccurate.4. Based on your aesthetics, which one do you prefer?For example, you might prefer one poem over another poem.5.And so on ... ultimately, you should decide which response is better based on your judgment and based on your own preference.There are four options for you to choose from: Payment structure Payment structureIn general, we aim to pay crowd workers $15/hour on average.Our current estimate is that it will usually not take you more than a minute to compare each pair of responses on average.Therefore, we currently pay $0.25 per pair of responses.Qualification QualificationIn the qualification round, you will need to compare 24 pairs of responses and we pay $6 for this part.We also allocate 2 minutes for you to read the annotation guideline, for which we pay $0.50.In total, we pay $6.50 for the qualification round.Main HIT Main HITIn total, we have around 20K pairs to annotate and we plan to spread the annotation over 2 weeks.In each HIT after the qualification round, you can work on 15 pairs of responses.We pay $3.75 for each HIT.
Using large language models to simulate multiple humans. Gati Aher, Rosa I Arriaga, Adam Tauman, Kalai , arXiv:2208.102642022arXiv preprint</p>
<p>Thinking fast and slow with deep learning and tree search. Thomas Anthony, Zheng Tian, David Barber, 201730Advances in neural information processing systems</p>
<p>Out of one, many: Using language models to simulate human samples. Ethan C Lisa P Argyle, Nancy Busby, Joshua Fulda, Christopher Gubler, David Rytting, Wingate, arXiv:2209.068992022arXiv preprint</p>
<p>Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Dara Vinh Q Tran, Jianmo Bahri, Ni, arXiv:2111.10952Towards extreme multi-task scaling for transfer learning. 2021arXiv preprint</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova Dassarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam Mccandlish, Chris Olah, Jared Kaplan, A general language assistant as a laboratory for alignment. 2021</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam Mccandlish, Chris Olah, Ben Mann, Jared Kaplan, 2022</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Fine-tuning language models to find agreement among humans with diverse preferences. Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat Mcaleese, Amelia Glaese, John Aslanides, Matt Botvinick, Advances in Neural Information Processing Systems. 202235</p>
<p>On the opportunities and risks of foundation models. R Bommasani, arXiv:2108.072582021arXiv preprint</p>
<p>Rank analysis of incomplete block designs: I. the method of paired comparisons. Ralph Allan, Bradley , Milton E Terry, Biometrika. 393/41952</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.015402016Openai gym. arXiv preprint</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems (NeurIPS). 2020</p>
<p>Improving code generation by training with natural language feedback. Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Kyunghyun Samuel R Bowman, Ethan Cho, Perez, arXiv:2303.167492023arXiv preprint</p>
<p>Cheng- , Han Chiang, Hung-Yi Lee, arXiv:2305.01937Can large language models be an alternative to human evaluations?. 2023arXiv preprint</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730Advances in neural information processing systems</p>
<p>Surreal: Open-source reinforcement learning framework and robot manipulation benchmark. Linxi Fan, Yuke Zhu, Jiren Zhu, Zihua Liu, Orien Zeng, Anchit Gupta, Joan Creus-Costa, Silvio Savarese, Li Fei-Fei, Conference on Robot Learning. PMLR2018</p>
<p>Erik Daniel Freeman, Anton Frey, Sertan Raichuk, Igor Girgin, Olivier Mordatch, Bachem, arXiv:2106.13281Brax-a differentiable physics engine for large scale rigid body simulation. 2021arXiv preprint</p>
<p>Leo Gao, John Schulman, Jacob Hilton, arXiv:2210.10760Scaling laws for reward model overoptimization. 2022arXiv preprint</p>
<p>Koala: A dialogue model for academic research. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, Dawn Song, March 2023</p>
<p>Improving alignment of dialogue agents via targeted human judgements. Amelia Glaese, Nat Mcaleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, arXiv:2209.143752022arXiv preprint</p>
<p>Learning from dialogue after deployment: Feed yourself. Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, Jason Weston, arXiv:1901.054152019chatbot! arXiv preprint</p>
<p>Unity: A general platform for intelligent agents. Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen, Jonathan Harper, Chris Elion, Chris Goy, Yuan Gao, Hunter Henry, Marwan Mattar, arXiv:1809.026272018arXiv preprint</p>
<p>An alternate objective function for Markovian fields. S Kakade, Y W Teh, S Roweis, International Conference on Machine Learning (ICML). 2002</p>
<p>Ai personification: Estimating the personality of language models. Saketh Reddy Karra, Son Nguyen, Theja Tulabandhula, arXiv:2204.120002022arXiv preprint</p>
<p>N S Keskar, B Mccann, L R Varshney, C Xiong, R Socher, arXiv:1909.05858CTRL: A Conditional Transformer Language Model for Controllable Generation. 2019arXiv preprint</p>
<p>Revisiting the weaknesses of reinforcement learning for neural machine translation. Samuel Kiegeland, Julia Kreutzer, arXiv:2106.089422021arXiv preprint</p>
<p>Pretraining language models with human preferences. Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L Buckley, Jason Phang, Ethan Samuel R Bowman, Perez, arXiv:2302.085822023arXiv preprint</p>
<p>Can neural machine translation be improved with user feedback?. Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, Stefan Riezler, arXiv:1804.059582018arXiv preprint</p>
<p>A reinforcement learning approach to interactivepredictive neural machine translation. Kin Tsz, Julia Lam, Stefan Kreutzer, Riezler, arXiv:1805.015532018arXiv preprint</p>
<p>Aligning text-to-image models using human feedback. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Shixiang Shane Gu, arXiv:2302.121922023arXiv preprint</p>
<p>Dialogue learning with human-in-the-loop. Jiwei Li, Alexander H Miller, Sumit Chopra, Marc ' , Aurelio Ranzato, Jason Weston, arXiv:1611.098232016arXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Chain of hindsight aligns language models with feedback. Liu, Sferrazza, Abbeel, arXiv:2302.026762023arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Yang Liu, Dan Iter, Xu Yichong, Wang Shuohang, Chenguang Xu Ruochen, Zhu, G-Eval, arXiv:2303.16634Nlg evaluation using gpt-4 with better human alignmentg. 2023arXiv preprint</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, arXiv:2301.136882023arXiv preprint</p>
<p>Quark: Controllable text generation with reinforced unlearning. X Lu, S Welleck, J Hessel, L Jiang, L Qin, P West, P Ammanabrolu, Y Choi, Advances in Neural Information Processing Systems. 2022</p>
<p>Quark: Controllable text generation with reinforced unlearning. Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, Yejin Choi, Advances in neural information processing systems. 352022</p>
<p>Self-refine: Iterative refinement with selffeedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, arXiv:2104.087732021arXiv preprint</p>
<p>Reinforcement learning for bandit neural machine translation with simulated human feedback. Khanh Nguyen, Hal Daumé, Iii , Jordan Boyd-Graber, arXiv:1707.074022017arXiv preprint</p>
<p>. OpenAI. Introducing chatgpt. </p>
<p>Model index for researchers. Openai, </p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023arXiv preprint</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. Sung Joon, Lindsay Park, Carrie Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Romain Paulus, Caiming Xiong, Richard Socher, arXiv:1705.04304A deep reinforced model for abstractive summarization. 2017arXiv preprint</p>
<p>Baolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. 2023arXiv preprint</p>
<p>Discovering language model behaviors with model-written evaluations. Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, arXiv:2212.092512022arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, arXiv:2305.182902023arXiv preprint</p>
<p>Is reinforcement learning (not) for natural language processing. Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi, arXiv:2210.01241Benchmarks, baselines, and building blocks for natural language policy optimization. 2022arXiv preprint</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, arXiv:2110.082072021arXiv preprint</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, Jan Leike, arXiv:2206.058022022arXiv preprint</p>
<p>Training language models with language feedback at scale. Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez, arXiv:2303.167552023arXiv preprint</p>
<p>High-dimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, arXiv:1506.024382015arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, Proximal policy optimization algorithms. 2017</p>
<p>When life gives you lemons, make cherryade: Converting feedback from bad responses into good labels. Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, Jing Xu, arXiv:2210.158932022arXiv preprint</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L , M Lai, A Bolton, Nature. 55076762017</p>
<p>Offline rl for natural language generation with implicit language q learning. Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, Sergey Levine, arXiv:2206.118712022arXiv preprint</p>
<p>Bandit structured prediction for learning from partial feedback in statistical machine translation. Artem Sokolov, Stefan Riezler, Tanguy Urvoy, arXiv:1601.044682016arXiv preprint</p>
<p>Learning to summarize from human feedback. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano, 2020</p>
<p>Lyceum: An efficient and scalable ecosystem for robot learning. Colin Summers, Kendall Lowrey, Aravind Rajeswaran, Siddhartha Srinivasa, Emanuel Todorov, Learning for Dynamics and Control. PMLR2020</p>
<p>Alpaca: A strong, replicable instruction-following modely. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, March 2023</p>
<p>Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego De Las, David Casas, Budden, arXiv:1801.00690Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. 2018arXiv preprint</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, 2012 IEEE/RSJ international conference on intelligent robots and systems. IEEE2012</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Turingbench: A benchmark environment for turing test in the age of neural text generation. Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, Dongwon Lee, arXiv:2109.132962021arXiv preprint</p>
<p>Solving math word problems with process-and outcomebased feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, 2022</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, 2022</p>
<p>Supernaturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Dialog-based language learning. J E Weston, Advances in Neural Information Processing Systems (NeurIPS). 2016</p>
<p>Google's neural machine translation system: Bridging the gap between human and machine translation. Y Wu, M Schuster, Z Chen, Q V Le, M Norouzi, W Macherey, M Krikun, Y Cao, Q Gao, K Macherey, arXiv:1609.081442016arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.056852023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
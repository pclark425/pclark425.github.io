<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9375 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9375</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9375</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-273821704</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.02454v2.pdf" target="_blank">Graph-based Confidence Calibration for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Reliable confidence estimation is essential for enhancing the trustworthiness of large language models (LLMs), especially in high-stakes scenarios. Despite its importance, accurately estimating confidence in LLM responses remains a significant challenge. In this work, we propose using an auxiliary learning model to assess response correctness based on the self-consistency of multiple outputs generated by the LLM. Our method builds a consistency graph to represent the agreement among multiple responses and uses a graph neural network (GNN) to estimate the likelihood that each response is correct. Experiments demonstrate that this method has strong calibration performance on various benchmark datasets and generalizes well to out-of-domain cases.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9375",
    "paper_id": "paper-273821704",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0047729999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graph-based Confidence Calibration for Large Language Models
22 May 2025</p>
<p>Yukun Li yukun.li@tufts.edu 
Department of Computer Science
Tufts University</p>
<p>Sijia Wang sijiawang@vt.edu 
Department of Computer Science
Virginia Tech</p>
<p>Lifu Huang lfuhuang@ucdavis.edu 
Department of Computer Science Virginia Tech
University of California
Davis</p>
<p>Li-Ping Liu liping.liu@tufts.edu 
Department of Computer Science
Tufts University</p>
<p>Graph-based Confidence Calibration for Large Language Models
22 May 20252CD31568072A0F97A79478F55A1C8862arXiv:2411.02454v2[cs.CL]https:openreview. netforum? id= BDPvuD5FTg
Reliable confidence estimation is essential for enhancing the trustworthiness of large language models (LLMs), especially in high-stakes scenarios.Despite its importance, accurately estimating confidence in LLM responses remains a significant challenge.In this work, we propose using an auxiliary learning model to assess response correctness based on the self-consistency of multiple outputs generated by the LLM.Our method builds a consistency graph to represent the agreement among multiple responses and uses a graph neural network (GNN) to estimate the likelihood that each response is correct.Experiments demonstrate that this method has strong calibration performance on various benchmark datasets and generalizes well to out-of-domain cases.</p>
<p>Introduction</p>
<p>In recent years, large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks such as question answering (Wei et al., 2022;Shen et al., 2023;Zheng et al., 2023;Qin et al., 2023;Singhal et al., 2023), text summarization (Tang et al., 2023;Deroy et al., 2023;Tam et al., 2023;Roit et al., 2023), and even creative writing (Gómez-Rodríguez &amp; Williams, 2023;Wang et al., 2024;Deng et al., 2024).Despite their impressive performance, LLMs often give incorrect responses in question-answering tasks.One particularly important challenge lies in calibrating the confidence levels of LLM-generated responses (Kuhn et al., 2022;Ulmer et al., 2022;Van Landeghem et al., 2022;Vazhentsev et al., 2023;Ulmer et al., 2024).Accurate confidence estimation is vital for deploying LLMs in the real world, as it enables users to gauge the reliability of the model's predictions and make informed decisions accordingly.On the contrary, miscalibrated confidence may lead to over-reliance on incorrect responses or unnecessary skepticism toward the correct ones.For example, a misleading response may steer a patient in the harmful direction when making health decisions; it may also cause an investor to make impulsive financial choices.</p>
<p>In this work, we focus on calibrating LLMs' confidence to better reflect the correctness of their responses.This task is challenging in several aspects.First, due to LLMs' superior ability to generate text, mistakes in their response often occur at the semantic level, making them hard to detect even for humans.There are methods using an auxiliary Language Model (e.g., DeBERTa (He et al., 2020)) to verify whether the LLM's response appropriately answers the question (Ulmer et al., 2024).Since the LLM is supposed to be much stronger than the LM, the LLM should be able to avoid most mistakes that can be detected by an LM; so this type of method may omit a significant fraction of wrong answers.Second, it is hard to detect mistakes from the LLM's internal working mechanism.Because the LLM uses many hidden layers to process the information, it is hard to discern the signal from a small number of hidden units.Even if this is possible, it is not easy to apply this type of method to black-box LLMs.</p>
<p>Recently, there has been some progress in quantifying the model's confidence in its own responses through consistency among the outputs generated by the model itself (Chen &amp; Mueller, 2023;Lin et al., 2024).These approaches demonstrate a strong correlation between an LLM's self-consistency and the actual correctness of its responses.However, because these methods depend on hand-crafted features, they often fail to accurately calibrate confidence, resulting in a mismatch between predicted confidence levels and the true accuracy of the answers.This raises an important research question: can we improve confidence calibration by learning from patterns of consistency across the LLM's own responses?</p>
<p>In this work, we propose an auxiliary learning model to improve confidence calibration.We begin by constructing similarity graphs from the LLM's multiple responses to the same questions, where graph edges represent the degree of agreement between responses.We then train a separate calibration model using graph neural networks (GNNs) to predict the correctness of each response.The key insight is that consistency among responses carries strong signals of correctness -for instance, a response that aligns well with many others is more likely to be accurate.Importantly, our model operates solely on response consistency and does not analyze the actual language content.This work focuses on practical scenarios where correctness reflects alignment with the training data, and does not address the case where training data itself contains consistent but incorrect information.The latter case is a more challenging scenario investigated by ongoing research (Biester et al., 2024;Shi et al., 2023;Krishnan &amp; Wu, 2019).</p>
<p>We further investigate the problem of transferring a calibration model across different question domains, which is crucial when target domains lack sufficient training data.Despite its importance, this problem has received limited attention in the literature.Our study demonstrates that the proposed auxiliary calibration model can generalize to new domains with minimal performance degradation.This generalization is from the observation that self-consistency can serve as a broadly applicable signal for confidence, enabling the learning model that relies solely on self-consistency to achieve strong transfer performance.</p>
<p>We evaluate the performance of the proposed method with an extensive empirical study that includes four datasets from different question domains.Empirical results show that our method achieves strong performance.Besides the improved calibration performance, our model enhances the ranking of an LLM's responses.The study has also tested our model and competing models in out-of-domain settings.The results show that the proposed method shows robust performance when generalizing to new domains.</p>
<p>In summary, our main contributions are:</p>
<p>• A learning-based GNN framework: We propose a learning-based framework leveraging GNNs to calibrate confidence values of LLMs' responses.</p>
<p>• Enhanced calibration performance: We conduct an extensive empirical study to evaluate the proposed method and show that it substantially outperforms recent methods in confidence calibration across several widely used benchmark datasets.</p>
<p>• Improved out-of-domain generalizability: We investigate the scenario of out-of-domain (OOD) confidence calibration and show the superior performance of the proposed method in this setting.</p>
<p>Related Work</p>
<p>Due to the urgent need to improve the reliability of LLMs, confidence estimation and calibration for these models have become active areas of research.Existing research in LLM uncertainty quantification can be summarized into two main categories: uncertainty quantification and confidence calibration (Geng et al., 2023).Confidence estimation for short responses (e.g., for multi-choice or yes-no questions) is generally less complicated than for long responses (Ye et al., 2024).For a brief response, the LLM's output logits are informative about its confidence; the easy comparisons of responses to the true answer facilitate both calibration and evaluation.Confidence estimation for long responses cannot simply depend on LLM's output logits (Duan et al., 2023;Bakman et al., 2024) because the logits indicate more about the probability of text and less about the semantics behind it.There are also methods using the internal state of an LLM (Ren et al., 2022;Beigi et al., 2024), but it is not always available to have such information about the LLM interface.</p>
<p>Another approach is to check the LLM's consistency in its responses.Kotelanski et al. (2023) demonstrate that repeated sampling and consistency checks across multiple outputs can serve as reliable proxies for model confidence.Manakul et al. (2023) generate multiple responses from the LLM and check the consistency between responses using various methods, including querying the LLM.Chen &amp; Mueller (2023) combine the consistency between responses and the LLM's self-reflection certainty to quantify the uncertainty.Kuhn et al. (2022) consider confidence from semantic equivalence and proposes a method based on clustering of responses.Lin et al. (2024) organize responses in a graph with their pairwise semantic similarity and then extract graph statistics for confidence estimation.Zhang et al. (2024) examine methods of comparing responses via entailment and contradiction relationships.These studies highlight the importance of semantic consistency in ranking an LLM's responses.However, manually designed features are limited in their ability to capture the full extent of self-consistency among LLM responses, leading to poor calibration performance.</p>
<p>To better calibrate the confidence estimation, some methods directly use correctness labels in their calibration procedures.Mielke et al. (2022) train a calibrator to predict the correctness of a response for a given question.With a similar idea, Ulmer et al. (2024) train a language model (e.g., DeBERTa) based on question-response pairs to predict the probability of responses' correctness.Based on SelfCheckGPT (Manakul et al., 2023) and JAFC (Tian et al., 2023), Chen et al. (2024) train supervised models to reduce grouping losses and improve the confidence estimation.The method by Liu et al. (2024) uses an LLM's latent representations to predict the correctness of responses.Detommaso et al. (2024) use the "multicalibration" technique to calibrate the probability of correctness.Fadeeva et al. (2023) offer a detailed comparative study of various confidence estimation methods, providing empirical evidence on their effectiveness across different tasks.However, these studies have not sufficiently exploited response consistency to predict the probabilities of the responses being correct.</p>
<p>Method</p>
<p>Our ultimate goal is to quantify the probability of the correctness of a response from an LLM.Since the LLM can give a correct answer with different phrases, we need to consider the probability that the response is semantically correct.</p>
<p>Background:</p>
<p>The formulation of semantic equivalence (Kuhn et al., 2022) provides a framework for our analysis.Let R be the space of all possible responses.Given a question q, the space R is divided into a set C q of semantic classes: R = ∪ C∈Cq C and C ′ ∩ C = ∅ for any two different semantic classes C, C ′ ∈ C q .Two responses r 1 , r 2 ∈ C in the same equivalent class are considered as the same semantic response: if one is the correct answer, the other is correct as well.Under an LLM, the semantic response C has probability:
p(C|q) = r∈C p(r|q). (1)
Here p(r|q) is the probability of a single response from the LLM.We also say that p(C|q) is the LLM's confidence in the semantic response C.</p>
<p>However, it is non-trivial to define the equivalent class, and we will discuss the approximation later.To estimate p(C|q), one approach is through semantic similarities between response samples of an LLM for the same question q.Let (r 1 , . . ., r n ) be n responses from the same question q, and they form k clusters Cq = { C1 , . . ., Ck } by their semantic similarity.We can use natural language inference (NLI) systems to predict the relationships (e.g., entailment and contradiction) between responses and derive their similarity.</p>
<p>We assume that each cluster C is from a different semantic class C, then p(C|q) can be approximated by
p(C|q) ≈ | C| n .
(2)</p>
<p>From the cluster probabilities, the uncertainty of the LLM on the question q is estimated as the entropy of the empirical distribution over clusters (Kuhn et al., 2022), and the confidence of a response r i ∈ C is estimated as | C|/n (assuming similarity values are binary) (Lin et al., 2024).Now, we depart from the setup of semantic classes and consider the correctness of responses.Let C * be the correct semantic answer to question q.Without knowing C * , a common assumption is that the model's confidence p(C|q) in the semantic response C reflects the correctness.For example, p(C|q) is approximately the probability of correctness:
p( Ck ′ ⊆ C * ) ≈ | Ck ′ | n . (3)
With this assumption, the more certain the model is about a semantic response, the more likely the response is correct.Conversely, a wide variation in the LLM's responses indicates low confidence in all responses r i and low accuracy.This pattern is also found in previous studies (Kuhn et al., 2022;Lin et al., 2024).</p>
<p>While a positive correlation exists between the LLM's confidence and the probability of correctness, the two quantities are unlikely to have the equal relationship shown in (3).Therefore, we need further calibration to estimate the probability of correctness.The overall framework of our confidence calibration model.Given an input question, our approach first generates multiple responses from the LLM and constructs a similarity-weighted graph based on these responses.This graph serves as the input for the GNN model, which calibrates the confidence of the LLM responses.In the weighted graph, the edge weight w ij is defined as sim cos (emb(r i ), emb(r j )), where i, j = 1, . . ., n.A higher weight indicates greater similarity between the responses.We also use cluster memberships as the node features to enhance the performance.</p>
<p>Confidence calibration as graph learning problem</p>
<p>Now, we set a supervised learning problem and train a model to calibrate the confidence of the correctness of responses.We first consider the correctness labels of the LLM's responses.In the supervised setting, we have a ground-truth answer a to the question q.Then we use a to assign correctness labels to sampled responses {r 1 , r 2 , ..., r n } for the same question q.</p>
<p>In our work, we have two approaches to label these responses.In the first approach, we use the ROUGE similarity.Specifically, we compute the ROUGE similarity sim R (r i , a) between a sampled response and the correct answer to decide the correctness label.
y i = 1[sim R (a, r i ) ≥ τ ], i = 1, . . . , n.
(4)</p>
<p>Here 1[•] is one if the condition is true or 0 otherwise.As shown in previous studies (Lin &amp; Och, 2004) and our own study, the ROUGE metric is reasonably accurate in measuring semantic similarity between short sentences.We follow the previous work, and set τ = 0.3 (Kuhn et al., 2022).</p>
<p>In the second method, we utilize the LLM to generate correctness labels.Specifically, we provide the question q and the standard answer a as the context, then ask whether the response r i answers the question q.The response from the LLM is then used as the label for r i .We denote the procedure as
y i = llm y (q, a, r i ) (5)
We provide the prompt for labeling in the Appendix F.</p>
<p>Both methods are automatic and can scale up to large datasets.To guaranteed the labeling quality, we also include a relatively small set of manual labels in our study.For each question, a human labeler inspects the question q and the true answer a, and then assign the correctness label y i to a response r i .</p>
<p>We then consider the input to the calibration model.We form a similar graph G over responses to encode information about their consistency.The graph contains the clustering structure of responses and likely further useful information to predict the correctness of responses.The graph G = (V, E, w) is a fully connected graph, with the node set V consisting of n responses and the edge weight w ij being the similarity between the pair of responses (r i , r j ).We compute the similarity from the two responses' embeddings.In particular, we first use the Sentence-BERT model (Reimers &amp; Gurevych, 2019) to compute the two responses' vector representations and then compute the cosine similarity
w ij = sim cos (emb(r i ), emb(r j )), i, j = 1, . . . , n.(6)
Here, emb(•) represents the embedding function.</p>
<p>Then, we treat the problem as a node classification problem (Xiao et al., 2022).In particular, we run a GNN gnn(•) to predict the probability of each response being correct p = gnn θ (G).( 7)</p>
<p>Here p ∈ [0, 1] n contains the probabilities for n responses being correct.</p>
<p>To provide clustering information to the GNN, we first run the K-means clustering algorithm on the responses' embeddings and assign cluster ids from 0 to K − 1 based on the order from largest to smallest (ties are randomly broken).Then, we feed each response's cluster membership as a one-hot feature input to the GNN.Therefore, the GNN's predictions are purely based on the relationships between responses in semantic space.We choose NOT to feed in the embedding vectors of responses to avoid the GNN's dependency on textual information.This helps the GNN to generalize to questions from different domains.The overall framework is shown in Fig 1.</p>
<p>The main purpose of the learning model is to calibrate p.One approach is to minimize the cross-entropy loss of p against correctness labels.The loss computed from the question q is
ℓ q = − n i=1 y i log pi + (1 − y i ) log(1 − pi ) (8)
Note that the loss is consistent marginally since the loss is minimized when pi = p(y i |G).An alternative approach is to minimize the squared error (y i − pi ), from which we get similar performances.We choose the cross-entropy loss in our work.A further consideration is to explicitly consider the similarity between pi and pj given the response similarity w ij .We leave such exploration to the future.</p>
<p>Improve the estimation through multiple prompts</p>
<p>It is well known that the syntactic form of a question influences responses and introduces additional variance.The variance of question forms from users may pass to the variance of responses and cause even diverse responses due to the lack of confidence.To account for question variances in real applications, we analyze the LLM's responses to multiple prompts derived from the same question.These responses are treated as answers to the same semantic question.We then apply the same method as before to predict the correctness of each response.</p>
<p>In particular, we rephrase the original question q into k different forms {q 1 , ..., q k } while maintaining the original sentence's semantic meaning.We employ a multiple rephrased questions strategy for answer sampling.Specifically, we prompt the GPT-4 to give k different but with the same meaning rephrased questions for the given question q.Then, we sample n/k responses from the LLM for each rephrased question and still get a total of n responses, from which the confidence calibration is the same as we have described above.For questions about which the LLM is less certain, the model is more likely to produce diverse responses.In this scenario, confidence calibration is more accurate because the model's uncertainty becomes more apparent.</p>
<p>Experiments</p>
<p>We evaluate the calibration performance of our proposed framework by comparing it against baseline methods.We also test their performances in the OOD setting, where their calibration parameters are decided on one dataset, but they are tested on a different dataset.We further analyze the quality of automatic labels and the sensitivities of our method under different hypter-parameter settings.</p>
<p>Dataset and experiment setup</p>
<p>Dataset: We conduct experiments on four datasets: (1) CoQA (Reddy et al., 2019), an open-book conversational question answering task; (2) TriviaQA (Joshi et al., 2017), a commonsense QA task.(3) Truth-fulQA (Lin et al., 2022a), a comparably more challenging dataset for factual QA tasks.and (4) HotpotQA, a question answering dataset that requires models to find and combine information from multiple passages to answer complex questions.We repeat the experiments 10 times, each with a different train/validation split and test the performance on the test set.</p>
<p>Baselines: We compare our methods with the following baselines.Length-normalized sequence likelihoods (Seq.likelihood) (Malinin &amp; Gales, 2021;Kuhn et al., 2022) is a standard measure for confidence.This method calculates the likelihood of each sequence and normalizes it by the length of the sequence to provide a fair comparison between different lengths of sequences.Platt scaling (Platt, 1999), a variant of the sequence likelihood baseline, applies Platt scaling to the raw likelihoods.GraphSpectral (Lin et al., 2024) uses the graph theory to estimate the confidence.Then we also include post-hoc uncertainty calibration, GraphSpectral+Iso and GraphSpectral+Platt into the baseline methods.Self-check GPT (Manakul et al., 2023) checked the consistency between responses querying the LLM.Verbalized Uncertainty (Lin et al., 2022b;Tian et al., 2023;Xiong et al., 2024) generates verbal statements about the model's confidence in its predictions.Verbalized Qual maps the confidence percent (Verbalized %) into numerical values.APRICOT (Ulmer et al., 2024), a supervised method, fine-tunes the Deberta language model to predict confidence scores for LLM outputs.Furthermore, we also include the baseline of applying two post-hoc uncertainty calibration methods, APRICOT+Iso and APRICOT+Platt, to adjust the confidence scores obtained by Apricot.We performed all the baseline experiments utilizing the open-source codebase and used the default hyperparameters.</p>
<p>Graph construction:</p>
<p>For each question, we generate 30 candidate answers through LLM prompting.</p>
<p>Each generated response is then processed using the Sentence-BERT (SBERT) model (Reimers &amp; Gurevych, 2019) to obtain the answer's high-dimensional embeddings.To quantify the semantic alignment between all responses, we compute the cosine similarity between every pair of answer embeddings.These scores are then utilized as edge weights in our similarity graph, where each node represents an individual answer, and the edges weights signify the degree of semantic relation between them.</p>
<p>Model hyper-parameters:</p>
<p>To ensure our model can capture complex and abstract features at each layer, our model comprises three GNN layers, with embedding dimensions of 256, 512, and 1024 for the first, second, and third layers, respectively.The initial learning rate was set to 10 −4 .If the validation loss did not show improvement over ten consecutive epochs, the learning rate was reduced by a factor of 0.9.The optimization was performed using the Adam optimizer, configured with hyperparameters β 1 = 0.9 and β 2 = 0.98.</p>
<p>LLMs:</p>
<p>We assess our confidence calibration method on two LLMs with good performance: Llama3-8B (Llama3)(Meta, 2024), and Vicuna-7b-v1.5 (Vicuna) (Zheng et al., 2024).</p>
<p>Labeling the data: To obtain the correctness label for CoQA and TriviaQA datasets, we followed previous work (Kuhn et al., 2022) and used the ROUGE-L metric for labeling.For the TruthfulQA dataset, given its focus on factual correctness and longer answers, we employed GPT4 (Potsaweel, 2024;Liu et al., 2023;Badshah &amp; Sajjad, 2024) to generate the labels.In addition, we conducted experiments on a smaller subset of data using human-annotated labels to validate the reliability of our automatic labeling process.Details of this manual annotation study are provided in Section4.4.</p>
<p>Evaluation metrics:</p>
<p>The evaluation metrics include Expectation Calibration Error (ECE), Brier Score, and AUROC.Specifically, (1) ECE quantifies the consistency between the prediction error and the uncertainty of the prediction.An ideal calibration curve should exhibit a lower ECE.It measures the consistency between the prediction error and the confidence of the prediction.Specifically, the confidence interval is grouped into fixed bins, and the average of the difference between the confidence and error in each bin is compared.
Formally, ECE is calculated as ECE = B b=1 n b N |acc(b) − conf (b)|,
where n b is the number of predictions in bin b, N is the total number of data points and acc(b) and conf (b) are the accuracy and confidence of bin b, respectively.(2) Brier Score (Brier, 1950), which is the mean squared difference between predicted probabilities and the actual binary results.Lower Brier Scores indicate better performance.(3) AUROC to indicate the models' discriminatory ability.</p>
<p>Further experimental configurations and prompting strategy are provided in the Appendix.The experiments are conducted on NVIDIA A100 GPUs with 80GB of memory.</p>
<p>Experiment results</p>
<p>For the Llama3 model, the confidence calibration performance on TriviaQA is shown in Table 1.For the TriviaQA dataset, it can be observed that the likelihood-based method performs poorly on the calibration error (ECE and Brier Score) and AUROC due to unreliable model prediction probability (Zhang et al., 2024).Platt scaling improves the ECE post-calibration and enhances the model's discriminative ability, resulting in higher AUROC results.However, this method cannot capture the semantic equivalence among answers, leading to sub-optimal performance.The Verbalized and Verbalized Qual prompts LLM to output confidence for their answers, improving AUROC by 3 − 5% compared with the likelihood baseline.However, it faces the overconfidence issue; thus, the calibration errors are still high.The GraphSpectral method can produce good confidence estimations, but its calibration performance is poor.Even with the addition of techniques such as Isotonic Calibration or Platt Scaling, this issue can only be partially mitigated.The auxiliary DeBERTa method combines the LLM outputs, Chain-of-Thoughts (CoT) outputs, and verbalized confidence to fine-tune the DeBERTa model for predicting confidence.Our method captures the prediction confidence based on the graph structure of LLM's responses in semantic space and achieves better ECE results.The ECE is reduced from 0.07 to 0.03 and improves the AUROC from 0.72 to 0.86 compared with the baseline calibration methods.The experiment results on TruthfulQA, HotpotQA and CoQA for the Llama3 model are shown in Table 1.These results show a similar trend, with our model achieving superior performance in confidence calibration compared to the baseline methods.Furthermore, we also compare the confidence calibration performance for the Vicuna model on the TriviaQA, CoQA, TruthfulQA and HotpotQA datasets.The results are summarized in Table 2. Our model consistently improves the calibration error compared to the baseline methods.Both GraphSpectral and our method have a similar assumption that the consistency level between responses indicates the confidence levels of these responses.However, GraphSpectral uses simple graph statistics to measure the confidence level of responses and could not capture complex relationships between response patterns and confidence levels (e.g.patterns beyond clustering structures).As a comparison, by framing the problem as a learning problem, our method has better opportunities to discover such relationships and provides a better calibration performance.Self-Check GPT uses its own evaluation on whether the context supports the answers and heavily relies on the LLM model's capability to do self-reflection, which can also be hallucinated.Thus the generated confidence scores are not calibrated well with empirical accuracy.</p>
<p>We present the reliability diagrams for all methods on TriviaQA to better understand the model improvement.</p>
<p>The reliability diagram is created by binning responses into 10 bins according to their confidence values.</p>
<p>Then the frequency of correctness is also computed for each bin.With an ideal calibration, the confidence value in each bin should match the frequency of correctness.At the same time, a good calibration model   should differentiate responses according to their confidence levels and give a wide distribution of confidence values in these bins.</p>
<p>The reliability diagram is shown in Fig. 2. (We also show other reliability diagrams for the different methods for Llamas on TriviaQA and CoQA in the Appendix E).The figure presents the reliability diagrams for different methods.In these diagrams, both the color intensity and the percentage numbers within each bar represent the proportion of total responses that fall into each respective bin.Specifically, larger proportions are depicted with colors closer to purple, while the height of each bar indicates the ratio of correct predictions within that bin.Our framework achieves a broad spread of responses across the bins, showing good differentiation capabilities; at the same time, the bar heights closely follow the diagonal line, indicating a good calibration performance.However, baseline methods cannot reach the same performance.The likelihood-based confidence methods exhibit significant overconfidence, indicating many responses rated with high confidence are actually wrong.The Auxiliary DeBERTa (APRICOT) method, which integrates LLM outputs, Chain-of-Thought (CoT) outputs, and verbalized confidence to train an auxiliary DeBERTa model, enhances the AUROC.However, it still experiences some overconfidence issues, potentially caused by the inherent overconfidence in the input verbalized confidence scores.Furthermore, the baseline methods' reliability diagrams revealed that this method frequently assigned high confidence scores to incorrect predictions, deviating markedly from the ideal calibration represented by the diagonal line.For example, the verbalized method's predictions in the highest confidence bins (80-90%) were significantly below the corresponding empirical accuracy, indicating a tendency to overestimate the certainty of its outputs.</p>
<p>Confidence calibration in the OOD setting</p>
<p>Domain shift poses significant challenges for deploying machine learning models in real-world scenarios where data variability is expected.To comprehensively assess the robustness and generalization capabilities of our proposed model compared to baseline methods, we conducted a series of out-of-domain (OOD) evaluations.Results and Analysis: Table .3 shows the OOD performance of the baseline methods.The OOD experiment results revealed that our model maintained a high level of performance across tested domains.Specifically, the model demonstrated consistent calibration, as evidenced by low ECE values and strong discriminative ability, reflected in high AUROC scores on in-domain and OOD datasets.For example, while the model achieved an ECE of 0.03 and an AUROC of 0.86 on TriviaQA (in-domain), it maintained an ECE of 0.077 and an AUROC of 0.77 on CoQA.Furthermore, the Brier scores across domains remained within acceptable ranges, demonstrating reliable probabilistic predictions even when faced with unfamiliar data distributions.The relatively small increase in ECE and a slight decrease in AUROC for OOD datasets suggest that while there is some degradation in performance, the model retains substantial robustness and accuracy.This is primarily because similarity graph patterns are highly invariant to the data distribution.Specifically, our model employs the consistency graph and the clustering feature that does not alter with data distribution shifts, enabling it to maintain stable performance across different datasets.</p>
<p>In contrast, Apricot typically relies on specific dataset features, which leads to poor performance in OOD scenarios.Furthermore, calibration methods like the Platt scaling can improve the confidence calibration in-domain, but their calibration effectiveness remains limited under domain shift scenarios.This is because this calibration technique mainly adjusts the output probabilities but does not fundamentally address the biases introduced by feature representation changes across distributions.</p>
<p>Checking the quality automatic labels</p>
<p>In our previous experiment, we used an automated method to assign the correctness labels to responses by comparing them to true answers with ROUGE-L scores.One question is whether these labels are reliable.</p>
<p>In this experiment, we manually annotate the labels of a small set of LLM-generated responses and examine the accuracy of labels generated from ROUGE-L scores.We also use these manual labels to evaluate topperforming algorithms.</p>
<p>To label the data manually, a human labeler checks the true answer of a question and labels the correctness of a response generated by the LLM.Then, automatic labels are compared to manual labels to get the accuracy value.Limited by our resources, we label 600 questions for each of the three datasets, TriviaQA, CoQA, and HotpotQA.Table 4 provides a breakdown of the accuracy across different datasets.The results  (Kuhn et al., 2022).</p>
<p>We further evaluated our model and two strong baselines (GraphSpectral+Platt and Apricot+Platt) with manual labels.As shown in Table 5 and Table 6, our model still outperforms the baselines, which is consistent with our previous evaluation results with automatic labels.</p>
<p>Sensitivity analysis</p>
<p>In this subsection, we conducted several sensitivity analyses of our model.</p>
<p>Number of training samples</p>
<p>We conducted experiments to examine the relationship between performance and the amount of training data.Specifically, we tested our model performance on the Llama3 TriviaQA dataset and varied the training size from 100 to 4000.The results are displayed in Table 7.We observed that the model's performance does not drop significantly with the reduced training data.These experimental results indicate that the model performs well with limited data availability, demonstrating its applicability in real-world scenarios where only smaller datasets are available.We also tested the baseline performance, the results are shown in Appendix E.</p>
<p>Hyperparameter sensitivity</p>
<p>We conduct the sensitivity analysis of our model's calibration error performance concerning two key configurations: the number of sampled answers used to construct the graph and the number of Graph Convolutional Network (GCN) layers in the GNN model.The results are displayed in Fig. 3.The experiments are conducted using the Llama3 model on the TriviaQA dataset.For Fig. 3 (a) experiments, we varied the number of sampled answers from 10 to 50 while keeping other configurations and hyperparameters fixed, as described in the experimental setup.We observe that increasing the number of sampled answers slightly improves performance, which then stabilizes.In Fig. 3(b), the sensitivity to the number of GCN layers indicates that our model remains stable with 1 to 4 layers, with the best performance observed at 3 layers.</p>
<p>Conclusion and Future Work</p>
<p>In summary, in this work, we proposed an effective strategy of calibrating the confidence an LLM's responses by learning an auxiliary GNN model on the self-consistency pattern among responses to the same questions.</p>
<p>Experiments demonstrate that the proposed approach improves confidence calibration significantly across several datasets compared to baseline methods.Our calibration model enhances the reliability of LLMs by evaluating response accuracy, enabling them to abstain from uncertain queries and empowering users to determine trust levels, thereby promoting responsible deployment in society.However, there are instances where an LLM might be highly confident in an incorrect semantic response, resulting in a consistency graph similar to that of a correct answer.In such cases, our calibration model may not provide an accurate confidence estimation.Without a straightforward solution to this problem, we leave the study of this issue to the future.</p>
<p>Broader Impacts: Our work's advancement in calibrating the confidence levels of LLM responses carries important social implications: by accurately estimating the likelihood that a response is correct, an LLM can choose to abstain from answering difficult questions or signal its level of confidence to help users make more informed decisions.We believe this work will enhance the trustworthiness of LLMs and contribute positively to their responsible use in society.</p>
<p>C Additional Cases</p>
<p>To better understand our method intuitively, we have collected a few examples to show the difference between our algorithm and APRICOT.To summarize our observation here:</p>
<ol>
<li>Multiple responses to the same question does reveal the LLM's confidence in its answers.2. The LLM's self-evaluation of confidence is often much higher than it should be -the LLM is overconfident about its responses.3. The chain-of-thought responses used by ApriCoT add some information to make each answer more complete and reasonable in the spirit of 1, but it mainly adds the information within one response, not as much information as the multiple responses used by ours.GCC-estimated confidence: 0.22</li>
</ol>
<p>CoT response: All the President's Men.</p>
<p>Self-evaluation: 95</p>
<p>ApriCoT-estimated confidence: 0.81</p>
<p>Example 3:</p>
<p>Question: BS is the international car registration of which country?</p>
<p>True answer:: Bahamas.</p>
<p>LLM response: Germany.</p>
<p>More responses from the LLM: Bahamas.Bahrain.Bangladesh.Bahamas.Belgium.Bahamas.Germany.Bhutan.Belgium.</p>
<p>GCC-estimated confidence: 0.34</p>
<p>CoT response: Belgium Self-evaluation: 98</p>
<p>ApriCoT-estimated confidence: 0.61</p>
<p>D Additional Visualizations</p>
<p>Besides the cases we show in the previous section.Here, we present several case examples and visualize the response patterns.We performed dimension reduction of LLM's responses to different questions and then plotted their embeddings to the 2-dimensional space.Fig 4 shows the responses generated by Llama3 as an example.From the figure, we observe that answers with higher confidence levels tend to cluster closely together, indicating consistency and reliability in these responses.In contrast, answers with lower confidence levels exhibit greater diversity, reflecting a broader range of possibilities.This behavior aligns well with our initial assumption, demonstrating that higher confidence responses are more consistent, while lower confidence responses capture a wider variety of potential answers.</p>
<p>E Additional results</p>
<p>Additional reliability plots We showed all reliability diagrams for Llama3 for TriviaQA in Fig. 5 and CoQA dataset in Fig. 6.To summarize the trends, we observe that Platt scaling narrows the range to the middle value.Verbalized uncertainty cannot generate a wider range of confidence values.GraphSpecral with Platt tends to generate a wider range of confidence values, but the bias can not be improved across all cases, resulting in the bar height not following the diagonal line closely.Our model can predict a wider range of confidence values and achieve better calibration in all settings, with the auxiliary consistency graph and clustering features contributing to improved calibration overall.</p>
<p>Additional baseline results</p>
<p>In Table 8, we showed the performance of the baseline method under varying training sizes.As the number of training data decreases, the ece will drop from 0.096 to 0.165.</p>
<p>Figure1: The overall framework of our confidence calibration model.Given an input question, our approach first generates multiple responses from the LLM and constructs a similarity-weighted graph based on these responses.This graph serves as the input for the GNN model, which calibrates the confidence of the LLM responses.In the weighted graph, the edge weight w ij is defined as sim cos (emb(r i ), emb(r j )), where i, j = 1, . . ., n.A higher weight indicates greater similarity between the responses.We also use cluster memberships as the node features to enhance the performance.</p>
<p>Figure 2 :
2
Figure 2: Reliability diagrams for different methods using 10 bins each for Vicuna on TriviaQA.The color, as well as the percentage number within each bar, indicates the proportion of total responses contained in each bin.Larger values are represented by colors closer to purple, and the height indicates the ratio of correct ones.We prefer a wide spread of responses in different bins (strong ability to differentiate responses) and bin heights along the diagonal line (accurate calibration).Our model outperforms others with a broader bin spread and better alignment with the diagonal for calibration accuracy.</p>
<p>Captain Jack Sparrow's father Edward Teague in the Pirates of the Caribbean films?True answer:: Keith Richards LLM response: David Schofield More responses from the LLM: Martin Klebba.Keith Richards, Geoffrey Rush, Martin Klebba.Keith Richards.Martin Klebba.David Schofield.(only list 7 responses here to save space) GCC-estimated confidence: 0which film will you find the Rodger Young?True answer:: Starship Troopers LLM response: The Bridge on the River Kwai.More responses from the LLM: The Greatest Story Ever Told.The Best Years of Our Lives.The Bridge on the River Kwai.The Best Years of Our Lives (1946).1949's Battleground.The Best Years of Our Lives.</p>
<p>Figure 4 :
4
Figure 4: Visualization of the generated response patterns</p>
<p>Table 1 :
1
Comparison of confidence calibration performance on TriviaQA, CoQA, TruthfulQA and HotpotQA dataset for Llama3
MethodTriviaQACoQATruthfulQAHotpotQABrier↓AUROC↑ECE↓Brier↓AUROC↑ECE↓Brier↓AUROC↑ECE↓Brier↓AUROC↑ECE↓GraphSpectral (GS).223 ± .002.842 ± .002.0762 ± .007.193 ± .001.762 ± .008.110 ± .019.332 ± .002.667 ± .012 .239 ± .019 .172 ± .017.783 ± .006.097 ± .014GS + Iso.167 ± .011.842 ± .002.058 ± .002.162 ± .008.762 ± .008.054 ± .002.191 ± .015.667 ± .012.088± .007.163 ± .012.783 ± .006.087 ± .021GS + Platt.165 ± .012.842 ± .002.049 ± .002.161 ± .009.762 ± .008.042 ± .001.221 ± .013.667 ± .012.151± .008.160 ± .014.783 ± .006.177 ± .012Self-checkGPT.332 ± .031.652 ± .020.187 ± .002.209 ± .020.633 ± .027.178 ± .010.362 ± .028.566 ± .028.353± .030.283 ± .014.673 ± .022.122 ± .030Seq. likelihood.536± .015.591 ± .002.220 ± .002.382 ± .012.571 ± .028.173 ± .009.465 ± .008.582 ± .025 .052 ± .009 .463 ± .018.651 ± .002.105± .012Platt.276 ± .006.591 ± .002.052 ± .002.258 ± .000.571 ± .0280.090 ± .009.271 ± .007.582 ± .025 .053 ± .008 .220 ± .0120.651 ± .002.142± .008Verbalized Qual.322 ± .034.618 ± .002.142 ± .002.302 ± .021.681 ± .022.160 ± .007.320 ± .037.622 ± .016 .140 ± .008 .358 ± .012.652 ± .006.150 ± .029Verbalized %.253 ± .021.663 ± .008.033 ± .002.423 ± .0120.662 ± .027.216 ± .002.540 ± .035.573 ± .029 .331 ± .008 .319 ± .014.672 ± .002.220± .023APRICOT.145 ± .002.723 ± .003.074 ± .005.173 ± .006.751 ± .022.132 ± .006.201 ± .003.657 ± .034 .062 ± .011 .171 ± .014 .823 ± .011 .081± .009APRICOT+Iso.182 ± .012.723 ± .003.073 ± .004.171 ± .009.751 ± .022.097 ± .003.200 ± .003.657 ± .034 .059 ± .011 .180 ± .012 .823 ± .011 .073± .002APRICOT+Platt.173 ± .018.723 ± .003.042 ± .004.169 ± .012.751 ± .022.069 ± .008.230 ± .003.657 ± .034 .056 ± .011 .171 ± .018 .823 ± .011 .071± .010Ours.136 ± .000 .864 ± .002 .035 ± .004 .124 ± .000.768 ± .009 .013 ± .003 .151 ± .003 .732 ± .012 .037±.013 .142 ± .000 .815 ± .002.023 ± .004Ours(Multi prompts) .141 ± .002.853 ± .002.036 ± .008 .118 ± .000 .776 ± .012 .015 ± .007.173 ± .003 .736 ± .007 .039 ± .013 .142 ± .000 .821 ± .002 .021 ± .006</p>
<p>Table 2 :
2
Comparison of confidence calibration performance on TriviaQA, CoQA, TruthfulQA and HotpotQA dataset for Vicuna
MethodTriviaQACoQATruthfulQAHotpotQABrier↓AUROC↑ECE↓Brier↓AUROC↑ECE↓Brier↓AUROC↑ECE↓Brier↓AUROC↑ECE↓GraphSpectral (GS).196±.000.792±.006.112±.014.275±.004.696±.004.202±.011.286±.006.647±.009.226±.013.162±.076.673±.008.202±.009GS + Iso.196±.000.792±.006.059±.008.245±.002.696±.004.037±.014.297±.008.647±.009.092±.004.165±.058.673±.008.085±.028GS + Platt.172±.000.792±.006.067±.009.228±.002.696±.004.055±.028.307±.007.647±.009.183±.003.160±.049.673±.008.073±.049Self-checkGPT.355±.001.640±.003.183±.014.221±.004.648±.009.192±.010.281±.012.552±.008.308±.017.295±.187.652±.187.370±.187Seq. likelihood.485±.002.581±.002.420±.029.302±.012.688±.002.169±.090.325±.022.587±.010.205±.012.493±.220.630±.050.223±.220Platt.342±.002.581±.002.255±.016.308±.015.688±.002.165±.008.288±.014.587±.010.181±.005.232±.050.630±.050.259±.050Verbalized Qual.393±.002.631±.007.029±.014.455±.022.495±.004 .009±.001 .471±.034.482±.060 .018±.005 .220±.140.652±.140.142±.140Verbalized %.402 ±.001 .523±.005.383±.012.492±.025.539±.003.324±.029.580±.022.566±.009.387±.017.342±.033.683±.033.033±.033APRICOT.196±.000.783±.006.068±.007.193±.004.742±.006.073±.009 .197±.007 .769±.002.118±.005.152±.001.782±.022.074±.074APRICOT+Iso.187±.000.783±.006.049±.004.193±.005.742±.006.064±.007 .197±.007 .769±.002.092±.008.142±.001.782±.022.073±.073APRICOT+Platt.186±.000.783±.006.052±.004.193±.005.742±.002.049±.004.204±.006.769±.002.085±.005.150±.001.782±.022.042±.042Ours.169±.000 .816±.002 .028±.004.184±.001.754±.004.032±.004.202±.003 .774±.001 .059±.006.132±.000 .</p>
<p>791±.002 .022±.002 Ours</p>
<p>(Multi prompts) .165±.000 .815±.006 .</p>
<p>025±.003 .168±.001 .763±.004
.030±.006.202±.004.764±.001.063±.004 .131±.000 .790±.003.025±.009</p>
<p>Table 3 :
3
Evaluation in the OOD setting.Models are trained on the TriviaQA from Llama3 responses and tested on out-of-domain datasets We evaluate the confidence calibration of different approaches under out-of-domain settings.We have two experiment configurations:
DatasetMethodBrier AUROC ECEGraphSpectral(w platt)0.170.720.095Llama3 CoQAApricot0.240.590.154Ours0.130.770.077GraphSpectral(w platt)0.320.630.324Llama3 TruthfulQAApricot0.250.540.197Ours0.230.660.16GraphSpectral(w platt)0.240.530.07Vicuna TriviaQAApricot0.190.760.13Ours0.170.810.07GraphSpectral(w platt)0.350.550.26Vicuna CoQAApricot0.240.590.08Ours0.220.730.10Experiment setup:</p>
<p>out-of-domain dataset OODD, and out-of-domain LLMs (OODL).</p>
<p>For OODD, we train the confidence calibration model on TriviaQA from Llama3 responses and test it on CoQA Llama3 and TruthfulQA Llama3 answers.For OODL, we use the same training data from Llama3 but test the Vicuna model's responses on the TriviaQA and CoQA datasets.We compare our model with the Apricot and GraphSpectral (with Platt scaling) methods.</p>
<p>Table 4 :
4
The accuracy of labels generated from ROUGE-L scores.The high accuracy indicates the reliability of automatic labels, as well as our evaluation above.
TriviaQA CoQA HotpotQAAccuracy of automatic labels 0.960.920.89</p>
<p>Table 5 :
5
Evaluation with manual labels (Llama3)
DatasetMethodAUROC ECEBrierGS+Platt0.790.050 0.15TriviaQAApricot+Platt 0.770.059 0.13Ours0.810.037 0.12GS+Platt0.740.056 0.12CoQAApricot+Platt 0.720.062 0.11Ours0.760.023 0.09GS+Platt0.760.088 0.19HotpotQAApricot+Platt 0.780.052 0.18Ours0.810.030 0.15</p>
<p>Table 6 :
6
Evaluation with manual labels (Vicuna)
DatasetMethodAUROC ECEBrierGS+Platt0.780.053 0.18TriviaQAApricot+Platt 0.770.052 0.17Ours0.800.032 0.16GS+Platt0.690.049 0.19CoQAApricot+Platt 0.720.057 0.15Ours0.770.032 0.12GS+Platt0.680.065 0.13HotpotQAApricot+Platt 0.770.076 0.14Ours0.780.041 0.12</p>
<p>Table 7
7: Performance under varying Training Sample Sizes Tr. size ECE AUROC Brier 100 0.082 0.812 0.201 300 0.062 0.836 0.187 500 0.059 0.842 0.181 1000 0.047 0.853 0.177 4000 0.029 0.860 0.14ECE0.0150 0.0250 0.0225 0.0200 0.0175 0.0125 0.0100 0.0075 0.005012 Number of GCN Layers 3 Figure 3: Sensitivity analysis of our model 4 0.020 0.018 0.016 0.012 0.014 ECE 0.010 0.008 10 20 30 40 0.006 Number of Sampled Answers 50</p>
<p>Table 8 :
8
Performance under varying Training Sample Sizes for the baseline methods(Apricot)</p>
<h1>of Training Samples ECE AUROC Brier1000.1650.6110.2293000.1330.6340.2115000.1120.6950.20410000.1050.7220.19240000.0960.7430.187</h1>
<p>AcknowledgmentWe thank all reviewers and the action editor for their insightful feedback.Li-Liping Liu's work was supported by NSF Award 2239869.We would like to thank Xu Han and Jacob Boerma for their help in labeling the data used in this work.A Hyperparameters and Model configurationsModel hyper-parameters:Our model used three GCN(or GIN) layers; typically, the embedding dimension was 256, 512, and 1024 for each layers.For the training process, we used the binary cross-entropy loss with a decaying learning rate that reduced the learning rate by 0.9 if the validation loss did not improve 10 epochs (with an initial learning rate of 10 − 4 and a minimum learning rate of 10 −7 ).The optimizer was Adam with β 1 = 0.9 and β 2 = 0.98.The batch size was 16, 32.For the rephrased prompts, we set k = 3, n = 30, so for each rephrased question, we sampled ten answers.While calculating the ECE, we divide the confidence into B = 10 bins.Evaluation Setup:For each question, we evaluate the confidence prediction corresponding to the most likely answers from the LLM response.The setup is consistent with the baseline methods.Graph construction:For each question, we prompt the LLM to give 30 answers, and the temperature for LLM is set to be 0.6.For each answer, the SentenceBert modelReimers &amp; Gurevych (2019)is used to get each answer's embedding.The cosine similarity between each answer's embedding is taken as the edge weight of the graph.We apply the K-Means clustering method to cluster similar semantic responses.The maximum cluster number is set as 3.B Computational costWe performed all experiments on NVIDIA A100 GPUs with 80GB of memory.Generating 30 responses using the Llama3 and Vicuna models for 6000 questions from CoQA and TriviaQA data required up to 4 hours, with an average of approximately 2 seconds per question.The CoQA dataset demanded more processing time due to the longer contextual information in the input.The time can be shortened by parallel sampling.F Prompting strategyHere, we showed the prompts to generate the rephrasing questions.Prompts for rephrasing questionsYou are a helpful assistant.I have a question that I would like to see it rephrased in multiple ways.Please take the original question and generate several rephrased versions while maintaining the same meaning, and the question can only have one direct answer.Here is the original question: . . . .Please provide four distinct rephrases of the question.The prompts for labeling:Prompts for labelingYou will be provided with a question, a reference answer, and a student's answer.Please evaluate the student's answer based on the reference answer and provide your score for the student's answer in the format: "Score: ".Assign a score of 0 for incorrect and 1 for correct.For example, "Score: 0" or "Score: 1".Do not include any additional information.Question: {. . .} Student answer: {. . .} Reference answer: {. . .} Now, please enter your score.Score:G Sensitivity and ablationsG.1 Sensitivity to accuracy metricIn this section, we evaluate the sensitivity of the threshold of accuracy metric of our models.From the results, it show that our method is relative insensitive of the threshold.We also provide the comparison with using sentence embedding feature as the node feature.We tested this method on TriviaQA.We got Brier scores 0.21, AUROC 0.75, and ECE values 0.11.The results indicate that GNN with sentence embedding as the node feature can produce worse results than our proposed approach.We see clear overfitting issues when GNN uses semantic features: the validation quickly shoots up after the initial dip.We conclude that GNN using semantic features could not generalize to test data.G.3 Use the Rouge similarity as the weight of the similarity graphWe provide the results of using Rouge-L as the weight of the similarity graph as shown in Table10.For the dataset with long-form answers (e.g., TruthfulQA), the performance is much worse than using the clusterID feature, From the results, we conclude Rouge-L is sensitive to the length of the responses.
Reference-guided verdict: Llms-as-judges in automatic evaluation of free-form text. Sher Badshah, Hassan Sajjad, arXiv:2408.092352024arXiv preprint</p>
<p>Mars: Meaning-aware response scoring for uncertainty estimation in generative llms. Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, Salman Avestimehr, arXiv:2402.117562024arXiv preprint</p>
<p>Internalinspector : Robust confidence estimation in llms through internal states. Mohammad Beigi, Ying Shen, Runing Yang, Zihao Lin, Qifan Wang, Ankith Mohan, Jianfeng He, Ming Jin, Chang-Tien Lu, Lifu Huang, arXiv:2406.120532024arXiv preprint</p>
<p>Llmclean: Context-aware tabular data cleaning via llm-generated ofds. Fabian Biester, Mohamed Abdelaal, Daniel Del, Gaudio , European Conference on Advances in Databases and Information Systems. Springer2024</p>
<p>Verification of Forecasts Expressed in Terms of Probability. Glenn W Brier, 10.1175/1520-0493(1950)078&lt;0001:VOFEIT&gt;2.0.CO;2Monthly Weather Review. 7811January 1950</p>
<p>Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. Jiuhai Chen, Jonas Mueller, 2023</p>
<p>Reconfidencing llms from the grouping loss perspective. Lihu Chen, Alexandre Perez-Lebel, Fabian M Suchanek, Gaël Varoquaux, arXiv:2402.049572024arXiv preprint</p>
<p>Can ai write classical chinese poetry like humans? an empirical study inspired by turing test. Zekun Deng, Hao Yang, Jun Wang, 2024</p>
<p>How ready are pre-trained abstractive models and llms for legal case judgement summarization?. Aniket Deroy, Kripabandhu Ghosh, Saptarshi Ghosh, 2023</p>
<p>Multicalibration for confidence scoring in llms. Gianluca Detommaso, Martin Bertran, Riccardo Fogliato, Aaron Roth, 2024</p>
<p>Shifting attention to relevance: Towards the uncertainty estimation of large language models. Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing Xu, Bhavya Kailkhura, Kaidi Xu, arXiv:2307.013792023arXiv preprint</p>
<p>Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, arXiv:2311.07383Lm-polygraph: Uncertainty estimation for language models. 2023arXiv preprint</p>
<p>A survey of language model confidence estimation and calibration. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, Iryna Gurevych, arXiv:2311.082982023arXiv preprint</p>
<p>A confederacy of models: a comprehensive evaluation of LLMs on creative writing. Carlos Gómez, -Rodríguez , Paul Williams, 10.18653/v1/2023.findings-emnlp.966Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, arXiv:2006.036542020arXiv preprint</p>
<p>TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer, 10.18653/v1/P17-1147Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. Regina Barzilay, Min-Yen Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171</p>
<p>Maia Kotelanski, Robert Gallo, Ashwin Nayak, Thomas Savage, arXiv:2312.03733Methods to estimate large language model confidence. 2023arXiv preprint</p>
<p>Alphaclean: Automatic generation of data cleaning pipelines. Sanjay Krishnan, Eugene Wu, arXiv:1904.118272019arXiv preprint</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. Chin-Yew Lin, Franz Josef, Och , Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL-04). the 42nd annual meeting of the association for computational linguistics (ACL-04)2004</p>
<p>TruthfulQA: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, 10.18653/v1/2022.acl-long.229Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 2022a1</p>
<p>Teaching models to express their uncertainty in words. Stephanie Lin, Jacob Hilton, Owain Evans, Transactions on Machine Learning Research. 2835-88562022b</p>
<p>Generating with confidence: Uncertainty quantification for black-box large language models. Zhen Lin, Shubhendu Trivedi, Jimeng Sun, Transactions on Machine Learning Research. 2024</p>
<p>Uncertainty estimation and quantification for llms: A simple supervised approach. Linyu Liu, Yu Pan, Xiaocheng Li, Guanting Chen, arXiv:2404.159932024arXiv preprint</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-Eval, Nlg evaluation using gpt-4 with better human alignment. 2023</p>
<p>Uncertainty estimation in autoregressive structured prediction. Andrey Malinin, Mark Gales, International Conference on Learning Representations. 2021</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark Gales, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>. Meta. Meta llama. 32024</p>
<p>Reducing conversational agents' overconfidence through linguistic calibration. Sabrina J Mielke, Arthur Szlam, Emily Dinan, Y-Lan Boureau, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. John C Platt, Advances in Large Margin Classifiers. MIT Press1999</p>
<p>Truthful-qa-llm-judges. Potsaweel, 2024</p>
<p>WebCPM: Interactive web search for Chinese long-form question answering. Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, Jie Zhou, 10.18653/v1/2023.acl-long.499Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>CoQA: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, 10.1162/tacl_a_00266Transactions of the Association for Computational Linguistics. 72019</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Out-of-distribution detection and selective generation for conditional language models. Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, Peter J Liu, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Factually consistent summarization via reinforcement learning with textual entailment feedback. Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Leonard Hussenot, Orgad Keller, Nikola Momchev, Sabela Ramos Garea, Piotr Stanczyk, Nino Vieillard, Olivier Bachem, Gal Elidan, Avinatan Hassidim, Olivier Pietquin, Idan Szpektor, 10.18653/v1/2023.acl-long.344Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Flan-moe: Scaling instructionfinetuned language models with sparse mixture of experts. Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Y Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, Denny Zhou, 10.48550/arXiv.2305.147052023</p>
<p>Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer, arXiv:2310.16789Detecting data from large language models. 2023arXiv preprint</p>
<p>Towards expert-level medical question answering with large language models. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera Y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S Sara Mahdavi, Joelle Barral, Dale Webster, Greg S Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, Vivek Natarajan, 2023</p>
<p>Evaluating the factual consistency of large language models through news summarization. Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, Colin Raffel, 10.18653/v1/2023.findings-acl.322Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Evaluating large language models on medical evidence summarization. Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan Nestor, Ali Soroush, Pierre Elias, Ziyang Xu, Ying Ding, Greg Durrett, Justin Rousseau, Chunhua Weng, Yifan Peng, 10.1038/s41746-023-00896-7Digital Medicine. 62023</p>
<p>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D Manning, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Exploring predictive uncertainty and calibration in NLP: A study on the impact of method &amp; data scarcity. Dennis Ulmer, Jes Frellsen, Christian Hardmeier, 10.18653/v1/2022.findings-emnlp.198Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022</p>
<p>Calibrating large language models using their generations only. Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, Seong Joon Oh, arXiv:2403.059732024arXiv preprint</p>
<p>Benchmarking scalable predictive uncertainty in text classification. Jordy Van Landeghem, Matthew Blaschko, Bertrand Anckaert, Marie-Francine Moens, 10.1109/ACCESS.2022.3168734IEEE Access. 102022</p>
<p>Hybrid uncertainty quantification for selective text classification in ambiguous tasks. Artem Vazhentsev, Gleb Kuzmin, Akim Tsvigun, Alexander Panchenko, Maxim Panov, Mikhail Burtsev, Artem Shelmanov, 10.18653/v1/2023.acl-long.652Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Foundation models for creative writing. Yuchen Eleanor Jiang, and Wangchunshu Zhou2024</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022</p>
<p>Graph neural networks in node classification: survey and evaluation. Machine Vision and Applications. Shunxin Xiao, Shiping Wang, Yuanfei Dai, Wenzhong Guo, 2022334</p>
<p>Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek F Wong, Emine Yilmaz, Shuming Shi, Zhaopeng Tu, arXiv:2401.12794Benchmarking llms via uncertainty quantification. 2024arXiv preprint</p>
<p>Caiqi Zhang, Fangyu Liu, Marco Basaldella, Nigel Collier, Luq, arXiv:2403.20279Long-text uncertainty quantification for llms. 2024arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>Why does chatgpt fall short in providing truthful answers?. Shen Zheng, Jie Huang, Kevin Chen, -Chuan Chang, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
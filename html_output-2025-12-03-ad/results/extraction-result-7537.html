<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7537 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7537</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7537</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-248376906</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2022.emnlp-main.26.pdf" target="_blank">Translation between Molecules and Natural Language</a></p>
                <p><strong>Paper Abstract:</strong> We present MolT5 - a self-supervised learning framework for pretraining models on a vast amount of unlabeled natural language text and molecule strings. MolT5 allows for new, useful, and challenging analogs of traditional vision-language tasks, such as molecule captioning and text-based de novo molecule generation (altogether: translation between molecules and language), which we explore for the first time. Since MolT5 pretrains models on single-modal data, it helps overcome the chemistry domain shortcoming of data scarcity. Furthermore, we consider several metrics, including a new cross-modal embedding-based metric, to evaluate the tasks of molecule captioning and text-based molecule generation. Our results show that MolT5-based models are able to generate outputs, both molecules and captions, which in many cases are high quality.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7537.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7537.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular T5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence Transformer framework that jointly pretrains on unlabeled natural language (C4) and SMILES strings (ZINC) using the T5 denoising objective, then fine-tunes for molecule captioning (SMILES→text) and text‑guided de novo molecule generation (text→SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5 (T5-derived encoder-decoder Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Variants: Small (MolT5-Small reported; small initialized from T5-Small, MolT5-Small ~60M reported), Base (MolT5-Base), Large (MolT5-Large initialized from T5-Large, T5-Large ≈770M parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Base pretrained language model initialized from public T5 checkpoints, further pre-trained on mixed monomodal corpora (C4+ZINC) with a denoising 'replace corrupted spans' objective, then task-fine-tuned (supervised seq2seq) on ChEBI-20</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Cheminformatics (molecular design and description)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Two text-based simulation tasks: (1) Molecule captioning — generate a natural-language description/caption for an input molecule represented as a SMILES string; (2) Text-guided de novo molecule generation — generate a SMILES string for a molecule matching a given natural-language description.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>No few-shot prompting; model is fine-tuned in a supervised seq2seq manner. Pretraining used denoising (replace corrupted spans) on two monolingual corpora (natural language C4 and SMILES ZINC) without explicit cross-modal alignment. Inference decoding: standard beam search (beam size = 5) by default; an alternative 'high-validity' decoding uses diverse beam search (beam width and group = 30, diversity penalty = 0.5) and selects first syntactically valid beam via RDKit.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Captioning: BLEU-2/4, ROUGE-1/2/L, METEOR, Text2Mol cross-modal embedding similarity; Generation: BLEU, Exact SMILES match, Levenshtein distance, MACCS/RDK/Morgan fingerprint Tanimoto similarities (FTS), Fréchet ChemNet Distance (FCD), Text2Mol similarity, Validity (percent valid SMILES via RDKit). Retrieval metrics (MRR/Hits@K) used to probe diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Captioning (MolT5-Large vs T5-Large on ChEBI-20 test): BLEU-2 0.594 (MolT5-Large), BLEU-4 0.508, ROUGE-1 0.654, ROUGE-2 0.510, ROUGE-L 0.594, METEOR 0.614, Text2Mol 0.582 (ground-truth Text2Mol baseline ~0.609). Generation (MolT5-Large on ChEBI-20 test): BLEU-2 0.854, Exact SMILES match 0.311, Levenshtein 16.071, MACCS FTS 0.834, RDK FTS 0.746, Morgan FTS 0.684, FCD 1.20, Text2Mol 0.554, Validity 0.905.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Baselines reported on same splits: RNN-GRU (4-layer) and a vanilla 6-layer Transformer performed substantially worse; T5 variants (pretrained on text-only) used as strong baselines: for captioning T5-Large BLEU-2 0.558, METEOR 0.586, Text2Mol 0.563; for generation T5-Large Exact 0.279, BLEU-2 0.854, Validity 0.902. RNN and Transformer baselines had much lower retrieval/Text2Mol and molecular-similarity scores.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model size (small/base/large) — larger models perform better', 'Pretraining corpus composition: C4 (text) vs ZINC (SMILES) vs combined C4+ZINC — combined pretraining improved captioning and (when normalized by validity) generation metrics', 'Pretraining scale (1M pretraining steps used) and initialization from T5 checkpoints', 'Fine-tuning on limited gold data (ChEBI-20; dataset size and domain scarcity)', 'Decoding strategy (standard beam search vs diverse-beam high-validity decoding) strongly influences validity of generated SMILES', 'Molecule string representation (SMILES) limitations affect validity and generation quality', 'Evaluation choices (string-based vs fingerprint/embedding-based metrics) influence apparent performance']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Pretraining: 1,000,000 steps, batch size 256 evenly split between text and SMILES (C4+ZINC). Fine-tuning: 50,000 steps on ChEBI-20 (80/10/10 split). Sequence length cap: 512 tokens. Default inference beam size = 5; high-validity decoding uses diverse beam (beam width/group 30, diversity penalty 0.5) and RDKit filtering. Hardware: TPUs for pretraining/finetuning; V100 GPUs for RNN/Transformer baselines. Converted HuggingFace checkpoints reported as well (small output differences).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>SMILES syntactic invalidity (some generated outputs invalid); SMILES representation has known weaknesses (SELFIES noted as alternative but incompatible with pretrained T5); limited size and domain coverage of ChEBI-20 causes out-of-distribution inputs to fail or to return nearest-known compounds; missing details such as atom charge in some generations (e.g. Ruthenium example); potential biases from large internet pretraining; model can produce molecules that are chemically plausible but novel/unverified and should not be used for medical purposes without rigorous evaluation; retrieval-based diversity analysis shows some models collapse to repeating outputs (vanilla Transformer), though MolT5 generations are sufficiently distinct to be retrievable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Translation between Molecules and Natural Language', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7537.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7537.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 (Text-to-Text Transfer Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Publicly available encoder-decoder Transformer (T5) pretrained on large text corpora (C4) using a span-denoising objective; used here as a baseline model (small/base/large checkpoints) fine-tuned for the same SMILES↔text tasks to evaluate how text-only pretraining performs as a text-based simulator in chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (public checkpoints: t5.1.1.small, t5.1.1.base, t5.1.1.large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Small, Base, Large (T5-Large ≈ 770M parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Pretrained text-only seq2seq model (C4 pretraining) fine-tuned on ChEBI-20 for molecule captioning and text→SMILES generation (no molecule pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Cheminformatics (used as a text-based simulator for molecule descriptions and SMILES generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Same tasks as MolT5: SMILES→text molecule captioning and text→SMILES de novo molecule generation via supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Supervised fine-tuning (seq2seq) on ChEBI-20; inference via beam search (beam=5). No few-shot or chain-of-thought prompting used.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Same suite as MolT5: BLEU-2/4, ROUGE, METEOR, Text2Mol similarity for captioning; for generation BLEU, Exact SMILES match, Levenshtein, MACCS/RDK/Morgan FTS, FCD, Text2Mol, Validity.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Captioning (T5-Large on ChEBI-20 test): BLEU-2 0.558, BLEU-4 0.467, ROUGE-1 0.630, ROUGE-2 0.478, ROUGE-L 0.569, METEOR 0.586, Text2Mol 0.563. Generation (T5-Large): BLEU-2 0.854, Exact SMILES match 0.279, Levenshtein 16.721, MACCS FTS 0.823, RDK FTS 0.731, Morgan FTS 0.670, FCD ≈1.22, Text2Mol 0.552, Validity 0.902.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared against RNN-GRU and vanilla Transformer baselines (both much weaker); when compared to MolT5, T5 (text-only pretraining) achieves close performance for many metrics but MolT5 shows statistically significant improvements on captioning and modest but often significant improvements on some molecular-similarity metrics and validity.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Absence of molecule-specific pretraining (text-only pretraining) vs combined text+SMILES pretraining (MolT5)', 'Model scale (larger T5 improves over smaller T5 variants)', 'Fine-tuning on ChEBI-20 (small gold dataset) — limited supervised data affects performance', 'Decoding settings (beam size) affect generation validity and exact matches']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Fine-tuned for 50,000 steps on ChEBI-20, sequence length limit 512 tokens, dropout settings varied by model size (large uses 0.1), inference beam size = 5, T5 checkpoints converted to HuggingFace for reported inference as well.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Because T5 lacks molecule-specific pretraining, it sometimes produces less chemically-aligned outputs (e.g., lower Text2Mol scores) and lower validity without molecule pretraining; still, large text-only pretrained T5-Large can generate many valid and relevant molecules, indicating that large text LMs have some capacity to act as text-based simulators in chemistry but benefit from domain pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Translation between Molecules and Natural Language', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Text2mol: Cross-modal molecule retrieval with natural language queries <em>(Rating: 2)</em></li>
                <li>Chemformer: A pre-trained transformer for computational chemistry <em>(Rating: 2)</em></li>
                <li>MegaMolBART <em>(Rating: 2)</em></li>
                <li>Molecular sets (moses): a benchmarking platform for molecular generation models <em>(Rating: 1)</em></li>
                <li>ChemBERTa: Large-scale selfsupervised pretraining for molecular property prediction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7537",
    "paper_id": "paper-248376906",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "MolT5",
            "name_full": "Molecular T5",
            "brief_description": "A sequence-to-sequence Transformer framework that jointly pretrains on unlabeled natural language (C4) and SMILES strings (ZINC) using the T5 denoising objective, then fine-tunes for molecule captioning (SMILES→text) and text‑guided de novo molecule generation (text→SMILES).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MolT5 (T5-derived encoder-decoder Transformer)",
            "model_size": "Variants: Small (MolT5-Small reported; small initialized from T5-Small, MolT5-Small ~60M reported), Base (MolT5-Base), Large (MolT5-Large initialized from T5-Large, T5-Large ≈770M parameters)",
            "model_type": "Base pretrained language model initialized from public T5 checkpoints, further pre-trained on mixed monomodal corpora (C4+ZINC) with a denoising 'replace corrupted spans' objective, then task-fine-tuned (supervised seq2seq) on ChEBI-20",
            "scientific_domain": "Chemistry / Cheminformatics (molecular design and description)",
            "simulation_task_description": "Two text-based simulation tasks: (1) Molecule captioning — generate a natural-language description/caption for an input molecule represented as a SMILES string; (2) Text-guided de novo molecule generation — generate a SMILES string for a molecule matching a given natural-language description.",
            "prompting_strategy": "No few-shot prompting; model is fine-tuned in a supervised seq2seq manner. Pretraining used denoising (replace corrupted spans) on two monolingual corpora (natural language C4 and SMILES ZINC) without explicit cross-modal alignment. Inference decoding: standard beam search (beam size = 5) by default; an alternative 'high-validity' decoding uses diverse beam search (beam width and group = 30, diversity penalty = 0.5) and selects first syntactically valid beam via RDKit.",
            "evaluation_metric": "Captioning: BLEU-2/4, ROUGE-1/2/L, METEOR, Text2Mol cross-modal embedding similarity; Generation: BLEU, Exact SMILES match, Levenshtein distance, MACCS/RDK/Morgan fingerprint Tanimoto similarities (FTS), Fréchet ChemNet Distance (FCD), Text2Mol similarity, Validity (percent valid SMILES via RDKit). Retrieval metrics (MRR/Hits@K) used to probe diversity.",
            "reported_accuracy": "Captioning (MolT5-Large vs T5-Large on ChEBI-20 test): BLEU-2 0.594 (MolT5-Large), BLEU-4 0.508, ROUGE-1 0.654, ROUGE-2 0.510, ROUGE-L 0.594, METEOR 0.614, Text2Mol 0.582 (ground-truth Text2Mol baseline ~0.609). Generation (MolT5-Large on ChEBI-20 test): BLEU-2 0.854, Exact SMILES match 0.311, Levenshtein 16.071, MACCS FTS 0.834, RDK FTS 0.746, Morgan FTS 0.684, FCD 1.20, Text2Mol 0.554, Validity 0.905.",
            "baseline_accuracy": "Baselines reported on same splits: RNN-GRU (4-layer) and a vanilla 6-layer Transformer performed substantially worse; T5 variants (pretrained on text-only) used as strong baselines: for captioning T5-Large BLEU-2 0.558, METEOR 0.586, Text2Mol 0.563; for generation T5-Large Exact 0.279, BLEU-2 0.854, Validity 0.902. RNN and Transformer baselines had much lower retrieval/Text2Mol and molecular-similarity scores.",
            "factors_reported": [
                "Model size (small/base/large) — larger models perform better",
                "Pretraining corpus composition: C4 (text) vs ZINC (SMILES) vs combined C4+ZINC — combined pretraining improved captioning and (when normalized by validity) generation metrics",
                "Pretraining scale (1M pretraining steps used) and initialization from T5 checkpoints",
                "Fine-tuning on limited gold data (ChEBI-20; dataset size and domain scarcity)",
                "Decoding strategy (standard beam search vs diverse-beam high-validity decoding) strongly influences validity of generated SMILES",
                "Molecule string representation (SMILES) limitations affect validity and generation quality",
                "Evaluation choices (string-based vs fingerprint/embedding-based metrics) influence apparent performance"
            ],
            "experimental_conditions": "Pretraining: 1,000,000 steps, batch size 256 evenly split between text and SMILES (C4+ZINC). Fine-tuning: 50,000 steps on ChEBI-20 (80/10/10 split). Sequence length cap: 512 tokens. Default inference beam size = 5; high-validity decoding uses diverse beam (beam width/group 30, diversity penalty 0.5) and RDKit filtering. Hardware: TPUs for pretraining/finetuning; V100 GPUs for RNN/Transformer baselines. Converted HuggingFace checkpoints reported as well (small output differences).",
            "limitations_or_failure_modes": "SMILES syntactic invalidity (some generated outputs invalid); SMILES representation has known weaknesses (SELFIES noted as alternative but incompatible with pretrained T5); limited size and domain coverage of ChEBI-20 causes out-of-distribution inputs to fail or to return nearest-known compounds; missing details such as atom charge in some generations (e.g. Ruthenium example); potential biases from large internet pretraining; model can produce molecules that are chemically plausible but novel/unverified and should not be used for medical purposes without rigorous evaluation; retrieval-based diversity analysis shows some models collapse to repeating outputs (vanilla Transformer), though MolT5 generations are sufficiently distinct to be retrievable.",
            "uuid": "e7537.0",
            "source_info": {
                "paper_title": "Translation between Molecules and Natural Language",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "T5 (baseline)",
            "name_full": "T5 (Text-to-Text Transfer Transformer)",
            "brief_description": "Publicly available encoder-decoder Transformer (T5) pretrained on large text corpora (C4) using a span-denoising objective; used here as a baseline model (small/base/large checkpoints) fine-tuned for the same SMILES↔text tasks to evaluate how text-only pretraining performs as a text-based simulator in chemistry.",
            "citation_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "mention_or_use": "use",
            "model_name": "T5 (public checkpoints: t5.1.1.small, t5.1.1.base, t5.1.1.large)",
            "model_size": "Small, Base, Large (T5-Large ≈ 770M parameters)",
            "model_type": "Pretrained text-only seq2seq model (C4 pretraining) fine-tuned on ChEBI-20 for molecule captioning and text→SMILES generation (no molecule pretraining).",
            "scientific_domain": "Chemistry / Cheminformatics (used as a text-based simulator for molecule descriptions and SMILES generation)",
            "simulation_task_description": "Same tasks as MolT5: SMILES→text molecule captioning and text→SMILES de novo molecule generation via supervised fine-tuning.",
            "prompting_strategy": "Supervised fine-tuning (seq2seq) on ChEBI-20; inference via beam search (beam=5). No few-shot or chain-of-thought prompting used.",
            "evaluation_metric": "Same suite as MolT5: BLEU-2/4, ROUGE, METEOR, Text2Mol similarity for captioning; for generation BLEU, Exact SMILES match, Levenshtein, MACCS/RDK/Morgan FTS, FCD, Text2Mol, Validity.",
            "reported_accuracy": "Captioning (T5-Large on ChEBI-20 test): BLEU-2 0.558, BLEU-4 0.467, ROUGE-1 0.630, ROUGE-2 0.478, ROUGE-L 0.569, METEOR 0.586, Text2Mol 0.563. Generation (T5-Large): BLEU-2 0.854, Exact SMILES match 0.279, Levenshtein 16.721, MACCS FTS 0.823, RDK FTS 0.731, Morgan FTS 0.670, FCD ≈1.22, Text2Mol 0.552, Validity 0.902.",
            "baseline_accuracy": "Compared against RNN-GRU and vanilla Transformer baselines (both much weaker); when compared to MolT5, T5 (text-only pretraining) achieves close performance for many metrics but MolT5 shows statistically significant improvements on captioning and modest but often significant improvements on some molecular-similarity metrics and validity.",
            "factors_reported": [
                "Absence of molecule-specific pretraining (text-only pretraining) vs combined text+SMILES pretraining (MolT5)",
                "Model scale (larger T5 improves over smaller T5 variants)",
                "Fine-tuning on ChEBI-20 (small gold dataset) — limited supervised data affects performance",
                "Decoding settings (beam size) affect generation validity and exact matches"
            ],
            "experimental_conditions": "Fine-tuned for 50,000 steps on ChEBI-20, sequence length limit 512 tokens, dropout settings varied by model size (large uses 0.1), inference beam size = 5, T5 checkpoints converted to HuggingFace for reported inference as well.",
            "limitations_or_failure_modes": "Because T5 lacks molecule-specific pretraining, it sometimes produces less chemically-aligned outputs (e.g., lower Text2Mol scores) and lower validity without molecule pretraining; still, large text-only pretrained T5-Large can generate many valid and relevant molecules, indicating that large text LMs have some capacity to act as text-based simulators in chemistry but benefit from domain pretraining.",
            "uuid": "e7537.1",
            "source_info": {
                "paper_title": "Translation between Molecules and Natural Language",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Text2mol: Cross-modal molecule retrieval with natural language queries",
            "rating": 2
        },
        {
            "paper_title": "Chemformer: A pre-trained transformer for computational chemistry",
            "rating": 2
        },
        {
            "paper_title": "MegaMolBART",
            "rating": 2
        },
        {
            "paper_title": "Molecular sets (moses): a benchmarking platform for molecular generation models",
            "rating": 1
        },
        {
            "paper_title": "ChemBERTa: Large-scale selfsupervised pretraining for molecular property prediction",
            "rating": 1
        }
    ],
    "cost": 0.01631175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Translation between Molecules and Natural Language</p>
<p>Carl Edwards 
University of Illinois Urbana-Champaign</p>
<p>Tuan Lai tuanml2@illinois.edu 
University of Illinois Urbana-Champaign</p>
<p>the Moonshot Factory</p>
<p>Kevin Ros kjros2@illinois.edu 
University of Illinois Urbana-Champaign</p>
<p>Garrett Honke 
the Moonshot Factory</p>
<p>Kyunghyun Cho kyunghyun.cho@nyu.edu 
New York University
4 Genentech</p>
<p>Heng Ji hengji@illinois.edu 
University of Illinois Urbana-Champaign</p>
<p>Translation between Molecules and Natural Language
7874C63D81839A612A145E56B5B1706E
We present MolT5 -a self-supervised learning framework for pretraining models on a vast amount of unlabeled natural language text and molecule strings.MolT5 allows for new, useful, and challenging analogs of traditional vision-language tasks, such as molecule captioning and text-based de novo molecule generation (altogether: translation between molecules and language), which we explore for the first time.Since MolT5 pretrains models on single-modal data, it helps overcome the chemistry domain shortcoming of data scarcity.Furthermore, we consider several metrics, including a new cross-modal embedding-based metric, to evaluate the tasks of molecule captioning and text-based molecule generation.Our results show that MolT5-based models are able to generate outputs, both molecules and captions, which in many cases are high quality 1 .</p>
<p>Introduction</p>
<p>Imagine a future where a doctor can write a few sentences describing a specialized drug for treating a patient and then receive the exact structure of the desired drug.Although this seems like science fiction now, with progress in integrating natural language and molecules, it might well be possible in the future.Historically, drug creation has commonly been done by humans who design and build individual molecules.In fact, bringing a new drug to market can cost over a billion dollars and take over ten years (Gaudelet et al., 2021).Recently, there has been considerable interest in using new deep learning tools to facilitate in silico drug design-a field often called cheminformatics (Rifaioglu et al., 2018).Yet, many of these experiments still focus on molecules and their low-level properties such as logP (the octanol-water partition coefficient) (Bagal et al., 2021).In the future, * indicates equal contributions. 1 All resources are publicly available at github.com/blendernlp/MolT5</p>
<p>The molecule is an eighteen-membered homodetic cyclic peptide which is isolated from Oscillatoria sp. and exhibits antimalarial activity against the W2 chloroquine-resistant strain of the malarial parasite, Plasmodium falciparum.It has a role as a metabolite and an antimalarial.It is a homodetic cyclic peptide, a member of 1,3oxazoles, a member of 1,3-thiazoles and a macrocycle.we foresee a need for a higher-level control over molecule design, which can easily be facilitated by natural language.</p>
<p>In this work, we pursue an ambitious goal of translating between molecules and language by proposing two new tasks: molecule captioning and text-guided de novo molecule generation.In molecule captioning, we take a molecule (e.g., as a SMILES string) and generate a caption that describes it (Figure 2).In text-guided molecule generation, the task is to create a molecule that matches a given natural language description (Figure 1).These new tasks would help to accelerate research in multiple scientific domains by enabling chemistry domain experts to generate new molecules and better understand them using natural language.</p>
<p>While our proposed molecule-language tasks share some similarities with vision-language tasks, they have several inherent difficulties that separate them from existing vision-language analogs: 1) creating annotations for molecules requires significant domain expertise, 2) thus, it is significantly more difficult to acquire large numbers of molecule-</p>
<p>SMILES representation 3D View</p>
<p>Caption</p>
<p>The molecule is an organic disulfide isolated from the whole broth of the marine-derived fungus Exserohilum rostratum and has been shown to exhibit antineoplastic activity.It has a role as a metabolite and an antineoplastic agent.It is a bridged compound, a lactam, an organic disulfide, an organic heterohexacyclic compound, a secondary alcohol, a cyclic ketone and a diol.</p>
<p>Molecule Captioning Image Captioning</p>
<ol>
<li>
<p>a cat sitting on top of an open laptop computer.</p>
</li>
<li>
<p>a cat that is sitting on top of a lap top.</p>
</li>
<li>
<p>a cat is sitting on the keyboard of a laptop.(Chen et al., 2015) and molecule captioning.Molecule captioning is considerably more difficult because of the increased linguistic variety in possible captions.description pairs, 3) the same molecule can have many functions and thus be described in very different ways, which causes 4) existing evaluation measures based on reference descriptions, such as BLEU, to fail to adequately evaluate these tasks.</p>
</li>
</ol>
<p>To address the issue of data scarcity (i.e., difficulties 1 and 2), we propose a new self-supervised learning framework named MolT5 (Molecular T5) that is inspired by the recent progress in pretraining multilingual models (Devlin et al., 2019;Liu et al., 2020).MolT5 first pretrains a model on a vast amount of unlabeled natural language text and molecule strings using a simple denoising objective.After that, the pretrained model is finetuned on limited gold standard annotations.Furthermore, to adequately evaluate models for molecule captioning or generation, we consider various kinds of metrics and also adopt a new metric based on Text2Mol (Edwards et al., 2021).We repurpose this retrieval model for assessing the similarity between the ground truth molecule/description and the generated description/molecule, respectively.</p>
<p>To the best of our knowledge, there is no work yet on molecule captioning or text-guided molecule generation.The closest existing work to molecule captioning falls within the scope of image captioning (Vinyals et al., 2015).However, molecule captioning is arguably much more challenging due to the increased linguistic variety in possible captions (Figure 2).A molecule could be described with an IUPAC name, with one of many different synthetic routes from known precursor molecules, in terms of the properties (e.g.carcinogenic or lipophilic), with the applications of the molecule (e.g. a dye, an antipneumonic, or an antifungal), or in terms of its functional groups (e.g."substituted by hydroxy groups at positions 5 and 7 and a methyl group at position 8"), among other methods.</p>
<p>In summary, our main contributions are: 1.We propose two new tasks: 1) molecule captioning, where a description is generated for a given molecule, and 2) text-based de novo molecule generation, where a molecule is generated to match a given text description.2. We consider multiple evaluation metrics for these new tasks, and we adopt a new crossmodal retrieval similarity metric based on Text2Mol (Edwards et al., 2021).3. We propose MolT5: a self-supervised learning framework for jointly training a model on molecule string representations and natural language text, which can then be finetuned on a cross-modal task.</p>
<p>Tasks</p>
<p>With the ambitious goal of bi-directional translation between molecules and language, we propose two new novel tasks: molecule captioning (Section 2.1) and text-based molecule generation (Section 2.2).</p>
<p>Molecule Captioning</p>
<p>For any given molecule, the goal of molecule captioning is to describe the molecule and what it does.An example is shown in Figure 2. Molecules are often represented as SMILES strings (Weininger, 1988;Weininger et al., 1989), a linearization of the molecular graph which can be interpreted as a language for molecules.Thus, this task can be considered an exotic translation task, and sequence to sequence models serve as excellent baselines.</p>
<p>Text-Based de Novo Molecule Generation</p>
<p>The goal of the de novo molecule generation task is to train a model which can generate a variety of possible new molecules.Existing work tends to focus on evaluating the model coverage of the chemical space (Polykovskiy et al., 2020).Instead, we propose generating molecules based on a natural language description of the desired moleculethis is essentially swapping the input and output for the captioning task.An example of this task is shown in Figure 1.Recent work, such as DALL•E (Ramesh et al., 2021(Ramesh et al., , 2022)), which generates images from text, has shown the ability to seamlessly integrate multiple properties, such as chairs and avocados, in an image.This points towards similar applications in the molecule generation domain via the usage of natural language.</p>
<p>3 Evaluation Metrics</p>
<p>Text2Mol Metric</p>
<p>Since we are considering new cross-modal tasks between molecules and text, we also introduce a new cross-modal evaluation metric.This is based on Text2Mol (Edwards et al., 2021), which aims to train a retrieval model to rank molecules given their text descriptions.Since the ranking function uses cosine similarity between embeddings, a trained model can be repurposed for evaluating the similarity between the ground truth molecule/description and the generated description/molecule (respectively).To this end, we first train a base multi-layer perceptron (MLP) model from Text2Mol.This model is then used to generate similarities of the candidate molecule-description pairs, which can be compared to the average similarity of the ground truth molecule-description pairs.We also note that negative molecule-description pairs have an average similarity of roughly zero.</p>
<p>Evaluating Molecule Captioning</p>
<p>Traditionally, captioning tasks have been evaluated by natural language generation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005).Un-like captioning tasks such as COCO (Chen et al., 2015), which has several captions per image, in our task we only have one reference caption.This makes these metrics less effective, especially because there are many non-overlapping ways to describe a molecule.Nevertheless, for comparison, we still report these scores (e.g., aggregated sentence-level METEOR scores).</p>
<p>Evaluating Text-Based de Novo Molecule Generation</p>
<p>Considerable interest has grown in applying deep generative models to de novo molecule generation.</p>
<p>Because of this, a number of metrics have been proposed, such as novelty and scaffold similarity (Polykovskiy et al., 2020).However, many of these metrics do not apply to our problem-we want our generated molecule to match the input text instead of being generally diverse.Instead, we consider metrics which measure the distance of the generated molecule to either the ground truth molecule or the ground truth description, such as our proposed Text2Mol-based metric.</p>
<p>We employ three fingerprint metrics: MACCS FTS, RDK FTS, and Morgan FTS, where FTS stands for fingerprint Tanimoto similarity (Tanimoto, 1958).MACCS (Durant et al., 2002), RDK (Schneider et al., 2015), and Morgan (Rogers and Hahn, 2010) are each fingerprinting methods for molecules.The fingerprints of two molecules are compared using Tanimoto similarity (also known as Jaccard index), and the average similarity over the evaluation dataset is reported.See (Campos and Ji, 2021) for more details.We also report exact SMILES string matches, Levenshtein distance (Miller et al., 2009), and SMILES BLEU scores.Preuer et al. (2018) propose Fréchet ChemNet Distance (FCD), which is inspired by the Fréchet Inception Distance (FID) (Heusel et al., 2017).FCD is based on the penultimate layer of a network called "ChemNet", which was trained to predict the activity of drug molecules.Thus, FCD takes into account chemical and biological information about molecules in order to compare them.This allows molecules to be compared based on the latent information required to predict useful properties rather than a string-based metric.</p>
<p>In the case of models which use SMILES strings, generated molecules can be syntactically invalid.Therefore, we also report validity as the percent of molecules which can be processed by RDKIT Figure 3: A diagram of our framework.We first pre-train MolT5 on a large amount of data of both SMILES string and natural language using the "replace corrupted spans" objective (Raffel et al., 2020).After the pre-training stage, MolT5 can be easily fine-tuned for either the task of molecule captioning or generation (or both).(Landrum, 2021) as in (Polykovskiy et al., 2020).</p>
<p>MolT5 -Multimodal Text-Molecule Representation Model</p>
<p>We can crawl a massive amount of text from the Internet.For example, Raffel et al. (2020) built a Common Crawl-based dataset that contains over 700 GB of reasonably clean and natural English text.On the other hand, over a billion molecules are also available from public databases such as ZINC-15 (Sterling and Irwin, 2015a).Inspired by the progress in large-scale pretraining (Ramesh et al., 2021), we propose a new self-supervised learning framework named MolT5 (Molecular T5) to leverage the vast amount of unlabeled natural language text and molecule strings.</p>
<p>Figure 3 shows an overview of MolT5.We first initialize an encoder-decoder Transformer model (Vaswani et al., 2017) using one of the public checkpoints of T5.1.1 2 , an improved version of T5 (Raf-2 https://tinyurl.com/t511-ckptsfel et al., 2020).After that, we pretrain the model using the "replace corrupted spans" objective (Raffel et al., 2020).More specifically, during each pretraining step, we sample a minibatch comprising both natural language sequences and SMILES sequences.For each sequence, some words in the sequence are randomly chosen for corruption.Each consecutive span of corrupted tokens is replaced by a sentinel token (shown as [X] and [Y] in Figure 3).Then the task is to predict the dropped-out spans. 3olecules (e.g.represented as SMILES strings) can be thought of as a language with a very unique grammar.Then, intuitively, our pretraining stage essentially trains a single language model on two monolingual corpora from two different languages, and there is no explicit alignment between the two corpora.This approach is similar to how some multilingual language models such as mBERT (Devlin et al., 2019) and mBART (Liu et al., 2020) were pretrained.As models such as mBERT demonstrate ex-cellent cross-lingual capabilities (Pires et al., 2019), we also expect models pretrained using MolT5 to be useful for text-molecule translation tasks.</p>
<p>After the pretraining process, we can finetune the pretrained model for either molecule captioning or generation (depicted by the bottom half of Figure 3).In molecule generation, the input is a description, and the output is the SMILES representation of the target molecule.On the other hand, in molecule captioning, the input is the SMILES string of some molecule, and the output is a caption describing the input molecule.</p>
<p>Experiments and Results</p>
<p>Data</p>
<p>Pretraining Data As described in Section 4, the pretraining stage of MolT5 requires two monolingual corpora: one consisting of natural language text and the other consisting of molecule representations.We use the "Colossal Clean Crawled Corpus" (C4) (Raffel et al., 2020) as the pretraining dataset for the textual modality.For the molecular modality, we directly utilize the 100 million SMILES strings used in Chemformer (Irwin et al., 2021).As these strings were selected from the ZINC-15 dataset (Sterling and Irwin, 2015b), we refer to this pretraining dataset as ZINC from this point.</p>
<p>Finetuning and Evaluation Data</p>
<p>We use ChEBI-20 (Edwards et al., 2021) as our gold standard dataset for finetuning and evaluation.It consists of 33,010 molecule-description pairs, which are separated into 80/10/10% train/validation/test splits.We use ChEBI-20 to finetune MolT5-based models and to train baseline models.Many captions in ChEBI-20 contain a name for the molecule at the start of the string (e.g., "Rostratin D is an organic disulfide isolated from ...").To force the models to focus on the semantics of the description, we replace the molecule's name with "The molecule is [...]" (e.g., "The molecule is an organic disulfide isolated from ...").</p>
<p>Baselines</p>
<p>Any sequence-to-sequence model is applicable to our new tasks (i.e., molecule captioning and generation).We implement the following baselines:</p>
<ol>
<li>
<p>RNN-GRU (Cho et al., 2014).We implement a 4-layer GRU recurrent neural network.The encoder is bidirectional.</p>
</li>
<li>
<p>Transformer (Vaswani et al., 2017).We train a vanilla Transformer model consisting of six encoder and decoder layers.</p>
</li>
<li>
<p>T5 (Raffel et al., 2020).We experiment with three public T5.1.1 checkpoints4 : small, base, and large.We finetune each checkpoint for molecule captioning or molecule generation using the t5x framework (Roberts et al., 2022).</p>
</li>
</ol>
<p>We train the baseline models on ChEBI-20 using SMILES representations for the molecules.Molecule captioning and generation are trained with molecules as input/output and text as output/input.More information about the baselines and the hyperparameters is in the appendix.</p>
<p>Pretraining Process</p>
<p>We first initialize an encoder-decoder Transformer model using a public checkpoint of T5.1.1 (either t5.1.1.small, t5.1.1.base, or t5.1.1.large).We then pretrain the model on the combined dataset of C4 and ZINC (i.e., C4+ZINC) for 1 million steps.Each step uses a batch size of 256 evenly split between text and molecule sequences.After this, we finetune the pretrained model on ChEBI-20 for either molecule captioning or generation.The number of finetuning steps is 50,000.</p>
<p>Molecule Captioning</p>
<p>Table 1 shows the overall molecule captioning results.The pretrained models, either T5 or MolT5, are considerably better at generating realistic language to describe a molecule than the RNN and Transformer baselines.The RNN is more capable of extracting relevant properties from molecules than the Transformer, but it generally produces ungrammatical outputs.On the other hand, the Transformer produces grammatical outputs, but they tend to repeat the same properties, such as carcinogenic, regardless of whether they apply.For this reason, the Text2Mol scores are much lower for the Transformer model, since its outputs match the given molecule much less frequently.We speculate that the ChEBI-20 dataset is too small to effectively train a Transformer without large-scale pretraining.We find that our additional pretraining of MolT5 results in a reasonable increase over T5 in captioning performance on both the traditional NLG metrics and our Text2Mol metric for each model size.Finally, we refer the reader to Section H in
d -glucoside ------- ----------------- -a -------------- ----------------- ----------------- -------------[…]
the molecule is the stable isotope of helium with relative atomic mass 3. 016029.the least abundant ( 0. 000137 atom percent ) isotope of naturally occurring helium.</p>
<p>The molecule is a GDP-Dglucose in which the anomeric centre of the pyranose fragment has alphaconfiguration.It is a GDP-Dglucose and a ribonucleoside 5'-diphosphate-alpha-Dglucose.It is a conjugate acid of a GDP-alpha-D-glucose(2-).</p>
<p>1 the appendix for information about the statistical significance of our results.</p>
<p>Several examples of different models' outputs are shown in Figure 4 and Appendix Figure 9.In (1), MolT5's description matches best, identifying the molecule as a "GDP-L-galactose".MolT5 is usually able to recognize what general class of molecule it is looking at (e.g.cyclohexanone, maleate salt, etc.).In general, all models often look for the closest compound they know and base their caption on that.The argon atom, example (2) with SMILES '[39Ar]', is not present in the training dataset bonded to any other atoms (likely because it is an inert noble gas).All models recognize that (2) is a single atom, but they are unable to describe it.In (3), the models try to caption a histological dye.MolT5 captions the molecule as an azure histological dye, which is very close to the ground truth "brilliant cresyl blue", while T5 does not.</p>
<p>Text-Based de novo Molecule Generation</p>
<p>In the molecule generation task, the pretrained models also perform much better than the RNN and Transformer (Table 2).Although it is well known that scaling model size and pretraining data leads to significant performance increases (Kaplan et al., 2020), it was still surprising to see the results.For example, a default T5 model, which was only pretrained on text data, is capable of generating molecules which are much closer to the ground truth than the RNN and which are often valid.This trend also persists as language model size scales, since T5-large with 770M parameters outperforms the specifically pretrained MolT5-small with 60M parameters.Still, the pretraining in MolT5 slightly improves some molecule generation results, with especially large gains in validity.Finally, Section H in the appendix has information about the statistical significance of our results.</p>
<p>We show results for the models in Figure 5 and  4,6,7,16,18,21).In many cases, MolT5 obtains exact matches with the ground truth (2,3,4,6,7,8,10,12,17,20,21).( 3) is an interesting case, since it shows that MolT5 can understand crystalline solids like hydrates.(2) is another interesting example; it is the longest SMILES string, at 474 characters, which MolT5 is able to generate an exact match for.MolT5 understands peptides and can produce them from descriptions (2,15,17).It also shows this ability for saccharides (6, 21) and enzymes (8,20).</p>
<p>MolT5 is able to understand rare atoms such as Ruthenium (5).However, in this case it still misses the atom's charge.Some example descriptions, such as (1), lack details so the molecules generated by MolT5 may be interesting to investigate.</p>
<p>Probing the Model</p>
<p>We conduct probing tests on the model for certain input properties, which are shown in Appendix J.</p>
<p>Often, the model will generate molecules that it knows matches the input description from the finetuning data.It also creates solutions from these as well by adding various ions (e.g.".</p>
<p>[Na+]").In some cases, it generates molecules not appearing in finetuning data (sometimes successfully sometimes not).For example, given the input "The molecule is a corticosteroid.",the first molecule generated is a well known corticosteroid called corticosterone.</p>
<p>The fifth molecule generated is not present in the PubChem database.Based on a structure similarity search, it is most closely related to the androgenic steroid Fluoxymesterone and the corticosteroid Hydrocortisone.</p>
<p>6 Related Work</p>
<p>Multimedia Representation</p>
<p>Much recent work on multimedia representations falls into training large vision-language models (Su et al., 2020;Lu et al., 2019;Chen et al., 2020).CLIP (Radford et al., 2021) trains a zero-shot image classifier by using natural language labels which can be easily extended.A modification of CLIP's contrastive loss function, which follows (Sohn, 2016), is applied by Text2Mol (Edwards et al., 2021)  Their natural language generation is constrained to the specific reaction steps in their dataset-the main purpose of their model is to create the steps for a reaction rather than describing molecules.</p>
<p>Image Captioning and Text-Guided Image Generation</p>
<p>Image captioning has been studied extensively (Pan et al., 2004;Lu et al., 2018;Hossain et al., 2019;Stefanini et al., 2021).Many recent studies tend to pretrain Transformer-based models on massive text-image corpora (Li et al., 2020;Hu et al., 2022).Work has also been done in the biomedical domain (Pavlopoulos et al., 2019), a close cousin of the chemistry domain, where tasks tend to be focused on diagnosis of various image types such as x-rays (Demner-Fushman et al., 2016).</p>
<p>The reverse problem, text-guided image generation, has proven considerably more challenging (Khan et al., 2021).Several attempts have used GAN-based methods (Reed et al., 2016;Zhang et al., 2017;Xu et al., 2018).Recent work has shown remarkable results.DALL• E (Ramesh et al., 2021(Ramesh et al., , 2022) ) can seamlessly fuse multiple concepts together to generate a realistic image.</p>
<p>Molecule Representation</p>
<p>Molecule representation has been a long-standing problem in the field of cheminformatics.Traditionally, fingerprinting methods have been a preferred technique to featurize molecule structural representations (Rogers and Hahn, 2010;Cereto-Massagué et al., 2015).These approaches do not allow representations to be learned from data.In recent years, advances in machine learning and NLP have been applied to this problem.A popular input for these algorithms has been SMILES strings (Weininger, 1988;Weininger et al., 1989), which are a computer-readable linearization of molecule graphs.Jaeger et al. (2018) use the Morgan fingerprinting algorithm to convert each molecule into a 'sentence' of its substructures, to which it applies the Word2vec algorithm (Mikolov et al., 2013a,b).Duvenaud et al. (2015) use neural methods to learn fingerprints.Other advances such as BERT (Devlin et al., 2019) have also been applied to the domain, such as MolBERT (Fabian et al., 2020) and ChemBERTa (Chithrananda et al., 2020), which use SMILES strings as inputs to pretrain a BERT-esque model.Work has been done to use the molecule graph structure and known reactions for learning representations (Wang et al., 2022).Schwaller et al. (2021b)  There has been particular interest in training generative models for de novo molecule discovery.Bagal et al. ( 2021) apply a GPT-style decoder for this task.Lu and Zhang (2022) apply a T5 model to SMILES strings for multitask reaction prediction problems.MegaMolBART5 trains a BART model on 500M SMILES strings from the ZINC-15 dataset (Sterling and Irwin, 2015b) 7 Conclusions and Future Work</p>
<p>In this work, we propose MolT5, a self-supervised learning framework for pretraining models on a vast amount of unlabeled text and molecule strings.Furthermore, we propose two new tasks: molecule captioning and text-guided molecule generation, for which we explore various evaluation methods.Together, these tasks allow for translation between natural language and molecules.Using MolT5, we are able to obtain high scores for both tasks.</p>
<p>Broader Impacts</p>
<p>Our proposed model and tasks will have the following broader impacts.1) It will help to democratize molecular AI, allowing chemistry experts to take advantage of new AI technologies for discovering new life-changing drugs by interacting in the natural language, because it is most natural for humans to provide explanations and requirements in natural language.2) Text-based molecule generation enables the ability to generate molecules with specific functions (such as taste) rather than properties, enabling the next generation of chemistry where custom molecules are used for each application.Specifically-designed molecular solutions have the potential to revolutionize fields such as medicine and material science.3) Our models, whose weights we will release, will allow further research in the NLP community on the applications of multimodal text-molecule models.</p>
<p>Risks</p>
<p>MolT5, like other large language models, can potentially be abused.First, there may be biases learned by the model due to its large-scale training data.These biases may affect what type of molecules are generated when the model is prompted about certain diseases.Thus, any molecules discovered by usage of MoLT5 should strictly evaluated by standard clinical processes before being considered for medicinal use.Another risk is that the model may be used to discover potentially dangerous molecules instead of beneficial ones.It is difficult to predict what exact molecules may be discovered via usage of our work.However, while there is this unfortunate potential for misuse of the technology, knowledge of dangerous molecule's existence and structure is generally not harmful due to the requisite technical knowledge and laboratory resources required to synthesize them in any meaningful quantity.Over-all, we believe these downsides are outweighed by the benefits to the research and pharmaceutical communities.</p>
<p>Limitations</p>
<p>Since this work focuses on a new application for large language models, many of the same limitations apply here.Namely, the model is trained on a large dataset collected from the Internet, so it may contain unintended biases.One limitation of our model is using SMILES strings -recent work (Krenn et al., 2020) proposes a string representation with validity guarantees.In practice, we found this to work poorly with pretrained T5 checkpoints (which were important from a computational perspective).We also note that some compounds in ChEBI-20 can cause validity problems in the default SELFIES implementation.We leave further investigation of this to future work.Finally, we stress that MolT5 was created for research purposes and generated molecules should not be used for medical purposes without careful evaluation by standard clinical testing first.</p>
<p>A Baselines and Hyperparameters</p>
<p>Any sequence-to-sequence model is applicable to our new tasks (i.e., molecule captioning and generation).We implement the following baselines:</p>
<ol>
<li>
<p>RNN-GRU (Cho et al., 2014).We implement a 4-layer GRU recurrent neural network with a hidden size of 512.We use a learning rate of 1e-4 and a batch size of 128 for molecule generation.For caption generation, a batch size of 116 is used.The number of training epochs is 50.Additionally, the encoder is bidirectional.For training, teacher forcing is used 50% of the time, and gradient clipping to 50 is applied.</p>
</li>
<li>
<p>Transformer (Vaswani et al., 2017).We train a vanilla Transformer model consisting of six encoder and decoder layers.The number of training epochs is 40, the batch size is 16, and the learning rate is 1e-4.We use a linear decay with a warmup of 400 steps.</p>
</li>
<li>
<p>T5 (Raffel et al., 2020).We experiment with three public T5.1.1 checkpoints6 : small, base, and large.We finetune each checkpoint for molecule captioning or molecule generation using the open-sourced t5x framework (Roberts et al., 2022).The number of training steps is set to be 50,000.The dropout rate is set to be 0.0 for the small and base models, and it is set to be 0.1 for the large model.</p>
</li>
</ol>
<p>For other hyperparameters, we use the default values provided by the t5x framework.</p>
<p>We train the baseline models on the ChEBI-20 dataset using SMILES representations for the molecules.Molecule captioning and generation are trained with molecules as input/output and text as output/input.Sequences are limited to 512 tokens for input and output.During inference, a beam decoder with a beam size of 5 is used.</p>
<p>On the RNN and vanilla Transformer models, we use a character-split vocabulary for SMILES.</p>
<p>For the text vocabulary, we use SciBERT's 31,090token vocabulary (Beltagy et al., 2019).</p>
<p>B Reproducibility Checklist</p>
<p>The programs, trained models, and resources will be made publicly available.For training the RNN and Transformer baselines, we use NVIDIA Tesla V100 GPUs.For pretraining and finetuning T5related models, we use TPUs.</p>
<p>When testing on a MacBook Pro that has no access to GPUs, the average inference time of our MolT5-Base molecule generation model is 2.24 seconds/query.The average inference time of our large MolT5-Base molecule captioning model is 9.86 seconds/query.</p>
<p>C Decoding with Huggingface Model</p>
<p>For ease of adoption, we converted our original models trained using the t5x framework (Roberts et al., 2022) to HuggingFace-based models (Wolf et al., 2019).We will release the converted models on HuggingFace (HF) Hub.Due to implementation differences, the HF-based models produce slightly different outputs from the original models.Therefore, we also report the numbers of the HF-based models in Table 3 and Table 4.</p>
<p>D High Validity Molecule Generation</p>
<p>To increase the validity score of the molecule generation models, we consider a high-validity decoding strategy.We use diverse beam search (Vijayakumar et al., 2016) with a beam width and beam group of 30 and a diversity penalty of 0.5.Then, we use RD-Kit (Landrum, 2021) to select the first valid beam.On rare occasions, the beam size exceeds memory limitations, so we iteratively reduce the beam size by 5 for that input and try again.In Table 4, MolT5-Small-HV, MolT5-Base-HV, and MolT5-Large-HV denote models that use this decoding process.</p>
<p>E Ablations</p>
<p>We perform ablations on MolT5-Small pretraining.For molecule captioning (Table 5), pretraining on both C4 and ZINC is clearly more beneficial than pretraining only on C4 or only on ZINC.</p>
<p>For molecule generation, at first glance, pretraining on C4+ZINC seems not to outperform pretraining only on C4 (Table 6).However, note that except for BLEU, Exact, Levenshtein, and Validity, other metrics in Table 6 are computed using only syntactically valid molecules.Table 7 shows the normalized molecule generation results.After normalization, we see that pretraining on C4+ZINC outperforms pretraining only on C4 or only on ZINC according to most metrics.Finally, pretraining only on ZINC increases the validity score substantially.However, this leads to decreased similarity of the generated molecules to the ground truths.</p>
<p>F More Examples
Transformer RNN T5 MolT5 Input Ground Truth
The molecule is a member of the class of phhenylureas that is urea in which one of the nitrogens is substituted by a pchlorophenyl group while the other is substituted by two methyl groups.It has a role as a herbicide, a xenobiotic and an environmental contaminant.It is a member of monochlorobenzenes and a member of phenylureas.</p>
<p>The molecule is a perchlorometallate anion having six chlorines and ruthenium(IV) as the metal component.It is a perchlorometallate anion and a ruthenium coordination entity.</p>
<p>The molecule is a trisaccharide derivative that consists of 6-sulfated D-glucose having an alpha-L-fucosyl residue attached at position 3 and a beta-Dgalactosyl residue attached at position 4. It has a role as an epitope.It is a trisaccharide derivative and an oligosaccharide sulfate.</p>
<p>Invalid</p>
<p>The molecule is a monocarboxylic acid that is thyroacetic acid carrying four iodo substituents at positions 3, 3', 5 and 5'.It has a role as a thyroid hormone, a human metabolite and an apoptosis inducer.It is an iodophenol, a 2-halophenol, a monocarboxylic acid and an aromatic ether.</p>
<p>The molecule is a methylbutanoyl-CoA is the S-isovaleryl derivative of coenzyme A. It has a role as a mouse metabolite.It derives from an isovaleric acid and a butyryl-CoA.It is a conjugate acid of an isovaleryl-CoA(4-).</p>
<p>The molecule is an D-arabinose 5phosphate that is beta-Darabinofuranose attached to a phospahte group at position 5.It derives from a beta-Darabinofuranose.</p>
<p>The molecule is a guaiacyl lignin obtained by cyclodimerisation of coniferol.It has a role as a plant metabolite and an anti-inflammatory agent.It is a member of 1-benzofurans, a primary alcohol, a guaiacyl lignin and a member of guaiacols.It derives from a coniferol.</p>
<p>Invalid 8 9</p>
<p>Invalid Invalid The molecule is a synthetic piperidine derivative, effective against diarrhoea resulting from gastroenteritis or inflammatory bowel disease.It has a role as a mu-opioid receptor agonist, an antidiarrhoeal drug and an anticoronaviral agent.It is a member of piperidines, a monocarboxylic acid amide, a member of monochlorobenzenes and a tertiary alcohol.It is a conjugate base of a loperamide(1+).</p>
<p>The molecule is a steroid sulfate that is the 3-sulfate of androsterone.It has a role as a human metabolite and a mouse metabolite.It is a 17-oxo steroid, a steroid sulfate and an androstanoid.It derives from an androsterone.It is a conjugate acid of an androsterone sulfate(1-).It derives from a hydride of a 5alpha-androstane.</p>
<p>12</p>
<p>The molecule is a member of the class of chloroethanes that is ethane in which five of the six hydrogens are replaced by chlorines.A non-flammable, high-boiling liquid (b.p. 161-162℃) with relative density 1.67 and an odour resembling that of chloroform, it is used as a solvent for oil and grease, in metal cleaning, and in the separation of coal from impurities.It has a role as a non-polar solvent.</p>
<p>Invalid, fixed</p>
<p>The molecule is an ultra-long-chain primary fatty alcohol that is tetratriacontane in which one of the terminal methyl hydrogens is replaced by a hydroxy group It has a role as a plant metabolite.</p>
<p>13</p>
<p>Invalid Invalid</p>
<p>The molecule is an acyl-CoA that results from the formal condensation of the thiol group of coenzyme A with the carboxy group of (E)-2-benzylidenesuccinic acid.</p>
<p>It is a conjugate acid of an (E)-2benzylidenesuccinyl-CoA(5-).</p>
<p>Invalid Invalid</p>
<p>The molecule is a branched amino octasaccharide derivative that is beta-D-Man-(1-&gt;4)-beta-D-GlcNAc-(1-&gt;4)-beta-D-GlcNAc in which the mannosyl group is substituted at positions 3 and 6 by beta-D-GlcNAc-(1-&gt;2)-alpha-D-Man groups and the reducing-end N-acetyl-beta-Dglucosamine residue is substituted at position 6 by an alpha-L-fucosyl group.It has a role as an epitope.It is an amino octasaccharide and a glucosamine oligosaccharide.</p>
<p>Invalid</p>
<p>Transformer</p>
<p>RNN T5 MolT5 Input Ground Truth</p>
<p>The molecule is a benzazepine and a tetracyclic antidepressant.It has a role as an alpha-adrenergic antagonist, a serotonergic antagonist, a histamine antagonist, an anxiolytic drug, a H1receptor antagonist and a oneirogen.</p>
<p>22</p>
<p>Invalid</p>
<p>The molecule is a tetrazine that is 1,2,4,5tetrazine in which both of the hydrogens have been replaced by o-chlorophenyl groups.It has a role as a mite growth regulator and a tetrazine acaricide.It is an organochlorine acaricide, a member of monochlorobenzenes and a tetrazine.It derives from a hydride of a 1,2,4,5tetrazine.</p>
<p>23</p>
<p>The molecule is a derivative of phosphorous acid in which one of the acidic hydroxy groups has been replaced by amino.</p>
<p>24</p>
<p>Figure</p>
<p>Figure 1 :
1
Figure 1: An example output from our model for the molecule generation task.The left is the ground truth, and the right is a molecule generated from the given natural language caption.</p>
<p>Figure 2: An example of both the image captioning task (Chen et al., 2015) and molecule captioning.Molecule captioning is considerably more difficult because of the increased linguistic variety in possible captions.</p>
<p>Figure 4 :
4
Figure 4: Example captions generated by different models.</p>
<p>trains a BERT model to learn representations of chemical reactions.Schwaller et al. (2021a) leverages unsupervised representation learning with Transformers to extract an organic chemistry grammar.Unlike existing work, MolT5's molecule representations allow for translation between molecules and natural language.</p>
<p>Figure 6 :
6
Figure 6: More examples of interesting molecules generated by different models.</p>
<p>linkage.It has a role as a metabolite.It derives from a Lleucine and a L-aspartic acid.The molecule is a tripeptide composed of L-leucine, Lvaline and L-aspartic acid joined in sequence by peptide linkages.It has a role as a metabolite.It derives from a L-leucine, a L-valine and a L-aspartic acid.The molecule is a tripeptide composed of L-leucine, Lvaline and L-aspartic acid joined in sequence by peptide linkages.It has a role as a metabolite.It derives from a L-leucine, a L-valine and a L-aspartic acid.</p>
<p>Figure 9 :
9
Figure 9: More examples of interesting captions generated by different models.</p>
<p>Figure 10 :
10
Figure 10: Input: The molecule displays antimalarial properties.</p>
<p>Figure 11 :
11
Figure 11: Input: The molecule is a apoptosis inducer.</p>
<p>Figure 12 :
12
Figure 12: Input: The molecule is a blue dye.</p>
<p>Figure 13 :
13
Figure 13: Input: The molecule is a coagulent.</p>
<p>Figure 14 :
14
Figure 14: Input: The molecule is a corticosteroid.</p>
<p>Figure 15 :Figure 16 :
1516
Figure 15: Input: The molecule is a fluorochrome.</p>
<p>Figure 17 :
17
Figure 17: Input: The molecule is a green dye.</p>
<p>Figure 18 :
18
Figure 18: Input: The molecule is a histological dye.</p>
<p>Figure 19 :
19
Figure 19: Input: The molecule is a human metabolite.</p>
<p>Figure 20 :
20
Figure 20: Input: The molecule is a hydrocarbon which tastes really cool.</p>
<p>Figure 21 :
21
Figure 21: Input: The molecule is a liquid at room temperature.</p>
<p>Figure 22 :
22
Figure 22: Input: The molecule is a macrocycle.</p>
<p>Figure 23 :
23
Figure 23: Input: The molecule is a maleate salt.</p>
<p>Figure 24 :Figure 25 :
2425
Figure 24: Input: The molecule is a neurotransmitter agent.</p>
<p>Figure 26 :
26
Figure 26: Input: The molecule is a photovoltaic.</p>
<p>Figure 27 :Figure 28 :
2728
Figure 27: Input: The molecule is a pigment which converts sunlight into energy.</p>
<p>Figure 29 :
29
Figure 29: Input: The molecule is a purple dye.</p>
<p>Figure 30 :Figure 31 :
3031
Figure 30: Input: The molecule is a red dye.</p>
<p>Figure 32 :
32
Figure 32: Input: The molecule is a sulfonated xanthene.</p>
<p>Figure 33 :Figure 34 :
3334
Figure 33: Input: The molecule is a sweet tasting sugar additive.</p>
<p>Figure 35 :
35
Figure 35: Input: The molecule is able to lower blood pressure.</p>
<p>Figure 36 :Figure 37 :
3637
Figure 36: Input: The molecule is an adrenergic uptake inhibitor.</p>
<p>Figure 38 :
38
Figure 38: Input: The molecule is an anabolic agent.</p>
<p>Figure 39 :Figure 40 :
3940
Figure 39: Input: The molecule is an analgesic.</p>
<p>Figure 41 :
41
Figure 41: Input: The molecule is an antibiotic.</p>
<p>Figure 42 :Figure 43 :
4243
Figure 42: Input: The molecule is an antidepressant.</p>
<p>Figure 44 :
44
Figure 44: Input: The molecule is an antineoplastic agent.</p>
<p>Figure 45 :Figure 46 :
4546
Figure 45: Input: The molecule is an antiplasmodial drug.</p>
<p>Figure 47 :
47
Figure 47: Input: The molecule is an antitubercular agent.</p>
<p>Figure 48 :Figure 49 :
4849
Figure 48: Input: The molecule is an anti-ulcer drug.</p>
<p>Figure 50 :
50
Figure 50: Input: The molecule is a catabolic agent.</p>
<p>Figure 51 :Figure 52 :
5152
Figure 51: Input: The molecule is an explosive.</p>
<p>Figure 53 :
53
Figure 53: Input: The molecule is an insect attractant.</p>
<p>Figure 54 :
54
Figure 54: Input: The molecule is an insecticide.</p>
<p>Figure 55 :
55
Figure 55: Input: The molecule is an organofluorine compound.</p>
<p>Figure 56 :Figure 57 :
5657
Figure 56: Input: The molecule is blue.</p>
<p>Figure 58 :
58
Figure 58: Input: The molecule is blue blue blue.</p>
<p>Figure 59 :
59
Figure 59: Input: The molecule is blue blue blue blue.</p>
<p>Figure 61 :
61
Figure 61: Input: The molecule is electrically conductive.</p>
<p>Table 1 :
1
Molecule captioning results on the test split of CheBI-20.Rouge scores are F1 values.
ModelBLEU-2BLEU-4ROUGE-1ROUGE-2ROUGE-LMETEORText2MolGround Truth0.609RNN Transformer0.251 0.0610.176 0.0270.450 0.2040.278 0.0870.394 0.1860.363 0.1140.426 0.057T5-Small MolT5-Small0.501 0.5190.415 0.4360.602 0.6200.446 0.4690.545 0.5630.532 0.5510.526 0.540T5-Base MolT5-Base0.511 0.5400.423 0.4570.607 0.6340.451 0.4850.550 0.5780.539 0.5690.523 0.547T5-Large MolT5-Large0.558 0.5940.467 0.5080.630 0.6540.478 0.5100.569 0.5940.586 0.6140.563 0.582InputRNNTransformerT5MolT5Ground Truththe molecule is a gdp -The molecule is a GDP-L-The molecule is agalactose in which theGDP-L-galactoseanomeric oxygen is on thehaving beta-same side of the fucose ring asconfiguration at thethe methyl substituent. It has aanomeric centre of therole as a plant metabolite and aL-galactose fragment.mouse metabolite. It is aIt is a conjugate acid ofconjugate acid of a GDP-beta-a GDP-beta-L-L-galactose(2-).galactose(2-).2the molecule is stable metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic metallic […]the molecule is the stable isotope of thallium with relative atomic mass 202. 9723. the least abundant ( 29. 524 atom percent ) isotope of naturally occurring thallium.The molecule is the radioactive isotope of chromium with relative atomic mass 39.98286 and half-life of 138.376 days; the only naturally occurring isotope of chromium.The molecule is the stable isotope of spin 7/2. abundance and nuclear 100 atom percent natural atomic mass 44.955910, rubidium with relativetracer. a role as an isotopic life of 269 years. It has 38.964313 and a half-with atomic mass of The molecule is a trace radioisotope of argon3the molecule is a cationic fluorescent dye having 2,the molecule is a deuterated compound that is is is is isThe molecule is a quaternaryThe molecule is an organic cation that isThe molecule is an organic cation that is phenoxazin-5-3 -dimethyl -1, 2, 3, 4, 6an isotopologue ofammonium ion andphenoxazin-5-iumium substituted by methyl,-tetrahydro -1h -1, 2, 3,chloroform in which thea member ofsubstituted by amino andamino and diethylamino4, 6 -tetrahydropyridin -four hydrogen atoms havephenanthridines. Itmethylamino groups atgroups at positions 2, 3 and 71 -yl ] amino } aminobeen replaced byhas a role as anpositions 3 and 7respectively. Thegroup, respectively. it hasdeuterium. it is a deuteratedintercalator and arespectively. The chloridetetrachlorozincate salt salt isa role as a fluorochrome.compound and an alpha,fluorochrome.salt is the histological dyethe histological dye 'brilliantomega -dicarboxylic acid.'azure C'.cresyl blue'.</p>
<p>Table 2 :
2
(Campos and Ji, 2021)esults on the test split of CheBI-20.Except for BLEU, Exact, Levenshtein, and Validity, other metrics are computed using only syntactically valid molecules, as in(Campos and Ji, 2021).
ModelBLEU↑ Exact↑ Levenshtein↓ MACCS FTS↑ RDK FTS↑ Morgan FTS↑ FCD↓ Text2Mol↑ Validity↑Ground Truth1.0001.0000.01.0001.0001.0000.00.6091.0RNN0.6520.00538.090.5910.4000.3624.550.4090.542Transformer0.4990.00057.660.4800.3200.21711.320.2770.906T5-Small0.7410.06427.7030.7040.5780.5252.890.4790.608MolT5-Small0.7550.07925.9880.7030.5680.5172.490.4820.721T5-Base0.7620.06924.9500.7310.6050.5452.480.4990.660MolT5-Base0.7690.08124.4580.7210.5880.5292.180.4960.772T5-Large0.8540.27916.7210.8230.7310.6701.220.5520.902MolT5-Large0.8540.31116.0710.8340.7460.6841.200.5540.905InputRNNTransformerT5MolT5Ground Truth1The molecule is a sulfonated xanthene dye of absorption wavelength 573 nmInvalidand emission wavelength 591 nm. It hasa role as a fluorochrome.2The molecule is a linear 27-membered polypeptide comprising the sequenceLys-Gly-Lys-Gly-Lys-Gly-Lys-Gly-Lys-Gly-Glu-Asn-Pro-Val-Val-His-Phe-Phe-Tyr-Asn-Ile-Val-Thr-Pro-Arg-Thr-Pro.Corresponds to the sequence of the myelin basic protein 83-99 (MBP83-99)InvalidInvalidimmunodominant epitope with the lysylresidue at position 91 replaced by tyrosyl[MBP83-99(Y(91))] and with an (L-lysylglycyl)5 [(KG5)] linker attached tothe glutamine(83) (E(83)) residue.3The molecule is a hydrate that is the dihydrate form of manganese(II) chloride.It has a role as a MRI contrast agent and anutraceutical. It is a hydrate, an inorganicchloride and a manganese coordinationentity.Figure 5: Examples of molecules generated by different models.also in Figures 6, 7, and 8 in Appendix F, whichwe number by input description. Compared to T5,MolT5 is better able to understand instructions formanipulating molecules, as shown in examples (3,</p>
<p>Zeng et al. (2022)21)eval between molecule and text pairs.Edwards et al. (2021)also released the ChEBI-20 dataset of moleculedescription pairs, which is used for training and evaluation in this paper.Vall et al. (2021)leverage a contrastive loss between bioassay descriptions and molecules to predict activity between the two.Sun et al. (2021)uses cross-modal attention with molecule structures to improve chemical entity typing.Zeng et al. (2022)pretrain a language model to learn a joint representation between molecules and biomedical text via entity linking which they use for tasks such as relation extraction, molecule property prediction, and crossmodal retrieval like Text2Mol.Unlike our work, they do not explore generating text nor molecules.
Vaucher et al. (2020) create a dataset of chemicalequations and associated action sequences in natu-ral language. Vaucher et al. (2021) then leveragethis dataset to train a BART model which can planchemical reaction steps.</p>
<p>Table 3 :
3
HuggingFace model molecule captioning results for the different baseline models on the test split of CheBI-20.Rouge scores are F1 values.
ModelBLEU-2BLEU-4ROUGE-1ROUGE-2ROUGE-LMETEORText2MolGround Truth0.609RNN Transformer0.251 0.0610.176 0.0270.450 0.2040.278 0.0870.394 0.1860.363 0.1140.426 0.057T5-Small MolT5-Small0.515 0.5320.424 0.4450.613 0.6270.459 0.4770.568 0.5830.538 0.5570.527 0.543T5-Base MolT5-Base0.522 0.5510.432 0.4640.616 0.6370.461 0.4890.572 0.5940.545 0.5740.524 0.549T5-Large MolT5-Large0.555 0.5880.464 0.5020.632 0.6500.482 0.5070.585 0.6040.588 0.6140.564 0.582ModelBLEU↑ Exact↑ Levenshtein↓ MACCS FTS↑ RDK FTS↑ Morgan FTS↑ FCD↓ Text2Mol↑ Validity↑Ground Truth1.0001.0000.01.0001.0001.0000.00.6091.0RNN0.6520.00538.090.5910.4000.3624.550.4090.542Transformer0.4990.00057.660.4800.3200.21711.320.2770.906T5-Small0.7400.06130.050.7980.6810.6231.770.5410.597MolT5-Small0.7490.08228.8160.7800.6540.6011.350.5350.725MolT5-Small-HV0.6130.07530.4580.6990.5470.4821.440.4790.983T5-Base0.7690.06727.1120.8160.7010.6371.440.5540.654MolT5-Base0.7830.08224.8460.7880.6610.6021.160.5440.787MolT5-Base-HV0.6610.07328.2760.7210.5790.5091.380.5010.979T5-Large0.8560.28516.8450.8770.7940.7320.400.5870.959MolT5-Large MolT5-Large-HV0.858 0.8100.318 0.31415.957 16.7580.890 0.8720.813 0.7860.750 0.7220.38 0.440.590 0.5820.958 0.996</p>
<p>Table 4 :
4
HuggingFace model de novo molecule generation results for the different baseline models on the test split of CheBI-20.MolT5-Small-HV, MolT5-Base-HV, and MolT5-Large-HV are models that use a high-validity decoding process-see Appendix D.
PretrainingBLEU-2BLEU-4ROUGE-1ROUGE-2ROUGE-LMETEORText2MolGround Truth0.609C4-Only ZINC-Only C4+ZINC0.523 0.519 0.5320.433 0.434 0.4450.616 0.619 0.6270.463 0.466 0.4770.571 0.573 0.5830.545 0.548 0.5570.530 0.538 0.543</p>
<p>Table 5 :
5
Pretraining ablation results of molecule captioning for MolT5-Small on the test split of CheBI-20.Rouge scores are F1 values.
Pretraining BLEU↑ Exact↑ Levenshtein↓ MACCS FTS↑ RDK FTS↑ Morgan FTS↑ FCD↓ Text2Mol↑ Validity↑Ground Truth0.00.6091.0C4-Only0.7710.08126.840.8110.6970.6412.990.5550.635ZINC-Only0.7160.06332.9530.7010.5760.5242.750.4630.807C4+ZINC0.7490.08228.8160.780.6540.6012.600.5350.725</p>
<p>Table 6 :
6
Pretraining ablation results of molecule generation for MolT5-Small on the test split of CheBI-20.
Pretraining BLEU↑ Exact↑ Levenshtein↓ MACCS FTS↑ RDK FTS↑ Morgan FTS↑ FCD↓ Text2Mol↑ Validity↑Ground Truth0.00.6091.0C4-Only0.7710.08126.840.514990.442590.407044.710.352430.635ZINC-Only0.7160.06332.9530.565710.464830.422873.410.373640.807C4+ZINC0.7490.08228.8160.56550.474150.435723.590.387880.725</p>
<p>Table 7 :
7
Normalized pretraining ablation results of molecule generation for MolT5-Small on the test split of CheBI-20.Molecule-based results (FTS, FCD, Text2Mol) are normalized by multiplying by validity (for scores where higher is better) or dividing by validity (for scores where lower is better).</p>
<p>More examples of interesting molecules generated by different models.
InputRNNTransformerT5MolT5Ground Truth17 15 16The molecule is an eighteen-membered homodetic cyclic peptide which is isolated from Oscillatoria sp. and exhibits antimalarial activity against the W2 chloroquine-resistant strain of the malarial parasite, Plasmodium falciparum. It has a role as a metabolite and an antimalarial. It is a homodetic cyclic peptide, a member of 1,3-oxazoles, a member of 1,3-thiazoles and a macrocycle. The molecule is an N-carbamoylamino acid that is aspartic acid with one of its amino hydrogens replaced by a carbamoyl group. It has a role as a Saccharomyces cerevisiae metabolite, an Escherichia coli metabolite and a human metabolite. It is a N-carbamoyl-amino acid, an aspartic acid derivative and a C4-dicarboxylic acid. It is a conjugate acid of a N-carbamoylaspartate(2-). The molecule is a tripeptide composed of glycine, glycine and L-alanine residues joined in sequence. It has a role as a metabolite. a methyl substituent at position 3. It is produced during the anoxic metabolism of L-tryptophan in the mammalian digestive tract. It has a role as a mammalian metabolite and a human metabolite. The molecule is a member of the class of xanthenes that is used as a Zn(2+)-selective fluorescent indicator. It has a role as a histological dye, a chelator and a visual indicator. It is a member of xanthenes, a cyclic ketone, an aromatic ether, a member of phenols, an organofluorine compound, a tricarboxylic Figure 7: The molecule is a methylindole carrying acid and a substituted aniline.Invalid</p>
<p>8: More examples of interesting molecules generated by different models.
InputRNNTransformerT5MolT5Ground Truththe molecule is an organofluorinethe molecule is aThe molecule is a memberThe molecule is a memberThe molecule is a member ofcompound that is 1, 2, 3, 4 -triazol -deuteratedof the class of pyrazolesof the class of pyrazolesthe class of pyrazoles that is41h -1, 2, 4 -triazole which is substituted at positions 2, 3, and 5 by acompound that is is is is is anthat is 1H-pyrazole that is substituted at positions 1,that is 1H-pyrazole that is substituted at positions 1,1H-pyrazole that is substituted at positions 1, 3,2, 3, 5 -triazol -1 -yl group and atisotopologue of3, 4, and 5 by 2,6-3, 4, and 5 by 2,6-4, and 5 by 2,6-dichloro-4-position 5 by a 2 -( trifluoromethyl ) -chloroform indichloro-4-(trifluorodichloro-4-(trifluoro(trifluoromethyl)phenyl,1, 3, 5 -triazol -1 -yl group. it is anwhich the fourmethyl)phenyl, cyano,methyl)phenyl, cyano,cyano, (trifluoromethyl)organofluorine compound, anhydrogen atoms(trifluoromethyl)sulfinyl,(trifluoromethyl)sulfinyl,sulfanyl, and amino groups,organofluorine compound, anhave been replacedand amino groups,and amino groups,respectively. It is a metaboliteorganofluorine compound, anby deuterium. it isrespectively. It is a nitrile,respectively. It is a nitrile,of the agrochemical fipronil.organofluorine compound, ana deuterateda dichlorobenzene, aa dichlorobenzene, aIt has a role as a marineorganofluorine compound, ancompound, aprimary amino compound,primary amino compound,xenobiotic metabolite. It is aorganofluorine compound, angamma -lactama member of pyrazoles, aa member of pyrazoles, amember of pyrazoles, aorganofluorine compound, anand an aliphaticsulfoxide and a membersulfoxide and a memberdichlorobenzene, a memberorganofluorine compound, ansulfide.of (trifluoromethyl)of (trifluoromethyl)of (trifluoromethyl)benzenes,organofluorine compound and abenzenesbenzenesan organic sulfide and a5member of monochlorobenzenes. the molecule is a the molecule is a linear seventeen -fifteen -membered membered polypeptide oligoopeptide mbp83 -99 ( 91 ) ]. replaced by tyrosyl [ lysyl, lys residue at position 91 lysyl, lysyl, leucyl, epitope with the valyl lysyl, lysyl, lysyl, immunodominant prolyl, lysyl, lysyl, 83 -99 ( mbp83 -99 ) glutaminyl, lysyl, myelin basic protein lysyl […] lysyl, sequence of the lysyl, lysyl, leucyl, corresponds to the lysyl, lysyl, leucyl, val -thr -pro. leucyl, lysyl, leucyl, phe -phe -asn -ile -lysyl, leucyl, lysyl, pro -val -val -his -lysyl, lysyl, leucyl, comprising the sequence glu -asn -comprising glycyl, lysyl, lysyl, leucyl,The molecule is a linear 27-membered polypeptide comprising the sequence Lys-Gly-Lys-Gly-Lys-Gly-Lys-Gly-Lys-Gly-Glu-Asn-(E(83)) residue. attached to the glutamine(83) lysylglycyl)5 [(KG5)] linker 99(F(91))] and with an (L-phenylalanyl [MBP83-at position 91 replaced by epitope with the lysyl residue 99) immunodominant basic protein 83-99 (MBP83-the sequence of the myelin Arg-Thr-Pro. Corresponds to Phe-Asn-Ile-Val-Thr-Pro-Pro-Val-Val-His-Phe-Phe-The molecule is a linear 27-membered polypeptide comprising the sequence Lys-Gly-Lys-Gly-Lys-Gly-Lys-Gly-Lys-Gly-Glu-Asn-Pro-Val-Val-His-Phe-Phe-Phe-Asn-Ile-Val-Thr-Pro-Arg-Thr-Pro. Corresponds to the sequence of the myelin basic protein 83-99 (MBP83-99) immunodominant epitope with the lysyl residue at position 91 replaced by phenylalanyl [MBP83-99(F(91))] and with an (L-lysylglycyl)5 [(KG5)] linker attached to the glutamine(83) (E(83)) residue.nitrile. The molecule is a linear 27-membered polypeptide comprising the sequence Lys-Gly-Lys-Gly-Lys-Gly-Lys-Gly-Lys-Gly-Glu-Asn-Pro-Val-Val-His-Phe-Phe-Tyr-Asn-Ile-Val-Thr-Pro-Arg-Thr-Pro. Corresponds to the sequence of the myelin basic protein 83-99 (MBP83-99) immunodominant epitope with the lysyl residue at position 91 replaced by tyrosyl [MBP83-99(Y(91))] and with an (L-lysylglycyl)5 [(KG5)] linker attached to the glutamine(83) (E(83)) residue.6the molecule is an l -alpha -amino acid anion resulting from the removal of a protonthe molecule is the stable isotope of oxygen with relativeThe molecule is the D-enantiomer of methioninate. It has a role as anThe molecule is the D-enantiomer of methioninate. It has a role as anThe molecule is the D-enantiomer of methioninate. It has a role as an Escherichiafrom the carboxylic acidatomic mass 15. 99.Escherichia coli metabolite,Escherichia coli metabolite,coli metabolite and agroup of ( s ) -2 -hydroxy -lthe most abundant (a Saccharomyces cerevisiaea Saccharomyces cerevisiaeSaccharomyces cerevisiae-cysteinyl -l -cysteine. it is a99. 76 atom percentmetabolite and a bacterialmetabolite and a plantmetabolite. It is a conjugateconjugate base of a ( s ) -2 -) isotope of naturallymetabolite. It is a conjugatemetabolite. It is a conjugatebase of a D-methionine. It ishydroxy -l -methionine.occurring oxygen.base of a D-methionine. It isbase of a D-methionine. It isan enantiomer of a L-an enantiomer of a L-an enantiomer of a L-methioninate.methioninate.methioninate.the molecule is a sesquiterpene lactone. it has a role as an antineoplastic agent and a plant metabolite. it is a sesquiterpene lactone, an organic heterotricyclic compound and a secondary alcohol.the molecule is the stable isotope of oxygen with relative atomic mass 15. 999131, 100 atom percent natural abundance and nuclear spin 3 / 2.The molecule is a maleate salt obtained by combining acetophenazine with two molar equivalents of maleic acid. It has a role as a phenothiazine antipsychotic drug. It contains an acetophenazine.The molecule is a maleate salt obtained by combining rosuvastatin with one molar equivalent of maleic acid. It has a role as an antineoplastic agent and a B-Raf inhibitor. It contains a rosuvastatin(1+).The molecule is a maleate salt obtained by combining afatinib with two molar equivalents of maleic acid. Used for the first-line treatment of patients with metastatic non-small cell lung cancer. It has a role as a tyrosine kinase inhibitor and an antineoplastic agent. Itcontains an afatinib.the molecule is a dtdp -the molecule is theThe molecule is a dTDP-The molecule is a dTDP-The molecule is a dTDP-sugarsugar having 4 -dehydrostable isotope ofsugar having 4-dehydro-sugar having 4-dehydro-having 4-dehydro-2,6-dideoxy--6, 6 -dideoxy -alpha -dhelium with relative2,6-dideoxy-beta-L-2,6-dideoxy-alpha-D-alpha-D-glucose as the sugar-manno -oct -2 -atomic mass 3.glucose as the sugarglucose as the sugarcomponent. It has a role as aulosonic acid. it has a role016029. the leastcomponent. It is a dTDP-component. It is a dTDP-bacterial metabolite. It is a dTDP-as an escherichia coliabundant ( 0.sugar and a secondarysugar and a secondarysugar and a secondary alpha-metabolite and a mouse000137 atomalpha-hydroxy ketone. Italpha-hydroxy ketone. Ithydroxy ketone. It derives from ametabolite. it is apercent ) isotope ofderives from a dTDP-L-derives from a dTDP-D-dTDP-D-glucose. It is a conjugateconjugate acid of a dtdp -naturally occurringglucose.glucose.acid of a dTDP-4-dehydro-2,6-alpha -d -glucose ( 2 -).helium.dideoxy-alpha-D-glucose(2-).the molecule is athe molecule is thetetrapeptide composed ofstable isotope ofl -asparagine, l -aspartyl,oxygen with relativel -aspartic acid, and l -atomic mass 15. 99.aspartic acid units joinedthe most abundant (in sequence by peptide99. 99 atom percentlinkages. it has a role as a) isotope ofmetabolite. it derivesnaturally occurringfrom a l -glutamic acid.oxygen.
For more explanation of the pretraining task, we refer the readers to the original T5 paper(Raffel et al., 2020).
https://tinyurl.com/t511-ckpts
https://tinyurl.com/megamolbart
https://tinyurl.com/t511-ckpts
AcknowledgementWe would like to thank Martin Burke for his helpful discussion.This research is based upon work supported by the Molecule Maker Lab Institute: an AI research institute program supported by NSF under award No. 2019897 and No. 2034562.The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the U.S. Government.The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.G Testing Model Diversity with RetrievalTo test the diversity of generations, we apply a Text2Mol(Edwards et al., 2021)cross-modal retrieval model to the entire generated set of molecules or descriptions.In the case of molecules, we first take the molecules generated for our test set.We consider these molecules as our corpus and then use the descriptions (which were used to generate the molecules in the first place) as our queries.So, for each query we look at the rank of its generated molecule (the highest rank is 1).This process tests whether the Text2Mol retrieval model can differentiate between the generated (valid) molecules.Doing so means it can retrieve a specific molecule when given the description used to generate it.If the generative model did not sufficiently take the descriptions into consideration, then the retrieval model won't be able to distinguish between generated molecules and the scores will be very low (such as the transformer model, which frequently generates the same molecule/caption).As an example, consider that we have 10 descriptions of molecules.ModelMean Rank MRR Hits@1 Hits@10 Hits@100 Validity   For each description, we use a generative model to generate a molecule.Now, we treat these 10 generated molecules as our corpus.Using our retrieval model, we now consider each description as a query and try to retrieve the molecule that was generated from that description.If the retrieval model performs poorly, that means the molecules which were generated are difficult to distinguish from one another.By using this method with different generative models, we measure the relative diversity of generated molecules along with how well the generated molecules match the description.Results are reported in Tables8 and 9for retrieving generated molecules from descriptions and for retrieving generated descriptions from molecules, respectively.We use the same Text2Mol model for retrieval here as in the Text2Mol metric.For description of metrics, see(Edwards et al., 2021).Results indicate that MolT5 model generations are sufficiently distinct to be retrievable.In contrast, the outputs of the captioning transformer are essentially indistinguishable for the retrieval model.H Statistical SignificanceTo strengthen the quantitative results, we conducted statistical tests between T5-Large and MolT5-Large.For molecule captioning, we carried out paired t-tests.The computed p-values and test statistics are:• For ROUGE-1, the p-value is 1.53e-22.The test statistic is -9.841.• For ROUGE-2, the p-value is 3.27e-26.The test statistic is -10.683.• For ROUGE-L, the p-value is 3.58e-21.The test statistic is -9.509.• For METEOR, the p-value is 2.02e-21.The test statistic is -9.57.• For Text2Mol, the p-value is 1.053e-29.The test statistic is -11.431.Note that for every metric above, the higher the score, the better the performance.Since all the test statistics are negative and the p-values are extremely small, MolT5-Large produces significant improvements over T5-Large on the task of molecule captioning.For molecule generation, we conducted independent t-tests to compare between T5-Large and MolT5-Large:• For MACCS FTS, the p-value is 0.008.The test statistic is -2.652.• For RDK FTS, the p-value is 0.0092.The test statistic is -2.604.• For Morgan FTS, the p-value is 0.0153.The test statistic is -2.426.• For Levenshtein, the p-value is 0.064.The test statistic is 1.8544704091978725.• For Text2Mol, the p-value is 0.168.The test statistic is -1.376724743237994.Note that for Levenshtein, the lower the score, the better the performance.We see that the test statistics for all metrics except Levenshtein is negative.In addition, while the p-values now are typically larger than the ones computed for molecule captioning, the p-values for molecule generation are still reasonably small.Therefore, we can still conclude that MolT5-Large also produces significant improvements over T5-Large on the task of molecule generation.I NLP Capabilities of MolT5We finetune our MolT5-based models on some GLUE tasks and see similar results for MolT5 and T5.For example, our finetuned MolT5-base model achieved an accuracy score of 95.6% on SST-2.For comparison, T5-base achieved a score of 95.2%.Since our self-supervised learning framework uses a large amount of natural language text in addition to SMILES string, it is reasonable that our MolT5-based models still possess "typical" NLP capabilities.J Model Probing TestsTo generate a variety of output molecules given a single input, we employ diverse beam search(Vijayakumar et al., 2016)with a beam width and beam group of 30 and a diversity penalty of 0.5.The goal of these tests (shown in the following figures) is to explore molecule outputs given very specific desired properties.Note that these brief input descriptions are out-of-distribution from the finetuning data.In the following figures, the top 10 valid molecules are shown for each prompt (order: left to right, top to bottom).
Molgpt: Molecular generation using a transformer-decoder model. Viraj Bagal, Rishal Aggarwal, Deva Vinod, Priyakumar, Journal of Chemical Information and Modeling. 2021</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Img2smi: Translating molecular structure images to simplified molecular-input line-entry system. Daniel Campos, Heng Ji, arXiv:2109.042022021arXiv preprint</p>
<p>Molecular fingerprint similarity search in virtual screening. Adrià Cereto-Massagué, María José Ojeda, Cristina Valls, Miquel Mulero, Santiago Garcia-Vallvé, Gerard Pujadas, Methods. 712015</p>
<p>Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, C Lawrence Zitnick, ArXiv, abs/1504.00325Microsoft coco captions: Data collection and evaluation server. 2015</p>
<p>Uniter: Universal image-text representation learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Computer Vision -ECCV 2020. ChamSpringer International Publishing2020</p>
<p>Chemberta: Large-scale selfsupervised pretraining for molecular property prediction. Seyone Chithrananda, Gabe Grand, Bharath Ramsundar, arXiv:2010.098852020arXiv preprint</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, 10.3115/v1/D14-1179Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational Linguistics2014</p>
<p>Preparing a collection of radiology examinations for distribution and retrieval. Dina Demner-Fushman, Marc D Kohli, Sonya E Marc B Rosenman, Laritza Shooshan, Sameer Rodriguez, George R Antani, Clement J Thoma, Mcdonald, Journal of the American Medical Informatics Association. 2322016</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>Reoptimization of mdl keys for use in drug discovery. Burton A Joseph L Durant, Douglas R Leland, James G Henry, Nourse, Journal of chemical information and computer sciences. 4262002</p>
<p>Convolutional networks on graphs for learning molecular fingerprints. Dougal David K Duvenaud, Jorge Maclaurin, Rafael Iparraguirre, Timothy Bombarell, Alán Hirzel, Ryan P Aspuru-Guzik, Adams, Advances in neural information processing systems. 201528</p>
<p>Text2mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Molecular representation learning with language models and domain-relevant auxiliary tasks. Thomas Benedek Fabian, Héléna Edlich, Joshua Gaspar, Marco Meyers, Mohamed Fiscato, Ahmed, arXiv:2011.13230Marwin Segler,. 2020arXiv preprint</p>
<p>Utilizing graph machine learning within drug discovery and development. Thomas Gaudelet, Ben Day, Jyothish Arian R Jamasb, Cristian Soman, Gertrude Regep, Jeremy Br Liu, Richard Hayter, Charles Vickers, Jian Roberts, Tang, Briefings in bioinformatics. 2261592021</p>
<p>Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, 201730</p>
<p>A comprehensive survey of deep learning for image captioning. Ferdous Md Zakir Hossain, Sohel, ACM Computing Surveys (CsUR). 5162019Mohd Fairuz Shiratuddin, and Hamid Laga</p>
<p>Scaling up vision-language pre-training for image captioning. Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, Lijuan Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Chemformer: A pre-trained transformer for computational chemistry. Ross Irwin, Spyridon Dimitriadis, Jiazhen He, Esben Bjerrum, 10.26434/chemrxiv-2021-v2pnn2021ChemRxiv</p>
<p>Mol2vec: unsupervised machine learning approach with chemical intuition. Sabrina Jaeger, Simone Fulle, Samo Turk, Journal of chemical information and modeling. 5812018</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Transformers in vision: A survey. Salman Khan, Muzammal Naseer, Munawar Hayat, Fahad Syed Waqas Zamir, Mubarak Shahbaz Khan, Shah, 2021ACM Computing Surveys (CSUR</p>
<p>Selfreferencing embedded strings (selfies): A 100% robust molecular string representation. Mario Krenn, Florian Häse, Akshatkumar Nigam, Machine Learning: Science and Technology. 14450242020Pascal Friederich, and Alan Aspuru-Guzik</p>
<p>Rdkit: Open-source cheminformatics software. Greg Landrum, 2021</p>
<p>Oscar: Objectsemantics aligned pre-training for vision-language tasks. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, European Conference on Computer Vision. Springer2020</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Multilingual denoising pretraining for neural machine translation. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer, 20208Transactions of the Association for Computational Linguistics</p>
<p>Entity-aware image caption generation. Di Lu, Spencer Whitehead, Lifu Huang, Heng Ji, Shih-Fu Chang, Proc. 2018 Conference on Empirical Methods in Natural Language Processing. 2018 Conference on Empirical Methods in Natural Language essing2018. EMNLP2018</p>
<p>Vilbert: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Proceedings of the 33rd International Conference on Neural Information Processing Systems. the 33rd International Conference on Neural Information Processing Systems2019</p>
<p>Unified deep learning model for multitask reaction predictions with explanation. Jieyu Lu, Yingkai Zhang, Journal of Chemical Information and Modeling. 2022</p>
<p>Efficient estimation of word representations in vector space. Tomás Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, 1st International Conference on Learning Representations, ICLR 2013. Scottsdale, Arizona, USA2013a. May 2-4, 2013Workshop Track Proceedings</p>
<p>Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. 2013b</p>
<p>Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? levenshtein distance, spell checker. Frederic P Miller, Agnes F Vandome, John Mcbrewster, 2009hamming distance</p>
<p>Automatic image captioning. Jia-Yu Pan, Hyung-Jeong Yang, Pinar Duygulu, Christos Faloutsos, 2004 IEEE International Conference on Multimedia and Expo (ICME). IEEE20043IEEE Cat</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>A survey on biomedical image captioning. John Pavlopoulos, Vasiliki Kougia, Proceedings of the second workshop on shortcomings in vision and language. the second workshop on shortcomings in vision and language2019and Ion Androutsopoulos</p>
<p>How multilingual is multilingual BERT?. Telmo Pires, Eva Schlinger, Dan Garrette, 10.18653/v1/P19-1493Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, Italy2019Association for Computational Linguistics</p>
<p>Molecular sets (moses): a benchmarking platform for molecular generation models. Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Frontiers in pharmacology. 1119312020</p>
<p>Fréchet chemnet distance: A metric for generative models for molecules in drug discovery. Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, Günter Klambauer, Journal of chemical information and modeling. 5892018</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. the 38th International Conference on Machine Learning, ICML 2021PMLR2021. 18-24 July 2021139of Proceedings of Machine Learning Research</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.06125Hierarchical textconditional image generation with clip latents. 2022arXiv preprint</p>
<p>Zero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, International Conference on Machine Learning. PMLR2021</p>
<p>Generative adversarial text to image synthesis. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee, International conference on machine learning. PMLR2016</p>
<p>Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases. Ahmet Sureyya Rifaioglu, Heval Atas, 10.1093/bib/bby061Briefings in Bioinformatics. 2052018Maria Jesus Martin, Rengul Cetin-Atalay, Volkan Atalay, and Tunca Dogan</p>
<p>Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Jacob Marc Van Zee, Sebastian Austin, Livio Baldini Goodman, Haitang Soares, Sasha Hu, Aakanksha Tsvyashchenko, Jasmijn Chowdhery, Jannis Bastings, Xavier Bulian, Jianmo Garcia, Andrew Ni, Chen, arXiv:2203.17189Scaling up models and data with t5x and seqio. Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, Andrea Gesmundo, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp2022arXiv preprint</p>
<p>Journal of chemical information and modeling. David Rogers, Mathew Hahn, 201050Extendedconnectivity fingerprints</p>
<p>Get your atoms in order -an opensource implementation of a novel and robust molecular canonicalization algorithm. Nadine Schneider, Roger A Sayle, Gregory A Landrum, Journal of chemical information and modeling. 552015</p>
<p>Extraction of organic chemistry grammar from unsupervised learning of chemical reactions. Philippe Schwaller, Benjamin Hoover, Jean-Louis Reymond, Hendrik Strobelt, Teodoro Laino, Science Advances. 715e41662021a</p>
<p>Mapping the space of chemical reactions using attention-based neural networks. Philippe Schwaller, Daniel Probst, Alain C Vaucher, H Vishnu, David Nair, Teodoro Kreutter, Jean-Louis Laino, Reymond, Nature Machine Intelligence. 322021b</p>
<p>Improved deep metric learning with multi-class n-pair loss objective. Kihyuk Sohn, Proceedings of the 30th International Conference on Neural Information Processing Systems. the 30th International Conference on Neural Information Processing Systems2016</p>
<p>Giuseppe Fiameni, and Rita Cucchiara. Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli, arXiv:2107.06912From show to tell: A survey on image captioning. 2021arXiv preprint</p>
<p>Zinc 15 -ligand discovery for everyone. T Sterling, John J Irwin, Journal of Chemical Information and Modeling. 552015a</p>
<p>Zinc 15-ligand discovery for everyone. Teague Sterling, John J Irwin, Journal of chemical information and modeling. 55112015b</p>
<p>VL-BERT: pretraining of generic visual-linguistic representations. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Fine-grained chemical entity typing with multimodal knowledge representation. Chenkai Sun, Weijiang Li, Jinfeng Xiao, Nikolaus Nova Parulian, Chengxiang Zhai, Heng Ji, 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE2021</p>
<p>Elementary mathematical theory of classification and prediction. T Taffee, Tanimoto, 1958</p>
<p>Bioassayclr: Prediction of biological activity for novel bioassays based on rich textual descriptions. Andreu Vall, Sepp Hochreiter, Günter Klambauer, 2021ELLIS Machine Learning for Molecule Discovery Workshop</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Inferring experimental procedures from textbased representations of chemical reactions. Alain C Vaucher, Philippe Schwaller, Joppe Geluykens, H Vishnu, Anna Nair, Teodoro Iuliano, Laino, Nature communications. 1212021</p>
<p>Automated extraction of chemical synthesis actions from experimental procedures. Alain C Vaucher, Federico Zipoli, Joppe Geluykens, H Vishnu, Philippe Nair, Teodoro Schwaller, Laino, Nature communications. 1112020</p>
<p>K Ashwin, Michael Vijayakumar, Cogswell, Qing Ramprasath R Selvaraju, Stefan Sun, David Lee, Dhruv Crandall, Batra, arXiv:1610.02424Diverse beam search: Decoding diverse solutions from neural sequence models. 2016arXiv preprint</p>
<p>Show and tell: A neural image caption generator. Oriol Vinyals, Alexander Toshev, Samy Bengio, D Erhan, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015. 2015</p>
<p>Chemical-reaction-aware molecule representation learning. Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han, Martin Burke, Proc. The International Conference on Learning Representations (ICLR2022). The International Conference on Learning Representations (ICLR2022)2022</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Smiles. 2. algorithm for generation of unique smiles notation. David Weininger, Arthur Weininger, Joseph L Weininger, Journal of chemical information and computer sciences. 2921989</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Jamie Brew, ArXiv, abs/1910.037712019</p>
<p>Attngan: Fine-grained text to image generation with attentional generative adversarial networks. Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Zheni Zeng, Yuan Yao, Zhiyuan Liu, Maosong Sun, Nature communications. 1312022</p>
<p>Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris N Metaxas, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8450 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8450</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8450</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-dbfb49de5ae1b351991d6c55e8179ff4640ed60e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dbfb49de5ae1b351991d6c55e8179ff4640ed60e" target="_blank">GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> GraphReader is introduced, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously, demonstrating superior performance on four challenging single-hop and multi-hop benchmarks.</p>
                <p><strong>Paper Abstract:</strong> Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8450.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8450.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphReader</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphReader (graph-based agent for long-context QA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based agent system that structures long documents into nodes (key elements + atomic facts) and autonomously explores the graph with a planner and a notebook to accumulate supporting facts, enabling a 4k-window LLM to answer questions over very long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GraphReader</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that (1) splits long text into chunks, (2) summarizes chunks into atomic facts and key elements, (3) builds a graph where nodes = (key element, atomic facts), and (4) runs autonomous exploration guided by a rational plan and a notebook, invoking functions (read_chunk, read_neighbor_node, etc.) for coarse-to-fine access.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-128k (used as LLM; GraphReader uses a 4k input window)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source GPT-4 API (gpt-4-1106-preview) used as the base LLM for planning, summarization, and agent decisions; experiments configured GraphReader to operate with a 4k token input window while the underlying model supports a larger context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context QA benchmarks (HotpotQA, 2WikiMultihopQA, MuSiQue, NarrativeQA, HotpotWikiQA-mixup, QuALITY, Natural Questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answering single-hop and multi-hop question answering over very long documents (context lengths up to 256k) by gathering supporting facts and reasoning to produce final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / multi-hop reasoning over long context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>structured external memory (graph) + episodic/working memory (notebook)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Construct a persistent graph index from the document: nodes store key elements and their atomic facts; during question answering an agent maintains a notebook (explicit memory buffer) that records supporting facts discovered during exploration; the agent uses function calls to read node atomic facts, open original chunks, or traverse neighbor nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Atomic facts extracted from chunks, normalized key elements (graph nodes), chunk IDs (pointers to original text), and the agent's notebook entries (supporting facts accumulated during exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Agent-driven selective access: semantic filtering via rational plan and node selection; explicit function calls (read_chunk(List[IDs]), read_neighbor_node(key), stop_and_read_neighbor, etc.) retrieve atomic facts or original chunks; final reasoning concatenates notebook entries (chain-of-thought) for answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 2 results (GraphReader, 4k input window): HotpotQA LR-1 84.3%, LR-2 89.7%, EM 55.0%, F1 70.0%; 2WikiMultihopQA LR-1 83.7%, LR-2 87.0%, EM 59.3%, F1 70.1%; MuSiQue LR-1 59.0%, LR-2 63.5%, EM 38.0%, F1 47.4%; NarrativeQA LR-1 65.0%, LR-2 80.0%, EM 15.5?, F1 29.8% (as reported in Table 2). On HotpotWikiQA-mixup (16k–256k) GraphReader consistently outperforms GPT-4-128k full read and other baselines; GraphReader maintains ~60% supporting-fact recall at 256k. (See paper tables for per-benchmark numbers.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to baselines (ReadAgent, GPT-4-128k full read, BM25/Ada RAG, GraphRAG, LongRAG), GraphReader outperforms them on multi-hop and single-hop long-context QA. Ablations in Table 4: removing the rational plan ('w/o Rational Plan') reduces performance (e.g., HotpotQA F1: 70.0% -> 63.8%); randomizing node selection ('w/o Node Selection') causes a larger drop (HotpotQA F1: 70.0% -> 54.1%). Additional analyses show optimal initial node count (5) and chunk size (L=2k). Recall analysis: atomic facts recall 76.4% vs final notebook recall 90.5% (Table 6), indicating the notebook enriches retrieved information.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structuring long documents as a graph of key elements + atomic facts and using an agent with a notebook to selectively access nodes enables a small-window LLM (4k) to outperform much larger direct-read baselines (GPT-4-128k) on very long contexts; the notebook increases recall of supporting facts and the rational plan and node-selection strategies are critical to performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on closed-source GPT-4 API (QPS/regional constraints); effectiveness depends on agent planning/reasoning quality; extra token cost vs some baselines (GraphReader avg. cost 52.8k tokens vs ReadAgent 48.7k), and graph construction overhead (but amortizable across multiple queries on same document).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8450.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8450.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReadAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReadAgent (human-inspired reading agent with gist memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent baseline that segments long texts into pages and constructs gist memories (a memory directory) to look up and read pages sequentially for question answering over very long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A human-inspired reading agent with gist memory of very long contexts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReadAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that condenses long documents into gist memory summaries (a memory directory) and performs page selection and sequential reading (with options for reading up to N pages), using LLM planning to navigate the gist memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-128k (used in experiments as the LLM for ReadAgent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source GPT-4 API, used to generate gist memories and to read selected pages with a large context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context QA benchmarks (same as GraphReader comparisons: HotpotQA, 2WikiMultihopQA, MuSiQue, NarrativeQA, HotpotWikiQA-mixup)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answering questions by retrieving and reading from gist memory pages derived from very long documents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / document retrieval + reading</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>gist memory directory (summarized pages), i.e., an external retrieval-style memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Condense pages into gist summaries stored in a memory directory; agent looks up summaries to select pages to read; reads pages sequentially (ReadAgent-S) and may maintain notes during reading.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Gist summaries (page-level), selected original pages (text chunks) and any notes produced during sequential reading.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Lookup in gist memory by relevance, then sequential page reading (allowed to read 1–5 pages depending on settings); selection heuristics and LLM planning guide retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 2 (ReadAgent, 128k input window): HotpotQA LR-1 72.3%, LR-2 78.7%, EM 48.0%, F1 62.0%; 2WikiMultihopQA LR-1 79.0%, LR-2 81.0%, EM 52.7%, F1 63.7%; MuSiQue LR-1 54.5%, LR-2 61.0%, EM 35.0%, F1 45.1%; NarrativeQA LR-1 63.0%, LR-2 75.5%, EM 5.0%, F1 18.9%. Token cost (HotpotWikiQA-mixup-256k): avg. cost 48.7k tokens (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared directly with GraphReader: ReadAgent underperforms, especially on extremely long contexts and multi-hop tasks. Paper argues ReadAgent's page-level gist memory lacks detailed content per page, making page selection harder and reducing recall as context length increases.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gist-memory approaches (ReadAgent) are effective but lose detailed information needed for multi-hop reasoning in very long contexts; GraphReader's finer-grained atomic facts + graph structure yields higher recall and better multi-hop performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Page-level gist summarization can omit details, leading to poor page selection for multi-hop questions; performance degrades more than GraphReader as context length increases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8450.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8450.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (BM25 / Ada-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation baselines (Okapi BM25 and OpenAI Ada-002 embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard retrieval-augmented baselines that retrieve top-k chunks using BM25 or vector embeddings (Ada-002), then pass those chunks to an LLM to answer questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The probabilistic relevance framework: BM25 and beyond</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG (BM25 / Ada-002 retrieval pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval-augmented pipeline: split document into chunks, retrieve top-k chunks by BM25 or embedding similarity (Ada-002), feed retrieved chunks and question into GPT-4 for answer generation under a 4k input window.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-128k (reader), BM25 (Okapi) and text-embedding-ada-002 (retrieval embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BM25 is a classical lexical retriever; text-embedding-ada-002 is OpenAI's embedding model used for semantic retrieval; GPT-4 used as reader over retrieved chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context QA benchmarks (HotpotQA, 2WikiMultihopQA, MuSiQue, NarrativeQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answering questions by retrieving top relevant chunks and letting an LLM (GPT-4) read them within a limited input window.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval memory (document chunks stored externally and retrieved at query time)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Index all chunks and retrieve top-k chunks by BM25 or embedding similarity at query time; pass retrieved chunks to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Original text chunks (retrieved passages) and any retrieved chunk summaries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Lexical retrieval (BM25) or semantic retrieval via embedding similarity (Ada-002) selecting top-1 or top-3 chunks depending on setup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 2 (examples): BM25 (top-3, 4k) HotpotQA LR-1 74.7%, LR-2 78.3%, EM 45.7%, F1 58.5%; Ada-002 (top-3, 4k) HotpotQA LR-1 72.0%, LR-2 77.3%, EM 45.0%, F1 58.1%. Generally RAG methods show the worst performance among compared approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>RAG methods perform poorly compared to direct full-text read by GPT-4-128k and agent approaches (GraphReader), likely due to retrieval failing to recall all supporting chunks needed for multi-hop answers; increasing number of retrieved chunks is limited by input window.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple retrieval (BM25/embedding) struggles on complex multi-hop long-context QA because it is hard to retrieve all supporting facts within the context window; GraphReader's structured graph+agent approach yields better recall and QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval recall limits performance; tradeoff between number/size of retrieved chunks and context window; retrieval may miss decoy-resistant multi-hop evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8450.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8450.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PEARL (Prompting large language models to plan and execute actions over long documents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting framework that decomposes complex questions into actions via action-mining, plan formulation, and execution to improve reasoning over long documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PEARL: Prompting large language models to plan and execute actions over long documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PEARL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Framework that mines actions from a document and uses planning/execution prompts to guide LLMs to perform multi-step reasoning over long documents; included as an agent-style baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-128k (used as baseline LLM in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source GPT-4 API used to perform PEARL prompting pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context QA benchmarks (same benchmarks as others)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Planning and executing decomposed actions over long documents to answer questions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / plan-and-execute agent</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit plan/execution memory (action sequences and intermediate results stored across steps)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Prompted decomposition into steps (actions) and executing them sequentially, with intermediate outputs used as context for subsequent steps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Intermediate step outputs and action plans (textual), possibly stored as notes during execution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation of prior step outputs; no explicit external graph used in the PEARL baseline presented.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 2 (PEARL, 128k): HotpotQA LR-1 74.7%, LR-2 79.0%, EM 46.3%, F1 60.4%; 2WikiMultihopQA LR-1 70.0%, LR-2 71.0%, EM 46.0%, F1 57.6%; MuSiQue LR-1 45.0%, LR-2 51.5%, EM 23.0%, F1 33.3%; NarrativeQA LR-1 43.5%, LR-2 48.0%, EM 7.5%, F1 16.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>PEARL is compared as an agent-style baseline; GraphReader outperforms PEARL on the evaluated datasets, indicating structured graph + notebook yields better retrieval/recall for multi-hop evidence than PEARL's plan/execution alone.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompted planning & execution helps, but GraphReader's explicit document graph + notebook retains more actionable facts for multi-hop reasoning across very long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>PEARL may not effectively capture long-range multi-hop dependencies without a structured external index like GraphReader's graph.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8450.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8450.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphRAG (graph-based RAG variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented baseline that uses LLMs to construct graph-based text indices (entity knowledge graph + summaries) to improve retrieval and answer composition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From local to global: A graph rag approach to query-focused summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GraphRAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Graph-based retrieval pipeline that first builds an entity knowledge graph and group summaries, then composes partial information from summaries to answer queries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-128k (used as reader in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM used to construct graph index and to read summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context QA benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Query-focused summarization and QA using graph-structured retrieval summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented question answering with graph index</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based index (document summaries / entity graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM constructs an entity knowledge graph and summary groups, which are used as retrieval units for answering queries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Entity nodes and group summaries derived from documents.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieve relevant summary nodes from the graph index to provide partial evidence for answer composition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 2 (GraphRAG, 128k): HotpotQA LR-1 73.7%, LR-2 80.3%, EM 49.7%, F1 59.7%; 2WikiMultihopQA LR-1 67.7%, LR-2 71.3%, EM 42.3%, F1 53.9%; MuSiQue LR-1 46.5%, LR-2 56.0%, EM 21.5%, F1 31.2%; NarrativeQA LR-1 52.0%, LR-2 66.5%, EM 15.0%, F1 23.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared with GraphReader: GraphRAG uses summaries as retrieval units and underperforms GraphReader on multi-hop benchmarks, suggesting that GraphReader's atomic-fact granularity + agent exploration captures more precise multi-hop evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Graph-based retrieval indices help but the fidelity/granularity of stored summaries matters for multi-hop QA; GraphReader's atomic-facts + agentic exploration improved recall and QA accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Summaries as retrieval units can miss fine-grained facts required for multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8450.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8450.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LongRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LongRAG (long retriever + long reader)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented approach that processes corpora into larger units ('long retriever' and 'long reader') to reduce retrieval burden and improve long-context performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LongRAG: Enhancing retrieval-augmented generation with long-context llms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LongRAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Method that preprocesses corpus into larger-sized units and employs specialized retrieval/reading components to handle long contexts more efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-128k (reader)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as the long reader in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context QA benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answering questions by retrieving larger document units and reading them with long-context LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented question answering</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieved large chunks (long units) as external memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Aggregate documents into large retrieval units to reduce retrieval count; use long-context reader to read the units.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Large aggregated text units (long chunks).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieval over aggregated units (long retriever) followed by long-reader processing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 2 (LongRAG, 128k): HotpotQA LR-1 75.7%, LR-2 78.3%, EM 48.7%, F1 63.9%; 2WikiMultihopQA LR-1 73.0%, LR-2 75.0%, EM 51.3%, F1 63.5%; MuSiQue LR-1 49.0%, LR-2 54.5%, EM 31.0%, F1 40.3%; NarrativeQA LR-1 60.5%, LR-2 69.0%, EM 15.0%, F1 27.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared with GraphReader: LongRAG improves retrieval efficiency but GraphReader shows superior multi-hop reasoning and recall via atomic-fact graph + agent exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Aggregating into large retrieval units helps but cannot fully replace fine-grained structured memory plus agentic selective access for complex multi-hop QA.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tradeoffs in unit size: too large units reduce retrieval resolution; too small units increase retrieval overhead and context-window pressure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8450.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8450.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemWalker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemWalker (walking down the memory maze)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior method that organizes documents into a tree and simulates an agent walking/searching memory to go beyond context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Walking down the memory maze: Beyond context limit through interactive reading</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemWalker</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prior work that organizes documents into a tree and uses agent-like exploration to traverse memory beyond the LLM's native context window.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context reading / QA (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Exploration of tree-structured memory to recover long-context information.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>document exploration / QA</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>tree-structured memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Interactive walking on a document tree, using LLM-guided traversal to access relevant parts of the corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Tree nodes containing summaries or passages.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Agent traversal of tree nodes guided by LLM decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned in related work as a prior agent-memory approach; GraphReader differs by using a graph (not tree) and richer atomized facts to better capture multi-hop dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Serves as a reference point showing interactive memory traversal; GraphReader extends these ideas using graph structure and a notebook.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tree structure may limit capturing multi-hop or long-range relationships present in graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8450.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8450.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KGP (Knowledge Graph Prompting for Multi-Document QA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that organizes documents into a knowledge graph and uses LLMs to generate queries over the graph; cited as related but not using agent planning/reflection in the same way as GraphReader.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge graph prompting for multi-document question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KGP</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs a knowledge graph from documents and uses LLMs to generate queries over the graph for multi-document QA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-document question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use of knowledge graph prompts to guide multi-document QA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering with knowledge graph retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph (document-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Graph index of entities and relations used to guide question answering; LLMs generate queries over graph rather than full agentic planning/exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Entities and relations; graph nodes representing document entities.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Graph-based lookup / query generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as related work; paper contrasts KGP's approach of using agent to generate queries with GraphReader's agentic planning + reflection and direct exploitation of graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows prior usage of graphs for multi-document QA; GraphReader leverages agentic exploration directly over the graph for better planning and reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>KGP primarily uses agent to generate queries, not fully leveraging agent planning/reflection for exploratory access.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8450.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8450.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebGPT (browser-assisted QA with human feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior system that simulates human actions (searching the web) with an agent to answer questions, cited as an example of agents retrieving unstructured information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WebGPT: Browser-assisted question-answering with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that performs simulated web actions (searches, clicks) to retrieve information and composes answers, supervised with human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Web-based question answering / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agentic retrieval from web and synthesis of answers with human-feedback-trained policy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>web retrieval + QA</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external web as memory (retrieved pages and browsing history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Agentic browsing actions store retrieved pages and use them to compose answers.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Retrieved web pages and browsing traces.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Search queries and click actions to obtain pages then read/aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned in related work as an agent retrieving unstructured external information; GraphReader differs by building a structured document graph and using a notebook to accumulate supporting facts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Exemplifies agentic retrieval from external sources; motivates agent-based methods for long-context retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Web-based retrieval faces its own noise and retrieval-selection tradeoffs; GraphReader targets internal long-document retrieval challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A human-inspired reading agent with gist memory of very long contexts <em>(Rating: 2)</em></li>
                <li>PEARL: Prompting large language models to plan and execute actions over long documents <em>(Rating: 2)</em></li>
                <li>Walking down the memory maze: Beyond context limit through interactive reading <em>(Rating: 2)</em></li>
                <li>From local to global: A graph rag approach to query-focused summarization <em>(Rating: 2)</em></li>
                <li>LongRAG: Enhancing retrieval-augmented generation with long-context llms <em>(Rating: 2)</em></li>
                <li>Knowledge graph prompting for multi-document question answering <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
                <li>The probabilistic relevance framework: BM25 and beyond <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8450",
    "paper_id": "paper-dbfb49de5ae1b351991d6c55e8179ff4640ed60e",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "GraphReader",
            "name_full": "GraphReader (graph-based agent for long-context QA)",
            "brief_description": "A graph-based agent system that structures long documents into nodes (key elements + atomic facts) and autonomously explores the graph with a planner and a notebook to accumulate supporting facts, enabling a 4k-window LLM to answer questions over very long contexts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GraphReader",
            "agent_description": "Agent that (1) splits long text into chunks, (2) summarizes chunks into atomic facts and key elements, (3) builds a graph where nodes = (key element, atomic facts), and (4) runs autonomous exploration guided by a rational plan and a notebook, invoking functions (read_chunk, read_neighbor_node, etc.) for coarse-to-fine access.",
            "model_name": "GPT-4-128k (used as LLM; GraphReader uses a 4k input window)",
            "model_description": "Closed-source GPT-4 API (gpt-4-1106-preview) used as the base LLM for planning, summarization, and agent decisions; experiments configured GraphReader to operate with a 4k token input window while the underlying model supports a larger context.",
            "task_name": "Long-context QA benchmarks (HotpotQA, 2WikiMultihopQA, MuSiQue, NarrativeQA, HotpotWikiQA-mixup, QuALITY, Natural Questions)",
            "task_description": "Answering single-hop and multi-hop question answering over very long documents (context lengths up to 256k) by gathering supporting facts and reasoning to produce final answers.",
            "task_type": "question answering / multi-hop reasoning over long context",
            "memory_used": true,
            "memory_type": "structured external memory (graph) + episodic/working memory (notebook)",
            "memory_mechanism": "Construct a persistent graph index from the document: nodes store key elements and their atomic facts; during question answering an agent maintains a notebook (explicit memory buffer) that records supporting facts discovered during exploration; the agent uses function calls to read node atomic facts, open original chunks, or traverse neighbor nodes.",
            "memory_representation": "Atomic facts extracted from chunks, normalized key elements (graph nodes), chunk IDs (pointers to original text), and the agent's notebook entries (supporting facts accumulated during exploration).",
            "memory_retrieval_method": "Agent-driven selective access: semantic filtering via rational plan and node selection; explicit function calls (read_chunk(List[IDs]), read_neighbor_node(key), stop_and_read_neighbor, etc.) retrieve atomic facts or original chunks; final reasoning concatenates notebook entries (chain-of-thought) for answer generation.",
            "performance_with_memory": "Table 2 results (GraphReader, 4k input window): HotpotQA LR-1 84.3%, LR-2 89.7%, EM 55.0%, F1 70.0%; 2WikiMultihopQA LR-1 83.7%, LR-2 87.0%, EM 59.3%, F1 70.1%; MuSiQue LR-1 59.0%, LR-2 63.5%, EM 38.0%, F1 47.4%; NarrativeQA LR-1 65.0%, LR-2 80.0%, EM 15.5?, F1 29.8% (as reported in Table 2). On HotpotWikiQA-mixup (16k–256k) GraphReader consistently outperforms GPT-4-128k full read and other baselines; GraphReader maintains ~60% supporting-fact recall at 256k. (See paper tables for per-benchmark numbers.)",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared to baselines (ReadAgent, GPT-4-128k full read, BM25/Ada RAG, GraphRAG, LongRAG), GraphReader outperforms them on multi-hop and single-hop long-context QA. Ablations in Table 4: removing the rational plan ('w/o Rational Plan') reduces performance (e.g., HotpotQA F1: 70.0% -&gt; 63.8%); randomizing node selection ('w/o Node Selection') causes a larger drop (HotpotQA F1: 70.0% -&gt; 54.1%). Additional analyses show optimal initial node count (5) and chunk size (L=2k). Recall analysis: atomic facts recall 76.4% vs final notebook recall 90.5% (Table 6), indicating the notebook enriches retrieved information.",
            "key_findings": "Structuring long documents as a graph of key elements + atomic facts and using an agent with a notebook to selectively access nodes enables a small-window LLM (4k) to outperform much larger direct-read baselines (GPT-4-128k) on very long contexts; the notebook increases recall of supporting facts and the rational plan and node-selection strategies are critical to performance.",
            "limitations_or_challenges": "Relies on closed-source GPT-4 API (QPS/regional constraints); effectiveness depends on agent planning/reasoning quality; extra token cost vs some baselines (GraphReader avg. cost 52.8k tokens vs ReadAgent 48.7k), and graph construction overhead (but amortizable across multiple queries on same document).",
            "uuid": "e8450.0",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ReadAgent",
            "name_full": "ReadAgent (human-inspired reading agent with gist memory)",
            "brief_description": "An agent baseline that segments long texts into pages and constructs gist memories (a memory directory) to look up and read pages sequentially for question answering over very long contexts.",
            "citation_title": "A human-inspired reading agent with gist memory of very long contexts",
            "mention_or_use": "use",
            "agent_name": "ReadAgent",
            "agent_description": "Agent that condenses long documents into gist memory summaries (a memory directory) and performs page selection and sequential reading (with options for reading up to N pages), using LLM planning to navigate the gist memory.",
            "model_name": "GPT-4-128k (used in experiments as the LLM for ReadAgent)",
            "model_description": "Closed-source GPT-4 API, used to generate gist memories and to read selected pages with a large context.",
            "task_name": "Long-context QA benchmarks (same as GraphReader comparisons: HotpotQA, 2WikiMultihopQA, MuSiQue, NarrativeQA, HotpotWikiQA-mixup)",
            "task_description": "Answering questions by retrieving and reading from gist memory pages derived from very long documents.",
            "task_type": "question answering / document retrieval + reading",
            "memory_used": true,
            "memory_type": "gist memory directory (summarized pages), i.e., an external retrieval-style memory",
            "memory_mechanism": "Condense pages into gist summaries stored in a memory directory; agent looks up summaries to select pages to read; reads pages sequentially (ReadAgent-S) and may maintain notes during reading.",
            "memory_representation": "Gist summaries (page-level), selected original pages (text chunks) and any notes produced during sequential reading.",
            "memory_retrieval_method": "Lookup in gist memory by relevance, then sequential page reading (allowed to read 1–5 pages depending on settings); selection heuristics and LLM planning guide retrieval.",
            "performance_with_memory": "Table 2 (ReadAgent, 128k input window): HotpotQA LR-1 72.3%, LR-2 78.7%, EM 48.0%, F1 62.0%; 2WikiMultihopQA LR-1 79.0%, LR-2 81.0%, EM 52.7%, F1 63.7%; MuSiQue LR-1 54.5%, LR-2 61.0%, EM 35.0%, F1 45.1%; NarrativeQA LR-1 63.0%, LR-2 75.5%, EM 5.0%, F1 18.9%. Token cost (HotpotWikiQA-mixup-256k): avg. cost 48.7k tokens (Table 5).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared directly with GraphReader: ReadAgent underperforms, especially on extremely long contexts and multi-hop tasks. Paper argues ReadAgent's page-level gist memory lacks detailed content per page, making page selection harder and reducing recall as context length increases.",
            "key_findings": "Gist-memory approaches (ReadAgent) are effective but lose detailed information needed for multi-hop reasoning in very long contexts; GraphReader's finer-grained atomic facts + graph structure yields higher recall and better multi-hop performance.",
            "limitations_or_challenges": "Page-level gist summarization can omit details, leading to poor page selection for multi-hop questions; performance degrades more than GraphReader as context length increases.",
            "uuid": "e8450.1",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RAG (BM25 / Ada-002)",
            "name_full": "Retrieval-Augmented Generation baselines (Okapi BM25 and OpenAI Ada-002 embeddings)",
            "brief_description": "Standard retrieval-augmented baselines that retrieve top-k chunks using BM25 or vector embeddings (Ada-002), then pass those chunks to an LLM to answer questions.",
            "citation_title": "The probabilistic relevance framework: BM25 and beyond",
            "mention_or_use": "use",
            "agent_name": "RAG (BM25 / Ada-002 retrieval pipelines)",
            "agent_description": "Retrieval-augmented pipeline: split document into chunks, retrieve top-k chunks by BM25 or embedding similarity (Ada-002), feed retrieved chunks and question into GPT-4 for answer generation under a 4k input window.",
            "model_name": "GPT-4-128k (reader), BM25 (Okapi) and text-embedding-ada-002 (retrieval embedding)",
            "model_description": "BM25 is a classical lexical retriever; text-embedding-ada-002 is OpenAI's embedding model used for semantic retrieval; GPT-4 used as reader over retrieved chunks.",
            "task_name": "Long-context QA benchmarks (HotpotQA, 2WikiMultihopQA, MuSiQue, NarrativeQA)",
            "task_description": "Answering questions by retrieving top relevant chunks and letting an LLM (GPT-4) read them within a limited input window.",
            "task_type": "question answering / retrieval-augmented generation",
            "memory_used": true,
            "memory_type": "external retrieval memory (document chunks stored externally and retrieved at query time)",
            "memory_mechanism": "Index all chunks and retrieve top-k chunks by BM25 or embedding similarity at query time; pass retrieved chunks to the LLM.",
            "memory_representation": "Original text chunks (retrieved passages) and any retrieved chunk summaries",
            "memory_retrieval_method": "Lexical retrieval (BM25) or semantic retrieval via embedding similarity (Ada-002) selecting top-1 or top-3 chunks depending on setup.",
            "performance_with_memory": "Table 2 (examples): BM25 (top-3, 4k) HotpotQA LR-1 74.7%, LR-2 78.3%, EM 45.7%, F1 58.5%; Ada-002 (top-3, 4k) HotpotQA LR-1 72.0%, LR-2 77.3%, EM 45.0%, F1 58.1%. Generally RAG methods show the worst performance among compared approaches.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "RAG methods perform poorly compared to direct full-text read by GPT-4-128k and agent approaches (GraphReader), likely due to retrieval failing to recall all supporting chunks needed for multi-hop answers; increasing number of retrieved chunks is limited by input window.",
            "key_findings": "Simple retrieval (BM25/embedding) struggles on complex multi-hop long-context QA because it is hard to retrieve all supporting facts within the context window; GraphReader's structured graph+agent approach yields better recall and QA performance.",
            "limitations_or_challenges": "Retrieval recall limits performance; tradeoff between number/size of retrieved chunks and context window; retrieval may miss decoy-resistant multi-hop evidence.",
            "uuid": "e8450.2",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "PEARL",
            "name_full": "PEARL (Prompting large language models to plan and execute actions over long documents)",
            "brief_description": "A prompting framework that decomposes complex questions into actions via action-mining, plan formulation, and execution to improve reasoning over long documents.",
            "citation_title": "PEARL: Prompting large language models to plan and execute actions over long documents",
            "mention_or_use": "use",
            "agent_name": "PEARL",
            "agent_description": "Framework that mines actions from a document and uses planning/execution prompts to guide LLMs to perform multi-step reasoning over long documents; included as an agent-style baseline.",
            "model_name": "GPT-4-128k (used as baseline LLM in experiments)",
            "model_description": "Closed-source GPT-4 API used to perform PEARL prompting pipeline.",
            "task_name": "Long-context QA benchmarks (same benchmarks as others)",
            "task_description": "Planning and executing decomposed actions over long documents to answer questions.",
            "task_type": "question answering / plan-and-execute agent",
            "memory_used": true,
            "memory_type": "implicit plan/execution memory (action sequences and intermediate results stored across steps)",
            "memory_mechanism": "Prompted decomposition into steps (actions) and executing them sequentially, with intermediate outputs used as context for subsequent steps.",
            "memory_representation": "Intermediate step outputs and action plans (textual), possibly stored as notes during execution.",
            "memory_retrieval_method": "Prompt concatenation of prior step outputs; no explicit external graph used in the PEARL baseline presented.",
            "performance_with_memory": "Table 2 (PEARL, 128k): HotpotQA LR-1 74.7%, LR-2 79.0%, EM 46.3%, F1 60.4%; 2WikiMultihopQA LR-1 70.0%, LR-2 71.0%, EM 46.0%, F1 57.6%; MuSiQue LR-1 45.0%, LR-2 51.5%, EM 23.0%, F1 33.3%; NarrativeQA LR-1 43.5%, LR-2 48.0%, EM 7.5%, F1 16.2%.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "PEARL is compared as an agent-style baseline; GraphReader outperforms PEARL on the evaluated datasets, indicating structured graph + notebook yields better retrieval/recall for multi-hop evidence than PEARL's plan/execution alone.",
            "key_findings": "Prompted planning & execution helps, but GraphReader's explicit document graph + notebook retains more actionable facts for multi-hop reasoning across very long contexts.",
            "limitations_or_challenges": "PEARL may not effectively capture long-range multi-hop dependencies without a structured external index like GraphReader's graph.",
            "uuid": "e8450.3",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GraphRAG",
            "name_full": "GraphRAG (graph-based RAG variant)",
            "brief_description": "A retrieval-augmented baseline that uses LLMs to construct graph-based text indices (entity knowledge graph + summaries) to improve retrieval and answer composition.",
            "citation_title": "From local to global: A graph rag approach to query-focused summarization",
            "mention_or_use": "use",
            "agent_name": "GraphRAG",
            "agent_description": "Graph-based retrieval pipeline that first builds an entity knowledge graph and group summaries, then composes partial information from summaries to answer queries.",
            "model_name": "GPT-4-128k (used as reader in experiments)",
            "model_description": "LLM used to construct graph index and to read summaries.",
            "task_name": "Long-context QA benchmarks",
            "task_description": "Query-focused summarization and QA using graph-structured retrieval summaries.",
            "task_type": "retrieval-augmented question answering with graph index",
            "memory_used": true,
            "memory_type": "graph-based index (document summaries / entity graph)",
            "memory_mechanism": "LLM constructs an entity knowledge graph and summary groups, which are used as retrieval units for answering queries.",
            "memory_representation": "Entity nodes and group summaries derived from documents.",
            "memory_retrieval_method": "Retrieve relevant summary nodes from the graph index to provide partial evidence for answer composition.",
            "performance_with_memory": "Table 2 (GraphRAG, 128k): HotpotQA LR-1 73.7%, LR-2 80.3%, EM 49.7%, F1 59.7%; 2WikiMultihopQA LR-1 67.7%, LR-2 71.3%, EM 42.3%, F1 53.9%; MuSiQue LR-1 46.5%, LR-2 56.0%, EM 21.5%, F1 31.2%; NarrativeQA LR-1 52.0%, LR-2 66.5%, EM 15.0%, F1 23.1%.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared with GraphReader: GraphRAG uses summaries as retrieval units and underperforms GraphReader on multi-hop benchmarks, suggesting that GraphReader's atomic-fact granularity + agent exploration captures more precise multi-hop evidence.",
            "key_findings": "Graph-based retrieval indices help but the fidelity/granularity of stored summaries matters for multi-hop QA; GraphReader's atomic-facts + agentic exploration improved recall and QA accuracy.",
            "limitations_or_challenges": "Summaries as retrieval units can miss fine-grained facts required for multi-hop reasoning.",
            "uuid": "e8450.4",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LongRAG",
            "name_full": "LongRAG (long retriever + long reader)",
            "brief_description": "A retrieval-augmented approach that processes corpora into larger units ('long retriever' and 'long reader') to reduce retrieval burden and improve long-context performance.",
            "citation_title": "LongRAG: Enhancing retrieval-augmented generation with long-context llms",
            "mention_or_use": "use",
            "agent_name": "LongRAG",
            "agent_description": "Method that preprocesses corpus into larger-sized units and employs specialized retrieval/reading components to handle long contexts more efficiently.",
            "model_name": "GPT-4-128k (reader)",
            "model_description": "Used as the long reader in experiments.",
            "task_name": "Long-context QA benchmarks",
            "task_description": "Answering questions by retrieving larger document units and reading them with long-context LLMs.",
            "task_type": "retrieval-augmented question answering",
            "memory_used": true,
            "memory_type": "retrieved large chunks (long units) as external memory",
            "memory_mechanism": "Aggregate documents into large retrieval units to reduce retrieval count; use long-context reader to read the units.",
            "memory_representation": "Large aggregated text units (long chunks).",
            "memory_retrieval_method": "Retrieval over aggregated units (long retriever) followed by long-reader processing.",
            "performance_with_memory": "Table 2 (LongRAG, 128k): HotpotQA LR-1 75.7%, LR-2 78.3%, EM 48.7%, F1 63.9%; 2WikiMultihopQA LR-1 73.0%, LR-2 75.0%, EM 51.3%, F1 63.5%; MuSiQue LR-1 49.0%, LR-2 54.5%, EM 31.0%, F1 40.3%; NarrativeQA LR-1 60.5%, LR-2 69.0%, EM 15.0%, F1 27.0%.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared with GraphReader: LongRAG improves retrieval efficiency but GraphReader shows superior multi-hop reasoning and recall via atomic-fact graph + agent exploration.",
            "key_findings": "Aggregating into large retrieval units helps but cannot fully replace fine-grained structured memory plus agentic selective access for complex multi-hop QA.",
            "limitations_or_challenges": "Tradeoffs in unit size: too large units reduce retrieval resolution; too small units increase retrieval overhead and context-window pressure.",
            "uuid": "e8450.5",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MemWalker",
            "name_full": "MemWalker (walking down the memory maze)",
            "brief_description": "A referenced prior method that organizes documents into a tree and simulates an agent walking/searching memory to go beyond context limits.",
            "citation_title": "Walking down the memory maze: Beyond context limit through interactive reading",
            "mention_or_use": "mention",
            "agent_name": "MemWalker",
            "agent_description": "Prior work that organizes documents into a tree and uses agent-like exploration to traverse memory beyond the LLM's native context window.",
            "model_name": "",
            "model_description": "",
            "task_name": "Long-context reading / QA (prior work)",
            "task_description": "Exploration of tree-structured memory to recover long-context information.",
            "task_type": "document exploration / QA",
            "memory_used": true,
            "memory_type": "tree-structured memory",
            "memory_mechanism": "Interactive walking on a document tree, using LLM-guided traversal to access relevant parts of the corpus.",
            "memory_representation": "Tree nodes containing summaries or passages.",
            "memory_retrieval_method": "Agent traversal of tree nodes guided by LLM decisions.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned in related work as a prior agent-memory approach; GraphReader differs by using a graph (not tree) and richer atomized facts to better capture multi-hop dependencies.",
            "key_findings": "Serves as a reference point showing interactive memory traversal; GraphReader extends these ideas using graph structure and a notebook.",
            "limitations_or_challenges": "Tree structure may limit capturing multi-hop or long-range relationships present in graphs.",
            "uuid": "e8450.6",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "KGP",
            "name_full": "KGP (Knowledge Graph Prompting for Multi-Document QA)",
            "brief_description": "Referenced prior work that organizes documents into a knowledge graph and uses LLMs to generate queries over the graph; cited as related but not using agent planning/reflection in the same way as GraphReader.",
            "citation_title": "Knowledge graph prompting for multi-document question answering",
            "mention_or_use": "mention",
            "agent_name": "KGP",
            "agent_description": "Constructs a knowledge graph from documents and uses LLMs to generate queries over the graph for multi-document QA.",
            "model_name": "",
            "model_description": "",
            "task_name": "Multi-document question answering",
            "task_description": "Use of knowledge graph prompts to guide multi-document QA.",
            "task_type": "question answering with knowledge graph retrieval",
            "memory_used": true,
            "memory_type": "knowledge graph (document-derived)",
            "memory_mechanism": "Graph index of entities and relations used to guide question answering; LLMs generate queries over graph rather than full agentic planning/exploration.",
            "memory_representation": "Entities and relations; graph nodes representing document entities.",
            "memory_retrieval_method": "Graph-based lookup / query generation.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as related work; paper contrasts KGP's approach of using agent to generate queries with GraphReader's agentic planning + reflection and direct exploitation of graph structure.",
            "key_findings": "Shows prior usage of graphs for multi-document QA; GraphReader leverages agentic exploration directly over the graph for better planning and reflection.",
            "limitations_or_challenges": "KGP primarily uses agent to generate queries, not fully leveraging agent planning/reflection for exploratory access.",
            "uuid": "e8450.7",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "WebGPT",
            "name_full": "WebGPT (browser-assisted QA with human feedback)",
            "brief_description": "Prior system that simulates human actions (searching the web) with an agent to answer questions, cited as an example of agents retrieving unstructured information.",
            "citation_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "mention_or_use": "mention",
            "agent_name": "WebGPT",
            "agent_description": "Agent that performs simulated web actions (searches, clicks) to retrieve information and composes answers, supervised with human feedback.",
            "model_name": "",
            "model_description": "",
            "task_name": "Web-based question answering / retrieval",
            "task_description": "Agentic retrieval from web and synthesis of answers with human-feedback-trained policy.",
            "task_type": "web retrieval + QA",
            "memory_used": true,
            "memory_type": "external web as memory (retrieved pages and browsing history)",
            "memory_mechanism": "Agentic browsing actions store retrieved pages and use them to compose answers.",
            "memory_representation": "Retrieved web pages and browsing traces.",
            "memory_retrieval_method": "Search queries and click actions to obtain pages then read/aggregate.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned in related work as an agent retrieving unstructured external information; GraphReader differs by building a structured document graph and using a notebook to accumulate supporting facts.",
            "key_findings": "Exemplifies agentic retrieval from external sources; motivates agent-based methods for long-context retrieval.",
            "limitations_or_challenges": "Web-based retrieval faces its own noise and retrieval-selection tradeoffs; GraphReader targets internal long-document retrieval challenges.",
            "uuid": "e8450.8",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A human-inspired reading agent with gist memory of very long contexts",
            "rating": 2
        },
        {
            "paper_title": "PEARL: Prompting large language models to plan and execute actions over long documents",
            "rating": 2
        },
        {
            "paper_title": "Walking down the memory maze: Beyond context limit through interactive reading",
            "rating": 2
        },
        {
            "paper_title": "From local to global: A graph rag approach to query-focused summarization",
            "rating": 2
        },
        {
            "paper_title": "LongRAG: Enhancing retrieval-augmented generation with long-context llms",
            "rating": 2
        },
        {
            "paper_title": "Knowledge graph prompting for multi-document question answering",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 1
        },
        {
            "paper_title": "The probabilistic relevance framework: BM25 and beyond",
            "rating": 1
        }
    ],
    "cost": 0.02244225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models</h1>
<p>Shilong Li^{∗1}, Yancheng He^{∗1}, Hangyu Guo^{∗1}, Xingyuan Bu^{∗†‡1}, Ge Bai^{1}, Jie Liu^{2,3},
Jiaheng Liu^{1}, Xingwei Qu^{4}, Yangguang Li^{3}, Wanli Ouyang^{2,3}, Wenbo Su^{1}, Bo Zheng^{1}
^{1}Alibaba Group ^{2}The Chinese University of Hong Kong
^{3}Shanghai AI Laboratory ^{4}University of Manchester
zhuli.lsl@taobao.com, xingyuanbu@gmail.com</p>
<h6>Abstract</h6>
<p>Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have made great progress on natural language understanding and generation (Zhao et al., 2023; Liu et al., 2024a; Feng et al., 2022; Peng et al., 2020; Xv et al., 2022; Peng et al., 2023b; Bu et al., 2021). However, transformer-based LLMs still struggle in handling long contexts due to the limitation of context window and memory usage.</p>
<p>Current techniques for solving the long-context tasks of LLMs can be divided into two perspectives: 1) Model-level, which includes finetuning with modified positional embeddings (Chen et al.,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Performance on LV-Eval at 5 context length levels. GraphReader outperforms existing open-sourced and closed-source models while demonstrating a scalable performance in very long contexts. In contrast, other models exhibit a significant decrease in performance as context length increases.</p>
<p>2023b; Zhu et al., 2023; Peng et al., 2023a; Ding et al., 2024), and applying transformer variants with modified attention mechanisms (Dai et al., 2019; Munkhdalai et al., 2024; Gu and Dao, 2023); 2) Agent-level, i.e., employing retrieval-augmented LLM or agent to process long contexts with a limited context window LLM (Nakano et al., 2021; Lee et al., 2024).</p>
<p>However, model-level methods typically train LLMs with target length texts, posing challenges in constructing training datasets and incurring high training costs (Zhu et al., 2023). Additionally, long-context LLMs optimized with these methods tend to overlook crucial details in long contexts, known as "lost in the middle" (Liu et al., 2024b), limiting their ability to address complex tasks, such as multi-hop questions. Agent-level approaches transform input text into a tree (Chen et al., 2023a) or paginated pages (Lee et al., 2024), failing to capture multi-hop and long-range dependencies, thus</p>
<p>limiting their effectiveness on very long contexts, as shown in Figure 1.</p>
<p>To address these issues, we propose a graphbased agent named GraphReader. As illustrated in Figure 2, GraphReader first segments long texts into discrete chunks, extracts essential information, and compresses these into key elements and atomic facts. These key elements and facts are then used to construct a graph with nodes representing key elements and their associated atomic facts. This graph structure effectively captures long-range dependencies and multi-hop relationships within long text. Subsequently, GraphReader autonomously explores this graph using predefined functions, guided by a step-by-step rational plan. Based on a given question, the agent progressively accesses information from coarse key elements and atomic facts to detailed original text chunks, taking notes and reflecting until it gathers sufficient information to generate an answer. In summary, our main contributions are threefold:</p>
<ul>
<li>We introduce GraphReader, a novel agent system designed to organize long texts into a graph structure, leveraging predefined functions and notebook to facilitate planning and reflection during exploration.</li>
<li>GraphReader establishes a scalable long-context capability based on a 4 k context window, demonstrating performance that is comparable to or surpasses GPT-4 with a 128 k context window across varying context lengths.</li>
<li>Extensive experiments conducted on four challenging benchmarks demonstrate that GraphReader achieves superior performance in complex single-hop and multi-hop QA tasks.</li>
</ul>
<h2>2 Related Work</h2>
<p>Long-Context LLMs Recent efforts (Chen et al., 2023b; Ding et al., 2024; Peng et al., 2023a) have focused on positional interpolation (PI) to enhance long-context capabilities. However, these methods require training on full-length texts, leading to significant increases in data and training costs (Chen et al., 2023c; Fu et al., 2024; Bai et al., 2024b). Thus, PoSE (Zhu et al., 2023) and SkipAlign (Wu et al., 2024a) investigate data skip strategy, but tend to neglect detailed information in long texts (Liu et al., 2024b; Bai et al., 2024a; Wu et al., 2024b). Furthermore, despite how extensively the context window is expanded, it remains constrained by a
predefined fixed length. To address these limitations, transformer variants with modified attention mechanisms have been proposed (Dai et al., 2019; Gu and Dao, 2023; Munkhdalai et al., 2024). However, these models are prone to losing earlier information.</p>
<p>Retrieval Retrieval Augmented Generation (RAG) leverages an extensive database of documents to extract task-related information that aids in response generation. Many efforts investigate various levels of retrieval granularity, including tokens (Khandelwal et al., 2019), entities (Févry et al., 2020; De Jong et al., 2021), and chunks (Liu, 2024; LangChain-team, 2024). Other approaches have explored diverse retrieval methods, such as BM25 (Rasooli and Tetreault, 2015) and learning-based strategies (Khattab and Zaharia, 2020; Sachan et al., 2023; Sun et al., 2021). Despite its capabilities, RAG faces challenges in addressing complex questions due to difficulties in developing robust decision-making mechanisms. In contrast, we employ agents that use planning and reflection to gather essential information, effectively tackling complex problems.</p>
<p>Agent for Retrieval Recent work has increasingly leveraged LLMs as agents to tackle complex problems, utilizing their strong planning and reflection abilities (Yao et al., 2022; Park et al., 2023). These abilities have been applied to complex tasks such as function call (Li et al., 2023) and KGQA (Sun et al., 2023; Luo et al., 2023). Agents are also capable of retrieving unstructured information. For example, WebGPT (Nakano et al., 2021) simulates human actions to search on internet for specific answers. Additionally, MemWalker (Chen et al., 2023a) and PEARL (Sarthi et al., 2024) organize documents into a tree structure, while ReadAgent (Lee et al., 2024) condenses documents into a gist memory directory. However, these approaches often struggle with multi-hop questions. KGP (Wang et al., 2024) organizes documents into graphs, but it primarily uses the agent to generate queries, thereby not fully exploiting the agent's capabilities for planning and reflection.</p>
<h2>3 Approach</h2>
<h3>3.1 Preliminary</h3>
<p>GraphReader is built on a graph $\mathcal{G}={\mathcal{V}, \mathcal{E}}$, where each node $v_{i} \in \mathcal{V}$ contains a key element $k_{i}$ and a set of summarized content, namely atomic facts $\mathcal{A}<em i="i">{i}$. In other words, $v</em>\right}$. And}=\left{k_{i}, \mathcal{A}_{i</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The illustration of our GraphReader approach, consisting of graph construction, graph exploration, and answer reasoning.</p>
<p>Each edge $e_{ij} \in \mathcal{E}$ represents the relationship between nodes $v_i$ and $v_j$. This graph structure enables GraphReader to capture global information from the input document $D$ within a limited context window, allowing it to decide whether to explore the current node in detail or jump to a neighboring node. During graph exploration, GraphReader collects supporting facts and terminates the exploration once sufficient information has been gathered to answer the question. As illustrated in Figure 2, the entire process of GraphReader consists of the following three phases: graph construction, graph exploration, and answer reasoning. The prompts utilized in these three stages are detailed in Appendix A, and a detailed example of our process can be found in Appendix I.</p>
<h3>3.2 Graph Construction</h3>
<p>To extract nodes from a document $D$ within the LLM's context limit, we first split $D$ into chunks of maximum length $L$ while preserving paragraph structure. For each chunk, we prompt the LLM to summarize it into atomic facts, the smallest indivisible facts that simplify the original text. We also prompt the LLM to extract key elements from each atomic fact like essential nouns, verbs, and adjectives. After processing all chunks, we normalize the key elements as described by Lu et al. (2023) to handle lexical noise and granularity issues, creating a final set of key elements. We then construct each node $v_i = (k_i, \mathcal{A}_i)$, where $k_i$ is a key element and $\mathcal{A}_i$ is the set of atomic facts corresponding to $k_i$. Finally, we link two nodes $v_i$ and $v_j$ if key element $k_i$ appears in $\mathcal{A}_j$ and vice versa.</p>
<h3>3.3 Graph Exploration</h3>
<h4>3.3.1 Agent Initialization</h4>
<p>Given a graph $\mathcal{G}$ and a question $Q$, our goal is to design an agent that can autonomously explore the graph using predefined functions. The agent begins by maintaining a notebook to record supporting facts, which are eventually used to derive the final answer. Then the agent performs two key initializations: defining the rational plan and selecting the initial node.</p>
<p><strong>Rational Plan</strong> To tackle complex real-world multi-hop questions, pre-planning the solution is</p>
<p>crucial. The agent breaks down the original question step-by-step, identifies the key information needed, and forms a rational plan.</p>
<p>Initial Node Choosing strategic starting points is essential for improving search efficiency. The agent evaluates the key elements of all nodes $\mathcal{V}$ and selects $N$ initial nodes based on the question and the rational plan.</p>
<h3>3.3.2 Exploration</h3>
<p>After selecting $N$ initial nodes as starting points, an agent explores each initial node by first exploring atomic facts, then chunks of the node. Next, it explores neighboring nodes, guided by the question and rational plan. The agent continuously updates the notebook with relevant information during the exploration process.</p>
<p>Exploring Atomic Facts It is impractical to include all original text chunks related to a node within the context window. Therefore, the agent employs a coarse-to-fine strategy, progressing from reading atomic facts to the original text, as all atomic facts can fit within the context window. Initially, all atomic facts associated with a node are grouped by their corresponding chunks, labeled with the respective chunk IDs, and fed to the agent. This allows the agent to capture an overview of each chunk by reading all groups of atomic facts. Meanwhile, the agent utilizes the question, rational plan, and notes in its notebook to reflect on the required clues and determine which chunk is likely to contain useful information. Subsequently, the agent is provided with two functions: 1) read_chunk, if the agent identifies certain chunks as valuable for further reading, it will complete the function parameters with the chunk IDs, i.e., read_chunk(List[ID]), and append these IDs to a chunk queue. 2) stop_and_read_neighbor, conversely, if the agent deems that none of the chunks are worth further reading, it will finish reading this node and proceed to explore neighboring nodes.</p>
<p>Exploring Chunks When the chunk queue is non-empty, it indicates that the agent has identified multiple text chunks of interest. We then traverse the queue, reading each chunk. This step is essential because atomic facts merely summarize key information and provide brief clues, whereas specific details are best obtained directly from the original text chunks. While reading the chunks, the agent will once again consider the question and the plan, thinking about what can be added to the current notebook. Any supporting facts discovered will be recorded in the notebook. Depending on the updated notebook, the agent will then select one of the following four functions: 1) search_more, if supporting fact is insufficient, the agent will continue exploring chunks in the queue; 2) read_previous_chunk and 3)read_subsequent_chunk, due to truncation issues, adjacent chunks might contain relevant and useful information, the agent may insert these IDs to the queue; 4) termination, if sufficient information has been gathered for answering the question, the agent will finish exploration.</p>
<p>Exploring Neighbors Once the atomic facts and chunk queue of the current node have been fully processed, it indicates that this node has been thoroughly explored, and the agent needs to access the next node. Taking into account the question, rational plan, and the content of the notebook, the agent checks all neighboring nodes, i.e., key elements, and performs one of two functions: 1) read_neighbor_node, the agent selects a neighboring node that might be helpful in answering the question and re-enters the process of exploring atomic facts and chunks; 2) termination, the agent determines that none of the neighboring nodes contain useful information, it finish the exploration.</p>
<h3>3.4 Answer Reasoning</h3>
<p>After $N$ agents have independently gathered information and stopped their exploration, we will compile all notes from each agent for reasoning and generating the final answer. Employing Chain-of-Thought (Wei et al., 2022), the LLM first analyzes each note by considering complementary information from other memories and using a majority voting strategy to resolve any inconsistencies. Ultimately, the LLM will consider all the available information to generate the final answer.</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Settings</h3>
<p>Evaluation Benchmarks We conduct experiments on two types of long-context QA benchmarks, including multi-hop long-context QA, i.e., HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022), and a single-hop long-context QA benchmark, i.e., NarrativeQA (Kociský et al., 2018) from LongBench (Bai et al., 2023). Additionally, we</p>
<p>also incorporate HotpotWikiQA-mixup from LVEval (Yuan et al., 2024), a multi-hop benchmark that features five levels of text length: $16 \mathrm{k}, 32 \mathrm{k}$, $64 \mathrm{k}, 128 \mathrm{k}$, and 256 k . Table 1 presents the statistics about these benchmarks, and detailed information is provided in Appendix C.</p>
<p>Evaluation Metrics We employ several automatic evaluation metrics, i.e., $F_{1}$ score, Exact Match (EM) score, and an optimized $F_{1}{ }^{<em>}$ score, as introduced by LV-Eval (Yuan et al., 2024). Specifically, $F_{1}{ }^{</em>}$ first computes the recall of golden answer keywords and only calculates the $F_{1}$ score if it exceeds a certain threshold. Otherwise, the score defaults to zero. Despite the cost-effectiveness of automatic metrics, their accuracy may be affected by the response format. Hence, we implement LLM Raters for answer correctness evaluation using an LLM, denoted as LLM-Rating-1 (LR-1) and LLM-Rating-1 (LR-2), following ReadAgent (Lee et al., 2024). Details on the evaluation metrics can be found in Appendix B.</p>
<p>Baseline Methods We compare our approach with the following baselines: retrieval augmented generation (RAG), long-context LLM, and agentbased methods. (1) RAG: We choose Okapi BM25 (Robertson and Zaragoza, 2009) or OpenAI API embedding model Ada-002 to retrieve the chunks most relevant to the question and employ GPT-4-128k (gpt-4-1106-preview) to read retrieved chunks and answer the question. In addition to traditional RAG methods, we also compared GraphRAG (Edge et al., 2024) and LongRAG (Jiang et al., 2024), which utilize LLM to enhance RAG ability. (2) Long-context LLM: We select GPT-4-128k for directly reading full text when the text content fits within the input window, or for segmenting the text into chunks for sequential reading. (3) Agent-based Method: We select ReadAgent (Lee et al., 2024) and PEARL (Sun et al., 2024), which employ an agent-based system for the execution of retrieval and reading processes for long-context QA. The detailed description of these methods is provided in Appendix D.</p>
<p>Implementation Details In our experiments, we employ GPT-4-128k for both our method and baseline approaches, setting the temperature to 0.2 . For GraphReader, the input window size is configured to 4 k tokens unless stated otherwise. We limit the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: The statistics of benchmarks employed in our evaluation. The token number is calculated using the GPT-4 tokenizer from the TikToken. #Samples denote the total number of benchmarks.
maximum chunk size to 2 k tokens, initiate searches from 5 initial nodes, and impose a function call limit of 10 for each search path.</p>
<h3>4.2 Main Results</h3>
<p>The results of three types of methods on four multihop long-context benchmarks and one single-hop long-context benchmark are shown in Table 2 and Table 3. Based on the results, we have the following findings:</p>
<p>Results of RAG methods As the results shown in Table 2, RAG methods based on BM25 and Ada002 exhibit the worst performance in comparison to long-context LLM and agent-based methods. A possible reason is that text retrieval has difficulty recalling all chunks that contain the supporting facts for answering the input question. Although increasing the number of recalled chunks could improve the performance of text retrieval, the context window will limit the effectiveness of these RAG methods.</p>
<p>Results of Long-Context LLMs From the results shown in Table 2, we can see that employing GPT-4-128k to directly answer the question with long contexts significantly outperforms RAG methods and even outperforms ReadAgent on three long-context benchmarks. This is because of the superior performance of GPT-4-128k in processing long texts and executing multi-hop reasoning tasks. Additionally, the lengths of these four benchmarks are significantly shorter than the 128 k context window, thereby mitigating the impact of "lost in the middle" on the model's performance.</p>
<p>Results of Agent-based Methods By comparing our approach with all baselines in Table 2, it is obvious that our approach consistently performs better than them on four long-context benchmarks and demonstrates superior performance in multihop long-context tasks. In our approach, benefiting</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Input</th>
<th>HotpotQA</th>
<th></th>
<th></th>
<th></th>
<th>2WikiMultihopQA</th>
<th></th>
<th></th>
<th></th>
<th>MuSiQue</th>
<th></th>
<th></th>
<th></th>
<th>NarrativeQA</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Window</td>
<td>LR-1</td>
<td>LR-2</td>
<td>EM</td>
<td>$F_{1}$</td>
<td>LR-1</td>
<td>LR-2</td>
<td>EM</td>
<td>$F_{1}$</td>
<td>LR-1</td>
<td>LR-2</td>
<td>EM</td>
<td>$F_{1}$</td>
<td>LR-1</td>
<td>LR-2</td>
<td>EM</td>
<td>$F_{1}$</td>
</tr>
<tr>
<td>BM25 (top-1)</td>
<td>$4 k$</td>
<td>57.7</td>
<td>63.0</td>
<td>33.7</td>
<td>43.8</td>
<td>36.0</td>
<td>39.0</td>
<td>25.0</td>
<td>30.4</td>
<td>33.0</td>
<td>36.5</td>
<td>19.0</td>
<td>23.9</td>
<td>29.5</td>
<td>34.5</td>
<td>4.0</td>
<td>11.3</td>
</tr>
<tr>
<td>BM25 (top-3)</td>
<td>$4 k$</td>
<td>74.7</td>
<td>78.3</td>
<td>45.7</td>
<td>58.5</td>
<td>59.7</td>
<td>62.0</td>
<td>42.3</td>
<td>51.9</td>
<td>43.5</td>
<td>49.5</td>
<td>25.0</td>
<td>31.1</td>
<td>44.5</td>
<td>52.5</td>
<td>7.0</td>
<td>20.5</td>
</tr>
<tr>
<td>Ada-002 (top-1)</td>
<td>$4 k$</td>
<td>63.0</td>
<td>70.7</td>
<td>40.0</td>
<td>53.2</td>
<td>57.0</td>
<td>59.3</td>
<td>41.0</td>
<td>49.4</td>
<td>34.5</td>
<td>37.0</td>
<td>20.0</td>
<td>26.6</td>
<td>37.5</td>
<td>46.5</td>
<td>5.0</td>
<td>15.5</td>
</tr>
<tr>
<td>Ada-002 (top-3)</td>
<td>$4 k$</td>
<td>72.0</td>
<td>77.3</td>
<td>45.0</td>
<td>58.1</td>
<td>65.7</td>
<td>66.7</td>
<td>44.7</td>
<td>55.3</td>
<td>40.0</td>
<td>45.5</td>
<td>24.5</td>
<td>32.1</td>
<td>45.5</td>
<td>53.0</td>
<td>7.5</td>
<td>19.5</td>
</tr>
<tr>
<td>GPT-4-128k</td>
<td>128k</td>
<td>83.3</td>
<td>88.3</td>
<td>53.0</td>
<td>68.4</td>
<td>77.3</td>
<td>80.0</td>
<td>58.7</td>
<td>70.0</td>
<td>52.0</td>
<td>59.5</td>
<td>33.5</td>
<td>42.7</td>
<td>63.5</td>
<td>77.0</td>
<td>11.5</td>
<td>29.4</td>
</tr>
<tr>
<td>GPT-4-128k (chunk)</td>
<td>$4 k$</td>
<td>71.3</td>
<td>74.7</td>
<td>45.7</td>
<td>59.5</td>
<td>59.3</td>
<td>62.3</td>
<td>40.7</td>
<td>50.5</td>
<td>41.0</td>
<td>43.0</td>
<td>23.0</td>
<td>32.1</td>
<td>58.0</td>
<td>69.5</td>
<td>9.50</td>
<td>25.5</td>
</tr>
<tr>
<td>GPT-4-128k (chunk w/ notes)</td>
<td>$4 k$</td>
<td>72.3</td>
<td>76.7</td>
<td>45.7</td>
<td>59.5</td>
<td>65.7</td>
<td>68.7</td>
<td>46.3</td>
<td>56.6</td>
<td>39.5</td>
<td>43.0</td>
<td>25.0</td>
<td>32.5</td>
<td>56.5</td>
<td>65.0</td>
<td>8.5</td>
<td>24.3</td>
</tr>
<tr>
<td>ReadAgent</td>
<td>128k</td>
<td>72.3</td>
<td>78.7</td>
<td>48.0</td>
<td>62.0</td>
<td>79.0</td>
<td>81.0</td>
<td>52.7</td>
<td>63.7</td>
<td>54.5</td>
<td>61.0</td>
<td>35.0</td>
<td>45.1</td>
<td>63.0</td>
<td>75.5</td>
<td>5.0</td>
<td>18.9</td>
</tr>
<tr>
<td>Pearl</td>
<td>128k</td>
<td>74.7</td>
<td>79.0</td>
<td>46.3</td>
<td>60.4</td>
<td>70.0</td>
<td>71.0</td>
<td>46.0</td>
<td>57.6</td>
<td>45.0</td>
<td>51.5</td>
<td>23.0</td>
<td>33.3</td>
<td>43.5</td>
<td>48.0</td>
<td>7.5</td>
<td>16.2</td>
</tr>
<tr>
<td>LongRAG</td>
<td>128k</td>
<td>75.7</td>
<td>78.3</td>
<td>48.7</td>
<td>63.9</td>
<td>73.0</td>
<td>75.0</td>
<td>51.3</td>
<td>63.5</td>
<td>49.0</td>
<td>54.5</td>
<td>31.0</td>
<td>40.3</td>
<td>60.5</td>
<td>69.0</td>
<td>15.0</td>
<td>27.0</td>
</tr>
<tr>
<td>GraphRAG</td>
<td>128k</td>
<td>73.7</td>
<td>80.3</td>
<td>49.7</td>
<td>59.7</td>
<td>67.7</td>
<td>71.3</td>
<td>42.3</td>
<td>53.9</td>
<td>46.5</td>
<td>56.0</td>
<td>21.5</td>
<td>31.2</td>
<td>52.0</td>
<td>66.5</td>
<td>15.0</td>
<td>23.1</td>
</tr>
<tr>
<td>GraphReader</td>
<td>$4 k$</td>
<td>84.3</td>
<td>89.7</td>
<td>55.0</td>
<td>70.0</td>
<td>83.7</td>
<td>87.0</td>
<td>59.3</td>
<td>70.1</td>
<td>59.0</td>
<td>63.5</td>
<td>38.0</td>
<td>47.4</td>
<td>65.0</td>
<td>80.0</td>
<td>15.5</td>
<td>29.8</td>
</tr>
<tr>
<td>Golden</td>
<td>$4 k$</td>
<td>92.3</td>
<td>93.7</td>
<td>57.0</td>
<td>73.8</td>
<td>88.3</td>
<td>89.7</td>
<td>63.0</td>
<td>73.4</td>
<td>66.0</td>
<td>69.0</td>
<td>45.0</td>
<td>56.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance (\%) comparison of different baselines on datasets from LongBench. The best performance and the second-best performance are denoted in bold and underlined fonts, respectively. "Golden" denotes the settings in which we add question and its supporting facts to LLM directly.</p>
<p>| Method | Input <br> Window | HotpotWikiQA-mixup | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | LR-1 | LR-2 | $F_{1}{ }^{<em>}$ | LR-1 | LR-2 | $F_{1}{ }^{</em>}$ | LR-1 | LR-2 | $F_{1}{ }^{<em>}$ | LR-1 | LR-2 | $F_{1}{ }^{</em>}$ | LR-1 | LR-2 | $F_{1}{ }^{<em>}$ | LR-1 | LR-2 | $F_{1}{ }^{</em>}$ | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Method</th>
<th>Results(%)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>LR-1</td>
<td>LR-2</td>
<td>$F_{1}$</td>
</tr>
<tr>
<td>HotpotQA</td>
<td>GraphReader</td>
<td>84.3</td>
<td>89.7</td>
<td>70.0</td>
</tr>
<tr>
<td></td>
<td>w/o Rational Plan</td>
<td>81.7</td>
<td>87.7</td>
<td>63.8</td>
</tr>
<tr>
<td></td>
<td>w/o Node Selection</td>
<td>66.0</td>
<td>71.7</td>
<td>54.1</td>
</tr>
<tr>
<td>2WikiMultihopQA</td>
<td>GraphReader</td>
<td>83.7</td>
<td>87.0</td>
<td>70.1</td>
</tr>
<tr>
<td></td>
<td>w/o Rational Plan</td>
<td>81.3</td>
<td>86.0</td>
<td>65.4</td>
</tr>
<tr>
<td></td>
<td>w/o Node Selection</td>
<td>65.3</td>
<td>68.7</td>
<td>49.7</td>
</tr>
<tr>
<td>MuSiQue</td>
<td>GraphReader</td>
<td>59.0</td>
<td>63.5</td>
<td>47.4</td>
</tr>
<tr>
<td></td>
<td>w/o Rational Plan</td>
<td>56.0</td>
<td>61.0</td>
<td>42.4</td>
</tr>
<tr>
<td></td>
<td>w/o Node Selection</td>
<td>35.0</td>
<td>38.5</td>
<td>25.2</td>
</tr>
<tr>
<td>NarrativeQA</td>
<td>GraphReader</td>
<td>65.0</td>
<td>80.0</td>
<td>29.8</td>
</tr>
<tr>
<td></td>
<td>w/o Rational Plan</td>
<td>63.0</td>
<td>78.5</td>
<td>26.6</td>
</tr>
<tr>
<td></td>
<td>w/o Node Selection</td>
<td>53.0</td>
<td>65.5</td>
<td>24.0</td>
</tr>
</tbody>
</table>
<p>Table 4: The results of our ablation study. "w/o Rational Plan" refers to removing the rational plan in the agent initialization stage, and "w/o Node Selection" denotes applying the random selection of initial nodes and neighbor nodes in graph exploration.
pact of extremely long context on our GraphReader. As shown in Table 3, compared with all baselines, our GraphReader not only consistently outperforms these methods across text lengths ranging from 16 k to 256 k tokens but also exhibits robustness with the expansion of context length. It indicates that our method is still effective in handling extremely long texts by graph exploration with limited context window LLMs. With the increase in the length of the input context, the performance of GPT-4-128k full-text reading degrades gradually. As a comparison, our method achieves a performance gain of $10.53 \%$ relatively on LR-1 over GPT-4-128k full-text reading under 16k context length. With the context length increasing to 128 k , our method achieves a performance gain of $75.00 \%$ relatively over GPT-4-128k. This can be attributed to the fact that as the context length increases, the impact of the "lost in the middle" effect on GPT-4-128k becomes progressively more severe. Secondly, we observe that ReadAgent significantly underperforms our method in handling extremely long contexts. This is because the lack of detailed information about the content of each page can make page selection very difficult for ReadAgent, especially when dealing with extremely long contexts. This further demonstrates that our method can effectively address the challenges of processing extremely long context with limited context window LLMs by exploring graphs containing fine-grained information.</p>
<h3>4.3 Ablation study</h3>
<p>The Effect of Rational Plan In the graph exploration stage, we introduce a rational plan to help the agent analyze complex input questions step by step, guiding the agent in exploring the graph. To verify the effectiveness of the rational plan, we removed it during agent initialization and conducted experiments on four long-context QA benchmarks. Table 4 shows that the rational plan is effective in guiding the agent in node selection and exploration on the graph.</p>
<p>The Effect of Node Selection We conduct randomly selecting initial nodes and neighbor nodes experiments to demonstrate the necessity of our system in selecting which nodes to visit based on reasoning about the required information. As shown in Table 4, random selection results in a significant performance drop, with an average decline of $18 \%$. This demonstrates that GraphReader carefully considers node selection, leading to more reasonable and effective exploration.</p>
<p>Impact of the Number of Initial Nodes We conduct experiments with different initial node counts on multi-hop and single-hop QA datasets to assess the effect of the number of initial nodes on GraphReader's performance. The results are shown in Figure 3. Increasing the number of nodes improves performance up to a certain point, with optimal performance at 5 initial nodes, which we set as the default. However, beyond this threshold, performance declines, especially in single-hop scenarios, likely due to increased noise from too many initial nodes.</p>
<p>Impact of the Chunk Size We investigate the impact of chunk size $L$ on GraphReader's performance. As shown in Figure 4, the best performance is achieved with $L=2 k$. When $L$ exceeds a certain threshold, performance declines because larger chunks cause the model to overlook essential details. Conversely, smaller chunks lead to more semantic truncation, hindering comprehension and accuracy in extracting atomic facts. Thus, we chose $L=2 k$ as the default chunk size.</p>
<h3>4.4 Further Analysis</h3>
<p>Cost Analysis To assess the inference cost of our approach, we compare the average token consumption of ReadAgent and GraphReader for individual questions. As shown in Table 5, GraphReader uses only 1.08 times more tokens than ReadAgent ( $52.8 \mathrm{k} / 48.7 \mathrm{k}$ ), yet achieves more than double the performance improvement, demonstrating its superiority. More importantly, our method has signifi-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of GraphReader with different initial node numbers on 2WikiMultihopQA and NarrativeQA. Results show the robustness of GraphReader towards different initial node numbers.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The impact of chunk size <em>L</em> of GraphReader on the 256k length level of HotpotWikiQA-mixup.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Avg. Ctx. #Tokens</th>
<th>Avg. Cost #Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReadAgent</td>
<td>358.3k</td>
<td>48.7k</td>
</tr>
<tr>
<td>GraphReader</td>
<td>358.3k</td>
<td>52.8k</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of token consumption per question between ReadAgent and GraphReader on HotpotWikiQA-mixup-256k, where "Avg. Ctx. #Tokens" refers to the average token number of the original dataset. The "Avg. Cost #Tokens" comprise both input tokens and output tokens during exploration.</p>
<p>cant advantages in single-document multiple-query scenarios, where only one graph needs to be constructed. Subsequent QA can be performed on this graph, thereby reducing the overall token consumption.</p>
<p><strong>Recall Rate Analysis</strong> To evaluate our method's advantages in key information recall, we utilize GPT-4 to assess the recall of supporting facts on the HotpotWikiQA-mixup dataset. As shown in Figure 5, our model consistently outperforms other baseline methods, regardless of the input length. As context length increases from 16k to 256k, re-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Recall of supporting facts by different methods on HotpotWikiQA-mixup.</p>
<table>
<thead>
<tr>
<th>Source</th>
<th>Recall(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>SF-wise</td>
</tr>
<tr>
<td>Atomic Facts</td>
<td>76.4</td>
</tr>
<tr>
<td>Final Notebook</td>
<td>90.5</td>
</tr>
</tbody>
</table>
<p>Table 6: GraphReader's recall performance at different granularities on HotpotQA. "SF-wise" refers to the granularity of supporting facts, and "Sample-wise" refers to the granularity of sample evaluation.</p>
<p>call of supporting facts declines across all methods. However, GraphReader maintains around 60% recall at 256k context length, in contrast to the significant degradation in ReadAgent. This demonstrates GraphReader's scalability and effectiveness in processing long contexts. Further details and evaluation prompts can be found in Appendix F.</p>
<p>To further demonstrate the recall rate of GraphReader at different granularities, we calculate the recall rate of <em>Supporting Facts</em> and <em>Sample</em> granularity respectively using the same method, detailed in the Appendix F. The granularity of supporting facts refers to the recall rate of all supporting facts across the entire dataset. As for sample granularity, a sample is considered to be recalled only if all of its supporting facts are recalled. As shown in the Table 6, the recall for the final notebook is slightly higher than the recall of atomic facts, which indicates that our method is capable of extracting more valid information from chunks during the exploration, indirectly reflecting its intelligence and effectiveness in exploration.</p>
<h2>5 Conclusion</h2>
<p>This paper introduces GraphReader, a graph-based agent designed to enhance the long-context capabilities of large language models. GraphReader organizes long texts into graph structures and employs an autonomous agent to explore the graph, successfully establishing long-range dependencies within a relatively small 4 k context window. Experiments demonstrate that GraphReader outperforms GPT-4 with a 128 k input length across various long-context single-hop and multi-hop questionanswering benchmarks.</p>
<h2>6 Limitations</h2>
<p>Firstly, GraphReader is constructed using an off-the-shelf GPT-4 API. Since it is close-sourced, there may be potential restrictions such as limits on Queries Per Second (QPS) and regional constraints. Therefore, future work will involve collecting data, training models, and making them open-source to contribute to the wider community. Secondly, the efficiency of the agent depends on its planning and reasoning capabilities. Future research will also explore enhancements of these features to improve the effectiveness of our method.</p>
<h2>References</h2>
<p>Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, et al. 2024a. Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues. arXiv preprint arXiv:2402.14762.</p>
<p>Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. 2024b. Longalign: A recipe for long context alignment of large language models. arXiv preprint arXiv:2401.18058.</p>
<p>Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. ArXiv, abs/2308.14508.</p>
<p>Xingyuan Bu, Junran Peng, Junjie Yan, Tieniu Tan, and Zhaoxiang Zhang. 2021. Gaia: A transfer learning system of object detection that fits your needs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 274-283.</p>
<p>Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023a. Walking down the memory maze: Beyond context limit through interactive reading. arXiv preprint arXiv:2310.05029.</p>
<p>Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595.</p>
<p>Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023c. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.</p>
<p>Michiel De Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William Cohen. 2021. Mention memory: incorporating textual knowledge into transformers through entity mention attention. arXiv preprint arXiv:2110.06176.</p>
<p>Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. 2024. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753.</p>
<p>Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local to global: A graph rag approach to query-focused summarization. ArXiv, abs/2404.16130.</p>
<p>Weixin Feng, Xingyuan Bu, Chenchen Zhang, and Xubin Li. 2022. Beyond bounding box: Multimodal knowledge learning for object detection. arXiv preprint arXiv:2205.04072.</p>
<p>Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. 2020. Entities as experts: Sparse memory access with entity supervision. arXiv preprint arXiv:2004.07202.</p>
<p>Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. 2024. Data engineering for scaling language models to 128k context. arXiv preprint arXiv:2402.10171.</p>
<p>Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752.</p>
<p>Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing A multi-hop QA dataset for comprehensive evaluation of reasoning steps. In COLING, pages 6609-6625. International Committee on Computational Linguistics.</p>
<p>Ziyan Jiang, Xueguang Ma, and Wenhu Chen. 2024. Longrag: Enhancing retrieval-augmented generation with long-context llms. ArXiv, abs/2406.15319.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172.</p>
<p>Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 3948.</p>
<p>Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Trans. Assoc. Comput. Linguistics, 6:317-328.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>LangChain-team. 2024. LangChain.
Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. 2024. A human-inspired reading agent with gist memory of very long contexts. arXiv preprint arXiv:2402.09727.</p>
<p>Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. 2023. Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources. In The Twelfth International Conference on Learning Representations.</p>
<p>Jerry Liu. 2024. LlamaIndex.
Jie Liu, Zhanhui Zhou, Jiaheng Liu, Xingyuan Bu, Chao Yang, Zhong Han-Sen, and Wanli Ouyang. 2024a. Iterative length-regularized direct preference optimization: A case study on improving 7b language models to gpt-4 level. arXiv preprint arXiv:2406.11817.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173.</p>
<p>Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. # instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In The Twelfth International Conference on Learning Representations.</p>
<p>Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning on graphs: Faithful and interpretable large language model reasoning. arXiv preprint arXiv:2310.01061.</p>
<p>Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. 2024. Leave no context behind: Efficient infinite context transformers with infiniattention. arXiv preprint arXiv:2404.07143.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. 2022. QuALITY: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336-5358, Seattle, United States. Association for Computational Linguistics.</p>
<p>Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-22.</p>
<p>Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023a. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071.</p>
<p>Junran Peng, Xingyuan Bu, Ming Sun, Zhaoxiang Zhang, Tieniu Tan, and Junjie Yan. 2020. Largescale object detection in the wild from imbalanced multi-labels. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9709-9718.</p>
<p>Junran Peng, Qing Chang, Haoran Yin, Xingyuan Bu, Jiajun Sun, Lingxi Xie, Xiaopeng Zhang, Qi Tian, and Zhaoxiang Zhang. 2023b. Gaia-universe: Everything is super-netify. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):11856-11868.</p>
<p>Mohammad Sadegh Rasooli and Joel R. Tetreault. 2015. Yara parser: A fast and accurate dependency parser. Computing Research Repository, arXiv:1503.06733. Version 2.</p>
<p>Stephen E. Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3:333-389.</p>
<p>Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. 2023. Questions are all you need to train a dense passage retriever. Transactions of the Association for Computational Linguistics, 11:600-616.</p>
<p>Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. 2024. Raptor: Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.</p>
<p>Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum,</p>
<p>and Jian Guo. 2023. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. arXiv preprint arXiv:2307.07697.</p>
<p>Simeng Sun, Kalpesh Krishna, Andrew MattarellaMicke, and Mohit Iyyer. 2021. Do long-range language models actually use long-range context? arXiv preprint arXiv:2109.09115.</p>
<p>Simeng Sun, Yang Liu, Shuohang Wang, Dan Iter, Chenguang Zhu, and Mohit Iyyer. 2024. PEARL: Prompting large language models to plan and execute actions over long documents. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 469-486, St. Julian's, Malta. Association for Computational Linguistics.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition. Trans. Assoc. Comput. Linguistics, 10:539-554.</p>
<p>Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. 2024. Knowledge graph prompting for multi-document question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19206-19214.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.</p>
<p>Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li. 2024a. Long context alignment with short instructions and synthesized positions. arXiv preprint arXiv:2405.03939.</p>
<p>Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. 2024b. Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models. arXiv preprint arXiv:2402.14660.</p>
<p>Guipeng Xv, Si Chen, Chen Lin, Wanxian Guan, Xingyuan Bu, Xubin Li, Hongbo Deng, Jian Xu, and Bo Zheng. 2022. Visual encoding and debiasing for ctr prediction. In Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management, pages 4615-4619.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In EMNLP, pages 2369-2380. Association for Computational Linguistics.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.</p>
<p>Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. 2024. Lv-eval: A balanced longcontext benchmark with 5 length levels up to 256k. ArXiv, abs/2402.05136.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. abs/2303.18223.</p>
<p>Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400.</p>
<p>A GraphReader Prompt</p>
<p>Figure 6 illustrates the prompt used for Graph Construction. Figures 7 to 11 present the prompts employed for Graph Exploration. Figure 12 shows the prompt used for Answer Reasoning.</p>
<h2>B LLM Rater Evaluation Details</h2>
<p>Given a question, a golden answer, and an answer to be evaluated, we utilize an LLM to assess the accuracy of the latter based on the question and correct answer. This involves two scores: LLM-Rating-1 (LR-1) and LLM-Rating-2 (LR-2), where LR-1 represents a strict scoring criterion, and LR-2 is a more lenient one. Following the approach of ReadAgent, if either LLM Rater deems an answer correct, it is considered as such. If the strict scorer finds an answer incorrect while the lenient scorer deems it partially correct, we classify the answer as partially correct; otherwise, it is adjudged incorrect. The prompts used for evaluation are presented in Figure 13 and Figure 14 respectively.</p>
<p>For the evaluation, we utilize GPT-4-128k as the LLM Rater, with the temperature set to 0.1 .</p>
<h2>C Dataset</h2>
<p>Multi-hop QA Datasets HotpotQA features a collection of 2-hop questions directly authored by native speakers, based on two interconnected paragraphs. 2WikiMultihopQA is comprised of complex questions up to 5 -hops in length, constructed through carefully designed templates to prevent the possibility of shortcut solutions.</p>
<p>In the MuSiQue dataset, questions are intricately crafted starting from straightforward scenarios that require up to 4 -hops reasoning. Annotators subsequently rephrase these with a dual purpose: to avoid shortcut answers and to maintain a natural linguistic quality. Each question within the original datasets is complemented by 2-4 supporting paragraphs, delivering evidence for simple one-step reasoning, alongside multiple paragraphs designed to serve as decoys.</p>
<p>HotpotWikiQA-mixup originates from LV-Eval and employs a construction method known as a mixup. This method randomly blends support documents with various distracting documents to generate five different context lengths for a given QA pair, including 16k, 32k, 64k, 128k, and 256k. Due to the excessive length of this dataset, we select the first 50 data entries from each different context length for experimentation to control costs.</p>
<p>Single-hop QA Datasets NarrativeQA is a dataset designed to test comprehension abilities for long documents, primarily sourced from movie scripts. As a single-hop QA dataset, the information required to answer its questions appears at a single location within the text.</p>
<p>Real-World Datasets QuALITY (Pang et al., 2022) is a long-text multiple-choice questionanswering dataset, with questions crafted by contributors who are familiar with the complete passages, making it more representative of real-world QA scenarios. We handle it as straightforward QA problems. Natural Questions (Kwiatkowski et al., 2019) includes real anonymous aggregated queries from Google along with corresponding Wikipedia pages, providing another excellent resource for authentic long-text QA situations.</p>
<h2>D Baseline Methods</h2>
<p>Full or Chunked Text Content For texts with fewer tokens than the LLM's input window, we can input the text directly into the LLM to obtain an answer. We refer to this method as Full Text Read, with the specific prompt provided in Figure 15. However, this approach is not applicable to texts exceeding the token limit of the LLM's input window. In such cases, Lee et al. truncated the text to fit it into the LLM, but this method obviously results in information loss. We propose a method that does not lose information, offering a better comparison. This method involves dividing the entire text into chunks (using the same chunking method as GraphReader) and then having the LLM read these chunks sequentially according to the text order, thus enabling the handling of overly long texts with a limited input window. During the reading process, there are two main strategies: Chunk Read and Chunk Read with Notes. In the Chunk Read approach, the LLM only sees the current chunk during each reading, which is suitable for single-hop QA tasks. In the Chunk Read with Notes approach, the LLM can summarize useful information from the current chunk and provide it to the subsequent reading process, which is suitable for multi-hop QA tasks.</p>
<p>In the experiment, we divide the chunks in the same way as GraphReader, and the maximum length of the chunk is set to 2 k . The specific prompts are in Figure 16 and 17 respectively.</p>
<p>Retrieval-Augmented Generation (RAG) RAG is a commonly used approach for addressing long-</p>
<p>text problems. In this work, we compare the traditional RAG method, including retrieval methods based on Okapi BM25 (Robertson and Zaragoza, 2009) and the OpenAI API embedding model (text-embedding-ada-002). Specifically, we first split the text into chunks in the same method as GraphReader, then use the aforementioned methods to calculate the relevance scores between the question and these chunks, and finally input the top$n$ chunks with the highest relevance scores together with the question for the LLM to answer. To ensure a fair comparison, we control the input window to 4 k in the experiments. Specifically, in order to fill the input window as much as possible, we set the maximum length of the chunk to 38 k when selecting the top-1 chunk for answering; when opting for the top-3 chunks, we set the maximum length of each chunk to 1 k . The specific prompt can be found in Figure 18.</p>
<p>In addition to traditional RAG methods, we also compared GraphRAG (Edge et al., 2024) and LongRAG (Jiang et al., 2024). GraphRAG utilizes LLMs to construct a graph-based text index in two distinct stages. The first stage involves extracting an entity knowledge graph from the source documents, while the second stage focuses on generating summaries for groups of entities. When a question is posed, each summary provides partial information, which is then combined into the final answer for the user. LongRAG introduces a "long retriever" and a "long reader", allowing the entire corpus to be processed into larger-sized units, which reduces the number of units needed during retrieval and alleviates the burden on the retriever.</p>
<p>Agent Style Methods We also compared our method with similar approaches for handling long texts with small input windows, such as ReadAgent (Lee et al., 2024). ReadAgent is a method that segments long texts and generates gist memories, which are then looked up to search for information in order to answer questions. In the experiments, for datasets from LongBench, we adopted the default hyperparameters declared in the ReadAgent paper, specifically a max_words of 600 and min_words of 280 when splitting pages. For HotpotWikiQA-mixup from LV-Eval, we scaled these two hyperparameters using the same approach as in the ReadAgent paper. Specifically, for datasets with lengths of 256 k and 128 k , we used max_words=10000 and min_words=2000; for those with lengths of $64 \mathrm{k}, 32 \mathrm{k}$ and 16 k , we used max_words=5000 and min_words=1000. At the same time, we employed the ReadAgent-S method, which ReadAgent claims to be the most effective, reading the pages in sequence. Additionally, we allowed reading up to 5 pages (Look up 1-5 pages).</p>
<p>We also compared PEARL (Sun et al., 2024), a prompting framework to enhance reasoning capabilities for long documents. PEARL is structured into three stages: action mining, plan formulation, and plan execution. It decomposes complex questions into actionable steps and utilizes LLMs for zero-shot or few-shot prompting execution.</p>
<h2>E Additional Experimental Results</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Input <br> Window</th>
<th>QuALITY</th>
<th></th>
<th></th>
<th></th>
<th>Natural Question</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>LB-1</td>
<td>LB-2</td>
<td>EM</td>
<td>$F_{1}$</td>
<td>LB-1</td>
<td>LB-2</td>
<td>EM</td>
<td>$F_{1}$</td>
</tr>
<tr>
<td>GPT-4-128k</td>
<td>128 k</td>
<td>45.7</td>
<td>60.3</td>
<td>2.7</td>
<td>9.9</td>
<td>75.0</td>
<td>81.0</td>
<td>41.0</td>
<td>57.2</td>
</tr>
<tr>
<td>Pearl</td>
<td>128 k</td>
<td>52.3</td>
<td>72.7</td>
<td>3.0</td>
<td>9.6</td>
<td>74.0</td>
<td>79.0</td>
<td>38.0</td>
<td>56.8</td>
</tr>
<tr>
<td>LongRAG</td>
<td>128 k</td>
<td>52.3</td>
<td>67.0</td>
<td>3.7</td>
<td>11.6</td>
<td>77.7</td>
<td>83.0</td>
<td>47.3</td>
<td>60.0</td>
</tr>
<tr>
<td>GraphRAG</td>
<td>128 k</td>
<td>46.0</td>
<td>68.7</td>
<td>2.0</td>
<td>6.1</td>
<td>67.0</td>
<td>77.0</td>
<td>47.0</td>
<td>53.2</td>
</tr>
<tr>
<td>GraphReader</td>
<td>4 k</td>
<td>57.3</td>
<td>82.3</td>
<td>4.3</td>
<td>14.3</td>
<td>79.0</td>
<td>84.7</td>
<td>48.3</td>
<td>62.1</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance (\%) comparison of different baselines on two additional datasets. The best performance and the second-best performance are denoted in bold and underlined fonts, respectively.</p>
<p>Table 7 presents additional experimental results for two datasets, QuALITY and Natural Questions, both of which are highly relevant to real-world question-answering scenarios. The results indicate that our method significantly outperforms other baseline models in real-world scenarios.</p>
<h2>F Evaluation Recall for Supporting Facts</h2>
<p>We evaluate the recall rate of supporting facts for different methods using GPT-4-128k, with the temperature set to 0.1 . Figure 19 shows the specific evaluation prompt.</p>
<p>For GraphReader, we evaluate the memory recorded in the final notebook. For ReadAgent, the evaluation focused on the final text segments reviewed. In the case of Chunk Read with Notes, we evaluate both the memory and the chunk read at the time of the final answer; for the RAG methods, we assess the retrieved chunks.</p>
<h2>G The Analysis of Function Calls</h2>
<p>To verify the rationality and utility of agent actions under various circumstances of GraphReader, we made statistics on its function calls at each stage across two datasets. From the statistical results in Table 8, it can be observed that each piece of data</p>
<p>will perform an average of 3 to 4 actions, corresponding to the average number of function calls in the table. This indicates the effectiveness of the graph we constructed, with GraphReader being able to swiftly locate key information while minimizing resource usage. Furthermore, each action has a certain probability of being chosen, justifying the rationality of the action set. Among them, the most commonly used action on multi-hop QA tasks is to read neighbor nodes, and the most common action on single-hop QA tasks is to read chunks. This difference is caused by the fact that multi-hop questions need to gather information contained by multiple nodes to answer questions, while singlehop data sets often require only one atomic fact.</p>
<h1>H Statistics of Graph</h1>
<p>The statistics of graphs from various datasets are presented in Table 9. For longer texts, there tends to be a higher average number of nodes and atomic facts. After normalization, each node has an average of about 10 neighbor nodes. This is because the number of key elements occurring simultaneously in each atomic fact is generally of this magnitude. Furthermore, the aggregation of similar nodes caused by normalization results in a slight increase in the number of neighboring nodes.</p>
<p>On average, each node is associated with about 2 atomic facts, and the average number of atomic facts in the node with the most atomic facts in each graph ranges from 15 to 50 , indicating a relatively even distribution of atomic facts. The maximum average number of atomic facts is found in NarrativeQA, a possible explanation being that NarrativeQA is mainly derived from movie scripts, where characters, such as the protagonist, appear frequently throughout the text, thus including a larger number of atomic facts.</p>
<h2>I GraphReader Example</h2>
<p>This section presents a case study of the GraphReader workflow. Figure 20 displays the posed question alongside the answer and pertinent supporting passages. Subsequently, Figure 21 delineates the methodology for constructing the graph. Figure 22 further elaborates on the initialization of a pre-planned rational path by GraphReader and the selection of initial nodes. Figure 23 illustrates the sequence of function invocations during the exploration phase. Finally, Figure 24 showcases how GraphReader formulates the answer by leveraging the insights obtained through exploration.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">#Avg. Function Call</th>
<th style="text-align: center;">Stage</th>
<th style="text-align: center;">Stage Ratio(\%)</th>
<th style="text-align: center;">Function</th>
<th style="text-align: center;">Call Ratio(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">Exploring Atomic Facts</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">read_chunk <br> stop_and_read_neighbor</td>
<td style="text-align: center;">$\begin{aligned} &amp; 46.5 \ &amp; 53.5 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Chunks</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">search_more <br> read_previous_chunk <br> read_subsequent_chunk <br> termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 12.1 \ &amp; 21.1 \ &amp; 22.9 \ &amp; 43.9 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Neighbors</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">read_neighbor_node termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 35.5 \ &amp; 65.5 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">2WikiMultihopQA</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">Exploring Atomic Facts</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">read_chunk <br> stop_and_read_neighbor</td>
<td style="text-align: center;">$\begin{aligned} &amp; 48.6 \ &amp; 51.4 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Chunks</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">search_more <br> read_previous_chunk <br> read_subsequent_chunk <br> termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 14.5 \ &amp; 25.1 \ &amp; 23.3 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Neighbors</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">read_neighbor_node termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 37.3 \ &amp; 62.7 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">MuSiQue</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">Exploring Atomic Facts</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">read_chunk <br> stop_and_read_neighbor</td>
<td style="text-align: center;">$\begin{aligned} &amp; 41.3 \ &amp; 58.7 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Chunks</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">search_more <br> read_previous_chunk <br> read_subsequent_chunk <br> termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 19.1 \ &amp; 26.6 \ &amp; 25.7 \ &amp; 28.6 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Neighbors</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">read_neighbor_node termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 40.1 \ &amp; 59.9 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">NarrativeQA</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">Exploring Atomic Facts</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">read_chunk <br> stop_and_read_neighbor</td>
<td style="text-align: center;">$\begin{aligned} &amp; 64.5 \ &amp; 35.5 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Chunks</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">search_more <br> read_previous_chunk <br> read_subsequent_chunk <br> termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 4.1 \ &amp; 35.3 \ &amp; 32.6 \ &amp; 28.0 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Neighbors</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">read_neighbor_node termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 51.4 \ &amp; 48.6 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Statistics of function calls on MuSiQue and NarrativeQA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">dataset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sample Dimension</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sample \&amp; Node Dimension</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">node num</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">atomic facts num</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">neighbor node num</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">atomic facts num</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">avg.</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">avg.</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">avg. avg.</td>
<td style="text-align: center;">avg. max</td>
<td style="text-align: center;">avg. avg.</td>
<td style="text-align: center;">avg. max</td>
</tr>
<tr>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">583.8</td>
<td style="text-align: center;">1945.0</td>
<td style="text-align: center;">244.0</td>
<td style="text-align: center;">645.0</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">263.1</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">17.8</td>
</tr>
<tr>
<td style="text-align: center;">2WikiMultihopQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">515.8</td>
<td style="text-align: center;">1691.0</td>
<td style="text-align: center;">217.7</td>
<td style="text-align: center;">545.0</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">215.7</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: center;">MusiQue</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1029.4</td>
<td style="text-align: center;">2142.0</td>
<td style="text-align: center;">419.9</td>
<td style="text-align: center;">586.0</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">253.4</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">15.6</td>
</tr>
<tr>
<td style="text-align: center;">NarrativeQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">966.0</td>
<td style="text-align: center;">3110.0</td>
<td style="text-align: center;">515.5</td>
<td style="text-align: center;">1296.0</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">652.6</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: center;">HotpotWikiQA-mixup</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">1741.6</td>
<td style="text-align: center;">3822.0</td>
<td style="text-align: center;">749.7</td>
<td style="text-align: center;">1043.0</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">231.0</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">32 k</td>
<td style="text-align: center;">2827.3</td>
<td style="text-align: center;">5086.0</td>
<td style="text-align: center;">1257.4</td>
<td style="text-align: center;">1694.0</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">263.3</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">29.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">64 k</td>
<td style="text-align: center;">5054.1</td>
<td style="text-align: center;">8918.0</td>
<td style="text-align: center;">2360.0</td>
<td style="text-align: center;">3015.0</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">227.2</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">128 k</td>
<td style="text-align: center;">8828.5</td>
<td style="text-align: center;">14592.0</td>
<td style="text-align: center;">4437.9</td>
<td style="text-align: center;">5182.0</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">302.0</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">19.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">256 k</td>
<td style="text-align: center;">14853.3</td>
<td style="text-align: center;">24981.0</td>
<td style="text-align: center;">8632.8</td>
<td style="text-align: center;">9478.0</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">427.6</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">27.8</td>
</tr>
</tbody>
</table>
<p>Table 9: Graph statistical data. Under the Sample dimension, "avg." indicates the average number of nodes in each graph, and "max" refers to the largest node count across all graphs. The same logic applies to atomic facts num. In the Sample \&amp; Node dimensions, "avg. avg." denotes the average of the average neighbor node counts per graph, and "avg. max" means the average of the maximum neighbor node counts per graph. This approach is also used for counting atomic facts num.</p>
<p>You are now an intelligent assistant tasked with meticulously extracting both key elements and atomic facts from a long text.</p>
<ol>
<li>Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g., actions), and adjectives (e.g., states, feelings) that are pivotal to the text's narrative.</li>
<li>Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include propositions, theories, existences, concepts, and implicit elements like logic, causality, event sequences, interpersonal relationships, timelines, etc.</li>
</ol>
<p>Requirements:
#####</p>
<ol>
<li>Ensure that all identified key elements are reflected within the corresponding atomic facts.</li>
<li>You should extract key elements and atomic facts comprehensively, especially those that are important and potentially query-worthy and do not leave out details.</li>
<li>Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He, She to actual names).</li>
<li>Ensure that the key elements and atomic facts you extract are presented in the same language as the original text (e.g., English or Chinese).</li>
<li>You should output a total of key elements and atomic facts that do not exceed 1024 tokens.</li>
<li>Your answer format for each line should be: [Serial Number], [Atomic Facts], [List of Key Elements, separated with 'l']
#####</li>
</ol>
<p>Example:
#####
User:
One day, a father and his little son ......
Assistant:</p>
<ol>
<li>One day, a father and his little son were going home. I father I little son I going home</li>
<li>......
#####
Please strictly follow the above format. Let's begin.</li>
</ol>
<p>Figure 6: The prompt for key elements and atomic facts extraction.</p>
<p>As an intelligent assistant, your primary objective is to answer the question by gathering supporting facts from a given article. To facilitate this objective, the first step is to make a rational plan based on the question. This plan should outline the step-by-step process to resolve the question and specify the key information required to formulate a comprehensive answer.</p>
<p>Example:
#####
User: Who had a longer tennis career, Danny or Alice?
Assistant: In order to answer this question, we first need to find the length of Danny's and Alice's tennis careers, such as the start and retirement of their careers, and then compare the two.
#####
Please strictly follow the above format. Let's begin.</p>
<p>Figure 7: The prompt for rational plan.</p>
<p>As an intelligent assistant, your primary objective is to answer questions based on information contained within a text. To facilitate this objective, a graph has been created from the text, comprising the following elements:</p>
<ol>
<li>Text Chunks: Chunks of the original text.</li>
<li>Atomic Facts: Smallest, indivisible truths extracted from text chunks.</li>
<li>Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic facts derived from different text chunks.</li>
</ol>
<p>Your current task is to check a list of nodes, with the objective of selecting the most relevant initial nodes from the graph to efficiently answer the question. You are given the question, the rational plan, and a list of node key elements. These initial nodes are crucial because they are the starting point for searching for relevant information.</p>
<p>Requirements:
#####</p>
<ol>
<li>Once you have selected a starting node, assess its relevance to the potential answer by assigning a score between 0 and 100 . A score of 100 implies a high likelihood of relevance to the answer, whereas a score of 0 suggests minimal relevance.</li>
<li>Present each chosen starting node in a separate line, accompanied by its relevance score. Format each line as follows: Node: [Key Element of Node], Score: [Relevance Score].</li>
<li>Please select at least 10 starting nodes, ensuring they are non-repetitive and diverse.</li>
<li>In the user's input, each line constitutes a node. When selecting the starting node, please make your choice from those provided, and refrain from fabricating your own. The nodes you output must correspond exactly to the nodes given by the user, with identical wording.
#####</li>
</ol>
<p>Example:
#####
User:
Question: {QUESTION}
Plan: {RATIONAL PLAN}
Nodes: {LIST OF KEY ELEMENTS}
Assistant:{LIST OF SELECTED NODES}
#####</p>
<p>Finally, I emphasize again that you need to select the starting node from the given Nodes, and it must be consistent with the words of the node you selected. Please strictly follow the above format. Let's begin.</p>
<p>Figure 8: The prompt for initial node selection.</p>
<p>As an intelligent assistant, your primary objective is to answer questions based on information contained within a text. To facilitate this objective, a graph has been created from the text, comprising the following elements:</p>
<ol>
<li>Text Chunks: Chunks of the original text.</li>
<li>Atomic Facts: Smallest, indivisible truths extracted from text chunks.</li>
<li>Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic facts derived from different text chunks.</li>
</ol>
<p>Your current task is to check a node and its associated atomic facts, with the objective of determining whether to proceed with reviewing the text chunk corresponding to these atomic facts. Given the question, the rational plan, previous actions, notebook content, and the current node's atomic facts and their corresponding chunk IDs, you have the following Action Options:
$# # # # #$</p>
<ol>
<li>read_chunk(List[ID]): Choose this action if you believe that a text chunk linked to an atomic fact may hold the necessary information to answer the question. This will allow you to access more complete and detailed information.</li>
<li>stop_and_read_neighbor(): Choose this action if you ascertain that all text chunks lack valuable information.
$# # # # #$</li>
</ol>
<p>Strategy:
$# # # # #$</p>
<ol>
<li>Reflect on previous actions and prevent redundant revisiting nodes or chunks.</li>
<li>You can choose to read multiple text chunks at the same time.</li>
<li>Atomic facts only cover part of the information in the text chunk, so even if you feel that the atomic facts are slightly relevant to the question, please try to read the text chunk to get more complete information.
$# # # # #$</li>
</ol>
<p>Response format:
$# # # # #$
<em>Updated Notebook</em>: First, combine your current notebook with new insights and findings about the question from current atomic facts, creating a more complete version of the notebook that contains more valid information.
<em>Rationale for Next Action</em>: Based on the given question, the rational plan, previous actions, and notebook content, analyze how to choose the next action.
<em>Chosen Action</em>: read_chunk(List[ID]) or stop_and_read_neighbor(). (Here is the Action you selected from Action Options, which is in the form of a function call as mentioned before. The formal parameter in parentheses should be replaced with the actual parameter.)
$# # # # #$</p>
<p>Finally, it is emphasized again that even if the atomic fact is only slightly relevant to the question, you should still look at the text chunk to avoid missing information. You should only choose stop_and_read_neighbor() when you are very sure that the given text chunk is irrelevant to the question. Please strictly follow the above format. Let's begin.</p>
<p>Figure 9: The prompt for exploring atomic facts.</p>
<p>As an intelligent assistant, your primary objective is to answer questions based on information within a text. To facilitate this objective, a graph has been created from the text, comprising the following elements:</p>
<ol>
<li>Text Chunks: Segments of the original text.</li>
<li>Atomic Facts: Smallest, indivisible truths extracted from text chunks.</li>
<li>Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic facts derived from different text chunks.</li>
</ol>
<p>Your current task is to assess a specific text chunk and determine whether the available information suffices to answer the question. Given the question, rational plan, previous actions, notebook content, and the current text chunk, you have the following Action Options:
#####</p>
<ol>
<li>search_more(): Choose this action if you think that the essential information necessary to answer the question is still lacking.</li>
<li>read_previous_chunk(): Choose this action if you feel that the previous text chunk contains valuable information for answering the question.</li>
<li>read_subsequent_chunk(): Choose this action if you feel that the subsequent text chunk contains valuable information for answering the question.</li>
<li>termination(): Choose this action if you believe that the information you have currently obtained is enough to answer the question. This will allow you to summarize the gathered information and provide a final answer.
#####</li>
</ol>
<p>Strategy:
#####
5. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.
6. You can only choose one action.
#####</p>
<p>Response format:
#####
<em>Updated Notebook</em>: First, combine your previous notes with new insights and findings about the question from current text chunks, creating a more complete version of the notebook that contains more valid information.
<em>Rationale for Next Action</em>: Based on the given question, rational plan, previous actions, and notebook content, analyze how to choose the next action.
<em>Chosen Action</em>: search_more() or read_previous_chunk() or read_subsequent_chunk() or termination(). (Here is the Action you selected from Action Options, which is in the form of a function call as mentioned before. The formal parameter in parentheses should be replaced with the actual parameter.)
#####</p>
<p>Please strictly follow the above format. Let's begin.</p>
<p>Figure 10: The prompt for exploring chunks.</p>
<p>As an intelligent assistant, your primary objective is to answer questions based on information within a text. To facilitate this objective, a graph has been created from the text, comprising the following elements:</p>
<ol>
<li>Text Chunks: Segments of the original text.</li>
<li>Atomic Facts: Smallest, indivisible truths extracted from text chunks.</li>
<li>Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic facts derived from different text chunks.</li>
</ol>
<p>Your current task is to assess all neighboring nodes of the current node, with the objective of determining whether to proceed to the next neighboring node. Given the question, rational plan, previous actions, notebook content, and the neighbors of the current node, you have the following Action Options:
#####</p>
<ol>
<li>read_neighbor_node(key element of node): Choose this action if you believe that any of the neighboring nodes may contain information relevant to the question. Note that you should focus on one neighbor node at a time.</li>
<li>termination(): Choose this action if you believe that none of the neighboring nodes possess information that could answer the question.
#####</li>
</ol>
<p>Strategy:
#####
3. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.
4. You can only choose one action. This means that you can choose to read only one neighbor node or choose to terminate.
#####</p>
<p>Response format:
####
<em>Rationale for Next Action</em>: Based on the given question, rational plan, previous actions, and notebook content, analyze how to choose the next action.
<em>Chosen Action</em>: read_neighbor_node(neighbor_node) or termination(). (Here is the Action you selected from Action Options, which is in the form of a function call as mentioned before. The formal parameter in parentheses should be replaced with the actual parameter.)
#####</p>
<p>Please strictly follow the above format. Let's begin.</p>
<p>Figure 11: The prompt for exploring neighbors.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>https://platform.openai.com/docs/guides/embeddings/ embedding-models&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>https://github.com/openai/tiktoken&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
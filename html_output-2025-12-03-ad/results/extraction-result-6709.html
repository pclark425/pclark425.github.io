<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6709 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6709</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6709</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-4ba57555bef02f988f2ed3bab2f102733dc55221</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4ba57555bef02f988f2ed3bab2f102733dc55221" target="_blank">Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> An innovative process-oriented math process reward model called Math-Shepherd, which assigns a reward score to each step of math problem solutions, which holds significant potential for the future evolution of LLMs.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we present an innovative process-oriented math process reward model called \textbf{Math-Shepherd}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K and 28.6\%$\to$33.0\% on MATH). The accuracy can be further enhanced to 89.1\% and 43.5\% on GSM8K and MATH with the verification of Math-Shepherd, respectively. We believe that automatic process supervision holds significant potential for the future evolution of LLMs.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6709.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6709.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATH-SHEPHERD (PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Math-SHEPHERD process reward model (Process Reward Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A process-oriented reward/verifier that assigns a score to each reasoning step of a math solution, trained on automatically constructed step-wise supervision (no human annotation) and used for reranking and stepwise RL (PPO). Demonstrates consistent gains over outcome-only verifiers and self-consistency on GSM8K and MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MATH-SHEPHERD (process reward model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer-based classifier / verifier (trained as step-level scorer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (reward models trained on 7B, 13B, 34B, 70B bases in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Automatically constructed process-wise supervision: generate candidate step-by-step solutions with generators (fine-tuned on MetaMATH), then use a 'completer' LLM to sample N continuations from each intermediate step and label steps by hard-estimation (HE) or soft-estimation (SE) based on how often continuations reach the golden answer.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH (MATH500 subset for some verification experiments); also tested OOD on Hungarian national final exam</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems (grade-school arithmetic in GSM8K; harder contest/olympiad-style problems in MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems with chain-of-thought style step-by-step solutions</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school; MATH: harder/higher-difficulty contest problems; OOD exam: high-school final exam level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Generators fine-tuned on MetaMATH produce step-by-step solutions; verification uses best-of-N reranking (N up to 256) and self-consistency grouping; training labels produced via completer sampling (N up to 8 in dataset construction experiments) and HE/SE estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (best-of-N for verification; greedy-decoding accuracy for RL), cross-entropy for annotation quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>As verifier, MATH-SHEPHERD improved many base models (verification on 256 candidates): LLaMA2-70B: GSM8K 93.2% / MATH500 44.5%; LLemma-34B: GSM8K 90.9% / MATH500 46.0%; DeepSeek-67B: GSM8K 93.3% / MATH500 47.0%. When combined with self-consistency highest reported: DeepSeek-67B MATH500 48.1%. As reward model for stepwise PPO: Mistral-7B greedy accuracy improved from 77.9% to 84.1% on GSM8K and 28.6% to 33.0% on MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>PRM outputs a step-level sigmoid score; final solution score taken as minimum over step scores. Training labels produced by completer sampling (HE: binary if any completion reaches golden answer; SE: frequency of correct completions). Analysis shows HE reaches ~86% agreement with human labels at N=4 using a strong completer; SE aligns closer to human distribution as N increases. Larger reward models are more robust; PRM gives increasing advantage over ORM as number of candidates N grows and on harder datasets. Completer capability and training data quality strongly affect annotation quality (better completers => better labels).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Automatic process annotation is noisy; HE can produce false positives as N grows; computational cost of completion is high; PRM trained on automated labels still contains annotation noise; verification can harm performance when the verifier is weaker than the generator (smaller reward model verifying larger generator); some incorrect arithmetic steps may still be assigned non-negligible scores, though PRM better localizes step errors than ORM.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Larger reward/verifier models yield better and more robust verification (70B reward models scale well with number of candidates; 7B reward models degrade with more candidates). PRM is more data-efficient than ORM (PRM shows ~4% advantage over ORM at small training sizes like 10k), and PRM's advantage grows on harder datasets (MATH). Increasing candidate count N increases PRM advantage; better completers and more/better training data improves annotation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6709.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6709.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic Process Annotation (HE / SE / completer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic process annotation via completer sampling with Hard Estimation (HE) and Soft Estimation (SE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to label intermediate reasoning steps automatically by sampling N continuations from a given step via a completer LLM and labeling that step by whether completions reach the golden answer (HE binary or SE frequency). Enables training of step-level PRMs without human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>automatic annotation pipeline (completer-based HE/SE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>method / pipeline (uses decoder LLM completer models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Constructed by sampling 15 solutions per problem from small generators; deduplicated; used a completer (LLemma-7B in dataset construction) to produce N continuations per intermediate step (N up to 8 in data construction; experiments used N=4 and N=8 for quality analysis). Training data sourced from MetaMATH-augmented GSM8K and MATH training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (analysis) and MATH (dataset construction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>annotation of multi-step math word problem solutions (used to enable arithmetic reasoning verification and RL)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language step-by-step solutions (chain-of-thought style)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>applied to GSM8K (grade-school) and MATH (hard)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Use a completer LLM to decode N subsequent reasoning paths from each intermediate step; label via HE (exists correct completion) or SE (fraction correct).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>annotation agreement with manual labels (accuracy), cross-entropy loss vs human label distribution</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Manual check: HE using LLaMA2-70B as completer achieved ~86% agreement with human labels at N=4; SE aligns closer to human label distribution as N increases (cross-entropy loss decreased).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Quality depends strongly on completer strength and training data: larger/better completers reduce cross-entropy loss; SE converges toward human distributions as N increases while HE does not; larger N reduces variance but may introduce false positives. Heuristic comparisons (NLI and rule-based methods) underperform this completer-sampling approach.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>HE can generate false positives at larger N (step labeled 'good' though underlying step is flawed); overall annotation has noise and some mismatch with human labels especially with weaker completers or weak training sets; computationally expensive due to many completions per step.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Annotation quality improves with more capable completers and better training data (MetaMATH/augmented data), and with larger N for SE; trade-off between computation and label quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6709.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6709.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B (open-source decoder-only model, fine-tuned on MetaMATH in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter decoder-only LLM used as a generator and as a base model for reward training / PPO experiments; shows notable improvement on arithmetic benchmarks when supervised with step-by-step PRM and stepwise PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mistral 7b.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B (MetaMATH fine-tuned generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on the MetaMATH dataset (task-specific math augmentation); base pretraining details not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems, arithmetic and algebraic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems with step-by-step chain-of-thought style outputs</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school; MATH: harder contest style</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Fine-tuned on MetaMATH; generation evaluated with greedy decoding and best-of-N sampling; verification methods include self-consistency (majority voting), ORM, PRM (MATH-SHEPHERD); reinforcement via step-by-step PPO supervised by PRM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (greedy decoding for RL experiments; best-of-N accuracy for verification experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Greedy-decoding (Table 2): base MetaMATH 77.9% (GSM8K) / 28.6% (MATH); +RFT 79.0% / 29.9%; +ORM-PPO 81.8% / 31.3%; +MATH-SHEPHERD step-by-step PPO 84.1% / 33.0%. Verification (best-of-256, Table 3) after step-by-step PPO: Self-Consistency + MATH-SHEPHERD 89.1% (GSM8K) / 43.5% (MATH500).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Step-by-step PPO supervised by PRM yields larger gains than PPO with outcome-only ORM; PRM provides per-step feedback enabling finer-grained reinforcement. No low-level neuron/attention probing reported for arithmetic processing itself.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Vanilla verification methods (weaker reward models) can be inferior to self-consistency after RL; arithmetic mistakes observed in examples (incorrect sums, wrong algebraic manipulations) which PRM tended to detect at step level while ORM sometimes failed to detect them.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Step-by-step PPO with PRM provides substantial improvement for a 7B generator; further gains achieved when paired with stronger verifiers for reranking (larger reward models).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6709.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6709.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2-70B (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2-70B (open foundation model fine-tuned on MetaMATH for experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter decoder-only model used as generator and a base for training reward models; shows strong verification performance with MATH-SHEPHERD yielding high accuracy on GSM8K and improved results on MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama 2: Open foundation and fine-tuned chat models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-70B (MetaMATH fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on MetaMATH for math reasoning experiments; base pretraining beyond scope of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems with chain-of-thought outputs</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school; MATH: harder</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>MetaMATH fine-tuning; generation with best-of-N sampling (256 candidate solutions for verification); self-consistency grouping and PRM/ORM reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (best-of-256 verification)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1 (verification, 256 outputs): Self-Consistency 88.0% (GSM8K) / 39.4% (MATH500); ORM 91.8% / 40.4%; Self-Consistency + ORM 92.0% / 42.0%; MATH-SHEPHERD 93.2% / 44.5%; Self-Consistency + MATH-SHEPHERD 92.4% / 45.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Larger reward/verifier models (70B) improve steadily as number of candidates increases; PRM (MATH-SHEPHERD) yields superior reranking by evaluating step-level potential; minimum-step score aggregation used to score full solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Combining self-consistency and PRM can sometimes reduce performance on easier datasets if the reward model is already strong, indicating interplay between majority-vote signals and verifier confidence; no low-level mechanistic probing of numeric representations provided.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Large-size generators and reward models show strong gains from PRM verification; reward-model size mattersâ€”larger verifiers validate smaller generators effectively and scale well with candidate count.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6709.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6709.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLemma-34B (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLemma-34B (open math-focused language model fine-tuned on MetaMATH in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 34B-parameter math-focused open model used as generator and as a base for training reward models for MATH experiments; benefits from MATH-SHEPHERD verification with large accuracy gains on MATH500.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llemma: An open language model for mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLemma-34B (MetaMATH fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on MetaMATH; model background from referenced Llemma work but fine-tuning specifics in this paper use MetaMATH.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH (MATH500 subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems (arithmetic, algebra, contest-style problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems with chain-of-thought outputs</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school; MATH: harder</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>MetaMATH fine-tuning; best-of-N sampling (256) for verification; self-consistency grouping, ORM and PRM comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (best-of-256 verification)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1: Self-Consistency 82.6% (GSM8K) / 44.2% (MATH500); ORM 90.0% / 43.7%; Self-Consistency + ORM 89.6% / 45.4%; MATH-SHEPHERD 90.9% / 46.0%; Self-Consistency + MATH-SHEPHERD 89.7% / 47.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>PRM improved more on MATH relative to GSM8K, emphasizing PRM advantage on harder tasks; reward model trained on LLemma-34B-base outputs for MATH experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When verifier is mismatched in capacity relative to generator, verification can harm performance; automatic annotations and PRM still contain some noise.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>PRM yields consistent improvements for 34B base; combining PRM with self-consistency yields best MATH500 results for this model family.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6709.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6709.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-67B (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-67B (open-source model used as generator, MetaMATH-fine-tuned in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 67B-parameter open model evaluated as a generator; with MATH-SHEPHERD verification it achieves state-of-the-art open-source verification results on GSM8K and competitive results on MATH500 without external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek llm: Let there be answers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-67B (MetaMATH fine-tuned generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>67B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on MetaMATH for experiments; base pretraining not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems with chain-of-thought outputs</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school; MATH500: hard</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>MetaMATH fine-tuning; best-of-N sampling (256) and verification via Self-Consistency, ORM, and PRM (MATH-SHEPHERD).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (best-of-256 verification)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1: Self-Consistency 88.2% (GSM8K) / 45.4% (MATH500); ORM 92.6% / 45.3%; Self-Consistency + ORM 92.4% / 47.0%; MATH-SHEPHERD 93.3% / 47.0%; Self-Consistency + MATH-SHEPHERD 92.5% / 48.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>PRM provides better step-level discrimination, enabling selection of correct solutions among many candidates; larger-verifier advantage visible when validating generator outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Some combinations (e.g., self-consistency plus verifier) show non-monotonic changes; automatic labeling noise and completer limitations remain concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>High-capacity generator + high-capacity verifier combination yields top open-source performance; verification gains increase with candidate count and verifier size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6709.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6709.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority-vote over chain-of-thought samples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/aggregation technique that groups candidate solutions by final answer and selects the most frequent answer (majority vote) across chain-of-thought samples; used as a baseline and combined with verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>self-consistency aggregation (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoding/aggregation strategy (not a model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not applicable (aggregation technique applied to generator outputs); generators are fine-tuned on MetaMATH in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH (MATH500)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language questions; uses many chain-of-thought samples per question</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>varies by benchmark; used on grade-school and harder problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Sample multiple chain-of-thought solutions (best-of-N) and take majority answer or combine with reward-model weighting (SC+RM formula used in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (best-of-N with N up to 256)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: LLaMA2-70B Self-Consistency: GSM8K 88.0% / MATH500 39.4% (Table 1). SC combined with PRM sometimes improves MATH performance but can harm GSM8K when the reward model is already strong.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>SC groups solutions by final answer and can be combined multiplicatively with reward model scores per group (paper uses sum of indicator * RM score). Works well for many generators but interplay with strong PRM can be complex (combination sometimes hurts performance).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Majority-vote can be biased toward common but incorrect reasoning patterns; when reward model is strong, combining SC with RM can reduce performance on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>SC benefits from more samples N but gains are outpaced by PRM as N increases on harder datasets; SC is complementary to PRM in many configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Let's verify step by step <em>(Rating: 2)</em></li>
                <li>Solving math word problems with process-and outcome-based feedback <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Making language models better reasoners with step-aware verifier <em>(Rating: 2)</em></li>
                <li>Let's reward step by step: Step-level reward model as the navigators for reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6709",
    "paper_id": "paper-4ba57555bef02f988f2ed3bab2f102733dc55221",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "MATH-SHEPHERD (PRM)",
            "name_full": "Math-SHEPHERD process reward model (Process Reward Model)",
            "brief_description": "A process-oriented reward/verifier that assigns a score to each reasoning step of a math solution, trained on automatically constructed step-wise supervision (no human annotation) and used for reranking and stepwise RL (PPO). Demonstrates consistent gains over outcome-only verifiers and self-consistency on GSM8K and MATH.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MATH-SHEPHERD (process reward model)",
            "model_family": "transformer-based classifier / verifier (trained as step-level scorer)",
            "model_size": "various (reward models trained on 7B, 13B, 34B, 70B bases in experiments)",
            "training_data_description": "Automatically constructed process-wise supervision: generate candidate step-by-step solutions with generators (fine-tuned on MetaMATH), then use a 'completer' LLM to sample N continuations from each intermediate step and label steps by hard-estimation (HE) or soft-estimation (SE) based on how often continuations reach the golden answer.",
            "benchmark_name": "GSM8K and MATH (MATH500 subset for some verification experiments); also tested OOD on Hungarian national final exam",
            "task_type": "multi-step math word problems (grade-school arithmetic in GSM8K; harder contest/olympiad-style problems in MATH)",
            "problem_format": "natural-language word problems with chain-of-thought style step-by-step solutions",
            "difficulty_level": "GSM8K: grade-school; MATH: harder/higher-difficulty contest problems; OOD exam: high-school final exam level",
            "prompting_method": "Generators fine-tuned on MetaMATH produce step-by-step solutions; verification uses best-of-N reranking (N up to 256) and self-consistency grouping; training labels produced via completer sampling (N up to 8 in dataset construction experiments) and HE/SE estimation.",
            "performance_metric": "accuracy (best-of-N for verification; greedy-decoding accuracy for RL), cross-entropy for annotation quality",
            "performance_value": "As verifier, MATH-SHEPHERD improved many base models (verification on 256 candidates): LLaMA2-70B: GSM8K 93.2% / MATH500 44.5%; LLemma-34B: GSM8K 90.9% / MATH500 46.0%; DeepSeek-67B: GSM8K 93.3% / MATH500 47.0%. When combined with self-consistency highest reported: DeepSeek-67B MATH500 48.1%. As reward model for stepwise PPO: Mistral-7B greedy accuracy improved from 77.9% to 84.1% on GSM8K and 28.6% to 33.0% on MATH.",
            "internal_analysis": "PRM outputs a step-level sigmoid score; final solution score taken as minimum over step scores. Training labels produced by completer sampling (HE: binary if any completion reaches golden answer; SE: frequency of correct completions). Analysis shows HE reaches ~86% agreement with human labels at N=4 using a strong completer; SE aligns closer to human distribution as N increases. Larger reward models are more robust; PRM gives increasing advantage over ORM as number of candidates N grows and on harder datasets. Completer capability and training data quality strongly affect annotation quality (better completers =&gt; better labels).",
            "failure_modes": "Automatic process annotation is noisy; HE can produce false positives as N grows; computational cost of completion is high; PRM trained on automated labels still contains annotation noise; verification can harm performance when the verifier is weaker than the generator (smaller reward model verifying larger generator); some incorrect arithmetic steps may still be assigned non-negligible scores, though PRM better localizes step errors than ORM.",
            "scaling_trend": "Larger reward/verifier models yield better and more robust verification (70B reward models scale well with number of candidates; 7B reward models degrade with more candidates). PRM is more data-efficient than ORM (PRM shows ~4% advantage over ORM at small training sizes like 10k), and PRM's advantage grows on harder datasets (MATH). Increasing candidate count N increases PRM advantage; better completers and more/better training data improves annotation quality.",
            "uuid": "e6709.0",
            "source_info": {
                "paper_title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Automatic Process Annotation (HE / SE / completer)",
            "name_full": "Automatic process annotation via completer sampling with Hard Estimation (HE) and Soft Estimation (SE)",
            "brief_description": "A method to label intermediate reasoning steps automatically by sampling N continuations from a given step via a completer LLM and labeling that step by whether completions reach the golden answer (HE binary or SE frequency). Enables training of step-level PRMs without human annotation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "automatic annotation pipeline (completer-based HE/SE)",
            "model_family": "method / pipeline (uses decoder LLM completer models)",
            "model_size": null,
            "training_data_description": "Constructed by sampling 15 solutions per problem from small generators; deduplicated; used a completer (LLemma-7B in dataset construction) to produce N continuations per intermediate step (N up to 8 in data construction; experiments used N=4 and N=8 for quality analysis). Training data sourced from MetaMATH-augmented GSM8K and MATH training sets.",
            "benchmark_name": "GSM8K (analysis) and MATH (dataset construction)",
            "task_type": "annotation of multi-step math word problem solutions (used to enable arithmetic reasoning verification and RL)",
            "problem_format": "natural-language step-by-step solutions (chain-of-thought style)",
            "difficulty_level": "applied to GSM8K (grade-school) and MATH (hard)",
            "prompting_method": "Use a completer LLM to decode N subsequent reasoning paths from each intermediate step; label via HE (exists correct completion) or SE (fraction correct).",
            "performance_metric": "annotation agreement with manual labels (accuracy), cross-entropy loss vs human label distribution",
            "performance_value": "Manual check: HE using LLaMA2-70B as completer achieved ~86% agreement with human labels at N=4; SE aligns closer to human label distribution as N increases (cross-entropy loss decreased).",
            "internal_analysis": "Quality depends strongly on completer strength and training data: larger/better completers reduce cross-entropy loss; SE converges toward human distributions as N increases while HE does not; larger N reduces variance but may introduce false positives. Heuristic comparisons (NLI and rule-based methods) underperform this completer-sampling approach.",
            "failure_modes": "HE can generate false positives at larger N (step labeled 'good' though underlying step is flawed); overall annotation has noise and some mismatch with human labels especially with weaker completers or weak training sets; computationally expensive due to many completions per step.",
            "scaling_trend": "Annotation quality improves with more capable completers and better training data (MetaMATH/augmented data), and with larger N for SE; trade-off between computation and label quality.",
            "uuid": "e6709.1",
            "source_info": {
                "paper_title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Mistral-7B (evaluated)",
            "name_full": "Mistral 7B (open-source decoder-only model, fine-tuned on MetaMATH in this work)",
            "brief_description": "A 7B-parameter decoder-only LLM used as a generator and as a base model for reward training / PPO experiments; shows notable improvement on arithmetic benchmarks when supervised with step-by-step PRM and stepwise PPO.",
            "citation_title": "Mistral 7b.",
            "mention_or_use": "use",
            "model_name": "Mistral-7B (MetaMATH fine-tuned generator)",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Fine-tuned on the MetaMATH dataset (task-specific math augmentation); base pretraining details not specified in this paper.",
            "benchmark_name": "GSM8K and MATH",
            "task_type": "multi-step word problems, arithmetic and algebraic reasoning",
            "problem_format": "natural-language word problems with step-by-step chain-of-thought style outputs",
            "difficulty_level": "GSM8K: grade-school; MATH: harder contest style",
            "prompting_method": "Fine-tuned on MetaMATH; generation evaluated with greedy decoding and best-of-N sampling; verification methods include self-consistency (majority voting), ORM, PRM (MATH-SHEPHERD); reinforcement via step-by-step PPO supervised by PRM.",
            "performance_metric": "accuracy (greedy decoding for RL experiments; best-of-N accuracy for verification experiments)",
            "performance_value": "Greedy-decoding (Table 2): base MetaMATH 77.9% (GSM8K) / 28.6% (MATH); +RFT 79.0% / 29.9%; +ORM-PPO 81.8% / 31.3%; +MATH-SHEPHERD step-by-step PPO 84.1% / 33.0%. Verification (best-of-256, Table 3) after step-by-step PPO: Self-Consistency + MATH-SHEPHERD 89.1% (GSM8K) / 43.5% (MATH500).",
            "internal_analysis": "Step-by-step PPO supervised by PRM yields larger gains than PPO with outcome-only ORM; PRM provides per-step feedback enabling finer-grained reinforcement. No low-level neuron/attention probing reported for arithmetic processing itself.",
            "failure_modes": "Vanilla verification methods (weaker reward models) can be inferior to self-consistency after RL; arithmetic mistakes observed in examples (incorrect sums, wrong algebraic manipulations) which PRM tended to detect at step level while ORM sometimes failed to detect them.",
            "scaling_trend": "Step-by-step PPO with PRM provides substantial improvement for a 7B generator; further gains achieved when paired with stronger verifiers for reranking (larger reward models).",
            "uuid": "e6709.2",
            "source_info": {
                "paper_title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLaMA2-70B (evaluated)",
            "name_full": "LLaMA2-70B (open foundation model fine-tuned on MetaMATH for experiments)",
            "brief_description": "A 70B-parameter decoder-only model used as generator and a base for training reward models; shows strong verification performance with MATH-SHEPHERD yielding high accuracy on GSM8K and improved results on MATH.",
            "citation_title": "Llama 2: Open foundation and fine-tuned chat models",
            "mention_or_use": "use",
            "model_name": "LLaMA2-70B (MetaMATH fine-tuned)",
            "model_family": "decoder-only transformer",
            "model_size": "70B",
            "training_data_description": "Fine-tuned on MetaMATH for math reasoning experiments; base pretraining beyond scope of this paper.",
            "benchmark_name": "GSM8K and MATH",
            "task_type": "multi-step arithmetic and math word problems",
            "problem_format": "natural-language word problems with chain-of-thought outputs",
            "difficulty_level": "GSM8K: grade-school; MATH: harder",
            "prompting_method": "MetaMATH fine-tuning; generation with best-of-N sampling (256 candidate solutions for verification); self-consistency grouping and PRM/ORM reranking.",
            "performance_metric": "accuracy (best-of-256 verification)",
            "performance_value": "Table 1 (verification, 256 outputs): Self-Consistency 88.0% (GSM8K) / 39.4% (MATH500); ORM 91.8% / 40.4%; Self-Consistency + ORM 92.0% / 42.0%; MATH-SHEPHERD 93.2% / 44.5%; Self-Consistency + MATH-SHEPHERD 92.4% / 45.2%.",
            "internal_analysis": "Larger reward/verifier models (70B) improve steadily as number of candidates increases; PRM (MATH-SHEPHERD) yields superior reranking by evaluating step-level potential; minimum-step score aggregation used to score full solutions.",
            "failure_modes": "Combining self-consistency and PRM can sometimes reduce performance on easier datasets if the reward model is already strong, indicating interplay between majority-vote signals and verifier confidence; no low-level mechanistic probing of numeric representations provided.",
            "scaling_trend": "Large-size generators and reward models show strong gains from PRM verification; reward-model size mattersâ€”larger verifiers validate smaller generators effectively and scale well with candidate count.",
            "uuid": "e6709.3",
            "source_info": {
                "paper_title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLemma-34B (evaluated)",
            "name_full": "LLemma-34B (open math-focused language model fine-tuned on MetaMATH in this work)",
            "brief_description": "A 34B-parameter math-focused open model used as generator and as a base for training reward models for MATH experiments; benefits from MATH-SHEPHERD verification with large accuracy gains on MATH500.",
            "citation_title": "Llemma: An open language model for mathematics",
            "mention_or_use": "use",
            "model_name": "LLemma-34B (MetaMATH fine-tuned)",
            "model_family": "decoder-only transformer",
            "model_size": "34B",
            "training_data_description": "Fine-tuned on MetaMATH; model background from referenced Llemma work but fine-tuning specifics in this paper use MetaMATH.",
            "benchmark_name": "GSM8K and MATH (MATH500 subset)",
            "task_type": "multi-step math word problems (arithmetic, algebra, contest-style problems)",
            "problem_format": "natural-language word problems with chain-of-thought outputs",
            "difficulty_level": "GSM8K: grade-school; MATH: harder",
            "prompting_method": "MetaMATH fine-tuning; best-of-N sampling (256) for verification; self-consistency grouping, ORM and PRM comparisons.",
            "performance_metric": "accuracy (best-of-256 verification)",
            "performance_value": "Table 1: Self-Consistency 82.6% (GSM8K) / 44.2% (MATH500); ORM 90.0% / 43.7%; Self-Consistency + ORM 89.6% / 45.4%; MATH-SHEPHERD 90.9% / 46.0%; Self-Consistency + MATH-SHEPHERD 89.7% / 47.3%.",
            "internal_analysis": "PRM improved more on MATH relative to GSM8K, emphasizing PRM advantage on harder tasks; reward model trained on LLemma-34B-base outputs for MATH experiments.",
            "failure_modes": "When verifier is mismatched in capacity relative to generator, verification can harm performance; automatic annotations and PRM still contain some noise.",
            "scaling_trend": "PRM yields consistent improvements for 34B base; combining PRM with self-consistency yields best MATH500 results for this model family.",
            "uuid": "e6709.4",
            "source_info": {
                "paper_title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DeepSeek-67B (evaluated)",
            "name_full": "DeepSeek-67B (open-source model used as generator, MetaMATH-fine-tuned in experiments)",
            "brief_description": "A 67B-parameter open model evaluated as a generator; with MATH-SHEPHERD verification it achieves state-of-the-art open-source verification results on GSM8K and competitive results on MATH500 without external tools.",
            "citation_title": "Deepseek llm: Let there be answers.",
            "mention_or_use": "use",
            "model_name": "DeepSeek-67B (MetaMATH fine-tuned generator)",
            "model_family": "decoder-only transformer",
            "model_size": "67B",
            "training_data_description": "Fine-tuned on MetaMATH for experiments; base pretraining not detailed in this paper.",
            "benchmark_name": "GSM8K and MATH500",
            "task_type": "multi-step math word problems",
            "problem_format": "natural-language word problems with chain-of-thought outputs",
            "difficulty_level": "GSM8K: grade-school; MATH500: hard",
            "prompting_method": "MetaMATH fine-tuning; best-of-N sampling (256) and verification via Self-Consistency, ORM, and PRM (MATH-SHEPHERD).",
            "performance_metric": "accuracy (best-of-256 verification)",
            "performance_value": "Table 1: Self-Consistency 88.2% (GSM8K) / 45.4% (MATH500); ORM 92.6% / 45.3%; Self-Consistency + ORM 92.4% / 47.0%; MATH-SHEPHERD 93.3% / 47.0%; Self-Consistency + MATH-SHEPHERD 92.5% / 48.1%.",
            "internal_analysis": "PRM provides better step-level discrimination, enabling selection of correct solutions among many candidates; larger-verifier advantage visible when validating generator outputs.",
            "failure_modes": "Some combinations (e.g., self-consistency plus verifier) show non-monotonic changes; automatic labeling noise and completer limitations remain concerns.",
            "scaling_trend": "High-capacity generator + high-capacity verifier combination yields top open-source performance; verification gains increase with candidate count and verifier size.",
            "uuid": "e6709.5",
            "source_info": {
                "paper_title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-Consistency (majority-vote over chain-of-thought samples)",
            "brief_description": "A decoding/aggregation technique that groups candidate solutions by final answer and selects the most frequent answer (majority vote) across chain-of-thought samples; used as a baseline and combined with verifiers.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "self-consistency aggregation (SC)",
            "model_family": "decoding/aggregation strategy (not a model)",
            "model_size": null,
            "training_data_description": "Not applicable (aggregation technique applied to generator outputs); generators are fine-tuned on MetaMATH in experiments.",
            "benchmark_name": "GSM8K and MATH (MATH500)",
            "task_type": "multi-step math word problems",
            "problem_format": "natural-language questions; uses many chain-of-thought samples per question",
            "difficulty_level": "varies by benchmark; used on grade-school and harder problems",
            "prompting_method": "Sample multiple chain-of-thought solutions (best-of-N) and take majority answer or combine with reward-model weighting (SC+RM formula used in paper).",
            "performance_metric": "accuracy (best-of-N with N up to 256)",
            "performance_value": "Example: LLaMA2-70B Self-Consistency: GSM8K 88.0% / MATH500 39.4% (Table 1). SC combined with PRM sometimes improves MATH performance but can harm GSM8K when the reward model is already strong.",
            "internal_analysis": "SC groups solutions by final answer and can be combined multiplicatively with reward model scores per group (paper uses sum of indicator * RM score). Works well for many generators but interplay with strong PRM can be complex (combination sometimes hurts performance).",
            "failure_modes": "Majority-vote can be biased toward common but incorrect reasoning patterns; when reward model is strong, combining SC with RM can reduce performance on some datasets.",
            "scaling_trend": "SC benefits from more samples N but gains are outpaced by PRM as N increases on harder datasets; SC is complementary to PRM in many configurations.",
            "uuid": "e6709.6",
            "source_info": {
                "paper_title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Let's verify step by step",
            "rating": 2
        },
        {
            "paper_title": "Solving math word problems with process-and outcome-based feedback",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Making language models better reasoners with step-aware verifier",
            "rating": 2
        },
        {
            "paper_title": "Let's reward step by step: Step-level reward model as the navigators for reasoning",
            "rating": 1
        }
    ],
    "cost": 0.020797999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Math-Shepherd: Verify and ReInforce LLMs STEP-bY-STEP WITHOUT HUMAN ANNOTATIONS</h1>
<p>Peiyi Wang ${ }^{1 \dagger}$ Lei Li ${ }^{3}$ Zhihong Shao ${ }^{4}$ R.X. Xu ${ }^{2}$ Damai Dai ${ }^{1}$ Yifei Li ${ }^{5}$<br>Deli Chen ${ }^{2}$ Y. Wu ${ }^{2}$ Zhifang Sui ${ }^{1}$<br>${ }^{1}$ National Key Laboratory for Multimedia Information Processing, Peking University<br>${ }^{2}$ DeepSeek-AI ${ }^{3}$ The University of Hong Kong<br>${ }^{4}$ Tsinghua University ${ }^{5}$ The Ohio State University<br>{wangpeiyi9979, nlp.lilei}@gmail.com<br>li.14042@osu.edu szf@pku.edu.cn</p>
<p>Project Page: Math-SHEPHERD</p>
<h4>Abstract</h4>
<p>In this paper, we present an innovative process-oriented math process reward model called Math-SHEPHERD, which assigns a reward score to each step of math problem solutions. The training of Math-SHEPHERD is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-SHEPHERD in two scenarios: 1) Verification: Math-SHEPHERD is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) Reinforcement Learning: Math-SHEPHERD is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-SHEPHERD, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-SHEPHERD significantly improves the accuracy of Mistral-7B ( $77.9 \% \rightarrow 84.1 \%$ on GSM8K and $28.6 \% \rightarrow 33.0 \%$ on MATH). The accuracy can be further enhanced to $89.1 \%$ and $43.5 \%$ on GSM8K and MATH with the verification of Math-SHEPHERD, respectively. We believe that automatic process supervision holds significant potential for the future evolution of LLMs.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We evaluate the performance of various LLMs with Math-SHEPHERD on the GSM8K and MATH datasets. All base models are finetuned with the MetaMath dataset (Yu et al., 2023b). The +SHEPHERD results are obtained by selecting the best one from 256 candidates using MATHSHEPHERD. We observe that Math-SHEPHERD is compatible with different LLMs. The results of GPT-4 (early) are from Bubeck et al. (2023).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 INTRODUCTION</h1>
<p>Large language models (LLMs) have demonstrated remarkable capabilities across various tasks (Park et al., 2023; Kaddour et al., 2023; Song et al.; Li et al., 2023a; Wang et al., 2023a; Chen et al., 2023; Zheng et al., 2023; Wang et al., 2023c), However, even the most advanced LLMs face challenges in complex multi-step mathematical reasoning problems (Lightman et al., 2023; Huang et al., 2023). To address this issue, prior research has explored different methodologies, such as pretraining (Azerbayev et al., 2023), fine-tuning (Luo et al., 2023; Yu et al., 2023b; Wang et al., 2023b), prompting (Wei et al., 2022; Fu et al., 2022), and verification (Wang et al., 2023d; Li et al., 2023b; Zhu et al., 2023; Leviathan et al., 2023). Among these techniques, verification has recently emerged as a favored method. The motivation behind verification is that relying solely on the top-1 result may not always produce reliable outcomes. A verification model can rerank candidate responses, ensuring higher accuracy and consistency in the outputs of LLMs. In addition, a good verification model can also offer invaluable feedback for further improvement of LLMs (Uesato et al., 2022; Wang et al., 2023b; Pan et al., 2023).</p>
<p>The verification models generally fall into the outcome reward model (ORM) (Cobbe et al., 2021; Yu et al., 2023a) and process reward model (PRM) (Li et al., 2023b; Uesato et al., 2022; Lightman et al., 2023; Ma et al., 2023). The ORM assigns a confidence score based on the entire generation sequence, whereas the PRM evaluates the reasoning path step-by-step. PRM is advantageous due to several compelling reasons. One major benefit is its ability to offer precise feedback by identifying the specific location of any errors that may arise, which is a valuable signal in reinforcement learning and automatic correction. Besides, The PRM exhibits similarities to human behavior when assessing a reasoning problem. If any steps contain an error, the final result is more likely to be incorrect, mirroring the way human judgment works. However, gathering data to train a PRM can be an arduous process. Uesato et al. (2022) and Lightman et al. (2023) utilize human annotators to provide process supervision annotations, enhancing the performance of PRM. Nevertheless, annotation by humans, particularly for intricate multi-step reasoning tasks that require advanced annotator skills, can be quite costly, which hinders the advancement and practical application of PRM.</p>
<p>To tackle the problem, in this paper, we propose an automatic process annotation framework. Inspired by Monte Carlo Tree Search (Kocsis \&amp; SzepesvÃ¡ri, 2006; Coulom, 2006; Silver et al., 2016; Åšwiechowski et al., 2023), we define the quality of an intermediate step as its potential to deduce the correct final answer. By leveraging the correctness of the answer, we can automatically gather step-wise supervision. Specifically, given a math problem with a golden answer and a step-by-step solution, to achieve the label of a specific step, we utilize a fine-tuned LLM to decode multiple subsequent reasoning paths from this step. We further validate whether the decoded final answer matches with the golden answer. If a reasoning step can deduce more correct answers than another, it would be assigned a higher correctness score.</p>
<p>We use this automatic way to construct the training data for MATH-SHEPHERD, and verify our ideas on two widely used mathematical benchmarks, GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). We explore the effectiveness of MATH-SHEPHERD in two scenarios: 1) verification: MATH-SHEPHERD is utilized for reranking multiple outputs generated by LLMs; 2) reinforcement learning: MATH-SHEPHERD is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With the verification of MATH-SHEPHERD, a series of open-source LLMs from 7B to 70B demonstrates exceptional performance. For instance, the step-by-step PPO with MATHSHEPHERD significantly improves the accuracy of Mistral-7B ( $77.9 \% \rightarrow 84.1 \%$ on GSM8K and $28.6 \% \rightarrow 33.0 \%$ on MATH). The accuracy can be further enhanced to $89.1 \%$ and $43.5 \%$ on GSM8K and MATH with verification. DeepSeek 67B (DeepSeek, 2023) achieves accuracy rates of $93.3 \%$ on the GSM8K dataset and $48.1 \%$ on the MATH dataset with verification of MATH-SHEPHERD. To the best of our knowledge, these results are unprecedented for open-source models that do not rely on additional tools.</p>
<p>Our main contributions are as follows:</p>
<p>1) We propose a framework to automatically construct process supervision datasets without human annotations for math reasoning tasks.</p>
<p>2) We evaluate our method on both step-by-step verification and reinforcement learning scenarios. Extensive experiments on two widely used mathematical benchmarks - GSM8K and MATH, in addition to a series of LLMs ranging from 7B to 70B, demonstrate the effectiveness of our method.
3) We empirically analyze the key factors for training high-performing process reward models, shedding light on future directions toward improving reasoning capability with automatic step-bystep verification and supervision.</p>
<h1>2 Related Works</h1>
<p>Improving and eliciting mathematical reasoning abilities of LLMs. Mathematical reasoning tasks are one of the most challenging tasks for LLMs. Researchers have proposed various methods to improve or elicit the mathematical reasoning ability of LLMs, which can be broadly divided into three groups: 1) pre-training: The pre-training methods (OpenAI, 2023; Anil et al., 2023; Touvron et al., 2023; Azerbayev et al., 2023) pre-train LLMs on a vast of datasets that are related to math problems, such as the Proof-Pile and ArXiv (Azerbayev et al., 2023) with a simple next token prediction objective. 2) fine-tuning: The fine-tuning methods (Yu et al., 2023b; Luo et al., 2023; Yue et al., 2023; Wang et al., 2023b; Gou et al., 2023) can also enhance the mathematical reasoning ability of LLMs. The core of fine-tuning usually lies in constructing high-quality question-response pair datasets with a chain-of-thought reasoning process. and 3) prompting: The prompting methods (Wei et al., 2022; Zhang et al., 2023; Fu et al., 2022; Bi et al., 2023) aim to elicit the mathematical reasoning ability of LLMs by designing prompting strategy without updating the model parameters, which is very convenient and practical.</p>
<p>Mathematical reasoning verification for LLMs. Except for directly improving and eliciting the mathematical reasoning potential of LLMs, the reasoning results can be boosted via an extra verifier for selecting the best answer from multiple decoded candidates. There are two primary types of verifiers: the Outcome Reward Model (ORM) and the Process Reward Model (PRM). The ORM allocates a score to the entire solution while the PRM assigns a score to each individual step in the reasoning process. Recent findings by (Lightman et al., 2023) suggest that PRM outperforms ORM. In addition to verification, reward models can offer invaluable feedback for further training of generators (Uesato et al., 2022; Pan et al., 2023). Compared to ORM, PRM provides more detailed feedback, demonstrating greater potential to enhance generator (Wu et al., 2023). However, training a PRM requires access to expensive human-annotated datasets (Uesato et al., 2022; Lightman et al., 2023), which hinders the advancement and practical application of PRM. Therefore, in this paper, we aim to build a PRM for mathematical reasoning without human annotation, and we explore the effectiveness of the automatic PRM with both verification and reinforcement learning scenarios.</p>
<h2>3 Methodology</h2>
<p>In this section, we first present our task formulation to evaluate the performance of reward models (Â§3.1). Subsequently, we outline two typical categories of reward models, ORM and PRM(Â§3.2). Then, we introduce our methodology to automatically build the training dataset for PRM(Â§3.3), breaking the bottleneck of heavy reliance on manual annotation in existing work (Uesato et al., 2022; Lightman et al., 2023).</p>
<h3>3.1 Task Formulation</h3>
<p>We evaluate the performance of the reward model in two scenarios:</p>
<p>Verification Following (Lightman et al., 2023), we consider a best-of-N selection evaluation paradigm. Specifically, given a problem $p$ in the testing set, we sample N candidate solutions from a generator. These candidates are then scored using a reward model, and the highest-scoring solution is selected as the final answer. An enhanced reward model elevates the likelihood of selecting the solution containing the correct answer, consequently raising the success rate in solving mathematical problems for LLMs.</p>
<p>Reinforcement learning We also use the automatically constructed PRM to supervise LLMs with step-by-step PPO. In this scenario, we evaluate the accuracy of the LLMsâ€™ greedy decoding output. An enhanced reward model is instrumental in training higher-performing LLMs.</p>
<h1>3.2 Reward Models for Mathematical Problem</h1>
<p>ORM Given a mathematical problem $p$ and its solution $s$, ORM $(P \times S \rightarrow \mathbb{R})$ assigns a single real-value to $s$ to indicate whether $s$ is correct. ORM is usually trained with a cross-entropy loss (Cobbe et al., 2021; Li et al., 2023b):</p>
<p>$$
\mathcal{L}<em s="s">{O R M}=y</em>\right)
$$} \log r_{s}+\left(1-y_{s}\right) \log \left(1-r_{s</p>
<p>where $y_{s}$ is the golden answer of the solution $s, y_{s}=1$ if $s$ is correct, otherwise $y_{s}=0 . r_{s}$ is the sigmoid score of $s$ assigned by ORM. The success of the reward model hinges on the effective construction of the high-quality training dataset. As the math problem usually has a certain answer, we can automatically construct the training set of ORM by two steps: 1) sampling some candidate solutions for a problem from a generator; 2) assigning the label to each sampling solution by checking whether its answer is correct. Although false positives solutions that reach the correct answer with incorrect reasoning will be misgraded, previous studies have proven that it is still effective for training a good ORM (Lightman et al., 2023; Yu et al., 2023a).</p>
<p>PRM Take a step further, PRM $\left(P \times S \rightarrow \mathbb{R}^{+}\right)$assigns a score to each reasoning step of $s$, which is usually trained with:</p>
<p>$$
\mathcal{L}<em i="1">{P R M}=\sum</em>\right)
$$}^{K} y_{s_{i}} \log r_{s_{i}}+\left(1-y_{s_{i}}\right) \log \left(1-r_{s_{i}</p>
<p>where $y_{s_{i}}$ is the golden answer of $s_{i}$ (the $i$-th step of $s$ ), $r_{s_{i}}$ is the sigmoid score of $s_{i}$ assigned by PRM and $K$ is the number of reasoning steps for $s$. (Lightman et al., 2023) also conceptualizes the PRM training as a three-class classification problem, in which each step is classified as either 'good', 'neutral', or 'bad'. In this paper, we found that there is not much difference between the binary and the three classifications, and we regard PRM training as the binary classification. Compared to ORM, PRM can provide more detailed and reliable feedback (Lightman et al., 2023). However, there are currently no automated methods available for constructing high-quality PRM training datasets. Previous works (Uesato et al., 2022; Lightman et al., 2023) typically resort to costly human annotations. While PRM manages to outperform ORM (Lightman et al., 2023), the annotation cost invariably impedes both the development and application of PRM.</p>
<h3>3.3 Automatic Process Annotation</h3>
<p>In this section, we propose an automatic process annotation framework to mitigate the annotation cost issues associated with PRM. We first define the quality of a reasoning step, followed by the introduction of our solution that obviates the necessity for human annotation.</p>
<h3>3.3.1 DEFINITION</h3>
<p>Inspired by Monto Carlo Tree Search (Kocsis \&amp; SzepesvÃ¡ri, 2006; Coulom, 2006; Silver et al., 2016; Åšwiechowski et al., 2023), we define the quality of a reasoning step as its potential to deduce the correct answer. This criterion stems from the primary objective of the reasoning process, which essentially is a cognitive procedure aiding humans or intelligent agents in reaching a well-founded outcome (Huang \&amp; Chang, 2023). Therefore, a step that has the potential to deduce a well-founded result can be considered a good reasoning step. Analogous to ORM, this definition also introduces some degree of noise. Nevertheless, we find that it is beneficial for effectively training a good PRM.</p>
<h3>3.3.2 SOLUTION</h3>
<p>Completion To quantify and estimate the potential for a give reasoning step $s_{i}$, as shown in Figure 2, we use a 'completer' to finalize N subsequent reasoning processes from this step: $\left{\left(s_{i+1, j}, \cdots, s_{K_{j}, j}, a_{j}\right)\right}<em j="j">{j=1}^{N}$, where $a</em>$.}$ and $K_{j}$ are the decoded answer and the total number of steps for the $j$-th finalized solution, respectively. Then, we estimate the potential of this step based on the correctness of all decoded answers $A=\left{a_{j}\right}_{j=1}^{N</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison for previous automatic outcome annotation and our automatic process annotation. (a): automatic outcome annotation assigns a label to the entire solution $S$, dependent on the correctness of the answer; (b) automatic process annotation employs a 'completer' to finalize N reasoning processes ( $\mathrm{N}=3$ in this figure) for an intermediate step ( $s_{1}$ in this figure), subsequently use hard estimation (HE) and soft estimation (SE) to annotate this step based on all decoded answers.</p>
<p>Estimation In this paper, we use two methods to estimate the quality $y_{s_{i}}$ for the step $s_{i}$, hard estimation (HE) and soft estimation (SE). HE supposes that a reasoning step is good as long as it can reach the correct answer $a^{*}$ :</p>
<p>$$
y_{s_{i}}^{H E}=\left{\begin{array}{ll}
1 &amp; \exists a_{j} \in A, a_{j}=a^{*} \
0 &amp; \text { Otherwise }
\end{array}\right.
$$</p>
<p>SE assumes the quality of a step as the frequency with which it reaches the correct answer:</p>
<p>$$
y_{s_{i}}^{S E}=\frac{\sum_{j=1}^{N} \mathbb{I}\left(a_{j}=a^{*}\right)}{N}
$$</p>
<p>Once we gather the label of each step, we can train PRM with the cross-entropy loss. In conclusion, our automatic process annotation framework defines the quality of a step as its potential to deduce the correct answer and achieve the label of each step by completion and estimation.</p>
<h1>3.4 Ranking for Verification</h1>
<p>Following (Lightman et al., 2023), we use the minimum score across all steps to represent the final score of a solution assigned by PRM. We also explore the combination of self-consistency and reward models following (Li et al., 2023b). In this context, we initially classify solutions into distinct groups according to their final answers. Following that, we compute the aggregate score for each group. Formally, the final prediction answer based on N candidate solutions is:</p>
<p>$$
a_{s c+r m}=\arg \max <em i="1">{a} \sum</em>\right)
$$}^{N} \mathbb{I}\left(a_{i}=a\right) \cdot R M\left(p, S_{i</p>
<p>Where $R M\left(p, S_{i}\right)$ is the score of the $i$-th solution assigned by ORM or PRM for problem $p$.</p>
<h3>3.5 ReInforce Learning with Process Supervision</h3>
<p>Upon achieving PRM, we employ reinforcement learning to train LLMs. We implement Proximal Policy Optimization (PPO) in a step-by-step manner. This method differs from the conventional strategy that utilizes PPO with ORM, which only offers a reward at the end of the response. Conversely, our step-by-step PPO offers rewards at the end of each reasoning step.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Verifiers</th>
<th>GSM8K</th>
<th>MATH500</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA2-70B: MetaMATH</td>
<td>Self-Consistency</td>
<td>88.0</td>
<td>39.4</td>
</tr>
<tr>
<td></td>
<td>ORM</td>
<td>91.8</td>
<td>40.4</td>
</tr>
<tr>
<td></td>
<td>Self-Consistency + ORM</td>
<td>92.0</td>
<td>42.0</td>
</tr>
<tr>
<td></td>
<td>MATH-SHEPHERD (Ours)</td>
<td>93.2</td>
<td>44.5</td>
</tr>
<tr>
<td></td>
<td>Self-Consistency + MATH-SHEPHERD (Ours)</td>
<td>92.4</td>
<td>45.2</td>
</tr>
<tr>
<td>LLemma-34B: MetaMATH</td>
<td>Self-Consistency</td>
<td>82.6</td>
<td>44.2</td>
</tr>
<tr>
<td></td>
<td>ORM</td>
<td>90.0</td>
<td>43.7</td>
</tr>
<tr>
<td></td>
<td>Self-Consistency + ORM</td>
<td>89.6</td>
<td>45.4</td>
</tr>
<tr>
<td></td>
<td>MATH-SHEPHERD (Ours)</td>
<td>90.9</td>
<td>46.0</td>
</tr>
<tr>
<td></td>
<td>Self-Consistency + MATH-SHEPHERD (Ours)</td>
<td>89.7</td>
<td>47.3</td>
</tr>
<tr>
<td>DeepSeek-67B: MetaMATH</td>
<td>Self-Consistency</td>
<td>88.2</td>
<td>45.4</td>
</tr>
<tr>
<td></td>
<td>ORM</td>
<td>92.6</td>
<td>45.3</td>
</tr>
<tr>
<td></td>
<td>Self-Consistency + ORM</td>
<td>92.4</td>
<td>47.0</td>
</tr>
<tr>
<td></td>
<td>MATH-SHEPHERD (Ours)</td>
<td>93.3</td>
<td>47.0</td>
</tr>
<tr>
<td></td>
<td>Self-Consistency + MATH-SHEPHERD (Ours)</td>
<td>92.5</td>
<td>48.1</td>
</tr>
</tbody>
</table>
<p>Table 1: Performances of different LLMs on GSM8K and MATH with different verification strategies. The reward models are trained based on LLama2-70B and LLemma-34B on GSM8K and MATH, respectively. The verification is based on 256 outputs.</p>
<h1>4 EXPERIMENTS</h1>
<p>Datasets We conduct our experiments using two widely used math reasoning datasets, GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). For the GSM8K dataset, we leverage the whole test set in both verification and reinforcement learning scenarios. For the MATH dataset, in the verification scenario, due to the computation cost, we employ a subset MATH500 that is identical to the test set of Lightman et al. (2023). The subset consists of 500 representative problems, and we find that the subset evaluation produces similar results to the full-set evaluation. To assess different verification methods, we generate 256 candidate solutions for each test problem. We report the mean accuracy of 3 groups of sampling results. In the reinforcement learning scenario, we use the whole test set to evaluate the model performance. We train LLMs with MetaMATH (Yu et al., 2023b).</p>
<p>Parameter Setting Our experiments are based on a series of large language models, LLaMA27B/13B/70B (Touvron et al., 2023), LLemma-7B/34B (Azerbayev et al., 2023), Mistral-7B (Jiang et al., 2023) and DeepSeek-67B (DeepSeek, 2023). We train the generator and completer for 3 epochs on MetaMATH. We train the Mistral-7B with a learning rate of 5e-6. For other models, The learning rates are set to $2 \mathrm{e}-5,1 \mathrm{e}-5$, and $6 \mathrm{e}-6$ for the $7 \mathrm{~B} / 13 \mathrm{~B}, 34 \mathrm{~B}$, and $67 \mathrm{~B} / 70 \mathrm{~B}$ LLMs, respectively. To construct the training dataset of ORM and PRM, we train 7B and 13B models for a single epoch on the GSM8K and MATH training sets. Subsequently, we sample 15 solutions per problem from each model for the training set. Following this, we eliminate duplicate solutions and annotate the solutions at each step. We use LLemma-7B as the completer with the decoded number $\mathrm{N}=8$. Consequently, we obtain around 170 k solutions for GSM8K and 270k solutions for MATH. For verification, we choose LLaMA2-70B and LLemma-34B as the base models to train reward models for GSM8K and MATH, respectively. For reinforcement learning, we choose Mistral-7B as the base model to train reward models and use it to supervise LLama2-7B and Mistral-7B generators. The reward model is trained in 1 epoch with a learning rate 1e-6. For the sake of convenience, we train the PRM using the hard estimation version because it allows us to utilize a standard language modeling pipeline by selecting two special tokens to represent 'has potential' and 'no potential' labels, thereby eliminating the need for any specific model adjustments. In reinforcement learning, the learning rate is $4 \mathrm{e}-7$ and 1e-7 for LLaMA2-7B and Mistral-7B, respectively. The Kullback-Leibler coefficient is set to 0.04 . We implement a cosine learning rate scheduler, employing a minimal learning rate set to 1e-8. We use 3D parallelism provided by hfai ${ }^{1}$ to train all models with the max sequence length of 512 .</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLaMA2-7B: MetaMATH</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">19.2</td>
</tr>
<tr>
<td style="text-align: center;">+ RFT</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">19.9</td>
</tr>
<tr>
<td style="text-align: center;">+ ORM-PPO</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">20.8</td>
</tr>
<tr>
<td style="text-align: center;">+ MATH-SHEPHERD-step-by-step-PPO (Ours)</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">21.6</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B: MetaMATH</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">28.6</td>
</tr>
<tr>
<td style="text-align: center;">+ RFT</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">29.9</td>
</tr>
<tr>
<td style="text-align: center;">+ ORM-PPO</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">31.3</td>
</tr>
<tr>
<td style="text-align: center;">+ MATH-SHEPHERD-step-by-step-PPO (Ours)</td>
<td style="text-align: center;">$\mathbf{8 4 . 1}$</td>
<td style="text-align: center;">$\mathbf{3 3 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Performances of different 7B models on GSM8K and MATH with greedy decoding. We use the questions in MetaMATH for RFT and PPO training. Both LLaMA2-7B and Mistral-7B are supervised by Mistral-7B-ORM and -MATH-SHEPHERD.</p>
<p>Baselines and Metrics In the verification scenario, following (Lightman et al., 2023), we evaluate the performance of our reward model by comparing it against the Self-consistency (majority voting) and outcome reward model. The accuracy of the best-of-N solution is utilized as the evaluation metric. For PRM, the minimum score across all steps is adopted to represent the final score of a solution. In the reinforcement scenario, we compare our step-by-step supervision with the outcome supervision provided by ORM, and Rejective Sampling Fine-tuning (RFT) (Yuan et al., 2023), we sample 8 responses for each question in MetaMATH for RFT. We use the accuracy of LLMsâ€™ greedy decoding output to assess the performance.</p>
<h1>4.1 Main ReSults</h1>
<p>MATH-SHEPHERD as verifier Table 1 presents the performance comparison of various methods on GSM8K and MATH. We find that: 1) As the verifier, MATH-SHEPHERD consistently outperforms self-consistency and ORM on two datasets with all generators. Specifically, enhanced by MATHSHEPHERD, DeepSeek-67B achieves $93.3 \%$ and $48.1 \%$ accuracy on GSM8K and MATH; 2) In comparison to GSM8K, PRM achieves a greater advantage over ORM on the more challenging MATH dataset; This outcome aligns with the findings in Uesato et al. (2022) and Lightman et al. (2023). The former discovers that PRM and ORM yield similar results on GSM8K, whereas the latter shows that PRM significantly outperforms ORM on the MATH dataset. This could be attributed to the relative simplicity of the GSM8K dataset compared to MATH, i.e., the GSM8K dataset necessitates fewer steps for problem-solving. As a result, ORM operates efficiently when handling this particular dataset; 3) In GSM8K, when combined with self-consistency, there's a drop in performance, whereas in MATH, performance improves. These results indicate that if the reward model is sufficiently powerful for a task, combining it with self-consistency may harm the verification performance.</p>
<p>MATH-SHEPHERD as reward model on reinforcement learning Table 2 presents the performance of different LLMs with greedy decoding outputs. As is shown: 1) step-by-step PPO significantly improves the performance of two supervised fine-tuned models. For example, Mistral-7B with step-by-step PPO achieves $84.1 \%$ and $33.0 \%$ on the GSM8K and MATH datasets, respectively; 2) RFT only slightly improves the model performance, we believe this is because MetaMATH already has conducted some data augmentation strategies like RFT; 3) the vanilla PPO with ORM can also enhance the model performance. However, it does not perform as well as the step-by-step PPO supervised by MATH-SHEPHERD, demonstrating the potential of step-by-step supervision.</p>
<p>MATH-SHEPHERD as both reward models and verifiers We also combine the reinforcement learning and the verification. As shown in Table 3: 1) reinforcement learning and verification are complementary. For example, in MATH, step-by-step PPO Mistral-7B outperforms supervised fine-tuning Mistral-7B $7.2 \%$ accuracy with self-consistency as the verifier; The performance gap is even larger than that of greedy decoding results, i.e., $4.4 \%$; 2) after reinforcement learning, the vanilla verification methods with only reward models is inferior to self-consistency, we think the</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Verifiers</th>
<th>GSM8K</th>
<th>MATH500</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mistral-7B: MetaMATH</td>
<td>Self-Consistency</td>
<td>83.9</td>
<td>35.1</td>
</tr>
<tr>
<td></td>
<td>ORM</td>
<td>86.2</td>
<td>36.4</td>
</tr>
<tr>
<td></td>
<td>Self-Consistency + ORM</td>
<td>86.6</td>
<td>38.0</td>
</tr>
<tr>
<td></td>
<td>MATH-SHEPHERD (Ours)</td>
<td>87.1</td>
<td>37.3</td>
</tr>
<tr>
<td></td>
<td>Self-Consistency + MATH-SHEPHERD (Ours)</td>
<td>86.3</td>
<td>38.3</td>
</tr>
<tr>
<td>Mistral-7B: MetaMATH</td>
<td>Self-Consistency</td>
<td>87.4</td>
<td>42.3</td>
</tr>
<tr>
<td></td>
<td>ORM</td>
<td>87.6</td>
<td>41.3</td>
</tr>
<tr>
<td>+step-by-step PPO (Ours)</td>
<td>Self-Consistency + ORM</td>
<td>89.0</td>
<td>43.1</td>
</tr>
<tr>
<td></td>
<td>MATH-SHEPHERD (Ours)</td>
<td>88.4</td>
<td>41.1</td>
</tr>
<tr>
<td></td>
<td>Self-Consistency + MATH-SHEPHERD (Ours)</td>
<td>$\mathbf{8 9 . 1}$</td>
<td>$\mathbf{4 3 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of reinforcement learning and verification combination. The reward models are trained based on Mistral-7B. The verification is based on 256 outputs.
reason is that the initial reward model is not sufficient to supervise the more powerful model after PPO. These results can also show the potential of iterative reinforcement learning, which we leave for future work.</p>
<h1>5 ANALYSIS</h1>
<h3>5.1 Performance with Different Number of Candidate Solutions</h3>
<p>Figure 3 illustrates the performance comparison of various strategies when applied to different numbers of candidates ranging from 1 to 256 on two benchmarks. The key observations are as follows: 1) PRM exhibits consistent superior performance when compared to both ORM and majority voting, with the degree of this superiority becoming more pronounced as N escalates. 2) In MATH, our automatically annotated datasets outperform the human-annotated PRM800K (Lightman et al., 2023). We ascribe this superiority to the distribution gap and the data quantity. Specifically, PRM800K is annotated based on the output from GPT-4, and consequently, a discrepancy arises for the output of open-source LLaMA models fine-tuned on MetaMATH. Furthermore, when considering the quantity of data, our automated reward model data exhibits both high scalability and a reduced labeling cost. Consequently, our dataset is four times larger than that provided in PRM800K. Overall, these results further underscore the effectiveness and potential of our method.</p>
<h3>5.2 Quality of the Automatic Process Annotations</h3>
<p>In this section, we explore the quality of our automatic PRM dataset. To achieve this, we manually annotate 160 steps sampled from the training set of GSM8K and use different completers to infer from each step to achieve their label. We find that:</p>
<p>Automatic process annotation exhibits satisfactory quality. Figure 4(a) demonstrates that utilizing LLaMA2-70B trained on MetaMATH as the completer, the accuracy of the hard estimation (HE) reaches $86 \%$ when N equals 4 . This suggests that our automatically constructed dataset is of high quality. However, we observed a decline in the accuracy of the constructed dataset with further increases in N. Our analysis indicates that larger values for N may lead to false positives.</p>
<p>Figure 4(b) shows the cross-entropy loss between SE and HE labels compared to the human-annotated distribution: as N increases, SE progressively aligns closer to the standard distribution, in contrast to HE which does not exhibit similar behavior. It is essential to note that at $\mathrm{N}=4$, HE achieves an accuracy of $86 \%$. We can theoretically attain higher quality data exceeding $86 \%$ accuracy by utilizing SE. However, we discovered that the performance of the verifier exhibits no substantial divergence whether trained with either SE or HE. This may be attributable to the already high-quality annotations provided by HE.</p>
<p>Furthermore, we also delve into other automatic process annotation methodologies. For instance, (Li et al., 2023b) employs a natural language inference (NLI) model and a string match rule to annotate a</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of LLaMA2-70B using different verification strategies across different numbers of solution candidates on GSM8K and MATH.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Quality of process annotation on GSM8K. (a): Accuracy of the process annotation using different completer; (b): Loss of the process annotation using different completer; (c): Loss of the process annotation using the same completer with different training data.
given step. The NLI-based method annotates a step as correct if it is entailment with any step in the reference solutions. The Rule-based method annotates a step as correct if its support number precisely matches that of any steps in the reference solutions. As demonstrated in Table 4, our annotation strategy exhibits substantial superiority over the two approaches.</p>
<p>The ability of the LLM completer plays an important role in the data quality. We employ a completer to finalize multiple subsequent reasoning processes for a given step. Therefore, we investigate the impact of the LLM completer.</p>
<p>Figure 4(b) presents the cross-entropy loss across diverse completers trained on MetaMath. The results indicate that a larger completer is adept at generating superior-quality datasets. Figure 4(c) depicts the cross-entropy loss of LLaMA2-70B trained with different datasets. 'Normal' denotes the original GSM8K training dataset; 'Weak' refers to the Normal set excluding examples whose questions are in our 160 evaluation set; while 'Augmented' symbolizes MetaMath, an augmented version of the Normal set.</p>
<p>The findings suggest that high-quality training sets allow the model to operate more proficiently as a completer. Importantly, the 'Weak' set exhibits a markedly larger loss than other datasets. This insight drives us to infer that LLMs should acquire the questions in advance to enhance their performance as completers. We can also conjecture that a stronger foundational model, coupled with superior training data, could further enhance the quality of automatic annotation.</p>
<h1>5.3 Influence of the Pre-trained Base Models</h1>
<p>To conduct an exhaustive evaluation of MATH-SHEPHERD's effectiveness, we performed a diverse range of experiments using model sizes 7B, 13B, and 70B.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">Accuracy (\%)</th>
<th style="text-align: center;">Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DIVERSE-NLI (Li et al., 2023b)</td>
<td style="text-align: left;">DeBERTa (He et al., 2020)</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">5.43</td>
</tr>
<tr>
<td style="text-align: left;">DIVERSE-NLI (Li et al., 2023b)</td>
<td style="text-align: left;">LLaMA2-13B</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">3.27</td>
</tr>
<tr>
<td style="text-align: left;">DIVERSE-Rule (Li et al., 2023b)</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">3.43</td>
</tr>
<tr>
<td style="text-align: left;">MATH-SHEPHERD</td>
<td style="text-align: left;">LLaMA2-13B $(\mathrm{N}=4)$</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">2.05</td>
</tr>
</tbody>
</table>
<p>Table 4: The comparison between NLI/Rule-based automatic process annotation methods from Li et al. (2023b) and our method.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance of different verification strategies on different sizes of generators and verifiers.</p>
<p>Figures 5(a), 5(b), and 3(a) display the results from the 7B, 13B, and 70B generators paired with equal-sized reward models, respectively. It becomes evident that PRM exhibits superiority over self-consistency and ORM across all sizes of base models. Moreover, bigger reward models prove to be more robust; for instance, the accuracy of the 70B reward models escalates as the number of candidate solutions rises, while the 7B reward models show a decreasing trend.</p>
<p>Figure 5(c) and 5(d) presents the performance of 7B and 70B generators interfaced with differentsized reward models. The findings illustrate that utilizing a larger reward model to validate the output of a smaller generator significantly enhances performance. Conversely, when a smaller reward model is employed to validate the output of a larger generator, the verification process adversely impacts the model's performance compared to SC. These results substantiate that we should utilize a more potent reward model for validating or supervising the generator.</p>
<h1>5.4 INFLUENCE OF THE NUMBER OF DATA</h1>
<p>We delve deeper into the analysis of PRM and ORM by utilizing varying quantities of training data. As depicted in Figure 6(a), it is clear that PRM exhibits superior data efficiency. Specifically, it outperforms ORM by approximately $4 \%$ accuracy when applying a modestly sized training dataset (i.e., 10k instances). Furthermore, PRM seems to have a higher potential ceiling than ORM. These observations highlight the efficacy of PRM for verification purposes.</p>
<h3>5.5 Out-of-DISTRIBUTION PERFORMANCE</h3>
<p>To further demonstrate the effectiveness of our method, we conduct an out-of-distribution evaluation on the Hungarian national final exam ${ }^{2}$, which consists of 33 questions. The total score of these questions is 100 . We use the LLemma-34B trained on MetaMATH to serve as the generator and generate 256 candidate solutions for each question. We use LLemma-34B-ORM and LLemma-34B-PRM to select the solution for each question. As shown in Figure 6(b): 1) both LLemma-34B-ORM and LLemma-34B-PRM outperform the origin LLemma-34B, showing the reward model can generalize to other domains; 2) PRM outperforms ORM 9 scores, further demonstrating the superiority of PRM.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: (a): Performance of different reward models using different numbers of training data; (b) performance of different verification strategies on the out-of-distribution Hungarian national exam.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">The first term of an arithmetic sequence is 18. The sum of the first six terms of the sequence is equal to the sum of the first seven terms. Show that the sum of the first 13 terms of the sequence is 0 and calculate the 13th term of the sequence.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Step 1: Let the common difference of the arithmetic sequence be $d$. Math-SHEPHERD: 0.99 . <br> Step 2: The sum of the first six terms of the sequence is $18+(18+d)+(18+2 d)+(18+3 d)+(18+4 d)+$ $(18+5 d)=108+30 d$. Math-Shepherd: 0.85 . <br> Step 6: Therefore, $d=-3$. Math-Shepherd: 0.85 . <br> Step 8: The 13th term of the sequence is $18-36=-18$. Math-Shepherd: 0.93 . ORM Score: 0.73 .</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Step 1: Let the common difference of the arithmetic sequence be $d$. Math-Shepherd: 0.99 . <br> Step 2: The sum of the first six terms is $18+(18+d)+(18+2 d)+(18+3 d)+(18+4 d)+(18+5 d)=$ $90 * 15 d$. Math-Shepherd: 0.53 . <br> Step 6: Dividing by -6 , we find that $d=-2$. Math-Shepherd: 0.38 . <br> Step 9: The 13th term of the sequence is $18-26=-8$. Math-Shepherd: 0.38 . ORM Score: 0.84 .</td>
</tr>
</tbody>
</table>
<p>Table 5: A case study from the Hungarian national exam. Red text denotes the mistake that ORM fails to detect.</p>
<p>We also conduct a case study to intuitively demonstrate the effectiveness of Math-SHEPHERD. As outlined in Table 5, when presented with a question from the Hungarian national final exam, our Math-SHEPHERD accurately selected the correct solution from a pool of 256 potential solutions, which ORM failed. Moreover, Math-SHEPHERD displayed superior discernment by precisely identifying incorrect steps within the solutions selected by ORM. Notably, it recognized errors in Step 2, Step 6, and Step 9 and so on, and subsequently assigned them lower scores relative to those for steps present in the correct solutions.</p>
<h1>6 LIMITATIONS</h1>
<p>Our paper has some limitations, which we leave for future work:
The computational cost of the completion process. To determine the label of each reasoning step, we utilize a 'completer' to decode N subsequent reasoning processes. We observe that as N increases, so does the quality of automatic annotations. However, this completion process demands a lot of computing resources, potentially imposing a limitation on the usage of our method. Despite this limitation, the cost remains significantly lower than human annotation. Furthermore, we are optimistic that advancements in efficient inference techniques such as speculative decoding (Xia et al., 2022; Leviathan et al., 2023) and vLLM (Kwon et al., 2023) could mitigate this limitation.</p>
<p>The automatic process annotation consists of noise. Similar to the automatic outcome annotation, our automatic process annotation also has noise. Despite this, our experiments verify the efficacy of our method for training a PRM. In particular, the PRM trained on our dataset outperforms the</p>
<p>human-annotated PRM800K dataset. However, a noticeable gap remains between PRM800K and the candidate responses generated by the open-source models utilized in this study, which may result in the invalidation of PRM800K. As a result, the impact of this potential noise on PRM performance is still undetermined. A comprehensive comparison between human and automated annotations is envisaged for future studies. Furthermore, we assert that integrating human and automated process annotations could play a vital role in constructing robust and efficient process supervision.</p>
<h1>7 CONCLUSION</h1>
<p>In this paper, we introduce a process-oriented math verifier called MATH-SHEPHERD, which assigns a reward score to each step of the LLM's outputs on math problems. The training of MATH-SHEPHERD is achieved using automatically constructed process-wise supervision data, thereby eradicating the necessity for labor-intensive human annotation. Remarkably, this automatic methodology correlates strongly with human annotations. Extensive experiments in both verification and reinforcement learning scenarios demonstrate the effectiveness of our method.</p>
<h2>REFERENCES</h2>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.</p>
<p>Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, and Huajun Chen. When do program-of-thoughts work for reasoning? arXiv preprint arXiv:2308.15452, 2023.</p>
<p>SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Tianyu Liu, and Baobao Chang. Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond. arXiv preprint arXiv:2310.02071, 2023.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>RÃ©mi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pp. 72-83. Springer, 2006.</p>
<p>DeepSeek. Deepseek llm: Let there be answers. https://github.com/deepseek-ai/ DeepSeek-LLM, 2023.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1049-1065, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.67. URL https://aclanthology.org/2023.findings-acl.67.</p>
<p>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.</p>
<p>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023.</p>
<p>Levente Kocsis and Csaba SzepesvÃ¡ri. Bandit based monte-carlo planning. In European conference on machine learning, pp. 282-293. Springer, 2006.</p>
<p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611-626, 2023.</p>
<p>Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 19274-19286. PMLR, 2023.</p>
<p>Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. M3it: A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387, 2023a.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315-5333, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.291. URL https://aclanthology.org/2023.acl-long. 291.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.</p>
<p>Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. Let's reward step by step: Step-level reward model as the navigators for reasoning. arXiv preprint arXiv:2310.10080, 2023.</p>
<p>OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.</p>
<p>Sarah Pan, Vladislav Lialin, Sherin Muckatira, and Anna Rumshisky. Let's reinforce step by step. arXiv preprint arXiv:2311.05821, 2023.</p>
<p>Joon Sung Park, Joseph Oâ€™Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pp. 1-22, 2023.</p>
<p>David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.</p>
<p>Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian Li. Restgpt: Connecting large language models with real-world applications via restful apis. corr, abs/2306.06624, 2023. doi: 10.48550. arXiv preprint arXiv.2306.06624.</p>
<p>Maciej Åšwiechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek MaÅ„dziuk. Monte carlo tree search: A review of recent modifications and applications. Artificial Intelligence Review, 56(3): 2497-2562, 2023.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. Making large language models better reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023b.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023c.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023d. URL https://openreview.net/ pdf?id=1PL1NIMMrw.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. arXiv preprint arXiv:2306.01693, 2023.</p>
<p>Heming Xia, Tao Ge, Furu Wei, and Zhifang Sui. Lossless speedup of autoregressive translation with generalized aggressive decoding. arXiv preprint arXiv:2203.16487, 2022.</p>
<p>Fei Yu, Anningzhe Gao, and Benyou Wang. Outcome-supervised verifiers for planning in mathematical reasoning. arXiv preprint arXiv:2311.09724, 2023a.</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023b.</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.</p>
<p>Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. Cumulative reasoning with large language models. arXiv preprint arXiv:2308.04371, 2023.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. Solving math word problems via cooperative reasoning induced language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4471-4485, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.245. URL https://aclanthology.org/2023.acl-long. 245.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_ exam&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
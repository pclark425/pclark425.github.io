<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7622 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7622</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7622</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-274597072</p>
                <p><strong>Paper Title:</strong> Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks</p>
                <p><strong>Paper Abstract:</strong> Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker can combine the data knowledge of multiple attackers to create a more effective attack model, which can be referred to cross-dataset attacks. Moreover, if knowledge can be extracted with the help of Large Language Models (LLMs), the attack capability will be more significant. In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and LLMs. The LLM is applied to process datasets with different data structures in cross-dataset attacks. Each attacker fine-tunes the LLM on their specific dataset to generate a tailored attack model. We then introduce a novel model merging method to integrate the parameters of these attacker-specific models effectively. The result is a merged attack model with superior generalization capabilities, enabling effective attacks not only on the attackers’ datasets but also on previously unseen (out-of-domain) datasets. We conducted extensive experiments in four datasets to demonstrate the effectiveness of our method. Additional experiments with three different GNN and LLM architectures further illustrate the generality of our approach. In summary, we present a new link stealing attack method that facilitates collaboration among multiple attackers to develop a powerful, universal attack model that reflects realistic real-world scenarios.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7622",
    "paper_id": "paper-274597072",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005161,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks</p>
<p>Faqian Guan 0000-0002-5701-8311
Member, IEEETianqing Zhu tqzhu@cityu.edu.mo 0000-0003-3411-7947
Wenhan Chang 
Member, IEEEWei Ren 0000-0001-8590-1737
Senior Member, IEEEWanlei Zhou 0000-0002-1680-2521</p>
<p>City University of Macau
999078MacaoChina</p>
<p>China University of Geosciences
430074WuhanChina</p>
<p>City University of Macau
999078MacaoChina</p>
<p>Zhongnan University of Economics and Law
430073WuhanChina</p>
<p>China University of Geosciences
430074WuhanChina</p>
<p>Harbin Institute of Technology
1982HarbinChina</p>
<p>Australian National University
CanberraAustralia, in</p>
<p>Institute of Data Science
City University of Macau
Macao SARChina</p>
<p>Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks
3B144C0687F09007D30BD93CE8EB48AB10.1109/TDSC.2025.3591543Received 7 December 2024; revised 13 July 2025; accepted 17 July 2025. Date of publication 22 July 2025; date of current version 4 November 2025. This work was supported by the FDCT under Grant 0080/2024/RIA2.Link stealing attackslarge language models (LLMs)graph neural networks (GNNs)privacy attacksmodel merging
Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications.Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities.Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers.However, we find that an attacker can combine the data knowledge of multiple attackers to create a more effective attack model, which can be referred to cross-dataset attacks.Moreover, if knowledge can be extracted with the help of Large Language Models (LLMs), the attack capability will be more significant.In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and LLMs.The LLM is applied to process datasets with different data structures in cross-dataset attacks.Each attacker fine-tunes the LLM on their specific dataset to generate a tailored attack model.We then introduce a novel model merging method to integrate the parameters of these attacker-specific models effectively.The result is a merged attack model with superior generalization capabilities, enabling effective attacks not only on the attackers' datasets but also on previously unseen (out-of-domain) datasets.We conducted extensive experiments in four datasets to demonstrate the effectiveness of our method.Additional experiments with three different GNN and LLM architectures further illustrate the generality of our approach.In summary, we present a new link stealing attack method that facilitates collaboration among multiple attackers to develop a powerful, universal attack model that reflects realistic real-world scenarios.</p>
<p>Alongside research into the applications of GNNs, some researchers also investigate privacy attacks targeting GNNs [2].</p>
<p>Similarly to privacy attacks in the image and text domains, attackers in GNN privacy attacks aim to extract sensitive information.In particular, GNNs are susceptible to a unique type of privacy attack called the link stealing attack [3], where attackers extract the connections between nodes of the training graph data by compromising the target model.For example, in finance, link stealing attacks can expose associations between certain users in transaction networks, enabling fraudsters to identify potential targets or establish chains of fraudulent transactions and account associations.In social networks, link stealing attacks can help attackers deduce relationships between nodes without direct data access.</p>
<p>Given the practical significance of link stealing attacks, numerous researchers have investigated these attacks on GNNs [3], [4], [5], [6], [7], [8].The primary method in link stealing attacks involves comparing the similarity between the posterior probabilities of different nodes to infer the existence of links [3], [4], [5].Other researchers infer the existence of links by observing changes in the posterior probabilities of nodes after introducing perturbations to the original graph data [6].However, previous studies have not explored the potential of leveraging the knowledge of multiple attackers to collaboratively train a more generalized and effective attack model.Furthermore, due to constraints such as privacy protections, commercial interests, and policy regulations, these attackers cannot share data, which complicates collaborative training efforts.Attacking multiple datasets required separate models for each dataset, resulting in substantial training time and resource consumption [9], [10].Developing a universal attack model that can simultaneously target multiple datasets would markedly reduce the need for time, resources, and computational infrastructure.In summary, conducting collaborative link stealing attacks by combining multiple attackers would be much more powerful.However, this novel attack presents two key challenges: r How to implement cross-dataset link stealing attacks.r How to aggregate knowledge from multiple attackers with- out sharing local data.To address the first challenge, we adopt large language models (LLMs) [11] to perform a cross-dataset link stealing attack.LLMs utilize the attention mechanism within the transformer architecture, which is particularly effective at processing variablelength inputs in parallel, allowing LLMs to adapt well to datasets with differing feature dimensions and lengths [12].Additionally, LLMs can capture subtle linguistic nuances, complex semantic structures, and intricate text patterns.These depth of understanding empower LLMs to achieve state-of-the-art performance across a range of tasks [13], [14], [15].Thus, by introducing LLMs into the development of a link stealing attack model, we aim to address the difficulties associated with cross-dataset attacks while also enhancing overall attack effectiveness.</p>
<p>To address the second challenge, we employ model merging techniques to aggregate knowledge from multiple attackers.Model merging combines the strengths of individual models to produce a single, more robust model.Specifically, each attacker independently trains a model based on their unique dataset.Instead of sharing data, attackers share only their model parameters, which are then merged to create a unified model.This approach allows multiple attackers to collaboratively enhance the effectiveness of link stealing attacks by integrating the strengths of each model, resulting in a universal link stealing attack model without compromising local data privacy.</p>
<p>In this paper, we propose a novel link stealing attack method that integrates LLMs and model merging techniques.These methods enable multiple attackers to collaborate and create a more effective attack model capable of performing cross-dataset attacks.First, we introduce the use of LLMs for link stealing attacks, leveraging their ability to process variable-length data and perform cross-dataset operations.To improve the LLM's performance in link stealing tasks, we design specific prompts that incorporate both textual features and node posterior probabilities.This allows each attacker to independently fine-tune their LLM-based model using their own dataset and knowledge.Next, we propose a novel model merging approach to combine the LLM-based attack models developed by multiple attackers and construct a universal link stealing attack model.This approach involves three key steps: LLM parameter dropping, parameter selection, and parameter merging.These steps facilitate the integration of individual attackers' knowledge, resulting in a generalized attack model.Moreover, we demonstrate that the model merging method can effectively target out-of-domain data.These data differ in distribution from the training set, and the attackers possess no prior knowledge of them.This highlights the enhanced real-world applicability of our approach.Finally, we validate the effectiveness of our approach through thorough theoretical analysis and extensive experimental evaluation.</p>
<p>In summary, our paper contributes the following.</p>
<p>r We propose a novel link stealing attack method that fo- cuses on combining the knowledge of multiple attackers to collaboratively create a more capable and generalized attack model.This approach enables the development of more realistic and effective attacks, better aligned with real-world scenarios.</p>
<p>r Our method introduces the use of LLMs for link stealing attacks, leveraging their ability to handle cross-dataset scenarios and achieve improved attack performance.</p>
<p>r We propose a novel model merging method to integrate knowledge from multiple attackers, resulting in a universal attack model capable of executing attacks across diverse datasets, including previously unseen (out-ofdomain) datasets.</p>
<p>II. PRELIMINARY</p>
<p>A. Notations</p>
<p>We denote G as the graph data and X as the node features in the graph.N and T represent the numerical and textual features of nodes, respectively.Nodes in the graph are represented as u and v, with N(v) indicating the neighboring nodes of node v. P denotes the posterior probability of nodes obtained from the target model.H represents the hidden state of a feature vector, T refers to the target model, and A refers to the attack model.Y and Ŷ denote the ground truth and predicted labels of nodes, respectively.Link and Unlink indicate the presence and absence of a link between nodes.θ represents the parameters of the pre-trained LLM, while δ denotes the parameter updates in the LLM model.λ is the weight assigned to a model during the merging process, and d is the dropout probability.W and B represent the neural network parameters and bias term, respectively, while σ is the activation function.Lastly, γ is the scaling factor used in the model merging process.The notations used throughout this paper are summarized in Table I.</p>
<p>B. Graph Neural Networks</p>
<p>Graph Neural Networks (GNNs) have emerged as a powerful framework to process data with graph structures.GNNs have demonstrated impressive results in various graph-based applications by learning effective representations of both node features and relational information within the graph.The central mechanism of GNNs is the message passing, or neighborhood aggregation, process, where the information is propagated through the graph structure.Each node updates its representation by aggregating the features of its neighbors and combining them with its own features.The updated representation of a node in the k-th layer of the GNN can be expressed as follows:
h (k) v = UPDATE (k) h (k−1) v , e (k) v e (k) v = AGG (k−1) h (k−1) u : ∀u ∈ N(v) ∪ v (1)
where N(v) denotes the neighbors of node v. h
(k) v
is the representation of node v after the k-th layer update, while h</p>
<p>C. Large Language Models</p>
<p>Large Language Models (LLMs) [11] are sophisticated artificial intelligence models specifically designed for natural language processing tasks.These models use deep learning architectures, often based on transformer models, to understand and generate human-like language by capturing complex linguistic patterns, context, and nuanced meanings in text.They are termed "large" because they are trained on extensive datasets and contain an exceptionally high number of parameters, often reaching billions or even trillions.The more parameters a model has, the more information it can encode, which enhances its performance in a wide range of tasks.However, deploying and managing these large-parameter models in practical applications can be challenging because of their substantial computational requirements.</p>
<p>D. Link Stealing Attack</p>
<p>A link stealing attack is a privacy attack that targets inference of private links between nodes within graph-structured data [2].In link stealing attacks, attackers aim to infer the existence of links between nodes by leveraging both node knowledge and response information obtained from accessing the target model.Specifically, as illustrated in Fig. 1, a service provider trains a model using company data and deploys it for user access, typically to serve commercial purposes.This deployed model becomes the target of link stealing attacks.Attackers first form node pairs by pairing nodes whose connection status they wish to infer.By querying the target model, they then obtain the posterior probabilities of these node pairs.Using the original node information alongside the acquired posterior probabilities, attackers construct a link stealing attack method.This method enables attackers to infer the presence or absence of links between node pairs, posing a significant privacy threat to the service provider.</p>
<p>Link Stealing Attacks vs. Link Prediction: Link stealing attacks share certain similarities with link prediction tasks in GNNs, as both aim to infer potential or existing links between nodes.However, they differ substantially in both motivation and methodology.Below, we summarize the key similarities and differences between the two tasks.</p>
<p>r Similarities: Both tasks aim to determine whether a link exists between two nodes in a graph.In both scenarios, node embeddings, graph topology, and node features can be used as input to infer unknown or hidden links.</p>
<p>r Differences: The primary difference between the two tasks lies in their underlying motivation.Link prediction is a benign and authorized task, commonly used for graph completion, recommendation systems, and other legitimate downstream applications.In contrast, link stealing is a malicious and adversarial activity that aims to uncover private or sensitive links which the model owner did not intend to disclose.While link prediction enables useful and approved functionalities, link stealing violates data privacy and poses serious security risks, such as revealing undisclosed relationships in social networks.Moreover, the methodologies used in these tasks differ significantly.Link prediction models are typically trained using node features and labeled links within a supervised learning framework.In contrast, link stealing attacks often exploit response information obtained through queries to the target model, combined with auxiliary knowledge of node attributes available to the attacker.</p>
<p>E. Model Merging</p>
<p>With the rise of LLMs, researchers have increasingly explored ways to leverage different LLMs trained on separate datasets or tasks through model merging [16].This approach aims to harness the distinct strengths of each model, creating a more versatile and universal LLM.Model merging combines two or more trained models into a single, unified model that incorporates the benefits of each original model.By merging model parameters, this technique enables the creation of a more capable model without requiring data sharing, thus safeguarding data privacy.One straightforward method for model merging is parameter averaging.Given n models with parameters θ 1 , θ 2 , . . ., θ n , in the parameter averaging method, the parameters of the merged model can be calculated as:
θ merge = 1 n n i=1 θ i (2)
Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>III. PROBLEM DEFINITION</p>
<p>Attack Goal: In a link stealing attack, attackers use the attack method A to infer whether there are links between nodes based on response information R obtained by accessing the target model T. Specifically, the attacker aims to determine whether there is a link between nodes u and v. First, the attacker inputs the nodes u and v into T for query.The target model T then outputs R based on the features of nodes u and v.The attacker analyzes the attack method A and infers whether there is a link between the nodes u and v by combining R with the features of the nodes u and v.This attack can be formally defined as follows.</p>
<p>A : {u, v}, {r u , r v } → (Link, U nlink)
T : {u, v} → {r u , r v } (3)
In this paper, r u and r v represent the posterior probabilities of node predictions for nodes u and v provided by the target model T. Link indicates that there is a link between u and v, while Unlink indicates that there is no link.</p>
<p>Target Model: The target model, T, is also known as the victim model in link stealing attacks.The service provider trains T on the target dataset and deploys it on the Internet for user access.Attackers then access the deployed model T and obtain its response information R.This information is used to steal node links in the target dataset.In this paper, T is a node classification model.</p>
<p>2) Attack Setting: In this paper, we study link stealing attacks by simulating a real-world scenario.Multiple attackers, or a multinational company with different departments, aim to jointly train a more efficient and robust attack model.However, the attackers do not want to share the data they have collected at a high cost with others.</p>
<p>Attacker Knowledge: Each attacker or department has its own independently collected dataset, denoted as G = {G 1 , G 2 , . . ., G n }.These datasets contain links between certain nodes.However, attackers do not have knowledge of the structure or parameters of the target model.They can only interact with the model by sending nodes and obtaining the posterior probabilities of the corresponding nodes.This scenario corresponds to the most threatening black-box setting in privacy attacks [2].</p>
<p>In joint training of the attack model, data sharing among attackers is avoided due to the high cost of data collection.Instead, attackers share only their trained model parameters for collaborative training.Specifically, each attacker uses their own dataset and the derived posterior probabilities to train a link stealing attack model.By applying a suitable model merging method, the strengths of each individual model are integrated, achieving the objective of joint training without the need for data exchange.</p>
<p>IV. METHODOLOGY</p>
<p>A. Attack Overview</p>
<p>In this paper, we study link stealing attacks, where attackers target a deployed model to infer links from its training dataset.An overview of the proposed link stealing attack is illustrated in Fig. 2. The service provider invests substantial resources to collect the target dataset and train the target model, which is then deployed online for user access to fulfill commercial or profitable objectives.</p>
<p>In our proposed link stealing attack method, we focus on two key steps: Introduce LLM and Model Merging.These steps enables cross-dataset attacks using LLMs and combine multiple attackers' knowledge to create a more efficient, generalized model.Specifically, in Attacker Step 1: Introducing LLM, attacker possesses unique data knowledge, including node features, and can access publicly available pre-trained LLMs.As part of the attack process, attackers act as users by sending their node features to the target model and obtaining posterior probabilities for these nodes.Leveraging the obtained posterior probabilities, the original node features, and a pre-trained LLM, each attacker fine-tunes their LLM to develop an individual attack model capable of performing cross-dataset attacks.In Attacker Step 2: Model Merging, to further enhance the attack's effectiveness, attackers share their fine-tuned LLM parameters and apply a novel model merging technique to integrate their knowledge.This results in a unified and efficient attack model with superior generalization capabilities.</p>
<p>In summary, we introduce LLMs to enable cross-dataset attacks and propose a novel model merging method to integrate the knowledge of multiple attackers.The following sections provide a detailed explanation of the use of LLMs for link stealing attacks and the proposed model merging technique.</p>
<p>B. Introducing LLM for Cross-Dataset Link Stealing Attacks</p>
<p>We introduce LLMs to perform cross-dataset link stealing attacks.LLMs leverage the attention mechanism inherent in their transformer architecture, which is particularly effective at handling variable-length data [11].This flexibility allows LLMs to process diverse features across different datasets, making them highly suitable for cross-dataset link stealing tasks.</p>
<p>To ensure the success of model merging, we standardize the methods and model architectures used by participating attackers for training their LLMs.The process for each standardized attacker to train the LLM mainly involves prompt design and fine-tuning of the LLM.</p>
<p>Prompts are a common mechanism in LLMs and have become the mainstream paradigm for adapting LLMs to specific tasks.With the help of prompts, LLMs can handle various complex tasks.By providing specific prompts to the LLM as input, we can guide the model towards producing the desired output and obtaining the expected results.Similarly, to successfully execute the link stealing attack task using an LLM, designing a customized prompt for this task is crucial.</p>
<p>In many natural language processing tasks, a well-designed prompt specific to the task allows an LLM to produce the expected results without the need for fine-tuning.However, in certain tasks, such as link stealing attacks, the original LLM may not generate the desired results.For example, when attempting a link stealing attack, the LLM's response might be as follows:</p>
<p>Based on the information provided, it is not possible to determine if the two papers have a link.</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.Fig. 2. Overview of the proposed link stealing attack method.The upper section illustrates the service provider offering the target model for user access.The lower section details the attackers' process of creating attack models using our proposed method.In Attacker Step 1, Attackers introduce LLMs and leverage their respective node features to fine-tune and develop multiple LLM-based link stealing attack models.In Attacker Step 2, the attackers, through model merging techniques, combine the strengths of these individual models to create a more robust merged LLM model for executing link stealing attacks.</p>
<p>To effectively integrate link stealing attacks with an LLM, it is necessary to adjust the model.Task-specific prompt finetuning is an effective technique for tailoring an LLM to specific tasks [17].We employ this technique to optimize the LLM for successful execution of the link stealing attack.</p>
<p>Next, in this paper, we will detail our approach to Prompt Design and Fine-tuning of LLM to obtain multiple attack models.</p>
<p>1) Prompt Design: In prompt design, attackers first access the target model using the node features to obtain the posterior probabilities of the corresponding nodes.To facilitate the inference of links between nodes, we group all nodes for which we need to infer the existence of a link into node pairs.The features of each node pair include the node features X and the posterior probabilities R of the two nodes, as shown below:
A : {(x v , r v ), (x u , r u )} Node Pair , → (Link, U nlink) (4)
where x v and x u represent the node features of nodes v and u, including textual descriptions.Meanwhile, p v and p u represent the posterior probabilities generated by the target model for nodes v and u, respectively.Attackers can directly input these node pairs into the attack model to determine whether a link exists between the two nodes in the pair.</p>
<p>Using the obtained node pairs, we design prompts for the LLM.Each prompt consists of three main components: i) Node pair information: This includes the node features with textual descriptions available to the attacker, as well as the posterior probabilities obtained from the target model.ii) Human question: A natural language query formulated to perform a link stealing attack based on the given node pair information.iii) LLM response: The output generated by the LLM in response to the human question, indicating whether a link exists between the nodes in the pair.We visualize a prompt example for link stealing attacks, as shown in Fig. 3.This prompt is based on a citation dataset.The node pair information includes the titles and abstracts of two papers, along with the posterior probabilities obtained from the target model.The human question asks the LLM to determine whether a link exists between the two papers based on the information provided.The LLM's response indicates the model's answer: Yes signifies that a link exists between the two papers, while No indicates that no link.</p>
<p>2) Fine-Tuning of LLM to Obtain Multiple Attack Models: The original LLM cannot effectively complete the link stealing attack task.After designing specific prompts for link stealing attacks, each attacker uses the designed prompt to package the knowledge of individual attackers.Then, each attacker finetunes the LLM using these prompts to obtain their individual model capable of performing link stealing attacks.As illustrated in Fig. 4, the fine-tuning process for each attacker can be decomposed into the following four steps:</p>
<p>r Training Data: The training dataset refers to each attacker data used to fine-tune the LLM but not share.In this paper, the data consists of each attacker well-designed prompts, which are composed of node pair information.This section also covers the tokenization of the training Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.dataset.Tokenization is the process of breaking down text into smaller units known as tokens, which enables the LLM to process and understand the text more effectively [18].</p>
<p>r Architecture Supporting Cross-Dataset Attacks: LLMs typically employ deep neural network architectures, such as the Transformer [19] architecture.Transformers are designed to process sequences of arbitrary lengths, a flexibility enabled by their reliance on attention mechanisms and positional encodings rather than a fixed network structure tied to input size.This capability makes LLM well-suited for cross-dataset link stealing attacks involving datasets with varying data lengths.Common LLMs include Vicuna-7B, Vicuna-13B [20], and LongChat [21].</p>
<p>r Generation: Generation refers to the LLM producing text in response to an input prompt during training or inference.</p>
<p>The generated text serves as the answer to the prompt.In this paper, the generation specifically involves determining whether there is a connection between the node pairs in the prompt, resulting in a Yes or No answer.</p>
<p>r Fine-tuning: Fine-tuning refers to the process of further training a pre-trained model on a small, task-specific dataset to adapt it to a particular application scenario.In the context of link stealing attacks, each attacker compares the predicted output Ŷ generated by the LLM with the true label Y to compute the loss and update the model.Specifically, each attacker employs the cross-entropy loss function L CE , as defined below:
ŷ = LLM(Prompt{(x v , p v ), (x u , p u )}) L CE (y, ŷ) = − [y log(ŷ + (1 − y) log (1 − ŷ)] (5)</p>
<p>C. Model Merging to Combine Multiple LLM-Based Link Stealing Attack Models</p>
<p>In this paper, to enable each attacker to collaboratively train a more comprehensive link stealing attack model without exchanging data, we propose a novel model merging method.This approach allows us to harness the strengths of each LLMbased link stealing attack model-trained independently on each attacker's dataset-to create a more effective, unified model.Notably, our method achieves this enhanced performance without requiring data sharing or retraining.The merging process consists of three main steps: Drop, Elect, and Merge.Next, we detail each step of our approach.In this section, we denote θ as the pre-trained LLM parameters shared among all attackers, and {θ 1 , θ 2 , . . ., θ n } as the parameters of the LLM attack models obtained by each attacker through fine-tuning based on their respective knowledge.We define the δ t parameter as the difference between the parameters of each attack model and the base model, expressed as δ t = θ t − θ.</p>
<p>1) Drop: As is well known, many redundant parameters have little to no impact on the performance of a model.LLMs also face the problem of parameter redundancy.In other words, directly pruning these redundant parameters results in minimal decline in model performance.Therefore, we first prune the LLM parameters before merging.</p>
<p>Inspired by [22], [23], we employ a pruning technique called Drop to reduce the redundant parameters of the LLM and enhance its generalization ability.</p>
<p>Drop differs from common random pruning methods, such as Dropout [22].We perform pruning by removing parameters with smaller delta magnitudes, as shown in Fig. 5. First, we sort the delta parameters {δ 1 , δ 2 , . . .δ n } of the LLM to be merged according to their magnitudes.Based on the sorting results, we assign different drop probabilities, with those having smaller magnitudes receiving higher drop probabilities.The formulation is as follows:
{r 1 , r 2 , . . . , r n } = rank ({δ 1 , δ 2 , . . . δ n }) Δ i = n * r i d i = d min + Δ i (6)
where d min is assigned to the maximum magnitude delta parameter as the minimum probability of dropping, defined as
d min = d − 2 .
Here, d is the average drop probability, and represents the maximum change in drop probability based on magnitude.</p>
<p>By sorting delta magnitudes, we obtain different drop probabilities d i .Next, we use d i to perform the delta sampling step:
m i ∼ Bernoulli(d i ) δi = (1 − m i ) δ i δi = δi 1 − d i (7)
where m i = 1 indicates that δ i is dropped, and m i = 0 indicates that it remains.Similar to Dropout [22], to maintain consistency in the final predictions after dropping deltas, we rescale the remaining deltas.Rescaling according to the inverse proportion</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>of the drop 1 − d i can restore the predicted value to its proximity before the drop.We will provide a detailed proof in the subsequent section.</p>
<p>2) Elect: To further reduce the interference from abnormal delta parameters δ during merging, we perform selective filtering of the delta parameters.We employ a masking technique, where a valid position is represented by 1 and an unusable or filled position is represented by 0. By using masks, we can mitigate the influence of anomalies during merging and enhance the effectiveness of the model integration.Specifically, we first calculate the sign of the sum of delta parameters of all models at position k to determine the dominant direction: S = sgn( T t=1 δt k ).We then apply the mask to the parameters at position k.The mask value is set to 1 when the sign of each model's increment aligns with the sign of the overall sum; otherwise, it is set to 0. The formulation is as follows:
mask = 1 if sgn( δt k ) = S; 0 else (8)
where T is the number of LLM models being merged.After obtaining the mask, we apply it to the delta parameters, resulting in δ t k = mask ( δt k ). 3) Merge: When fusing multiple models, previous methods often rely on calculating the average value of several models for merging [23], [24].Although this approach can achieve merging, it does not take into account the performance of each individual model, which can lead to inferior models negatively impacting the performance of the merged model.</p>
<p>To address this issue, we propose a new merging strategy that considers each model's accuracy by assigning different weights, denoted as λ, during the merging process.Higher weights are given to attack models trained on datasets that are more distinct and harder for models trained on other datasets to predict effectively.Specifically, we calculate the accuracy of each LLM-based link stealing attack model trained on a single dataset and evaluated across multiple datasets.By comparing the maximum and minimum accuracies on each dataset, we observe that the maximum accuracy typically occurs when the training and testing datasets are the same, whereas the minimum accuracy is often observed when they differ.</p>
<p>Next, we analyze the difference between the maximum and minimum accuracies.A larger difference indicates that a dataset is more unique and shares fewer similarities with other datasets, making it harder for models trained on other datasets to transfer effectively to it.During model merging, we can increase the merging weights for attack models trained on such distinctive datasets, enhancing the overall generalization ability of the merged model.Finally, we apply the Softmax function to the differences between maximum and minimum accuracies to establish the final weights.This method assigns higher weights to attack models trained on datasets that are challenging to predict and lower weights to those trained on datasets that are easier to predict, thus producing a merged attack model with stronger generalization capabilities.This process can be formulated as:
λ t = exp(Max(Acc t ) − Min(Acc t )) exp(Max(Acc) − Min(Acc))(9)
Fig. 6.Illustrative example of the weight calculation process for λ.</p>
<p>We simulate an example to illustrate the calculation process of λ, as shown in Fig. 6.Each attacker initially trains three attack models on three separate datasets.These models are then evaluated on the same three datasets to obtain their link stealing accuracy values.Next, we calculate the difference in attack accuracy for each dataset by subtracting the minimum accuracy from the maximum accuracy, yielding difference values of 0.09, 0.04, and 0.05.By applying the Softmax function to these values, we derive the λ weight values for each attack model during merging: 0.5, 0.22, and 0.28, respectively.</p>
<p>Using the obtained weight λ, we perform model fusion.Let δ merge denote the merged delta parameter, as shown below:
δ merge k = T t=1 λ t * δ t k (10)
Finally, we obtain the merged model parameters as
θ merge k = θ + δ merge k
. By assigning different weights λ to models trained on different datasets during the merging process, we create a more reasonable and generalization-capable merged model.This weighted approach ensures that models with better performance contribute more significantly to the final merged model, improving its overall effectiveness and adaptability.</p>
<p>D. Link Stealing</p>
<p>After obtaining the merged model through model merging, we use it to carry out link stealing attacks on the target dataset.Compared to an attack model trained by a single attacker with limited knowledge, the merged model has stronger generalization capabilities, allowing it to conduct attacks across multiple datasets more effectively.</p>
<p>The attack process proceeds as follows: First, the attacker sends the features of the two nodes targeted for link stealing to the target model, obtaining the posterior probabilities for these nodes.Next, using a specially designed prompt that combines the node features and posterior probabilities, the attacker generates input features for the link stealing attack model.Finally, the merged model leverages this prompt to infer whether a link exists between the two nodes in question, effectively conducting the link stealing attack.</p>
<p>The algorithm 1 presents our proposed link stealing attack method and outlines the process for launching attacks.In Step 0, the attacker accesses the target model to obtain the posterior probabilities of the nodes.Steps 1-6 describe the construction process of our proposed attack method.Specifically, in Step
θ merge = θ + T t=1 λ t • δ t 16:
Step 7: Link stealing attack 17: Link or U nlink = LLM(θ merge , Prompt) 18: return Link or U nlink fine-tunes the original LLM using these prompts to develop their respective attack models.In Step 3, the attackers calculate the magnitude of the changes in the model parameters before and after fine-tuning.In Steps 4 and 5, based on these magnitudes, attackers perform drop, scaling, and elect operations on their model parameters.In Step 6, the attacker merges the processed changes in the model parameters of each model to obtain the final parameters of the merged model.Step 7 is the execution phase of the link stealing attack, where the attacker carries out the attack using the merged model and a prompt containing information on the two nodes for which the attacker seeks to steal the link.</p>
<p>E. Theoretical Analysis</p>
<p>In this section, we provide a theoretical analysis to explain why the introduction of LLMs facilitates cross-dataset attacks.Additionally, we examine the impact of the Drop and Scaling techniques in model merging on the overall effectiveness of the attack.</p>
<p>1) LLMs Enable Cross-Dataset Link Stealing Attacks: An LLM is trained on extensive and diverse datasets, encompassing textual and structural patterns across various domains.Consequently, its learned representation function, LLM(•), exhibits a robust ability to generalize between datasets with different distributions.Formally: LLM(X) = F(Trans(X)) (11) where Trans(•) refers to the transformer architecture [19]
(X) = σ(WX + B) (12)
where W is a learned weight matrix of size k × n (for ndimensional inputs).If X ∈ R m with m = n, the matrix multiplication WX becomes improper.Thus, an MLP cannot handle the issue of variable data lengths.Additionally, B represents the bias term, and σ is the activation function.</p>
<p>It should be noted that LLMs have achieved state-of-the-art performance in various fields [11].By effectively leveraging the textual features of the nodes, LLMs can significantly enhance the performance of link stealing attacks.In previous link stealing attacks, methodological limitations prevented researchers from using the textual and structural features of the nodes.Therefore, introducing LLMs for link stealing attacks offers the potential to surpass traditional methods in terms of performance.</p>
<p>Based on the analysis and assumptions above, we conclude that introducing LLMs enables cross-dataset attacks and markedly improves attack performance.</p>
<p>2) Drop and Scaling Maintain Attack Effectiveness: Here, we analyze the efficiency of the Drop and Scaling techniques in merging multiple models.First, let us define the model parameters as θ and the corresponding deltas as δ.Let X be the input embedding vector and H represent the output embedding.The model's predictions can be formulated as:
E[h] = E n i=1 (θ i + δ i ) x i = n i=1 x i E[θ i ] + n i=1 x i E[δ i ] = n i=1 x i θ i + n i=1 x i δ i = h + Δh(13)
Let d denote the probability of drop in the drop and scaling process and let γ represent the scaling factor.After applying Drop and Scaling to the model's parameters, the predictions can be formulated as:
E[ ĥ] = E n i=1 θ i + δi x i = n i=1 x i E[θ i ] + n i=1 x i E[ δi ] = n i=1 x i θ i + n i=1 x i (1 − d i ) * γ * δi = h + n i=1 x i * (1 − d i ) * γ * δ i (14)
Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>TABLE II DATASET STATISTICS</p>
<p>By setting γ = 1 1−d i , we ensure E[ ĥ] = E[h], thereby concluding that the expected value of the rescaled output matches the original prediction before applying the Drop and Scaling procedure.This ensures the consistency of the model's predictions.Specifically, we can express this as follows:
E[ ĥ] = h + n i=1 x i * δ i E[ ĥ] = h + Δh E[ ĥ] = E<a href="15">h</a>
Based on this theoretical analysis, we can infer that applying Drop and Scaling to the LLM in a link stealing attack does not impair the attack's performance.</p>
<p>V. EXPERIMENTAL EVALUATIONS</p>
<p>A. Experimental Setup 1) Datasets:</p>
<p>To verify the effectiveness of the proposed method, we conducted experiments on four datasets: Cora, Citeseer, Pubmed [25], and Ogbn-arxiv [26].Cora, Citeseer, and Pubmed are widely used in GNN research, allowing effective comparisons with existing methods.We included Ogbn-arxiv, a larger dataset, to align with the current trend of increasing dataset sizes in research.Table II provides an overview of these dataset statistics.</p>
<p>2) Dataset Configuration: In link stealing attacks, we assume that attackers have knowledge of a varying number of links based on the dataset size.As shown in Table II, the Attack Links column specifies the number of links known to the attacker.Specifically, attackers are aware of 2,000 links in the Cora and Citeseer datasets, 5,000 in Pubmed, and 30, 000 in Ogbn-arxiv.To ensure fairness during model training, an equal number of unlinked edges are randomly selected for inclusion in the training process.</p>
<p>3) Models: To explore the effectiveness of our proposed method, we used Vicuna-7B [20] to conduct link stealing attacks against the Graph Convolutional Network (GCN) [25], a widely adopted Graph Neural Network (GNN) model.Vicuna-7B is also a prominent large language model (LLM) used for various tasks.</p>
<p>Additionally, to assess the versatility of our method, we extended our experiments to include multiple GNN models and LLM architectures.The GNN models tested included GCN, Graph Attention Networks (GAT) [27], and GraphSAGE (SAGE) [28], which are popular in GNN research.Similarly, we evaluated the method using several LLM architectures, including Vicuna-7B, Vicuna-13B [20], and LongChat [21].This comprehensive testing demonstrates the robustness and generalizability of our approach across a range of models.</p>
<p>4) Implementation Details: In our model implementation, to ensure stable convergence during training, we adopt configurations consistent with prior work [29], [30].Specifically, all LLMs are configured with more than 32 transformer layers, 32 attention heads, and 4,096-dimensional embeddings.We use a batch size of 2, a learning rate of 2e −5 , a warmup ratio of 3e −2 , and a maximum input length of 2,048 tokens.The training process runs for 3 epochs.All GNN models consist of 2 layers, with ReLU applied as the activation function in the hidden layer and Softmax used in the output layer.For GNN training, we use 1,600 epochs and a learning rate of e −3 .All experimental results reported in this paper are averaged over multiple runs to ensure statistical reliability and reduce the impact of random fluctuations.</p>
<p>5) Evaluation Metric:</p>
<p>We use Accuracy and F1 score to evaluate the quality of the model.Accuracy refers to the proportion of samples correctly predicted by the model out of the total number of samples.It reflects the overall prediction accuracy, i.e., the ratio of correct predictions to all predictions.The F1 score is the harmonic mean of precision and recall, providing a balanced measure of the model's performance, especially useful for imbalanced datasets.</p>
<p>B. Evaluation of the Attack Model Trained Under a Single Dataset</p>
<p>We first explore the effectiveness of our proposed LLM-based Link Stealing Attacks in scenarios where the attacker only has access to links from a single dataset.In this setting, the attacker possesses partial link information from a single dataset and does not know the links in other datasets.</p>
<p>1) Effectiveness of LLM-Based Link Stealing Attacks: Table III presents the experimental results of our proposed LLM-based link stealing attack method across four datasets: Cora, Citeseer, Pubmed, and Ogbn-arxiv.In the table, the Feature method indicates that the attacker uses node features alone to carry out the attack.PP represents attacks carried out using posterior probabilities of the target model.PP+Feature combines both node features and posterior probabilities for the attack.LLM (Our) refers to the results obtained by our proposed LLM-based link stealing attack method, trained on a single dataset.</p>
<p>As shown in the table, the LLM-based link stealing attack method proposed in this paper outperforms previous methods in both accuracy and F1 score.Specifically, our method improves accuracy by at least 3% and F1 score by at least 2% across the datasets.On the Pubmed dataset, the improvement is most significant, with a 7% increase in both accuracy and F1.Additionally, on the Ogbn-arxiv dataset, our method achieves remarkable results, with an accuracy of 97.48% and an F1 score of 97.49%.</p>
<p>To provide a more intuitive understanding of the superiority of our method, we visualized the comparison results in Fig. 7. From both Table III and Fig. 7, it is clear that the LLM-based link   stealing attack method we proposed significantly outperforms previous approaches.This shows that our method effectively combines the textual features of the nodes with the traditional link stealing attack features, thus improving the success rate of the attacks.</p>
<p>2) Evaluation of Cross-Dataset Link Stealing Attacks: Previous link stealing attack methods [3], [5] struggle to perform cross-domain dataset attacks because the attack features (posterior probabilities) differ in dimensions across datasets.Consequently, a single model can only carry out attacks on a single dataset.In contrast, the LLM-based link stealing attack method handles variable-length data features effectively, overcoming this limitation and enabling cross-dataset attacks.To test this, we applied the LLM attack model trained on a single dataset to attack all datasets.The experimental results are shown in Fig. 8.</p>
<p>As shown in the figure, the LLM-based link stealing attack model, trained on a single dataset (Cora, Citeseer, Pubmed, or Ogbn-arxiv), can successfully attack all four datasets simultaneously.Notably, in some cases, attacks using a model trained on a different dataset outperform previous non-LLM methods trained and tested on the same dataset.For instance, the LLM attack However, due to the inherent differences among various datasets, using an attack model trained on one dataset to attack another with significant differences may not yield optimal results.For example, the attack model trained on Cora achieves 90.34% accuracy on Cora but only 77.3% on Pubmed, which is clearly a suboptimal performance.To address this issue, this paper introduces a model merging approach, enabling the creation of a comprehensive attack model with strong generalization capabilities.This method allows for joint training of the model without requiring multiple attackers to interact with different datasets, ultimately enhancing the effectiveness of the attack across diverse datasets.</p>
<p>C. The Allocation of the Weight Factor λ During Model Merge</p>
<p>In Section IV-C, we discussed that during model merging, different weights are allocated based on the varying accuracy of each attack model.Table IV presents the accuracy results of the LLM-based method when trained on a single dataset-Cora, Citeseer, Pubmed, or Ogbn-arxiv-and evaluated across all four datasets.In this table, Cora-LLM refers to the attack model trained using Cora as the training set.These accuracy scores guide the weight allocation, allowing the final merged model to maximize performance by drawing on the strengths of the top-performing models for each dataset.</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.To determine weight allocation during model merging, we first identify the maximum accuracy for attacks on each dataset.Generally, an attack model performs best when its training set matches its testing set.Table IV shows the highest accuracy rates for attacks on Cora, Citeseer, Pubmed, and Ogbn-arxiv, achieved when each dataset was also used as the training set, reaching values of 90.34%, 93.26%, 94.08%, and 97.48%, respectively.Next, we find the minimum attack performance on each dataset, allowing us to compute the range (MAX-MIN).This range reveals variations in attack difficulty across datasets.For instance, the Pubmed dataset achieves 94.08% accuracy under the Pubmed-trained LLM attack model, while the same model trained on Cora yields only 77.3% on Pubmed-a difference of 16.78%.This highlights Pubmed's challenge level, as models trained on other datasets struggle to achieve comparable performance.Therefore, in model merging, we can assign a higher weight to the Pubmed-LLM model, as it effectively addresses a more challenging dataset.Conversely, the Citeseer dataset exhibits more consistency across models: accuracy remains around 90.98%, regardless of the training dataset, showing only a 2.28% difference from the Citeseer-trained model.This suggests Citeseer's lower attack complexity, and therefore, the Citeseer-LLM weight can be reduced.</p>
<p>TABLE V RESULTS OF DIFFERENT MODEL MERGING METHODS FOR LINK STEALING ATTACKS</p>
<p>By using MAX-MIN values, we assign higher weights to models trained on more challenging datasets, enhancing overall merge model performance, including generalization for unseen data.Finally, applying a Softmax operation to the MAX-MIN results yields final weights of 0.28, 0.06, 0.46, and 0.20, as shown in Fig. 9, optimizing the robustness of the merged model across datasets.Importantly, to mitigate the influence of noise introduced by randomness, both the MAX and MIN values used in this process are averaged over multiple experimental runs.</p>
<p>D. Effectiveness Analysis of the Proposed Merging Method</p>
<p>Here, we examine the effectiveness of our proposed model merging method.We demonstrate its advantages by comparing it with established merging methods, including Ties [31], Dare [24], and Della [23].The experimental results are presented in Table V.</p>
<p>As shown in Table V, our method achieves superior merging effectiveness in link stealing attacks compared to other approaches.The Mean method, the simplest merging technique, averages the parameters of the models, initially facilitating model merging with an accuracy of 89.83% and an F1 score of 89.87%, demonstrating the viability of link stealing attacks for model merging.However, it shows significant performance loss, especially on the Pubmed and Ogbn-arxiv datasets, where accuracy drops exceed 6%.</p>
<p>Advanced methods like Ties, Dare, and Della improve merging performance over Mean, particularly benefiting datasets with lower attack accuracy, such as Cora and Citeseer.For instance, Della_ties achieves accuracy close to that of individual attack models trained specifically on Cora and Citeseer.However, similar to Mean, these methods experience a 1%-3% accuracy decrease on Pubmed and Ogbn-arxiv.</p>
<p>In contrast, our method more effectively preserves prediction performance across all datasets.Even for Pubmed and Ogbnarxiv, where baseline merged models perform relatively well, our method maintains competitive performance, with only a 0.8% decrease on Pubmed and 1.4% on Ogbn-arxiv.On average, across the four datasets, our method achieves the highest overall accuracy of 93.33% and an F1 score of 93.40%, surpassing all existing methods.To visually illustrate the superiority of our method, we provide a comparative column chart of all existing methods in Fig. 10.</p>
<p>E. Performance Analysis of the Merging Method Using Different Parameters</p>
<p>In model merging, two critical hyperparameters are involved.The first is p, which represents the average drop probability, and the second is , indicating the maximum allowable variation in drop probability based on parameter magnitude.Here, we Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.explore how these two hyperparameters affect the performance of model merging.</p>
<p>1) Effect of Varying Drop Probability P :</p>
<p>To analyze the impact of varying drop probabilities p on the merged model's performance, we conducted experiments with p = [0.1,0.3, 0.5, 0.7, 0.9], as illustrated in Fig. 5.The accuracy and F1 scores presented in the figure represent both the average and individual performance of the merged model across four datasets.</p>
<p>From the perspective of average performance, Fig. 5 shows that both excessively low and excessively high drop probabilities negatively affect the performance of the merged model.When the drop probability p is too low, such as at p = 0.1, model merging fails to achieve optimal performance due to parameter redundancy.LLMs contain numerous finely tuned parameters, leading to significant redundancy [24].If these redundant parameters are not pruned before merging, they can skew the parameter calculations and ultimately degrade the performance of the merged model.</p>
<p>Conversely, when the drop probability is too high, such as p = 0.7 or p = 0.9, an excessive number of parameters are removed.This aggressive pruning eliminates some essential parameters, which also leads to performance degradation.Based on these observations, we illustrate this trade-off in Fig. 11(a).Selecting an appropriate drop probability p is therefore essential for achieving optimal model merging outcomes.In this paper, unless otherwise specified, we set p = 0.5, as it yields the best overall performance.</p>
<p>It is worth noting that on smaller-scale graphs, such as those in the Cora and Citeseer datasets, the best performance is observed at lower drop probabilities, specifically p = 0.1 or p = 0.3.This suggests that for tasks involving smaller graphs, moderately reducing the drop probability can lead to improved results.However, overall, setting p = 0.5 yields the most balanced and robust performance across datasets.</p>
<p>2) Effect of Varying Maximum Magnitude : Similarly, we investigated the impact of varying the maximum magnitude on model merging performance.The experimental results are presented in Fig. 11(b).</p>
<p>As shown in the figure, incorporating a magnitude constraint can enhance the performance of the merged model.However, the value of should not be set too high.The results indicate that when = 0.1, the merged model achieves the best attack performance, both overall and across each individual dataset.Specifically, when the drop probability is set to p = 0.5 and the maximum magnitude is set to = 0.1, model merging yields optimal results for link stealing attacks.</p>
<p>F. Performance Analysis of the Proposed Method With Different Model Architectures</p>
<p>Here, we explore the generality of our proposed method.First, we examine whether our method can be effectively applied across various LLM model architectures.Next, we investigate its effectiveness when performing attacks on different GNN architectures.The following section provides a detailed analysis of the experimental results.</p>
<p>1) Attack Using Different LLM Architectures: In the previous experiments, we adopted Vicuna-7B as the LLM architecture for conducting link stealing attacks.In this section, we further explore the use of other large language models, such as Vicuna-13B and LongChat, to examine the generality of our proposed method.The experimental results are presented in Table VI.As shown in the table, even when employing different architectures of large language models, the proposed link stealing attack method effectively combines the advantages of each attacker's LLM-based attack model and achieves optimal average performance.A minimum mean accuracy of 91.26% and an F1 score of 91.61% are achieved across four datasets.Notably, when comparing attack performances across various datasets, our attack method sometimes achieves better performance than an attack model trained and tested on the same dataset.</p>
<p>In the table, the bold and underlined text represent the best and second-best attack results on the respective datasets.Generally, the attack models generated by our model merging method consistently rank among the top two across all datasets.In other words, the performance of these attack models is either the best or the second-best, with the top-ranked model typically being the one trained by the attacker using the same dataset as the target dataset.This outcome demonstrates the strong generalization ability of the proposed method and highlights its applicability across various LLM architectures.</p>
<p>2) Attack on Different GNN Architectures: Similarly, in addition to conducting link stealing attacks on the GCN target model in the previous experiments, we also carried out tests on other GNN target models, such as GAT and SAGE.The experimental results are shown in Table VII.As indicated in the table, our attack method achieves an average accuracy and F1 score exceeding 92% across models.For each dataset, our method consistently ranks in the top two in terms of performance, with Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.the top rank typically held by the attack model trained on the same dataset as the target dataset.These experimental results demonstrate that our method is applicable across different GNN target models and exhibits strong generalizability.</p>
<p>G. Performance Analysis on Out-of-Domain Data</p>
<p>In this section, we examine the performance of the merging method on datasets unknown to the attackers, referred to as out-of-domain data.Additionally, we also conduct experiments on Cora, Citeseer, Pubmed, and Ogbn-arxiv to evaluate the generality of our method on out-of-domain data.To assess its effectiveness, we merge the attack models trained on three out of the four datasets and then use the resulting merged model to attack the remaining dataset.For example, we merge the attack models individually trained by attackers on the Citeseer, Pubmed, and Ogbn-arxiv datasets and evaluate the merged model's performance on the Cora dataset, which is considered out-of-domain since the attackers have no access to it.The experimental results are shown in Fig. 12.In the figure , Our (w/o Target Dataset) represents the model derived from merging the three attack models.</p>
<p>The figure demonstrates the effectiveness of our method on out-of-domain data.As shown, the merged model, derived from combining the three attack models, retains the best attack performance on out-of-domain data among the individual attack models.Specifically, when the Cora dataset is used as the out-ofdomain dataset, the merged model achieves attack performance comparable to that of the attack model trained on Ogbn-arxiv for attacking Cora.Similarly, when the Citeseer and Pubmed datasets are treated as out-of-domain datasets, the merged model sustains the highest attack performance observed with the attack Fig. 12.Comparison of our method using three attack models for merging and attacking out-of-domain data.For the attack on Cora, we perform model merging using the attack models trained on Citeseer, Pubmed, and Ogbn-arxiv.For the attack on Citeseer, we perform model merging using the attack models trained on Cora, Pubmed, and Ogbn-arxiv.For the attack on Pubmed, we perform model merging using the attack models trained on Cora, Citeseer, and Ogbn-arxiv.For the attack on Ogbn-arxiv, we perform model merging using the attack models trained on Cora, Citeseer, and Pubmed.model trained on Ogbn-arxiv.Finally, when Ogbn-arxiv is used as the out-of-domain dataset, the merged model maintains the best attack performance of the attack model trained on Citeseer for targeting Ogbn-arxiv.</p>
<p>These findings suggest that our merging method effectively retains the highest performance on out-of-domain data among the models included in the merging process.This capability poses a significant threat to unknown datasets and aligns well with real-world attack scenarios.</p>
<p>H. Possible Defense</p>
<p>Here, we evaluate the robustness of the proposed attack method under a representative defense strategy.A commonly used approach to defending against privacy attacks involves perturbing the output of the target model by adding noise, such as through differential privacy (DP) [32], [33].In this evaluation, we investigate the effectiveness of our proposed attack when noise is added to the posterior probabilities of the target models.The experimental results are presented in Fig. 13.</p>
<p>In Fig. 13, we compare the performance of the single LLM and the merged LLM before and after applying DP as a defense mechanism.As expected, injecting noise into the posterior probabilities reduces the effectiveness of the link-stealing attack in both models.Notably, the merged model experiences an average performance drop of approximately 4%, indicating that DP serves as a partially effective mitigation strategy.</p>
<p>Despite this reduction, our method still achieves a success rate of around 90%, demonstrating its resilience against DP-based defenses.This highlights the robustness and effectiveness of our approach, while also underscoring the need for more advanced and comprehensive defense mechanisms against such attacks.</p>
<p>VI. RELATED WORK</p>
<p>A. Privacy Attacks on Graph Neural Networks</p>
<p>Graph Neural Networks (GNNs) [2] contain a wealth of private information, which has led many researchers to focus on privacy attacks targeting these models [34], [35].In such attacks, adversaries extract model details and sensitive data by querying deployed GNNs.Studies like [36], [37], [38] have model extraction attacks on GNNs, where attackers utilize the target model's posterior probabilities or intermediatelayer embeddings to capture its information and construct a surrogate model that closely replicates the original model's functionality and performance.</p>
<p>In addition, [39], [40], [41] have investigated membership inference attacks on GNNs, focusing on determining whether specific data instances were included in the target model's training set.These works build a shadow model that replicates the target GNN, allowing attackers to generate a shadow training dataset.By comparing the shadow model's responses to those of the target model on similar data, attackers can infer if particular graph data was part of the target GNN's training set.Zhang et al. [42] introduced a novel variation of membership inference attacks that, instead of identifying individual data points in the target model's training set, aims to determine whether a specific subgraph is embedded within a larger graph structure.</p>
<p>In attribute inference attacks on GNNs, attackers seek to infer specific or statistical information about the training graph data based on the target model's responses.Wang et al. [43] conducted a comprehensive study on Group Property Inference Attacks (GPIA) within graph neural networks, focusing on two types of attributes: node group attributes, representing collective information of specific node groups, and link group attributes, representing collective information of specific link groups.Zhang et al. [42] also investigated attribute inference attacks through graph embeddings, aiming to deduce fundamental attributes of the target graph, such as the number of nodes, number of edges, and graph density.</p>
<p>Link Stealing Attacks: A link stealing attack is a specific type of attribute inference attack where attackers extract information on node links in the training graph dataset by analyzing the target model's responses.He et al. [3] first introduced the concept of link stealing attacks and examined their effectiveness under varying levels of attacker knowledge.In [3], [4], [5], researchers determined the presence of links between nodes by assessing the similarity of posterior probabilities obtained from the target model.Similarly, in [6], link inference was achieved by perturbing the graph data and comparing posterior probability changes before and after the perturbation.</p>
<p>However, previous studies have not addressed how to conduct cross-dataset link stealing attacks, nor have they explored methods for enabling multiple attackers to jointly execute such attacks.These issues remain open areas for further research.</p>
<p>B. Large Language Models</p>
<p>Large Language Models (LLMs) [11] have transformed natural language processing, delivering outstanding performance in both academic and industrial contexts.LLMs, including models like GPT [44], Vicuna [20] and LongChat [21], often contain hundreds of millions to billions of parameters, enabling them to capture intricate language patterns and contextual nuances.Given their superior capabilities, extensive research on LLMs has emerged across various fields.</p>
<p>Zeng et al. [13] developed an open-source bilingual large language model that excels in both English and Chinese, leveraging specialized training strategies focused on selection, efficiency, and stability.Lin et al. [14] explored the application of LLMs in media detection, addressing limitations of previous methods that relied on specific models and datasets, which restricted adaptability and performance on out-of-domain data.Liu et al. [15] investigated the integration of LLMs into recommendation systems, enhancing content-based recommendation by incorporating both open-source and closed-source LLMs into the inference process.</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>In recent years, researchers have explored the integration of LLMs with GNNs to harness the complementary strengths of both architectures.Tang et al. [30] proposed a graph-oriented LLM trained via a two-stage instruction-tuning paradigm that combines self-supervised graph signals with task-specific instructions to enhance generalization across graph tasks.Zhang et al. [45] introduced GraphTranslator, a framework that aligns graph models with LLMs to combine structural understanding and natural language flexibility, achieving strong zero-shot performance in tasks such as node classification.He et al. [46] leverage semantic knowledge from LLMs to improve the initial node embeddings in GNNs, enhancing performance in downstream tasks.Ye et al. [47] replace the GNN predictor with an LLM, increasing the effectiveness of graph-based tasks by representing graph structures as natural language and using instruction prompts to utilize the expressive qualities of language.Liu et al. [48] align GNNs and LLMs within a shared vector space, incorporating textual knowledge into the graph, which enhances reasoning capabilities and overall model performance.Guo et al. [29] proposed GraphEdit, an approach that employs LLMs to capture complex relationships within graph-structured data, addressing the limitations of graph structure information and enabling LLMs to learn graph structures effectively.Collectively, these propose innovative strategies for aligning graph models with LLMs, significantly expanding the application scope and reasoning capacity of LLMs in graph-based domains.</p>
<p>However, prior research has not explored the use of LLMs for privacy attacks on GNNs.In this paper, we introduce LLMs for link stealing attacks and design a specialized prompt to fine-tune the LLM, optimizing it specifically for link stealing tasks.</p>
<p>C. Model Merging</p>
<p>Model merging techniques combine multiple task-specific models into a unified, versatile model without necessitating data exchange or additional training [16].Yadav et al. [31] tackled information loss in traditional merging methods, focusing on two main interference sources: redundant parameter values and conflicting parameter signs across models.Yu et al. [24] introduced DARE, a model merging approach that addresses parameter redundancy, especially within the highly redundant SFT delta parameters of LLMs.They proposed an efficient solution to reduce these delta parameters significantly without requiring data exchange, retraining, or even a GPU.Building on these advancements, Deep et al. [23] introduced a magnitude-based method in model merging, where redundant parameter values are pruned, and the remaining parameters are rescaled, leading to enhanced model merging effectiveness.</p>
<p>However, these methods often overlook the relative strengths of each model, allowing poorly performing models to negatively impact the merged model's performance.In this paper, we propose a novel merging method to address this limitation.By assigning weights to each model based on its performance, our approach grants higher weights to better-performing models during the merging process, thus enhancing the final model's effectiveness.</p>
<p>VII. DISSCUSION</p>
<p>This paper introduces large language models (LLMs) to significantly improve the success rate of link stealing attacks.However, a notable limitation of the proposed approach is the increased computational cost introduced by LLMs.Due to their vast number of parameters, these models require considerably more time and computational resources than traditional link stealing methods.As part of future work, we aim to develop more efficient attack variants that preserve high success rates while reducing inference time and computational overhead.</p>
<p>LLMs require rich textual features to support effective experimentation.Currently, among available graph datasets, only citation datasets typically provide sufficient textual information, as demonstrated in prior work [29], [46], [48].Therefore, our research primarily focuses on citation datasets.In future work, we plan to extend our approach to other types of graph datasets, such as social networks, recommender systems, and biological graphs, in order to demonstrate that our method offers an effective and generalizable solution for link stealing attacks.</p>
<p>It is important to note that the prompts used in this study are specifically tailored to citation datasets and are constructed using paper titles, abstracts, and posterior probabilities.When extending the approach to other domains, the prompt design will need to be adjusted to incorporate domain-specific textual features.Nonetheless, due to the strong generalization capabilities of LLMs, we observe that minor modifications to the prompt structure-such as changing the order of the input papers or resetting the answer format (e.g., using True or False)-do not significantly affect the effectiveness of the link stealing attacks.Furthermore, some large language models impose input length constraints (e.g., a maximum of 2048 tokens), which necessitates careful handling of longer feature inputs [17], [20].In our experiments, the use of titles, abstracts, and posterior probabilities results in prompts that typically span only a few hundred tokens, well within the supported range of most general-purpose LLMs.However, when adapting the method to other domains with longer textual features, this limitation must be carefully considered.</p>
<p>In this paper, we explored a defense strategy based on adding noise to posterior probabilities.While this approach reduced the attack success rate, it remained relatively high at approximately 90%.For future work, we recommend investigating more effective defense mechanisms.Additionally, inspired by the work of Zhang et al. [49], future studies should explore privacypreserving strategies that achieve a better trade-off between minimal performance degradation and strong privacy protection.</p>
<p>VIII. CONCLUSION</p>
<p>In this paper, we propose a novel link stealing attack method that combines the knowledge of multiple attackers to perform cross-dataset link stealing attacks against graph neural networks.We introduce large language models to carry out cross-dataset attacks, addressing the challenge of varying data lengths in crossdataset scenarios.To create a universal model by integrating knowledge from multiple attackers, we propose a novel model merging method.This approach merges attack models trained by individual attackers through three key steps: model dropping, parameter selection, and model merging.These steps allow attackers to effectively leverage the strengths of each attack model, enabling the merged model to deliver strong performance on in-domain data within the attackers' knowledge and achieve notable results on out-of-domain data previously unknown to the attackers.We provide theoretical proofs for the effectiveness of the proposed method and validate its performance through extensive experiments.We show that the proposed approach not only delivers excellent performance on in-domain data but also achieves effective attacks on out-of-domain datasets, aligning with real-world requirements.</p>
<p>initial input feature (i.e., x v ).The function AGG(•) aggregates information from neighboring nodes, and UPDATE(•) incorporates this aggregated information into the node's representation.</p>
<p>Fig. 1 .
1
Fig. 1.Overview of general link stealing attacks.The upper section depicts the service provider offering the target model for user access, while the lower section illustrates attackers executing the attack process.</p>
<p>Fig. 3 .
3
Fig. 3. Prompts designed for LLM-based link stealing attacks.</p>
<p>Fig. 4 .
4
Fig. 4. Steps to fine-tune LLM for link stealing attacks.</p>
<p>Fig. 5 .
5
Fig. 5. Schematic diagram of mapping weights to be inversely proportional to dropout probability.</p>
<p>Algorithm 1 : 1 : 4 : 6 : 8 : 3 : 12 : 5 : 14 :
11468312514
1, each attacker first designs the link stealing attack prompts based on their possessed knowledge.Then, in Step 2, each attacker Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.The Process of Multiple Attackers Collaborating to Perform Link Stealing Attacks.Input: Graph Node Features X, Original LLM Parameters θ, Maximum Change , and Drop Probability d Output: Link or U nlink Step 0: Obtain node posterior probabilities 2: P = T(X) 3: for i = 0 to T do Step 1: Designing prompt 5: Prompt = {(x v , p v ), (x u , p u )} Step 2: Fine-tuning the LLM model using prompt 7: θ t = LLM(θ, Prompt) Step Compute delta parameters 9: δ t = θ t − θ 10: Step 4: Apply Drop and Scaling 11: δt = Drop_Scaling(δ t , , d) Step Apply mask and election 13: δ = mask δ Step 6: Merge deltas to obtain merged parameters 15:</p>
<p>Fig. 7 .
7
Fig. 7. Comparison of different link stealing attack methods trained under a single dataset.</p>
<p>Fig. 8 .
8
Fig. 8. Heat map of LLM performance in LLM-base link stealing attacks across different datasets.</p>
<p>Fig. 9 .
9
Fig. 9. Allocation of weight λ for each attack model during model merging.</p>
<p>Fig. 10 .
10
Fig. 10.Comparison of model merging method in link stealing attack.</p>
<p>Fig. 11 .
11
Fig. 11.Impact of different drop probabilities d and maximum magnitude on model merging results.</p>
<p>Fig. 13 .
13
Fig.13.Attack result comparison between our proposed method and DP defense.</p>
<p>all other tokens, capturing global dependencies without being constrained by sequence length.As a result, the LLM can effectively handle variable data lengths.Additionally, F(•) represents other neural networks incorporated into the model.
Traditional link stealing attacks lack pre-training on diversedata and depend entirely on supervised learning within a singledataset. The representation function of an MLP, MLP(•), istypically expressed as:MLP
within the LLM.The transformer architecture enables the model to dynamically focus on different parts of the input sequence, regardless of its length.It tokenizes the input data; for example, in natural language processing, a sentence is decomposed into individual words or sub-word units (tokens).Each input token attends to</p>
<p>TABLE III RESULTS
III
OF DIFFERENT LINK STEALING ATTACKS TRAINED UNDER A SINGLE DATASET</p>
<p>TABLE IV RESULTS
IV
OF CALCULATING ALLOCATION WEIGHTS FOR THE MERGED MODEL BASED ON ACCURACY FROM LLM-BASED TRAINING ACROSS FOUR ATTACKERS ON THEIR RESPECTIVE DATASETS</p>
<p>model trained on Ogbn-arxiv achieves an accuracy of 89.93% on Cora and 92.48% on Citeseer, surpassing the previous methods' accuracy of 87.83% and 90.91%, respectively.This shows that our proposed LLM-based link stealing attack method can be generalized across different datasets and still achieve superior results.</p>
<p>TABLE VI RESULTS
VI
OF THE PROPOSED ATTACK METHOD USING DIFFERENT LLM ARCHITECTURES FOR LINK STEALING ATTACKS TABLE VII OF OUR PROPOSED ATTACK METHOD FOR LINK STEALING ATTACKS ON VARIOUS GNN TARGET MODELS</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.</p>
<p>The graph neural network model. F Scarselli, M Gori, A C Tsoi, M Hagenbuchner, G Monfardini, IEEE Trans. Neural Netw. 201Jan. 2009</p>
<p>Graph neural networks: A survey on the links between privacy and security. F Guan, T Zhu, W Zhou, K R Choo, Artif. Intell. Rev. 5722024</p>
<p>Stealing links from graph neural networks. X He, J Jia, M Backes, N Z Gong, Y Zhang, Proc. USENIX Secur. Symp. USENIX Secur. Symp2021</p>
<p>Demystifying uneven vulnerability of link stealing attacks against graph neural networks. H Zhang, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. Learn2023</p>
<p>Link stealing attacks against inductive graph neural networks. Y Wu, Proc. Priv. Enhancing Technol. Priv. Enhancing Technol20242024</p>
<p>Node injection link stealing attack. O Zari, J Parra-Arnau, A Ünsal, M Önen, arXiv:2307.135482023</p>
<p>Large language models for link stealing attacks against graph neural networks. F Guan, T Zhu, H Sun, W Zhou, P S Yu, IEEE Trans. Big Data. 114Aug. 2025</p>
<p>Data and model poisoning backdoor attacks on wireless federated learning, and the defense mechanisms: A comprehensive survey. Y Wan, Y Qu, W Ni, Y Xiang, L Gao, E Hossain, IEEE Commun. Surv. Tut. 263Third Quarter 2024</p>
<p>A privacy-preserving and untraceable group data sharing scheme in cloud computing. J Shen, H Yang, P Vijayakumar, N Kumar, IEEE Trans. Dependable Secure Comput. 19042022</p>
<p>PrivacyEAFL: Privacyenhanced aggregation for federated learning in mobile crowdsensing. M Zhang, S Chen, J Shen, W Susilo, Trans. Info. For. Sec. 182023</p>
<p>Fundamental capabilities of large language models and their applications in domain scenarios: A survey. J Li, Proc. Annu. Meeting Assoc. Comput. Linguistics. Annu. Meeting Assoc. Comput. Linguistics2024</p>
<p>When software security meets large language models: A survey. X Zhu, W Zhou, Q.-L Han, W Ma, S Wen, Y Xiang, IEEE/CAA J. Automatica Sinica. 122Feb. 2025</p>
<p>GLM-130B: An open bilingual pre-trained model. A Zeng, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2023</p>
<p>Indivec: An exploration of leveraging large language models for media bias detection with fine-grained bias indicators. L Lin, L Wang, X Zhao, J Li, K Wong, Proc. Conf. Eur. Chapter Assoc. Conf. Eur. Chapter Assoc2024</p>
<p>ONCE: Boosting content-based recommendation with both open-and closed-source large language models. Q Liu, N Chen, T Sakai, X Wu, Proc. ACM Int. Conf. Web Search Data Mining. ACM Int. Conf. Web Search Data Mining2024</p>
<p>Merge, ensemble, and cooperate! A survey on collaborative strategies in the era of large language models. J Lu, Z Pang, M Xiao, Y Zhu, R Xia, J Zhang, arXiv:2407.06089CoRR. 2024</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Comput. Surv. 559352023</p>
<p>Neural machine translation of rare words with subword units. R Sennrich, B Haddow, A Birch, Proc. Annu. Meeting Assoc. Comput. Linguistics. Annu. Meeting Assoc. Comput. Linguistics2016</p>
<p>A comprehensive survey on applications of transformers for deep learning tasks. S Islam, Expert Syst. Appl. 2411226662024</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. L Zheng, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2023</p>
<p>How long can open-source LLMS truly promise on context length?. D Li, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2023</p>
<p>A survey on dropout methods and experimental verification in recommendation. Y Li, IEEE Trans. Knowl. Data Eng. 357Jul. 2023</p>
<p>DELLA-merging: Reducing interference in model merging through magnitude-based sampling. P T Deep, R Bhardwaj, S Poria, arXiv:2406.116172024</p>
<p>Language models are super mario: Absorbing abilities from homologous models as a free lunch. L Yu, B Yu, H Yu, F Huang, Y Li, Proc. 41st Int. Conf. Mach. Learn. 41st Int. Conf. Mach. LearnVienna, Austria2024</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2017</p>
<p>Graph robustness benchmark: Benchmarking the adversarial robustness of graph machine learning. Q Zheng, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2021</p>
<p>Graph attention networks. P Velickovic, G Cucurull, A Casanova, A Romero, P Liò, Y Bengio, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. Representations2018</p>
<p>Inductive representation learning on large graphs. W L Hamilton, Z Ying, J Leskovec, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2017</p>
<p>GraphEdit: Large language models for graph structure learning. Z Guo, arXiv:2402.151832024</p>
<p>GraphGPT: Graph instruction tuning for large language models. J Tang, Proc. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval2024</p>
<p>TIES-MERGING: Resolving interference when merging models. P Yadav, D Tam, L Choshen, C A Raffel, M Bansal, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. Syst2023</p>
<p>GRID: Protecting training graph from link stealing attacks on GNN models. J Lou, X Yuan, R Zhang, X Yuan, N Gong, N Tzeng, Proc. IEEE Symp. Secur. Privacy. IEEE Symp. Secur. Privacy2025</p>
<p>Recent advances of differential privacy in centralized deep learning: A systematic survey. L Demelius, R Kern, A Trügler, ACM Comput. Surv. 576282025</p>
<p>PpNNT: Multiparty privacy-preserving neural network training system. Q Feng, D He, J Shen, M Luo, K.-K R Choo, IEEE Trans. Artif. Intell. 51Jan. 2024</p>
<p>Next-generation consumer electronics data auditing scheme toward cloud-edge distributed and resilient machine learning. Y Li, J Shen, P Vijayakumar, C.-F Lai, A Sivaraman, P K Sharma, IEEE Trans. Consum. Electron. 701Feb. 2024</p>
<p>Model extraction attacks on graph neural networks: Taxonomy and realisation. B Wu, X Yang, S Pan, X Yuan, Proc. null2022</p>
<p>Model stealing attacks against inductive graph neural networks. Y Shen, X He, Y Han, Y Zhang, Proc. IEEE Symp. Secur. Privacy. IEEE Symp. Secur. Privacy2022</p>
<p>A realistic model extraction attack against graph neural networks. F Guan, T Zhu, H Tong, W Zhou, Knowl.-Based Syst. 2024. 112144</p>
<p>Adapting membership inference attacks to GNN for graph classification: Approaches and implications. B Wu, X Yang, S Pan, X Yuan, Proc. Int. Conf. Des. Mater. Int. Conf. Des. Mater2021</p>
<p>Attention-based membership inference attacks on graph neural network through topological features. F Guan, T Zhu, H Tong, W Zhou, 10.1109/TDSC.2025.3586251IEEE Trans. Dependable Secure Comput. 12025</p>
<p>Topology-based node-level membership inference attacks on graph neural networks. F Guan, T Zhu, W Zhou, P S Yu, 10.1109/TBDATA.2025.3558855IEEE Trans. Big Data. 12025</p>
<p>Inference attacks against graph neural networks. Z Zhang, M Chen, M Backes, Y Shen, Y Zhang, Proc. USENIX Secur. Symp. USENIX Secur. Symp2022</p>
<p>Group property inference attacks against graph neural networks. X Wang, W H Wang, Proc. ACM SIGSAC Conf. ACM SIGSAC Conf2022</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. </p>
<p>GPT-3: Its nature, scope, limits, and consequences. L Floridi, M Chiriatti, Minds Mach. 3042020</p>
<p>Graphtranslator: Aligning graph model to large language model for open-ended tasks. M Zhang, Proc. World Wide Web Conf. World Wide Web Conf2024</p>
<p>Explanations as features: LLM-based features for text-attributed graphs. X He, X Bresson, T Laurent, B Hooi, arXiv:2305.195232023</p>
<p>Natural language is all a graph needs. R Ye, C Zhang, R Wang, S Xu, Y Zhang, arXiv:2308.071342023</p>
<p>Multi-modal molecule structure-text model for text-based retrieval and editing. S Liu, Nat. Mac. Intell. 5122023</p>
<p>Unraveling privacy risks of individual fairness in graph neural networks. H Zhang, X Yuan, S Pan, Proc. Int. Conf. Data Eng. Int. Conf. Data Eng2024</p>            </div>
        </div>

    </div>
</body>
</html>
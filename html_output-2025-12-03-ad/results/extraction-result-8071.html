<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8071 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8071</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8071</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-384f91e20bd3516b35e564c7f9b43ddd46656b86</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/384f91e20bd3516b35e564c7f9b43ddd46656b86" target="_blank">Finding Blind Spots in Evaluator LLMs with Interpretable Checklists</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> FBI is proposed, a novel framework designed to examine the proficiency of Evaluator LLMs in assessing four critical abilities in other LLMs: factual accuracy, instruction following, coherence in long-form writing, and reasoning proficiency, by introducing targeted perturbations in answers generated by LLMs.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are increasingly relied upon to evaluate text outputs of other LLMs, thereby influencing leaderboards and development decisions. However, concerns persist over the accuracy of these assessments and the potential for misleading conclusions. In this work, we investigate the effectiveness of LLMs as evaluators for text generation tasks. We propose FBI, a novel framework designed to examine the proficiency of Evaluator LLMs in assessing four critical abilities in other LLMs: factual accuracy, instruction following, coherence in long-form writing, and reasoning proficiency. By introducing targeted perturbations in answers generated by LLMs, that clearly impact one of these key capabilities, we test whether an Evaluator LLM can detect these quality drops. By creating a total of 2400 perturbed answers covering 22 perturbation categories, we conduct a comprehensive study using different evaluation strategies on five prominent LLMs commonly used as evaluators in the literature. Our findings reveal significant shortcomings in current Evaluator LLMs, which failed to identify quality drops in over 50% of cases on average. Single-answer and pairwise evaluations demonstrated notable limitations, whereas reference-based evaluations showed comparatively better performance. These results underscore the unreliable nature of current Evaluator LLMs and advocate for cautious implementation in practical applications.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8071.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8071.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior LLM-human agreement claims</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claims of strong correlations between LLM evaluators and human evaluations (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior studies that reported strong correlations between LLM-based evaluators and human judgments, while noting that such claims require more nuanced, fine-grained scrutiny.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Finding Blind Spots in Evaluator LLMs with Interpretable Checklists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>General NLG evaluation (LLM-based evaluators vs human evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>prior-claims may be overly broad; need finer-grained assessment across factuality, instruction following, long-form coherence, and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>The paper notes that although prior work (e.g., Dubois et al., Zheng et al.) reported strong correlations with human evaluations, accepting LLMs as reliable judges requires more nuanced, fine-grained tests across multiple axes; this motivates the FBI benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Noted in intro as efficiency and reduced cost/time compared to humans</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Cited as background; no new experiments here — used as motivation for more granular meta-evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Finding Blind Spots in Evaluator LLMs with Interpretable Checklists', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8071.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8071.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FBI detection failure rates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reported detection/failure rates of LLM evaluators on the FBI benchmark (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's core experimental finding: commonly-used LLM evaluators (GPT-4-TURBO, Gemini-1.5-Pro, Claude-3-Opus, Llama-3-70B-Instruct, Prometheus 2) fail to detect targeted quality-degrading perturbations in model outputs at high rates; on average, evaluator LLMs 'failed to identify quality drops in over 50% of cases'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Finding Blind Spots in Evaluator LLMs with Interpretable Checklists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Meta-evaluation of LLM evaluators on text generation (factuality, instruction following, long-form coherence, reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FBI (2400 human-vetted perturbed answers across 22 perturbation categories)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4-TURBO; also evaluated: Gemini-1.5-Pro, Claude-3-Opus, Llama-3-70B-Instruct, Prometheus 2</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Mix of proprietary and open models; evaluations run at temperature 0 for reproducibility; GPT-4-TURBO used as primary evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Graduate-student annotators (manual vetting of perturbations; created/validated gold and perturbed answers and labeled score-invariant cases)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>undetected-perturbation rate / percent of instances where score/verdict not affected by perturbation (or percent times gold not chosen in pairwise; percent perfect scores in reference-based)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.5</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>high undetected error rates (>50% overall); especially poor on many long-form and factual perturbations (e.g., comprehensiveness, chronology, removed facts); struggles with fluency/spelling/grammar perturbations; single-answer and pairwise paradigms particularly weak; explanations sometimes identify errors but scores unchanged</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Evaluator LLMs often overlook perturbations that should lower quality; reasoning perturbations are detected more reliably than many factual or long-form errors; reference-guided evaluations improve detection vs reference-less setups; adding rubrics/axes sometimes improves pairwise/reference evaluations but can worsen single-answer scoring; explanations occasionally mention errors without corresponding score penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Paper reiterates general advantages (efficiency, lower cost/time) but warns against over-reliance given high failure rates</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Three paradigms tested: (i) single-answer scoring (Vanilla*, Vanilla, Rubric, Axis, Axis+Rubric), (ii) pairwise comparison (Pairwise*, Pairwise, Rules, Axis, Axis+Rules), (iii) reference-guided single-answer scoring; metrics: % of instances where score unchanged by perturbation (single-answer), % times gold not chosen (pairwise, order-swapped to mitigate position bias), % perfect scores awarded to perturbed answer (reference). Evaluations at temperature 0.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Finding Blind Spots in Evaluator LLMs with Interpretable Checklists', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8071.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8071.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Observed evaluator failure modes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Specific failure modes and limitations of evaluator LLMs observed in FBI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents concrete failure modes of LLM evaluators: high miss rates on many perturbation types, detection in explanations not reflected in scores, sensitivity differences across axes, and interaction with prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Finding Blind Spots in Evaluator LLMs with Interpretable Checklists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Detection of targeted perturbations in LLM-generated answers across factual, instruction-following, long-form, and reasoning dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FBI</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4-TURBO (primary) and others</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>See prior entry; experiments include multiple prompting strategies (rubrics, axis, rules)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators used to create and validate perturbations; not used as parallel scorers in comparison experiments</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>does not penalize perturbed responses despite identifying errors in explanations; poor sensitivity to 'remove fact', 'comprehensiveness', 'chronology', 'assumptions', 'sequence' errors; fluency/spelling errors often missed; position/self-preference/verbosity biases reported in related work and relevant here</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Explanations sometimes call out errors the model then fails to penalize; reference-based scoring performs relatively better; reasoning errors (calculations, formulas) are more reliably detected than many factual and long-form issues; more complex rubrics do not uniformly help and may hurt single-answer detection.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Error categories enumerated across 22 perturbation types; detection counted when evaluator penalizes perturbed answer (single-answer) or picks gold (pairwise); detailed per-category detection statistics reported in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Finding Blind Spots in Evaluator LLMs with Interpretable Checklists', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8071.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8071.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explanations vs scores discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrepancy between evaluators' textual explanations and their numeric/verdict outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds many instances where evaluator LLMs mention or explain detected errors but their issued score/verdict does not reflect a penalty, and using a secondary model to parse explanations provides only marginal improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Finding Blind Spots in Evaluator LLMs with Interpretable Checklists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Assessment of whether explanations reveal perturbation detection beyond numeric scores</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Subset of FBI instances where scores unchanged but explanations present</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Various (explanations analyzed; GPT-3.5-TURBO used to parse explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Primary evaluators generated explanations; GPT-3.5-TURBO was prompted to read explanations and identify reported errors</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>N/A (analysis of model-generated explanations; humans created the perturbations and validated them earlier)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>explanations may mention errors but scores remain unchanged; analyzing explanations yields only marginal gains in detected perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Prompting a secondary LLM to analyze evaluator explanations identified some additional perturbations, but the improvement was small — explanations are 'only marginally helpful' in recovering missed penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Selected instances where perturbed answers were scored equal to gold; used GPT-3.5-TURBO to parse explanations and flag mentioned errors; compared detection-by-explanation vs detection-by-score.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Finding Blind Spots in Evaluator LLMs with Interpretable Checklists', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8071.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8071.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Related-work biases</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Previously reported evaluator biases (position, self-preference, verbosity) referenced and relevant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references literature documenting evaluator LLM biases — position bias, self-preference (favoring own generations), and verbosity/style biases — and treats these as relevant limitations when using LLMs as judges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Finding Blind Spots in Evaluator LLMs with Interpretable Checklists</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Background context for evaluation reliability (affects all NLG evaluation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>position bias; self-preference bias (LLMs favor their own generations); verbosity/style bias (prefers fluent/longer outputs even if incorrect)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>The paper notes these biases from prior work and mitigates position bias by swapping order in pairwise evaluations; highlights that such biases complicate trusting LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Pairwise evaluations performed twice with swapped positions to mitigate position bias; other biases discussed as caveats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Finding Blind Spots in Evaluator LLMs with Interpretable Checklists', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging lllm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Alpacafarm: A simulation framework for methods that learn from human feedback <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Are large language model-based evaluators the solution to scaling up multilingual evaluation? FINDINGS <em>(Rating: 2)</em></li>
                <li>Evaluating large language models at evaluating instruction following <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8071",
    "paper_id": "paper-384f91e20bd3516b35e564c7f9b43ddd46656b86",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "Prior LLM-human agreement claims",
            "name_full": "Claims of strong correlations between LLM evaluators and human evaluations (prior work)",
            "brief_description": "The paper cites prior studies that reported strong correlations between LLM-based evaluators and human judgments, while noting that such claims require more nuanced, fine-grained scrutiny.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
            "evaluation_task": "General NLG evaluation (LLM-based evaluators vs human evaluations)",
            "dataset_name": "",
            "judge_model_name": "",
            "judge_model_details": "",
            "human_evaluator_type": "",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "prior-claims may be overly broad; need finer-grained assessment across factuality, instruction following, long-form coherence, and reasoning",
            "qualitative_findings": "The paper notes that although prior work (e.g., Dubois et al., Zheng et al.) reported strong correlations with human evaluations, accepting LLMs as reliable judges requires more nuanced, fine-grained tests across multiple axes; this motivates the FBI benchmark.",
            "advantages_of_llm_judge": "Noted in intro as efficiency and reduced cost/time compared to humans",
            "experimental_setting": "Cited as background; no new experiments here — used as motivation for more granular meta-evaluation",
            "uuid": "e8071.0",
            "source_info": {
                "paper_title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "FBI detection failure rates",
            "name_full": "Reported detection/failure rates of LLM evaluators on the FBI benchmark (this paper)",
            "brief_description": "The paper's core experimental finding: commonly-used LLM evaluators (GPT-4-TURBO, Gemini-1.5-Pro, Claude-3-Opus, Llama-3-70B-Instruct, Prometheus 2) fail to detect targeted quality-degrading perturbations in model outputs at high rates; on average, evaluator LLMs 'failed to identify quality drops in over 50% of cases'.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
            "evaluation_task": "Meta-evaluation of LLM evaluators on text generation (factuality, instruction following, long-form coherence, reasoning)",
            "dataset_name": "FBI (2400 human-vetted perturbed answers across 22 perturbation categories)",
            "judge_model_name": "GPT-4-TURBO; also evaluated: Gemini-1.5-Pro, Claude-3-Opus, Llama-3-70B-Instruct, Prometheus 2",
            "judge_model_details": "Mix of proprietary and open models; evaluations run at temperature 0 for reproducibility; GPT-4-TURBO used as primary evaluator",
            "human_evaluator_type": "Graduate-student annotators (manual vetting of perturbations; created/validated gold and perturbed answers and labeled score-invariant cases)",
            "agreement_metric": "undetected-perturbation rate / percent of instances where score/verdict not affected by perturbation (or percent times gold not chosen in pairwise; percent perfect scores in reference-based)",
            "agreement_score": 0.5,
            "reported_loss_aspects": "high undetected error rates (&gt;50% overall); especially poor on many long-form and factual perturbations (e.g., comprehensiveness, chronology, removed facts); struggles with fluency/spelling/grammar perturbations; single-answer and pairwise paradigms particularly weak; explanations sometimes identify errors but scores unchanged",
            "qualitative_findings": "Evaluator LLMs often overlook perturbations that should lower quality; reasoning perturbations are detected more reliably than many factual or long-form errors; reference-guided evaluations improve detection vs reference-less setups; adding rubrics/axes sometimes improves pairwise/reference evaluations but can worsen single-answer scoring; explanations occasionally mention errors without corresponding score penalties.",
            "advantages_of_llm_judge": "Paper reiterates general advantages (efficiency, lower cost/time) but warns against over-reliance given high failure rates",
            "experimental_setting": "Three paradigms tested: (i) single-answer scoring (Vanilla*, Vanilla, Rubric, Axis, Axis+Rubric), (ii) pairwise comparison (Pairwise*, Pairwise, Rules, Axis, Axis+Rules), (iii) reference-guided single-answer scoring; metrics: % of instances where score unchanged by perturbation (single-answer), % times gold not chosen (pairwise, order-swapped to mitigate position bias), % perfect scores awarded to perturbed answer (reference). Evaluations at temperature 0.",
            "uuid": "e8071.1",
            "source_info": {
                "paper_title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Observed evaluator failure modes",
            "name_full": "Specific failure modes and limitations of evaluator LLMs observed in FBI",
            "brief_description": "The paper documents concrete failure modes of LLM evaluators: high miss rates on many perturbation types, detection in explanations not reflected in scores, sensitivity differences across axes, and interaction with prompting strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
            "evaluation_task": "Detection of targeted perturbations in LLM-generated answers across factual, instruction-following, long-form, and reasoning dimensions",
            "dataset_name": "FBI",
            "judge_model_name": "GPT-4-TURBO (primary) and others",
            "judge_model_details": "See prior entry; experiments include multiple prompting strategies (rubrics, axis, rules)",
            "human_evaluator_type": "Human annotators used to create and validate perturbations; not used as parallel scorers in comparison experiments",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "does not penalize perturbed responses despite identifying errors in explanations; poor sensitivity to 'remove fact', 'comprehensiveness', 'chronology', 'assumptions', 'sequence' errors; fluency/spelling errors often missed; position/self-preference/verbosity biases reported in related work and relevant here",
            "qualitative_findings": "Explanations sometimes call out errors the model then fails to penalize; reference-based scoring performs relatively better; reasoning errors (calculations, formulas) are more reliably detected than many factual and long-form issues; more complex rubrics do not uniformly help and may hurt single-answer detection.",
            "advantages_of_llm_judge": "",
            "experimental_setting": "Error categories enumerated across 22 perturbation types; detection counted when evaluator penalizes perturbed answer (single-answer) or picks gold (pairwise); detailed per-category detection statistics reported in tables.",
            "uuid": "e8071.2",
            "source_info": {
                "paper_title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Explanations vs scores discrepancy",
            "name_full": "Discrepancy between evaluators' textual explanations and their numeric/verdict outputs",
            "brief_description": "The paper finds many instances where evaluator LLMs mention or explain detected errors but their issued score/verdict does not reflect a penalty, and using a secondary model to parse explanations provides only marginal improvement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
            "evaluation_task": "Assessment of whether explanations reveal perturbation detection beyond numeric scores",
            "dataset_name": "Subset of FBI instances where scores unchanged but explanations present",
            "judge_model_name": "Various (explanations analyzed; GPT-3.5-TURBO used to parse explanations)",
            "judge_model_details": "Primary evaluators generated explanations; GPT-3.5-TURBO was prompted to read explanations and identify reported errors",
            "human_evaluator_type": "N/A (analysis of model-generated explanations; humans created the perturbations and validated them earlier)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "explanations may mention errors but scores remain unchanged; analyzing explanations yields only marginal gains in detected perturbations",
            "qualitative_findings": "Prompting a secondary LLM to analyze evaluator explanations identified some additional perturbations, but the improvement was small — explanations are 'only marginally helpful' in recovering missed penalties.",
            "advantages_of_llm_judge": "",
            "experimental_setting": "Selected instances where perturbed answers were scored equal to gold; used GPT-3.5-TURBO to parse explanations and flag mentioned errors; compared detection-by-explanation vs detection-by-score.",
            "uuid": "e8071.3",
            "source_info": {
                "paper_title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Related-work biases",
            "name_full": "Previously reported evaluator biases (position, self-preference, verbosity) referenced and relevant",
            "brief_description": "The paper references literature documenting evaluator LLM biases — position bias, self-preference (favoring own generations), and verbosity/style biases — and treats these as relevant limitations when using LLMs as judges.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
            "evaluation_task": "Background context for evaluation reliability (affects all NLG evaluation tasks)",
            "dataset_name": "",
            "judge_model_name": "",
            "judge_model_details": "",
            "human_evaluator_type": "",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "position bias; self-preference bias (LLMs favor their own generations); verbosity/style bias (prefers fluent/longer outputs even if incorrect)",
            "qualitative_findings": "The paper notes these biases from prior work and mitigates position bias by swapping order in pairwise evaluations; highlights that such biases complicate trusting LLM judges.",
            "advantages_of_llm_judge": "",
            "experimental_setting": "Pairwise evaluations performed twice with swapped positions to mitigate position bias; other biases discussed as caveats.",
            "uuid": "e8071.4",
            "source_info": {
                "paper_title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging lllm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Alpacafarm: A simulation framework for methods that learn from human feedback",
            "rating": 2
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2
        },
        {
            "paper_title": "Are large language model-based evaluators the solution to scaling up multilingual evaluation? FINDINGS",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models at evaluating instruction following",
            "rating": 1
        }
    ],
    "cost": 0.016406499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Finding Blind Spots in Evaluator LLMs with Interpretable Checklists</h1>
<p>Sumanth Doddapaneni ${ }^{\star 1,2}$ Mohammed Safi Ur Rahman Khan ${ }^{\star 1,2}$<br>Sshubam Verma ${ }^{1}$ Mitesh M. Khapra ${ }^{1,2}$<br>${ }^{1}$ Nilekani Centre at AI4Bharat ${ }^{2}$ Indian Institute of Technology, Madras<br>Correspondence: (sumanthd, miteshk)@cse.iitm.ac.in, safikhan@ai4bharat.org<br>https://huggingface.co/datasets/ai4bharat/FBI<br>https://github.com/AI4Bharat/FBI</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) are increasingly relied upon to evaluate text outputs of other LLMs, thereby influencing leaderboards and development decisions. However, concerns persist over the accuracy of these assessments and the potential for misleading conclusions. In this work, we investigate the effectiveness of LLMs as evaluators for text generation tasks. We propose FBI, a novel framework designed to examine the proficiency of Evaluator LLMs in assessing four critical abilities in other LLMs: factual accuracy, instruction following, coherence in long-form writing, and reasoning proficiency. By introducing targeted perturbations in answers generated by LLMs, that clearly impact one of these key capabilities, we test whether an Evaluator LLM can detect these quality drops. By creating a total of 2400 perturbed answers covering 22 perturbation categories, we conduct a comprehensive study using different evaluation strategies on five prominent LLMs commonly used as evaluators in the literature. Our findings reveal significant shortcomings in current Evaluator LLMs, which failed to identify quality drops in over $50 \%$ of cases on average. Singleanswer and pairwise evaluations demonstrated notable limitations, whereas reference-based evaluations showed comparatively better performance. These results underscore the unreliable nature of current Evaluator LLMs and advocate for cautious implementation in practical applications. Code and data are available at https://github.com/AI4Bharat/FBI.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) are gaining widespread acceptance as the gold standard for evaluation in numerous applications, thanks to their efficiency and significant reductions in cost \&amp; time compared to human evaluators (Kim et al., 2023, 2024a; Chiang and Lee, 2023; Chen et al., 2023;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We present FBI, our novel meta-evaluation framework designed to assess the robustness of evaluator LLMs across diverse tasks and evaluation strategies.</p>
<p>Dubois et al., 2023). Furthermore, Evaluator LLMs are increasingly being utilized in the creation and maintenance of leaderboards for benchmarking various AI models (Watts et al., 2024; Zheng et al., 2023). While this reliance on LLMs offers significant advantages, it also presents potential drawbacks that warrant careful consideration. If LLMs are not effective evaluators, the resulting rankings and assessments could be fundamentally flawed, leading to inaccurate conclusions and misguided decisions. Therefore, it is crucial to pause and rigorously assess the evaluation capabilities of LLMs.</p>
<p>Recent studies have explored the effectiveness of LLMs as evaluators and have reported strong correlations with human evaluations (Dubois et al., 2023; Zheng et al., 2023). While these findings are promising, accepting LLMs as reliable evaluators necessitates more nuanced assessments (Zeng et al., 2023). As LLMs become integral in a diverse range of tasks, they are expected to demonstrate a wide array of abilities, including factual accu-</p>
<p>racy, instruction following, coherence in long-form writing, and reasoning proficiency. Consequently, it is crucial to determine if Evaluator LLMs can indeed do a fine grained assessment of these varied abilities. Specifically, can they evaluate factual correctness, grammar, spelling, mathematical proficiency, and adherence to instructions in answers generated by other LLMs? (ref. Fig. 1) The necessity for such thorough fine-grained assessments is underscored by the Checklist (Ribeiro et al., 2020) approach, initially applied to BERT (Devlin et al., 2019) and subsequently adapted in studies across various tasks and models (Sai et al., 2021).</p>
<p>In this work, we introduce FBI, a comprehensive framework designed to Find Blind spots in evaluator LLMs using an Interpretable checklist across four fundamental text generation abilities: (a) factual accuracy, (b) instruction following, (c) coherence in long-form writing, and (d) reasoning proficiency. To rigorously assess an Evaluator LLM's ability to grade answers along these dimensions, we introduce perturbations that degrade the quality of the answer in one of these areas, expecting that good Evaluator LLMs will detect these quality drops and adjust their scores accordingly. Additionally, we develop quality-preserving perturbations where an Evaluator LLM should maintain consistent scoring. A detailed description of the 22 perturbation categories that we used is provided in Table 2. Staring with 500 prompts, we first generate long-form responses using GPT-4-TURBO. We then use a human-in-the-loop approach, to systematically perturb these responses, resulting in a dataset of 2400 tuples, where each tuple contains a prompt, response, and perturbed response.</p>
<p>Using the generated perturbations, we employed three evaluation paradigms (a) single-answer evaluation, (b) pairwise evaluation, and (c) referenceguided evaluation. Within each paradigm, we try multiple popular strategies of using Evaluator LLMs, such as, providing a rubric, asking for a justification, specifying the axis of evaluation, etc. Using these strategies, we assess the evaluation capabilities of five widely-used Evaluator LLMs. Our findings indicate that LLMs are currently far from being reliable evaluators for text generation tasks. Even with the best models and evaluation strategies, Evaluator LLMs failed to identify errors in over $50 \%$ of cases, on average. Interestingly, across all evaluation strategies, we observed that all popular Evaluator LLMs consistently performed poorly. Notably, even basic perturbation categories,
such as, fluency perturbations (e.g. spellings and grammar) posed challenges for the evaluators. We also observed cases where Evaluator LLMs did not adjust their scores for perturbed responses despite correctly identifying the perturbations in their explanations. When used for single-answer grading and pairwise evaluation, Evaluator LLMs showed significant limitations, suggesting they are not reliable in these setups. In contrast, when used for reference-based evaluation, they demonstrated relatively better performance. Overall, our experiments uncovered significant blind spots in Evaluator LLMs, warranting caution in their direct application in practical settings.</p>
<h2>2 Related Work</h2>
<p>LLMs as Evaluators. LLMs have been increasingly used for automated evaluation for various NLG tasks (Wang et al., 2023a; Chiang and yi Lee, 2023; Kocmi and Federmann, 2023). We broadly classify this into two paradigms - (i) referencedriven evaluations (Fu et al., 2023; Kim et al., 2023), and (ii) reference-free evaluations (Liu et al., 2023; Zheng et al., 2023). The evaluator is either asked for a score (score-based evaluation) (Liu et al., 2023; Zheng et al., 2023; Hada et al., 2023) or to choose the best amongst two given responses (pairwise comparison evaluation) (Zheng et al., 2023; Wang et al., 2023b; Liusie et al., 2023). Additionally, various open-source evaluation-specific trained models have also been proposed (Wang et al., 2023d; Kim et al., 2023; Zhu et al., 2023). Further, advanced ensemble approaches include evaluation via multi-agent interactions (Chan et al., 2023; Zhang et al., 2023) or with external agents (Min et al., 2023; Hasanbeig et al., 2023).
Biases in Evalautor LLMs. Studies around Evaluator LLMs have highlighted the various biases - position bias (Zheng et al., 2023; Wang et al., 2023c), self preference bias (Panickssery et al., 2024; Liu et al., 2023), verbosity bias (Wu and Aji, 2023; Zeng et al., 2023), etc. Various approaches, including chain-of-thought reasoning (Zheng et al., 2023; Zeng et al., 2023), position-swapping (Zeng et al., 2023), among others, have been suggested to mitigate some of these. Recent studies (Hada et al., 2023; Saha et al., 2023) also show the effectiveness of the evaluators can be increased by evaluating specific axes and providing detailed rubrics/rules (Ye et al., 2023; Kim et al., 2024a).</p>
<p>Evaluation of Evaluator LLMs. Critically analysing evaluation metrics and suggesting methods to improve their robustness has always been of interest to the NLP community (Sai B et al., 2023; Mathur et al., 2020). Recent studies have evaluated the efficacy of LLMs as evaluators for specific types of tasks (Hada et al., 2024; Shen et al., 2023) and evaluation paradigms (Wang et al., 2023b,a) by assessing their agreement with human evaluations (Hada et al., 2023; Chiang and Lee, 2023; Zheng et al., 2023). Additionally, the robustness of these evaluators has been tested using adversarial examples (Kamoi et al., 2024; Chen et al., 2024; Wu and Aji, 2023), further showing their strengths and weaknesses.</p>
<p>Our proposed framework represents a significant departure from these existing approaches in several key aspects. First, we focus on a broader set of essential abilities: factual understanding, instruction following, long-form writing, and reasoning. Second, all prompts and the 2400 perturbed answers in our framework are carefully crafted and/or validated by humans, ensuring high quality and relevance to the abilities being evaluated. Third, our framework offers finer granularity in perturbation types, allowing us to finely identify and isolate the capabilities and limitations of Evaluator LLMs. This detailed analysis assists in making more knowledgeable choices about when to utilize LLMs as evaluators. Lastly, we focus on three popular evaluation paradigms, viz., referenceless single answer scoring, reference-less pairwise comparison, and reference based scoring, thereby providing a comprehensive toolkit for evaluating LLM performance across different dimensions.</p>
<h2>3 FBI: Meta-Evaluation Checklist</h2>
<p>We introduce FBI, a meta-evaluation benchmark designed to assess the capabilities of Evaluator LLMs in examining the outputs of other LLMs across four distinct task abilities: (i) Factual accuracy, (ii) Reasoning ability, (iii) instruction following, and (iv) proficiency in long-form writing. Each instance within the benchmark comprises a tuple ( $I, A_{\text {gold }}, A_{\text {perturb }}$ ), where $I$ represents the input instruction or prompt given to the model, $A_{\text {gold }}$ denotes the correct or gold answer, and $A_{\text {perturb }}$ signifies a perturbed version of the gold answer. The perturbed answers, $A_{\text {perturb }}$, are generated by introducing specific types of errors across each of the four task abilities (Table 2) to evaluate whether</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;"># Instances</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Long Form (LF)</td>
<td style="text-align: center;">528</td>
</tr>
<tr>
<td style="text-align: center;">Grammar</td>
<td style="text-align: center;">92</td>
</tr>
<tr>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Consistency</td>
<td style="text-align: center;">84</td>
</tr>
<tr>
<td style="text-align: center;">Chronology</td>
<td style="text-align: center;">71</td>
</tr>
<tr>
<td style="text-align: center;">CoHERENCE</td>
<td style="text-align: center;">91</td>
</tr>
<tr>
<td style="text-align: center;">COMPREHENSIVENESS</td>
<td style="text-align: center;">90</td>
</tr>
<tr>
<td style="text-align: center;">Factual (F)</td>
<td style="text-align: center;">483</td>
</tr>
<tr>
<td style="text-align: center;">Contextual</td>
<td style="text-align: center;">94</td>
</tr>
<tr>
<td style="text-align: center;">Entity</td>
<td style="text-align: center;">87</td>
</tr>
<tr>
<td style="text-align: center;">Incorrect Fact</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: center;">Number Errors</td>
<td style="text-align: center;">74</td>
</tr>
<tr>
<td style="text-align: center;">Opposite Fact</td>
<td style="text-align: center;">91</td>
</tr>
<tr>
<td style="text-align: center;">Remove Fact</td>
<td style="text-align: center;">69</td>
</tr>
<tr>
<td style="text-align: center;">Instruction Following (IF)</td>
<td style="text-align: center;">379</td>
</tr>
<tr>
<td style="text-align: center;">Do More</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Do Less</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">Ignore Format</td>
<td style="text-align: center;">99</td>
</tr>
<tr>
<td style="text-align: center;">Sequence Errors</td>
<td style="text-align: center;">49</td>
</tr>
<tr>
<td style="text-align: center;">Assumptions</td>
<td style="text-align: center;">81</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning (R)</td>
<td style="text-align: center;">494</td>
</tr>
<tr>
<td style="text-align: center;">Calculations</td>
<td style="text-align: center;">149</td>
</tr>
<tr>
<td style="text-align: center;">Copying Numbers</td>
<td style="text-align: center;">83</td>
</tr>
<tr>
<td style="text-align: center;">Final Errors</td>
<td style="text-align: center;">97</td>
</tr>
<tr>
<td style="text-align: center;">Incorrect Units</td>
<td style="text-align: center;">77</td>
</tr>
<tr>
<td style="text-align: center;">Wrong Formula</td>
<td style="text-align: center;">88</td>
</tr>
<tr>
<td style="text-align: center;">Score Invariant (SI)</td>
<td style="text-align: center;">516</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">2400</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of perturbations across all the 4 task abilities and each of the perturbation categories.</p>
<p>LLM evaluators can accurately identify and account for these errors in the perturbed answers.</p>
<p>The perturbations are based on perturbation categories carefully crafted by human annotators, informed by the prevalent failure modes in current LLMs (Min et al., 2023; Wu et al., 2023; Zhou et al., 2023b). These human annotators are graduate students who are well aware of the typical errors made by LLMs. Such human oversight is used throughout the benchmark's development, from prompt selection (§ 3.1) to defining perturbation categories (§ 3.2) and creating the perturbations (§ 3.3). To ensure a high standard of accuracy and reliability, all perturbations within FBI undergo rigorous manual vetting(§ 3.4). Table 1 presents some statistics about FBI, and the detailed generation process is discussed in the following sub-sections.</p>
<h3>3.1 Prompt Selection</h3>
<p>We selected six test sets containing prompts in English, viz., WizardLM (Xu et al., 2023), MT Bench (Zheng et al., 2023), UltraChat (Ding et al., 2023), LIMA (Zhou et al., 2023a), LLMBar (Zeng</p>
<p>et al., 2023), and IFEval (Zhou et al., 2023b). These test sets were selected for their recency and because they contain prompts for long-form generation, creativity, and open-ended tasks that require instruction-following. Collectively, these test sets comprise of 1809 prompts. We manually categorized each prompt into one of the 4 task categories: Long Form Writing (LF): These prompts require generating long pieces of text and explore generic topics, often including detailed analysis and storytelling. For example, How can I improve my time management skills?
Factual (F): These prompts seek objective information or facts. For example, What is the primary function of a capacitor in an electrical circuit?
Instruction Following (IF): These prompts require executing specific steps or guidelines to achieve a particular outcome or answer. For example, Write a poem with four lines and the following words: peace, sky, race, ground.
Reasoning (R): These prompts necessitate the application of logic, mathematics, and critical thinking to analyze information and draw conclusions. For example, A bat and a ball together cost $\$ 1.10$. The bat costs $\$ 1.00$ more than the ball. How much does the ball cost?</p>
<p>We sampled 100 questions from each of the four abilities, supplementing prompts requiring reasoning ability from the GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) benchmarks. Additionally, we created 200 prompts tailored to instruction following to address specific perturbation categories ${ }^{1}$. The gold answers $\left(A_{\text {gold }}\right)$ for all prompts were generated using the GPT-4-TURBO model. To ensure the quality and accuracy of $A_{\text {gold }}$, we conducted manual verification by randomly sampling $25 \%$ instances from each category and found that the gold answers maintain a high level of correctness. Importantly, we emphasize that the quality of gold answers is not critical in our study, as our primary focus is on directional score changes (i.e., we are interested in knowing if a perturbed answer with clear errors scores relatively lower than the original answer which did not have these errors).</p>
<h3>3.2 Perturbation Categories</h3>
<p>LLMs exhibit numerous failure modes, encompassing shortcomings in reasoning (Wu et al., 2023;</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Wei et al., 2022), factuality (Hu et al., 2024; Min et al., 2023), instruction-following (Zhou et al., 2023b; Li et al., 2023), and, in some instances, coherence and consistency (Naismith et al., 2023; Shen et al., 2023) in generated text. Given that we utilize Evaluator LLMs to assess responses in one or more of these abilities, it is imperative for the evaluator to excel in them. Our perturbations across each task ability are crafted keeping these failure modes in mind, as presented in Table 2. While our perturbations are primarily designed to decrease scores, we also develop score-invariant perturbations (§ 3.5), which are intended not to affect the score relative to the gold answer.</p>
<h3>3.3 Perturbation Generation</h3>
<p>To generate perturbed answers ( $A_{\text {perturb }}$ ) along each of the defined categories (§ 3.2), we use GPT-4-TURBO by prompting it with specific instructions tailored to each perturbation category. The model was tasked with producing perturbed answers and explaining the reasoning behind each perturbation. We iteratively refined the instructions by manually reviewing a sample of $25 \%$ of perturbed answers for each category, till we were satisfied with the generated perturbations.</p>
<h3>3.4 Human-In-The-Loop</h3>
<p>While GPT generally succeeds in generating the expected perturbations, we observed instances where the model (i) deviates from the intended perturbation, (ii) produces the incorrect style of perturbation, or (iii) accurately generates the reasoning but fails to reflect it in $A_{\text {perturb }}$. To address these inconsistencies, we meticulously vet all generated perturbations through a manual review process. Each perturbed answer produced by GPT-4-TURBO is examined against $A_{\text {gold }}$, and then categorized as valid, invalid, or score invariant. A perturbation is considered valid only if it should logically result in a scoring penalty as determined by human annotators. The vetting is carried out by students who possess a comprehensive understanding of LLM literature, holding at least a bachelor's or master's degree. To aid in validating perturbations, we developed a tool, the details of which are outlined in Appendix A.</p>
<h3>3.5 Score-Invariant Perturbations</h3>
<p>Score-invariant perturbations are those modifications that do not warrant a scoring penalty. These are collected in two ways: (i) human annotators</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Perturbation Axis</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">Grammar <br> Spelling <br> Consistency <br> ChronologY <br> Coherence <br> COMPREHENSIVENESS</td>
<td style="text-align: center;">Introducing grammatical errors in the answer. Eg: This is good $\rightarrow$ This are good. Introducing "valid" spelling errors in the answer. Eg: Toxicity $\rightarrow$ Tocixity. Introducing errors in the "consistency" of the answer (like tone, terminology, etc.) Introducing errors in the chronological or the logical flow of the answer. Introducing errors that affect the coherence of the answer. Introducing vagueness, irrelevance or lack of context in the answer.</td>
</tr>
<tr>
<td style="text-align: center;">F</td>
<td style="text-align: center;">Contextual <br> Entity <br> Incorrect Fact <br> Number Errors <br> Opposite Fact <br> Remove Fact</td>
<td style="text-align: center;">Replacing fact with a contextually similar incorrect fact. Eg: electricity $\rightarrow$ magnetism. Replacing a named entity with an incorrect entity. Eg: Poland $\rightarrow$ London. Adding a new contextually relevant incorrect fact in the answer. Introducing errors in the various numbers reported in the answer. Eg: $1987 \rightarrow 1887$. Replacing a fact in the answer with its negation. Eg: ... will have $\ldots \rightarrow$... wont have .... Removing a fact critical to the correctness and completeness of the answer.</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">Do Less <br> Do More <br> IGNORE FORMAT <br> SEQUENCE ERRORS <br> ASSUMPTIONS</td>
<td style="text-align: center;">Doing less than what is explicitly requested in the question. Doing more than what is explicitly requested in the question. Ignoring the formatting and other constraints mentioned in the question. Ignoring the sequence in the response when explicitly requested in the instruction. Making new incorrect assumptions about the instruction.</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">Calculations <br> Copying Numbers <br> Final Errors <br> Incorrect Units <br> Wrong Formula</td>
<td style="text-align: center;">Introducing calculation errors in the answer. Eg: $2+3=5 \rightarrow 2+3=6$ Introducing errors while considering the numbers mentioned in the instruction. Introducing errors only the final reported answer while retaining the correct solution. Introducing errors in the units reported and considered in the answer. Introducing errors in the formula used in the answer. Eg: $\pi r^{2} \rightarrow 2 \pi r$</td>
</tr>
<tr>
<td style="text-align: center;">SI</td>
<td style="text-align: center;">Score Invariant</td>
<td style="text-align: center;">Introducing modifications in the answer which would not result in a score penalty.</td>
</tr>
</tbody>
</table>
<p>Table 2: Perturbation categories across each of the task abilities. The green highlights indicate the original text and the red highlights indicated the perturbed text. Complete examples of each perturbation can be found in supplementary material.
categorize specific instances from our initial list as invariant (§ 3.4), and (ii) prompting Gemini-1.5Pro model to paraphrase $A_{\text {gold }}$ ensuring retention of all original facts and details followed by human verification on a sample. We collect 516 score invariant perturbations in total.</p>
<h2>4 Strategies for using Evaluator LLMs</h2>
<p>In this section, we outline the prompting strategies employed by Evaluator LLMs benchmarked on FB1. An Evaluator LLM, $f(\cdot)$, takes the input instruction, LLM generated response and an evaluation prompt, $P_{\text {eval }}$, as input, and is required to generate a score and an optional explanation. To make the evaluation more robust, the evaluator may also be provided with additional information specifying the axes of evaluation, rubrics, rules, and other criteria. Our study focuses on 3 evaluation paradigms: (i) Single-answer scoring (§4.1), (ii) Pairwise comparison (§4.2), and (iii) Referenceguided evaluation (§4.3). For all the strategies evaluation prompts $P_{\text {eval }}$ are adapted from Zheng et al. (2023); Zeng et al. (2023); Hada et al. (2023).</p>
<h3>4.1 Single Answer Scoring</h3>
<p>In this paradigm, evaluator $f(\cdot)$ is tasked with scoring a model response based solely on its parameterized knowledge.</p>
<p>Vanilla* (Zheng et al., 2023): In this strategy, the evaluator $f(\cdot)$ is presented with only the input instruction $I$ and a model response $A_{\text {model }}$. The role of $f(\cdot)$ is to evaluate $A_{\text {model }}$ and assign a score, denoted as $f\left(P_{\text {eval }}, I, A_{\text {model }}\right) \rightarrow($ score $)$.</p>
<p>Vanilla (Zheng et al., 2023): This strategy extends "Vanilla*", where the evaluator $f(\cdot)$ is tasked not only with scoring the model response $A_{\text {model }}$ but also providing an explanation for the score - represented as $f\left(P_{\text {eval }}, I, A_{\text {model }}\right) \rightarrow($ exp, score $)$.</p>
<p>Rubric (Zeng et al., 2023): In this strategy, in addition to the instruction $I$ and the model response $A_{\text {model }}$, we also provide a grading rubric $R$. The evaluator $f(\cdot)$ is prompted to first generate an explanation followed by a score - represented as $f\left(P_{\text {eval }}, R, I, A_{\text {model }}\right) \rightarrow($ exp, score $)$.</p>
<p>Axis (Hada et al., 2023): In this strategy, the evaluator $f(\cdot)$ is prompted to assess the model response, $A_{\text {model }}$, along a designated axis, $A x$, aligning with the category of the instruction (§ 3.1).</p>
<p>For instance, factual questions are evaluated along the hallucination axis to determine the presence of fabricated content. This process is formally represented as $f\left(P_{\text {eval }}, A x, I, A_{\text {model }}\right) \rightarrow$ (exp, score).</p>
<p>Axis+Rubric (Hada et al., 2023): In this strategy, the evaluator $f(\cdot)$ is provided with both a specific evaluation axis $A x$ and detailed scoring rubrics $R$ for that axis. The is formally represented as $f\left(P_{\text {eval }}, A x, R, I, A_{\text {model }}\right) \rightarrow($ exp, score $)$.</p>
<h3>4.2 Pairwise Comparison</h3>
<p>In this paradigm, evaluator $f(\cdot)$ is tasked to choose the better response from the two given options by again relying on its parameterized knowledge.</p>
<p>Pairwise* (Zheng et al., 2023): The evaluator $f(\cdot)$ here is given only an instruction $I$ and two model responses $A_{1}$ and $A_{2}$ and is tasked to determine the better response or mark both as equally valid. This is formally represented as $f\left(P_{\text {eval }}, I, A_{1}, A_{2}\right) \rightarrow($ verdict $)$.</p>
<p>Pairwise (Zheng et al., 2023): This strategy extends "Pairwise*", where the evaluator is tasked not only with choosing the better response but also providing an explanation for the verdict - represented as $f\left(P_{\text {eval }}, I, A_{1}, A_{2}\right) \rightarrow($ exp, verdict $)$.</p>
<p>Rules (Zeng et al., 2023): In this strategy, in addition to the instruction $I$ and the two model responses $A_{1}, A_{2}$, the evaluator $f(\cdot)$ is given detailed rules for evaluation and is asked to generate an explanation followed by the verdict. This process is formally represented as $f\left(P_{\text {eval }}, R, I, A_{1}, A_{2}\right) \rightarrow$ (exp, verdict).</p>
<p>Axis (Hada et al., 2023): Extending the Axis strategy defined in $\operatorname{Sec} \S 4.1$, the evaluator $f(\cdot)$ is asked to choose the better response along a designated axis $A x$. The evaluator is prompted with the instruction $I$, two model responses $A_{1}, A_{2}$, and the description of the axis $A x$ - represented as $f\left(P_{\text {eval }}, A x, R, I, A_{1}, A_{2}\right) \rightarrow($ exp, verdict $)$.</p>
<p>Axis+Rules (Zeng et al., 2023; Hada et al., 2023): Extending the Axis+Rubric strategy defined in Sec $\S 4.1$, this strategy involves choosing the better response along the designated axis $A x$. The evaluator is prompted with the instruction $I$, two model responses $A_{1}, A_{2}$, details about the axis $A x$, and detailed rules for evaluation - represented as $f\left(P_{\text {eval }}, A x, R, I, A_{1}, A_{2}\right) \rightarrow($ exp, verdict $)$.</p>
<h3>4.3 Reference-guided Single Answer Scoring</h3>
<p>In this paradigm, the evaluator $f(\cdot)$ is tasked to score a response by comparing against a reference. It is important to note that this approach may not be feasible for many open-ended questions.</p>
<p>Reference (Zheng et al., 2023): In this strategy, given an instruction $I$, a model response $A_{\text {model }}$, and a ground truth reference answer $A_{\text {gold }}$, the evaluator $f(\cdot)$ is tasked with scoring the model response, along with giving an explanation. This is formally represented as $f\left(P_{\text {eval }}, I, A_{\text {gold }}, A_{\text {model }}\right) \rightarrow($ exp, score $)$.</p>
<h2>5 Experiments</h2>
<p>We use GPT-4-TURBO as our primary evaluation model, given its widespread adoption (Zeng et al., 2023; Hada et al., 2024; Min et al., 2023). We also extend our analysis to other proprietary models - Gemini-1.5-Pro (Team et al., 2024) and Claude-3-Opus (Anthropic, 2024), opensource models like Llama-3-70B-Instruct ${ }^{\circledR}$ (Meta, 2024), and trained evaluator models like Prometheus 2 (Kim et al., 2024b) ${ }^{2}$. All evaluations are conducted at a temperature of zero to ensure reproducibility.
In single answer scoring (§ 4.1) paradigm, we measure the percentage of instances where the score remains unchanged by the perturbation as our metric. Ideally, except for score-invariant perturbations, the evaluator should penalize the score of the perturbed answer. For pairwise comparison paradigm (§ 4.2), we include our "gold" answer as one of the responses, requiring the evaluator to select the best response between the "gold" and the "perturbed" answer. Here, we measure the percentage of times the evaluator does not choose the gold answer as our metric. To mitigate position bias (Wang et al., 2023c), we conduct each evaluation twice, swapping the order of the gold and perturbed responses. For reference-guided single answer scoring paradigm (§ 4.3), the gold answer serves as the reference. Here, we measure the percentage of times the evaluator awards a perfect score to the perturbed answer as our metric.</p>
<h3>5.1 Is GPT-4-Turbo a good evaluator?</h3>
<p>Referring to the first section of Table 3, we observe that in the case of single answer scoring,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Strategy</th>
<th style="text-align: left;">$\mathbf{L F}_{\downarrow}$</th>
<th style="text-align: left;">$\mathbf{F}_{\downarrow}$</th>
<th style="text-align: left;">$\mathbf{I F}_{\downarrow}$</th>
<th style="text-align: left;">$\mathbf{R}_{\downarrow}$</th>
<th style="text-align: left;">$\mathbf{S I}_{\uparrow}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Single Answer Scoring</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Vanilla*</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.71</td>
<td style="text-align: left;">$\mathbf{0 . 2 2}$</td>
<td style="text-align: left;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">Vanilla</td>
<td style="text-align: left;">$\mathbf{0 . 5 7}$</td>
<td style="text-align: left;">$\mathbf{0 . 5 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 5 7}$</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">Rubric</td>
<td style="text-align: left;">0.85</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">0.80</td>
<td style="text-align: left;">0.33</td>
<td style="text-align: left;">0.96</td>
</tr>
<tr>
<td style="text-align: left;">Axis</td>
<td style="text-align: left;">0.83</td>
<td style="text-align: left;">0.74</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.43</td>
<td style="text-align: left;">0.96</td>
</tr>
<tr>
<td style="text-align: left;">Axis+Rubric</td>
<td style="text-align: left;">0.86</td>
<td style="text-align: left;">0.76</td>
<td style="text-align: left;">0.77</td>
<td style="text-align: left;">0.37</td>
<td style="text-align: left;">$\mathbf{0 . 9 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Pairwise Comparison</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Pairwise*</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">0.52</td>
<td style="text-align: left;">0.83</td>
<td style="text-align: left;">0.36</td>
<td style="text-align: left;">$\mathbf{0 . 9 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Pairwise</td>
<td style="text-align: left;">0.77</td>
<td style="text-align: left;">0.46</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.35</td>
<td style="text-align: left;">0.74</td>
</tr>
<tr>
<td style="text-align: left;">Rules</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.63</td>
<td style="text-align: left;">0.68</td>
<td style="text-align: left;">0.41</td>
<td style="text-align: left;">0.74</td>
</tr>
<tr>
<td style="text-align: left;">Axis</td>
<td style="text-align: left;">0.64</td>
<td style="text-align: left;">0.44</td>
<td style="text-align: left;">$\mathbf{0 . 5 9}$</td>
<td style="text-align: left;">$\mathbf{0 . 2 7}$</td>
<td style="text-align: left;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">Axis+Rules</td>
<td style="text-align: left;">$\mathbf{0 . 6 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 4 2}$</td>
<td style="text-align: left;">0.61</td>
<td style="text-align: left;">0.32</td>
<td style="text-align: left;">0.72</td>
</tr>
<tr>
<td style="text-align: left;">Reference-guided Single Answer Scoring</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: left;">0.26</td>
<td style="text-align: left;">0.11</td>
<td style="text-align: left;">0.49</td>
<td style="text-align: left;">0.04</td>
<td style="text-align: left;">0.63</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of different evaluation strategies using GPT-4-TURBO. The numbers indicate the percentage of instances where the score/verdict generated by the LLM evaluator is not affected by the perturbation. Lower values $(\downarrow)$ indicate better performance in all categories except SI . * denotes evaluators that only give a score without any justification.</p>
<p>GPT-4-TURBO fails to lower its score for the perturbed answer in a majority of the cases, except for Reasoning tasks. Further, the performance of GPT-4-TURBO is better when using simpler strategies, such as, Vanilla* and Vanilla, as compared to the more advanced strategies with explicit rubrics and/or specified axis of evaluation. This could imply that while adding additional rubrics and criteria may increase the overall thoroughness, it may not necessarily enhance the model's ability to detect subtler errors.</p>
<p>Now, referring to the second section of Table 3, we observe that in the case of pairwise comparison, GPT-4-TURBO fails to detect the perturbed answer in majority of the cases, except for Reasoning tasks. Further, in contrast to the above, in this case, advanced strategies perform better than the basic strategies. This indicates that for comparative evaluations, having detailed specific rules can help improve the reliability of the models. Lastly, referring to the first row of the last section of Table 3 , we observe that when a reference is provided, GPT-4-TURBO performs much better but there are still a notable number of failures. The evaluator, despite being presented with the gold answer marked as a reference answer, fails to recognize the perturbations in many cases, except for reasoning tasks where it performs very well. Our overall verdict is that GPT-4-TURBO is not a good evaluator as it fails to detect perturbations which cause a drop in</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Strategy</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">$\mathbf{L F}_{\downarrow}$</th>
<th style="text-align: center;">$\mathbf{F}_{\downarrow}$</th>
<th style="text-align: center;">$\mathbf{I F}_{\downarrow}$</th>
<th style="text-align: center;">$\mathbf{R}_{\downarrow}$</th>
<th style="text-align: center;">$\mathbf{S I}_{\uparrow}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{0 . 5 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 4}$</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">$\mathbf{0 . 2 5}$</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">Vanilla</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">$\mathbf{0 . 5 4}$</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Axis+Rules</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{0 . 6 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">$\mathbf{0 . 6 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Reference</td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{0 . 0 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 5}$</td>
<td style="text-align: center;">0.13</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.38</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of the performance of different models across the best-observed evaluation strategies. Lower values $(\downarrow)$ indicate better performance in all categories except SI .</p>
<p>the quality of the answer.</p>
<h3>5.2 How do other popular Evaluator LLMs perform?</h3>
<p>We extend our evaluation to other models and compare their performance when using the 3 best strategies identified in Table 3. Table 4 shows that GPT-4-TURBO consistently outperforms other models in both the reference-less paradigms. Due to the high API cost of using the CLAUDE-3-OPus model, we restrict its evaluation to only the Vanilla strategy, and note that it performed poorly as an Evaluator LLM.</p>
<p>In the reference-based paradigm, LLAMA-3-70B-INSTRUCT model surprisingly outperforms all others. Upon manually reviewing few instances, we observe that LLAMA-3-70B-INSTRUCT is a stringent evaluator and rarely awards perfect scores to even very well-formed answers when presented with a reference answer. While this may suggest that LLAMA-3-70B-INSTRUCT has a high evaluation standard, it also raises concerns about overlyrelying on the reference answer, which is typically not available in most practical scenarios. To further investigate this, we evaluate all the models on Score Invariant perturbations (Section $\S 3.5$ ) using the Reference evaluation strategy. Consistent with our prior observations, LLAMA-3-70B-INSTRUCT seldom awards perfect scores, doing so only in $13 \%$ of the cases as shown in Table 4. Lastly, looking at the last row of Table 4, we observe that even trained Evaluator LLMs like Prometheus 2 are</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison of perturbations detected solely by score analysis versus those identified with explanations. The highlighted region marked with stars denotes perturbations detected in explanations but not reflected in scores. Despite this, a significant proportion of perturbations remain undetected.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">LF↓</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">F↓</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">IF↓</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">R↓</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1-3</td>
<td style="text-align: center;">1-5</td>
<td style="text-align: center;">1-3</td>
<td style="text-align: center;">1-5</td>
<td style="text-align: center;">1-3</td>
<td style="text-align: center;">1-5</td>
<td style="text-align: center;">1-3</td>
<td style="text-align: center;">1-5</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;">A+R</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.38</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparing performance of Rubrics and Axis+Rubrics strategies with score range of 1-3 and 1-5. The numbers indicate the percentage of instances where the score generated by the LLM evaluator is not affected by the perturbation. Lower values (↓) indicate better performance in all categories.</p>
<p>worse than other general Evaluator LLMs.</p>
<h3>5.3 Does it help to look beyond scores?</h3>
<p>In addition to scoring, our evaluators also generate explanations that provide a justification for each score. We investigate whether these explanations detect the perturbations, even though this is not reflected in the scores. We prompt GPT-3.5TURBO model with explanations from the instances where the evaluator rated the perturbed answer as equal to the gold answer, asking it to identify if any mistake or error has been reported in the explanation. Figure 2 reveals that explanations are only marginally helpful. Although perturbations are sometimes identified, they are overlooked or not considered significant enough to penalize the score. It is important to note that all the perturbations here were intended to incur a scoring penalty. Thus, while explicitly considering the explanations offers a slight improvement in the evaluator's performance, the overall performance is still poor.</p>
<h3>5.4 What about score-invariant perturbations?</h3>
<p>We evaluate different Evaluator LLMs using score-invariant perturbations (§ 3.5). Ideally, the evaluator should not reduce its score for these perturbations in score-based evaluations and should deem both responses correct in pairwise evaluations. Referring to Table 3, in reference-less scoring, GPT-4-TURBO performs better when using non-vanilla evaluating strategies, while in pairwise comparison, it performs better when using simpler evaluation strategies. Similarly, as shown in Table 4, we observe that other Evaluator LLMs also perform well in a majority of cases. However, there is still a significant number of responses with score-invariant perturbations that they rate poorly.</p>
<h3>5.5 Does increasing the range help in scoring?</h3>
<p>Based on recommendations from Hada et al. (2023), our initial set-up for the Rubrics and Axis+Rubrics evaluators used a scoring range of 1 to 3 . To explore whether a wider scoring range could enhance the evaluators' ability to identify and account for the perturbations, we extended the range to 1 to 5 . Results presented in Table 5 suggest that this broader range slightly improves the evaluators' performance, perhaps due to the availability of more flexibility in scoring decisions.</p>
<h2>6 Conclusion</h2>
<p>We propose FBI, a novel framework designed to evaluate the proficiency of Evaluator LLMs in assessing four critical abilities: factual accuracy, instruction adherence, coherence in longform writing, and reasoning proficiency, through targeted perturbations. Our comprehensive study, involving 2400 perturbed answers across 22 categories and using three evaluation paradigms (singleanswer, pairwise, and reference-guided evaluation), reveals significant shortcomings in current Evaluator LLMs. Our findings show that even the most advanced models failed to identify quality drops in over $50 \%$ of cases on average. While reference-based evaluations performed relatively better, single-answer and pairwise evaluations demonstrated notable limitations. These results underscore the unreliable nature of current Evaluator LLMs and advocate for cautious implementation in practical applications. We hope that the FBI framework will be further extended and used for continued meta-evaluation of Evaluator LLMs.</p>
<h2>Limitations</h2>
<p>In our evaluation setup, detailed in Section 4, we concentrate on three primary evaluation paradigms: single-answer assessment, pairwise comparison, and reference-guided evaluation within a single model context and leave out multi-agent metaevaluation and for future work. While we have compiled a list of perturbation categories, we believe it is not exhaustive and there is room for further expansion. Our evaluation framework encompasses four fundamental task abilities, with plans to explore more advanced capabilities such as multilingual generation, tool usage, and planning in future work.</p>
<h2>Ethics</h2>
<p>All annotations described in Section 3 were done by students from our research group, all of whom hold at least a bachelor's or master's degree. This annotation was done as a part of their routine research work. The datasets used in this paper are all available under permissible licenses, and we adhere strictly to their intended usage, maintaining compliance with licensing requirements. Additionally, the code used for our evaluations and perturbation generation will be made publicly available under the MIT License ${ }^{3}$. We only used ChatGPT ${ }^{4}$ for assistance purely with the language of the paper, e.g., paraphrasing, spell-checking, or polishing the author's original content, without suggesting new content.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank EkStep Foundation and Nilekani Philanthropies for their generous grant, which supported this research. We extend our gratitude to Ananth, Devilal, Niharika, Nikhil, Sakshi, Sparsh, and Suhaas, Suriya for their invaluable assistance with manual audits. We also thank Raj Dabre and Anoop Kunchukuttan for their insightful discussions. We thank Google for supporting Sumanth's work through the Google Ph.D. Fellowship.</p>
<h2>References</h2>
<p>Anthropic. 2024. Introducing the next generation of claude. https://www.anthropic.com/news/ claude-3-family. Accessed: 2024-06-14.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. CoRR, abs/2308.07201.</p>
<p>Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. 2023. Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study. CoRR, abs/2304.00723.</p>
<p>Yiming Chen, Chen Zhang, Danqing Luo, Luis Fernando D'Haro, Robby T. Tan, and Haizhou Li. 2024. Unveiling the achilles' heel of nlg evaluators: A unified adversarial framework driven by large language models. arXiv preprint arXiv: 2405.14646.</p>
<p>Cheng-Han Chiang and Hung yi Lee. 2023. Can large language models be an alternative to human evaluations? Annual Meeting of the Association for Computational Linguistics.</p>
<p>David Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 15607-15631. Association for Computational Linguistics.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 30293051. Association for Computational Linguistics.</p>
<p>Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv: 2302.04166.</p>
<p>Rishav Hada, Varun Gumma, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. 2024. Metal: Towards multilingual meta-evaluation. arXiv preprint arXiv: 2404.01667.</p>
<p>Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, M. Choudhury, Kalika Bali, and Sunayana Sitaram. 2023. Are large language model-based evaluators the solution to scaling up multilingual evaluation? FINDINGS.</p>
<p>Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe Vieira Frujeri, and Ida Momennejad. 2023. Allure: Auditing and improving llm-based evaluation of text using iterative in-context-learning. arXiv preprint arXiv: 2309.13701.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.</p>
<p>Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, and Zhijiang Guo. 2024. Towards understanding factual knowledge of large language models. In The Twelfth International Conference on Learning Representations.</p>
<p>Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, and Rui Zhang. 2024. Evaluating llms at detecting errors in llm responses. arXiv preprint arXiv: 2404.03602.</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2023. Prometheus: Inducing finegrained evaluation capability in language models. CoRR, abs/2310.08491.</p>
<p>Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang, Jinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, Se June Joo, Miyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon Ye, Bill Yuchen Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024a. The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models.</p>
<p>Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon</p>
<p>Seo. 2024b. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv: 2405.01535.</p>
<p>Tom Kocmi and C. Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. European Association for Machine Translation Conferences/Workshops.</p>
<p>Zekun Li, Baolin Peng, Pengcheng He, and Xifeng Yan. 2023. Evaluating the instruction-following robustness of large language models to prompt injection. arXiv preprint arXiv: 2308.10819.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment. Conference on Empirical Methods in Natural Language Processing.</p>
<p>Adian Liusie, Potsawee Manakul, and Mark J. F. Gales. 2023. Llm comparative assessment: Zero-shot nlg evaluation through pairwise comparisons using large language models. arXiv preprint arXiv: 2307.07889.</p>
<p>Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984-4997, Online. Association for Computational Linguistics.</p>
<p>Meta. 2024. Introducing meta llama 3: The most capable openly available llm to date. https://ai.meta. com/blog/meta-llama-3/. Accessed: 2024-06-14.</p>
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv: 2305.14251.</p>
<p>Ben Naismith, Phoebe Mulcaire, and Jill Burstein. 2023. Automated evaluation of written discourse coherence using GPT-4. In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 394-403, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Arjun Panickssery, Samuel R. Bowman, and Shi Feng. 2024. Llm evaluators recognize and favor their own generations. arXiv preprint arXiv: 2404.13076.</p>
<p>Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 49024912, Online. Association for Computational Linguistics.</p>
<p>Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 2023. Branch-solve-merge improves large language model evaluation and generation. CoRR, abs/2310.15123.</p>
<p>Ananya B. Sai, Tanay Dixit, Dev Yashpal Sheth, Sreyas Mohan, and Mitesh M. Khapra. 2021. Perturbation CheckLists for evaluating NLG evaluation metrics. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7219-7234, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ananya Sai B, Tanay Dixit, Vignesh Nagarajan, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra, and Raj Dabre. 2023. IndicMT eval: A dataset to meta-evaluate machine translation metrics for Indian languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14210-14228, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. 2023. Large language models are not yet human-level evaluators for abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4215-4233, Singapore. Association for Computational Linguistics.</p>
<p>Gemini Team, Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry, Lepikhin, Timothy Lillicrap, Jean baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, Luke Viltits, Oscar Chang, Nobuyuki Morioka, George Tucker, Ce Zheng, Oliver Woodman, Nithya Attaluri, Tomas Kocisky, Evgenii Eltyshev, Xi Chen, Timothy Chung, Vittorio Selo, Siddhartha Brahma, Petko Georgiev, Ambrose Slone, Zhenkai Zhu, James Lottes, Siyuan Qiao, Ben Caine, Sebastian Riedel, Alex Tomala, Martin Chadwick, Juliette Love, Peter Choy, Sid Mittal, Neil Houlsby, Yunhao Tang, Matthew Lamm, Libin Bai, Qiao Zhang, Luheng He, Yong Cheng, Peter Humphreys, Yujia Li, Sergey Brin, Albin Cassirer, Yingjie Miao, Lukas Zilka, Taylor Tobin, Kelvin Xu, Lev Proleev, Daniel Sohn, Alberto Magni, Lisa Anne Hendricks, Isabel Gao, Santiago Ontanon, Oskar Bunyan, Nathan Byrd, Abhanshu Sharma, Biao Zhang, Mario Pinto, Rishika Sinha, Harsh Mehta, Dawei Jia, Sergi Caelles, Albert Webson, Alex Morris, Becca Roelofs, Yifan Ding, Robin Strudel, Xuehan Xiong, Marvin Ritter, Mostafa Dehghani, Rahma Chaabouni, Abhijit Karmarkar, Guangda Lai, Fabian Mentzer, Bibo Xu, YaGuang Li, Yujing Zhang, Tom Le Paine, Alex</p>
<p>Goldin, Behnam Neyshabur, Kate Baumli, Anselm Levskaya, Michael Laskin, Wenhao Jia, Jack W. Rae, Kefan Xiao, Antoine He, Skye Giordano, Lakshman Yagati, Jean-Baptiste Lespiau, Paul Natsev, Sanjay Ganapathy, Fangyu Liu, Danilo Martins, Nanxin Chen, Yunhan Xu, Megan Barnes, Rhys May, Arpi Vezer, Junhyuk Oh, Ken Franko, Sophie Bridgers, Ruizhe Zhao, Boxi Wu, Basil Mustafa, Sean Sechrist, Emilio Parisotto, Thanumalayan Sankaranarayana Pillai, Chris Larkin, Chenjie Gu, Christina Sorokin, Maxim Krikun, Alexey Guseynov, Jessica Landon, Romina Datta, Alexander Pritzel, Phoebe Thacker, Fan Yang, Kevin Hui, Anja Hauth, Chih-Kuan Yeh, David Barker, Justin Mao-Jones, Sophia Austin, Hannah Sheahan, Parker Schuh, James Svensson, Rohan Jain, Vinay Ramasesh, Anton Briukhov, DaWoon Chung, Tamara von Glehn, Christina Butterfield, Priya Jhakra, Matthew Wiethoff, Justin Frye, Jordan Grimstad, Beer Changpinyo, Charline Le Lan, Anna Bortsova, Yonghui Wu, Paul Voigtlaender, Tara Sainath, Shane Gu, Charlotte Smith, Will Hawkins, Kris Cao, James Besley, Srivatsan Srinivasan, Mark Omernick, Colin Gaffney, Gabriela Surita, Ryan Burnell, Bogdan Damoc, Junwhan Ahn, Andrew Brock, Mantas Pajarskas, Anastasia Petrushkina, Seb Noury, Lorenzo Blanco, Kevin Swersky, Arun Ahuja, Thi Avrahami, Vedant Misra, Raoul de Liedekerke, Mariko Iinuma, Alex Polozov, Sarah York, George van den Driessche, Paul Michel, Justin Chiu, Rory Blevins, Zach Gleicher, Adrià Recasens, Alban Brustemi, Elena Gribovskaya, Aurko Roy, Wiktor Gworak, Sébastien M. R. Arnold, Lisa Lee, James Lee-Thorp, Marcello Maggioni, Enrique Piqueras, Kartikeya Badola, Sharad Vikram, Lucas Gonzalez, Anirudh Baddepudi, Evan Senter, Jacob Devlin, James Qin, Michael Azzam, Maja Trebacz, Martin Polacek, Kashyap Krishnakumar, Shuo yiin Chang, Matthew Tung, Ivo Penchev, Rishabh Joshi, Kate Olszewska, Carrie Muir, Mateo Wirth, Ale Jakse Hartman, Josh Newlan, Sheleem Kashem, Vijay Bolina, Elahe Dabir, Joost van Amersfoort, Zafarali Ahmed, James Cobon-Kerr, Aishwarya Kamath, Arnar Mar Hrafnkelsson, Le Hou, Ian Mackinnon, Alexandre Frechette, Eric Noland, Xiance Si, Emanuel Taropa, Dong Li, Phil Crone, Anmol Gulati, Sébastien Cevey, Jonas Adler, Ada Ma, David Silver, Simon Tokumine, Richard Powell, Stephan Lee, Kiran Vodrahalli, Samer Hassan, Diana Mincu, Antoine Yang, Nir Levine, Jenny Brennan, Mingqiu Wang, Sarah Hodkinson, Jeffrey Zhao, Josh Lipschultz, Aedan Pope, Michael B. Chang, Cheng Li, Laurent El Shafey, Michela Paganini, Sholto Douglas, Bernd Bohnet, Fabio Pardo, Seth Odoom, Mihaela Rosca, Cicero Nogueira dos Santos, Kedar Soparkar, Arthur Guez, Tom Hudson, Steven Hansen, Chulayuth Asawaroengchai, Ravi Addanki, Tianhe Yu, Wojciech Stokowiec, Mina Khan, Justin Gilmer, Jaehoon Lee, Carrie Grimes Bostock, Keran Rong, Jonathan Caton, Pedram Pejman, Filip Pavetic, Geoff Brown, Vivek Sharma, Mario Lučić, Rajkumar Samuel, Josip Djolonga, Amol Mandhane, Lars Lowe Sjösund, Elena Buchatskaya, Elopeth White, Natalie Clay, Jiepu Jiang, Hyeontaek Lim, Ross Hemsley, Zeyncep Cankara, Jane Labanowski, Nicola De Cao, David</p>
<p>Steiner, Sayed Hadi Hashemi, Jacob Austin, Anita Gergely, Tim Blyth, Joe Stanton, Kaushik Shivakumar, Aditya Siddhant, Anders Andreassen, Carlos Araya, Nikhil Sethi, Rakesh Shivanna, Steven Hand, Ankur Bapna, Ali Khodaei, Antoine Miech, Garrett Tanzer, Andy Swing, Shantanu Thakoor, Lora Aroyo, Zhufeng Pan, Zachary Nado, Jakub Sygnowski, Stephanie Winkler, Dian Yu, Mohammad Saleh, Loren Maggiore, Yamini Bansal, Xavier Garcia, Mehran Kazemi, Piyush Patil, Ishita Dasgupta, Iain Barr, Minh Giang, Thais Kagohara, Ivo Danihelka, Amit Marathe, Vladimir Feinberg, Mohamed Elhawaty, Nimesh Ghelani, Dan Horgan, Helen Miller, Lexi Walker, Richard Tanburn, Mukarram Tariq, Disha Shrivastava, Fei Xia, Qingze Wang, ChungCheng Chiu, Zoe Ashwood, Khuslen Baatarsukh, Sina Samangooei, Raphaël Lopez Kaufman, Fred Alcober, Axel Stjerngren, Paul Komarek, Katerina Tsihlas, Anudhyan Boral, Ramona Comanescu, Jeremy Chen, Ruibo Liu, Chris Welty, Dawn Bloxwich, Charlie Chen, Yanhua Sun, Fangxiaoyu Feng, Matthew Mauger, Xerxes Dotiwalla, Vincent Hellendoorn, Michael Sharman, Ivy Zheng, Krishna Haridasan, Gabe Barth-Maron, Craig Swanson, Dominika Rogozitiska, Alek Andreev, Paul Kishan Rubenstein, Ruoxin Sang, Dan Hurt, Gamaleldin Elsayed, Renshen Wang, Dave Lacey, Anastasija Ilić, Yao Zhao, Adam Iwanicki, Alejandro Lince, Alexander Chen, Christina Lyu, Carl Lebsack, Jordan Griffith, Meenu Gaba, Paramjit Sandhu, Phil Chen, Anna Koop, Ravi Rajwar, Soheil Hassas Yeganeh, Solomon Chang, Rui Zhu, Soroush Radpour, Elnaz Davoodi, Ying Ian Lei, Yang Xu, Daniel Toyama, Constant Segal, Martin Wicke, Hanzhao Lin, Anna Bulanova, Adrià Puigdomènech Badia, Nemanja Rakićević, Pablo Sprechmann, Angelos Filos, Shaobo Hou, Víctor Campos, Nora Kassner, Devendra Sachan, Meire Fortunato, Chimezie Iwuanyanwu, Vitaly Nikolaev, Balaji Lakshminarayanan, Sadegh Jazayeri, Mani Varadarajan, Chetan Tekur, Doug Fritz, Misha Khalman, David Reitter, Kingshuk Dasgupta, Shourya Sarcar, Tina Ornduff, Javier Snaider, Fantine Huot, Johnson Jia, Rupert Kemp, Nejc Trdin, Antha Vijayakumar, Lucy Kim, Christof Angermueller, Li Lao, Tianqi Liu, Haibin Zhang, David Engel, Somer Greene, Anaïs White, Jessica Austin, Lilly Taylor, Shereen Ashraf, Dangyi Liu, Maria Georgaki, Irene Cai, Yana Kulizhskaya, Sonam Goenka, Brennan Saeta, Ying Xu, Christian Frank, Dario de Cesare, Brona Robenek, Harry Richardson, Mahmoud Alnahlawi, Christopher Yew, Priya Ponnapalli, Marco Tagliasacchi, Alex Korchemniy, Yelin Kim, Dinghua Li, Bill Rosgen, Kyle Levin, Jeremy Wiesner, Praseem Banzal, Praveen Srinivasan, Hongkun Yu, Çağlar Ünlü, David Reid, Zora Tung, Daniel Finchelstein, Ravin Kumar, Andre Elisseeff, Jin Huang, Ming Zhang, Ricardo Aguilar, Mai Giménez, Jiawei Xia, Olivier Dousse, Willi Gierke, Damion Yates, Komal Jalan, Lu Li, Eri Latorre-Chimoto, Duc Dung Nguyen, Ken Durden, Praveen Kallakuri, Yaxin Liu, Matthew Johnson, Tomy Tsai, Alice Talbert, Jasmine Liu, Alexander Neitz, Chen Elkind, Marco Selvi, Mimi Jasarevic, Livio Baldini Soares, Albert Cui, Pidong Wang, Alek Wenjiao Wang, Xinyu Ye, Krystal Kallarackal,</p>
<p>Lucia Loher, Hoi Lam, Josef Broder, Dan HoltmannRice, Nina Martin, Bramandia Ramadhana, Mrinal Shukla, Sujoy Basu, Abhi Mohan, Nick Fernando, Noah Fiedel, Kim Paterson, Hui Li, Ankush Garg, Jane Park, DongHyun Choi, Diane Wu, Sankalp Singh, Zhishuai Zhang, Amir Globerson, Lily Yu, John Carpenter, Félix de Chaumont Quitry, Carey Radebaugh, Chu-Cheng Lin, Alex Tudor, Prakash Shroff, Drew Garmon, Dayou Du, Neera Vats, Han Lu, Shariq Iqbal, Alex Yakubovich, Nilesh Tripuraneni, James Manyika, Haroon Qureshi, Nan Hua, Christel Ngani, Maria Abi Raad, Hannah Forbes, Jeff Stanway, Mukund Sundararajan, Victor Ungureanu, Colton Bishop, Yunjie Li, Balaji Venkatraman, Bo Li, Chloe Thornton, Salvatore Scellato, Nishesh Gupta, Yicheng Wang, Ian Tenney, Xihui Wu, Ashish Shenoy, Gabriel Carvajal, Diana Gage Wright, Ben Bariach, Zhuyun Xiao, Peter Hawkins, Sid Dalmia, Clement Farabet, Pedro Valenzuela, Quan Yuan, Ananth Agarwal, Mia Chen, Wooyeol Kim, Brice Hulse, Nandita Dukkipati, Adam Paszke, Andrew Bolt, Kiam Choo, Jennifer Beattie, Jennifer Prendki, Harsha Vashisht, Rebeca SantamariaFernandez, Luis C. Cobo, Jarek Wilkiewicz, David Madras, Ali Elqursh, Grant Uy, Kevin Ramirez, Matt Harvey, Tyler Liechty, Heiga Zen, Jeff Seifert, Clara Huiyi Hu, Andrey Khorlin, Maigo Le, Asaf Aharoni, Megan Li, Lily Wang, Sandeep Kumar, Norman Casagrande, Jay Hoover, Dalia El Badawy, David Soergel, Denis Vnukov, Matt Miecnikowski, Jiri Simsa, Praveen Kumar, Thibault Sellam, Daniel Vlasic, Samira Daruki, Nir Shabat, John Zhang, Guolong Su, Jiageng Zhang, Jeremiah Liu, Yi Sun, Evan Palmer, Alireza Ghaffarkhah, Xi Xiong, Victor Cotruta, Michael Fink, Lucas Dixon, Ashwin Sreevatsa, Adrian Goedeckemeyer, Alek Dimitriev, Mohsen Jafari, Remi Crocker, Nicholas FitzGerald, Aviral Kumar, Sanjay Ghemawat, Ivan Philips, Frederick Liu, Yannie Liang, Rachel Sterneck, Alena Repina, Marcus Wu, Laura Knight, Marin Georgiev, Hyo Lee, Harry Askham, Abhishek Chakladar, Annie Louis, Carl Crous, Hardie Cate, Dessie Petrova, Michael Quinn, Denese Owusu-Afriyie, Achintya Singhal, Nan Wei, Solomon Kim, Damien Vincent, Milad Nasr, Christopher A. Choquette-Choo, Reiko Tojo, Shawn Lu, Diego de Las Casas, Yuchung Cheng, Tolga Bolukbasi, Katherine Lee, Saaber Fatehi, Rajagopal Ananthanarayanan, Miteyan Patel, Charbel Kaed, Jing Li, Shreyas Rammohan Belle, Zhe Chen, Jaclyn Konzelmann, Siim Pöder, Roopal Garg, Vinod Koverkathu, Adam Brown, Chris Dyer, Rosanne Liu, Azade Nova, Jun Xu, Alanna Walton, Alicia Parrish, Mark Epstein, Sara McCarthy, Slav Petrov, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv: 2403.05530.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv: 2303.04048.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. arXiv preprint arXiv: 2305.17926.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023c. Large language models are not fair evaluators. CoRR, abs/2305.17926.</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. 2023d. Pandalm: An automatic evaluation benchmark for LLM instruction tuning optimization. CoRR, abs/2306.05087.</p>
<p>Ishaan Watts, Varun Gumma, Aditya Yadavalli, Vivek Seshadri, Swami Manohar, and Sunayana Sitaram. 2024. Pariksha: A scalable, democratic, transparent evaluation platform for assessing indic large language models.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903.</p>
<p>Minghao Wu and Alham Fikri Aji. 2023. Style over substance: Evaluation biases for large language models. arXiv preprint arXiv: 2307.03025.</p>
<p>Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2023. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv preprint arXiv: 2307.02477.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. CoRR, abs/2304.12244.</p>
<p>Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. 2023. FLASK: fine-grained language model evaluation based on alignment skill sets. CoRR, abs/2307.10928.</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2023. Evaluating large language models at evaluating instruction following. CoRR, abs/2310.07641.</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv: 2308.01862.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023a. LIMA: less is more for alignment. CoRR, abs/2305.11206.</p>
<p>Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023b. Instruction-following evaluation for large language models. CoRR, abs/2311.07911.</p>
<p>Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv: 2310.17631.</p>
<h2>A Manual Verication Process of the Perturbations</h2>
<p>We engaged 17 graduate student volunteers with a good understanding of Large Language Models to manually verify the perturbations. Each annotator was provided with the instruction, the original gold answer, and the GPT-4-TURBO generated perturbed answer. They were tasked with classifying each perturbation into one of five categories: (i) Valid Perturbation, (ii) Invalid Perturbation, (iii) Score Invariant Perturbation, (iv) Not Relevant, and (v) Not Sure. Additionally, annotators were given explanations of the expected perturbations and the reasons why GPT-4-TURBO considered them valid.</p>
<p>To facilitate this process, we developed a straightforward application, the interface of which is depicted in Figure 3. This tool highlights the differences between the original and perturbed answers to aid easy identification.</p>
<p>Annotators were instructed to label an answer as "Valid Perturbation" only if they believed the perturbation warranted a score penalty relative to the gold answer. Perturbations not affecting the score were to be labeled "Score Invariant". If a perturbation was deemed incorrect or not reflected in the perturbed answer, annotators were asked to adjust the perturbation manually. Perturbations irrelevant to the category were to be marked as "Not Relevant".</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Screenshot of the User Application developed for validating perturbations.</p>
<h1>B Detailed Results of Single Answer Evaluators</h1>
<p>Detailed results of Single Answer evaluators can be found in Table 6, 7, 8, 9, 10.</p>
<h2>C Detailed Results of Pairwise Evaluators</h2>
<p>Detailed results of Pairwise Evaluators can be found in Table 11, 12, 13, 14, 15.</p>
<h2>D Detailed Results of Reference-Guided Evaluators</h2>
<p>Detailed results of Reference-guided Evaluators can be found in Table 16, 17</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Perturbation Type</th>
<th style="text-align: center;">TOTAL <br> Errors</th>
<th style="text-align: center;">Detected <br> Errors</th>
<th style="text-align: center;">Undetected <br> Errors</th>
<th style="text-align: center;">\% Undetected <br> Errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">COHERENCE</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPREHENSIVENESS</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">0.91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CONSISTENCY</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAMMAR</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHRONOLOGY</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SPELLING</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">0.89</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">528</td>
<td style="text-align: center;">146</td>
<td style="text-align: center;">383</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: center;">F</td>
<td style="text-align: center;">CONTEXTUAL</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">0.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ENTITY</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INCORRECT FACT</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NUMBER ERRORSs</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">0.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPPOSITE FACT</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">0.57</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">REMOVE FACT</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">0.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">483</td>
<td style="text-align: center;">159</td>
<td style="text-align: center;">324</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">ASSUMPTIONS</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do LESS</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">0.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do MORE</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IONORE FORMAT</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SEQUENCE ERRORS</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">0.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">269</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">CALCULATIONS</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">121</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">0.19</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COPYING NUMBERS</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FINAL ERRORS</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INCORRECT UNITS</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WRONG FORMULA</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">494</td>
<td style="text-align: center;">383</td>
<td style="text-align: center;">111</td>
<td style="text-align: center;">0.22</td>
</tr>
</tbody>
</table>
<p>Table 6: Results from evaluating FBI using Vanilla* evaluator. An error is said to be detected if the evaluator penalizes the score of the perturbed answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Perturbation Type</th>
<th style="text-align: center;">TOTAL <br> Errors</th>
<th style="text-align: center;">Detected <br> Errors</th>
<th style="text-align: center;">Undetected <br> Errors</th>
<th style="text-align: center;">\% Undetected <br> Errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">COHERENCE</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPREHENSIVENESS</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CONSISTENCY</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAMMAR</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">0.57</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHRONOLOGY</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SPELLING</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">528</td>
<td style="text-align: center;">225</td>
<td style="text-align: center;">303</td>
<td style="text-align: center;">0.57</td>
</tr>
<tr>
<td style="text-align: center;">F</td>
<td style="text-align: center;">CONTEXTUAL</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ENTITY</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INCORRECT FACT</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">0.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NUMBER ERRORS</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPPOSITE FACT</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">REMOVE FACT</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">483</td>
<td style="text-align: center;">220</td>
<td style="text-align: center;">260</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">ASSUMPTIONS</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do LESS</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do MORE</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.38</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IONORE FORMAT</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SEQUENCE ERRORS</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">161</td>
<td style="text-align: center;">217</td>
<td style="text-align: center;">0.57</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">CALCULATIONS</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">112</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COPYING NUMBERS</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FINAL ERRORS</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INCORRECT UNITS</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WRONG FORMULA</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">494</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">124</td>
<td style="text-align: center;">0.25</td>
</tr>
</tbody>
</table>
<p>Table 7: Results from evaluating FBI using Vanilla evaluator. An error is said to be detected if the evaluator penalizes the score of the perturbed answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Perturbation Type</th>
<th style="text-align: center;">TOTAL <br> Errors</th>
<th style="text-align: center;">Detected <br> Errors</th>
<th style="text-align: center;">Undetected <br> Errors</th>
<th style="text-align: center;">\% Undetected <br> Errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">COHERENCE</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">0.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPREHENSIVENESS</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CONSISTENCY</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAMMAR</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHRONOLOGY</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SPELLING</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">528</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">449</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: center;">F</td>
<td style="text-align: center;">CONTEXTUAL</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ENTITY</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INCORRECT FACT</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NUMBER ERRORS</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPPOSITE FACT</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">REMOVE FACT</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">483</td>
<td style="text-align: center;">131</td>
<td style="text-align: center;">352</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">ASSUMPTIONS</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do LESS</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">0.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do MORE</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IUNORE FORMAT</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SEQUENCE ERRORS</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">305</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">CALCULATIONS</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COPYING NUMBERS</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FINAL ERRORS</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INCORRECT UNITS</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WRONG FORMULA</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">494</td>
<td style="text-align: center;">332</td>
<td style="text-align: center;">162</td>
<td style="text-align: center;">0.33</td>
</tr>
</tbody>
</table>
<p>Table 8: Results from evaluating FBI using Rubrics evaluator. An error is said to be detected if the evaluator penalizes the score of the perturbed answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Perturbation Type</th>
<th style="text-align: center;">TOTAL <br> Errors</th>
<th style="text-align: center;">Detected <br> Errors</th>
<th style="text-align: center;">Undetected <br> Errors</th>
<th style="text-align: center;">\% Undetected <br> Errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">COHERENCE</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPREHENSIVENESS</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CONSISTENCY</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAMMAR</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">0.82</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHRONOLOGY</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SPELLING</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">0.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">528</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">438</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;">F</td>
<td style="text-align: center;">CONTEXTUAL</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ENTITY</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INCORRECT FACT</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NUMBER ERRORS</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPPOSITE FACT</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">REMOVE FACT</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">483</td>
<td style="text-align: center;">127</td>
<td style="text-align: center;">356</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">ASSUMPTIONS</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">0.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do LESS</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do MORE</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IUNORE FORMAT</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SEQUENCE ERRORS</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">284</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">CALCULATIONS</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COPYING NUMBERS</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FINAL ERRORS</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INCORRECT UNITS</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WRONG FORMULA</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">494</td>
<td style="text-align: center;">308</td>
<td style="text-align: center;">186</td>
<td style="text-align: center;">0.43</td>
</tr>
</tbody>
</table>
<p>Table 9: Results from evaluating FBI using Axis evaluator. An error is said to be detected if the evaluator penalizes the score of the perturbed answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Perturbation Type</th>
<th style="text-align: center;">TOTAL <br> Errors</th>
<th style="text-align: center;">Detected <br> Errors</th>
<th style="text-align: center;">Undetected <br> Errors</th>
<th style="text-align: center;">\% Undetected <br> Errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">COHERENCE</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPREHENSIVENESS</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CONSISTENCY</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAMMAR</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHRONOLOGY</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">528</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">454</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;">F</td>
<td style="text-align: center;">Contextual</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">0.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ENTITY</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INCORRECT FACT</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NUMBER ERRORS</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Opposite FACT</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Remove Fact</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">483</td>
<td style="text-align: center;">114</td>
<td style="text-align: center;">369</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">Assumptions</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do Less</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do More</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ignore Format</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sequence Errors</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">0.92</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">293</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">Calculations</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COPYING NUMBERS</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Final Errors</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INCORRECT UNITS</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong Formula</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">494</td>
<td style="text-align: center;">310</td>
<td style="text-align: center;">184</td>
<td style="text-align: center;">0.37</td>
</tr>
</tbody>
</table>
<p>Table 10: Results from evaluating FBI using Axis+Rubrics evaluator. An error is said to be detected if the evaluator penalizes the score of the perturbed answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Perturbation Type</th>
<th style="text-align: center;">TOTAL <br> Errors</th>
<th style="text-align: center;">G</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;">Both $\checkmark$</th>
<th style="text-align: center;">Both $\boldsymbol{X}$</th>
<th style="text-align: center;">$\neq$</th>
<th style="text-align: center;">\% Undetected <br> Errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">CoHERENCE</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPREHENSIVENESS</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0.88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CONSISTENCY</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAMMAR</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHRONOLOGY</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">528</td>
<td style="text-align: center;">141</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">318</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: center;">F</td>
<td style="text-align: center;">Contextual</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EntitY</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IncORRECT FACT</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number Errors</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0.61</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Opposite Fact</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Remove Fact</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">483</td>
<td style="text-align: center;">234</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">132</td>
<td style="text-align: center;">0.52</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">Assumptions</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do Less</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do More</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ignore Format</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sequence Errors</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">178</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">109</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">Calculations</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Copying Numbers</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Final Errors</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Incorrect Units</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0.38</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong Formula</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">494</td>
<td style="text-align: center;">316</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">0.36</td>
</tr>
</tbody>
</table>
<p>Table 11: Results from evaluating FB1 using the Pairwise» evaluator. An error is said to be detected if the evaluator chooses the Gold Answer. G indicates the number of times the evaluator has chosen the Gold Answer, $\mathbf{P}$ for the Perturbed Answer, Both $\checkmark$ when both answers are correct, Both $X$ when both are incorrect, and $\neq$ for verdict inconsistencies.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Perturbation Type</th>
<th style="text-align: center;">TOTAL <br> Errors</th>
<th style="text-align: center;">G</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;">Both $\checkmark$</th>
<th style="text-align: center;">Both $\boldsymbol{X}$</th>
<th style="text-align: center;">$\neq$</th>
<th style="text-align: center;">\% Undetected <br> Errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">CoHERENCE</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPREHENSIVENESS</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CONSISTENCY</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">0.88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAMMAR</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHRONOLOGY</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">528</td>
<td style="text-align: center;">121</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">190</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">209</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: center;">F</td>
<td style="text-align: center;">Contextual</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.19</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Entity</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IncorReCT FACT</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number Errors</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">0.52</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Opposite Fact</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0.57</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Remove Fact</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">483</td>
<td style="text-align: center;">253</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">172</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">Assumptions</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do Less</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do More</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0.52</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ignore Format</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sequence Errors</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">121</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">134</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">Calculations</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Copying Numbers</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Final Errors</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IncorReCT Units</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong Formula</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">494</td>
<td style="text-align: center;">253</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">117</td>
<td style="text-align: center;">0.35</td>
</tr>
</tbody>
</table>
<p>Table 12: Results from evaluating FBI using the Pairwise evaluator. An error is said to be detected if the evaluator chooses the Gold Answer. G indicates the number of times the evaluator has chosen the Gold Answer, $\mathbf{P}$ for the Perturbed Answer, Both $\checkmark$ when both answers are correct, Both $X$ when both are incorrect, and $\neq$ for verdict inconsistencies.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Perturbation Type</th>
<th style="text-align: center;">TOTAL <br> Errors</th>
<th style="text-align: center;">G</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;">Both $\checkmark$</th>
<th style="text-align: center;">Both $\boldsymbol{X}$</th>
<th style="text-align: center;">$\neq$</th>
<th style="text-align: center;">\% Undetected <br> Errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">CoHERENCE</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPREHENSIVENESS</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CONSISTENCY</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">0.88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAMMAR</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0.91</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHRONOLOGY</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">528</td>
<td style="text-align: center;">132</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">196</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;">F</td>
<td style="text-align: center;">Contextual</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0.61</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EntitY</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">0.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IncORRECT FACT</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number Errors</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Opposite Fact</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Remove Fact</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">483</td>
<td style="text-align: center;">178</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">235</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">Assumptions</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do Less</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do More</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ignore Format</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sequence Errors</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">0.68</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">Calculations</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Copying Numbers</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Final Errors</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">0.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Incorrect Units</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.39</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong Formula</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">494</td>
<td style="text-align: center;">277</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">152</td>
<td style="text-align: center;">0.41</td>
</tr>
</tbody>
</table>
<p>Table 13: Results from evaluating FBI using the Rules evaluator. An error is said to be detected if the evaluator chooses the Gold Answer. G indicates the number of times the evaluator has chosen the Gold Answer, $\mathbf{P}$ for the Perturbed Answer, Both $\checkmark$ when both answers are correct, Both $X$ when both are incorrect, and $\neq$ for verdict inconsistencies.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Perturbation Type</th>
<th style="text-align: center;">TOTAL <br> Errors</th>
<th style="text-align: center;">G</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;">Both $\checkmark$</th>
<th style="text-align: center;">Both $\boldsymbol{X}$</th>
<th style="text-align: center;">$\neq$</th>
<th style="text-align: center;">\% Undetected <br> Errors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LF</td>
<td style="text-align: center;">CoHERENCE</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPREHENSIVENESS</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CONSISTENCY</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.81</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRAMMAR</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.63</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CHRONOLOGY</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spelling</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">0.89</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">528</td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">201</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">135</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: center;">F</td>
<td style="text-align: center;">Contextual</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EntitY</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IncORRECT FACT</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Number Errors</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.39</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Opposite Fact</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Remove Fact</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">483</td>
<td style="text-align: center;">272</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">113</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;">IF</td>
<td style="text-align: center;">Assumptions</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do Less</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Do More</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ignore Format</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sequence Errors</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">379</td>
<td style="text-align: center;">157</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">104</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;">R</td>
<td style="text-align: center;">Calculations</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">108</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Copying Numbers</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Final Errors</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Incorrect Units</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wrong Formula</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TOTAL</td>
<td style="text-align: center;">494</td>
<td style="text-align: center;">358</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">0.27</td>
</tr>
</tbody>
</table>
<p>Table 14: Results from evaluating FBI using the Axis evaluator. An error is said to be detected if the evaluator chooses the Gold Answer. $\mathbf{G}$ indicates the number of times the evaluator has chosen the Gold Answer, $\mathbf{P}$ for the Perturbed Answer, Both $\checkmark$ when both answers are correct, Both $\boldsymbol{X}$ when both are incorrect, and $\neq$ for verdict inconsistencies.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://opensource.org/licenses/MIT
${ }^{4}$ https://chatgpt.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
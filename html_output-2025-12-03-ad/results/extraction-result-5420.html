<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5420 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5420</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5420</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-29acc890e521f7a6415666ab9eb3432c49b4587a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/29acc890e521f7a6415666ab9eb3432c49b4587a" target="_blank">Self-critiquing models for assisting human evaluators</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning, and suggests that even large models may still have relevant knowledge they cannot or do not articulate as critiques with both topic-based summarization and synthetic tasks.</p>
                <p><strong>Paper Abstract:</strong> We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own selfcritiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5420.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5420.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-critiquing models (topic-based summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised fine-tuned self-critiquing transformer models for topic-based summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer decoder models (GPT-style) fine-tuned jointly to perform a base task (topic-based summarization) and a critique task (produce natural-language critiques of summaries). Models produce critiqueability/helpfulness scores and generate natural-language critiques used to assist humans and to condition refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fine-tuned GPT-style transformer (OpenAI internal models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Foundation transformer-decoder models pre-trained autoregressively (GPT-style) and then supervised-fine-tuned on base (Q→A), critiqueability (Q,A→Yes/No), critique (Q,A→C), helpfulness, and refinement tasks; five pre-trained checkpoints of varying capability/loss were used (exact parameter counts not specified).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-critiquing / model-generated critiques (generate-then-reflect)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Models are trained to output natural-language critiques C given question Q and answer A (Q,A→C). A critiqueability discriminator (Q,A→Yes/No) provides a score to decide whether to ask for a critique; a helpfulness discriminator (Q,A,C→Yes/No) provides logits that are used as a helpfulness score to rank or select critiques. Critiques are shown to humans (assistance) or fed back to the same model as conditional refinement input (Q,A,C→A').</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Topic-based summarization (primary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Summarize a specific topic-focused aspect of a passage (query-based/question-focused summarization) where the summary should cover a particular topic rather than the whole passage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Human labelers provided with model-generated critiques found ~50% more critiques overall; on deliberately misleading human-written answers, intended flaws were found 45% ± 5% with assistance vs 27% ± 5% without; for model answers, fraction of inputs with any found critique rose from 54% (no assist) to 78% (assist); average human Likert quality rating for model answers decreased from 5.6 (no assist) to 4.9 (assist) when critiques were shown.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline (no model critique assistance): human labelers found fewer critiques (reference above), intended flaws found 27% ± 5% on misleading answers; model answers had critiques found on 54% of inputs and mean rating 5.6.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative human-subject evidence: showing model-written critiques to labelers increases the number of distinct critiques found (~50% increase overall), increases discovery of intended (deliberately hidden) flaws (45% vs 27%), increases number of valid critiques per example (0.96 ± 0.09 vs 0.62 ± 0.07), and causes lower subjective ratings of answer quality (indicating uncovered flaws). Most used model critiques were accepted with little editing (only 15.3% edited), indicating direct usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limitations reported include: lack of objective ground truth for the summarization task (some critiques may be nitpicky or misleading), models may produce invalid or misleading critiques (risk of labelers accepting invalid critiques), assisted labelers might err by accepting spurious critiques, and the approach depends on human evaluators to validate critiques. Smaller models are less useful; the technique was demonstrated primarily with larger fine-tuned models. The paper also notes that critique helpfulness remains below human-written critiques and that models may know flaws they cannot articulate as critiques (generator-discriminator-critique gap).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-critiquing models for assisting human evaluators', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5420.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5420.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conditional refinement (generate-then-reflect / best-of-N critiques)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critique-conditional refinement using model-generated critiques and best-of-N selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-then-reflect refinement pipeline where the model first generates multiple candidate critiques, ranks/selects the best critique(s) using its helpfulness score (best-of-N / rejection sampling), then conditions on the selected critique to produce a refined answer (Q,A,C→A').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fine-tuned GPT-style transformer (same family as above)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same family of transformer-decoder models fine-tuned jointly on base, critique, discrimination, and refinement tasks; used both to generate critiques and to produce conditional refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>critique-conditional refinement (generate-then-reflect) with best-of-N critique selection</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For a given answer, sample N critiques at some temperature, score each with the model's helpfulness discriminator, pick the top critique (best-of-N rejection sampling), and then prompt the model to generate a refinement of the original answer conditioned on the chosen critique. Direct (unconditional) refinements and best-of-N direct-sampling baselines were also compared.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Topic-based summarization (refinement experiments) and synthetic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Refining an existing answer to a question/summary by conditioning on a critique; evaluated both on the primary summarization task and on synthetic tasks where ground-truth critique validity is known.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative/aggregated results: For larger models, critique-conditional refinements (especially when using best-of-N selection of critiques optimized by helpfulness) outperform direct refinements and the original sample, showing positive win-rate versus the original; larger N improves conditional refinement quality. Exact numeric win rates are reported in figures but not as single consolidated numeric values in text; improvement is described as significant for large models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Direct refinements (Q,A→A') and best-of-N direct sampling baselines (ranking refinements by critiqueability) are competitive; in some compute-controlled comparisons, rejection-sampling directly on answers can match or outperform using critiques. Small models show little to no improvement from either refinement method.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Empirical comparisons: conditional refinements using selected model critiques achieve higher win-rates against the original sample than direct refinements for larger models, and increasing the number of candidate critiques (N) used for best-of-N selection improves conditional-refinement outcomes. The paper shows conditional refinement wins relative to original and to direct-refinement baselines in figures and reports that these effects scale with model capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: small models do not benefit (little to no effect); rejection-sampling baselines on answers are competitive and can be stronger, making critique-based pipelines not always compute-efficient; refinements were typically single-step (no extensive iterative multi-step refinement explored); sometimes conditional refinements are forced even on perfect answers, complicating evaluation; results are noisy and depend on selection/ranking heuristics (helpfulness and critiqueability scores).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-critiquing models for assisting human evaluators', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5420.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5420.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-critiquing on synthetic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-critiquing and critique-discrimination experiments on synthetic oracle tasks (Addition, 3-SAT, Alphabetize, RACE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments on synthetic tasks with oracles for critique validity to measure how well models can critique or detect flaws in answers when ground truth is available, enabling direct measurement of critique helpfulness and gaps between generator, discriminator, and critique performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fine-tuned GPT-style transformer (multiple sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-decoder models fine-tuned in two rounds on synthetic oracle-generated base tasks and subsequent critique/critiqueability demonstrations; variants correspond to different model sizes/datasets per scale point.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-critiquing with oracle-checked critiques and best-of-N sampling</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Models generate critiques for answers to synthetic tasks; critique validity is checked against an oracle enabling direct measurement of helpfulness. Best-of-N sampling and critiqueability/discrimination oracles are used for evaluation and selection in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthetic tasks: Addition, 3-SAT, Alphabetize, RACE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Controlled tasks where answers and critiques have binary ground-truth validity (e.g., identify wrong digit in addition, identify unsatisfied clause in 3-SAT, flag ordering mistakes, correct multiple-choice answers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>General trends: On some synthetic tasks (e.g., 3-SAT, RACE) models can produce useful self-critiques and the critique helpfulness increases with model scale; however, results vary by task—some tasks show positive critique performance gains, others show smaller or negative CD gaps. Exact task-level accuracies/win-rates are plotted in paper figures; the text reports that trends are less consistent than on summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Generator and discriminator best-of-N baselines (e.g., best-of-2 discriminator) typically improve sample quality vs single-sample generation; in some synthetic tasks the discriminator-based selection outperforms critique-based selection or the critique gap behaves differently (sometimes negative CD gap).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Using oracle judgments, the authors show larger models produce more helpful self-critiques on some synthetic tasks (Figure 5). They also measure generator-discriminator-critique (G-D-C) gaps and find task-dependent behaviors: GD and GC gaps are often positive, CD gap varies by task (positive for 3-SAT and topic summarization, negative for Addition and RACE), indicating critiques don't always fully articulate flaws the discriminator can detect.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limitations include task-specific variability: some synthetic tasks show that critiquing lags discrimination (positive CD gap negative in some tasks), critique performance scaling is inconsistent across synthetic tasks, and training data/dataset differences across model sizes complicate comparisons. The synthetic setup also differs from summarization experiments (e.g., different fine-tuning datasets per model size).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-critiquing models for assisting human evaluators', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AI safety via debate <em>(Rating: 2)</em></li>
                <li>Scalable agent alignment via reward modeling: a research direction <em>(Rating: 2)</em></li>
                <li>Learning to give checkable answers with prover-verifier games <em>(Rating: 2)</em></li>
                <li>Project Debater <em>(Rating: 1)</em></li>
                <li>Training a helpful and harmless assistant with reinforcement learning from human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5420",
    "paper_id": "paper-29acc890e521f7a6415666ab9eb3432c49b4587a",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "Self-critiquing models (topic-based summarization)",
            "name_full": "Supervised fine-tuned self-critiquing transformer models for topic-based summarization",
            "brief_description": "Transformer decoder models (GPT-style) fine-tuned jointly to perform a base task (topic-based summarization) and a critique task (produce natural-language critiques of summaries). Models produce critiqueability/helpfulness scores and generate natural-language critiques used to assist humans and to condition refinements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fine-tuned GPT-style transformer (OpenAI internal models)",
            "model_description": "Foundation transformer-decoder models pre-trained autoregressively (GPT-style) and then supervised-fine-tuned on base (Q→A), critiqueability (Q,A→Yes/No), critique (Q,A→C), helpfulness, and refinement tasks; five pre-trained checkpoints of varying capability/loss were used (exact parameter counts not specified).",
            "reflection_method_name": "self-critiquing / model-generated critiques (generate-then-reflect)",
            "reflection_method_description": "Models are trained to output natural-language critiques C given question Q and answer A (Q,A→C). A critiqueability discriminator (Q,A→Yes/No) provides a score to decide whether to ask for a critique; a helpfulness discriminator (Q,A,C→Yes/No) provides logits that are used as a helpfulness score to rank or select critiques. Critiques are shown to humans (assistance) or fed back to the same model as conditional refinement input (Q,A,C→A').",
            "num_iterations": 1,
            "task_name": "Topic-based summarization (primary)",
            "task_description": "Summarize a specific topic-focused aspect of a passage (query-based/question-focused summarization) where the summary should cover a particular topic rather than the whole passage.",
            "performance_with_reflection": "Human labelers provided with model-generated critiques found ~50% more critiques overall; on deliberately misleading human-written answers, intended flaws were found 45% ± 5% with assistance vs 27% ± 5% without; for model answers, fraction of inputs with any found critique rose from 54% (no assist) to 78% (assist); average human Likert quality rating for model answers decreased from 5.6 (no assist) to 4.9 (assist) when critiques were shown.",
            "performance_without_reflection": "Baseline (no model critique assistance): human labelers found fewer critiques (reference above), intended flaws found 27% ± 5% on misleading answers; model answers had critiques found on 54% of inputs and mean rating 5.6.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative human-subject evidence: showing model-written critiques to labelers increases the number of distinct critiques found (~50% increase overall), increases discovery of intended (deliberately hidden) flaws (45% vs 27%), increases number of valid critiques per example (0.96 ± 0.09 vs 0.62 ± 0.07), and causes lower subjective ratings of answer quality (indicating uncovered flaws). Most used model critiques were accepted with little editing (only 15.3% edited), indicating direct usefulness.",
            "limitations_or_failure_cases": "Limitations reported include: lack of objective ground truth for the summarization task (some critiques may be nitpicky or misleading), models may produce invalid or misleading critiques (risk of labelers accepting invalid critiques), assisted labelers might err by accepting spurious critiques, and the approach depends on human evaluators to validate critiques. Smaller models are less useful; the technique was demonstrated primarily with larger fine-tuned models. The paper also notes that critique helpfulness remains below human-written critiques and that models may know flaws they cannot articulate as critiques (generator-discriminator-critique gap).",
            "uuid": "e5420.0",
            "source_info": {
                "paper_title": "Self-critiquing models for assisting human evaluators",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Conditional refinement (generate-then-reflect / best-of-N critiques)",
            "name_full": "Critique-conditional refinement using model-generated critiques and best-of-N selection",
            "brief_description": "A generate-then-reflect refinement pipeline where the model first generates multiple candidate critiques, ranks/selects the best critique(s) using its helpfulness score (best-of-N / rejection sampling), then conditions on the selected critique to produce a refined answer (Q,A,C→A').",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fine-tuned GPT-style transformer (same family as above)",
            "model_description": "Same family of transformer-decoder models fine-tuned jointly on base, critique, discrimination, and refinement tasks; used both to generate critiques and to produce conditional refinements.",
            "reflection_method_name": "critique-conditional refinement (generate-then-reflect) with best-of-N critique selection",
            "reflection_method_description": "For a given answer, sample N critiques at some temperature, score each with the model's helpfulness discriminator, pick the top critique (best-of-N rejection sampling), and then prompt the model to generate a refinement of the original answer conditioned on the chosen critique. Direct (unconditional) refinements and best-of-N direct-sampling baselines were also compared.",
            "num_iterations": 1,
            "task_name": "Topic-based summarization (refinement experiments) and synthetic tasks",
            "task_description": "Refining an existing answer to a question/summary by conditioning on a critique; evaluated both on the primary summarization task and on synthetic tasks where ground-truth critique validity is known.",
            "performance_with_reflection": "Qualitative/aggregated results: For larger models, critique-conditional refinements (especially when using best-of-N selection of critiques optimized by helpfulness) outperform direct refinements and the original sample, showing positive win-rate versus the original; larger N improves conditional refinement quality. Exact numeric win rates are reported in figures but not as single consolidated numeric values in text; improvement is described as significant for large models.",
            "performance_without_reflection": "Direct refinements (Q,A→A') and best-of-N direct sampling baselines (ranking refinements by critiqueability) are competitive; in some compute-controlled comparisons, rejection-sampling directly on answers can match or outperform using critiques. Small models show little to no improvement from either refinement method.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Empirical comparisons: conditional refinements using selected model critiques achieve higher win-rates against the original sample than direct refinements for larger models, and increasing the number of candidate critiques (N) used for best-of-N selection improves conditional-refinement outcomes. The paper shows conditional refinement wins relative to original and to direct-refinement baselines in figures and reports that these effects scale with model capability.",
            "limitations_or_failure_cases": "Reported limitations include: small models do not benefit (little to no effect); rejection-sampling baselines on answers are competitive and can be stronger, making critique-based pipelines not always compute-efficient; refinements were typically single-step (no extensive iterative multi-step refinement explored); sometimes conditional refinements are forced even on perfect answers, complicating evaluation; results are noisy and depend on selection/ranking heuristics (helpfulness and critiqueability scores).",
            "uuid": "e5420.1",
            "source_info": {
                "paper_title": "Self-critiquing models for assisting human evaluators",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Self-critiquing on synthetic tasks",
            "name_full": "Self-critiquing and critique-discrimination experiments on synthetic oracle tasks (Addition, 3-SAT, Alphabetize, RACE)",
            "brief_description": "Experiments on synthetic tasks with oracles for critique validity to measure how well models can critique or detect flaws in answers when ground truth is available, enabling direct measurement of critique helpfulness and gaps between generator, discriminator, and critique performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fine-tuned GPT-style transformer (multiple sizes)",
            "model_description": "Transformer-decoder models fine-tuned in two rounds on synthetic oracle-generated base tasks and subsequent critique/critiqueability demonstrations; variants correspond to different model sizes/datasets per scale point.",
            "reflection_method_name": "self-critiquing with oracle-checked critiques and best-of-N sampling",
            "reflection_method_description": "Models generate critiques for answers to synthetic tasks; critique validity is checked against an oracle enabling direct measurement of helpfulness. Best-of-N sampling and critiqueability/discrimination oracles are used for evaluation and selection in some experiments.",
            "num_iterations": 1,
            "task_name": "Synthetic tasks: Addition, 3-SAT, Alphabetize, RACE",
            "task_description": "Controlled tasks where answers and critiques have binary ground-truth validity (e.g., identify wrong digit in addition, identify unsatisfied clause in 3-SAT, flag ordering mistakes, correct multiple-choice answers).",
            "performance_with_reflection": "General trends: On some synthetic tasks (e.g., 3-SAT, RACE) models can produce useful self-critiques and the critique helpfulness increases with model scale; however, results vary by task—some tasks show positive critique performance gains, others show smaller or negative CD gaps. Exact task-level accuracies/win-rates are plotted in paper figures; the text reports that trends are less consistent than on summarization.",
            "performance_without_reflection": "Generator and discriminator best-of-N baselines (e.g., best-of-2 discriminator) typically improve sample quality vs single-sample generation; in some synthetic tasks the discriminator-based selection outperforms critique-based selection or the critique gap behaves differently (sometimes negative CD gap).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Using oracle judgments, the authors show larger models produce more helpful self-critiques on some synthetic tasks (Figure 5). They also measure generator-discriminator-critique (G-D-C) gaps and find task-dependent behaviors: GD and GC gaps are often positive, CD gap varies by task (positive for 3-SAT and topic summarization, negative for Addition and RACE), indicating critiques don't always fully articulate flaws the discriminator can detect.",
            "limitations_or_failure_cases": "Limitations include task-specific variability: some synthetic tasks show that critiquing lags discrimination (positive CD gap negative in some tasks), critique performance scaling is inconsistent across synthetic tasks, and training data/dataset differences across model sizes complicate comparisons. The synthetic setup also differs from summarization experiments (e.g., different fine-tuning datasets per model size).",
            "uuid": "e5420.2",
            "source_info": {
                "paper_title": "Self-critiquing models for assisting human evaluators",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AI safety via debate",
            "rating": 2
        },
        {
            "paper_title": "Scalable agent alignment via reward modeling: a research direction",
            "rating": 2
        },
        {
            "paper_title": "Learning to give checkable answers with prover-verifier games",
            "rating": 2
        },
        {
            "paper_title": "Project Debater",
            "rating": 1
        },
        {
            "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "rating": 1
        }
    ],
    "cost": 0.012923,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-critiquing models for assisting human evaluators</h1>
<p>William Saunders<em> Catherine Yeh</em> Jeff Wu*<br>Steven Bills Long Ouyang Jonathan Ward Jan Leike<br>OpenAI</p>
<h4>Abstract</h4>
<p>We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own selfcritiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.</p>
<h2>1 Introduction</h2>
<h3>1.1 Motivation</h3>
<p>With increasingly capable language models, it is important to ensure models are trustworthy on difficult and high stakes tasks. For example, models are being used to write complex pieces of code $\left[\mathrm{CTJ}^{+} 21, \mathrm{LCC}^{+} 22\right]$ and answer open-ended questions about the world $\left[\mathrm{NHB}^{+} 21, \mathrm{MTM}^{+} 22\right]$. We would like to be able to train models that don't write buggy code or spread misinformation.</p>
<p>However, fully evaluating correctness of code or veracity of facts about the world requires a lot of effort and expertise. Techniques to train systems from human feedback [ $\mathrm{NR}^{+} 00$, Wes16, CLB ${ }^{+} 17$, JMD20, NMS $^{+} 21, \mathrm{SCC}^{+} 22$ ], fundamentally depend on humans' ability to demonstrate and evaluate the quality of model outputs. This leads to the problem of scalable oversight [AOS+16]: How can we effectively provide feedback to models on tasks that are difficult for humans to evaluate?
One idea to overcome this problem is to use AI systems to aid human evaluation. This basic idea comes up in many prior proposals, such as iterated amplification [CSA18], debate [ICA18], and recursive reward modeling $\left[\mathrm{LKE}^{+} 18\right]$. If we first train a model to perform simpler assistive tasks that humans can evaluate, then we can use this model to assist humans with the evaluation of harder tasks. A key assumption is that evaluating the assistance task is simpler than evaluating the "base"</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Assistance from our models reliably causes labelers to find more critiques, on answers generated from all three distributions (x-axis). Most of the critiques found in the assistance condition came directly from using model critiques. The number of used model critiques is comparable to the number of critiques found in the "no assist" condition.</p>
<p>Note: Throughout the paper, all error bars shown either use bootstrapping at the passage level or simply calculate standard error of the mean (when appropriate), and represent z = 1 (i.e., one standard deviation on each side). All results use data from test set passages which were held out from training.</p>
<p>task. For example, verifying a bug in code is easier than finding bugs. This idea can also be justified by making an analogy between scalable oversight and complexity theory (Appendix B).</p>
<p>In this work we explore a simple form of assistance: natural language critiques of model outputs. Critiques are a particularly natural form of assistance from the point of view of preventing misleading outputs. If a human evaluator doesn't carefully check a model's outputs, the model might learn to give solutions that look good to the evaluator but are systematically flawed in a way that exploits human biases. We hope an equally smart critique model can help humans to notice these flaws. If models can generate outputs they "know" have flaws, but cannot explain these flaws to human evaluators, then they won't be effective assistants. This further motivates us to improve a model's ability to critique relative to its ability to discriminate answer quality.</p>
<h3>1.2 Contributions</h3>
<p>We fine-tune large language models [BMR+20, CND+22, HBM+22] jointly on both a base task and its corresponding critique task. For the base task, we focus primarily on a topic-based summarization task of summarizing some particular aspect of a given passage. The critique task is to find errors in topic-based summaries, given a passage and topic. We additionally study some synthetic tasks.</p>
<p>Our key contributions are:</p>
<p><strong>(1) Model-written critiques help humans find flaws they would have missed (Figure 1, Section 3.4).</strong> Human labelers asked to find critiques of (model or human-written) answers find about 50% more critiques when given assistance from a critique model. Furthermore, with answers written to be deliberately misleading, assisted labelers find the intended critiques 50% more often.</p>
<p><strong>(2) Critique helpfulness scales favorably with model capabilities (Figure 4, Section 4.2).</strong> Larger models are generally better at critiquing themselves, despite having harder-to-critique answers. That is, their ability to critique keeps up with their ability to give more convincing answers. We generally observe similar but less consistent trends on synthetic tasks (Figure 5).</p>
<p><strong>(3) Large models can use critiques to help refine their own answers (Figure 6, Section 4.3).</strong> Model-generated critiques help models directly improve their own answers. Using rejection sampling to find good critiques makes this improvement larger than a baseline of refining directly without a critique. For both kinds of refinement, improvement scales favorably with model size, with small models showing no improvement.</p>
<table>
<thead>
<tr>
<th>Task type</th>
<th>Inputs $\rightarrow$ Output</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base</td>
<td>$Q \rightarrow A$</td>
<td>Given a question, output an answer to it</td>
</tr>
<tr>
<td>Critiqueability</td>
<td>$Q, A \rightarrow{\mathrm{Yes}, \mathrm{No}}$</td>
<td>Given a question, and an answer to it, output whether the answer contains flaws</td>
</tr>
<tr>
<td>Critique</td>
<td>$Q, A \rightarrow C$</td>
<td>Given a question, and an answer to it, output a natural language critique of the answer</td>
</tr>
<tr>
<td>Helpfulness</td>
<td>$Q, A, C \rightarrow{\mathrm{Yes}, \mathrm{No}}$</td>
<td>Given a question, an answer to it, and a critique of the answer, output whether the critique is valid and helpful</td>
</tr>
<tr>
<td>Conditional refinement</td>
<td>$Q, A, C \rightarrow A$</td>
<td>Given a question, an answer to it, and a critique of the answer, output a new answer that addresses the critique</td>
</tr>
<tr>
<td>Direct refinement</td>
<td>$Q, A \rightarrow A$</td>
<td>Given a question and an answer to it, output a new answer that improves the answer</td>
</tr>
</tbody>
</table>
<p>Table 1: The primary set of tasks our models are jointly trained on. $Q, A$, and $C$ represent the space of questions, answers, and critiques, respectively. In our case, they are all texts of limited token lengths. We also train on a small amount of data for exploratory auxiliary tasks, such as corroborating answers and retrieving supporting quotes of various kinds.
(4) We motivate and measure generator-discriminator-critique gaps (Section 5). We propose a new methodology to compare a model's ability to generate answers, discriminate answer quality, and critique answers. Using the methodology, we study the scaling trends on topic-based summarization and in synthetic domains. In our experiments we failed to find a clear trend showing critique performance catching up to discriminator performance, implying that larger models still have relevant knowledge they don't articulate as critiques. Future effort should be directed at studying and improving on critique performance relative to discrimination performance.
(5) We release our training datasets and samples from our assistance experiments. We release a dataset with tens of thousands of human-written critiques, refinements, critique evaluations, and more, used to train our topic-based summarization models. We also release a dataset from our assistance experiments, including a dataset of misleading answers and intended flaws.</p>
<h1>2 Dataset collection and model training</h1>
<p>At a high level, we start with collecting demonstrations of some "base task," and use supervised fine-tuning (SFT) to train models to do that task. We then collect demonstrations of critiques of the model's answers, and fine-tune a new model to jointly do the base task and critique task. We proceed to iterate, with many rounds of data collection for a variety of tasks, and with the models training jointly on all tasks.</p>
<h3>2.1 Structure of tasks</h3>
<p>First, we assume there is some arbitrary base task. We assume no structure to the task, except that there should be some input, which we call the question, and output, the answer. The critique task then asks for a flaw in the answer to be pointed out, given the question and answer pair.
We then define corresponding binary discrimination tasks, which judge the outputs to the base task (answers) and critique task (critiques). The answer discrimination task-whether the answer contains any flaws-is called critiqueability. We hope that whenever an answer is critiqueable, we would be able to generate a concrete critique. The critique discrimination task-whether a critique points out a legitimate shortcoming of the answer-is called helpfulness.
Finally, we define a refinement task, in which we ask for a new answer, in light of some critique of an answer. We call this conditional refinement, to distinguish it from the variant of direct refinementgiving a better answer given an existing answer without conditioning on a critique. Of course, we can also ask for critiqueability of refinement outputs.
For a summary of these tasks, see Table 1. For an example, see Table 2.</p>
<p>Question: Our King has always been a little different. He's very quiet I haven't really seen him give anyone orders at all either. Whenever something needs to be done I see him doing it. [. . .] That's when I believed everything he told me - while he took me adventuring with him. Now I know that there is a better life to be had out of this one. A less edged and more rounded one. Thank you king Steve. Minecraft Steve.
Summarize the story's setting.
Answer: The story is set in a medieval kingdom, where the king is the main character of the story. The king is described as a good and active king. The narrator is a child who has always wondered why the king is so different from everyone else.</p>
<p>Critiqueable: Yes
Critique: The answer should mention that the setting is a video game.
Helpful: Yes
Conditional refinement: The story is set in a medieval kingdom in the video game Minecraft. The king is the main character of the story. The king is described as a good and active king. The narrator is a child who has always wondered why the king is so different from everyone else.
Table 2: Representative example of a topic-based summarization task, and its associated tasks. See Appendix A. 5 for details on how we format our tasks (different than shown).</p>
<h1>2.2 Topic-based summarization</h1>
<p>We report most of our main results on the base task of topic-based summarization [Dan05, ZYY ${ }^{+} 21$ ], a task similar to or interchangeable with query-based summarization and question-focused summarization. In topic-based summarization, the summary focuses on a specific aspect of a text rather than trying to summarize the whole text. See Table 2 for an example.
We collected our own dataset of over 6,000 distinct topical queries and summaries, on over 2,000 distinct passages. Our distribution of passages is sampled from a dataset of short stories, Wikipedia articles, or web articles (mostly news) scraped from the internet. Most tasks were generated based on short texts with less than 2,048 tokens when encoded with the GPT-2 tokenizer [RWC ${ }^{+} 19$ ]. We also gathered some tasks based on texts with up to 4,096 tokens which were not used for training.
Our labelers generated between 1 and 8 topic-based summarization questions per passage, typically also including a topic not covered by the passage (for which the answer is empty). Summaries are up to a paragraph long - we targeted between 2-10 sentences unless the topic was missing. We aimed for these topics to be non-trivial to summarize in various ways. See Appendix A for details.</p>
<h3>2.2.1 Data collection</h3>
<p>We collect demonstrations on all the tasks mentioned in Section 2.1. Given a task for which we want to collect a demonstration, we can choose whether each input is generated from a model or human. We always use a human-generated question. All tasks but the base task require an answer as input, many for which we typically use outputs from our best model. For example, critique demonstrations are on model-generated answers, and helpfulness judgements are on model-generated critiques. For refinements the situation is more complex, and detailed in Appendix A.2.
Since we need model outputs for most demonstrations, we collect data in rounds. After each round, we train a model jointly on all task demonstrations collected thus far. We start with base task demonstration collection. Then with a model trained on only the base task, we collect demonstrations for critiqueability, critique, and refinement tasks using model-generated answers. Finally, we collect demonstrations for helpfulness tasks, by showing labelers model-generated critiques of modelgenerated answers.
For more details on our data collection, see Appendix A and Table 4. We publicly release all data used to train final models ${ }^{2}$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2.2.2 Models</h1>
<p>Similarly to [DL15, RNSS18, BHA ${ }^{+} 21$ ], we start with foundation models pre-trained to autoregressively predict the next token in a large text corpus. All of our models are transformer decoders $\left[\mathrm{VSP}^{+} 17\right]$ in the style of GPT-3 [RNSS18, BMR ${ }^{+} 20$ ].</p>
<p>We fine-tune pre-trained models using supervised learning to predict human labels on all of these tasks. Joint training means that there is no capability asymmetry between the base and critique models-thus we expect that any mistakes the base model "knows about" would also be "known" by the critique model.</p>
<p>We combine critiqueability tasks with answer "Yes" and critique tasks into a single training example (see Appendix A.5). Otherwise we have each example corresponding to a task, and shuffle all the examples for training. Note that our examples are not i.i.d. for multiple reasons: we have multiple questions per passage, the refinement demonstrations are collected at the same time as critique demonstrations, etc. See Appendix A for details.</p>
<p>Our models are trained for one epoch and we tune only the learning rate, with remaining hyperparameters fixed to be similar to pre-training.</p>
<p>We mask out all tokens except those corresponding to the human demonstrations. For example, in the critique task, we mask out the passage, topic, and answer being critiqued. See Appendix A. 5 for details on input format.</p>
<h2>Critiqueability and helpfulness score</h2>
<p>Recall that for discrimination tasks, we collect binary yes/no labels. Rather than sampling binary labels from our models, we can look directly at logits to recover a probability. Thus we often use the terms critiqueability score and helpfulness score to refer to the quantity $\frac{\operatorname{Pr}[\text { Yes }]}{\operatorname{Pr}[\text { Yes }]+\operatorname{Pr}[\text { No }]}$ on the corresponding input.</p>
<p>On the critique task we "force" the model to give a critique even if the answer is perfect. Separately, the critiqueability score can be used to determine whether to ask it to critique in the first place, and the helpfulness score can be used to determine whether the critique is good after the fact.</p>
<h2>Model scale</h2>
<p>We use five pre-trained models with varying capabilities. Our pre-trained models are unfortunately not directly comparable to one another (for example, due to different pre-training datasets). However, on models which are directly comparable, the number of parameters correlates strongly with supervised fine-tuning validation loss. Using loss as the natural way to compare models of different architecture is suggested by $\left[\mathrm{CCG}^{+} 22\right]$, though here we use loss measured on fine-tuning instead of pre-training since it is the dataset commonality. Thus throughout the paper, we use "model scale" to refer to loss, measured in nats per token, and use that instead of model size for scaling laws $\left[\mathrm{KMH}^{+} 20\right]$.</p>
<h3>2.3 Synthetic tasks</h3>
<p>We also report results on four "synthetic" tasks, described in Table 3. For these tasks, we don't require human data collection because we have binary ground truth for both answer and critique validity. We use hand-coded oracles for each of the base, critiqueability, critique, and helpfulness tasks.</p>
<p>Our tasks are chosen based on two criteria:</p>
<ol>
<li>Evaluating critiques is easier than evaluating the base tasks.</li>
<li>The task is difficult but possible for most models. We tweak free parameters (e.g. sentence length for the unscramble task or number of digits for addition) to achieve this.</li>
</ol>
<p>For our synthetic task models, we trained two rounds of models:</p>
<ol>
<li>First we train on 100,000 generated base tasks with oracle demonstrations.</li>
<li>We then add 100,000 critiqueability task demonstrations, sub-sampled such that exactly half have incorrect answers, and 50,000 critique task demonstrations on that half. Answers are sampled from the first model at temperature 0 , which we find improves accuracy. (We</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Base task description</th>
<th style="text-align: center;">Critique task description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Addition</td>
<td style="text-align: center;">Add two 6-digit numbers</td>
<td style="text-align: center;">A digit in the answer whose value is wrong, as well as the correct value for that digit (digits are indexed from least significant to most significant)</td>
</tr>
<tr>
<td style="text-align: center;">Question: $505579+900050$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer: 1505629</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Critique: Digit at index 6 should be 4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3-SAT</td>
<td style="text-align: center;">Given a satisfiable boolean formula in CNF form, output a satisfying assignment</td>
<td style="text-align: center;">A clause that is not satisfied</td>
</tr>
<tr>
<td style="text-align: center;">Question: Provide boolean values for $a, b, c, d, e, f, g, h, i$ that satisfy the following</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">formula: $(\neg i \vee \neg f \vee e) \wedge(\neg e \vee \neg g \vee c) \wedge(g \vee \neg f \vee d) \wedge(\neg g \vee f \vee a) \wedge \ldots$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer: $a=$ false, $b=$ true, $c=$ false, $d=$ true, $e=$ false, $f=$ false, $g=$ true, $h=$ false, $i=$ true</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Critique: The following clause is not satisfied: $(\neg g \vee f \vee a)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Alphabetize</td>
<td style="text-align: center;">Given a list of 18 words, sort them in alphabetical order</td>
<td style="text-align: center;">Either a missing/extra word in the resulting list, or a pair of adjacent words in the wrong order</td>
</tr>
<tr>
<td style="text-align: center;">Question: Alphabetize the following words: growing prompts determining recreation evolve payable ruled patrols estimate emergency fate shrimp urges intoxicated narrator revert players pharmaceutical</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer: determining emergency evolve estimate fate growing intoxicated narrator</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">patrols pharmaceutical payable players prompts recreation revert ruled shrimp urges</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Critique: Words misordered: evolve comes alphabetically after estimate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RACE</td>
<td style="text-align: center;">Provide the answers to two multiple choice questions about the same text passage. Questions are drawn from the RACE dataset [LXL ${ }^{\wedge}$ 17].</td>
<td style="text-align: center;">Specify a question with a wrong answer, and give the correct answer</td>
</tr>
<tr>
<td style="text-align: center;">Question: [passage]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Q1. Which one is the best title of this passage? A. Developing your talents. B. To face the fears about the future. C. Suggestions of being your own life coach. D. How to communicate with others.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Q2. How many tips does the writer give us? A. Two. B. Four. C. One. D. Three.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Answer: $1=\mathrm{C}, 2=0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Critique: Answer to question 2 should be A</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Synthetic tasks with examples
occasionally repeat tasks when accuracy is so low or high that sub-sampling cannot guarantee uniqueness.)</p>
<p>This setup differs from the setup of topic-based summarization in two ways: (1) Each different model size is fine-tuned on a qualitatively different dataset in the second round. For topic-based summarization, different models are all trained on the same dataset. (2) We don't do a third round of training on helpfulness tasks, although we do use the helpfulness oracle for evaluations.</p>
<h1>3 Assisting critique finding</h1>
<p>We ran experiments where our models assist human labelers at writing a set of critiques for answers. The assistance itself is a set of critiques shown to the labeler.</p>
<h3>3.1 Motivation</h3>
<p>We chose this task because:</p>
<ul>
<li>Finding critiques is an important subtask of evaluating answer quality in general.</li>
<li>We thought it would be the easiest task to use to measure the effect of model assistance. We initially tried a comparison-based task but it was more difficult to work with (see Appendix E).</li>
<li>Suggesting critiques is a particularly natural form of assistance for critique-finding.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Even though the largest contribution from our models is finding more minor errors, model assistance also helps uncover some major and critical errors (left). The majority of model-written critiques point out lack of summary coverage, but there is also increase in found inaccuracies (right).</p>
<p>Importantly, our models do not have to always produce valid critiques to be helpful to human labelers, though too many invalid critiques would waste the labelers' time.</p>
<h1>3.2 Setup</h1>
<p>Labelers are given a topic-based summarization question and summary, and instructed to write as many different possible critiques as they can think of for the summary. They are also instructed to only read as much of the text as necessary to write the critiques.
In the baseline no assist condition, the labeler had no assistance and had to write all critiques themselves. In the assist condition, the labeler would also see 8 model-generated critiques of the answer, one at each of the following temperatures: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 1.0]. Critiques were de-duplicated, sorted according to the model's helpfulness score, and shown to the human labeler along with their helpfulness scores. The labeler could click a button to "use" any of the model generated critiques. If they did this, it would pre-fill the labeler critique with the model-written text, which they could further edit. We use our best model for generating all assistance.
We use questions for longer passages (2048 to 3840 tokens in length) in order to observe a larger effect from assistance. We noticed no degradation in critique writing ability despite training only on passages up to length 2048.
For each question, we use three different kinds of answers to critique:</p>
<ul>
<li>Model: An answer sampled from our best model, the same as the critiquing model, using a randomly chosen temperature. ${ }^{3}$</li>
<li>Human: An answer generated by a labeler doing the base task honestly.</li>
<li>Human misleading: An answer generated by the same labeler to deliberately have a hard-to-detect flaw. (Details in Appendix A).</li>
</ul>
<p>This results in six total conditions. For each summary, we had six labelers finding critiques, assigning one to each of the six conditions. We ensured labelers had not seen the text before (if they had, it would get reassigned). We did not limit the amount of time the labeler took performing the task: we instructed them to take enough time to cover all possible critiques of the answer at least $90 \%$ of the time.</p>
<p>We also ask the labeler to provide a 1-7 Likert rating for quality of the answer, with 7 indicating a perfect answer. For each critique the labeler finds, we ask for a severity level, ranging from minor to critical, and a category (described in Appendix A).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.3 Findings</h1>
<p>We find that critiques written by our model significantly help labelers find critiques:</p>
<ol>
<li>Assistance helps labelers find more critiques under all answer conditions (Figure 1). In the assistance conditions, most labeler-written critiques "used" a model-critique and the number of used critiques is comparable to the number of critiques found without assistance. Only $15.3 \%$ of the used critiques were edited further.</li>
<li>Assistance does not appreciably affect the time taken by labelers (Figure 15). The primary effect is that labelers take a bit longer because they write more critiques when given assistance.</li>
<li>The number of critiques increases for all severity levels and critique categories (Figure 2). Though the biggest increases comes from minor critiques, we also find increases in major and critical critiques. Similarly, we see the biggest increase in coverage critiques, but also a modest increase in accuracy critiques.</li>
<li>With assistance answers are more frequently critiqued and are given lower ratings (Figure 16). This suggests that the increase in critiques translates to a different subjective sense of quality. For example, on model answers, unassisted labelers found critiques on just $54 \%$ of inputs and gave an average rating of 5.6, while assisted labelers found critiques on $78 \%$ of inputs and gave an average rating of 4.9 .</li>
</ol>
<p>Note that our critique model was trained on data from the same pool of labelers-we are essentially leveraging our models to bootstrap our labelers to be more careful at the base task.</p>
<h3>3.3.1 Checking for intended critiques</h3>
<p>One potential confounder for our results is that because labelers are shown many critiques, they are occasionally erring and accepting critiques that are not truly valid. In other words, we have no way to distinguish between a valid critique of a poor answer from a misleading critique of a valid answer.
To break this symmetry, we had the authors of the misleading answers also generate the corresponding "intended" critiques (usually just one per answer). We then ask them to check whether the other labelers in the assistance experiment found critiques covering the intended critiques. We also ask them to independently judge whether each of the new critiques was valid (we assume the intended critiques are valid).
We find that:</p>
<ol>
<li>With assistance, labelers find the intended critiques $45 \pm 5 \%$ of the time, whereas without assistance they only find it $27 \pm 5 \%$ of the time.</li>
<li>The fraction of critiques the author considers valid does not appreciably decrease with assistance ( $71 \%$ with assistance versus $73 \%$ without).</li>
<li>However, the number of valid critiques is still much higher, $0.96 \pm 0.09$ vs. $0.62 \pm 0.07$.</li>
<li>With assistance, labelers also find more valid and novel critiques, $0.24 \pm 0.06$ vs. $0.18 \pm 0.05$.</li>
</ol>
<h3>3.4 Dataset release</h3>
<p>We release a comprehensive dataset of results ${ }^{4}$. This includes the assistance provided, critiques used and written, ratings given, and the intended critiques. Random samples from this dataset can be found in Appendix F.2.</p>
<h2>4 Critique quality results</h2>
<p>In this section, we present a number of other results on critique quality. We find that critique quality is enabled by scale:</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Our model gives more helpful critiques than InstructGPT baselines, but still significantly less helpful critiques than humans.</p>
<ol>
<li>Larger models' critiques are rated as more helpful by humans. This holds even if making the answer distribution correspondingly difficult to critique by asking them to self-critique.</li>
<li>Larger models are able to improve outputs using critique-conditional refinements. We verify the critique is helping by comparing to a direct refinement baseline.</li>
</ol>
<h1>4.1 Helpfulness</h1>
<p>The simplest way to measure critique quality is by looking at helpfulness as judged by human labelers. To check that our supervised fine-tuned model is not overly nit-picky, we also asked labelers to mark whether each critique was clearly and unambiguously helpful.
We compare our best critique model to human-written critiques, and to baseline models. For baselines, we use a model trained in the style of InstructGPT [OWJ ${ }^{+} 22$ ] from the same pretrained model. We use this model both using a zero-shot instruction-based context, and with few-shot contexts in the style of $\left[\mathrm{RWC}^{+} 19, \mathrm{BMR}^{+} 20\right]$. For this evaluation, answers were generated randomly from either one of our large fine-tuned models, or an InstructGPT baseline model with zero-shot or few-shot prompting. We then evaluated on answers for which humans found critiques ("critiqueable answers").
Overall we find our model's critiques to be helpful more often than the baselines, but still substantially less helpful than human critiques (Figure 3). We found the InstructGPT models to give surprisingly helpful critiques, considering that they were not trained on our task at all.</p>
<h3>4.2 Self-critiquing helpfulness and scaling</h3>
<p>In Section 3.4, we showed that models are able to help humans find critiques on the distribution of answers coming from the same model.
One natural question to ask is: Should a model be able to reliably find flaws in its own outputs? After all, if it understands these flaws, it could have perhaps avoided them in the first place. However, there is at least one major reason you still might expect a model to identify its own mistakes: Recognizing errors is easier than avoiding them. Equivalently, verifying solutions is easier than finding them (compare to $\mathrm{P} \subseteq \mathrm{NP}$ from computational complexity theory).
It's possible that our model can identify and critique all of its mistakes. This motivates us to look at the percentage of the time poor outputs have helpful critiques. The higher this percentage, the easier it will be to assist humans in evaluation of the base task.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) More capable models have critiqueable outputs around $20 \%$ less often than the smallest models, according to labelers. Less than $15 \%$ of outputs are uncritiqueable for the worst models, and over $30 \%$ for the best models.
(b) Helpfulness of self-critiques, as judged by human labelers, both with and without filtering by when labelers found a critique themselves.</p>
<p>Fraction helpful for critiqueable outputs
<img alt="img-4.jpeg" src="img-4.jpeg" />
(c) Larger models are not only better at critiquing, but harder to critique - even filtering for only cases where labelers found a critique. The diagonal (spanning lower left to upper right) corresponds to the "critiqueable answers" line in 4 b .</p>
<p>Figure 4: More capable models are significantly better at self-critiquing (Figure 4b). Although more capable models get better at generating hard-to-critique answers (Figure 4c), their ability to critique their answers is improving more rapidly with scale. This is true even without adjusting for the fact that humans find fewer critiques of more capable models (Figure 4a). In all figures, we sample at the same random temperature for both the base task and critique task; the effects are equally visible at all temperature ranges (not pictured).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Helpfulness of self-critiques for synthetic tasks, according to a critique validity oracle. Like Figure 4, we show with and without filtering for critiqueable answers (according to a critiqueability oracle).</p>
<p>On topic-based summarization, we find that larger models are better at critiquing themselves (Figure 4b), even without filtering for critiqueable answers. This holds even though answers from larger models are harder to critique (Figure 4a, 4c).
One caveat is that our supervised dataset contains more critiques of outputs from larger models, since we typically use relatively capable answer models. However, we believe this effect to be minimal.
On synthetic tasks, we generally observe similar trends in the critiqueable case (Figure 5), though the story is less clear. Overall, we have no strong reason to believe positive critique scaling to be a fundamental trend. We also do not know, for example, whether the trend would also go away if we use reinforcement learning to train both the answer and critique model. Nevertheless, we believe models have only recently reached a scale where critiquing on realistic tasks is possible.</p>
<h1>4.3 Refinements</h1>
<p>Another check of whether model-generated critiques are useful is to compare critique-conditional refinements to direct refinements. In other words, we compare refinements generated using only an answer to refinements generated using both an answer and a critique of that answer.
In order to improve conditional refinement performance, we can improve the critique. To do that, we do best-of-N [SOW+20] against the helpfulness score; we sample N critiques, choose the best according to the model's helpfulness score, and use that critique for the conditional refinement. For direct refinements, we take best-of-N refinements using our model's critiqueability score.
In our refinement experiments we ask for a refinement regardless of whether the initial answer is critiqueable. If the initial answer were perfect, the model would have no chance at improving it. Thus in order to not "force" the model to refine, we compare the refinement to the original using the model's critiqueability score.
We also include baselines of the original "best-of-1" sample, and a best-of-8 sample (generating new answers from scratch, and ranking them by critiqueability). These experiments use temperature 0.5 to sample, which we believe to be near optimal for best-of-1 on all tasks (answering, critiquing, and refinements).</p>
<h1>4.3.1 Findings</h1>
<p>Our results are depicted in Figures 6 and 7 and samples can be found in Appendix F.3. Despite being somewhat noisy, these results suggest:</p>
<ol>
<li>Good critiques help refinement. Good critiques are useful for refinement. Conditional refinement appear to outperform direct refinements, but only with critiques selected via best-of-N against helpfulness. Larger N helps improve the conditional refinements.</li>
<li>Large model scale enables refinements. Both forms of refinement significantly outperform the original output for larger models, but have little to no effect for smaller models.</li>
<li>Using critiques may not be competitive if controlling for compute. Rejection sampling to select better critiques to use for refinements is competitive with rejection sampling on answers, a roughly compute-equalized baseline. ${ }^{5}$ However, rejection sampling on direct refinements appears to be a stronger baseline.</li>
</ol>
<h2>5 Generator-discriminator-critique (GDC) gaps</h2>
<p>In this section, we present results suggesting that models are not articulating all the problems they "know about." Furthermore, despite the positive results in critique scaling from Section 4.2, we do not see evidence that the gap between our models' discrimination and critique writing abilities is closing with scale.</p>
<h3>5.1 Setup</h3>
<p>In this section we consider the following three tasks:</p>
<ul>
<li>G: answer generation</li>
<li>D: answer discrimination (critiqueability)</li>
<li>C: answer critiquing</li>
</ul>
<p>In our main results from Section 4.2, we compared tasks G and C: To what extent can a model critique its own answers when they are poor? Comparing G and D is also interesting: Can a model tell when its own outputs are good or poor? As with critique scaling, we have two competing trends: The discriminators are getting better in an absolute sense, but the critiqueable answers may also be getting harder or subtler to critique.
Finally, we argue that the gap between D and C is especially interesting: if a model can tell an answer is poor, can it also point out the flaw to a human? If we could train models to always point out when they notice flaws, this could go a long way towards having trustworthy and aligned models. For more discussion, see Appendix C.</p>
<p>This motivates us to measure these quantities in such a way that:</p>
<ul>
<li>The different tasks can be compared on the same axis. For each pair, we will aim to measure a "XY gap" measuring the amount Y performance exceeds X performance</li>
<li>The GC gap corresponds to effectiveness of self-critiquing. A positive gap corresponds to ability to improve or check outputs by showing humans critiques.</li>
<li>The GD gap corresponds to the model's ability to know when answers it produces are poor. A positive gap corresponds to ability to improve outputs using a discriminator.</li>
<li>The CD gap corresponds to the model's ability to give human-understandable critiques on answers it "knows" are flawed (and inability to give convincing critiques on sound answers).</li>
</ul>
<p>Our hope is to ultimately use critiques for better training signal on difficult tasks. In a sense, we would like to take measurements that let us scope out how well this works without actually training our models on this task (see Appendix C.3.3).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) Comparison of critique-conditional refinements to three baselines: the original sample, a direct refinement, and a best-of-8. Small models are poor at refining. For large models, critique-conditional refinements outperform baselines.
(b) Using "forced" refinements, we see that small models are exceptionally bad at conditional refinements. In this setting, the model has no ability to opt out of critiquing or direct-refining.</p>
<p>Figure 6: Critiques help with refining answers. They are also competitive with direct refinements, and a best-of-8 baseline. However, these are only true at scale. Win rate is measured relative to the original (best-of-1) answer from the same model. All critiques and refinements are generated from the same model as the answer, and all generations are at $\mathrm{T}=0.5$.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(a) Win rate of critique-conditional refinement against the original answer. Better critiques (found via best-of-N against the helpfulness model with increasing N ) seem to improve refinements, though results are noisy.
<img alt="img-8.jpeg" src="img-8.jpeg" />
(b) Best-of-8 with direct refinements offers a more competitive baseline that possibly outperforms critique refinements. All 8 refinements are of the same original answer.</p>
<p>Figure 7: Critique refinement and direct refinement scaling with rejection sampling. Figure 7a assesses conditional refinements optimizing the critique against helpfulness score, whereas Figure 7 b assesses direct refinements optimizing the refinement against critiqueability score. Win rate is measured relative to the original (best-of-1) answer from the same model. All critiques and refinements are generated from the same model as the answer, and all generations are at $\mathrm{T}=0.5$.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 8: GDC gaps for topic-based summarization, using humans as ground truth. We measure sample quality using various metrics. "Diff" metrics subtract out the values for the generator. Note that best-of-2 against human win rate against best-of-1 would be exactly $75 \%$ if not for labelers marking ties. Overall, GD and GC gaps may be slightly increasing, but CD gap is positive and shows no trend.</p>
<p>In this section, we present one such way of measuring and our results using it.</p>
<h1>5.2 Measuring gaps</h1>
<p>We propose comparing these tasks to each other using the following methodology:</p>
<ul>
<li>G: What is the average performance of a generator sample?</li>
<li>D: What is the performance of the generator with best-of-N against the discriminator?</li>
<li>C: What is the performance of the generator with best-of-N against the severity of a critique?</li>
</ul>
<p>For measuring C, we essentially use critiques as a discriminator: to judge an answer we generate a critique and consider the answer poor if any critique is valid and severe, according to a human.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 9: GDC gaps for synthetic tasks, using an oracle as ground truth. We also show the oracle best-of-2 discriminator. Note that for binary tasks, win rate is a linear transformation of accuracy gaps. We do not see consistent trends with CD gaps.</p>
<p>Our definition for C is not necessarily canonical, but was chosen to be convenient to measure (see Appendix C.3.2).
With this definition, it is clear that we should generally expect a non-negative GD gap and GC gap. If not, then optimizing against the discriminator or critique training signal makes the generator actively worse. What to expect for the CD gap is less clear. For more discussion, see Appendix C.3.
On a practical level, using best-of-N to measure discrimination ability has many benefits:</p>
<ul>
<li>Unlike accuracy, we don't need to calibrate to the distribution. Our critiqueability models are all trained on the same dataset, but answer models have very different critiqueability (Figure 4a).</li>
<li>We are most interested in the ability to discriminate between answers to the same question, rather than between answers to different questions, which is conflated by the discriminator's ability to tell whether a question is difficult. Though our work uses discriminators, this also means the definitions generalize naturally to using a preference based reward model for D .</li>
</ul>
<p>For our primary results, we use $N=2$. This still leaves us with choices for how to measure performance of a sample, and how to measure critique quality.</p>
<h1>5.2.1 Sample quality</h1>
<p>We explore a number of ways to measure sample quality:</p>
<ul>
<li>Likert: We ask labelers to rate answers qualitatively on a 1-7 Likert scale.</li>
<li>Uncritiqueability (binary label): We ask for the fraction of "perfect" outputs according to humans. For synthetic tasks, we use the oracle, so uncritiqueability is the same as accuracy.</li>
<li>Win rate: We ask labelers for comparisons of the sample in question to a sample from some fixed distribution. In particular, if we use win rate against the generator (we can vary the distribution across model sizes), this makes measurements somewhat natural to compare across model sizes. This makes it so that G is always 0.5 , and D and C are always at most $1-\frac{1}{2 N}$. In the case where the outcome is binary, win rate is simply a linear transformation from accuracy: $\left(1+\mathrm{ACC}-\mathrm{acc}_{G}\right) / 2$.</li>
</ul>
<p>A large caveat is that for topic-based summarization, these methods are all potentially flawed, as suggested by our results in Section 3.4. This is especially problematic if labelers fail to find critiques in systematic ways (rather than, e.g. independently failing to find critiques with some probability).</p>
<h3>5.2.2 Critique severity</h3>
<p>For critique severity measurement, the most straightforward thing is to use human-evaluated severity, with unhelpful critiques being least severe. For topic-based summarization, we again use humans for ground truth, and additionally break ties between helpful critiques using severity.
For synthetic tasks, we again use binary helpfulness ground truth as if it were acting as the human being assisted. This simulates a human judgement in a setting where the humans can easily evaluate critiques, but not the base task answers.
A downside of this approach is that it makes it possible for the model to find flaws by "trying out" critiques it has low confidence in. This means that C effectively gets to leverage a bit of search against ground truth, which D does not get to do. Therefore one could expect a negative CD gap. However this "search effect" should be quite small in the limit of scale with larger models - humans are doing a relatively simple task of critique validation, compared to the difficult task of finding a critique. If the effect exists, it is a genuine advantage of having a human in the loop. For more discussion, see Appendix C.3.1.</p>
<h3>5.3 Results</h3>
<p>We report our main topic-based summarization gap results in Figure 8 and our synthetic task gap results in Figure 9. For topic-based summarization, we use a random non-zero temperature for the</p>
<p>answer, and use the same temperature for critiques. For synthetic tasks, we use temperature 1 for the answer, and temperature 0 for critiques ${ }^{6}$.</p>
<p>We find that:</p>
<ol>
<li>We consistently observe positive GD and GC gaps, as expected, except when models are completely unable to do the task.</li>
<li>We observe a positive CD gap for topic-based summarization and 3-SAT and negative gap for Addition and RACE.</li>
<li>For topic-based summarization, the CD gap is approximately constant across model scale.</li>
<li>For most synthetic tasks, CD gap may be decreasing with model size, but the opposite is true for RACE, where critiquing is close to oracle performance (and is easy relative to knowing when to critique).</li>
</ol>
<p>Overall, this suggests that gaps are task-specific, and it is not apparent whether we can close the CD gap in general.
We believe the CD gap will generally be harder to close for difficult and realistic tasks. For example, on topic-based summarization the discriminator may be able to identify the labeler who gave the answer based on their writing style, and guess that some labelers give more critiqueable answers, without knowing the critiques. This does not happen with synthetic tasks.
We believe much more work on gaps is possible, and present a few more variants and results in Appendix C.3. Overall, we are excited for future study of gaps as a way to measure headroom for pushing critique performance, and as a way to improve methods for scalable oversight.</p>
<h1>6 Related work</h1>
<p>Scalable alignment experiments. [CSA18] implement iterative amplification for algorithmic tasks. [ICA18] introduce debate and implement a toy version with sparse MNIST classification. [SRE ${ }^{+} 20$, BCOI20, $\mathrm{BCS}^{+} 20, \mathrm{PTP}^{+} 22$ ] conduct debate-like experiments on realistic tasks (checking claims about movie reviews, physics problems, and reading comprehension), with humans serving as debaters, generally with mixed results. Conversely, [AZWG21] study variants of debate with learned models serving as judges on toy tasks. [WOZ ${ }^{+} 21$ ] implements a variant of recursive reward modeling $\left[\mathrm{LKE}^{+} 18\right]$ on summarization tasks.</p>
<p>Human assistance with natural language. [LSSC22] use assistance to help humans create demonstrations to create challenging NLI datasets. [ZNC $\left.{ }^{+} 22\right]$ and $\left[\mathrm{PHS}^{+} 22\right]$ use model assistance to find adversarial examples for language model classifications and generations, respectively. [PKF $\left.{ }^{+} 19\right]$ help humans perform passage-based question-answering, without reading much of the passages.
For helping humans with evaluations, [FPP $\left.{ }^{+} 20\right]$ help humans fact-check claims faster and more accurately with natural language briefs. [GSR19] use language models to help humans discriminate whether text was generated by a model.</p>
<p>Critique datasets and models. [TVCM18] introduce a dataset of factual claims, along with supporting and refuting evidence. $\left[\mathrm{KAD}^{+} 18\right]$ introduce a dataset of critical peer reviews. [BCV16] mines disagreements from Twitter, and [ZCP17, PBSM $\left.{ }^{+} 21\right]$ from Reddit. [MST $\left.{ }^{+} 21\right]$ introduce a dataset of story critiques.
For model generated critiques, IBM's Project Debater [SBA $\left.{ }^{+} 21\right]$ trains models to engage in free text debates, including the ability to rebut arguments. Unlike our work, they focus on debating against humans rather than models.</p>
<p>Natural language refinements. Human natural language feedback has been used to improve models in many domains, such as computer vision [RLN $\left.{ }^{+} 18\right]$, program synthesis [EHA20, AON $\left.{ }^{+} 21\right]$, and summarization $\left[\mathrm{SCC}^{+} 22\right]$. $\left[\mathrm{PTA}^{+} 21\right]$ use large language models to fix security vulnerabilities</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>in code. More recently, [WWS ${ }^{+} 22$ b] propose using language models' own outputs to improve their answers on math word problems.</p>
<h1>7 Discussion</h1>
<p>We view our results as a proof of concept for feedback assistance as a solution to the problem of scalable oversight: Even though topic-based summarization isn't actually a hard task for human labelers, in our experiments we still see significant gains from AI assistance in the form of critiques.</p>
<h3>7.1 Implications for alignment research</h3>
<ol>
<li>Large language models are already capable enough to meaningfully assist human evaluation and the scaling trend in Figure 4 suggests that larger models may improve at assisting in evaluating their own outputs. The publicly available InstructGPT models are capable of critiquing well few-shot and even zero-shot (Figure 3). Overall, we believe there is potential to do empirical experiments for scalable oversight with today's models, using schemes similar to reward modeling [LKE $\left.{ }^{+} 18\right]$ or debate [IA19].</li>
<li>Generator-discriminator-critique gaps are promising ways to measure alignment properties of models. Studying gaps give us insight into quality of base task training signal without training those models (see Appendix C.3). Increasing the critique performance relative to generator and discriminator performance is an under-explored research area, where results should directly translate into better-aligned models. Studying gaps can also happen on smaller models in synthetic domains, like those in Table 3.</li>
<li>Learning from natural language feedback is feasible now. Feedback in preference learning $\left[\mathrm{CLB}^{+} 17\right]$ is very information-sparse, and humans typically spend several minutes on a comparison yielding a single bit of information. Ideally, models could use human natural language feedback to improve their own outputs [SCC $\left.{ }^{+} 22\right]$. In Section 4.3, we showed models can now condition on critiques as a form of feedback to improve their own outputs, results corroborated by recent works on "chain of thought" [WWS ${ }^{+} 22$ b]. This suggests teaching models with natural language feedback from humans is a very promising direction.</li>
</ol>
<h3>7.2 Limitations</h3>
<ol>
<li>Lack of ground truth. Our base task of topic-based summarization does not have a robust or objective process for validating the quality of the answers or critiques.
(a) Labelers may be misevaluating answers, by trusting the model summaries too much or by simply making mistakes.
(b) Some critiques found by the labelers using assistance were fairly unimportant or nitpicky. Agreement rate on comparisons of critiques (i.e. helpfulness rankings) were no higher than answer comparisons; both were around $75 \%$.
(c) Misleading critiques of good outputs may be indistinguishable from good critiques of poor outputs.
(d) More broadly, we do not address how to measure ground truth, which makes this research difficult. Our work relies on labelers, who already make mistakes and will be increasingly unreliable for harder tasks.</li>
<li>Assuming articulable reasoning. Our overall research direction does not address how to surface problematic outputs where a model cannot put into words what the problem is, which may be a core difficulty of the alignment problem [CCX21]. The CD gap could remain large after much effort using assistance.</li>
<li>
<p>Assuming reconcilable preferences. Critiques as a training signal may not make sense for more inherently subjective tasks, where labelers have differing preferences. It may be impossible to have uncritiqueable outputs (at least without specifying how to resolve disagreements). On the other hand, for subjective tasks having a strong critique model can make it easier to adapt a model to each labeler's individual preferences because it lets them rank the critiques they care about without having to find all of them.</p>
</li>
<li>
<p>Evaluation is not always easier than generation. For some tasks it will not be possible to find assistance tasks that are simpler to evaluate than the base task. For example, asking about how to solve climate change may result in complex economic questions. And asking complex economic questions may in turn ask for predictions about the effects of climate change.</p>
</li>
<li>Lack of difficulty. Our base task is not actually very hard for humans to evaluate, resulting in little headroom for assistance to help. Humans take up to around ten minutes to do the task, so we do not observe much speed-up from assistance. In general, model-assisted evaluation is most valuable on tasks that are actually difficult for humans to evaluate, and so positive results on an easier task might not be reproducible on harder tasks.</li>
<li>Under-optimized models. We only use supervised fine-tuning while models like InstructGPT [OWJ ${ }^{+}$22] trained on similar tasks benefit significantly from reinforcement learning as an additional step. This also means that our model is unlikely to output critiques that no human labeler would have written themselves.</li>
<li>Difficulty of setup. Our setup may be difficult to replicate. It requires large models, a lot of human data, and multiple rounds of training.</li>
</ol>
<h1>7.3 Future directions</h1>
<p>We believe our dataset and methods open up many interesting research avenues, which we are excited for researchers to explore. For example:</p>
<ul>
<li>Study human cognitive errors and misleading models: Future concerns about misalignment are currently very abstract. It would be useful to produce concrete examples of human supervision being systematically biased and leading ML training to produce systems that mislead their supervisors.</li>
<li>Reduce the discriminator-critique gap: We showed that models can learn to generate helpful critiques. But it would be useful to systematically study how far we can push critique training relative to discriminator performance and to understand the obstacles to having models explicate their knowledge.</li>
<li>Recursive reward modeling: We showed that critiques help human evaluations. A next step could be to improve model performance on the base task by training on assisted evaluations. Then, if we take assistance itself as a base task, we can then train assistants that help train assistants (e.g. critiquers of critiquers).</li>
<li>Study assistance methods: We experimented with critiques as one form of assistance, but did not compare it to any other forms of assistance. For example, explanations may be more natural for many tasks. More open-ended settings like question-answering or dialogue $\left[\mathrm{BJN}^{+}\right.$22] could potentially be better interfaces for assistance.</li>
<li>Iterative refinements: We collected a large dataset of refinements, but did not explore in depth how to best use these to improve model outputs. For example, one could do multiple refinement iterations, and combine that with best-of-N.</li>
<li>Disagreeing labelers: Critiques are potentially a natural way to reconcile raters' disagreements. For real-world tasks, such as summarizing current events, humans may have differing opinions on appropriate contextualization. Some humans may also be unaware of certain problems in outputs (e.g. unrecognized slurs, subtle implications), and model critiques are a possible way to surface them and increase agreement rates.</li>
<li>Using natural language to train models: discussed above in Section 7.1.</li>
</ul>
<p>For many of the above directions, we would also like to move to more difficult tasks, but which still have (more objective) ground truth. Some possibilities include coding-related tasks, mathematics, riddles (such as cryptic crosswords), and book-length question-answering.</p>
<h2>8 Acknowledgements</h2>
<p>We thank Rai Pokorný, John Schulman, Rachel Freedman, Jacob Hilton, Harri Edwards, Karl Cobbe, Pranav Shyam, and Owain Evans for providing feedback on the paper.</p>
<p>We'd like to thank Paul Christiano, Ethan Perez, Jérémy Scheurer, Angelica Chen, Jon Ander Campos for discussions about our project and Alex Gray for coining the name "generator-discriminator gap."</p>
<p>Finally, we'd like to thank all of our labelers for providing the data that was essential for training the models in this paper, including: Gabriel Paolo Ricafrente, Jack Kausch, Erol Can Akbaba, Maria Orzek, Stephen Ogunniyi, Jenny Fletcher, Tasmai Dave, Jesse Zhou, Gabriel Perez, Jelena Ostojic, Ife Riamah, Atresha Singh, Celina Georgette Paglinawan, Alfred Johann Lee, Sebastian Gonzalez, Oliver Horsfall, Bekah Guess, Medeea Bunea, and Cyra Mayell D. Emnace.</p>
<h1>References</h1>
<p>$\left[\mathrm{AON}^{+}\right.$21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.
$\left[\mathrm{AOS}^{+}\right.$16] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.
[AZWG21] Cem Anil, Guodong Zhang, Yuhuai Wu, and Roger Grosse. Learning to give checkable answers with prover-verifier games. arXiv preprint arXiv:2108.12099, 2021.
[BCOI20] Elizabeth Barnes, Paul Christiano, Long Ouyang, and Geoffrey Irving. Progress on AI safety via debate. URL https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1, 2020.
[BCS ${ }^{+}$20] Elizabeth Barnes, Paul Christiano, William Saunders, Joe Collman, Mark Xu, Chris Painter, Mihnea Maftei, and Ronny Fernandez. Debate update: Obfuscated arguments problem. URL https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem, 2020.
[BCV16] Tom Bosc, Elena Cabrio, and Serena Villata. DART: A dataset of arguments and their relations on Twitter. In Proceedings of the 10th edition of the Language Resources and Evaluation Conference, pages 1258-1263, 2016.
[BFL91] László Babai, Lance Fortnow, and Carsten Lund. Non-deterministic exponential time has two-prover interactive protocols. Computational complexity, 1(1):3-40, 1991.
$\left[\mathrm{BHA}^{+}\right.$21] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
[BJN ${ }^{+}$22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.
[BMR ${ }^{+}$20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
$\left[\mathrm{CCG}^{+}\right.$22] Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. arXiv preprint arXiv:2202.01169, 2022.
[CCX21] Paul Christiano, Ajeya Cotra, and Mark Xu. Eliciting latent knowledge: How to tell if your eyes deceive you. https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge, 2021.</p>
<p>[CLB ${ }^{+}$17] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, pages 4299-4307, 2017.
[CND ${ }^{+}$22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022.
[Cot21] Ajeya Cotra. The case for aligning narrowly superhuman models. https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models, 2021.
[CSA18] Paul Christiano, Buck Shlegeris, and Dario Amodei. Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575, 2018.
[CTJ ${ }^{+}$21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[Dan05] Hoa Trang Dang. Overview of DUC 2005. In Proceedings of the document understanding conference, volume 2005, pages 1-12, 2005.
[DL15] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079-3087, 2015.
[EHA20] Ahmed Elgohary, Saghar Hosseini, and Ahmed Hassan Awadallah. Speak to your parser: Interactive text-to-SQL with natural language feedback. arXiv preprint arXiv:2005.02539, 2020.
[FPP ${ }^{+}$20] Angela Fan, Aleksandra Piktus, Fabio Petroni, Guillaume Wenzek, Marzieh Saeidi, Andreas Vlachos, Antoine Bordes, and Sebastian Riedel. Generating fact checking briefs. arXiv preprint arXiv:2011.05448, 2020.
[GMR89] Shafi Goldwasser, Silvio Micali, and Charles Rackoff. The knowledge complexity of interactive proof systems. SIAM Journal on computing, 18(1):186-208, 1989.
[GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. Gltr: Statistical detection and visualization of generated text. arXiv preprint arXiv:1906.04043, 2019.
[HBM ${ }^{+}$22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
[IA19] Geoffrey Irving and Amanda Askell. Ai safety needs social scientists. Distill, 4(2):e14, 2019.
[ICA18] Geoffrey Irving, Paul Christiano, and Dario Amodei. AI safety via debate. arXiv preprint arXiv:1805.00899, 2018.
[JMD20] Hong Jun Jeon, Smitha Milli, and Anca D Dragan. Reward-rational (implicit) choice: A unifying formalism for reward learning. arXiv preprint arXiv:2002.04833, 2020.
$\left[\mathrm{KAD}^{+}\right.$18] Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine Van Zuylen, Sebastian Kohlmeier, Eduard Hovy, and Roy Schwartz. A dataset of peer reviews (peerread): Collection, insights and nlp applications. arXiv preprint arXiv:1804.09635, 2018.
$\left[\mathrm{KMH}^{+}\right.$20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
[LCC ${ }^{+}$22] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with AlphaCode. arXiv preprint arXiv:2203.07814, 2022.
[LKE ${ }^{+}$18] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We initially tried other settings which did not qualitatively change results but made win rates closer to $50 \%$ and error bars larger.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
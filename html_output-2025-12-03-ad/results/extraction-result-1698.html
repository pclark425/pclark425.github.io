<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1698 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1698</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1698</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-249431373</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2206.03139v1.pdf" target="_blank">Intra-agent speech permits zero-shot task acquisition</a></p>
                <p><strong>Paper Abstract:</strong> Human language learners are exposed to a trickle of informative, context-sensitive language, but a flood of raw sensory data. Through both social language use and internal processes of rehearsal and practice, language learners are able to build high-level, semantic representations that explain their perceptions. Here, we take inspiration from such processes of"inner speech"in humans (Vygotsky, 1934) to better understand the role of intra-agent speech in embodied behavior. First, we formally pose intra-agent speech as a semi-supervised problem and develop two algorithms that enable visually grounded captioning with little labeled language data. We then experimentally compute scaling curves over different amounts of labeled data and compare the data efficiency against a supervised learning baseline. Finally, we incorporate intra-agent speech into an embodied, mobile manipulator agent operating in a 3D virtual world, and show that with as few as 150 additional image captions, intra-agent speech endows the agent with the ability to manipulate and answer questions about a new object without any related task-directed experience (zero-shot). Taken together, our experiments suggest that modelling intra-agent speech is effective in enabling embodied agents to learn new tasks efficiently and without direct interaction experience.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1698.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1698.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gen-SS Captioner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Semi-Supervised Intra-Agent Speech Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative semi-supervised model that treats captions (language) as discrete latent variables and trains an image-conditional language encoder q_ω(y|x) together with a pretrained language prior and an image decoder (VQ-VAE + autoregressive transformer) to produce captions from images using large unlabeled image corpora plus sparse labeled captions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Generative semi-supervised intra-agent speech model</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Image-conditional language encoder: ResNet (20-layer variant) encoding 96×72 RGB frames to 432 hyper-pixels attended by a 4-layer causal transformer (256 embedding, 4 heads) producing logits over a 4,000 token SentencePiece vocab; a separate language prior transformer is pretrained over caption labels; image decoder: VQ-VAE (codebook size 512, compress to 432 tokens) plus an 8-layer autoregressive transformer (512 embedding) conditioned on caption transformer outputs via cross-attention. Trained with variational lower bound objective combining supervised paired loss J_p and unsupervised reconstruction objective J_u; optimization via V-MPO and Adam.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>paired captions + large unlabeled image corpus (Playhouse frames) with language captions as latent variables</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Unpaired dataset: ~2 billion Playhouse image frames (treated as unlabeled images). Paired dataset: 78,000 human-collected image captions (crowd workers, one highlighted object per image). For the 'drum' experiments, drum captions were held out and then reintroduced at titered sizes (examples: 585 labeled drum captions, 150 labeled drum captions). Language prior trained on all caption labels. Image VQ-VAE trained on all unlabeled images to form codebook (vocab 512, 432 tokens per image).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Playhouse embodied interaction tasks (lift drum; ask color of drum)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>3D multi-room Playhouse environment with two embodied avatars performing cooperative interactive tasks (object manipulation, navigation, question answering). Evaluation tasks (zero-shot): 'Lift the drum' — agent must find and lift a drum object spawned in the environment; 'What is the color of the drum?' — agent must answer the drum's color in language. Drum color sampled from 10 colors; episodes up to 2 minutes; rewards binary (1 for success).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Language outputs (captions) during pretraining; the model's action-equivalent during caption training is discrete token emission from a 4,000-token vocabulary (causal transformer output).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Agent produces language actions (π_l) and movement / manipulation actions (π_m) used for behavioral cloning; movement/motor actions correspond to Playhouse action primitives (discrete movement/manipulation commands as in Interactive Agents Team architecture) while language actions are discrete token emissions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>No hand-crafted mapping — transfer achieved via auxiliary supervision: the pretrained generative captioner is frozen and sampled captions are supplied frequently during agent behavioral cloning as (a) targets for the agent's language output policy (caption loss L_C) and (b) inputs/positives for a caption-matching auxiliary classifier (L_CM). Gradients from these auxiliary losses affect shared components responsible for action selection, enabling the agent to learn correlations between visual observations, language outputs, and movement policies.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB vision (96×72 input resolution) encoded by ResNet to produce hyper-pixels; access to object identities and colors for evaluation (Playhouse provides ground-truth object metadata). No explicit depth or proprioception mentioned beyond standard Playhouse observations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Agents trained with the generative semi-supervised captioner providing both caption loss and caption-matching loss (+C +CM) but with no drum task interaction data achieved ~>0.5 human-normalized reward (reported as 'more than 0.5') on drum tasks, corresponding to ≈70% of the performance of agents trained with direct drum interaction data (+D). For reference, +D agents achieved ~75% human-normalized reward on 'lift drum' and ~80% on 'ask color drum'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baseline agent trained without drum interaction data and without intra-agent speech auxiliary losses performed poorly on drum evaluation tasks (near 0 relative to human-normalized reward); supervised caption-only models (S585) with 585 labeled drum captions performed substantially worse than the semi-supervised approach (e.g., far lower true-positive drum mention and lower downstream task performance).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Zero task-directed interaction episodes (i.e., 0 additional drum interaction episodes) plus as few as 150 labeled drum captions (G150) yielded near-equivalent performance on the 'ask color' task and only a slight decrease on 'lift drum'; 585 labeled drum captions (G585) produced high caption recognition (≈70% drum true-positive) and strong downstream transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>To reach the upper-bound embodied performance shown in the paper, agents trained with explicit drum interaction used +D which comprised 9,344 drum-containing interaction episodes; supervised caption learning would need far more labeled captions (paper estimates supervised would need >300× more labeled drum captions to match semi-supervised drum true-positive rates).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Semi-supervised pretraining enabled ~70% of full +D performance with 0 task-directed drum episodes, saving ~9,344 interaction episodes; for caption learning, semi-supervised methods matched supervised models trained on ~3× more labeled data and achieved drum recognition with 585 labeled examples whereas supervised required >>300× more labeled examples to match drum true-positive (large sample-efficiency gains).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large unlabeled image corpus (2B frames) enabling image-conditioned language learning; language-conditioned image reconstruction objective (generative VQ-VAE + language prior) that binds visual patterns to language tokens; dense sampled captions during agent training providing frequent language supervision; object-centric captioning (crowd-captioned highlighted object) aligning language to object semantics; freezing the pretrained module and using it as a stable language teacher.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>No explicit symbolic mapping from language tokens to low-level motor primitives — transfer relies on gradient-based auxiliary supervision which can be less direct for some motor skills (e.g., precise manipulation). Residual gap in lift performance when caption data is very scarce (small drop when using only 150 captions for lift). Possible perception-action mismatch between captioner training (image reconstruction-conditioned) and embodied agent motor demands.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A generative semi-supervised intra-agent speech model pretrained on large unlabeled image corpora plus a small number of labeled captions yields highly sample-efficient acquisition of language about novel objects and confers substantial zero-shot transfer to 3D embodied tasks: with as few as 150–585 labeled captions (and no task-directed interaction episodes) agents obtained ≈70% of the performance of agents trained with ~9.3k object-specific interaction episodes; semi-supervised captioning matched supervised models trained on ~3× more labeled data and outperformed supervised learning by large margins for novel-object recognition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intra-agent speech permits zero-shot task acquisition', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1698.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1698.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contrast-SS Captioner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Semi-Supervised Intra-Agent Speech Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive/energy-based variant that replaces the generative image-decoder term with a large-batch multi-class contrastive classifier (image-caption matching), training an image-conditional language encoder q_ω(y|x) by treating sampled captions as positives in a softmax classifier over batch elements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Contrastive semi-supervised intra-agent speech model</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Images encoded with the same ResNet (as the generative variant) and mean-pooled to 1024-d vectors; captions encoded with a transformer; an image-caption matching classifier implemented via softmax of dot-products e^{f(x)·g(y)} over large batches is trained (two-way loss matching image→caption and caption→image). The contrastive loss substitutes for log p_θ(x|y) in the variational gradient, enabling q_ω updates without an explicit image decoder. Trained with very large batch sizes (unlabeled images up to 2,048 per batch plus 128 paired).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>paired captions + large unlabeled image corpus (contrastive image-caption matching)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Same corpora as the generative variant: ~2 billion Playhouse frames as unlabeled images and 78,000 human-collected paired captions. Large-batch contrastive training included 2,048 unlabeled images per batch (total batch 2,176 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Playhouse embodied interaction tasks (lift drum; ask color of drum)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same Playhouse evaluation tasks used for generative variant: randomized multi-room 3D Playhouse environments where the agent must locate and lift a drum or answer the drum's color on demand. Evaluation averaged over 1,000 episodes per task and normalized to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Discrete token emission for captions (SentencePiece 4,000 token vocabulary) used as the contrastive model's caption representations; contrastive training itself uses caption embeddings rather than action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Agent movement/manipulation actions (discrete Playhouse primitives) and language actions (discrete token emissions from language policy π_l).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Same mechanism as for the generative variant — pretrained contrastive encoder is sampled to produce captions which are used as targets (L_C) and as inputs/positives for caption-matching (L_CM) during agent behavioral cloning; transfer is via auxiliary supervision rather than an explicit symbolic mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB images (96×72) processed by ResNet encoder; caption matching requires representation alignment between visual encoder and caption encoder embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Contrastive semi-supervised models outperformed purely supervised caption baselines on caption metrics and enabled downstream transfer to embodied drum tasks when used to provide intra-agent speech supervision; specific embodied-task numeric performance reported in aggregate with generative model results (agents receiving caption losses from semi-supervised models reached ≈70% of +D performance, >0.5 human-normalized reward).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baseline agents without semi-supervised intra-agent speech performed poorly on drum tasks; supervised caption models trained only on limited labeled captions performed worse than semi-supervised contrastive models on both caption recognition and downstream transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Contrastive semi-supervised model used the same small labeled-caption regimes (examples highlighted: 150 and 585 labeled drum captions) to enable high downstream performance; contrastive training used very large unlabeled batches (2,048 unlabeled images per batch) to form the matching loss.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Without semi-supervised pretraining, achieving comparable caption recognition or downstream transfer would have required substantially more labeled captions (paper reports semi-supervised equals supervised with ≈3× more labeled data for general caption metrics and indicates >300× more labeled drum captions would be needed to match drum true-positive rates).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Large batch contrastive pretraining improved label efficiency: semi-supervised contrastive approach equaled supervised models trained on ~3× more labeled data and enabled downstream zero-shot behavior with only 150–585 labeled captions versus needing many thousands of interaction episodes for comparable task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-batch contrastive matching aligns image and language embedding spaces; inclusion of sampled captions as positives during agent training provides dense language supervision; the contrastive objective avoids modeling a high-dimensional image decoder while still teaching semantically meaningful captions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Contrastive method depends on very large negative-sample batches (practical compute/ memory cost) and may be sensitive to batch composition; like the generative model, transfer is indirect (auxiliary losses) so fine motor control tasks not tightly coupled to linguistic descriptions may see less benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A contrastive semi-supervised intra-agent speech model, trained on massive unlabeled images plus sparse captions and large-batch image-caption matching, yields data-efficient language learning and supports zero-shot transfer to embodied object-manipulation and QA tasks when its sampled captions are used as auxiliary supervision during agent behavioral cloning; performance and sample-efficiency gains are comparable to the generative variant and both outperform supervised baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intra-agent speech permits zero-shot task acquisition', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1698.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1698.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Playhouse Agent (+Intra-Speech)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interactive Playhouse Embodied Agent augmented with Pretrained Intra-Agent Speech Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied agent (based on the DeepMind Interactive Agents architecture) trained by behavioral cloning on human Playhouse interactions, augmented with a frozen pretrained intra-agent speech module whose sampled captions are used as auxiliary targets and inputs (caption loss and caption-matching loss) to shape language-conditioned action policies and improve zero-shot behavior about novel objects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Playhouse interactive agent with pretrained intra-agent speech module</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Agent architecture follows Interactive Agents Team: encodes visual and textual observations, has separate language (π_l) and movement (π_m) policies, trained with behavioral cloning loss L_BC. Augmented by two auxiliary losses during BC training: caption loss L_C (agent's language output trained to match sampled captions from pretrained module) and caption-matching loss L_CM (auxiliary classifier D predicts whether an input caption matches the current visual observation). The pretrained intra-agent speech module (generative or contrastive variant) is frozen while used to sample captions at high frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>language-caption pretraining (paired captions) and large unlabeled image corpus used to learn intra-agent speech; behavioral policy training used Playhouse human interaction data (imitation learning).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretraining: 78K human-collected paired captions, ~2B unlabeled Playhouse frames; for some experiments drum captions were withheld then reintroduced in sizes 150 and 585. Behavioral cloning training data: Playhouse human interaction corpus (~3 years of interactions, ~2B frames total) with 9,344 drum-containing interactions removed in 'zero-shot' evaluations. Agent behavioral cloning optimization used same hyperparameters as Interactive Agents Team [22].</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Lift drum; Ask color of drum (Playhouse zero-shot evaluation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Evaluation tasks instantiate a drum object in randomized Playhouse environments: 'Lift the drum' requires locating and lifting any drum (episode terminates with reward 1 upon lifting drum); 'What is the color of the drum?' requires producing the drum color as language output. Each agent evaluated over 1,000 episodes per task; rewards normalized by human average to [0,1].</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level natural language outputs from the agent's language policy (token-level discrete emissions from a 4k SentencePiece vocabulary) used both for dialogue and to answer evaluation questions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete movement/manipulation action primitives (Playhouse movement policy π_m) combined with language emission actions (π_l). The paper does not enumerate exact low-level motor primitives but follows the Interactive Agents Team action space (discrete navigation/manipulation commands).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Mapping is learned implicitly: the frozen pretrained intra-agent speech module generates captions that serve as supervisory signals (targets) for the agent's language outputs and as positives for caption-matching detection; gradients from these auxiliary losses flow into shared perception and action-selection components, thereby associating visual observations and language with appropriate movement/manipulation actions through the behavioral cloning objective. No explicit symbolic mapping layer is used.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB visual observations (96×72 input size), object identity and color metadata available for evaluation; agent encoders process visual + text observations consistent with Playhouse architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Agent trained without any drum interaction episodes but with caption and caption-matching auxiliary losses from pretrained intra-agent speech reached more than 0.5 human-normalized reward (≈70% of the +D agent's performance) on the drum tasks; +C+CM achieved approximately 70% of the performance of +D (which reached ≈0.75 for lift and ≈0.8 for color). Using only 150 labeled drum captions (G150) produced near-equal performance on the color QA task and only a slight reduction on lift.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baseline agent without intra-agent speech auxiliary supervision and without drum interactions performed poorly on drum tasks (substantially below 0.5 human-normalized reward); supervised caption models trained on the same small caption sets (e.g., S585) performed worse both at caption recognition and downstream embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Zero drum interaction episodes required; as few as 150 labeled drum captions (plus the large unlabeled image corpus used in semi-supervised pretraining) produced strong zero-shot task performance (particularly for color QA); 585 labeled drum captions achieved robust caption recognition (≈70% drum true-positive) and downstream transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Agents that achieved the highest embodied performance trained with explicit drum interactions used +D = 9,344 drum-containing interaction episodes; supervised captioning baselines would need many more labeled captions (paper reports >300× more labeled drum captions to match semi-supervised detection).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>The pretrained intra-agent speech module enabled agents to obtain ≈70% of the +D performance with 0 task-specific interaction episodes, effectively replacing ~9.3k interaction episodes with 150–585 labeled captions plus large-scale unlabeled image data; semi-supervised captioning provided ≈3× label-efficiency for general caption metrics and orders-of-magnitude label-efficiency for novel-object recognition versus supervised baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Dense sampled captions from the pretrained module during agent training provided frequent language supervision; the captioning models produced object-centric linguistic labels that aligned with manipulation tasks; the large unlabeled image corpus allowed the captioner to learn robust visual-linguistic associations that generalized to objects never seen in task-directed interactions; freezing the captioner provided stable targets.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Transfer is indirect (auxiliary supervision) rather than via explicit symbolic action mapping; some motor skills (precise lifting) still showed slight degradation when caption data was extremely scarce (150 captions) indicating limits to language-only supervision for fine manipulation; potential brittleness to differences between captioning training distribution and embodied task visual contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Augmenting a behavioral-cloning Playhouse agent with a frozen, pretrained intra-agent speech module (generative or contrastive semi-supervised captioner) and using sampled captions as auxiliary supervision enables substantial zero-shot acquisition of object-directed behaviors: agents with no task-specific interaction episodes but with 150–585 labeled captions plus semi-supervised pretraining achieved ≈70% of the performance of agents trained on ~9.3k object interaction episodes, demonstrating that language pretraining on image-caption data can meaningfully transfer to 3D embodied motor and question-answering tasks via auxiliary losses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Intra-agent speech permits zero-shot task acquisition', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Creating multimodal interactive agents with imitation and self-supervised learning <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Learning language-conditioned robot behavior from offline data and crowd-sourced annotation <em>(Rating: 2)</em></li>
                <li>Human instruction-following with deep reinforcement learning via transfer-learning from text <em>(Rating: 1)</em></li>
                <li>Zero experience required: Plug & play modular transfer learning for semantic visual navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1698",
    "paper_id": "paper-249431373",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "Gen-SS Captioner",
            "name_full": "Generative Semi-Supervised Intra-Agent Speech Model",
            "brief_description": "A generative semi-supervised model that treats captions (language) as discrete latent variables and trains an image-conditional language encoder q_ω(y|x) together with a pretrained language prior and an image decoder (VQ-VAE + autoregressive transformer) to produce captions from images using large unlabeled image corpora plus sparse labeled captions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "Generative semi-supervised intra-agent speech model",
            "model_agent_description": "Image-conditional language encoder: ResNet (20-layer variant) encoding 96×72 RGB frames to 432 hyper-pixels attended by a 4-layer causal transformer (256 embedding, 4 heads) producing logits over a 4,000 token SentencePiece vocab; a separate language prior transformer is pretrained over caption labels; image decoder: VQ-VAE (codebook size 512, compress to 432 tokens) plus an 8-layer autoregressive transformer (512 embedding) conditioned on caption transformer outputs via cross-attention. Trained with variational lower bound objective combining supervised paired loss J_p and unsupervised reconstruction objective J_u; optimization via V-MPO and Adam.",
            "pretraining_data_type": "paired captions + large unlabeled image corpus (Playhouse frames) with language captions as latent variables",
            "pretraining_data_details": "Unpaired dataset: ~2 billion Playhouse image frames (treated as unlabeled images). Paired dataset: 78,000 human-collected image captions (crowd workers, one highlighted object per image). For the 'drum' experiments, drum captions were held out and then reintroduced at titered sizes (examples: 585 labeled drum captions, 150 labeled drum captions). Language prior trained on all caption labels. Image VQ-VAE trained on all unlabeled images to form codebook (vocab 512, 432 tokens per image).",
            "embodied_task_name": "Playhouse embodied interaction tasks (lift drum; ask color of drum)",
            "embodied_task_description": "3D multi-room Playhouse environment with two embodied avatars performing cooperative interactive tasks (object manipulation, navigation, question answering). Evaluation tasks (zero-shot): 'Lift the drum' — agent must find and lift a drum object spawned in the environment; 'What is the color of the drum?' — agent must answer the drum's color in language. Drum color sampled from 10 colors; episodes up to 2 minutes; rewards binary (1 for success).",
            "action_space_text": "Language outputs (captions) during pretraining; the model's action-equivalent during caption training is discrete token emission from a 4,000-token vocabulary (causal transformer output).",
            "action_space_embodied": "Agent produces language actions (π_l) and movement / manipulation actions (π_m) used for behavioral cloning; movement/motor actions correspond to Playhouse action primitives (discrete movement/manipulation commands as in Interactive Agents Team architecture) while language actions are discrete token emissions.",
            "action_mapping_method": "No hand-crafted mapping — transfer achieved via auxiliary supervision: the pretrained generative captioner is frozen and sampled captions are supplied frequently during agent behavioral cloning as (a) targets for the agent's language output policy (caption loss L_C) and (b) inputs/positives for a caption-matching auxiliary classifier (L_CM). Gradients from these auxiliary losses affect shared components responsible for action selection, enabling the agent to learn correlations between visual observations, language outputs, and movement policies.",
            "perception_requirements": "RGB vision (96×72 input resolution) encoded by ResNet to produce hyper-pixels; access to object identities and colors for evaluation (Playhouse provides ground-truth object metadata). No explicit depth or proprioception mentioned beyond standard Playhouse observations.",
            "transfer_successful": true,
            "performance_with_pretraining": "Agents trained with the generative semi-supervised captioner providing both caption loss and caption-matching loss (+C +CM) but with no drum task interaction data achieved ~&gt;0.5 human-normalized reward (reported as 'more than 0.5') on drum tasks, corresponding to ≈70% of the performance of agents trained with direct drum interaction data (+D). For reference, +D agents achieved ~75% human-normalized reward on 'lift drum' and ~80% on 'ask color drum'.",
            "performance_without_pretraining": "Baseline agent trained without drum interaction data and without intra-agent speech auxiliary losses performed poorly on drum evaluation tasks (near 0 relative to human-normalized reward); supervised caption-only models (S585) with 585 labeled drum captions performed substantially worse than the semi-supervised approach (e.g., far lower true-positive drum mention and lower downstream task performance).",
            "sample_complexity_with_pretraining": "Zero task-directed interaction episodes (i.e., 0 additional drum interaction episodes) plus as few as 150 labeled drum captions (G150) yielded near-equivalent performance on the 'ask color' task and only a slight decrease on 'lift drum'; 585 labeled drum captions (G585) produced high caption recognition (≈70% drum true-positive) and strong downstream transfer.",
            "sample_complexity_without_pretraining": "To reach the upper-bound embodied performance shown in the paper, agents trained with explicit drum interaction used +D which comprised 9,344 drum-containing interaction episodes; supervised caption learning would need far more labeled captions (paper estimates supervised would need &gt;300× more labeled drum captions to match semi-supervised drum true-positive rates).",
            "sample_complexity_gain": "Semi-supervised pretraining enabled ~70% of full +D performance with 0 task-directed drum episodes, saving ~9,344 interaction episodes; for caption learning, semi-supervised methods matched supervised models trained on ~3× more labeled data and achieved drum recognition with 585 labeled examples whereas supervised required &gt;&gt;300× more labeled examples to match drum true-positive (large sample-efficiency gains).",
            "transfer_success_factors": "Large unlabeled image corpus (2B frames) enabling image-conditioned language learning; language-conditioned image reconstruction objective (generative VQ-VAE + language prior) that binds visual patterns to language tokens; dense sampled captions during agent training providing frequent language supervision; object-centric captioning (crowd-captioned highlighted object) aligning language to object semantics; freezing the pretrained module and using it as a stable language teacher.",
            "transfer_failure_factors": "No explicit symbolic mapping from language tokens to low-level motor primitives — transfer relies on gradient-based auxiliary supervision which can be less direct for some motor skills (e.g., precise manipulation). Residual gap in lift performance when caption data is very scarce (small drop when using only 150 captions for lift). Possible perception-action mismatch between captioner training (image reconstruction-conditioned) and embodied agent motor demands.",
            "key_findings": "A generative semi-supervised intra-agent speech model pretrained on large unlabeled image corpora plus a small number of labeled captions yields highly sample-efficient acquisition of language about novel objects and confers substantial zero-shot transfer to 3D embodied tasks: with as few as 150–585 labeled captions (and no task-directed interaction episodes) agents obtained ≈70% of the performance of agents trained with ~9.3k object-specific interaction episodes; semi-supervised captioning matched supervised models trained on ~3× more labeled data and outperformed supervised learning by large margins for novel-object recognition.",
            "uuid": "e1698.0",
            "source_info": {
                "paper_title": "Intra-agent speech permits zero-shot task acquisition",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Contrast-SS Captioner",
            "name_full": "Contrastive Semi-Supervised Intra-Agent Speech Model",
            "brief_description": "A contrastive/energy-based variant that replaces the generative image-decoder term with a large-batch multi-class contrastive classifier (image-caption matching), training an image-conditional language encoder q_ω(y|x) by treating sampled captions as positives in a softmax classifier over batch elements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "Contrastive semi-supervised intra-agent speech model",
            "model_agent_description": "Images encoded with the same ResNet (as the generative variant) and mean-pooled to 1024-d vectors; captions encoded with a transformer; an image-caption matching classifier implemented via softmax of dot-products e^{f(x)·g(y)} over large batches is trained (two-way loss matching image→caption and caption→image). The contrastive loss substitutes for log p_θ(x|y) in the variational gradient, enabling q_ω updates without an explicit image decoder. Trained with very large batch sizes (unlabeled images up to 2,048 per batch plus 128 paired).",
            "pretraining_data_type": "paired captions + large unlabeled image corpus (contrastive image-caption matching)",
            "pretraining_data_details": "Same corpora as the generative variant: ~2 billion Playhouse frames as unlabeled images and 78,000 human-collected paired captions. Large-batch contrastive training included 2,048 unlabeled images per batch (total batch 2,176 in experiments).",
            "embodied_task_name": "Playhouse embodied interaction tasks (lift drum; ask color of drum)",
            "embodied_task_description": "Same Playhouse evaluation tasks used for generative variant: randomized multi-room 3D Playhouse environments where the agent must locate and lift a drum or answer the drum's color on demand. Evaluation averaged over 1,000 episodes per task and normalized to human performance.",
            "action_space_text": "Discrete token emission for captions (SentencePiece 4,000 token vocabulary) used as the contrastive model's caption representations; contrastive training itself uses caption embeddings rather than action sequences.",
            "action_space_embodied": "Agent movement/manipulation actions (discrete Playhouse primitives) and language actions (discrete token emissions from language policy π_l).",
            "action_mapping_method": "Same mechanism as for the generative variant — pretrained contrastive encoder is sampled to produce captions which are used as targets (L_C) and as inputs/positives for caption-matching (L_CM) during agent behavioral cloning; transfer is via auxiliary supervision rather than an explicit symbolic mapping.",
            "perception_requirements": "RGB images (96×72) processed by ResNet encoder; caption matching requires representation alignment between visual encoder and caption encoder embeddings.",
            "transfer_successful": true,
            "performance_with_pretraining": "Contrastive semi-supervised models outperformed purely supervised caption baselines on caption metrics and enabled downstream transfer to embodied drum tasks when used to provide intra-agent speech supervision; specific embodied-task numeric performance reported in aggregate with generative model results (agents receiving caption losses from semi-supervised models reached ≈70% of +D performance, &gt;0.5 human-normalized reward).",
            "performance_without_pretraining": "Baseline agents without semi-supervised intra-agent speech performed poorly on drum tasks; supervised caption models trained only on limited labeled captions performed worse than semi-supervised contrastive models on both caption recognition and downstream transfer.",
            "sample_complexity_with_pretraining": "Contrastive semi-supervised model used the same small labeled-caption regimes (examples highlighted: 150 and 585 labeled drum captions) to enable high downstream performance; contrastive training used very large unlabeled batches (2,048 unlabeled images per batch) to form the matching loss.",
            "sample_complexity_without_pretraining": "Without semi-supervised pretraining, achieving comparable caption recognition or downstream transfer would have required substantially more labeled captions (paper reports semi-supervised equals supervised with ≈3× more labeled data for general caption metrics and indicates &gt;300× more labeled drum captions would be needed to match drum true-positive rates).",
            "sample_complexity_gain": "Large batch contrastive pretraining improved label efficiency: semi-supervised contrastive approach equaled supervised models trained on ~3× more labeled data and enabled downstream zero-shot behavior with only 150–585 labeled captions versus needing many thousands of interaction episodes for comparable task performance.",
            "transfer_success_factors": "Large-batch contrastive matching aligns image and language embedding spaces; inclusion of sampled captions as positives during agent training provides dense language supervision; the contrastive objective avoids modeling a high-dimensional image decoder while still teaching semantically meaningful captions.",
            "transfer_failure_factors": "Contrastive method depends on very large negative-sample batches (practical compute/ memory cost) and may be sensitive to batch composition; like the generative model, transfer is indirect (auxiliary losses) so fine motor control tasks not tightly coupled to linguistic descriptions may see less benefit.",
            "key_findings": "A contrastive semi-supervised intra-agent speech model, trained on massive unlabeled images plus sparse captions and large-batch image-caption matching, yields data-efficient language learning and supports zero-shot transfer to embodied object-manipulation and QA tasks when its sampled captions are used as auxiliary supervision during agent behavioral cloning; performance and sample-efficiency gains are comparable to the generative variant and both outperform supervised baselines.",
            "uuid": "e1698.1",
            "source_info": {
                "paper_title": "Intra-agent speech permits zero-shot task acquisition",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Playhouse Agent (+Intra-Speech)",
            "name_full": "Interactive Playhouse Embodied Agent augmented with Pretrained Intra-Agent Speech Module",
            "brief_description": "An embodied agent (based on the DeepMind Interactive Agents architecture) trained by behavioral cloning on human Playhouse interactions, augmented with a frozen pretrained intra-agent speech module whose sampled captions are used as auxiliary targets and inputs (caption loss and caption-matching loss) to shape language-conditioned action policies and improve zero-shot behavior about novel objects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "Playhouse interactive agent with pretrained intra-agent speech module",
            "model_agent_description": "Agent architecture follows Interactive Agents Team: encodes visual and textual observations, has separate language (π_l) and movement (π_m) policies, trained with behavioral cloning loss L_BC. Augmented by two auxiliary losses during BC training: caption loss L_C (agent's language output trained to match sampled captions from pretrained module) and caption-matching loss L_CM (auxiliary classifier D predicts whether an input caption matches the current visual observation). The pretrained intra-agent speech module (generative or contrastive variant) is frozen while used to sample captions at high frequency.",
            "pretraining_data_type": "language-caption pretraining (paired captions) and large unlabeled image corpus used to learn intra-agent speech; behavioral policy training used Playhouse human interaction data (imitation learning).",
            "pretraining_data_details": "Pretraining: 78K human-collected paired captions, ~2B unlabeled Playhouse frames; for some experiments drum captions were withheld then reintroduced in sizes 150 and 585. Behavioral cloning training data: Playhouse human interaction corpus (~3 years of interactions, ~2B frames total) with 9,344 drum-containing interactions removed in 'zero-shot' evaluations. Agent behavioral cloning optimization used same hyperparameters as Interactive Agents Team [22].",
            "embodied_task_name": "Lift drum; Ask color of drum (Playhouse zero-shot evaluation tasks)",
            "embodied_task_description": "Evaluation tasks instantiate a drum object in randomized Playhouse environments: 'Lift the drum' requires locating and lifting any drum (episode terminates with reward 1 upon lifting drum); 'What is the color of the drum?' requires producing the drum color as language output. Each agent evaluated over 1,000 episodes per task; rewards normalized by human average to [0,1].",
            "action_space_text": "High-level natural language outputs from the agent's language policy (token-level discrete emissions from a 4k SentencePiece vocabulary) used both for dialogue and to answer evaluation questions.",
            "action_space_embodied": "Discrete movement/manipulation action primitives (Playhouse movement policy π_m) combined with language emission actions (π_l). The paper does not enumerate exact low-level motor primitives but follows the Interactive Agents Team action space (discrete navigation/manipulation commands).",
            "action_mapping_method": "Mapping is learned implicitly: the frozen pretrained intra-agent speech module generates captions that serve as supervisory signals (targets) for the agent's language outputs and as positives for caption-matching detection; gradients from these auxiliary losses flow into shared perception and action-selection components, thereby associating visual observations and language with appropriate movement/manipulation actions through the behavioral cloning objective. No explicit symbolic mapping layer is used.",
            "perception_requirements": "RGB visual observations (96×72 input size), object identity and color metadata available for evaluation; agent encoders process visual + text observations consistent with Playhouse architecture.",
            "transfer_successful": true,
            "performance_with_pretraining": "Agent trained without any drum interaction episodes but with caption and caption-matching auxiliary losses from pretrained intra-agent speech reached more than 0.5 human-normalized reward (≈70% of the +D agent's performance) on the drum tasks; +C+CM achieved approximately 70% of the performance of +D (which reached ≈0.75 for lift and ≈0.8 for color). Using only 150 labeled drum captions (G150) produced near-equal performance on the color QA task and only a slight reduction on lift.",
            "performance_without_pretraining": "Baseline agent without intra-agent speech auxiliary supervision and without drum interactions performed poorly on drum tasks (substantially below 0.5 human-normalized reward); supervised caption models trained on the same small caption sets (e.g., S585) performed worse both at caption recognition and downstream embodied tasks.",
            "sample_complexity_with_pretraining": "Zero drum interaction episodes required; as few as 150 labeled drum captions (plus the large unlabeled image corpus used in semi-supervised pretraining) produced strong zero-shot task performance (particularly for color QA); 585 labeled drum captions achieved robust caption recognition (≈70% drum true-positive) and downstream transfer.",
            "sample_complexity_without_pretraining": "Agents that achieved the highest embodied performance trained with explicit drum interactions used +D = 9,344 drum-containing interaction episodes; supervised captioning baselines would need many more labeled captions (paper reports &gt;300× more labeled drum captions to match semi-supervised detection).",
            "sample_complexity_gain": "The pretrained intra-agent speech module enabled agents to obtain ≈70% of the +D performance with 0 task-specific interaction episodes, effectively replacing ~9.3k interaction episodes with 150–585 labeled captions plus large-scale unlabeled image data; semi-supervised captioning provided ≈3× label-efficiency for general caption metrics and orders-of-magnitude label-efficiency for novel-object recognition versus supervised baselines.",
            "transfer_success_factors": "Dense sampled captions from the pretrained module during agent training provided frequent language supervision; the captioning models produced object-centric linguistic labels that aligned with manipulation tasks; the large unlabeled image corpus allowed the captioner to learn robust visual-linguistic associations that generalized to objects never seen in task-directed interactions; freezing the captioner provided stable targets.",
            "transfer_failure_factors": "Transfer is indirect (auxiliary supervision) rather than via explicit symbolic action mapping; some motor skills (precise lifting) still showed slight degradation when caption data was extremely scarce (150 captions) indicating limits to language-only supervision for fine manipulation; potential brittleness to differences between captioning training distribution and embodied task visual contexts.",
            "key_findings": "Augmenting a behavioral-cloning Playhouse agent with a frozen, pretrained intra-agent speech module (generative or contrastive semi-supervised captioner) and using sampled captions as auxiliary supervision enables substantial zero-shot acquisition of object-directed behaviors: agents with no task-specific interaction episodes but with 150–585 labeled captions plus semi-supervised pretraining achieved ≈70% of the performance of agents trained on ~9.3k object interaction episodes, demonstrating that language pretraining on image-caption data can meaningfully transfer to 3D embodied motor and question-answering tasks via auxiliary losses.",
            "uuid": "e1698.2",
            "source_info": {
                "paper_title": "Intra-agent speech permits zero-shot task acquisition",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Creating multimodal interactive agents with imitation and self-supervised learning",
            "rating": 2,
            "sanitized_title": "creating_multimodal_interactive_agents_with_imitation_and_selfsupervised_learning"
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "Learning language-conditioned robot behavior from offline data and crowd-sourced annotation",
            "rating": 2,
            "sanitized_title": "learning_languageconditioned_robot_behavior_from_offline_data_and_crowdsourced_annotation"
        },
        {
            "paper_title": "Human instruction-following with deep reinforcement learning via transfer-learning from text",
            "rating": 1,
            "sanitized_title": "human_instructionfollowing_with_deep_reinforcement_learning_via_transferlearning_from_text"
        },
        {
            "paper_title": "Zero experience required: Plug & play modular transfer learning for semantic visual navigation",
            "rating": 1,
            "sanitized_title": "zero_experience_required_plug_play_modular_transfer_learning_for_semantic_visual_navigation"
        }
    ],
    "cost": 0.016743,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Intra-agent speech permits zero-shot task acquisition
7 Jun 2022</p>
<p>Chen Yan 
LondonUK</p>
<p>Federico Carnevale 
LondonUK</p>
<p>Petko Georgiev petkoig@deepmind.com 
LondonUK</p>
<p>Adam Santoro adamsantoro@deepmind.com 
LondonUK</p>
<p>Aurelia Guy 7aureliaguy@gmail.com 
San FranciscoUSA</p>
<p>Alistair Muldal alimuldal@deepmind.com 
LondonUK</p>
<p>Chia-Chun Hung 
Isomorphic Labs London
UK</p>
<p>Josh Abramson jabramson@deepmind.com 
LondonUK</p>
<p>Timothy Lillicrap 
LondonUK</p>
<p>Gregory Wayne gregwayne@deepmind.com 
LondonUK</p>
<p>Intra-agent speech permits zero-shot task acquisition
7 Jun 2022EE8776CE7760844E9BEA133DB836F15BarXiv:2206.03139v1[cs.LG]
Human language learners are exposed to a trickle of informative, context-sensitive language, but a flood of raw sensory data.Through both social language use and internal processes of rehearsal and practice, language learners are able to build high-level, semantic representations that explain their perceptions.Here, we take inspiration from such processes of "inner speech" in humans(Vygotsky, 1934)to better understand the role of intra-agent speech in embodied behaviour.First, we formally pose intra-agent speech as a semi-supervised problem and develop two algorithms that enable visually grounded captioning with little labeled language data.We then experimentally compute scaling curves over different amounts of labeled data and compare the data efficiency against a supervised learning baseline.Finally, we incorporate intra-agent speech into an embodied, mobile manipulator agent operating in a 3D virtual world, and show that with as few as 150 additional image captions, intra-agent speech endows the agent with the ability to manipulate and answer questions about a new object without any related task-directed experience (zero-shot).Taken together, our experiments suggest that modelling intra-agent speech is effective in enabling embodied agents to learn new tasks efficiently and without direct interaction experience.Preprint.Under review.</p>
<p>Introduction</p>
<p>Contemporary language models learn from troves of text comprising billions, and sometimes trillions of tokens.This is not the same situation faced by embodied language learners, such as human children, whose raw sensory data vastly exceeds their social-linguistic experiences.Psychologists have long noted the existence of non-social, intra-personal dialogue-such as inner speech (silent) or private</p>
<p>Inference</p>
<p>There is a drum on the floor.</p>
<p>Put a pear on the table.I can see a drum in front of me.</p>
<p>Caption about new object (rare) Self supervision about new object Tasks about known objects</p>
<p>Figure 1: Learning to speak about new objects permits zero-shot motor task behavior.Is it possible for agents to perform complex tasks with a new object just by learning to name it?In this work, we show it is.We first trained agents to speak about objects that had never been used in any task, nor named in any interaction, by training agents to caption their observations (top row, left and middle).We then introduced interactive tasks involving other objects and trained agents with imitation learning (top right).Using only a small number of labeled captions that pertained to a new object category (drums), agents were able to perform tasks involving the new objects, such as lifting and delivering them to a second avatar (bottom).</p>
<p>speech (aloud)-which provides additional linguistic experiences beyond those encountered in social settings.Indeed, one possible reason for intra-personal speech may be to support the development of social speech [1][2][3][4][5], though the inverse causal relation may also be possible [1,6].</p>
<p>Intra-personal speech may also affect broader aspects of our cognition: the process wherein we privately speak about our perceptions subsequently impacts how we attend to, remember, or manipulate the objects of our perception [7][8][9][10][11][12].For example, our short-term memories tend to commit errors for items that are phonetically but not visually or semantically similar [8,9], and "self-talk" affects task performance on executive tasks both in children [11,12] and in adults [7], and has been widely used to improve athletic performance [10].</p>
<p>Inspired by these ideas, here we ask if we can leverage intra-agent speech as a tool for artificial intelligence research.Specifically, we ask the following questions: First, is it possible to model intra-agent speech as a machine learning problem, enabling us to make up for a paucity of language data when given access to a large amount of unlabeled data?Second, can mechanisms for learning intra-agent speech impact the subsequent behavior of an embodied agent?</p>
<p>Regarding the first, we formally structure intra-agent speech as a semi-supervised problem.In particular, we model intra-agent speech samples as variational latent variables representing images, and use this perspective to devise two algorithms enabling visual captioning given a paucity of directly labeled data.Regarding the second, we go on to show how embedding such algorithms in an embodied agent not only permits the capacity to speak about any given perception (that is, to caption, which is precisely what is being optimized for) but rather also permits zero-shot behaviors involving new objects that were never directly implicated in any task-directed experience (see Figure 1).</p>
<p>Approach</p>
<p>At a high level, we structured intra-agent speech as the production of language generated from inputs (which in our case manifests as captions of images), with two additional concerns: First, there will be a preponderance of unlabeled data (i.e., pure images) relative to labeled data (i.e., images with associated captions) in our captioning dataset, much like the situations faced by human children in nature and semi-supervised learning in machine learning.Second, the linguistic representations eventually produced will provide auxiliary supervision for an embodied, acting agent so that they may influence interactive behavior.Depicted is a caption loss.We also experimented with a contrastive "matching" loss wherein captions are provided as inputs</p>
<p>Figure 2: Using models of intra-agent speech to influence behavior.During intra-agent speech pre-training, agents receive image observations as input and produce either utterances that describe the observations (for the small number of inputs for which we have human annotated labels) or are tasked with reconstructing the images (using a generative model whose latent representations comprise language tokens).During behavioral training, the intra-agent speech module is frozen and is used to provide targets for an auxiliary captioning loss, whose gradients affect components responsible for action selection.</p>
<p>While in humans both inner speech and behavior develop simultaneously, here the system is trained in two stages for simplicity.First, we develop a semi-supervised language learning method to efficiently pre-train a capacity for intra-agent speech when given access to large amounts of unlabeled data and small amounts of labeled language data.We then explore how this capacity for intra-agent speech can be leveraged to shape subsequent interactive behavior in an embodied agent.A diagram of the semi-supervised learning approach to intra-agent speech and its role in acquiring behaviors is shown in Figure 2.</p>
<p>Methods for Semi-supervised Language Learning</p>
<p>Our data D are composed of a dataset D p of paired data {x, y} and a dataset D u of unpaired data {x}.In our interpretation, y corresponds to language (a caption) describing the image x.When observing unpaired data, we treat y as a latent variable that must be inferred to explain the observed data [13].We develop two variants, one generative and one contrastive, of the semi-supervised language learning algorithm in what follows.</p>
<p>Generative variant</p>
<p>The preponderance of our data is unlabeled.To model these data we can maximise a variational lower bound on the log marginal distribution, log p θ (x), corresponding to posterior inference.We will use a learned approximation to the true posterior distribution, q ω (y | x).We call this approximate distribution an "image-conditional language encoder" (or sometimes, simply, caption model) since its functional role is to convert images into a discrete latent code that is, ideally, a semantically meaningful language caption:
x∼Du log p θ (x) ≥ x∼Du E qω(y|x) [log p θ (x | y)] − D KL <a href="1">q ω (y | x) p φ (y)</a>
= J u .</p>
<p>In Equation 1, the space of latent codes (captions) is discrete and vast, complicating optimization.However, we can form a stochastic approximation by sampling from q ω (y | x) to estimate a gradient.
∇ ω J u = x∼Du E qω(y|x) ∇ ω log q ω (y | x) log p θ (x | y) + log p φ (y) − log q ω (y | x) .(2)
For simplicity, we pretrain the prior p φ (y) as a language model that learns on all of our caption data, and fix it during this optimisation phase.Thus, optimising J u corresponds to an entropy-regularised RL problem [14][15][16].The encoder q ω (y | x) serves as a policy that achieves reward for matching the language model prior while enabling the language-conditional decoder to reconstruct the image.</p>
<p>While this reward pressures the image-conditional language encoder to produce linguistically meaningful latent codes, we can also exert additional pressure by using a small number of images with associated captions ("labeled data"), and leverage supervised learning to learn log p θ (x | y), log p φ (y), log q ω (y | x):
J p = x,y∼Dp log p θ (x | y) + log p φ (y) + log q ω (y | x) .(3)
Over the combined paired and unpaired datasets, our complete objective is to maximise J = J p + J u .</p>
<p>Contrastive variant</p>
<p>The language-conditional image decoder p θ (x | y) is a potentially complicated term to model.To understand if this term represents a liability for the approach and to generate an algorithm with potentially complementary strengths, we also developed a contrastive, energy-based approach, amounting to multi-class classification, to approximate it [17][18][19].If we have a multi-class classifier that discriminates if a batch element y j is paired to a batch element x j , and we express its crossentropy loss for the correct index c = j as L({x b } B b=1 , y j , c = j), then we can show that the generative reconstruction loss log p θ (x j | y j ) is proportional to L({x b } B b=1 , y j , c = j) up to constant factors with respect to y (Appendix A.1). Thus, we can substitute L({x b } B b=1 , y j , c = j) for batch element j, log p θ (x j | y j ), in Equation 2, and the gradients with respect to ω remain the same in expectation.</p>
<p>We represented the softmax in the classifier as e f (x j ) g(y j ) B b=1 e f (x b ) g(y j ) , where f and g are networks that compress the image into a vector and the caption into a vector, respectively.To train this classifier, we adopted a similar approach to recent models for visual language representation learning [20,21] by minimizing the sum of two losses, one for matching each image to its paired caption in the batch, and one for matching each caption to its paired image:
L = 1 B B j=1 log e f (xj ) g(yj ) B b=1 e f (x b ) g(yj ) + 1 B B j=1 log e f (xj ) g(yj ) B b=1 e f (xj ) g(y b )(4)
The networks f and g are trained at the same time as the rest of the model, and samples from q ω (y j | x j ) are included as positive examples for the classifier along with the original paired data.</p>
<p>Intra-agent speech architecture and optimization</p>
<p>Image-conditional language encoder.The image-conditional language encoder received input images with resolution 96 × 72.We used the ResNet architecture described in [22], with strides (1, 1, 2, 2), 3 × 3 kernel size, and channel sizes (64,64,128,256) for a total of 20 layers.Language was produced by a 4-layer causal transformer with 256 embedding size and 4 attention heads [23], which attended to the 432 hyper-pixel vectors generated from the ResNet, and produced a sequence of logits corresponding to a 4, 000 token vocabulary.Language targets were encoded with a Senten-cePiece byte-pair encoder [24] trained using language data from [25].</p>
<p>Generative semi-supervised model.The language prior comprised a separate transformer pretrained on all the caption labels in the training data (and hence, did not have image inputs).For the image decoder, we first pre-trained a VQ-VAE [26] on all unlabeled images to define the VQ-VAE's codebook, compressing each image into 432 tokens with a vocabulary of 512.We then used an 8-layer transformer with 512 embedding size with causal masking to model the VQ-VAE tokens autoregressively.The conditioning captions were tokenized and embedded as in the image-conditional language encoder, and then processed by a distinct transformer with 4 layers, 256 embedding size, and 4 heads, whose output was then cross-attended by the image decoder to produce a reconstruction.</p>
<p>Contrastive semi-supervised model.For the contrastive model, images were first encoded using a ResNet with the same architecture as the image-conditional language encoder, and globally meanpooled into one vector with dimension 1024.Language was first encoded using a transformer with Optimization.We used V-MPO [16] to optimize q ω (y | x).The loss was optimized by the Adam optimizer [27] with β 1 = 0.9, β 2 = 0.999 and a learning rate of 2 × 10 −4 with early stopping at the lowest log-likelihood over captions in validation data.We trained all models with a batch size of 128 except for the contrastive classifier model, which also received 2, 048 unlabeled images per batch, giving a total batch size of 2, 176.We trained our models using Tensor Processing Units (TPUv3) [28].In all experiments the early stopping criteria was reached within 150K optimization steps.</p>
<p>Agent training</p>
<p>The agent was trained by behavioral cloning on a large corpus of human interactions in the Playhouse environment [22].Briefly, it comprises a 3D simulated environment wherein two embodied avatars interact with natural language to cooperatively accomplish tasks involving object manipulation, navigation, and question answering.In addition to receiving visual observations and producing motor actions, agents in this environment also receive language inputs and produce language outputs.This allows them to answer questions, participate in dialogue, ask for clarifications, and so on.</p>
<p>Our approach to training the linguistic representations of the agent therefore has two steps.First, by using semi-supervised language learning, we can amplify the impact of sparse language annotations provided for a small number of images in learning the intra-agent speech module.In turn, using this trained module, we are able to sample captions at high frequency as the agent acts, providing the agent with dense language supervision.Therefore, while the agent trains using behavioral cloning, we can sample captions from the pretrained intra-agent speech module, and provide these as either auxiliary inputs or targets to the agent so that they may influence the agent parameters optimized to produce embodied behaviors.</p>
<p>For our first loss -the caption loss -captions arising as intra-agent speech samples were provided as targets for the agent's language output policy:
L C = − 1 B B b=1 K t=0 ln π l (y c b,t |o b,≤t ),(5)
where π l is the language action policy of the agent, y c is the caption sample, o represents all visual and text observations, and K represents the maximum unroll length.To allow the agent to distinguish whether it was being tasked with emitting a caption of its current observation, or whether it was being tasked with emitting language for the purposes of interactive behavior (see [22]), the agent also received a corresponding binary indicator embedding as input.</p>
<p>While the caption loss trains the agent to speak about what it sees, the agent's language encoding (via linguistic inputs) remains untrained.To this end we introduced a second loss: the caption-matching loss.In the caption-matching loss, the agent is required to predict whether a given caption, provided as input, matches the observed image.Negative samples (i.e., captions that do not match the observation) are captions associated with images from elsewhere in the batch [29].To perform this prediction, an auxiliary classifier D is attached to the network of the agent that encodes both visual and text observations in a separate training pass from the behavioral cloning optimization.
L CM = − 1 B B b=1 K t=0 ln D(o X b,t , y c b,t ) + ln 1 − D(o X b,t , y c roll(b),t ) .(6)
Here o X represents the visual observation, and the caption sample is fed as a text observation.The roll() function rolls the index over batches, so that y c roll(b) represents a caption from another batch element as a negative example.The roll function was implemented as roll(b) = (b + 1) mod B.</p>
<p>And finally, the behavioral cloning loss over human language and movement action sequences is as described in Interactive Agents Team [22]:
L BC = − 1 B B b=1 K t=0 ln π l (a l b,t |o b,≤t ) + ln π m (a m b,t |o b,≤t ) ,(7)
where the π m and a m represents the movement policy and action, respectively.</p>
<p>The total loss is the sum of these losses L = L C +L CM +L BC .Other than the addition of the caption loss and caption-matching loss, we follow Interactive Agents Team [22] for the agent architecture and training hyper-parameters.</p>
<p>3 Experiments</p>
<p>Semi-supervised captioning</p>
<p>Hypothesis.We first sought to validate our method of semi-supervised learning.We hypothesized that with the additional unlabeled images, the semi-supervised model would be more data efficient (with respect to labelled data) than a supervised baseline.</p>
<p>Data.As mentioned, the domain in which we tested our methods is called the Playhouse, which originated in Interactive Agents Team [22].The authors compiled approximately three years worth of interaction data, comprising about 2 billion image frames.The authors have confirmed no personally identifiable information or offensive content is contained in the dataset and gave consent for us to access it.As there are no associated captions with any frames, we treat each frame in this dataset as a single image, and used all the data as our "unpaired" dataset.</p>
<p>For the "paired" dataset, we engaged with crowd raters to provide corresponding captions for 78K uniformly sampled images from the unpaired dataset.Raters were instructed to describe what they saw in the image, with particular reference to a randomly selected object indicated in a bounding box.Detailed instructions are included in Appendix A.2.</p>
<p>Evaluation.We measured the log probability of captions in the validation set, CIDEr score [30] and "color-object accuracy".As the images were generated from the Playhouse environment, we had access to the ground truth object identities and colors present in each image, allowing us to check if color-object pairs mentioned in a caption were indeed present in the image.We calculated color-object accuracy to be the proportion of correct color-object pairs among all mentioned in sampled captions, normalized by the same calculation from human captions.As there are usually many objects in an image and captions tend to mention just a few, the recall counterpart of this metric (number of correct color-object pairs divided by those present in the image) is not very informative.Captions were greedily sampled for the calculation of CIDEr score and color-object accuracy.</p>
<p>Results. Figure 3 shows sample data used to train the our models, as well as samples generated from the generative semi-supervised model.Models were able to both produce coherent and relevant captions given an image (e.g., "I can see a white bed where a green ball and a green duck are on it") and also generated realistic visual depictions of the Playhouse when conditioned on a language description.</p>
<p>Both semi-supervised methods performed better than the supervised baseline across all metrics, including settings where we artificially constrained the size of the labeled dataset (Figure 4).Notably, when using the full amount of data our semi-supervised methods exceeded human color-object accuracy.To further put the performance gain in context, by leveraging the unpaired dataset of images the semi-supervised methods performed at a level equivalent to training a supervised model on 3× more labeled data [31].</p>
<p>Learning captions of new objects</p>
<p>Hypothesis.Upon validating our methodology and amassing the relevant data, we were well-poised to ask the following questions: do semi-supervised methods of intra-agent speech allow a model to quickly learn to speak about a new object with little supervision?Data.To address the hypothesis, we first chose an object -a drum -and filtered our labeled caption data to remove all instances where the drum is either depicted visually or spoken about verbally.</p>
<p>While not strictly necessary, from the unlabeled dataset we also removed all instances where the underlying (and unobserved) instruction included mention of a drum, which ensured that all instances of drum appearances were truly "passive".For example, the unlabeled data may have included trajectories of experience wherein a players was tasked with removing a ball from a shelf, but there might have also happened to be a drum in view in the room.As seen from the captions, generative modeling of images using a pre-trained language prior, in conjunction with auxiliary supervised training on a small number of labeled samples, is sufficient to produce meaningful and relevant language captions of images.Moreover, the model is able to generate realistic-looking scenes from provided captions.See Appendix A.3 for more samples.Semi-supervised methods are trained with unlabeled data and titered amounts of labeled image-caption pairs.Here we calculate the log probability of validation set captions, the CIDEr score [30] and color-object accuracy.Color-object accuracy is calculated as the number of correct color-object pairs mentioned in the model caption samples divided by total number of color-object pairs mentioned in the caption.The same is calculated with human ground truth captions and the final score is calculated by the ratio of model over human color-object accuracy.Both semi-supervised models reach performances that equal that of the supervised model trained with 3x the data, and exceed human performance in color-object accuracy.Shading represents 95% confident intervals estimated from 3 random seeds for each data point.</p>
<p>Evaluation.We evaluated the model using three different metrics.We first created a validation dataset of all captions mentioning drum with their associated image, and evaluated the model's average log likelihood on this drum dataset (drum caption log likelihood).The drum true positive rate, which is the probability that it mentions a drum given that a drum is present, measures how the model recognizes drums in the images.We also calculate its counterpart, the probability of the model erroneously mentioning a drum when it is not present as the drum false positive rate.</p>
<p>Results.We then performed the same experiment as in Section 3.1 at varying levels of re-introduced labeled drum data, as seen in Figure 4.As expected, the model's ability to speak about drums scales with the amount of labeled drum data with which it is trained.Notably, the model's true positive rate reaches approximately 70% given only 585 labeled samples of drums.In contrast, the supervised method reaches just over 20%.This indicates that the semi-supervised methods are able to learn about new objects, implicitly, by virtue of their language-conditioned reconstruction objective.That is, the mere presence of drums in the visual observations was sufficient, in conjunction with a small amount of labeled drum captions, to elicit linguistic understanding of drums as relevant objects in the Playhouse.</p>
<p>Generative semi-supervised</p>
<p>Supervised</p>
<p>Drum caption log likelihood</p>
<p>Drum true positive rate Drum false positive rate logP(drum captions) P(drum mentioned|drum present) P(drum mentioned|drum not present)</p>
<p>Figure 5: Scaling performance of our semi-supervised methods over drum dataset sizes.We examined how the semi-supervised and supervised methods scaled across different amounts of labeled data for a novel object.The semi-supervised trained model had a higher log likelihood, was more likely to recognize a drum in the image (high true positive rate) while displaying a comparable error-rate (same false positive rate).Extrapolating from supervised true positive rate, the supervised method would need more than 300× more data to reach the same performance as the semi-supervised model.Shades represent 95% confident interval estimated from 3 random seeds for each data point.</p>
<p>Learning to manipulate a new object</p>
<p>Hypothesis.As a final test we tackled the final question: Can the ability to quickly learn to speak about an object confer advantages for learning behaviors involving the new object (such as manipulating the object, or answering questions about it)?</p>
<p>Methods and data.To answer this question we leveraged the semi-supervised models built in Section 3.2 to provide auxiliary intra-agent speech objectives.While we employed a two-stage process (learning to caption, and subsequently learning from those captions to shape behavior) for expedience and ease of experimental setup, in principle the two learning procedures could occur in parallel in a unified agent.The agent then proceeded with behavioral cloning-based training as in Interactive Agents Team [22] on data that was filtered to not include any drum-based instructions (resulting in 9, 344 removed interactions).</p>
<p>Evaluation.We evaluated agent performance on two tasks: in the "lift drum" task, the agent receives the instruction "lift a drum", and is required to find the drum in a randomized Playhouse environment and lift it.In the "ask color drum" task, a single drum is spawned in the environment.The agent receives the instruction "what is the color of the drum?", and is then required to output language that answers the question (See Appendix A.4 for details).We collected human performance on each of the tasks and normalized reward by the average human score (note that rewards are used for evaluation purposes only, and not for training).</p>
<p>Results.As seen in Figure 6, a baseline agent trained without any drum-based instructions performs poorly on drum-based evaluation tasks, indicating that there is minimal transfer to be had when learning from the base interaction dataset.On the other hand, a model trained with drum-based instructions (that is, one that trains on the 9, 344 episodes that we had filtered) displays an expected upper bound for this agent of just over 75% on "lifting drums", and just under 80% for answering questions about a drum's color, similar to previously reported results [22].</p>
<p>Agents trained with both the caption loss and caption-matching loss, but without any drum-based interaction data achieve approximately 70% of the performance attained by models trained directly on drum-based interaction data.This is a substantial increase over the baseline model, and directly indicates that learning to speak about drums, which is afforded by training a model on as few as 585 labeled drum examples using semi-supervised methods, allows an agent to subsequently exhibit behaviors that implicate the manipulation or mention of drums, without affecting performance on a known object (Appendix A.5).</p>
<p>We then tested whether we could reduce the number of labeled captions even further, and observed that with as few as 150 labeled captions we observed similar performance on drum-color question answering tasks, and only a slight decrease in drum lifting task performance.Figure 6: Zero-shot task-directed interaction with a novel object.We evaluated agent performance on two tasks that require interaction with the novel object (drum): in the first, the agent was tasked to locate and lift a drum, and in the second, the agent is required to mention the color of the drum using language.Our model with both the caption loss and caption matching loss (+CM) reaches more than 0.5 human normalized reward, comparable to approximately 70% of the performance of an agent trained with an additional 9344 episodes of interaction data including drums (+D; left two panels).This result also suggests the crucial role of semi-supervised training in zero-shot generalization (right two panels): with just 150 labeled captions of drum (G150), our model only displays a slight decrease in performance, but still significantly outperforms the supervised model trained on all the labelled captions (S585).Errorbars represent 95% confident interval estimated from 3 random seeds for each data point.</p>
<p>Related Work</p>
<p>Components.The modeling in this study builds on components developed for generative modeling (VQVAE-2: [32]) and contrastive learning (CLIP: [20]).</p>
<p>Task generalization in embodied agents.Our work is in the tradition of grounded language research in virtual environments [33][34][35].Recently, there have been efforts to demonstrate zero-shot task acquisition using a variety of means including with the use of pretrained models for language encoders [36] or visual-language models for zero-shot navigation [37].The majority of these works have focused on generalization to novel phrasings of task instructions [38,39] with some recent studies exploring generalization to new tasks given background knowledge from previous tasks [22,40].Similar to phenomena of zero-shot generalization in supervised classification and object detection [20,21,41], these studies have often relied on a large, diverse dataset providing supervision -where held-out tasks represent a novel recombination of parts of the training dataset (e.g., "compositional" generalization or systematicity [42]).Another line of research has focused on fewshot generalization [43][44][45][46] through meta-learning over a distribution of tasks -with generalization to other tasks within the distribution's support.Our work is complementary to both lines of workand presents a means by which a different learning objective can embody "mental practice" enabling generalisation.</p>
<p>Semi-supervised learning with natural language.The semi-supervised methods we used to train the intra-agent speech module are related to many previous methods that use language as a latent variable.For example, Kingma et al. [13] developed a semi-supervised, variational autoencoder (VAE) with discrete latent variables, and this work has also been extended to use language as the latent variable.VAEs with language latent variables have also been studied in language summarization [47], and in the context of multi-agent communication with natural language [48,49].Our work may present a valuable contribution to this area, showcasing the utility of modern generative and contrastive models in supporting data-efficient learning.Our work's analysis of the scaling curve [31] properties of semi-supervised captioning [50][51][52] may provide additional analytical tools for the field.</p>
<p>Language-conditioned image generation.The use of language to create sophisticated, controllable generative models has seen rapid development in recent years.Recent works based on diffusion models [53][54][55] and autoregressive models have produced some of the most impressive results.Notable work includes the DALL•E family of models [56,57] and GLIDE [58].Other work in the area has also employed VAEs [59] and GANs [60][61][62][63][64].</p>
<p>Conclusions and Future Work</p>
<p>In this work we built a model for intra-agent speech by leveraging semi-supervised learning techniques that maximize the impact of a small number of labeled samples in a sea of unlabeled samples.We further demonstrated how such models can help embodied agents execute interactive tasks without having to learn from direct interactive experience (zero-shot).</p>
<p>Although we developed the method in a virtual environment, it can be easily generalized to real world applications, applied with either good or bad intent.Embodied language agents acting as robots could be instructed to perform a wide manner of tasks, including harmful ones.Thus, if this work is to be applied in real settings, we advise extra caution to minimize unintended or harmful consequences.</p>
<p>This work is a first step into leveraging ideas from the psychology of inner and private speech in humans.While our source of speech was captioning, humans have a richer repertoire of techniques for intra-personal dialogue that affect modalities beyond vision.Direct next steps are, therefore, to explore a possible role of intra-agent speech in abstracting over time, and observing its effects on planning and reasoning.</p>
<p>A.2 Details for caption data collection We recruited participants (N = 80) through an internal crowdsourcing pool, and collected a total of 78K captions using a total of 4, 000 participant hours.The full details of our study design, including compensation rates, were reviewed by our institution's independent ethical review committee.All participants provided informed consent prior to completing tasks and were reimbursed for their time.Participants were provided with a link to the caption interface and the following task description:</p>
<p>You will be shown an image of a room, where one of the objects is highlighted with a dotted red rectangle.Follow the script, think of a description of the object and its surroundings and type it in the text field.The description should also have enough details such that another player in the room can easily find and understand which object you are referring to.</p>
<p>During data collection, the caption interface displayed a single frame randomly selected from human-human interaction data as described by [22] at a resolution of 320 × 240.An object from the image was randomly selected and highlighted by a bounding box.The participants were prompted to "[d]escribe the object in the red dotted rectangle, and its location relative to other objects or landmarks".After the participants input the caption in the text-box and clicked "submit", the image is refreshed and text-box is cleared for the collection of next caption.Numbered elements in caption interface: 1.The prompt for the participants.2. The text-box for the participants to input caption.3. Image to be captioned.4. Highlighted object in the scene.</p>
<p>A.3 Additional samples from generative semi-supervised model</p>
<p>Human: i can see a orange color table on floor and a green color object is present.</p>
<p>Human: there is a yellow table, which is in front of the white lamp.</p>
<p>Human: there is a red color rack right of the brown color shelf Human: there is a pink headphone on the floor which is near a green mug and a red headphone</p>
<p>Human: i can see blue table on which red hair dryer is placed</p>
<p>Human: there is a white box close to the green chair and green stool, in front of it we can see red headset and red car.</p>
<p>Human: i can see a pink color cupboard on top of the floor which is close to the wall.</p>
<p>Human: i can see a shelf under which there is another shelf and also i can see a bed on which there are some objects placed.</p>
<p>Caption samples</p>
<p>Language conditioned image samples</p>
<p>Human: i can see one orange color helicopter near the green bus.</p>
<p>Model: i can see a green car on the floor which is near the brown helicopter</p>
<p>Human: there is a ledge below another ledge Model: i can see a yellow duck on the green bed.</p>
<p>Human: on top of the wooden rack there is a green robot and a yellow lamp.under the wooden rack there is a violet lamp.</p>
<p>Model: i can see a wooden shelf at the wall.</p>
<p>Human: there is a cyan stool near two yellow objects Model: i can see a blue frame on the floor which is near the window.</p>
<p>Human: the bed is in pink color, and in rectangular shape which is placed on the floor.</p>
<p>Model: i can see a olive green color car on the pink color bed near the three objects on it.</p>
<p>Human: i can see a violet color table and white ball on it, on the left there is a green bed, teddy on floor.</p>
<p>Model: i can see a green bed placed on floor and there is a teddy placed in front of that bed.</p>
<p>Human: there is a green rocket in front of the orange wardrobe, close to the bed.</p>
<p>Model: i can see a brown shelf, which is above the violet bed.</p>
<p>Human: there is a green engine on the floor which is in between the red headset and pink chopper.</p>
<p>Model: i can see a cyan headphone placed on floor and there is a bed placed near that headphone.</p>
<p>A.4 Details about lift / ask color tasks</p>
<p>We modified the Playhouse environment [22] to create the lift drum and ask color drum tasks.These tasks were used for evaluation only, and the agent was never trained in these tasks.For these tasks, We first initialize a randomized playhouse environment as described in [22], which represents a randomized multi-room environment.We then spawn a drum object in the room where the agent avatar is spawned.The color of the drum object is randomly selected from the following list of 10 colors: "red", "yellow", "blue", "white", "green", "pink", "purple", "orange", "aquamarine", "magenta".</p>
<p>Lift task.In the lift task, the instruction "Lift the drum."appears after a random delay of up to 10 seconds.A reward of 1 is given if the agent lifts any drum object, and the episode terminates after reward is emitted.If the agent lifts any other object, or times out after 2 minutes, the episode terminates with a reward of 0.</p>
<p>Ask about color task.In the ask about color task, the instruction "What is the color of the drum?" appears after a random delay of up to 10 seconds.If the agent emits language that matches the color of the drum, a reward of 1 is given, and the episode terminates.Otherwise if the agent outputs any other text, or times out after 2 minutes with no language output, the episode terminates with a reward of 0.</p>
<p>For each agent, we averaged rewards collected from 1, 000 episodes for each task.We also collected human scores on these tasks, and used it to normalize the agent reward to a range of [0, 1].The human normalized score is reported in the manuscript.</p>
<p>A.5 Results on control objects</p>
<p>Precedural reward (human normalized) To confirm the improvement of performance on the drum task is a result of zero-shot generalization from drum captions without affecting the background ability the agent, we tested the agent on the tasks targeting a control object, the teddy bear.The teddy bear object is included in the background interaction data and the agent has been trained on data manipulating teddy bear.These tasks are similar to the lift and ask about color tasks described in Appendix A.4, except that the drum object is replaced with the teddy bear object, and all instance of the word "drum" in the instructions are replaced with "teddy bear".We show that the performance of these task is not different between different task, suggesting that our method of zero-shot learning new object does not affect background ability of the agent.</p>
<p>Ask about color
Drum
Figure 3 :
3
Figure 3: Data and Model Samples.Depicted are data used to train the semi-supervised model (left) and samples from the caption model (middle) and generative image model (right).As seen from the captions, generative modeling of images using a pre-trained language prior, in conjunction with auxiliary supervised training on a small number of labeled samples, is sufficient to produce meaningful and relevant language captions of images.Moreover, the model is able to generate realistic-looking scenes from provided captions.See Appendix A.3 for more samples.</p>
<p>Figure 4 :
4
Figure4: Scaling performance of semi-supervised models over labeled dataset sizes.Semi-supervised methods are trained with unlabeled data and titered amounts of labeled image-caption pairs.Here we calculate the log probability of validation set captions, the CIDEr score[30] and color-object accuracy.Color-object accuracy is calculated as the number of correct color-object pairs mentioned in the model caption samples divided by total number of color-object pairs mentioned in the caption.The same is calculated with human ground truth captions and the final score is calculated by the ratio of model over human color-object accuracy.Both semi-supervised models reach performances that equal that of the supervised model trained with 3x the data, and exceed human performance in color-object accuracy.Shading represents 95% confident intervals estimated from 3 random seeds for each data point.</p>
<p>B</p>
<p>= baseline without drum interaction, +C = caption loss, +M = caption matching loss, +D = drum interaction data, S585 = supervised caption model with 585 annotations of drum, G{150,585} = generative semi-supervised caption model with 150, 585 annotations of drum Human normalized reward Human normalized reward</p>
<p>Figure A1 :
A1
FigureA1: User interface and instructions for caption data collection.We recruited participants (N = 80) through an internal crowdsourcing pool, and collected a total of 78K captions using a total of 4, 000 participant hours.The full details of our study design, including compensation rates, were reviewed by our institution's independent ethical review committee.All participants provided informed consent prior to completing tasks and were reimbursed for their time.Participants were provided with a link to the caption interface and the following task description:</p>
<p>Figure A2 :
A2
Figure A2: Additional caption and image samples from generative semi-supervised model.</p>
<p>Figure A3 :
A3
FigureA3: Performance on manipulating a control object.To confirm the improvement of performance on the drum task is a result of zero-shot generalization from drum captions without affecting the background ability the agent, we tested the agent on the tasks targeting a control object, the teddy bear.The teddy bear object is included in the background interaction data and the agent has been trained on data manipulating teddy bear.These tasks are similar to the lift and ask about color tasks described in Appendix A.4, except that the drum object is replaced with the teddy bear object, and all instance of the word "drum" in the instructions are replaced with "teddy bear".We show that the performance of these task is not different between different task, suggesting that our method of zero-shot learning new object does not affect background ability of the agent.</p>
<p>Acknowledgments and Disclosure of FundingThe authors would like to thank Felix Hill and Nathaniel Wong for critical discussion and help with the development of scripted probe tasks, Chris Dyer and Daan Wierstra for reviewing and comments on the manuscript, and Alex Goldin and Guy Scully for organizational support.A AppendixA.1 Contrastive cross-entropy approximates generative log-likelihood Consider a batch of images {x b } B b=1 and a caption y corresponding to one of the images x j .This caption will come either from the paired dataset or as a sample from the image-conditional language encoder q ω (y | x j ).The posterior distribution over the classification of the image index c isTherefore, the multi-class cross-entropy over the correct index isWe can manipulate the third term into the form of an expectationand for large B, we can approximate 1In this large batch limit, the multi-class cross-entropy is proportional to the language-conditional image decoder log-likelihood up to constant factors in y:Therefore, if we have a multi-class classifier that discriminates if a batch element y is paired to a batch element x, we can substitute the generative log likelihood with the classifier's loss:, y, c = j)] + constant(y),(12)and gradients with respect to q ω (y | x j ) are the same in expectation.
Thought and language. Lev S Vygotsky, 1934MIT press</p>
<p>Private speech, executive functioning, and the development of verbal self-regulation. Ed Adam, Charles Ed Winsler, Ignacio Ed Fernyhough, Montero, 2009Cambridge University Press</p>
<p>Imitation improves language comprehension. Patti Adank, Peter Hagoort, Harold Bekkering, Psychological Science. 21122010</p>
<p>Private and inner speech and the regulation of social speech communication. Conchi San, Martín Martínez, Humbert Boada I Calbet, Peter Feigenbaum, Cognitive Development. 2632011</p>
<p>The phonological loop as a language learning device. Exploring Working Memory. Alan D Baddeley, Susan E Gathercole, Costanza Papagno, 2017</p>
<p>The role of private speech in the transition from collaborative to independent task performance in young children. Adam Winsler, Rafael M Diaz, Ignacio Montero, Early Childhood Research Quarterly. 1211997</p>
<p>Inner speech: development, cognitive functions, phenomenology, and neurobiology. Ben Alderson, - Day, Charles Fernyhough, Psychological bulletin. 14159312015</p>
<p>Short-term memory for word sequences as a function of acoustic, semantic and formal similarity. Alan D Baddeley, Quarterly journal of experimental psychology. 1841966</p>
<p>Speech-like coding of pictures in short-term memory. Diane J Schiano, Michael J Watkins, Memory &amp; Cognition. 911981</p>
<p>Evangelos Galanis, and Yiannis Theodorakis. Selftalk and sports performance: A meta-analysis. Antonis Hatzigeorgiadis, Nikos Zourbanos, Perspectives on Psychological Science. 642011</p>
<p>The development of verbal control over motor behavior: A replication and extension of luria's findings. S Virginia, Harriet Salatas Tinsley, Waters, Child Development. 1982</p>
<p>Private speech on an executive task: Relations with task difficulty and task performance. Charles Fernyhough, Emma Fradley, Cognitive development. 2012005</p>
<p>Semi-supervised learning with deep generative models. Shakir Durk P Kingma, Danilo Mohamed, Max Jimenez Rezende, Welling, Advances in neural information processing systems. 201427</p>
<p>Equivalence between policy gradients and soft q-learning. John Schulman, Xi Chen, Pieter Abbeel, arXiv:1704.064402017arXiv preprint</p>
<p>Sergey Levine, arXiv:1805.00909Reinforcement learning and control as probabilistic inference: Tutorial and review. 2018arXiv preprint</p>
<p>Francis Song, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan Clark, Hubert Soyer, Seb Jack W Rae, Arun Noury, Siqi Ahuja, Liu, arXiv:1909.12238Dhruva Tirumala, et al. V-mpo: On-policy maximum a posteriori policy optimization for discrete and continuous control. 2019arXiv preprint</p>
<p>Representation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, 20181807arXiv e-prints</p>
<p>Unsupervised feature extraction by time-contrastive learning and nonlinear ica. Aapo Hyvarinen, Hiroshi Morioka, Advances in Neural Information Processing Systems. 292016</p>
<p>Chris Dyer, arXiv:1410.8251Notes on noise contrastive estimation and negative sampling. 2014arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International Conference on Machine Learning. PMLR2021</p>
<p>Scaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, International Conference on Machine Learning. PMLR2021</p>
<p>Creating multimodal interactive agents with imitation and self-supervised learning. arXiv:2112.037632021arXiv preprintDeepMind Interactive Agents Team</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Taku Kudo, John Richardson, arXiv:1808.062262018arXiv preprint</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.114462021arXiv preprint</p>
<p>Advances in neural information processing systems. Aaron Van Den, Oriol Oord, Vinyals, 201730Neural discrete representation learning</p>
<p>Improved adam optimizer for deep neural networks. Zijun Zhang, 26th International Symposium on Quality of Service (IWQoS). IEEE2018. 2018</p>
<p>Cloud tensor processing units (tpus). Google Cloud, 2022-05-192022</p>
<p>Self-supervised multimodal versatile networks. Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelović, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, Andrew Zisserman, Advances in Neural Information Processing Systems. 202033</p>
<p>Cider: Consensus-based image description evaluation. Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems. Ali Razavi, 201932Aaron Van den Oord, and Oriol Vinyals</p>
<p>Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, arXiv:1706.06551Grounded language learning in a simulated 3d world. 2017arXiv preprint</p>
<p>Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, Yoshua Bengio, Babyai, arXiv:1810.08272A platform to study the sample efficiency of grounded language learning. 2018arXiv preprint</p>
<p>Learning latent plans from play. Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet, Conference on robot learning. PMLR2020</p>
<p>Human instruction-following with deep reinforcement learning via transfer-learning from text. Felix Hill, Sona Mokra, Nathaniel Wong, Tim Harley, arXiv:2005.093822020arXiv preprint</p>
<p>Zero experience required: Plug &amp; play modular transfer learning for semantic visual navigation. Ziad Al-Halah, Kristen Santhosh K Ramakrishnan, Grauman, arXiv:2202.024402022arXiv preprint</p>
<p>Learning languageconditioned robot behavior from offline data and crowd-sourced annotation. Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, Conference on Robot Learning. PMLR2022</p>
<p>Language conditioned imitation learning over unstructured data. Corey Lynch, Pierre Sermanet, arXiv:2005.076482020arXiv preprint</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn, Conference on Robot Learning. PMLR2022</p>
<p>Combined scaling for zero-shot transfer learning. Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, Quoc V Le, arXiv:2111.100502021arXiv preprint</p>
<p>A benchmark for systematic generalization in grounded language understanding. Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, Brenden M Lake, Advances in neural information processing systems. 202033</p>
<p>One-shot visual imitation learning via meta-learning. Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, Sergey Levine, Conference on robot learning. PMLR2017</p>
<p>Advances in neural information processing systems. Yan Duan, Marcin Andrychowicz, Bradly Stadie, Jonathan Openai, Jonas Ho, Ilya Schneider, Pieter Sutskever, Wojciech Abbeel, Zaremba, 201730One-shot imitation learning</p>
<p>Allan Zhou, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, Chelsea Finn, arXiv:1906.03352Watch, try, learn: Meta-learning from demonstrations and reward. 2019arXiv preprint</p>
<p>Hierarchical few-shot imitation with skill transition models. Kourosh Hakhamaneshi, Ruihan Zhao, Albert Zhan, Pieter Abbeel, Michael Laskin, arXiv:2107.089812021arXiv preprint</p>
<p>Language as a latent variable: Discrete generative models for sentence compression. Yishu Miao, Phil Blunsom, arXiv:1609.073172016arXiv preprint</p>
<p>Countering language drift via visual grounding. Jason Lee, Kyunghyun Cho, Douwe Kiela, arXiv:1909.044992019arXiv preprint</p>
<p>Multi-agent communication meets natural language: Synergies between functional and structural language learning. Angeliki Lazaridou, Anna Potapenko, Olivier Tieleman, arXiv:2005.070642020arXiv preprint</p>
<p>A semi-supervised framework for image captioning. Wenhu Chen, Aurelien Lucchi, Thomas Hofmann, arXiv:1611.053212016arXiv preprint</p>
<p>Semi-supervised image captioning via reconstruction. Bicheng Xu, Weirui Kong, Jiaxuan Chen, Proceedings of the Intern. Conf. on Comp. Vis.(ICCV). the Intern. Conf. on Comp. Vis.(ICCV)2017</p>
<p>Image captioning with very scarce supervised data: Adversarial semi-supervised learning approach. Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, In So Kweon, arXiv:1909.022012019arXiv preprint</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. 202033</p>
<p>Deep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, International Conference on Machine Learning. PMLR2015</p>
<p>Improved techniques for training score-based generative models. Yang Song, Stefano Ermon, Advances in neural information processing systems. 202033</p>
<p>Zero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, International Conference on Machine Learning. PMLR2021</p>
<p>Hierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.061252022arXiv preprint</p>
<p>Glide: Towards photorealistic image generation and editing with text-guided diffusion models. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, Mark Chen, arXiv:2112.107412021arXiv preprint</p>
<p>Tiago Ramalho, Tomáš Kočiskỳ, Frederic Besse, Gábor Eslami, Fabio Melis, Phil Viola, Karl Moritz Blunsom, Hermann, arXiv:1807.01670Encoding spatial relations from natural language. 2018arXiv preprint</p>
<p>Generative adversarial text to image synthesis. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee, International conference on machine learning. PMLR2016</p>
<p>Attngan: Fine-grained text to image generation with attentional generative adversarial networks. Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis. Minfeng Zhu, Pingbo Pan, Wei Chen, Yi Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Cross-modal contrastive learning for text-to-image generation. Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, Yinfei Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Improving text-to-image synthesis using contrastive learning. Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, Shihao Ji, arXiv:2107.024232021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
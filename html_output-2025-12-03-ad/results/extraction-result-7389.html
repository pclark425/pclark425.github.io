<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7389 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7389</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7389</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-272424210</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.03046v1.pdf" target="_blank">Oddballness: universal anomaly detection with language models</a></p>
                <p><strong>Paper Abstract:</strong> We present a new method to detect anomalies in texts (in general: in sequences of any data), using language models, in a totally unsupervised manner. The method considers probabilities (likelihoods) generated by a language model, but instead of focusing on low-likelihood tokens, it considers a new metric introduced in this paper: oddballness. Oddballness measures how ``strange'' a given token is according to the language model. We demonstrate in grammatical error detection tasks (a specific case of text anomaly detection) that oddballness is better than just considering low-likelihood events, if a totally unsupervised setup is assumed.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7389.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7389.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oddballness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oddballness (probability-complement anomaly score)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A score defined in this paper that measures how 'strange' a token/event is within a probability distribution produced by a language model; computed as the sum of positive differences between other event probabilities and the event's probability (equivalently 1 − 'probability of a probability'). Used as an unsupervised anomaly detector on token-level predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small, GPT2-XL, Yi-6b, Mistral 7b, RoBERTa Base, RoBERTa Large (also min/max ensemble of GPT2-XL & RoBERTa Large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained language models used to produce token-level probability distributions; includes decoder-only GPT-style models (GPT2 variants, Yi-6b, Mistral 7b) and encoder-only masked-LM models (RoBERTa variants). No task-specific fine-tuning for oddballness detection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (GPT2-small, GPT2-XL, Yi-6b (6B), Mistral 7b, RoBERTa-base/large — sizes as per original publications)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Score-based: compute oddballness from token probability distributions returned by a pretrained LM and threshold the oddballness to mark anomalous tokens (threshold tuned on development set).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text token sequences (grammatical error detection / token-level anomaly detection in sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>FCE dataset (Yannakoudakis et al. 2011) for English GED; MultiGED-2023 shared task datasets (multilingual: Czech, German, English-FCE, English-REALEC, Italian, Swedish).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F0.5 (development and test F0.5 scores; thresholds chosen to maximize F0.5 on dev set).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FCE (test F0.5): GPT2-small oddballness 39.19; GPT2-XL oddballness 40.52; Yi-6b oddballness 39.83; Mistral 7b oddballness 38.00; RoBERTa Base oddballness 34.86; RoBERTa Large oddballness 35.78; ensemble max(GPT2-XL, RoBERTa Large) oddballness test F0.5 = 43.15 (reported as best). MultiGED-2023 with Mistral 7b (Table 2, test F0.5, oddballness): Czech 46.61, German 36.68, English-FCE 35.86, English-REALEC 32.09, Italian 29.75, Swedish 38.93. With an additional prompt prepended (Table 3, oddballness test F0.5): Czech 47.75, German 39.26, English-FCE 36.37, English-REALEC 32.35, Italian 32.62, Swedish 41.99.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against: probability-thresholding (mark tokens with low probability), top-K prediction method (token considered anomalous if true token not in top-K predictions), and supervised GED models. Oddballness consistently outperformed simple probability-thresholding across all reported LM experiments and outperformed top-K in multilingual GED experiments; however supervised, task-specific models (e.g., BiLSTM by Rei & Yannakoudakis, BERT/ELECTRA variants reported in literature) achieved substantially higher scores (examples: Rei & Yannakoudakis Bi-LSTM dev F0.5 46.00 / test 41.10; other supervised systems report higher values).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot / unsupervised (no task-specific fine-tuning; only a single threshold hyperparameter tuned on the development set).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors report: (1) oddballness is not competitive with state-of-the-art fully supervised/fine-tuned GED systems; (2) learner-written texts (CEFR B level) can cause false positives (model flags non-fluent but correct words as errors); (3) some larger models (e.g., Mistral 7b) performed worse than smaller ones on these datasets—possibly due to domain mismatch or non-proficiency of writers; (4) oddballness detects spans likely to be erroneous but does not precisely label correction categories or token-level edits as supervised models do.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Oddballness: universal anomaly detection with language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7389.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7389.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-K (mask-prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-K token prediction anomaly detection (mask-and-check)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method (used in prior log-anomaly work and evaluated in this paper) that masks tokens, obtains the LM's top-K predicted tokens, and marks the actual token as anomalous if it is not among the top-K predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 7b (reported in multilingual GED experiments here); also referenced as used with LogBERT/LogGPT in related work</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mask-prediction / next-token prediction usage of pretrained LMs: for encoder-only models (masked LM) the masked token distribution is used; for decoder-only models a corresponding top-K next-token prediction procedure is used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>reported for Mistral 7b in experiments; K is a hyperparameter (reported values used per experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Discrete top-K membership test on LM predictions: if actual token not in top-K predicted tokens → anomalous.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>text token sequences (GED)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>MultiGED-2023 shared task datasets (evaluated with Mistral 7b); also referenced in related log-anomaly literature.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F0.5 (development and test).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MultiGED with Mistral 7b (Table 2, test F0.5): Czech TopK (K=30) test F0.5 = 38.55; German TopK (K=86) test F0.5 = 28.56; English-FCE TopK (K=200) test F0.5 = 31.60; English-REALEC TopK (K=380) test F0.5 = 28.10; Italian TopK (K=182) test F0.5 = 24.55; Swedish TopK (K=363) test F0.5 = 33.93. In these multilingual experiments TopK did not outperform the probability baseline in any language and was outperformed by oddballness.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly with probability-threshold method and oddballness; TopK underperformed relative to probability in these multilingual GED experiments and was generally worse than oddballness.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot unsupervised (K is tuned as a hyperparameter on dev; no fine-tuning of LM for anomaly detection).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors report TopK did not provide better results than the basic probability method in the multilingual GED experiments presented; implies TopK is not a reliable unsupervised detector in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Oddballness: universal anomaly detection with language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7389.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7389.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogBERT: Log anomaly detection via BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-only BERT-style model trained (semi-supervised) on log sequences for anomaly detection; during detection some tokens are masked and anomalies are detected if the true token is not among top-K predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogBERT: Log anomaly detection via BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogBERT (BERT-based encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer (BERT-style) trained on log data in semi-supervised fashion; used with a masked-token prediction and top-K rule for anomaly detection in logs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Mask-and-check (top-K): mask tokens and use masked-token prediction; if actual token not in top-K predicted tokens, label as anomalous.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Semi-supervised training on log sequences (per original LogBERT work; not further specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>log entry sequences (system logs).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Fine-tuned/semi-supervised (per original LogBERT description)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned as related work; no experimental details or failure cases provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Oddballness: universal anomaly detection with language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7389.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7389.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogGPT: Log anomaly detection via GPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only GPT-like model applied to log sequences for anomaly detection using a top-K prediction rule; model fine-tuned for anomaly detection in logs in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogGPT: Log anomaly detection via GPT.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogGPT (GPT-like decoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer (GPT-style) fine-tuned on log sequences; detection uses next-token/top-K mismatch as anomaly signal.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Top-K next-token prediction membership test on log sequences (prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on log sequences for anomaly detection (per original LogGPT paper).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>log entry sequences (system logs).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Fine-tuned (per original LogGPT description)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned as related work; paper notes the approach is similar to top-K used in experiments but does not provide further empirical details.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Oddballness: universal anomaly detection with language models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LogBERT: Log anomaly detection via BERT. <em>(Rating: 2)</em></li>
                <li>LogGPT: Log anomaly detection via GPT. <em>(Rating: 2)</em></li>
                <li>MultiGED-2023 shared task at NLP4CALL: Multilingual grammatical error detection. <em>(Rating: 2)</em></li>
                <li>A new dataset and method for automatically grading ESOL texts. <em>(Rating: 2)</em></li>
                <li>Context is key: Grammatical error detection with contextual word representations. <em>(Rating: 2)</em></li>
                <li>Grammatical error correction: A survey of the state of the art. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7389",
    "paper_id": "paper-272424210",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "Oddballness",
            "name_full": "Oddballness (probability-complement anomaly score)",
            "brief_description": "A score defined in this paper that measures how 'strange' a token/event is within a probability distribution produced by a language model; computed as the sum of positive differences between other event probabilities and the event's probability (equivalently 1 − 'probability of a probability'). Used as an unsupervised anomaly detector on token-level predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small, GPT2-XL, Yi-6b, Mistral 7b, RoBERTa Base, RoBERTa Large (also min/max ensemble of GPT2-XL & RoBERTa Large)",
            "model_description": "Pretrained language models used to produce token-level probability distributions; includes decoder-only GPT-style models (GPT2 variants, Yi-6b, Mistral 7b) and encoder-only masked-LM models (RoBERTa variants). No task-specific fine-tuning for oddballness detection.",
            "model_size": "various (GPT2-small, GPT2-XL, Yi-6b (6B), Mistral 7b, RoBERTa-base/large — sizes as per original publications)",
            "anomaly_detection_approach": "Score-based: compute oddballness from token probability distributions returned by a pretrained LM and threshold the oddballness to mark anomalous tokens (threshold tuned on development set).",
            "prompt_template": null,
            "training_data": null,
            "data_type": "text token sequences (grammatical error detection / token-level anomaly detection in sentences)",
            "dataset_name": "FCE dataset (Yannakoudakis et al. 2011) for English GED; MultiGED-2023 shared task datasets (multilingual: Czech, German, English-FCE, English-REALEC, Italian, Swedish).",
            "evaluation_metric": "F0.5 (development and test F0.5 scores; thresholds chosen to maximize F0.5 on dev set).",
            "performance": "FCE (test F0.5): GPT2-small oddballness 39.19; GPT2-XL oddballness 40.52; Yi-6b oddballness 39.83; Mistral 7b oddballness 38.00; RoBERTa Base oddballness 34.86; RoBERTa Large oddballness 35.78; ensemble max(GPT2-XL, RoBERTa Large) oddballness test F0.5 = 43.15 (reported as best). MultiGED-2023 with Mistral 7b (Table 2, test F0.5, oddballness): Czech 46.61, German 36.68, English-FCE 35.86, English-REALEC 32.09, Italian 29.75, Swedish 38.93. With an additional prompt prepended (Table 3, oddballness test F0.5): Czech 47.75, German 39.26, English-FCE 36.37, English-REALEC 32.35, Italian 32.62, Swedish 41.99.",
            "baseline_comparison": "Compared against: probability-thresholding (mark tokens with low probability), top-K prediction method (token considered anomalous if true token not in top-K predictions), and supervised GED models. Oddballness consistently outperformed simple probability-thresholding across all reported LM experiments and outperformed top-K in multilingual GED experiments; however supervised, task-specific models (e.g., BiLSTM by Rei & Yannakoudakis, BERT/ELECTRA variants reported in literature) achieved substantially higher scores (examples: Rei & Yannakoudakis Bi-LSTM dev F0.5 46.00 / test 41.10; other supervised systems report higher values).",
            "zero_shot_or_few_shot": "Zero-shot / unsupervised (no task-specific fine-tuning; only a single threshold hyperparameter tuned on the development set).",
            "limitations_or_failure_cases": "Authors report: (1) oddballness is not competitive with state-of-the-art fully supervised/fine-tuned GED systems; (2) learner-written texts (CEFR B level) can cause false positives (model flags non-fluent but correct words as errors); (3) some larger models (e.g., Mistral 7b) performed worse than smaller ones on these datasets—possibly due to domain mismatch or non-proficiency of writers; (4) oddballness detects spans likely to be erroneous but does not precisely label correction categories or token-level edits as supervised models do.",
            "computational_cost": null,
            "uuid": "e7389.0",
            "source_info": {
                "paper_title": "Oddballness: universal anomaly detection with language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Top-K (mask-prediction)",
            "name_full": "Top-K token prediction anomaly detection (mask-and-check)",
            "brief_description": "A method (used in prior log-anomaly work and evaluated in this paper) that masks tokens, obtains the LM's top-K predicted tokens, and marks the actual token as anomalous if it is not among the top-K predictions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral 7b (reported in multilingual GED experiments here); also referenced as used with LogBERT/LogGPT in related work",
            "model_description": "Mask-prediction / next-token prediction usage of pretrained LMs: for encoder-only models (masked LM) the masked token distribution is used; for decoder-only models a corresponding top-K next-token prediction procedure is used.",
            "model_size": "reported for Mistral 7b in experiments; K is a hyperparameter (reported values used per experiment).",
            "anomaly_detection_approach": "Discrete top-K membership test on LM predictions: if actual token not in top-K predicted tokens → anomalous.",
            "prompt_template": null,
            "training_data": null,
            "data_type": "text token sequences (GED)",
            "dataset_name": "MultiGED-2023 shared task datasets (evaluated with Mistral 7b); also referenced in related log-anomaly literature.",
            "evaluation_metric": "F0.5 (development and test).",
            "performance": "MultiGED with Mistral 7b (Table 2, test F0.5): Czech TopK (K=30) test F0.5 = 38.55; German TopK (K=86) test F0.5 = 28.56; English-FCE TopK (K=200) test F0.5 = 31.60; English-REALEC TopK (K=380) test F0.5 = 28.10; Italian TopK (K=182) test F0.5 = 24.55; Swedish TopK (K=363) test F0.5 = 33.93. In these multilingual experiments TopK did not outperform the probability baseline in any language and was outperformed by oddballness.",
            "baseline_comparison": "Compared directly with probability-threshold method and oddballness; TopK underperformed relative to probability in these multilingual GED experiments and was generally worse than oddballness.",
            "zero_shot_or_few_shot": "Zero-shot unsupervised (K is tuned as a hyperparameter on dev; no fine-tuning of LM for anomaly detection).",
            "limitations_or_failure_cases": "Authors report TopK did not provide better results than the basic probability method in the multilingual GED experiments presented; implies TopK is not a reliable unsupervised detector in this setting.",
            "computational_cost": null,
            "uuid": "e7389.1",
            "source_info": {
                "paper_title": "Oddballness: universal anomaly detection with language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LogBERT",
            "name_full": "LogBERT: Log anomaly detection via BERT",
            "brief_description": "An encoder-only BERT-style model trained (semi-supervised) on log sequences for anomaly detection; during detection some tokens are masked and anomalies are detected if the true token is not among top-K predictions.",
            "citation_title": "LogBERT: Log anomaly detection via BERT.",
            "mention_or_use": "mention",
            "model_name": "LogBERT (BERT-based encoder-only)",
            "model_description": "Encoder-only Transformer (BERT-style) trained on log data in semi-supervised fashion; used with a masked-token prediction and top-K rule for anomaly detection in logs.",
            "model_size": null,
            "anomaly_detection_approach": "Mask-and-check (top-K): mask tokens and use masked-token prediction; if actual token not in top-K predicted tokens, label as anomalous.",
            "prompt_template": null,
            "training_data": "Semi-supervised training on log sequences (per original LogBERT work; not further specified in this paper).",
            "data_type": "log entry sequences (system logs).",
            "dataset_name": null,
            "evaluation_metric": null,
            "performance": null,
            "baseline_comparison": null,
            "zero_shot_or_few_shot": "Fine-tuned/semi-supervised (per original LogBERT description)",
            "limitations_or_failure_cases": "Mentioned as related work; no experimental details or failure cases provided in this paper.",
            "computational_cost": null,
            "uuid": "e7389.2",
            "source_info": {
                "paper_title": "Oddballness: universal anomaly detection with language models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LogGPT",
            "name_full": "LogGPT: Log anomaly detection via GPT",
            "brief_description": "A decoder-only GPT-like model applied to log sequences for anomaly detection using a top-K prediction rule; model fine-tuned for anomaly detection in logs in prior work.",
            "citation_title": "LogGPT: Log anomaly detection via GPT.",
            "mention_or_use": "mention",
            "model_name": "LogGPT (GPT-like decoder-only)",
            "model_description": "Decoder-only Transformer (GPT-style) fine-tuned on log sequences; detection uses next-token/top-K mismatch as anomaly signal.",
            "model_size": null,
            "anomaly_detection_approach": "Top-K next-token prediction membership test on log sequences (prior work).",
            "prompt_template": null,
            "training_data": "Fine-tuned on log sequences for anomaly detection (per original LogGPT paper).",
            "data_type": "log entry sequences (system logs).",
            "dataset_name": null,
            "evaluation_metric": null,
            "performance": null,
            "baseline_comparison": null,
            "zero_shot_or_few_shot": "Fine-tuned (per original LogGPT description)",
            "limitations_or_failure_cases": "Mentioned as related work; paper notes the approach is similar to top-K used in experiments but does not provide further empirical details.",
            "computational_cost": null,
            "uuid": "e7389.3",
            "source_info": {
                "paper_title": "Oddballness: universal anomaly detection with language models",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LogBERT: Log anomaly detection via BERT.",
            "rating": 2,
            "sanitized_title": "logbert_log_anomaly_detection_via_bert"
        },
        {
            "paper_title": "LogGPT: Log anomaly detection via GPT.",
            "rating": 2,
            "sanitized_title": "loggpt_log_anomaly_detection_via_gpt"
        },
        {
            "paper_title": "MultiGED-2023 shared task at NLP4CALL: Multilingual grammatical error detection.",
            "rating": 2,
            "sanitized_title": "multiged2023_shared_task_at_nlp4call_multilingual_grammatical_error_detection"
        },
        {
            "paper_title": "A new dataset and method for automatically grading ESOL texts.",
            "rating": 2,
            "sanitized_title": "a_new_dataset_and_method_for_automatically_grading_esol_texts"
        },
        {
            "paper_title": "Context is key: Grammatical error detection with contextual word representations.",
            "rating": 2,
            "sanitized_title": "context_is_key_grammatical_error_detection_with_contextual_word_representations"
        },
        {
            "paper_title": "Grammatical error correction: A survey of the state of the art.",
            "rating": 1,
            "sanitized_title": "grammatical_error_correction_a_survey_of_the_state_of_the_art"
        }
    ],
    "cost": 0.013696749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Oddballness: universal anomaly detection with language models
September 6, 2024</p>
<p>Filip Graliński filipg@amu.edu.pl 
Ryszard Staruch ryszard.staruch@amu.edu.pl 
Krzysztof Jurkiewicz </p>
<p>Adam Mickiewicz University</p>
<p>Adam Mickiewicz University</p>
<p>Adam Mickiewicz University</p>
<p>Adam Mickiewicz University</p>
<p>Oddballness: universal anomaly detection with language models
September 6, 2024D91184CF586541C4C652F3A8FB0B4D23arXiv:2409.03046v1[cs.CL]
We present a new method to detect anomalies in texts (in general: in sequences of any data), using language models, in a totally unsupervised manner.The method considers probabilities (likelihoods) generated by a language model, but instead of focusing on low-likelihood tokens, it considers a new metric introduced in this paper: oddballness.Oddballness measures how "strange" a given token is according to the language model.We demonstrate in grammatical error detection tasks (a specific case of text anomaly detection) that oddballness is better than just considering low-likelihood events, if a totally unsupervised setup is assumed.</p>
<p>Introduction</p>
<p>Not all events with low probability are weird or oddball when they happen.For instance, the probability of a specific deal in the game of bridge is extremely low (p b = 1 5.36×10 28 for each deal).So every time you are dealt cards in bridge, something unfathomable happens?Of course not, actually an event of the very low probability p b must happen (with the probability 1!).</p>
<p>Another example, imagine two probability distributions:</p>
<ol>
<li>D 1 = {p 1 = 1 100 , p 2 = 99 100 }, 2. D 2 = {p 1 = 1 100 , p 2 = 1 100 , . . .p 100 = 1 100 }, Intuitively, p 1 is much more oddball in D 1 than p 1 in D 2 .So, how to measure oddballness?We already know that a low probability is not enough.Let us start with basic assumptions or axioms of oddballness.Then we will define oddballness and show their practical usage for anomaly detection when applied to probability distributions generated by language models.</li>
</ol>
<p>Let us assume a discrete probability distribution D = (Ω, Pr), where Ω could be finite or countably infinite.From now on, for simplicity, we define D just as a multiset of probabilities:
D = {p 1 , p 2 , p 3 , . . .} = {Pr(ω i ) : ω i ∈ Ω}.
We would like to define an oddballness measure1 for an outcome (elementary event) of a given probability p i within a distribution D:
ξ D (p i ), ξ D : D → [0, 1]
Let us define some common-sense axioms for oddballness:
(O0) ξ D (p i ) ∈ [0, 1] -let
us assume our measure is from 0 to 1, (O1) ξ D (0) = 1 -if an impossible event happens, that's pretty oddball!(O2) for any distribution ξ D (max{p i }) = 0 the most likely outcome is not oddball at all,
(O3) p i = p j → ξ D (p i ) = ξ D (p j ) -all
we know is a distribution, hence two outcomes of the same probability must have the same oddballness (within the same distribution),
(O4) p i &lt; p j → ξ D (p i ) ≥ ξ D (p j )
, if some outcome is less likely than another outcome, it cannot be less oddball,
(O5) (continuity) for any distribution D = {p 1 , p 2 , p 3 , . . .}, the function f (x) = ξ Dx (x), where D x = {x, p 2 × 1−x 1−p 1 , . . . , p i × 1−x 1−p 1 , .
. .}, is continuous -if we change the probabilities a little bit, the oddballness should not change much.</p>
<p>Note that (O2) implies the following two facts:
(F1) p i &gt; 0.5 → ξ D (p i ) = 0,
what is more likely than 50% is not oddball at all, (F2) for any distribution D = {p 1 = 1 N , . . ., p N = 1 N }, ξ D (p i ) = 0 -like in the bridge example.</p>
<p>Oddballness measure</p>
<p>Let us a define a measure that fulfils (O0)-(O5).First, let us define an auxiliary function:</p>
<p>x + = max(0, x) (In other words, this is the ReLU activation function.)Now let us assume a probability distribution D = {p 1 , p 2 , p 3 , . ..}.Let us define the following oddballness measure:
ξ D (p i ) = j g((p j − p i ) + ) j g(p j )
, where g is any monotonic and continuous function for which g(0) = 0 and g(1) = 1.This measure satisfies the axioms (O0)-(O5).</p>
<p>From now on, we assume the identity function g(x) = x (though, for instance x 2 or x 3 can be used as well); the oddballness measure simplifies to:
ξ D (p i ) = j (p j − p i ) + .
Let us check this measure for our distributions D 1 and D 2 given as examples:
ξ D (p 2 ) π D (p 2 ) p 1 = 0.05 p 2 = 0.25 p 3 = 0.7
Figure 1: Illustration of oddballness ξ D and "probability of probability" (π D ) for event ω 2 of probablity p 2 = 0.25 for D 3 = {p 1 = 0.7, p 2 = 0.25, p 3 = 0.05}
• ξ D 1 (p 1 ) = 0.98, • ξ D 1 (p 2 ) = 0, • ξ D 2 (p i ) = 0, Consider another example: D 3 = {p 1 = 0.7, p 2 = 0.25, p 3 = 0.05}, then: ξ D 3 (p 1 ) = 0, ξ D 3 (p 2 ) = (0.7 − 0.25) + + (0.25 − 0.25) + + (0.05 − 0.25) + = 0.45, ξ D 3 (p 3 ) = 0.85.</p>
<p>Oddballness as a complement of probability of probability</p>
<p>Interestingly, oddballness can be interpreted as the complement of the probability of a probability.By probability of a probability p i with respect to distribution D, or π D (p i ), we mean the probability that an event of probability p i (not necessarily ω i ) happens, with two extra assumptions:</p>
<p>• all probabilities smaller than p i are also summed up,</p>
<p>• for each event ω j with probability p j &gt; p i , we assume that it contains a "subevent" of probability p i , hence for each such event we sum p i in.</p>
<p>It can be shown that
π D (p i ) = 1 − ξ D (p i ).
Intuitively, it makes sense: An event is oddball if the probability of any event happening with similar probability is low.See Figure 1 for an illustration of the relation between oddballness and probability of probability.</p>
<p>What's the practical use?</p>
<p>The oddballness measure can be used to detect anomalies or errors, e.g. in a text, assuming that we have a good language model.The language model will give a probability distribution for any word in a text, some words will be given higher probability (likelihood), some lower.We could mark words with low probability as suspicious, but sometimes a low-probability event must occur.For instance, the distribution for the gap in the sentence:</p>
<p>I was born in . . ., a small village should be (for a good language model2 ) composed of a large number of names, each with a rather low probability.Hence, like in the bridge example, we should be not surprised to see a low-probability event.On the other hand, in the sentence: I was born in New . . .City any word other than York is pretty unlikely (and oddball).Therefore, rather than probability, the oddballness should be used -words with oddballness exceeded some threshold should be marked as suspicious, they are potential mistakes or anomalies to be checked by humans.This way, we could devise a grammar checking/proofreading system that is not trained or fine-tuned in a supervised manner for the specific task of error detection.</p>
<p>The notion of oddballness might not be that useful in the world before good language models, when usually only static discrete distributions were assumed.Language models, even for the same text, can generate vastly different types of probability distributions for each position:</p>
<p>• sometimes the model is almost certain and almost all probability will be assigned to one token,</p>
<p>• sometimes the model will predict a group of possible tokens plus a long tail of less likely tokens,</p>
<p>• and sometimes the model is uncertain and the entropy is high.</p>
<p>In this paper, we focus on applying oddballness to grammatical error detection (see Section 6).Some related (but not the same) ideas were, however, proposed in the field of log anomaly detection, as log sequences can be viewed as a modality similar to natural language.LogBERT by Guo et al. [2021] was trained on, in a semi-supervised way, on log sequences.During anomaly detection some tokens are masked and the probability distribution is obtained from LogBERT for each of them.If the probability of the actual token is not one of the K highest-likelihood tokens (K is a hyperparameter), the token is considered anomalous (we will refer to this method as topK later).LogGPT by Han et al. [2023] is a similar idea, but applied to an decoder-only GPT-like architecture, rather than an encoder-only Transformer, but still the same approach of considering topK prediction is taken for the anomaly detection itself, though the model is also fine-tuned specifically for anomaly detection.</p>
<p>In general, there is a vast body of literature on anomaly or outlier detection (see, for instance: Schölkopf et al. [2001], Breunig et al. [2000], Liu et al. [2008]).Oddballness is different, as it considers only probabilities from a language model (or any other statistical model) rather than any intrinsic feature of events in question.</p>
<p>Experiments with error detection</p>
<p>Table 1 presents the results on the FCE dataset Yannakoudakis et al. [2011].In each case, using the oddballness value as the threshold gives better results than using the probability value.All thresholds were adjusted to maximize the F0.5 score on the development set.The maximum oddballness value from the GPT2-XL and RoBERTa Large Liu [2019] models produced the best F0.5 score on the test set.The result is slightly better than the BiLSTM model by Rei and Yannakoudakis [2016], which was trained specifically to detect errors in texts, while GPT2-XL and RoBERTa Large are models which were trained, in a self-supervised manner, on the masked token prediction task.Although results based on the oddballness value are not competitive with state-of-the-art solutions, it should be noted that the oddballness technique does not involve any task-specific fine-tuning, except for single-hyperparameter tuning.Also, the texts were written by CEFR B level students, indicating that they may not be fully proficient in the language.This could cause the language model to flag not fluent words as incorrect and thus predict correct words as erroneous.This may also explain why the smaller GPT2-small model outperforms the much larger Mistral 7b model.This study demonstrates that the oddballness measure can yield superior results compared to using probability values for anomaly detection.We also tested the Mistral 7b model for multilingual GED datasets used in MultiGED-2023 Shared Task Volodina et al. [2023] using the same approach as in experiments for the FCE dataset.The results in Table 2 show that for all languages the oddballness method outperforms the probability method.We also tested adding the following prompt before each sentence: "An example of a grammatically correct text in any language that may be out of context: <example>" to make probability distribution more smooth.The results in Table 3 show that this trick helps in almost all experiments, but the improvements for the oddballness method are greater compared to the probability method.Looking at the thresholds we can also indicate that thresholds for the oddballness value are more universal compared to the probability thresholds.We also tested the top-K approach.For multilingual GED task it does not provide better results than probability method in any language.The best solutions for each dataset in the shared task are better compared to oddballness value results, but again those solutions are trained to predict incorrect tokens, whereas the oddballness method approach focus more on predicting spans in texts that are most likely errorneous without precisely labeling all incorrect tokens.</p>
<p>Conclusions</p>
<p>We have showed that using a new metric for anomalous events, oddballness, is better than just considering low-likelihood tokens, at least for grammatical error detection tasks.The method based on oddballness yields worse results than state-of-the-art models heavily fine-tuned for the task (Bryant et al. [2023]), but its great advantage is that it can be used for any language model, without any fine-tuning.This technique can be applied potentially to anomaly detection in sequences of any type of data, assuming that a "language" model was pre-trained.</p>
<p>Table 1 :
1
Results for the Grammatical Errror Detection FCE Dataset.Thresholds tuned with the development set.
ModelMethodThreshold Dev F0.5 Test F0.5 SubmissionUnsupervised methodsGPT2-smallProbability 0.000235.0037.74LinkGPT2-smallOddballness 0.8437.2739.19LinkGPT2-XLProbabilisty 0.000136.0038.86LinkGPT2-XLOddballness 0.8538.1740.52LinkYi-6bProbability 0.000534.3837.35LinkYi-6bOddballness 0.8536.7739.83LinkMistral 7bProbability 0.000333.6836.86LinkMistral 7bOddballness 0.8935.0438.00LinkRoBERTa BaseProbability 0.00532.6333.62LinkRoBERTa BaseOddballness 0.9133.0834.86LinkRoBERTa LargeProbability 0.01432.7433.39LinkRoBERTa LargeOddballness 0.8434.3335.78Linkmin(GPT2-XL, RoBERTa Large)Probability 0.000136.8839.31Linkmax(GPT2-XL, RoBERTa Large)Oddballness 0.8940.3243.15LinkSupervised methodsRei and Yannakoudakis [2016] Bi-LSTM-46.0041.10-Bell et al. [2019]BERT-base --57.28-Kaneko and Komachi [2019]MHMLA-61.65-Yuan et al. [2021]ELECTRA --72.93-</p>
<p>Table 2 :
2
Results for the Mistral 7b model on MultiGED-2023 shared task dataset
LanguageMethodThreshold Dev F0.5 Test F0.5CzechTopK3041.1938.55CzechProbability 0.00244.3441.90CzechOddballness 0.8449.1646.61GermanTopK8630.5528.56GermanProbability 0.00132.5331.36GermanOddballness 0.8937.6936.68English -FCETopK20029.1431.60English -FCEProbability 0.0000932.5134.42English -FCEOddballness 0.9035.0735.86English -REALEC TopK38027.6228.10English -REALEC Probability 0.0000631.0831.05English -REALEC Oddballness 0.9532.8932.09ItalianTopK1822.7624.55ItalianProbability 0.00323.6626.00ItalianOddballness 0.827.3129.75SwedishTopK3635.1433.93SwedishProbability 0.00337.3436.11SwedishOddballness 0.7940.2638.93</p>
<p>Table 3 :
3
Results for the Mistral 7b model on MultiGED-2023 shared task dataset with an additional prompt
LanguageMethodThreshold Dev F0.5 Test F0.5CzechTopK1841.2239.52CzechProbability 0.00244.2642.70CzechOddballness 0.8549.6847.75GermanTopK7231.7830.39GermanProbability 0.000834.4832.91GermanOddballness 0.8939.4439.26English -FCETopK14029.7631.72English -FCEProbability 0.000332.8734.91English -FCEOddballness 0.9035.9636.37English -REALEC TopK50027.7027.60English -REALEC Probability 0.0000830.4430.18English -REALEC Oddballness 0.9232.8132.35ItalianTopK7423.1724.55ItalianProbability 0.000825.2626.56ItalianOddballness 0.9231.6632.62SwedishTopK5036.9634.93SwedishProbability 0.00239.3438.26SwedishOddballness 0.8443.4541.99
Measure understood informally, not as defined in measure theory.
For this example, an encoder-only model trained on the masked language task should be assumed, for instance RoBERTaLiu [2019].</p>
<p>Context is key: Grammatical error detection with contextual word representations. S J Bell, H Yannakoudakis, M Rei, CoRR, abs/1906.065932019</p>
<p>LOF: identifying density-based local outliers. M M Breunig, H.-P Kriegel, R T Ng, J Sander, Proceedings of the 2000 ACM SIGMOD international conference on Management of data. the 2000 ACM SIGMOD international conference on Management of data2000</p>
<p>Grammatical error correction: A survey of the state of the art. C Bryant, Z Yuan, M R Qorib, H Cao, H T Ng, T Briscoe, 10.1162/coli_a_00478Computational Linguistics. 1530- 9312July 2023</p>
<p>LogBERT: Log anomaly detection via BERT. H Guo, S Yuan, X Wu, 2021 international joint conference on neural networks (IJCNN). IEEE2021</p>
<p>LogGPT: Log anomaly detection via GPT. X Han, S Yuan, M Trabelsi, 2023 IEEE International Conference on Big Data (BigData). IEEE2023</p>
<p>Multi-head multi-layer attention to deep language representations for grammatical error detection. M Kaneko, M Komachi, CoRR, abs/1904.073342019</p>
<p>Isolation forest. F T Liu, K M Ting, Z.-H Zhou, 2008 Eighth IEEE International Conference on Data Mining. IEEE2008</p>
<p>Y Liu, arXiv:1907.11692RoBERTa: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Compositional sequence labeling models for error detection in learner writing. M Rei, H Yannakoudakis, 10.18653/v1/P16-1112Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. K Erk, N A Smith, the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational LinguisticsAug. 20161</p>
<p>Estimating the support of a high-dimensional distribution. B Schölkopf, J C Platt, J Shawe-Taylor, A J Smola, R C Williamson, 10.1162/089976601750264965Neural Computation. 1372001</p>
<p>MultiGED-2023 shared task at NLP4CALL: Multilingual grammatical error detection. E Volodina, C Bryant, A Caines, O De Clercq, J.-C Frey, E Ershova, A Rosen, O Vinogradova, Proceedings of the 12th Workshop on NLP for Computer Assisted Language Learning. D Alfter, E Volodina, T François, A Jönsson, E Rennes, the 12th Workshop on NLP for Computer Assisted Language LearningTórshavn, Faroe IslandsLiU Electronic PressMay 2023</p>
<p>A new dataset and method for automatically grading ESOL texts. H Yannakoudakis, T Briscoe, B Medlock, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. D Lin, Y Matsumoto, R Mihalcea, the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesPortland, Oregon, USAAssociation for Computational LinguisticsJune 2011</p>
<p>Multi-class grammatical error detection for correction: A tale of two systems. Z Yuan, S Taslimipoor, C Davis, C Bryant, 10.18653/v1/2021.emnlp-main.687Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. M.-F Moens, X Huang, L Specia, S W -T, Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNov. 2021Online and Punta Cana</p>            </div>
        </div>

    </div>
</body>
</html>
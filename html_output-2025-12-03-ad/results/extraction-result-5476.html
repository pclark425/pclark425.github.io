<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5476 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5476</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5476</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-656a680c61d79c63367d1c88a0ab542b50ebe162</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/656a680c61d79c63367d1c88a0ab542b50ebe162" target="_blank">Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> The method of contrastive denoising with noisy chain-of-thought (CD-CoT) is proposed, which enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with only one clean rationale, which can be the minimal requirement for denoising-purpose prompting.</p>
                <p><strong>Paper Abstract:</strong> This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We construct NoRa dataset that is tailored to evaluate the robustness of reasoning in the presence of noisy rationales. Our findings on NoRa dataset reveal a prevalent vulnerability to such noise among current LLMs, with existing robust methods like self-correction and self-consistency showing limited efficacy. Notably, compared to prompting with clean rationales, base LLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more drastically by 2.2%-40.4% with inaccurate thoughts. Addressing this challenge necessitates external supervision that should be accessible in practice. Here, we propose the method of contrastive denoising with noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with only one clean rationale, which can be the minimal requirement for denoising-purpose prompting. This method follows a principle of exploration and exploitation: (1) rephrasing and selecting rationales in the input space to achieve explicit denoising and (2) exploring diverse reasoning paths and voting on answers in the output space. Empirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy over the base model and shows significantly stronger denoising capabilities than baseline methods. The source code is publicly available at: https://github.com/tmlr-group/NoisyRationales.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5476.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5476.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ISC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intrinsic Self-Correction (ISC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of self-correction methods where an LLM attempts to iteratively correct its own answers using only its internal capabilities (no external feedback); applied as a baseline in this paper and shown to often degrade reasoning quality on noisy rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models cannot self-correct reasoning yet.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-0613 (Base) (also evaluated across Gemini-Pro, LLaMA2-70B, Mixtral-8x7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-turbo-0613 used as the primary base LLM in experiments; other LLMs (Gemini-Pro, Llama2-70B, Mixtral-8x7B) were also evaluated for generality.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Intrinsic Self-Correction (ISC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model generates an initial response then uses its own internal mechanisms to find and correct mistakes (no external supervision); in the paper ISC represents methods that attempt internal iterative correction.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NoRa (Math Base-9, Math Base-11, Symbolic Equal/Longer, Commonsense)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NoRa is the dataset introduced in this paper that injects noisy rationales (irrelevant or inaccurate thoughts) into chain-of-thought (CoT) demonstrations across mathematics, symbolic, and commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>As reported in Table 3 (3-shot prompting): Example - Math Base-9 with ISC: clean 24.3%; irrelevant avg 15.0%; inaccurate avg 14.8%. (See Table 3 rows for ISC per-task numbers.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base (no ISC) for the same settings: Math Base-9 clean 46.4%; irrelevant avg 32.1%; inaccurate avg 13.1%. (See Table 3 base rows.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>None in this paper: ISC typically decreased performance relative to the base (non-self-corrected) model on many tasks (e.g., Math Base-9 clean: 46.4% -> 24.3% with ISC), demonstrating that intrinsic self-correction without external feedback did not improve and often harmed answer quality under noisy rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitation: ISC (self-correction without external feedback) fails reliably in reasoning tasks with noisy rationales; the paper cites prior work and its own experiments showing ISC often miscorrects and reduces accuracy (Observation 4.1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5476.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5476.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Polish (SP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative prompt-refinement/self-improvement approach that instructs the model to refine the problem statement or prompt (e.g., remove irrelevant details) before solving; evaluated here as a baseline for noisy rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-polish: Enhance reasoning in large language models via problem refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-0613 (Base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base LLM used for evaluating SP in the NoRa benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Polish (SP)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model is prompted to iteratively refine or rephrase the test problem (remove irrelevant information, clarify structure) before generating the final solution; treated as a self-correction baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NoRa (Math, Symbolic, Commonsense)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NoRa dataset with noisy chain-of-thought demonstrations; SP tested in 3-shot CoT settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Representative numbers from Table 3: Math Base-9 w/ SP: clean 26.2%; irrelevant avg 24.3%; inaccurate avg 17.6%. Symbolic Equal w/ SP: clean 23.2%; irrelevant avg 22.8%; inaccurate avg 23.2%. Commonsense w/ SP: clean 47.9%; irrelevant avg 47.7%; inaccurate avg 47.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base (no SP) numbers (Table 3): Math Base-9 clean 46.4% (SP lower); Symbolic Equal clean 32.7% (SP lower); Commonsense clean 45.7% (SP slightly higher on some noisy settings).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mixed: SP slightly improves some commonsense settings but generally does not recover performance on hard noisy mathematical tasks; paper notes SP only slightly improves commonsense and performs unsatisfactorily on many tasks (Observation 4.1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to robustly denoise inaccurate rationales in hard tasks; can underperform the base model in many settings and is insufficient without external supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5476.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5476.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative refinement method where an LLM produces an initial answer and then generates self-feedback across multiple dimensions to refine that answer repeatedly (generate-then-reflect paradigm); cited in related work but not directly evaluated in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Model generates an initial solution, then generates feedback/evaluation of its own output, and uses that feedback to iteratively revise the answer until stopping criteria; a generate-then-reflect pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various reasoning tasks (cited work applies to multiple tasks); in this paper referenced as related work for self-refinement approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Not evaluated on NoRa in this paper; referenced as an example of iterative self-improvement methods in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned in related work as an approach that can improve outputs in some settings (citation [52]); this paper does not provide empirical evaluation of Self-refine on NoRa.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes general concerns from the literature that self-feedback methods without external supervision may still fail to reliably correct reasoning errors (citing [29]); no direct NoRa results provided for Self-refine.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5476.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5476.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique where models produce provenance or checks of their outputs to verify correctness, used to improve few-shot extraction/clinical tasks; cited in related work but not used as an experimental baseline in NoRa.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-verification improves few-shot clinical information extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model provides supporting evidence or checks for its own outputs and uses these checks to identify and correct mistakes (a form of internal critique); discussed in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mentioned in context of few-shot clinical information extraction (related work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Not evaluated on NoRa in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Cited literature reports improvements in domain-specific tasks (reference [21]); this paper does not evaluate it on noisy rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated here; paper emphasizes that internal verification methods can be useful but general self-correction without external supervision can still fail (Observation 4.1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5476.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5476.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stochastic sampling and aggregation method that generates multiple chain-of-thought reasoning paths and aggregates (votes) the final answer to improve robustness; evaluated in this paper and often helps but does not perform explicit denoising of rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-0613 (Base); also evaluated with Gemini-Pro, LLaMA2-70B, Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base LLM and other models tested with self-consistency sampling and majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple diverse reasoning chains (via sampling/temperature perturbations) and aggregate answers (majority vote) to improve final-answer consistency; does not directly edit or denoise input rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NoRa (Math, Symbolic, Commonsense) with 3-shot CoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NoRa dataset with noisy rationales; SC evaluated by sampling multiple outputs per prompt and voting on answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Table 3 example numbers: Math Base-9 w/ SC: clean 61.5%; irrelevant avg 42.1%; inaccurate avg 18.5%. Symbolic Equal w/ SC: clean 35.3%; irrelevant avg 28.8%; inaccurate avg 30.0%. Commonsense w/ SC: clean 52.0%; irrelevant avg 45.3%; inaccurate avg 42.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base (no SC) numbers: Math Base-9 clean 46.4%; Symbolic Equal clean 32.7%; Commonsense clean 45.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper reports SC improves both clean and noisy reasoning on multiple tasks compared to base prompting (e.g., Math Base-9 clean 46.4% -> SC 61.5%), indicating aggregation of multiple sampled chains can boost accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SC does not perform explicit denoising of noisy rationales (Observation 4.2) and can be computationally expensive (sampling multiple chains); on some very hard symbolic tasks SC may still yield low absolute accuracy and cannot correct misleading input rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5476.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5476.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Denoise (SD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that masks parts of prompts and asks the model to reconstruct them (a denoising/self-reconstruction technique); evaluated as a baseline and shows limited denoising power on NoRa.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Certified robustness for large language models with self-denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-0613 (Base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base LLM used to run masking-and-reconstruction style self-denoising experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Denoising (SD)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Mask parts of the prompt and use the model to reconstruct / denoise the prompt, in an attempt to remove perturbations/noise before answering; treated as a self-consistency/self-denoising baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NoRa (Math, Symbolic, Commonsense)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NoRa dataset with noisy rationales; SD tested for its ability to reconstruct and thus denoise CoT demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Representative numbers (Table 3): Math Base-9 w/ SD: clean 47.9%; irrelevant avg 29.1%; inaccurate avg 16.8%. Commonsense w/ SD: clean 54.0%; irrelevant avg 57.8%; inaccurate avg 56.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base (no SD): Math Base-9 clean 46.4%; Commonsense clean 45.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mixed: SD slightly improves some clean and commonsense settings (e.g., Commonsense clean 45.7% -> SD 54.0%), but fails on harder tasks and sometimes collapses (e.g., symbolic longer task near 0%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SD can disrupt logical coherence of thought chains and fails to explicitly denoise noisy rationales for hard tasks (Observation 4.2); performance can degrade drastically for symbolic longer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5476.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5476.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmoothLLM (SM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smoothing-based robustness method that injects perturbations into prompts (randomized smoothing) to improve robustness; evaluated here and shows mixed results on noisy rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SmoothLLM: Defending large language models against jailbreaking attacks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-0613 (Base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base LLM evaluated with prompt perturbation / smoothing strategies to test robustness to noisy rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Prompt Smoothing / Perturbation (SM)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Apply input perturbations (random masks/noise/smoothing) and aggregate model outputs to improve robustness against adversarial/noisy prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NoRa (Math, Symbolic, Commonsense)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NoRa dataset with noisy rationales; SM used as a baseline representing randomized smoothing approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Representative numbers (Table 3): Math Base-9 w/ SM: clean 37.4%; irrelevant avg 23.1%; inaccurate avg 18.8%. Commonsense w/ SM: clean 53.3%; irrelevant avg 49.0%; inaccurate avg 48.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base (no SM): Math Base-9 clean 46.4%; Commonsense clean 45.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>SM improves some commonsense settings but underperforms on harder math/symbolic tasks compared to the base or other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Smoothing strategies may introduce random characters or disruptions in the reasoning chain (case examples in Table 12), which can break logic and fail to remove targeted noisy thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5476.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5476.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CD-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Denoising with Noisy Chain-of-Thought (CD-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's proposed method: uses one clean CoT demonstration to contrastively rephrase and select corrected versions of noisy rationales, then explores multiple rephrased contexts and aggregates answers by voting to denoise and improve final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-0613 (Base); also tested with Gemini-Pro, LLaMA2-70B, Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluated across the same LLMs used for baselines; primary experiments use GPT-3.5-turbo-0613.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>CD-CoT (contrastive rephrase + repeated reasoning + voting)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Four-step pipeline: (1) rephrase each noisy example N times by contrastive prompting with one clean demonstration; (2) select M rephrased candidates that preserve the original answer; (3) construct M contexts and run repeated reasoning (total budget D) to explore diverse outputs; (4) majority-vote over D answers to produce final answer. Balances input-space denoising (contrastive rephrasing/selection) and output-space exploration (repeated reasoning + voting).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>6</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NoRa (Math Base-9, Math Base-11, Symbolic Equal/Longer, Commonsense)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NoRa introduces noisy rationales (irrelevant or inaccurate thoughts) for in-context CoT demonstrations across mathematical, symbolic, and commonsense reasoning; CD-CoT is evaluated on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper summary: CD-CoT yields substantial gains over base under noisy settings â€” an average improvement of 17.8 percentage points in accuracy over the base model across NoRa (reported in text, Tab. 8). Example: on Math Base-9 CD-CoT achieves substantially higher accuracy (e.g., CD-CoT ~59.2% average vs base noisy averages much lower); on Math Base-9 under inaccurate rationales CD-CoT's decline versus clean is ~7.0% compared to much larger declines for other methods (textual claim). Specific table rows (Tab. 8, Tab. 11) report per-task values (e.g., CD-CoT on GPT-3.5 Math Base-9 shows 60.7% in certain configurations in Tab. 8/11).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base (no CD-CoT) numbers: e.g., GPT-3.5 base clean Math Base-9 46.4% and noisy/inaccurate averages much lower (see Table 3); CD-CoT improves noisy-setting accuracy by large margins versus base (average +17.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: average +17.8% accuracy vs base across datasets (Tab. 8); CD-CoT outperforms baselines (SCO, BT, CC) in most settings (text and Tab. 8/11). Qualitative: case studies (Table 12 and Appendix) show CD-CoT rephrases/rectifies noisy thoughts and preserves logical chain format.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires an additional clean CoT demonstration (external supervision) which the authors acknowledge as a practical requirement; relies on prompt-driven rephrasing and answer-matching selection so failure modes include failing to generate valid rephrasings or exceed token limits; computational cost increases due to repeated rephrasing and multiple reasoning runs (token counts and budgets reported in Tables 9 and 10). The paper also notes dependence on human-annotated clean rationales and suggests self-supervised variants as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet. <em>(Rating: 2)</em></li>
                <li>Self-polish: Enhance reasoning in large language models via problem refinement. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Certified robustness for large language models with self-denoising <em>(Rating: 2)</em></li>
                <li>Self-verification improves few-shot clinical information extraction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5476",
    "paper_id": "paper-656a680c61d79c63367d1c88a0ab542b50ebe162",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "ISC",
            "name_full": "Intrinsic Self-Correction (ISC)",
            "brief_description": "A class of self-correction methods where an LLM attempts to iteratively correct its own answers using only its internal capabilities (no external feedback); applied as a baseline in this paper and shown to often degrade reasoning quality on noisy rationales.",
            "citation_title": "Large language models cannot self-correct reasoning yet.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo-0613 (Base) (also evaluated across Gemini-Pro, LLaMA2-70B, Mixtral-8x7B)",
            "model_description": "GPT-3.5-turbo-0613 used as the primary base LLM in experiments; other LLMs (Gemini-Pro, Llama2-70B, Mixtral-8x7B) were also evaluated for generality.",
            "reflection_method_name": "Intrinsic Self-Correction (ISC)",
            "reflection_method_description": "The model generates an initial response then uses its own internal mechanisms to find and correct mistakes (no external supervision); in the paper ISC represents methods that attempt internal iterative correction.",
            "num_iterations": null,
            "task_name": "NoRa (Math Base-9, Math Base-11, Symbolic Equal/Longer, Commonsense)",
            "task_description": "NoRa is the dataset introduced in this paper that injects noisy rationales (irrelevant or inaccurate thoughts) into chain-of-thought (CoT) demonstrations across mathematics, symbolic, and commonsense reasoning.",
            "performance_with_reflection": "As reported in Table 3 (3-shot prompting): Example - Math Base-9 with ISC: clean 24.3%; irrelevant avg 15.0%; inaccurate avg 14.8%. (See Table 3 rows for ISC per-task numbers.)",
            "performance_without_reflection": "Base (no ISC) for the same settings: Math Base-9 clean 46.4%; irrelevant avg 32.1%; inaccurate avg 13.1%. (See Table 3 base rows.)",
            "has_performance_comparison": true,
            "evidence_of_improvement": "None in this paper: ISC typically decreased performance relative to the base (non-self-corrected) model on many tasks (e.g., Math Base-9 clean: 46.4% -&gt; 24.3% with ISC), demonstrating that intrinsic self-correction without external feedback did not improve and often harmed answer quality under noisy rationales.",
            "limitations_or_failure_cases": "Reported limitation: ISC (self-correction without external feedback) fails reliably in reasoning tasks with noisy rationales; the paper cites prior work and its own experiments showing ISC often miscorrects and reduces accuracy (Observation 4.1).",
            "uuid": "e5476.0",
            "source_info": {
                "paper_title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SP",
            "name_full": "Self-Polish (SP)",
            "brief_description": "An iterative prompt-refinement/self-improvement approach that instructs the model to refine the problem statement or prompt (e.g., remove irrelevant details) before solving; evaluated here as a baseline for noisy rationales.",
            "citation_title": "Self-polish: Enhance reasoning in large language models via problem refinement.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo-0613 (Base)",
            "model_description": "Same base LLM used for evaluating SP in the NoRa benchmarks.",
            "reflection_method_name": "Self-Polish (SP)",
            "reflection_method_description": "The model is prompted to iteratively refine or rephrase the test problem (remove irrelevant information, clarify structure) before generating the final solution; treated as a self-correction baseline.",
            "num_iterations": null,
            "task_name": "NoRa (Math, Symbolic, Commonsense)",
            "task_description": "NoRa dataset with noisy chain-of-thought demonstrations; SP tested in 3-shot CoT settings.",
            "performance_with_reflection": "Representative numbers from Table 3: Math Base-9 w/ SP: clean 26.2%; irrelevant avg 24.3%; inaccurate avg 17.6%. Symbolic Equal w/ SP: clean 23.2%; irrelevant avg 22.8%; inaccurate avg 23.2%. Commonsense w/ SP: clean 47.9%; irrelevant avg 47.7%; inaccurate avg 47.6%.",
            "performance_without_reflection": "Base (no SP) numbers (Table 3): Math Base-9 clean 46.4% (SP lower); Symbolic Equal clean 32.7% (SP lower); Commonsense clean 45.7% (SP slightly higher on some noisy settings).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Mixed: SP slightly improves some commonsense settings but generally does not recover performance on hard noisy mathematical tasks; paper notes SP only slightly improves commonsense and performs unsatisfactorily on many tasks (Observation 4.1).",
            "limitations_or_failure_cases": "Fails to robustly denoise inaccurate rationales in hard tasks; can underperform the base model in many settings and is insufficient without external supervision.",
            "uuid": "e5476.1",
            "source_info": {
                "paper_title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Self-refine",
            "name_full": "Self-Refine (iterative self-feedback)",
            "brief_description": "An iterative refinement method where an LLM produces an initial answer and then generates self-feedback across multiple dimensions to refine that answer repeatedly (generate-then-reflect paradigm); cited in related work but not directly evaluated in this paper's experiments.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Self-refine (iterative self-feedback)",
            "reflection_method_description": "Model generates an initial solution, then generates feedback/evaluation of its own output, and uses that feedback to iteratively revise the answer until stopping criteria; a generate-then-reflect pipeline.",
            "num_iterations": null,
            "task_name": "Various reasoning tasks (cited work applies to multiple tasks); in this paper referenced as related work for self-refinement approaches.",
            "task_description": "Not evaluated on NoRa in this paper; referenced as an example of iterative self-improvement methods in literature.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Mentioned in related work as an approach that can improve outputs in some settings (citation [52]); this paper does not provide empirical evaluation of Self-refine on NoRa.",
            "limitations_or_failure_cases": "Paper notes general concerns from the literature that self-feedback methods without external supervision may still fail to reliably correct reasoning errors (citing [29]); no direct NoRa results provided for Self-refine.",
            "uuid": "e5476.2",
            "source_info": {
                "paper_title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Self-verification",
            "name_full": "Self-Verification",
            "brief_description": "A technique where models produce provenance or checks of their outputs to verify correctness, used to improve few-shot extraction/clinical tasks; cited in related work but not used as an experimental baseline in NoRa.",
            "citation_title": "Self-verification improves few-shot clinical information extraction.",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Self-verification",
            "reflection_method_description": "The model provides supporting evidence or checks for its own outputs and uses these checks to identify and correct mistakes (a form of internal critique); discussed in related work.",
            "num_iterations": null,
            "task_name": "Mentioned in context of few-shot clinical information extraction (related work).",
            "task_description": "Not evaluated on NoRa in this paper.",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "evidence_of_improvement": "Cited literature reports improvements in domain-specific tasks (reference [21]); this paper does not evaluate it on noisy rationales.",
            "limitations_or_failure_cases": "Not evaluated here; paper emphasizes that internal verification methods can be useful but general self-correction without external supervision can still fail (Observation 4.1).",
            "uuid": "e5476.3",
            "source_info": {
                "paper_title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SC",
            "name_full": "Self-Consistency (SC)",
            "brief_description": "A stochastic sampling and aggregation method that generates multiple chain-of-thought reasoning paths and aggregates (votes) the final answer to improve robustness; evaluated in this paper and often helps but does not perform explicit denoising of rationales.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo-0613 (Base); also evaluated with Gemini-Pro, LLaMA2-70B, Mixtral-8x7B",
            "model_description": "Base LLM and other models tested with self-consistency sampling and majority voting.",
            "reflection_method_name": "Self-Consistency (SC)",
            "reflection_method_description": "Generate multiple diverse reasoning chains (via sampling/temperature perturbations) and aggregate answers (majority vote) to improve final-answer consistency; does not directly edit or denoise input rationales.",
            "num_iterations": 5,
            "task_name": "NoRa (Math, Symbolic, Commonsense) with 3-shot CoT prompting",
            "task_description": "NoRa dataset with noisy rationales; SC evaluated by sampling multiple outputs per prompt and voting on answers.",
            "performance_with_reflection": "Table 3 example numbers: Math Base-9 w/ SC: clean 61.5%; irrelevant avg 42.1%; inaccurate avg 18.5%. Symbolic Equal w/ SC: clean 35.3%; irrelevant avg 28.8%; inaccurate avg 30.0%. Commonsense w/ SC: clean 52.0%; irrelevant avg 45.3%; inaccurate avg 42.5%.",
            "performance_without_reflection": "Base (no SC) numbers: Math Base-9 clean 46.4%; Symbolic Equal clean 32.7%; Commonsense clean 45.7%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Paper reports SC improves both clean and noisy reasoning on multiple tasks compared to base prompting (e.g., Math Base-9 clean 46.4% -&gt; SC 61.5%), indicating aggregation of multiple sampled chains can boost accuracy.",
            "limitations_or_failure_cases": "SC does not perform explicit denoising of noisy rationales (Observation 4.2) and can be computationally expensive (sampling multiple chains); on some very hard symbolic tasks SC may still yield low absolute accuracy and cannot correct misleading input rationales.",
            "uuid": "e5476.4",
            "source_info": {
                "paper_title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SD",
            "name_full": "Self-Denoise (SD)",
            "brief_description": "A method that masks parts of prompts and asks the model to reconstruct them (a denoising/self-reconstruction technique); evaluated as a baseline and shows limited denoising power on NoRa.",
            "citation_title": "Certified robustness for large language models with self-denoising.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo-0613 (Base)",
            "model_description": "Base LLM used to run masking-and-reconstruction style self-denoising experiments.",
            "reflection_method_name": "Self-Denoising (SD)",
            "reflection_method_description": "Mask parts of the prompt and use the model to reconstruct / denoise the prompt, in an attempt to remove perturbations/noise before answering; treated as a self-consistency/self-denoising baseline.",
            "num_iterations": null,
            "task_name": "NoRa (Math, Symbolic, Commonsense)",
            "task_description": "NoRa dataset with noisy rationales; SD tested for its ability to reconstruct and thus denoise CoT demonstrations.",
            "performance_with_reflection": "Representative numbers (Table 3): Math Base-9 w/ SD: clean 47.9%; irrelevant avg 29.1%; inaccurate avg 16.8%. Commonsense w/ SD: clean 54.0%; irrelevant avg 57.8%; inaccurate avg 56.3%.",
            "performance_without_reflection": "Base (no SD): Math Base-9 clean 46.4%; Commonsense clean 45.7%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Mixed: SD slightly improves some clean and commonsense settings (e.g., Commonsense clean 45.7% -&gt; SD 54.0%), but fails on harder tasks and sometimes collapses (e.g., symbolic longer task near 0%).",
            "limitations_or_failure_cases": "SD can disrupt logical coherence of thought chains and fails to explicitly denoise noisy rationales for hard tasks (Observation 4.2); performance can degrade drastically for symbolic longer tasks.",
            "uuid": "e5476.5",
            "source_info": {
                "paper_title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SM",
            "name_full": "SmoothLLM (SM)",
            "brief_description": "A smoothing-based robustness method that injects perturbations into prompts (randomized smoothing) to improve robustness; evaluated here and shows mixed results on noisy rationales.",
            "citation_title": "SmoothLLM: Defending large language models against jailbreaking attacks.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo-0613 (Base)",
            "model_description": "Base LLM evaluated with prompt perturbation / smoothing strategies to test robustness to noisy rationales.",
            "reflection_method_name": "Prompt Smoothing / Perturbation (SM)",
            "reflection_method_description": "Apply input perturbations (random masks/noise/smoothing) and aggregate model outputs to improve robustness against adversarial/noisy prompts.",
            "num_iterations": null,
            "task_name": "NoRa (Math, Symbolic, Commonsense)",
            "task_description": "NoRa dataset with noisy rationales; SM used as a baseline representing randomized smoothing approaches.",
            "performance_with_reflection": "Representative numbers (Table 3): Math Base-9 w/ SM: clean 37.4%; irrelevant avg 23.1%; inaccurate avg 18.8%. Commonsense w/ SM: clean 53.3%; irrelevant avg 49.0%; inaccurate avg 48.7%.",
            "performance_without_reflection": "Base (no SM): Math Base-9 clean 46.4%; Commonsense clean 45.7%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "SM improves some commonsense settings but underperforms on harder math/symbolic tasks compared to the base or other baselines.",
            "limitations_or_failure_cases": "Smoothing strategies may introduce random characters or disruptions in the reasoning chain (case examples in Table 12), which can break logic and fail to remove targeted noisy thoughts.",
            "uuid": "e5476.6",
            "source_info": {
                "paper_title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CD-CoT",
            "name_full": "Contrastive Denoising with Noisy Chain-of-Thought (CD-CoT)",
            "brief_description": "This paper's proposed method: uses one clean CoT demonstration to contrastively rephrase and select corrected versions of noisy rationales, then explores multiple rephrased contexts and aggregates answers by voting to denoise and improve final answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo-0613 (Base); also tested with Gemini-Pro, LLaMA2-70B, Mixtral-8x7B",
            "model_description": "Evaluated across the same LLMs used for baselines; primary experiments use GPT-3.5-turbo-0613.",
            "reflection_method_name": "CD-CoT (contrastive rephrase + repeated reasoning + voting)",
            "reflection_method_description": "Four-step pipeline: (1) rephrase each noisy example N times by contrastive prompting with one clean demonstration; (2) select M rephrased candidates that preserve the original answer; (3) construct M contexts and run repeated reasoning (total budget D) to explore diverse outputs; (4) majority-vote over D answers to produce final answer. Balances input-space denoising (contrastive rephrasing/selection) and output-space exploration (repeated reasoning + voting).",
            "num_iterations": 6,
            "task_name": "NoRa (Math Base-9, Math Base-11, Symbolic Equal/Longer, Commonsense)",
            "task_description": "NoRa introduces noisy rationales (irrelevant or inaccurate thoughts) for in-context CoT demonstrations across mathematical, symbolic, and commonsense reasoning; CD-CoT is evaluated on these tasks.",
            "performance_with_reflection": "Paper summary: CD-CoT yields substantial gains over base under noisy settings â€” an average improvement of 17.8 percentage points in accuracy over the base model across NoRa (reported in text, Tab. 8). Example: on Math Base-9 CD-CoT achieves substantially higher accuracy (e.g., CD-CoT ~59.2% average vs base noisy averages much lower); on Math Base-9 under inaccurate rationales CD-CoT's decline versus clean is ~7.0% compared to much larger declines for other methods (textual claim). Specific table rows (Tab. 8, Tab. 11) report per-task values (e.g., CD-CoT on GPT-3.5 Math Base-9 shows 60.7% in certain configurations in Tab. 8/11).",
            "performance_without_reflection": "Base (no CD-CoT) numbers: e.g., GPT-3.5 base clean Math Base-9 46.4% and noisy/inaccurate averages much lower (see Table 3); CD-CoT improves noisy-setting accuracy by large margins versus base (average +17.8%).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: average +17.8% accuracy vs base across datasets (Tab. 8); CD-CoT outperforms baselines (SCO, BT, CC) in most settings (text and Tab. 8/11). Qualitative: case studies (Table 12 and Appendix) show CD-CoT rephrases/rectifies noisy thoughts and preserves logical chain format.",
            "limitations_or_failure_cases": "Requires an additional clean CoT demonstration (external supervision) which the authors acknowledge as a practical requirement; relies on prompt-driven rephrasing and answer-matching selection so failure modes include failing to generate valid rephrasings or exceed token limits; computational cost increases due to repeated rephrasing and multiple reasoning runs (token counts and budgets reported in Tables 9 and 10). The paper also notes dependence on human-annotated clean rationales and suggests self-supervised variants as future work.",
            "uuid": "e5476.7",
            "source_info": {
                "paper_title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet.",
            "rating": 2
        },
        {
            "paper_title": "Self-polish: Enhance reasoning in large language models via problem refinement.",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2
        },
        {
            "paper_title": "Certified robustness for large language models with self-denoising",
            "rating": 2
        },
        {
            "paper_title": "Self-verification improves few-shot clinical information extraction",
            "rating": 1
        }
    ],
    "cost": 0.02304075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?</h1>
<p>Zhanke Zhou ${ }^{1}$ Rong Tao ${ }^{1}$ Jianing Zhu ${ }^{1}$ Yiwen Luo ${ }^{2}$ Zengmao Wang ${ }^{2}$ Bo Han ${ }^{1 \dagger}$<br>${ }^{1}$ TMLR Group, Hong Kong Baptist University ${ }^{2}$ Wuhan University<br>{cszkzhou, csrongtao, csjnzhu, bhanml}@comp.hkbu.edu.hk<br>{luoyiwen, wangzengmao}@whu.edu.cn</p>
<h4>Abstract</h4>
<p>This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We construct NoRa dataset that is tailored to evaluate the robustness of reasoning in the presence of noisy rationales. Our findings on NoRa dataset reveal a prevalent vulnerability to such noise among current LLMs, with existing robust methods like self-correction and self-consistency showing limited efficacy. Notably, compared to prompting with clean rationales, GPT-3.5 drops by $1.4 \%-19.8 \%$ in accuracy with irrelevant thoughts and more drastically by $2.2 \%-40.4 \%$ with inaccurate thoughts. Addressing this challenge necessitates external supervision that should be accessible in practice. Here, we propose the method of contrastive denoising with noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with only one clean rationale, which can be the minimal requirement for denoising-purpose prompting. This method follows a principle of exploration and exploitation: (1) rephrasing and selecting rationales in the input space to achieve explicit denoising and (2) exploring diverse reasoning paths and voting on answers in the output space. Empirically, CD-CoT demonstrates an average improvement of $17.8 \%$ in accuracy over the base model and shows significantly stronger denoising capabilities than baseline methods. The source code is publicly available at: https://github.com/tmlr-group/NoisyRationales.</p>
<p>Input with Noisy Questions
Question-1 (Q1): In base-9, what is $86+57$ ? We know 6+6=12 and 3+7=10 in base 10.
Rationale-1 (R1): In base-9, the digits are " 012345678 ". We have $6+7=13$ in base10. Since we're in base- 9 , that exceeds the maximum value of 8 for a single digit. 13 $\bmod 9=4$, so the digit is 4 and the carry is 1. We have $8+5+1=14$ in base 10.14 $\bmod 9=5$, so the digit is 5 and the carry is 1. A leading digit 1 . So the answer is 154 .</p>
<p>Answer-1 (A1): 154.
...Q2, R2, A2, Q3, R3, A3...
Test Question: In base-9, what is $62+58$ ? We know 6+6=12 and 3+7=10 in base 10.</p>
<p>Input with Noisy Rationales
Question-1 (Q1): In base-9, what is $86+57$ ? Rationale-1 (R1): In base-9, the digits are " 012345678 ". We have $6+7=13$ in base10. $\underline{13+8=21}$. Since we're in base- 9 , that exceeds the maximum value of 8 for a single digit. $13 \bmod 9=4$, so the digit is 4 and the carry is 1 . We have $8+5+1=14$ in base $10.14 \bmod 9=5$, so the digit is 5 and the carry is $1 . \underline{5+9=14}$. A leading digit is 1. So the answer is 154 .</p>
<p>Answer-1 (A1): 154.
...Q2, R2, A2, Q3, R3, A3...
Test Question: In base-9, what is $62+58$ ?</p>
<p>Figure 1: Exemplars of noisy questions [68] and noisy rationales (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base- 9 calculation, while the misguiding base- 10 information is given in noisy questions or rationales.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Results of GPT-3.5 with 0-shot, 3-shot clean rationales, and 3-shot noisy rationales: Both inaccurate and irrelevant rationales degenerate performance significantly, while the proposed CDCoT improves robustness against noisy rationales.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Chain modeling of the noisy rationale problem: Recovering chain (3) from chain (1) with the guidance of chain (2). From question $x_{i}$ to answer $y_{i}$, the rationale of chain (3) includes clean thoughts $T_{i}^{(j)}$ and noisy thoughts $\hat{T}_{i}^{(j)}$.</p>
<h1>1 Introduction</h1>
<p>In-context learning (ICL) is a common approach in large language models (LLMs), enabling models to extrapolate from a few examples and adapt without fine-tuning [4, 84, 16]. However, ICL's efficacy is closely tied to the quality and clarity of the prompting examples, particularly in the prevailing chain-of-thought (CoT) strategy that provides rationales, i.e., intermediate reasoning steps to solve a question [85]. Recent research has shown that LLMs struggle with noisy questions: they are easily distracted by irrelevant context and exhibit instability with slight input modifications [68, 78, 107].
Notably, this work shifts focus from the well-studied noisy questions (Noisy-Q) problem to the underexplored noisy rationales (Noisy-R) problem, wherein factually inaccurate or irrelevant reasoning steps are paired with valid question-answer examples, as illustrated in Fig. 1. Here, the emphasis on Noisy-R is due to its practical challenges, with examples drawn from diverse sources such as crowdsourced platforms, dialogue systems, and machine-generated data ${ }^{2}$ [25, 45, 73, 2, 77, 48]. However, the robustness of LLMs against Noisy-R remains unknown. A new benchmarking dataset is needed to conduct a systematic evaluation of current LLMs and verify the corresponding countermeasures.
In this work, we first construct the NoRa (Noisy Rationales) dataset, a comprehensive testbed to evaluate the robustness of LLM reasoning against noisy rationales across various reasoning domains (in Sec. 3). The NoRa contains a total of 26391 questions, covering three types of reasoning tasks: mathematical, symbolic, and commonsense. We uniformly formalize the generation of noisy rationales by inserting irrelevant or inaccurate thoughts, controlling reasoning difficulty through noise ratios, and guaranteeing the overall prompting correctness without modifying the question or answer.
With the NoRa dataset, we evaluate several LLMs and reveal that all of them are intrinsically vulnerable to noisy rationales (in Sec. 4). For example, compared to prompting with clean rationales, GPT-3.5 exhibits an average $3.0 \%$ - $33.3 \%$ decrease in accuracy with noisy rationales, as in Fig. 2. Besides, only limited improvements are achieved with existing robust methods based on the model's intrinsic denoising ability, e.g., self-consistency [83] and self-denoise [102]. We show that Noisy-R is much more challenging than Noisy-Q, requiring context-specific knowledge to guide the denoising.
To solve this, we propose to rectify the rationales with only one clean CoT demonstration that can be the most attainable supervision in practice (in Sec. 5). We assume that LLMs can rectify rationales by contrasting a noisy rationale with a clean one, as in Fig. 3. Guided by this principle, we design the framework of Contrastive Denoising with noisy CoT (CD-CoT) with four steps: rationale rephrasing, rationale selecting, rationale exploring, and answer voting. Technically, the first two steps aim to achieve explicit denoising, while the last two steps are for diverse reasoning paths. Empirically, CD-CoT achieves an average improvement of $17.8 \%$ in accuracy w.r.t. the base model (refer to Tab. 8). Notably, it presents much stronger denoising power than baselines in rectifying the rationales.
Contributions. To our best knowledge, we are the first to investigate the problem of noisy rationales.</p>
<ul>
<li>We formalize the under-explored noisy rationale problem in the prevailing chain-of-thought prompting and construct the NoRa dataset to benchmark the robustness of LLMs against noisy rationales (Sec. 3).</li>
<li>We systematically evaluate LLMs with NoRa dataset and extract several insightful observations, e.g., the unsatisfactory robustness and limited denoising power of LLMs under noisy rationales (Sec. 4).</li>
<li>We propose to rectify the noisy rationales with only one clean CoT demonstration, design a simple yet effective method, CD-CoT, and verify its effectiveness through comprehensive experiments (Sec. 5).</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2 Related Work</h1>
<p>Limitations of in-context learning (ICL). Though effective, ICL suffers from the susceptibility to manual generation and selection of demonstrations (examples), where the ultimate performance is closely tied to the demonstrations' quality and clarity. Recent investigations on noisy questions have shown that (i) LLMs can be distracted by irrelevant or adversarial context, as they are designed to pay close attention to the context provided in the prompt [32, 58, 68, 78] and (ii) LLM reasoning is unstable, namely, small modifications to the prompt could potentially cause large variations in the model's output [102, 107]. Besides, another line of research regarding noisy answers [42, 18] justifies the feasibility of misleading an LLM to agree factual errors such as " $1+1=3$ " in base-10 calculation.
Countermeasures. Two intrinsic traits of LLMs are desirable for addressing the above limitations:</p>
<ul>
<li>Self-correction, wherein LLMs attempt to correct their initial responses based solely on their inherent capabilities without external feedback, e.g., by refining prompts through iterative corrections of responses or question trajectories [91, 89]. Although LLMs can learn to ignore irrelevant information by examples or instructions [68], they are proved to be still struggling to correct their responses without external feedback, and at times, their performance might even degrade after self-correction [29, 81].</li>
<li>Self-consistency aims to obtain a consistent answer against input perturbations. This is achieved by generating multiple responses via randomized smoothing on input questions [98] or diverse paths for answering one question [83] followed by the answer aggregation. This strategy brings improvements with extra costs for repeated reasoning. Moreover, it cannot explicitly rectify questions or rationales.
Noisy rationales, as the research focus of this work, mainly originates from (1) the inherent imperfections, inconsistencies, and inaccuracies of humans' cognitive processes [53, 10] and (2) the diversity, unpredictability, and hallucination of the LLMs' generative mechanisms [103, 30, 101]. A detailed literature review and discussion of noise rationales are in Appendix B and C, respectively.</li>
</ul>
<h2>3 The NoRa Dataset</h2>
<p>In this section, we introduce the NoRa (Noisy Rationales) dataset for benchmarking the robustness against noisy rationales. NoRa consists of 26391 questions and 5 subsets, covering mathematical, symbolic, and commonsense reasoning tasks, where ICL and CoT demonstrations play a crucial role.</p>
<h3>3.1 Definition of Noisy Rationales</h3>
<p>We start by formalizing the ICL and CoT demonstrations. Given a test question $x_{\text {test }}$ and an LLM $f_{\theta}$, one expects to get the correct answer $y_{\text {test }}$ as $f_{\theta}\left(x_{\text {test }}\right) \mapsto y_{\text {test }}$. This zero-shot manner cannot guarantee effectiveness, especially when encountering unfamiliar contexts or scenarios. To boost effectiveness, the ICL techniques prompt the LLM with a few examples $S_{n}=\left{\left(x_{i}, y_{i}\right)\right}<em i="i">{i=1}^{n}$ collected in the current context, each composed of a question $x</em>$ as}$ and answer $y_{i}$, and then construct the new input $x_{\mathrm{ICL}</p>
<p>$$
x_{\mathrm{ICL}}=\left[S_{n}, x_{\text {test }}\right]=\left[x_{1}, y_{1}, \ldots, x_{n}, y_{n}, x_{\text {test }}\right]
$$</p>
<p>The guidance by $S_{n}$ makes $f_{\theta}\left(x_{\mathrm{ICL}}\right) \mapsto y_{\text {test }}$ much easier than $f_{\theta}\left(x_{\text {test }}\right) \mapsto y_{\text {test }}$. Then, the CoT further refines $x_{\mathrm{ICL}}$ by constructing the step-by-step rationale $\mathcal{T}<em i="i">{i}$, consisting of several thoughts $T</em>$, namely,}^{(j)</p>
<p>$$
x_{\mathrm{CoT}}=\left[x_{1}, \mathcal{T}<em 1="1">{1}, y</em>}, \ldots, x_{n}, \mathcal{T<em n="n">{n}, y</em>}, x_{\text {test }}\right], \text { where } \mathcal{T<em i="i">{i}=\left[T</em>\right]
$$}^{(1)}, T_{i}^{(2)}, T_{i}^{(3)}, \ldots, T_{i}^{(k)</p>
<p>However, as aforementioned, the thoughts in CoT (Eqn. 2) can be noisy in practice. This noise can be attributed to (1) irrelevant thoughts, which are irrelevant but correct, or (2) inaccurate thoughts, which are relevant but factually wrong. Here, we uniformly formalize these two kinds of noise as</p>
<p>$$
\hat{\mathcal{T}}<em i="i">{i}=\left[T</em>}^{(1)}, \hat{T<em i="i">{i}^{(1)}, T</em>}^{(2)}, \hat{T<em i="i">{i}^{(2)}, \ldots, T</em>\right]
$$}^{(k)}, \hat{T}_{i}^{(k)</p>
<p>where $\hat{T}<em i="i">{i}^{(j)}$ represents a noisy thought (irrelevant or inaccurate) that is coherent with the previous clean thought $T</em>$ (relevant and correct). The following introduces the definition of noisy thoughts.
Irrelevant thoughts refer to incorporating irrelevant information unhelpful for solving the question, e.g., discussing the genetic overlap of siblings when the task is to deduce family roles in relationship reasoning. Redundant information may be introduced by the LLM's diverse response generation or}^{(j)</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Irrelevant Thoughts</th>
<th>Inaccurate Thoughts</th>
</tr>
</thead>
<tbody>
<tr>
<td>NoRa-Math</td>
<td>In base-9, digits run from 0 to 8 . We have $3+2=5$ in base10. Since we're in base-9, that doesn't exceed the maximum value of 8 for a single digit. $5 \bmod 9=5$, so the digit is 5 and the carry is 0 . There are five returns on Earth: the Atlantic, Pacific, Indian, Arctic, and Southern. We have $8+6+0=$ 14 in base 10. $14 \bmod 9=5$, so the digit is 5 and the carry is 1. A leading digit 1 . So the answer is 155 . Answer: 155</td>
<td>In base-9, digits run from 0 to 8 . We have $3+2=5$ in base10. $5+4=9$. Since we're in base-9, that doesn't exceed the maximum value of 8 for a single digit. $5 \bmod 9=5$, so the digit is 5 and the carry is $0 . \frac{5+9}{5}=14$. We have $8+6+0=$ 14 in base 10. $14 \bmod 9=5$, so the digit is 5 and the carry is 1. A leading digit 1 . So the answer is 155 . Answer: 155</td>
</tr>
<tr>
<td>NoRa-Symbolic</td>
<td>... "turn around right" means the agent needs to turn right, and repeat this action sequence four times to complete a 360-degree loop. Many GPS navigation systems will issue a turn around command if the driver deviates from the planned route. So, in action sequence is $\overline{1} \overline{\text { TURN RIGHT }}$ $\overline{1} \overline{\text { TURN RIGHT } 1 \text { TURN RIGHT } 1 \text { TURN RIGHT. ... }}$</td>
<td>... "turn around right" means the agent needs to turn right, and repeat this action sequence four times to complete a 360-degree loop. Turn opposite is $\overline{1} \overline{\text { TURN RIGHT }}$ $\overline{1} \overline{\text { TURN }}$ $\overline{1} 4 \overline{1} 5$. So, in action sequence is $\overline{1} \overline{\text { TURN RIGHT }}$ $\overline{1} \overline{\text { TURN RIGHT } 1 \text { TURN RIGHT } 1 \text { TURN RIGHT. ... }}$</td>
</tr>
<tr>
<td>NoRa-Com.</td>
<td>The relations path are son, sister, uncle, which means Francisco is David's son's sister's uncle. For son's sister, we have son's sister is daughter. So the relations path are reduced to daughter, uncle. In genetics, mitochondrial DNA is always inherited from the mother, making the mother-daughter genetic link unique. For daughter's uncle, we have daughter's uncle is brother. So the relations path are reduced to brother. Therefore, the answer is brother. Answer:brother</td>
<td>The relations path are son, sister, uncle, which means Francisco is David's son's sister's uncle. For son's sister, we have son's sister is daughter. So the relations path are reduced to daughter, uncle. For daughter's uncle, we have daughter's uncle is brother. We have brother, sister is brother. So the relations path are reduced to brother. Answer:brother</td>
</tr>
</tbody>
</table>
<p>Table 1: Noisy rationales (consisting noisy thoughts) sampled from the NoRa dataset. Full examples of NoRa are in Appendix C.6, and real-world examples of noisy rationales are in Appendix C.3.
by humans when clarifying concepts in problem-solving examples [5, 104]. Alternatively, we explore various semantic levels of "irrelevance" for constructing diverse irrelevant thoughts in Appendix F.4.
Inaccurate thoughts refer to factual errors in rationales that are common in mathematical calculation or transcription, e.g., " $5+5=10$ " is wrong in base- 9 calculation. This noise comes from algorithmic limitations, errors in training data, misinterpretations of instructions, and logical fallacies [40, 64].
Remark 3.1. Both types of noise only impact the finer details of the reasoning chain without affecting the correctness of question $x_{i}$ and answer $y_{i}$. This distinction ensures that the reasoning based on the noisy demonstration is not fundamentally flawed, namely, only the reasoning rationale $\overline{\mathcal{T}_{i}}$ is noisy.</p>
<h1>3.2 Tasks and Statistics</h1>
<p>The NoRa dataset covers the three types of reasoning tasks listed below. In noise generation, irrelevant thoughts, sourced from extraneous scientific or social facts, and inaccurate thoughts, arising from misguided reasoning, are both based on contextual thoughts of Eqn. 2 (see examples in Tab. 1).</p>
<ul>
<li>NoRa-Math. This task is derived from the Base Calculation dataset [88] for evaluating non-standard base arithmetic skills and features two sub-tasks of base-9 and base-11 addition. Here, the mastery of mathematical concepts and the calculation rules of specific bases are the keys to solving these tasks.</li>
<li>NoRa-Symbolic. We utilize the SCAN dataset [41] here, which aims to transform natural language into symbolic, machine-understandable instructions. This transformation is learned from the prompting examples, comprising two sub-tasks: (1) equal-length subtask, where the transformed instructions in both $S_{n}$ and $x_{\text {test }}$ have the equal length; and (2) longer-length subtask, where the transformed instructions in $x_{\text {test }}$ is longer than those in $S_{n}$, presenting an easy-to-hard generalization challenge.</li>
<li>NoRa-Commonsense. This task is constructed based on the CLUTRR dataset [71], which is geared towards family relation path reasoning, e.g., "who is aunt's sister's mother?" It requires the mastery and application of commonsense knowledge of relationships and cognitive skills for reasoning.
Noise Ratio. Given the noise ratio $\epsilon \in(0,1)$, the expected number of added noisy thoughts for a $k$-length CoT demonstration is $\lfloor\epsilon \cdot k+1 / 2\rfloor$. Specifically, for an irrelevant thought $\overline{\mathcal{T}}_{i}^{(j)}$ in $j$-th position of $i$-th example, a Bernoulli distribution $\operatorname{Bern}(\epsilon) \in{0,1}$ is adopted to indicate its binary existence. We also further investigate the impact of the number of noisy thoughts, provided in Appendix F.5.</li>
</ul>
<p>Statistics. Tab. 2 provides a categorization of task difficulties as Easy, Medium, and Hard, with corresponding noise ratios of $0.3,0.5$, and 0.8 . Regarding the number of thoughts in a rationale, Math entails 8 thoughts, Symbolic varies from 2 to 12 , and Commonsense requires 5 thoughts. Additionally, a detailed introduction to NoRa is in Appendix C.4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Difficulty</th>
<th style="text-align: center;">Noise</th>
<th style="text-align: center;">#total thoughts</th>
<th style="text-align: center;">#noisy thoughts</th>
<th style="text-align: center;">of prompting rationales (Avg.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ratio</td>
<td style="text-align: center;">Math Base-9</td>
<td style="text-align: center;">Math Base-11</td>
<td style="text-align: center;">Sym. Equal. Sym. Longer Com.</td>
</tr>
<tr>
<td style="text-align: center;">Easy</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">10 (2)</td>
<td style="text-align: center;">10 (2)</td>
<td style="text-align: center;">11.5 (2.7)</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">12 (4)</td>
<td style="text-align: center;">12 (4)</td>
<td style="text-align: center;">13.3 (4.5)</td>
</tr>
<tr>
<td style="text-align: center;">Hard</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">14 (6)</td>
<td style="text-align: center;">14 (6)</td>
<td style="text-align: center;">16.0 (7.1)</td>
</tr>
<tr>
<td style="text-align: center;">#questions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4024</td>
<td style="text-align: center;">9269</td>
<td style="text-align: center;">4182</td>
</tr>
</tbody>
</table>
<p>Table 2: Statistics of NoRa dataset.</p>
<p>| Task | Method $\mathcal{M}$ | $\operatorname{Acc}(\mathcal{M}, \mathcal{Q}, \mathcal{P}<em _irrelevant="{irrelevant" _text="\text">{\text {clean }}$ ) | $\operatorname{Acc}(\mathcal{M}, \mathcal{Q}, \mathcal{P}</em>$ ) | | | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| | | | Easy | Medium | Hard | Avg. | Easy | Medium | Hard | Avg. |
| Math Base-9 | Base | 46.4 | 39.3 | 30.2 | 26.6 | 32.1 | 23.2 | 10.1 | 6.0 | 13.1 |
| | w/ ISC [29] | 24.3 | 17.7 | 14.7 | 12.7 | 15.0 | 18.4 | 13.7 | 12.2 | 14.8 |
| | w/ SP [89] | 26.2 | 25.5 | 25.5 | 21.9 | 24.3 | 20.0 | 18.4 | 14.3 | 17.6 |
| | w/ SM [62] | 37.4 | 30.0 | 22.7 | 16.5 | 23.1 | 24.7 | 19.2 | 12.4 | 18.8 |
| | w/ SD [102] | 47.9 | 37.2 | 25.4 | 24.7 | 29.1 | 29.3 | 12.5 | 8.7 | 16.8 |
| | w/ SC [83] | 61.5 | 51.1 | 39.0 | 36.2 | 42.1 | 32.7 | 15.3 | 7.5 | 18.5 |
| Math Base-11 | Base | 23.9 | 19.1 | 13.6 | 10.7 | 14.5 | 14.0 | 6.7 | 3.6 | 8.1 |
| | w/ ISC [29] | 11.2 | 8.5 | 7.8 | 6.0 | 7.4 | 6.5 | 5.2 | 4.7 | 5.5 |
| | w/ SP [89] | 20.7 | 17.5 | 16.7 | 14.0 | 16.0 | 14.1 | 10.7 | 10.8 | 11.9 |
| | w/ SM [62] | 16.3 | 12.0 | 6.0 | 5.7 | 7.9 | 12.0 | 9.3 | 7.7 | 9.7 |
| | w/ SD [102] | 17.9 | 12.3 | 12.0 | 13.3 | 12.5 | 17.0 | 8.7 | 5.3 | 10.3 |
| | w/ SC [83] | 33.7 | 25.3 | 16.3 | 15.0 | 18.9 | 19.7 | 9.3 | 3.3 | 10.8 |
| Symbolic Equal | Base | 32.7 | 26.1 | 25.1 | 23.0 | 25.4 | 29.1 | 26.1 | 22.7 | 26.0 |
| | w/ ISC [29] | 25.9 | 20.0 | 16.3 | 15.5 | 17.3 | 19.2 | 18.3 | 18.1 | 18.5 |
| | w/ SP [89] | 23.2 | 23.0 | 22.6 | 22.7 | 22.8 | 23.7 | 22.5 | 23.5 | 23.2 |
| | w/ SM [62] | 25.0 | 20.7 | 19.7 | 16.7 | 19.0 | 21.0 | 20.3 | 20.0 | 20.4 |
| | w/ SD [102] | 9.9 | 10.1 | 10.9 | 10.3 | 10.4 | 10.1 | 10.9 | 10.4 | 10.5 |
| | w/ SC [83] | 35.3 | 31.0 | 28.3 | 27.0 | 28.8 | 33.3 | 30.7 | 26.0 | 30.0 |
| Symbolic Longer | Base | 9.2 | 6.3 | 7.2 | 6.0 | 6.5 | 7.0 | 6.8 | 6.0 | 6.6 |
| | w/ ISC [29] | 4.9 | 4.6 | 2.7 | 3.7 | 3.7 | 5.4 | 4.3 | 3.3 | 3.7 |
| | w/ SP [89] | 5.1 | 4.3 | 4.1 | 3.9 | 4.1 | 4.9 | 4.0 | 4.5 | 4.5 |
| | w/ SM [62] | 1.7 | 0.7 | 0.7 | 1.3 | 1.0 | 1.3 | 0.7 | 0.3 | 0.8 |
| | w/ SD [102] | 0.1 | 0.1 | 0.1 | 0.2 | 0.1 | 0.1 | 0.3 | 0.0 | 0.1 |
| | w/ SC [83] | 13.0 | 7.7 | 9.0 | 6.3 | 7.7 | 8.0 | 8.0 | 8.7 | 8.2 |
| Commonsense | Base | 45.7 | 44.3 | 42.3 | 41.4 | 42.7 | 36.7 | 33.4 | 28.3 | 32.8 |
| | w/ ISC [29] | 21.8 | 24.3 | 22.5 | 21.4 | 22.7 | 23.3 | 26.5 | 24.0 | 24.6 |
| | w/ SP [89] | 47.9 | 48.2 | 46.7 | 48.1 | 47.7 | 49.6 | 46.6 | 46.5 | 47.6 |
| | w/ SM [62] | 53.3 | 50.3 | 50.0 | 46.7 | 49.0 | 47.7 | 49.0 | 49.3 | 48.7 |
| | w/ SD [102] | 54.0 | 58.3 | 57.3 | 57.7 | 57.8 | 57.0 | 58.3 | 53.7 | 56.3 |
| | w/ SC [83] | 52.0 | 46.3 | 45.0 | 44.7 | 45.3 | 44.7 | 44.7 | 38.0 | 42.5 |}}$ ) | | | | | $\operatorname{Acc}(\mathcal{M}, \mathcal{Q}, \mathcal{P}_{\text {inaccurate }</p>
<p>Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The boldface numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of Base model are highlighted in gray.</p>
<h2>4 Evaluating Language Models on NoRa dataset</h2>
<p>In this section, we comprehensively evaluate representative LLMs and robust methods on the newly constructed NoRa dataset. We first introduce the basic evaluation setups and then present several observations on the unsatisfactory robustness of current LLMs and methods under noisy rationales.</p>
<p>Baseline methods. We select five representative methods as baselines to ensure a comprehensive assessment that encompasses the two traits of self-correction and self-consistency. ISC [29] and SP [89] exemplify self-correction, focusing on response rectification and prompt rephrasing, respectively. SM [62], SD [102], and SC [83] fall under self-consistency: SM [62] injects perturbations into prompts for robustness, SD [102] masks prompts and asks LLMs to reconstruct them, while SC directly samples outputs without denoising. These methods are further introduced in Appendix E.1.</p>
<p>LLM basis. We employ GPT-3.5-turbo-0613 [17] as our base LLM (denoted as Base) for the analyses presented in this study. In addition, we conduct evaluations on three supplementary models, including Gemini-Pro (Jan. 2024) [76], Llama2-70B [79], and Mixtral-8x7B [33]. For all baselines, we consistently set the temperature parameter $\tau$ to the value of 1. In order to obtain consistent results, we evaluate 300 questions for each task and repeat the model reasoning five times for each question.</p>
<p>Evaluation metric. Given a set of test question $\mathcal{Q}=\left{\left(x_{\text {test }}, y_{\text {test }}\right)\right}$ and a set of CoT-prompting examples $\mathcal{P}=\left[x_{1}, \mathcal{T}<em 1="1">{1}, y</em>}, \ldots, x_{n}, \mathcal{T<em n="n">{n}, y</em>}\right]$, we define the accuracy of the denoising method $\mathcal{M}$ with a specific LLM $f_{\theta}$, namely, $\operatorname{Acc}(\mathcal{M}, \mathcal{Q}, \mathcal{P})=\sum_{\left\langle x_{\text {test }}, y_{\text {test }}\right\rangle \in \mathcal{Q}} \mathbb{1}\left[\mathcal{M}\left(\mathcal{P}, x_{\text {test }}\right)=y_{\text {test }}\right] /|\mathcal{Q}|$. We report the results in percentage (%) with one decimal point. Therein, $\operatorname{Acc}\left(\mathcal{M}, \mathcal{Q}, \mathcal{P<em _irrelevant="{irrelevant" _text="\text">{\text {clean }}\right), \operatorname{Acc}\left(\mathcal{M}, \mathcal{Q}, \mathcal{P}</em>, \emptyset)$ represents the zero-shot result.}}\right)$, and $\operatorname{Acc}\left(\mathcal{M}, \mathcal{Q}, \mathcal{P}_{\text {inaccurate }}\right)$ indicate accuracy with clean, irrelevant, and inaccurate rationales, respectively. When there is no prompting example, i.e., $\mathcal{P}=\emptyset$, then $\operatorname{Acc}(\mathcal{M}, \mathcal{Q</p>
<p>Unreliability revealing with noisy rationales. We conduct the reasoning tasks on LLM with NoisyR and summarize the results in Tab. 3. Overall, the base LLM with all the existing reasoning methods is severely affected by irrelevant or inaccurate noise, with overall showing a $0.2 \%-25.3 \%$ decrease with irrelevant noise and a more drastic $0.1 \%-54.0 \%$ decrease with inaccurate noise compared with clean rationales. While robust methods like SP and SD exhibit resilience to noise on partial tasks, their performance remains inconsistent and often declines. To further reveal the unreliability, we start by analyzing the two categories of robust methods mentioned above in the following observations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">Temperature</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">00.3</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Base-9</td>
<td style="text-align: center;">clean</td>
<td style="text-align: center;">61.060</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">57.555 .346 .4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ina. easy</td>
<td style="text-align: center;">29.728 .0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">27.226 .621 .7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ina. hard</td>
<td style="text-align: center;">5.05</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">5.54.65 .0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Base-11</td>
<td style="text-align: center;">clean</td>
<td style="text-align: center;">34.033 .8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">31.629 .823 .9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">irr. easy</td>
<td style="text-align: center;">21.723 .1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">21.323 .319 .1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">irr. hard</td>
<td style="text-align: center;">17.017 .5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">15.514 .110 .7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Sym.(E)</td>
<td style="text-align: center;">clean</td>
<td style="text-align: center;">34.235 .8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">35.734 .632 .7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">irr. easy</td>
<td style="text-align: center;">28.631 .5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29.829 .128 .1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">irr. hard</td>
<td style="text-align: center;">27.026 .1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">26.224 .023 .0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Sym.(L)</td>
<td style="text-align: center;">clean</td>
<td style="text-align: center;">6.38.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.98.99 .3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ina. easy</td>
<td style="text-align: center;">5.07.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8.68.37 .0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ina. hard</td>
<td style="text-align: center;">4.06.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.36.26 .0</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Comparing performances of the base model with different temperatures. Sym.(E)/(L) are symbolic tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">#Prompting Examples</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">Base-9</td>
<td style="text-align: center;">clean</td>
<td style="text-align: center;">24.838 .346 .4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50.850 .5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ina.-easy</td>
<td style="text-align: center;">17.522 .223 .2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">25.425 .6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ina.-hard</td>
<td style="text-align: center;">11.36.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.05.75 .7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Base-11</td>
<td style="text-align: center;">clean</td>
<td style="text-align: center;">11.820 .423 .9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29.932 .1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">irr. easy</td>
<td style="text-align: center;">8.915 .919 .1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">21.726 .3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">irr. hard</td>
<td style="text-align: center;">7.710 .010 .7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">15.216 .1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Sym.(E)</td>
<td style="text-align: center;">clean</td>
<td style="text-align: center;">18.026 .532 .7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ina.-easy</td>
<td style="text-align: center;">17.323 .629 .1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ina.-hard</td>
<td style="text-align: center;">15.021 .022 .7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Sym.(L)</td>
<td style="text-align: center;">clean</td>
<td style="text-align: center;">2.77.7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">9.511 .312 .2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">irr. easy</td>
<td style="text-align: center;">2.35.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7.08.88 .9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">irr. hard</td>
<td style="text-align: center;">1.94.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Comparing performances of the base model with a varying number of examples ("â€”" denotes over token limit).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0-shot</td>
<td style="text-align: center;">clean</td>
</tr>
<tr>
<td style="text-align: center;">GPT3.5</td>
<td style="text-align: center;">Base-9</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">46.430 .310 .1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sym.(E)</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">32.725 .126 .1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Com.</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">45.742 .333 .4</td>
</tr>
<tr>
<td style="text-align: center;">Gemini</td>
<td style="text-align: center;">Base-9</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">88.072 .221 .2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sym.(E)</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">44.538 .936 .7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Com.</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">55.653 .233 .5</td>
</tr>
<tr>
<td style="text-align: center;">Llama2</td>
<td style="text-align: center;">Base-9</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">4.922 .92 .7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sym.(E)</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">10.18 .79 .1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Com.</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">42.341 .940 .2</td>
</tr>
<tr>
<td style="text-align: center;">Mixtral</td>
<td style="text-align: center;">Base-9</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">27.516 .33 .7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sym.(E)</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">19.317 .915 .1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Com.</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">37.534 .931 .1</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparing LLMs with 0 -shot, 3-shot clean, and 3-shot medium irrelevant (irr.) / inaccurate (ina.) rationales.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Zero-shot</th>
<th style="text-align: center;">Few-shot (No Shuffle)</th>
<th style="text-align: center;">Shuffle Questions $x_{i}$</th>
<th style="text-align: center;">Shuffle Rationales $\mathcal{T}_{i}$</th>
<th style="text-align: center;">Shuffle Answers $y_{i}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Math Base-9</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">45.5 (0.9\% $\downarrow$ )</td>
<td style="text-align: center;">34.5 (11.9\% $\downarrow$ )</td>
<td style="text-align: center;">35.7 (10.7\% $\downarrow$ )</td>
</tr>
<tr>
<td style="text-align: center;">Math Base-11</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">24.8 (0.9\% $\uparrow$ )</td>
<td style="text-align: center;">21.6 (2.3\% $\downarrow$ )</td>
<td style="text-align: center;">21.1 (11.7\% $\downarrow$ )</td>
</tr>
<tr>
<td style="text-align: center;">Symbolic Equal</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">32.7 (0.0\% $\downarrow$ )</td>
<td style="text-align: center;">32.8 (0.1\% $\uparrow$ )</td>
<td style="text-align: center;">32.3 (0.4\% $\downarrow$ )</td>
</tr>
<tr>
<td style="text-align: center;">Symbolic Longer</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">7.0 (2.2\% $\downarrow$ )</td>
<td style="text-align: center;">6.2 (3.0\% $\downarrow$ )</td>
<td style="text-align: center;">6.3 (2.9\% $\downarrow$ )</td>
</tr>
<tr>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">38.7 (7.0\% $\downarrow$ )</td>
<td style="text-align: center;">39.7 (6.0\% $\downarrow$ )</td>
<td style="text-align: center;">39.8 (5.9\% $\downarrow$ )</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance (in accuracy\%) on NoRa dataset under different few-shot shuffle configurations.</p>
<p>Observation 4.1. Self-correction methods perform poorly on most tasks with noisy rationales. Therein, ISC [29] and SP [89] rely on the inherent capabilities of LLMs to enhance the quality of generated responses. However, in the absence of external feedback, the model's self-correction ability in reasoning tasks is limited, often resulting in the miscorrection of the given content (see Tab. 12). SP can only slightly improve the accuracy of commonsense tasks, while ISC performs unsatisfactorily across all tasks. As can be seen from Tab. 3, these methods perform even worse than the base model.</p>
<p>Observation 4.2. Self-consistency methods can improve robustness without true denoising. Two self-consistency approaches, SM [62] and SD [102], are originally proposed to address Noisy-Q issues. When applied to our Noisy-R scenarios, they tend to easily disrupt the intrinsic logical coherence within the thought chain. Although these methods utilizing smooth strategies (e.g., random smoothing or masking) perform well on the commonsense dataset, they can hardly handle the more difficult reasoning tasks and even degenerate close to $0 \%$, e.g., in the Symbolic Longer task. Another method, SC [83], performs better than the base model in all tasks, improving both clean and noisy reasoning performance. However, SC does not conduct explicit denoising on rationales during its reasoning procedure. In addition, SC also requires a high computation cost (refer to Appendix F.2).
Besides these methods, next, we analyze LLMs' intrinsic properties under noisy rationales as follows.
Observation 4.3. Adjusting model temperature can help reasoning under noisy rationales. In Tab. 4, we evaluate the base LLM using different temperatures on 3-shot demonstrations. Overall, reducing temperature can enhance the model's accuracy under both noisy and clean rationale reasoning, compared to the default temperature of 1 . However, the relationship between temperature and accuracy is not linear for noisy reasoning; instead, there are multiple peaks in accuracy within the temperature range of 0 to 1 . Additionally, it is found that excessively low temperatures (e.g., 0 ) tend to result in verbose and repeated responses, which cause the model to exceed token limits up to $30 \%$ in symbolic tasks where the length of expected answers is quite variable among different questions.</p>
<h2>Observation 4.4. Prompting with more noisy examples boosts reasoning accuracy on most tasks.</h2>
<p>In Tab. 5, we evaluate the model using different numbers of exemplars while keeping the temperature at 1. In general, the LLM's accuracy will still improve as the number of noisy examples increases in the clean and noisy settings. However, it should be noted that in tasks with high-level noise from NoRa-Math, increasing prompting examples can degenerate accuracy. For example, in the base-9 inaccurate-hard task, prompting with noisy rationales is even worse than the 0 -shot accuracy of $7.2 \%$. Further, we provide a deeper analysis of increasing the number of noisy examples in Appendix F.6.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: CD-CoT's first two steps for data denoising. First, it rephrases the $i$-th noisy example by contrasting it with the clean example. Then, with the obtained $N$ rephrased examples, it selects the $M$ qualified candidates by checking the validity of the rephrased answers $\hat{y}<em N="N" i="i">{i 1}, \ldots, \hat{y}</em>$.
}$ w.r.t. $y_{i<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: CD-CoT constructs $M$ inputs ( $K$-shot) by allocating the $K \cdot M$ rephrased rationales. These inputs are concatenated with the clean example and test question and then fed to an LLM for reasoning separately. The obtained $D$ answers are equally voted to obtain the final answer $y$.</p>
<p>Observation 4.5. Different LLMs are generally vulnerable to noisy rationales. In Tab. 6, we evaluate different LLMs across three settings: 0 -shot CoT, 3-shot clean rationales, and 3-shot mediumlevel noisy rationales. Notably, Gemini-Pro outperforms GPT-3.5 in overall performance. However, it demonstrates a similar degree of sensitivity to noise, with a $2.4 \%-15.7 \%$ performance decline with irrelevant rationales and a $7.8 \%-66.8 \%$ decline with inaccurate rationales compared to clean rationales. While Mixtral 8x7B shows a slight underperformance compared to GPT-3.5, it also manifests a vulnerability to noise, incurring a $1.4 \%-11.2 \%$ loss with irrelevant rationales and a greater $4.2 \%-23.8 \%$ loss with inaccurate rationales. By contrast, Llama2-70B performs suboptimally, with a $0.4 \%-2.0 \%$ drop for irrelevant thoughts and a larger $1.0 \%-2.2 \%$ drop for inaccurate thoughts.
Further investigation. Inspired by Min et al. [54], we further explore the mapping among questions, rationales, and answers through shuffling experiments. Specifically, given the 3-shot prompting examples $\left{\left(x_{1}, \mathcal{T}<em 1="1">{1}, y</em>}\right),\left(x_{2}, \mathcal{T<em 2="2">{2}, y</em>}\right),\left(x_{3}, \mathcal{T<em 3="3">{3}, y</em>}\right)\right}$, we test three configurations, i.e., shuffle questions $\left{\left(x_{1}, \mathcal{T<em 3="3">{3}, y</em>}\right),\left(x_{2}, \mathcal{T<em 1="1">{1}, y</em>}\right),\left(x_{3}, \mathcal{T<em 2="2">{2}, y</em>}\right)\right}$, shuffle answers $\left{\left(x_{1}, \mathcal{T<em 3="3">{1}, y</em>}\right),\left(x_{2}, \mathcal{T<em 1="1">{2}, y</em>}\right),\left(x_{3}, \mathcal{T<em 2="2">{3}, y</em>}\right)\right}$, and shuffle rationales $\left{\left(x_{1}, \mathcal{T<em 1="1">{3}, y</em>}\right),\left(x_{2}, \mathcal{T<em 2="2">{1}, y</em>}\right),\left(x_{3}, \mathcal{T<em 3="3">{2}, y</em>\right)\right}$. These break the original mappings. The results under these configurations are shown in Tab. 7, which induces the following observation.</p>
<p>Observation 4.6. Shuffling the mappings of prompting examples degenerates the reasoning but still performs better than without prompting. This means that while LLMs may not heavily rely on the exact mapping (of question, rationale, and answer), they still benefit from demonstrating information even with shuffling. Notably, this finding is consistent with the conclusions of [54] that LLMs learn more abstract task information from the demonstrations rather than simply memorizing question-answer pairs. More importantly, LLMs are less vulnerable to shuffled mappings than noisy rationales. Unlike shuffling, the irrelevant or inaccurate information in noisy rationales introduces misleading elements that significantly interfere with the model's ability to learn correct task patterns, thereby resulting in more severe performance degradation. This extends [54]'s finding and shows that the quality of reasoning steps can be more crucial than the exact mapping of prompting examples.</p>
<h1>5 Method</h1>
<p>This section aims to enable LLMs to discern and remove noisy thoughts. The observations in Sec. 4 and previous works show that current LLMs cannot achieve this with their intrinsic denoising ability, even enhanced with self-denoising methods. Therefore, we would claim that the external supervision is necessary for enhancement, which should be sufficient for denoising and accessible in practice. Existing methods with external supervision $[29,81,9]$ require (1) oracle feedback on the test question, (2) human feedback of errors on specific tokens or positions, or (3) expert knowledge to construct detailed descriptions of specific tasks. By contrast, we believe that a clean CoT demonstration is more attainable and practical, which can be the minimal requirement for denoising-purpose prompting.
Therein, we assume that LLMs can identify noisy thoughts by contrasting a pair of noisy and clean rationales and discerning their differences, similar to contrastive learning [26, 6, 35]. Here, the denoising power could come from the abilities of the instruction following and step-by-step reasoning [84, 66]. Hence, we propose the framework of CD-CoT, Contrastive Denoising with noisy CoT. The design principle is to explore and then exploit, i.e., (1) rephrasing and selecting rationales in input space to achieve explicit denoising, and then (2) exploring diverse rationales and voting answers in output space for deriving the final answer, as in Figs. $4 \&amp; 5$. The details are as follows.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">CD</span><span class="o">-</span><span class="nf">CoT</span><span class="err">:</span><span class="w"> </span><span class="n">Contrastive</span><span class="w"> </span><span class="n">Denoising</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">Noisy</span><span class="w"> </span><span class="n">Chain</span><span class="o">-</span><span class="k">of</span><span class="o">-</span><span class="n">Thought</span><span class="p">.</span>
<span class="nl">Require</span><span class="p">:</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">LLM</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">f_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">prompt</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">contrastive</span><span class="w"> </span><span class="n">denoising</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">P</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">denoise</span><span class="w"> </span><span class="err">}}\</span><span class="p">),</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">test</span><span class="w"> </span><span class="err">}}\</span><span class="p">),</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">clean</span>
<span class="w">    </span><span class="n">example</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">C</span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">C</span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">y_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">C</span><span class="err">}}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="n">prompting</span><span class="w"> </span><span class="n">examples</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">S_</span><span class="err">{</span><span class="n">n</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">y_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">K</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="n">hyper</span><span class="o">-</span><span class="k">parameters</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="err">\</span><span class="p">),</span>
<span class="w">    </span><span class="ow">and</span><span class="w"> </span><span class="n">reasoning</span><span class="w"> </span><span class="n">budget</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="err">\{</span><span class="n">B_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">M</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="n">satisfies</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">Sigma_</span><span class="err">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">M</span><span class="err">}</span><span class="w"> </span><span class="n">B_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">=</span><span class="n">D</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">D</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">total</span><span class="w"> </span><span class="n">budget</span><span class="p">).</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="w"> </span><span class="n">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">initialize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">set</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">rephrased</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="err">\</span><span class="p">)</span><span class="o">-</span><span class="n">th</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">emptyset</span><span class="err">\</span><span class="p">).</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="w"> </span><span class="n">N</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="err">\#</span><span class="w"> </span><span class="n">Step</span><span class="o">-</span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="n">Rationale</span><span class="w"> </span><span class="n">Rephrasing</span><span class="w"> </span><span class="n">via</span><span class="w"> </span><span class="n">Supervised</span><span class="w"> </span><span class="n">Contrasting</span>
<span class="w">            </span><span class="n">obtain</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">rephrased</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">hat</span><span class="err">{\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">hat</span><span class="err">{</span><span class="n">y</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">f_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">P</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">denoise</span><span class="w"> </span><span class="err">}}\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">C</span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">C</span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">y_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">C</span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">x_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">y_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="k">match</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">hat</span><span class="err">{</span><span class="n">y</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">=</span><span class="n">y_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="n">store</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">rephrased</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">cup</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">hat</span><span class="err">{\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">hat</span><span class="err">{</span><span class="n">y</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="err">\}\</span><span class="p">).</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="err">\#</span><span class="w"> </span><span class="n">Step</span><span class="o">-</span><span class="mi">2</span><span class="err">:</span><span class="w"> </span><span class="n">Rationale</span><span class="w"> </span><span class="n">Selection</span>
<span class="w">        </span><span class="n">randomly</span><span class="w"> </span><span class="k">select</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">M</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">rephrased</span><span class="w"> </span><span class="n">examples</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">obtain</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">hat</span><span class="err">{\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">i</span><span class="w"> </span><span class="n">s</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">hat</span><span class="err">{\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="w"> </span><span class="n">s</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">hat</span><span class="err">{</span><span class="n">y</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="w"> </span><span class="n">s</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">M</span><span class="err">}\</span><span class="p">).</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="err">\#</span><span class="w"> </span><span class="n">Step</span><span class="o">-</span><span class="mi">3</span><span class="err">:</span><span class="w"> </span><span class="n">Rationale</span><span class="w"> </span><span class="n">Exploration</span>
<span class="w">    </span><span class="k">initialize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">set</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">answers</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">Y</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">emptyset</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="w"> </span><span class="n">M</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="n">construct</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">input</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">P</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">j</span><span class="w"> </span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">hat</span><span class="err">{\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}}</span><span class="n">_</span><span class="err">{</span><span class="n">j</span><span class="w"> </span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">hat</span><span class="err">{</span><span class="n">y</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">j</span><span class="w"> </span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="err">\}</span><span class="n">_</span><span class="err">{</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="err">}</span><span class="o">^</span><span class="err">{</span><span class="n">K</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">j</span><span class="w"> </span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">hat</span><span class="err">{\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}}</span><span class="n">_</span><span class="err">{</span><span class="n">j</span><span class="w"> </span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">hat</span><span class="err">{</span><span class="n">y</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">j</span><span class="w"> </span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">i</span><span class="err">\</span><span class="p">)</span><span class="o">-</span><span class="n">th</span><span class="w"> </span><span class="k">element</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">hat</span><span class="err">{\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">R</span><span class="err">}}</span><span class="n">_</span><span class="err">{</span><span class="n">j</span><span class="err">}\</span><span class="p">).</span>
<span class="w">        </span><span class="n">concatenate</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">P</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">clean</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">P</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">P</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">cup</span><span class="err">\</span><span class="nf">left</span><span class="err">\{\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">C</span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">T</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">C</span><span class="err">}}</span><span class="p">,</span><span class="w"> </span><span class="n">y_</span><span class="err">{\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">C</span><span class="err">}}\</span><span class="nf">right</span><span class="p">),</span><span class="w"> </span><span class="n">x_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">test</span><span class="w"> </span><span class="err">}}\</span><span class="nf">right</span><span class="err">\}\</span><span class="p">).</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="w"> </span><span class="n">B_</span><span class="err">{</span><span class="n">M</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="k">get</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">LLM</span><span class="w"> </span><span class="n">reasoning</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">y_</span><span class="err">{</span><span class="n">j</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">f_</span><span class="err">{\</span><span class="n">theta</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">P</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="n">i</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">            </span><span class="n">store</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">Y</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">Y</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">cup</span><span class="err">\</span><span class="nf">left</span><span class="err">\{</span><span class="n">y_</span><span class="err">{</span><span class="n">j</span><span class="err">}\</span><span class="nf">right</span><span class="err">\}\</span><span class="p">).</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="err">\#</span><span class="w"> </span><span class="n">Step</span><span class="o">-</span><span class="mi">4</span><span class="err">:</span><span class="w"> </span><span class="n">Answer</span><span class="w"> </span><span class="n">Voting</span>
<span class="w">    </span><span class="k">initialize</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">dictionary</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="nf">count</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">C</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">forall</span><span class="w"> </span><span class="n">y_</span><span class="err">{</span><span class="n">j</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">Y</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">C</span><span class="err">}\</span><span class="nf">left</span><span class="o">[</span><span class="n">y_{j}\right</span><span class="o">]=</span><span class="mi">0</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">j</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span><span class="err">\</span><span class="n">ldots</span><span class="w"> </span><span class="n">D</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">update</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">C</span><span class="err">}\</span><span class="nf">left</span><span class="o">[</span><span class="n">y_{j}\right</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">C</span><span class="err">}\</span><span class="nf">left</span><span class="o">[</span><span class="n">y_{j}\right</span><span class="o">]+</span><span class="mi">1</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">get</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">final</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">y</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">maximum</span><span class="w"> </span><span class="n">counts</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">arg</span><span class="w"> </span><span class="err">\</span><span class="nf">max</span><span class="w"> </span><span class="n">_</span><span class="err">{</span><span class="n">y</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">mathcal</span><span class="err">{</span><span class="n">C</span><span class="err">}</span><span class="o">[</span><span class="n">y</span><span class="o">]</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">y</span><span class="err">\</span><span class="p">).</span>
</code></pre></div>

<h1>5.1 Implementation</h1>
<p>Step-1: Rephrasing via Supervised Contrasting (1 to $N$ ). First, we establish a general prompt of contrastive rephrasing to construct a pair of contrastive examples, as shown in the template below. This steers the model towards learning from the clean example and then rephrasing and rectifying the noisy examples. To be specific, given one clean example and $K$ noisy examples, we generate $N$ rephrased rationales for each noisy example independently and obtain $K \cdot N$ rephrased rationales.</p>
<h2>Prompt of Contrastive Rationale Rephrasing:</h2>
<p>Here are two examples for the same type of task: the first example has correct explanation and correct answer, and the second example has distracted explanation and correct answer. Please follow the first example and give me the correct explanation and answer for the second example, which should be logically consistent with the first one.
First Example: Q: [Question], E: [Explanation], A: [Answer].
Second Example: Q: [Question], E: [Explanation], A: [Answer].
Step-2: Rationale Selection ( $N$ to $M, N \geq M$ ). Next, we employ answer matching to select those rephrased examples with unchanged answers, leaving behind a refined candidate pool. Subsequently, we randomly select $M$ rephrased rationales from the pool and concatenate them to form the contexts.</p>
<p>Step-3: Rationale Exploration ( $M$ to $D, M \leq D$ ). For the $M$ different contexts, we explore rationales by repeated reasoning with the budget of $D$ reasoning repetitions. Notably, a higher temperature parameter, e.g., 1 , is set to introduce more randomness in generating diverse rationales.
Step-4: Answer Voting ( $D$ to 1). Ultimately, all the $D$ answers are equally voted into a final answer.
Instantiation. By tuning the hyper-parameters $N, M$, and $D$, we balance exploration and exploitation in the input and output space. The overall procedure of our proposed CD-CoT is presented in Algorithm 1. Besides, we further explain the details of each step of this algorithm in Appendix E.2.</p>
<p>| Task | Method $\mathcal{M}$ | Additional Information | $\operatorname{Acc}\left(\mathcal{M}, \mathcal{Q}, P_{\text {clean }}\right)$ | $\operatorname{Acc}\left(\mathcal{M}, \mathcal{Q}, P_{\text {improved }}\right)$ | | | $\operatorname{Acc}\left(\mathcal{M}, \mathcal{Q}, P_{\text {improved }}\right)$ | | | $\operatorname{Acc}\left(\mathcal{M}, \mathcal{Q}, P_{\text {improved }}\right)$ | | | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| | | | | Easy | Medium | Hard | Avg. | Easy | Medium | Hard | Avg. |  |
| Math Base-9 | Base | Ground Truth | 46.4 | 39.3 | 30.3 | 26.6 | 32.1 | 23.2 | 10.1 | 6.0 | 13.1 |
| | w/ SCO [29] | | 53.6 | 46.3 | 39.6 | 36.4 | 40.8 | 34.7 | 22.0 | 17.7 | 24.8 |
| | w/ BT [81] | Noise Position | 47.2 | 39.2 | 34.2 | 29.9 | 34.4 | 30.1 | 18.4 | 14.1 | 20.9 |
| | w/ CC [9] | Clean Demo | 44.9 | 43.3 | 44.6 | 45.5 | 44.5 | 37.2 | 31.7 | 20.7 | 22.2 |
| | w/ CD-CoT (ours) | Clean Demo | 60.7 | 59.7 | 60.7 | 57.2 | 59.2 | 54.0 | 58.7 | 48.4 | 53.7 |
| Math Base-11 | Base | Ground Truth | 23.9 | 19.1 | 13.6 | 10.7 | 14.5 | 14.0 | 6.7 | 3.6 | 8.1 |
| | w/ SCO [29] | | 33.0 | 28.2 | 24.0 | 20.0 | 24.4 | 29.2 | 20.0 | 17.2 | 22.1 |
| | w/ BT [81] | Noise Position | 24.3 | 17.9 | 17.2 | 13.7 | 16.3 | 12.8 | 9.2 | 6.8 | 9.6 |
| | w/ CC [9] | Clean Demo | 22.3 | 19.1 | 18.4 | 18.2 | 18.6 | 19.0 | 15.3 | 14.6 | 16.3 |
| | w/ CD-CoT (ours) | Clean Demo | 31.0 | 33.7 | 32.7 | 34.7 | 33.7 | 29.0 | 30.7 | 25.3 | 28.3 |
| Symbolic Equal | Base | Ground Truth | 32.7 | 28.1 | 25.1 | 23.0 | 25.4 | 29.1 | 26.1 | 22.7 | 26.0 |
| | w/ SCO [29] | | 28.5 | 34.9 | 33.4 | 32.1 | 33.7 | 34.0 | 34.1 | 34.5 | 34.2 |
| | w/ BT [81] | Noise Position | 31.8 | 26.0 | 22.7 | 22.6 | 23.8 | 26.3 | 22.7 | 22.9 | 24.0 |
| | w/ CC [9] | Clean Demo | 27.8 | 23.8 | 22.7 | 22.0 | 22.8 | 31.3 | 23.0 | 29.9 | 31.4 |
| | w/ CD-CoT (ours) | Clean Demo | 42.7 | 44.7 | 42.7 | 44.0 | 43.8 | 42.6 | 41.3 | 42.7 | 42.2 |
| Symbolic Longer | Base | Ground Truth | 9.2 | 6.3 | 7.2 | 6.0 | 6.5 | 7.0 | 6.8 | 6.0 | 6.6 |
| | w/ SCO [29] | | 18.7 | 12.1 | 10.5 | 11.3 | 11.3 | 15.2 | 15.9 | 9.8 | 13.6 |
| | w/ BT [81] | Noise Position | 7.2 | 3.4 | 3.5 | 2.5 | 3.1 | 3.8 | 3.6 | 3.6 | 3.7 |
| | w/ CC [9] | Clean Demo | 9.4 | 9.8 | 7.9 | 7.9 | 8.5 | 8.5 | 7.4 | 6.5 | 7.5 |
| | w/ CD-CoT (ours) | Clean Demo | 12.3 | 12.0 | 13.0 | 12.3 | 12.3 | 10.0 | 11.0 | 11.1 |  |
| Commonsense | Base | Ground Truth | 45.7 | 44.3 | 42.3 | 41.4 | 42.7 | 36.7 | 33.4 | 28.3 | 32.8 |
| | w/ SCO [29] | | 63.5 | 60.1 | 56.1 | 60.3 | 58.8 | 56.2 | 58.5 | 57.9 | 57.5 |
| | w/ BT [81] | Noise Position | 47.7 | 23.5 | 28.3 | 32.5 | 28.1 | 11.6 | 11.0 | 15.8 | 12.8 |
| | w/ CC [9] | Clean Demo | 48.3 | 45.7 | 43.6 | 44.0 | 44.4 | 42.1 | 40.8 | 40.5 | 41.1 |
| | w/ CD-CoT (ours) | Clean Demo | 49.0 | 50.3 | 54.7 | 50.3 | 51.8 | 51.0 | 49.7 | 49.7 | 50.1 |</p>
<p>Table 8: Performance of denoising methods that require additional information for supervision.
Theoretical analysis. To understand the underlying mechanism of CD-CoT, we also conduct the theoretical analysis based on the distinguishability [90] of in-context learning. The full analysis is in Appendix D, where we find that the noisy demonstration in ICL can decrease the distinguishability of in-context matching with the clean-prompt distribution, while our method can mitigate this issue. Besides, we build a self-supervised variant of CD-CoT and empirically evaluate it in Appendix F.7.</p>
<h1>5.2 Empirical Study</h1>
<p>In this part, we empirically verify the effectiveness of CD-CoT and start by introducing the baselines.
Baseline methods. We employ three methods that require additional information: (1) Self-Correction with Oracle Feedback (SCO) [29] utilizes the ground truth answers of test questions to determine when to terminate the self-correction loop; (2) Backtracking (BT) [81] guides self-correction by providing the model with the position of the first noisy thought; (3) Contrastive Chain-of-Thought (CC) [9] conducts direct reasoning with all the noisy or clean examples without implicit or explicit denoising.
Main results. As in Tab. 8, CD-CoT demonstrates a significant performance improvement across all datasets, with an average improvement of $17.8 \%$ compared with the base model under noisy settings. Notably, on Math-Base-9, Math-Base-11, and Symbolic-Equal, CD-CoT surpasses all baseline methods by a significant margin. On Symbolic-Longer and Commonsense, CD-CoT only slightly lags behind SCO. However, SCO requires the ground truth answer to the test question, which should be unknown in practice, as pointed out in [29]. In comparison, CD-CoT only necessitates an additional clean demonstration, making it much more practical to apply across realistic scenarios. Notably, CD-CoT outperforms SCO in 20 out of 30 settings and surpasses BT, CC in all 30 settings.
Besides, CD-CoT displays remarkable resistance to the magnitude of noise. Therein, CD-CoT demonstrates enhanced resilience against inaccurate noise on mathematical tasks, which are quite challenging. For instance, on Math Base-9 with inaccurate rationales, the average accuracies of SCO and BT decline significantly by $28.8 \%$ and $26.3 \%$ compared to the accuracies with clean rationales. In contrast, CD-CoT exhibits a more modest decline of $7.0 \%$. An ablation study of components in Appendix F. 3 demonstrates the denoising power and performance gain of CD-CoT, attributed to its contrastive denoising with rationale rephrasing as well as repeated reasoning with voting components.
Ablation study of varying hyper-parameters. By manipulating the values of $N, M, D$, and $C$, we generate diverse algorithm instances. Here, $D$ denotes the reasoning times allocated to the $M$ inputs, while $C$ signifies whether the clean example is used in step 3. As demonstrated in Tab. 9, the clean example utilized by CD-CoT during the reasoning process plays a pivotal role. The omission of this clean example results in an average decrease of $3.3 \%$ and $4.5 \%$ in accuracy under irrelevant noise and inaccurate noise, respectively. Besides, the accuracy exhibits subtle variations when employing different algorithm instances, with the highest average accuracy observed at $51.3 \%$ and the lowest average accuracy at $49.3 \%$. Further, Tab. 10 presents the average number of tokens used in reasoning. We set $M=2$ to strike a balance. Please refer to Appendix E. 3 for detailed hyper-parameter selection.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyper-parameters</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\operatorname{Acc}(\mathcal{M}, \mathcal{Q}, \mathcal{P}_{\text {molevant }})$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\operatorname{Acc}(\mathcal{M}, \mathcal{Q}, \mathcal{P}_{\text {inaccurate }})$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$N$</td>
<td style="text-align: center;">$M$</td>
<td style="text-align: center;">$D$</td>
<td style="text-align: center;">$C$</td>
<td style="text-align: center;">Base-9</td>
<td style="text-align: center;">Sym.(E)</td>
<td style="text-align: center;">Com.</td>
<td style="text-align: center;">Base-9</td>
<td style="text-align: center;">Sym.(E)</td>
<td style="text-align: center;">Com.</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">51.0</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">55.7</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$2 \star 3$</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">49.7</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$2 \star 3$</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">53.0</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$1 \star 2 \star 2$</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">49.0</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$1 \star 2 \star 2$</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">52.3</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">48.7</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">50.7</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison of accuracy on medium-level tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyper-parameters</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">#Tokens in step-3 (inc)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">#Tokens in step-3 (ina.)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">N</td>
<td style="text-align: center;">M</td>
<td style="text-align: center;">D</td>
<td style="text-align: center;">C</td>
<td style="text-align: center;">Base-9</td>
<td style="text-align: center;">Sym.(E)</td>
<td style="text-align: center;">Com.</td>
<td style="text-align: center;">Base-9</td>
<td style="text-align: center;">Sym.(E) Com.</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">1440</td>
<td style="text-align: center;">3162</td>
<td style="text-align: center;">788</td>
<td style="text-align: center;">1428</td>
<td style="text-align: center;">3170</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">1301</td>
<td style="text-align: center;">2685</td>
<td style="text-align: center;">660</td>
<td style="text-align: center;">1295</td>
<td style="text-align: center;">2732</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$2 \star 3$</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">2175</td>
<td style="text-align: center;">4934</td>
<td style="text-align: center;">1269</td>
<td style="text-align: center;">2156</td>
<td style="text-align: center;">4989</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$2 \star 3$</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">1864</td>
<td style="text-align: center;">4044</td>
<td style="text-align: center;">1005</td>
<td style="text-align: center;">1842</td>
<td style="text-align: center;">4087</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$1 \star 2 \star 2$</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">2902</td>
<td style="text-align: center;">6704</td>
<td style="text-align: center;">1772</td>
<td style="text-align: center;">2878</td>
<td style="text-align: center;">6785</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$1 \star 2 \star 2$</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">2416</td>
<td style="text-align: center;">5360</td>
<td style="text-align: center;">1372</td>
<td style="text-align: center;">2393</td>
<td style="text-align: center;">5443</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Y</td>
<td style="text-align: center;">4368</td>
<td style="text-align: center;">10340</td>
<td style="text-align: center;">2764</td>
<td style="text-align: center;">4339</td>
<td style="text-align: center;">10514</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">3535</td>
<td style="text-align: center;">8099</td>
<td style="text-align: center;">2088</td>
<td style="text-align: center;">3506</td>
<td style="text-align: center;">8303</td>
</tr>
</tbody>
</table>
<p>Table 10: Comparison of #tokens on medium-level tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">$\begin{aligned} &amp; \operatorname{Acc}(\mathcal{M}, \mathcal{Q}, \mathcal{P}_{\text {molevant }}) \ &amp; \text { Base-9 } \end{aligned}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\begin{aligned} &amp; \operatorname{Acc}(\mathcal{M}, \mathcal{Q}, \mathcal{P}_{\text {inaccurate }}) \ &amp; \text { Base-9 } \end{aligned}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\begin{aligned} &amp; \operatorname{Acc}(\mathcal{M}, \mathcal{Q}, \mathcal{P}_{\text {inaccurate }}) \ &amp; \text { Base-9 } \end{aligned}$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-3.5-turbo</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CC</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CD-CoT</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Gemini-Pro</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CC</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CD-CoT</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-70B</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CC</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CD-CoT</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mixtral-8x7B</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">34.9</td>
<td style="text-align: center;">3.7</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BT</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CC</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CD-CoT</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 11: Comparing methods with different LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Original Noisy Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">In base-9, the digits are "012345678". 9+5+17. We have $6+7=13$ in base-10. Since we're in base-9, that exceeds the maximum value of 8 for a single digit. 13 mod $9=4$, so the digit is 4 and the carry is 1 . We have $8$ $+5+1=14$ in base-10. $14+5+12.14 \mathrm{mod} 9=5$, so the digit is 5 and the carry is 1 . A leading digit is 1 . So the answer is 154. Answer: 154.</td>
</tr>
<tr>
<td style="text-align: center;">SM In base-9, the digits are "012345678". 9+5 $=17$. We have $6+7=13$ in base-10. Since we're in base-9, 13 exceeds the maximum value of 8 for a single digit. 13 mod $9=4$, so the digit is 4 and the carry is 1 . We have $8+5+1=14$ in base 10. 14 mod $9=8$. 14 div $9=1$, so the digit is 8 and the carry is 1. A leading digit is 1 . So the answer is 154. Answer: 154.</td>
</tr>
<tr>
<td style="text-align: center;">SP In base-9, the digits range from 0 to 8 . Let's add 86 and 57 step by step. First, we add the rightmost digits, which are 6 and 7 . In base-10, $6+7$ equals 13. However, since we are in base-9, the maximum value for a single digit is 8 . Therefore, we need to find the remainder when 13 is divided by 9 . The remainder is 4 , so the digit in the units place is 4 and we have a carry of 1 . Next, we add the next digits, which are 8 and 5 , along with the carry of 1 . In base-10, $8+5+1$ equals 14. Again, we need to find the remainder when 14 is divided by 9 . The remainder is 5 , so the digit in the tens place is 5 and we have a carry of 1 . Finally, we have a leading digit of 1 . So the final answer in base-9 is 154. Answer: 154.</td>
</tr>
<tr>
<td style="text-align: center;">Ours In base-9, the digits are "012345678". We have $6+7=13$ in base-10. Since we're in base-9, that exceeds the maximum value of 8 for a single digit. 13 mod $9=4$, so the digit is 4 and the carry is 1 . We have $8+5+1=$ 14 in base 10. 14 mod $9=5$, so the digit is 5 and the carry is 1 . A leading digit is 1 . So the answer is 154. Answer: 154.</td>
</tr>
</tbody>
</table>
<p>Table 12: Comparison of rephrased rationales by different reasoning methods.</p>
<p>Ablation study with different LLMs. We examine the generalization of CD-CoT across different LLMs. As shown in Tab. 11, with comparably more powerful LLMs, e.g., GPT-3.5-turbo and GeminiPro, CD-CoT demonstrates notable improvements in average accuracy. It respectively achieves increases of $23.4 \%$ and $21.6 \%$ in accuracy compared to base models and surpasses all the baselines.</p>
<p>Case Study. We illustrate the denoising effects of various robust methods using Math Base-9 as an example. The results in Tab. 12 indicate that the introduction of random characters by SM disrupts the logic of the rationale. SD fails to eliminate all the noise while recovering the input content, and SP alters the original rationale's reasoning process even when noise removal is successful. In contrast, CD-CoT significantly removes noisy thoughts and ensures format alignment with the original rationale. More denoising examples and an entire case study are in Appendix F. 9 and G, respectively.</p>
<h1>6 Conclusion</h1>
<p>In this work, we investigate the under-explored problem of noisy rationales in LLMs. We introduce the NoRa dataset, which tests LLMs against irrelevant or inaccurate thoughts in question-answer scenarios. Our findings show LLMs' vulnerability to noisy rationales is inadequately mitigated by existing robust methods. We thereby design the CD-CoT method to enhance the robustness via contrastive denoising. The extension advocates for advancing LLMs by strategies, e.g., external knowledge bases with a retrieval-augmented framework, robust inductive reasoning to extract rules from noisy examples, and multi-modal data integration to enhance the robustness of LLM reasoning.</p>
<h1>Acknowledgements</h1>
<p>ZKZ, RT, JNZ, and BH were supported by Guangdong Basic and Applied Basic Research Foundation Nos. 2022A1515011652 and 2024A1515012399, NSFC General Program No. 62376235, HKBU Faculty Niche Research Areas No. RC-FNRA-IG/22-23/SCI/04, and HKBU CSD Departmental Incentive Scheme. YWL and ZMW were supported by National Natural Science Foundation of China under Grants 62271357, Natural Science Foundation of Hubei Province under Grants 2023BAB072, and Fundamental Research Funds for the Central Universities under Grants 2042023kf0134. The authors especially thank Jiangchao Yao, Xuan Li, and Xiao Feng for the constructive discussions and insightful suggestions that helped improve this work.</p>
<h2>References</h2>
<p>[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[2] Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when they're hallucinating references? arXiv preprint arXiv:2305.18248, 2023.
[3] Afra Feyza AkyÃ¼rek, Ekin AkyÃ¼rek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, and Niket Tandon. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. arXiv preprint arXiv:2305.08844, 2023.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.
[5] Paul Chandler and John Sweller. Cognitive load theory and the format of instruction. Cognition and instruction, 1991.
[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020.
[7] Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, Kaili Ma, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-ofdistribution generalization on graphs. In NeurIPS, 2022.
[8] Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, and James Cheng. Understanding and improving feature learning for out-of-distribution generalization. In NeurIPS, 2023.
[9] Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and Lidong Bing. Contrastive chain-of-thought prompting. arXiv preprint arXiv:2311.09277, 2023.
[10] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. Quac: Question answering in context. arXiv preprint arXiv:1808.07036, 2018.
[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 2023.
[12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[13] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In ICML, 2019.
[14] Michela Del Vicario, Antonio Scala, Guido Caldarelli, H Eugene Stanley, and Walter Quattrociocchi. Modeling confirmation bias and polarization. Scientific reports, 2017.</p>
<p>[15] Ronald A DeVore and George G Lorentz. Constructive approximation. 1993.
[16] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu , and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.
[17] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 2020.
[18] C Daniel Freeman, Laura Culp, Aaron Parisi, Maxwell L Bileschi, Gamaleldin F Elsayed, Alex Rizkowsky, Isabelle Simpson, Alex Alemi, Azade Nova, Ben Adlam, et al. Frontier language models are not robust to adversarial arithmetic, or" what do i need to say so you agree $2+2=5$ ? arXiv preprint arXiv:2311.07587, 2023.
[19] Chengguang Gan and Tatsunori Mori. Sensitivity and robustness of large language models to prompt in japanese. arXiv preprint arXiv:2305.08714, 2023.
[20] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal DaumÃ© Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 2021.
[21] Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel Galley, Jianfeng Gao, and Hoifung Poon. Self-verification improves few-shot clinical information extraction. arXiv preprint arXiv:2306.00024, 2023.
[22] Chen Gong, Qizhou Wang, Tongliang Liu, Bo Han, Jane You, Jian Yang, and Dacheng Tao. Instance-dependent positive and unlabeled learning with labeling bias estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.
[23] Thomas L Griffiths. Understanding human intelligence through human limitations. Trends in Cognitive Sciences, 2020.
[24] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NeurIPS, 2018.
[25] Alex Havrilla and Maia Iyer. Understanding the effect of noise in llm training data with algorithmic chains of thought. arXiv preprint arXiv:2402.04004, 2024.
[26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.
[27] Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. Can large language models understand real-world complex instructions? In AAAI, 2024.
[28] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.
[29] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. In $I C L R$, 2024.
[30] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023.
[31] Irving L Janis. Groupthink. IEEE Engineering Management Review, 2008.
[32] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In EMNLP, 2017.</p>
<p>[33] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.
[34] Shuyang Jiang, Yuhao Wang, and Yu Wang. Selfevolve: A code evolution framework via large language models. arXiv preprint arXiv:2306.02907, 2023.
[35] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, 2020.
[36] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023.
[37] BJK Kleijn and AW van der Vaart. The bernstein-von-mises theorem under misspecification. Electronic Journal of Statistics, 2012.
[38] Sin Yee Koh. The inversion of majority/minority at the de/reterritorialised urban higher education enclave: Xiamen university malaysia. Urban Studies, 2022.
[39] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022.
[40] Seonmin Koo, Chanjun Park, Seolhwa Lee, Jaehyung Seo, Sugyeong Eo, Hyeonseok Moon, and Heuiseok Lim. Uncovering the risks and drawbacks associated with the use of synthetic data for grammatical error correction. IEEE Access, 2023.
[41] Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In ICML, 2018.
[42] Fangyu Lei, Xiang Li, Yifan Wei, Shizhu He, Yiming Huang, Jun Zhao, and Kang Liu. S 3Ì hqa: A three-stage approach for multi-hop text-table hybrid question answering. arXiv preprint arXiv:2305.11725, 2023.
[43] Miaoran Li, Baolin Peng, and Zhu Zhang. Self-checker: Plug-and-play modules for factchecking with large language models. arXiv preprint arXiv:2305.14623, 2023.
[44] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize large language model to be jailbreaker. arXiv preprint arXiv:2311.03191, 2023.
[45] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv preprint arXiv:2310.06500, 2023.
[46] Ke Liang, Yue Liu, Sihang Zhou, Wenxuan Tu, Yi Wen, Xihong Yang, Xiangjun Dong, and Xinwang Liu. Knowledge graph contrastive learning based on relation-symmetrical structure. IEEE Transactions on Knowledge and Data Engineering, 2023.
[47] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, Fuchun Sun, and Kunlun He. A survey of knowledge graph reasoning on graph types: Static, dynamic, and multi-modal. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.
[48] Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. We're afraid language models aren't modeling ambiguity. arXiv preprint arXiv:2304.14399, 2023.
[49] Feng Liu, Bo Han, Tongliang Liu, Chen Gong, Gang Niu, Mingyuan Zhou, Masashi Sugiyama, et al. Probabilistic margins for instance reweighting in adversarial training. In NeurIPS, 2021.
[50] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.</p>
<p>[51] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.
[52] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.
[53] Richard E Mayer. Thinking and problem solving: An introduction to human cognition and learning. Scott, Foresman, 1977.
[54] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In EMNLP, 2022.
[55] Carey K Morewedge and Daniel Kahneman. Associative processes in intuitive judgment. Trends in cognitive sciences, 2010.
[56] Paul Norris. Emotional reasoning. University of Massachusetts Amherst, 2000.
[57] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188, 2023.
[58] Lalchand Pandia and Allyson Ettinger. Sorting through the noise: Testing robustness of information processing in pre-trained language models. In EMNLP, 2021.
[59] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023.
[60] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. In NeurIPS, 2021.
[61] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019.
[62] Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. Smoothllm: Defending large language models against jailbreaking attacks. arXiv preprint arXiv:2310.03684, 2023.
[63] Tomas Rokicki, Herbert Kociemba, Morley Davidson, and John Dethridge. The diameter of the rubik's cube group is twenty. SIAM REVIEW, 2014.
[64] Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. "everyone wants to do the model work, not the data work": Data cascades in high-stakes ai. In CHI, 2021.
[65] Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.
[66] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? In NeurIPS, 2023.
[67] JÃ©rÃ©my Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023.
[68] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael SchÃ¤rli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In ICML, 2023.
[69] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In NeurIPS, 2023.</p>
<p>[70] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 2016.
[71] Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L Hamilton. Clutrr: A diagnostic benchmark for inductive reasoning from text. arXiv preprint arXiv:1908.06177, 2019.
[72] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, AdriÃ  Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.
[73] Joe Stacey, Jianpeng Cheng, John Torr, Tristan Guigue, Joris Driesen, Alexandru Coca, Mark Gaynor, and Anders Johannsen. Lucid: Llm-generated utterances for complex and interesting dialogues. arXiv preprint arXiv:2403.00462, 2024.
[74] Zhenheng Tang, Yuxin Wang, Xin He, Longteng Zhang, Xinglin Pan, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Bingsheng He, et al. Fusionai: Decentralized training and deploying llms with massive consumer-level gpus. arXiv preprint arXiv:2309.01172, 2023.
[75] Zhenheng Tang, Xueze Kang, Yiming Yin, Xinglin Pan, Yuxin Wang, Xin He, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Amelie Chi Zhou, Bo Li, Bingsheng He, and Xiaowen Chu. Fusionllm: A decentralized llm training system on geo-distributed gpus with adaptive compression. arXiv preprint arXiv:2410.12707, 2024.
[76] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.
[77] David Thorstad. Cognitive bias in large language models: Cautious optimism meets antipanglossian meliorism. arXiv preprint arXiv:2311.10932, 2023.
[78] Qingyuan Tian, Hanlun Zhu, Lei Wang, Yang Li, and Yunshi Lan. R3 prompting: Review, rephrase and resolve for chain-of-thought reasoning in large language models under noisy context prompting: Review, rephrase and resolve for chain-of-thought reasoning in large language models under noisy context. In EMNLP, 2023.
[79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
[80] Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. Science, 1974.
[81] Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, and Victor CÃ¢rbune. Llms cannot find reasoning errors, but can correct them! arXiv preprint arXiv:2311.08516, 2023.
[82] Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. In NeurIPS, 2023.
[83] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR, 2023.
[84] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.
[85] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.</p>
<p>[86] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.
[87] G Elliott Wimmer and Daphna Shohamy. Preference by association: how memory mechanisms in the hippocampus bias decisions. Science, 2012.
[88] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin AkyÃ¼rek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv preprint arXiv:2307.02477, 2023.
[89] Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui, Qi Zhang, and Xuanjing Huang. Self-polish: Enhance reasoning in large language models via problem refinement. arXiv preprint arXiv:2305.14497, 2023.
[90] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.
[91] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023.
[92] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In NeurIPS, 2023.
[93] Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, May, 2023.
[94] Xi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning. In NeurIPS, 2022.
[95] Simon Chi Lok Yu, Jie He, Pasquale Minervini, and Jeff Z Pan. Evaluating the adversarial robustness of retrieval-based in-context learning for large language models. arXiv preprint arXiv:2405.15984, 2024.
[96] Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. Improving language models via plug-and-play retrieval feedback. arXiv preprint arXiv:2305.14002, 2023.
[97] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. In NeurIPS, 2022.
[98] Jiehang Zeng, Jianhan Xu, Xiaoqing Zheng, and Xuanjing Huang. Certified robustness to text adversarial attacks by randomized [mask]. Computational Linguistics, 2023.
[99] Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. arXiv preprint arXiv:2211.04486, 2022.
[100] Yongqi Zhang, Zhanke Zhou, Quanming Yao, Xiaowen Chu, and Bo Han. Adaprop: Learning adaptive propagation for graph neural network based knowledge graph reasoning. In SIGKDD, 2023.
[101] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.
[102] Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, and Shiyu Chang. Certified robustness for large language models with self-denoising. arXiv preprint arXiv:2307.07171, 2023.
[103] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In $I C L R, 2023$.</p>
<p>[104] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: A survey. ACM Transactions on Intelligent Systems and Technology, 2023.
[105] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.
[106] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In ICML, 2021.
[107] Hongyi Zheng and Abulhair Saparov. Noisy exemplars make large language models more robust: A domain-agnostic behavioral analysis. In EMNLP, 2023.
[108] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023.
[109] Denny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.
[110] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.
[111] Zhanke Zhou, Jiangchao Yao, Jiaxu Liu, Xiawei Guo, Quanming Yao, Li He, Liang Wang, Bo Zheng, and Bo Han. Combating bilateral edge noise for robust link prediction. In NeurIPS, 2023.
[112] Zhanke Zhou, Chenyu Zhou, Xuan Li, Jiangchao Yao, Quanming Yao, and Bo Han. On strengthening and defending graph reconstruction attack with markov chain approximation. In ICML, 2023.
[113] Zhanke Zhou, Yongqi Zhang, Jiangchao Yao, Quanming Yao, and Bo Han. Less is more: One-shot subgraph reasoning on large-scale knowledge graphs. In ICLR, 2024.
[114] Yilun Zhu, Joel Ruben Antony Moniz, Shruti Bhargava, Jiarui Lu, Dhivya Piraviperumal, Site Li, Yuan Zhang, Hong Yu, and Bo-Hsiang Tseng. Can large language models understand context? arXiv preprint arXiv:2402.00858, 2024.</p>
<h1>Appendix</h1>
<p>A Further Discussion ..... 19
B Related Work ..... 19
B. 1 In-context Learning ..... 20
B. 2 Self-correction ..... 21
B. 3 Self-consistency ..... 22
B. 4 External Supervision ..... 22
B. 5 Relation with Literature ..... 23
C Benchmark ..... 24
C. 1 Motivation ..... 24
C. 2 A Further Discussion on Noisy Rationales in CoT Demonstrations ..... 24
C. 3 Real-world Examples ..... 27
C. 4 Noise Generation ..... 31
C. 5 NoRa Datasheet ..... 32
C. 6 Full Examples of the NoRa Dataset ..... 34
D Theoretical Analysis ..... 37
E Implementation Details ..... 40
E. 1 Baseline Methods ..... 40
E. 2 Contrastive Denoising with Noisy Chain-of-Thought ..... 41
E. 3 Hyper-parameter Optimization ..... 41
F Full Experiments ..... 43
F. 1 Detailed Setups of the Experiments ..... 43
F. 2 Supplementary Results of the Main Experiments ..... 43
F. 3 The Superior Performance and Denoising Effectiveness ..... 45
F. 4 The Difficulty of Irrelevant Noise Semantics ..... 49
F. 5 The Number of Noisy Thoughts ..... 51
F. 6 The Number of Noisy Examples ..... 52
F. 7 Variants of CD-CoT ..... 53
F. 8 New Datasets and Large-scale Real Problems ..... 56
F. 9 Rephrased Examples of Different Denoising Methods ..... 59
G Further Case Study ..... 62
H NeurIPS Checklist ..... 64</p>
<h1>A Further Discussion</h1>
<p>Broader impact. This paper presents work that aims to advance the field of trustworthy machine learning and large language models. We do not find any negative societal consequences of our work. This paper does not raise any ethical concerns. This study does not involve human subjects, practices, data set releases, potentially harmful insights, methodologies, applications, potential conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security issues, legal compliance, or research integrity issues.
Limitations. The proposed CD-CoT method is currently dependent on human-annotated, clean rationales. Future enhancements could include developing a self-supervised variant that does not rely on such examples. Exploring strategies like using contrasting noisy examples or incorporating an external knowledge base, possibly through a retrieval-augmented denoising framework, may offer significant advances in automation and robustness of reasoning.
Extensions. CoT and its variants have predominantly focused on deductive reasoning, leaving inductive reasoning largely unexplored. Investigating the ability of LLMs to extract rules from noisy examples is a compelling area. Additionally, theoretical analysis of noisy ICL can offer deeper insights into the Noisy-R problem. Expanding the NoRa dataset to include multi-modal scenarios, particularly visual data, is also crucial for a more comprehensive understanding of the robustness of foundation models. Further extensions include knowledge-enhanced denoising [100, 46, 47, 113], generalization to out-of-distribution noisy scenarios [7, 8], and training to fundamentally improve the robustness of language models [75, 74].</p>
<h2>B Related Work</h2>
<p>In this section, we provide a detailed literature review as an extension of the preliminaries (Sec. 2), including in-context learning (Appendix B.1), self-correction methods (Appendix B.2), self-consistency methods (Appendix B.3), and external supervision (Appendix B.4). We further discuss the relation between our work and literature in Appendix B.5. We also provide Fig. 6 to better understand different reasoning settings.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Illustrations of different reasoning settings.</p>
<h1>B. 1 In-context Learning</h1>
<p>With the scaling of model size and corpus size [61, 4, 11, 105], large language models (LLMs) have demonstrated remarkable performance across a variety of tasks through in-context learning (ICL), that is, learning from a few input-output examples in the context. As a training-free framework, ICL can save on expensive training costs and be easily adapted to diverse new tasks [88, 1].
However, standard ICL faces challenges when addressing intricate reasoning problems. Wei et al. [85] proposes chain-of-thought prompting (CoT), a methodology that explicitly guides LLMs in generating sequential reasoning steps to enhance their performance on intricate reasoning tasks. In particular, CoT replaces the input-output exemplars in ICL with triplets in the form of <input, rationale, output>, thereby enabling the model to learn explicit reasoning processes.
In the literature, various versions of chain-of-thought prompting have been proposed. To alleviate the human effort required for constructing triplet exemplars, Kojima et al. [39] proposes zero-shot CoT, a method that explicitly encourages models to generate a chain of reasoning first and then derive the final answer by employing natural language prompts like "Let's think step by step". Wang et al. [83] shows that self-consistency sampling can improve reasoning accuracy by sampling diverse reasoning paths and then taking the majority vote. Zelikman et al. [97] proposes STaR, which leverages a small amount of human-annotated rationale data, along with a large dataset of question-label pairs without rationales. This approach iteratively generates additional rationales and enhances the model's ability to generate reasoning steps by fine-tuning.
Least-to-most prompting (LtM) [109] enhances the reasoning capability of models by decomposing a given problem into sub-problems. In particular, LtM initially breaks down the task into a series of consecutive sub-problems and subsequently answers them one by one. During the process of responding, the answer to the preceding sub-problem is incorporated into the prompt for the succeeding one. Tree of Thoughts (ToT) [92] extends LtM by exploring multiple reasoning possibilities at each step. Specifically, ToT first decomposes a given problem into several reasoning steps and generates multiple answers for each step, ultimately constructing a tree structure. Subsequently, ToT employs BFS or DFS to traverse the tree, yielding the final rationale and answer.
Vulnerabilities of ICL. Despite being promising, some works point out the brittleness and oversensitivity of ICL. Liu et al. [50], Perez et al. [60], Zhang et al. [99] demonstrate that ICL performance depends heavily on the choice of exemplars. Meanwhile, Zhao et al. [106], Lu et al. [51] observe that the arrangement order of in-context examples is also crucial to the ICL performance, potentially shifting results from near state-of-the-art to a random guessing. Moreover, Ye and Durrett [94], Gan and Mori [19], Zheng and Saparov [107], Zhang et al. [102] reveal LLMs' deficiencies when handling subtle perturbations within the prompts, even when such perturbations do not alter any semantic meaning. Yu et al. [95] explores the robustness of retrieval-augmented in-context learning (ICL) against demonstration attacks and test sample attacks. It focuses on perturbing the example questions (i.e., noisy questions) or labels, while our work focuses on the rationales of the examples (i.e., noisy rationales). In addition, previous work on safety [44, 112] and data noise [24, 49, 22, 111] might also inspire the robust problems in ICL. Overall, the enhancement of reasoning performance brought about by ICL is inherently unstable and susceptible to example selection, example ordering, and prompt perturbations. These observations underscore the importance the robustness of other aspects.
The aforementioned efforts primarily revolve around the idealized ICL, which utilizes high-quality prompts free from any noise or interference. Conversely, a parallel line of research has emerged, exploring the impact of noisy prompts on the performance of LLMs. Min et al. [54] examines the impact of in-context examples on ICL. This work observes that incorporating out-of-distribution input texts significantly diminishes the performance of standard question answering. Wei et al. [86] devises two different set-ups of ICL: ICL with flipped labels and ICL with semantically unrelated labels. Their investigation reveals that LLMs possess the capability to override semantic priors when confronted with in-context exemplars that contradict these priors. This phenomenon also suggests that larger models may be more susceptible to the influence of the noise present in examples. Shi et al. [68] examines the impact of irrelevant context on LLMs, and the results suggest that the inclusion of irrelevant information can significantly impair the performance of the models. These studies further illuminate the fragility and instability inherent in the reasoning capabilities of LLMs.
However, the previous works mainly consider the noisy questions/answers in standard ICL. In contrast, we move to the under-explored noisy rationales problem in the context of CoT, as illustrated in Fig. 6.</p>
<p>Numerous strategies have been proposed to address the vulnerabilities of LLM reasoning during in-context learning. These approaches can be categorized into self-correction and self-consistency, which are introduced as follows.</p>
<h1>B. 2 Self-correction</h1>
<p>Self-correction emerges as a promising direction to enhance LLM reasoning, where LLMs attempt to correct their initial responses based on feedback. One popular line of research involves utilizing manual labor or external systems to evaluate and refine models. However, this can be costly due to the manual labor involved. Another line of research leverages the LLM's inherent capabilities to correct its initial responses without the crutch of external feedback. This methodology is a promising way to make LLM-based solutions practical and deployable [57].</p>
<p>Self-correction with internal feedback. In this line of research, the LLM is required to correct response trajectories based solely on its inherent capabilities. Huang et al. [28] first demonstrates the self-improvement potential of LLMs by utilizing a pre-trained LLM to generate rationale-augmented answers for unlabeled questions using CoT and majority voting and then fine-tuning the LLM using those self-generated labels, eventually improving the general reasoning ability.
When addressing problems, people typically engage in trial and error, coupled with reflective thinking, to discern the correct solutions. Inspired by this, Madaan et al. [52] proposes Self-refine, a simple and direct approach to improving LLM's output. In this approach, an LLM is used to create an initial output. Then, the model provides feedback on its own output in multiple dimensions. Based on this feedback, the model refines its initial output and repeats this process until it reaches a specified limit or the LLM determines that no further adjustments are necessary.
Encouraged by the augmented efficacy achieved through self-feedback mechanisms, Ye et al. [93] releases SelFee, a new instruction-following language model that generates self-feedback on its response and self-revises based on the feedback. The development of SelFee involves the fine-tuning of LLaMA by utilizing training instances generated by ChatGPT.
In addition, Gero et al. [21] introduces Self-verification, suggesting that by asking LLMs to provide provenance for their own outputs and conducting checks, it is possible to alleviate LLMs' issues regarding accuracy and interpretability in crucial domains such as healthcare. On the other hand, Xi et al. [89] focuses on the simplicity and comprehensibility of the given questions, proposing Selfpolish (SP). This method instructs the LLM to iteratively refine the test question by removing irrelevant information and rearranging the logical structure, thereby improving the reasoning performance.
While the self-correction methodologies based on internal feedback appear promising, [29] categorizes such self-correction methods as intrinsic self-correction (ISC) and demonstrates that the model's performance drops on all benchmarks after using ISC. This work points out that LLMs struggle to self-correct their responses without external feedback, and the corrected responses often exhibit inferior quality compared to their initial counterparts. Saparov and He [65] reveals that while models are able to produce valid reasoning steps with high probability when dealing with proof problems, they struggle with proof planning. In other words, when models occasionally generate incorrect proof steps, they are not able to return to the correct path.
Building upon this observation, Tyen et al. [81] further decomposes the self-correction process into two core components: mistake finding and output correction. This work demonstrates that current state-of-the-art LLMs cannot find mistakes reliably, even in the most simple and unambiguous cases, and suggests this is a main contributing factor to LLMs' inability to self-correct reasoning errors.
Self-correction with external feedback. External feedback offers a valuable external perspective, proving particularly advantageous in pinpointing errors that the large language model may not inherently recognize [57]. The sources of external feedback can be categorized as 1) human feedback $[67,69,36,81], 2)$ external tools $[34,96,43]$, and 3) other models [59, 3].
Scheurer et al. [67] proposes Imitation Learning from Language Feedback (ILF), an approach leveraging informative human feedback that involves conditioning the model on input, initial output, and feedback; selecting the most feedback-incorporated refinement; and fine-tuning the model to maximize the chosen refinement's likelihood given the input.
Similarly, Shinn et al. [69] introduces Reflexion, which fortifies language agents by relying on linguistic feedback generated by themselves rather than weight updates, resulting in noteworthy enhancements compared to a baseline agent across a spectrum of tasks. Kim et al. [36] demonstrates</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The sources of noisy rationales are discussed in Appendix C with extensive real-world examples.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>